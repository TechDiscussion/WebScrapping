[
{"website": "Twitter-Engineering", "title": "Summer of Code 2013 Results", "author": ["‎@cra‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013/summer-of-code-2013-results.html", "abstract": "For the second time, we had the opportunity to participate in the Google Summer of Code (GSoC) and we want to share news on the resulting open source activities. Unlike many GSoC participating organizations that focus on a single ecosystem, we have a variety of projects that span multiple programming languages and communities. We worked on three projects with three amazing students over the summer. thank you to our @gsoc participants for another fun summer of code! https://t.co/VMBmpSjZG0 pic.twitter.com/XYi69OVNQT — Twitter Open Source ( @TwitterOSS ) September 30, 2013 Matrix optimizations for @Scalding Tomas Tauber ( @shoguncz ) worked closely with his mentor Oscar Boykin ( @posco ) to improve the performance of Scalding by adding matrix optimizations (see the commits ). For example, how should we multiply A*B*C? Perhaps (A*B)*C takes a lot longer than A*(B*C) due to the sizes of the matrices. What about matrices with huge skew, such as Twitter’s follower graph where some users have millions of followers, but most have only a handful? By optimizing at the Matrix API layer, we can easily reap the benefits at higher layers. This project added a scheduler to the formulas users write with Matrices and performs the computation in the optimal order, where optimal is in terms of intermediate data size and formula tree-depth. See the performance results for more information. Asynchronous DNS support for the @Netty_Project Mohamed Bakkar worked with the lead of the Netty project Trustin Lee ( @trustin ) to add a built-in asynchronous DNS resolver . Instead of using the blocking DNS resolver provided by JDK, the new resolver will prevent applications built on top of Netty from their performance being impacted by slow or overloaded DNS servers. Authentication support for @ApacheMesos Ilim Ugur ( @ilimugur ) worked with Mesos committer Vinod Kone ( @vinodkone ) to add an authentication stage in Mesos before letting frameworks and slaves talk to the master(s) thereby making the communication between the modules forming Mesos (masters, slaves and frameworks) more secure. Thanks As part of GSoC, students and mentoring organizations receive a stipend. We are donating our portion of the stipend to the Software Freedom Conservancy ( @conservancy ) which is a 501(c)(3) organization that helps provide a non-profit home and infrastructure for open source projects like Git and Selenium. We really enjoyed the opportunity to take part in Google Summer of Code. Thanks again to our three students, mentors and to Google for the program. We look forward to supporting it next year.", "date": "2013-10-01"},
{"website": "Twitter-Engineering", "title": "Streaming MapReduce with Summingbird", "author": ["‎@sritchie‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013/streaming-mapreduce-with-summingbird.html", "abstract": "Today we are open sourcing Summingbird on GitHub under the ALv2. we’re thrilled to open source @summingbird , streaming mapreduce with @scalding and @stormprocessor #hadoop https://t.co/cV3LkCdCot — Twitter Open Source ( @TwitterOSS ) September 3, 2013 Summingbird is a library that lets you write streaming MapReduce programs that look like native Scala or Java collection transformations and execute them on a number of well-known distributed MapReduce platforms like Storm and Scalding . For example, a word-counting aggregation in pure Scala might look like this: def wordCount(source: Iterable[String], store: MutableMap[String, Long]) = source.flatMap { sentence => toWords(sentence).map(_ -> 1L) }.foreach { case (k, v) => store.update(k, store.get(k) + v) } However, counting words in Summingbird looks like this: def wordCount[P <: Platform[P]] (source: Producer[P, String], store: P#Store[String, Long]) = source.flatMap { sentence => toWords(sentence).map(_ -> 1L) }.sumByKey(store) The logic is exactly the same and the code is almost the same. The main difference is that you can execute the Summingbird program in: batch mode (using Scalding on Hadoop) real-time mode (using Storm) hybrid batch/real-time mode (offers attractive fault-tolerance properties) Building key-value stores for real-time serving is a special focus. Summingbird provides you with the foundation you need to build rock solid production systems. History and Motivation Before Summingbird at Twitter, users that wanted to write production streaming aggregations would typically write their logic using a Hadoop DSL like Pig or Scalding. These tools offered nice distributed system abstractions: Pig resembled familiar SQL, while Scalding, like Summingbird, mimics the Scala collections API. By running these jobs on some regular schedule (typically hourly or daily), users could build time series dashboards with very reliable error bounds at the unfortunate cost of high latency. While using Hadoop for these types of loads is effective, Twitter is about real-time and we needed a general system to deliver data in seconds, not hours. Twitter’s release of Storm made it easy to process data with very low latencies by sacrificing Hadoop’s fault tolerant guarantees. However, we soon realized that running a fully real-time system on Storm was quite difficult for two main reasons: Recomputation over months of historical logs must be coordinated with Hadoop or streamed through Storm with a custom log loading mechanism Storm is focused on message passing and random-write databases are harder to maintain The types of aggregations one can perform in Storm are very similar to what’s possible in Hadoop, but the system issues are very different. Summingbird began as an investigation into a hybrid system that could run a streaming aggregation in both Hadoop and Storm, as well as merge automatically without special consideration of the job author. The hybrid model allows most data to be processed by Hadoop and served out of a read-only store. Only data that Hadoop hasn’t yet been able to process (data that falls within the latency window) would be served out of a datastore populated in real-time by Storm. But the error of the real-time layer is bounded, as Hadoop will eventually get around to processing the same data and will smooth out any error introduced. This hybrid model is appealing because you get well understood, transactional behavior from Hadoop, and up to the second additions from Storm. Despite the appeal, the hybrid approach has the following practical problems: Two sets of aggregation logic have to be kept in sync in two different systems Keys and values must be serialized consistently between each system and the client The client is responsible for reading from both datastores, performing a final aggregation and serving the combined results Summingbird was developed to provide a general solution to these problems. The Way of the Summingbird Since Summingbird was designed to provide a streaming MapReduce model that can always be run in real-time, batch, or hybrid-merged modes, certain design choices were made. All input or output data fits into one of a few categories: Source , Service , Store , or Sink (see the core concepts on the wiki ). Like Hadoop, all state is kept in the data, and not with the workers. Events enter the system via a Source . When we are computing a new value to serve, we are always merging into a Store , which holds the value for each key. That merging operation is always associative, and this associativity is exploited for both parallelism as well as enabling the merging of hybrid online/offline jobs. In algebraic terms, all our merge operations are Monoids or Semigroups , and we have a developed a considerable collection of them for reuse. To do a join or lookup, we use a Service . A Service is a kind of real-time Key-Value readable store, e.g. a read from a database. In the offline mode, a Service is implemented as one of several types of joins. When we want to export a data stream, we write to a Sink . In real-time, this might be pushing onto a queue, versus on Hadoop where this is just materializing a new directory that covers some date range. With these four primitives we can easily compose Summingbird jobs: a store of one job becomes a service of another; sinks from one job become sources for another. This composability is very powerful and allows modular scaling of your computations. Once a new derived data source has proven it’s value, it may become an input into the next computation. Each Platform, such as Storm or Scalding, defines its own notion of these four data concepts and jobs can be written in a way that is completely agnostic to how a particular platform handles the data input and output. Summingbird Hatchlings It took a village and a lot of collaboration to develop Summingbird, and many more projects were spawned because of its existence, notably: Algebird : Algebird is an abstract algebra library for Scala. Many of the data structures included in Algebird have Monoid implementations, making them ideal to use as values in Summingbird aggregations. Bijection : Summingbird uses the Bijection project’s Injection typeclass to share serialization between different execution platforms and clients. Chill : Summingbird’s Storm and Scalding platforms both use the Kryo library for serialization. Chill augments Kryo with a number of helpful configuration options, and provides modules for use with Storm, Scala, Hadoop. Chill is also used by the Berkeley Amp Lab’s Spark project. Tormenta : Tormenta provides a type-safe layer over Storm’s Scheme and Spout interfaces. Storehaus : Summingbird’s client is implemented using Storehaus’s async key-value store traits. The Storm platform makes use of Storehaus’s MergeableStore trait to perform real-time aggregations into a number of commonly used backing stores, including Memcached and Redis. We’re very excited about growing a community around Summingbird as we move beyond our initial release. Future Work and Getting Involved If you’re interested in getting involved, some of our future plans include: Support for more execution platforms, e.g. Spark and Akka Pluggable optimizations for the Producer graph layer Projection and filter pushdown Support for filter-aware data sources like Parquet Libraries of higher-level mathematics and machine learning code on top of Summingbird’s Producer primitives More extensions to Summingbird’s related projects (listed below) More data structures with Monoid instances via Algebird More key-value stores implementations via Storehaus More Storm data sources, via Tormenta More tutorials and examples with public data sources To learn more and find links to tutorials and information around the web, check out the wiki . The latest ScalaDocs are hosted on the project page and discussion occurs primarily on the Summingbird mailing list ( summingbird@librelist.com ). Feature requests or bugs should be reported on the GitHub issue tracker . If you’re looking to get involved with the project, introduce yourself on the mailing list and check out issues tagged as newbie for ideas on first contributions. We also recommend you follow @summingbird to stay in touch; we’ll be listening. Acknowledgements Summingbird was originally authored by Oscar Boykin ( @posco ), Sam Ritchie ( @sritchie ) and Ashutosh Singhal ( @daashu ). We would also like to thank Doug Tangren ( @softprops ), Ryan LeCompte ( @ryanlecompte ), Aaron Siegel ( @asiegel ), Bill Darrow ( @billdarrow ), Brian Wallerstein ( @bwallerstein ), Wen-Hao Lue ( @wlue ), Alex Roetter ( @aroetter ), Zameer Manji ( @zmanji ) and Dmitriy Ryaboy ( @squarecog ) for their valuable feedback. Finally, we’d like to thank the 40+ community of contributors to Algebird (18 contributors), Bijection (11 contributors), Chill (6 contributors), Tormenta (3 contributors) and Storehaus (8 contributors).", "date": "2013-09-03"},
{"website": "Twitter-Engineering", "title": "Dremel made simple with Parquet", "author": ["‎@J_‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013/dremel-made-simple-with-parquet.html", "abstract": "Columnar storage is a popular technique to optimize analytical workloads in parallel RDBMs. The performance and compression benefits for storing and processing large amounts of data are well documented in academic literature as well as several commercial analytical databases . The goal is to keep I/O to a minimum by reading from a disk only the data required for the query. Using Parquet at Twitter , we experienced a reduction in size by one third on our large datasets. Scan times were also reduced to a fraction of the original in the common case of needing only a subset of the columns. The principle is quite simple: instead of a traditional row layout, the data is written one column at a time. While turning rows into columns is straightforward given a flat schema, it is more challenging when dealing with nested data structures. We recently introduced Parquet , an open source file format for Hadoop that provides columnar storage. Initially a joint effort between Twitter and Cloudera, it now has many other contributors including companies like Criteo. Parquet stores nested data structures in a flat columnar format using a technique outlined in the Dremel paper from Google. Having implemented this model based on the paper, we decided to provide a more accessible explanation. We will first describe the general model used to represent nested data structures. Then we will explain how this model can be represented as a flat list of columns. Finally we’ll discuss why this representation is effective. To illustrate what columnar storage is all about, here is an example with three columns. In a row-oriented storage, the data is laid out one row at a time as follows: Whereas in a column-oriented storage, it is laid out one column at a time: There are several advantages to columnar formats. Organizing by column allows for better compression, as data is more homogenous. The space savings are very noticeable at the scale of a Hadoop cluster. I/O will be reduced as we can efficiently scan only a subset of the columns while reading the data. Better compression also reduces the bandwidth required to read the input. As we store data of the same type in each column, we can use encodings better suited to the modern processors’ pipeline by making instruction branching more predictable. The model To store in a columnar format we first need to describe the data structures using a schema . This is done using a model similar to Protocol buffers . This model is minimalistic in that it represents nesting using groups of fields and repetition using repeated fields. There is no need for any other complex types like Maps, List or Sets as they all can be mapped to a combination of repeated fields and groups. The root of the schema is a group of fields called a message. Each field has three attributes: a repetition, a type and a name. The type of a field is either a group or a primitive type (e.g., int, float, boolean, string) and the repetition can be one of the three following cases: required : exactly one occurrence optional : 0 or 1 occurrence repeated : 0 or more occurrences For example, here’s a schema one might use for an address book: message AddressBook { required string owner; repeated string ownerPhoneNumbers; repeated group contacts { required string name; optional string phoneNumber; } } Lists (or Sets) can be represented by a repeating field. A Map is equivalent to a repeating field containing groups of key-value pairs where the key is required. Columnar format A columnar format provides more efficient encoding and decoding by storing together values of the same primitive type. To store nested data structures in columnar format, we need to map the schema to a list of columns in a way that we can write records to flat columns and read them back to their original nested data structure. In Parquet, we create one column per primitive type field in the schema. If we represent the schema as a tree, the primitive types are the leaves of this tree. AddressBook example as a tree: To represent the data in columnar format we create one column per primitive type cell shown in blue. The structure of the record is captured for each value by two integers called repetition level and definition level. Using definition and repetition levels, we can fully reconstruct the nested structures. This will be explained in detail below. Definition levels To support nested records we need to store the level for which the field is null. This is what the definition level is for: from 0 at the root of the schema up to the maximum level for this column. When a field is defined then all its parents are defined too, but when it is null we need to record the level at which it started being null to be able to reconstruct the record. In a flat schema, an optional field is encoded on a single bit using 0 for null and 1 for defined. In a nested schema, we use an additional value for each level of nesting (as shown in the example), finally if a field is required it does not need a definition level. For example, consider the simple nested schema below: message ExampleDefinitionLevel { optional group a { optional group b { optional string c; } } } It contains one column: a.b.c where all fields are optional and can be null. When c is defined, then necessarily a and b are defined too, but when c is null, we need to save the level of the null value. There are 3 nested optional fields so the maximum definition level is 3. Here is the definition level for each of the following cases: The maximum possible definition level is 3, which indicates that the value is defined. Values 0 to 2 indicate at which level the null field occurs. A required field is always defined and does not need a definition level. Let’s reuse the same example with the field b now required : message ExampleDefinitionLevel { optional group a { required group b { optional string c; } } } The maximum definition level is now 2 as b does not need one. The value of the definition level for the fields below b changes as follows: Making definition levels small is important as the goal is to store the levels in as few bits as possible. Repetition levels To support repeated fields we need to store when new lists are starting in a column of values. This is what repetition level is for: it is the level at which we have to create a new list for the current value. In other words, the repetition level can be seen as a marker of when to start a new list and at which level. For example consider the following representation of a list of lists of strings: The column will contain the following repetition levels and values: The repetition level marks the beginning of lists and can be interpreted as follows: 0 marks every new record and implies creating a new level1 and level2 list 1 marks every new level1 list and implies creating a new level2 list as well. 2 marks every new element in a level2 list. On the following diagram we can visually see that it is the level of nesting at which we insert records: A repetition level of 0 marks the beginning of a new record. In a flat schema there is no repetition and the repetition level is always 0. Only levels that are repeated need a Repetition level : optional or required fields are never repeated and can be skipped while attributing repetition levels. Striping and assembly Now using the two notions together, let’s consider the AddressBook example again. This table shows the maximum repetition and definition levels for each column with explanations on why they are smaller than the depth of the column: In particular for the column contacts.phoneNumber , a defined phone number will have the maximum definition level of 2, and a contact without phone number will have a definition level of 1. In the case where contacts are absent, it will be 0. AddressBook { owner: \"Julien Le Dem\", ownerPhoneNumbers: \"555 123 4567\", ownerPhoneNumbers: \"555 666 1337\", contacts: { name: \"Dmitriy Ryaboy\", phoneNumber: \"555 987 6543\", }, contacts: { name: \"Chris Aniszczyk\" } } AddressBook { owner: \"A. Nonymous\" } We’ll now focus on the column contacts.phoneNumber to illustrate this. Once projected the record has the following structure: AddressBook { contacts: { phoneNumber: \"555 987 6543\" } contacts: { } } AddressBook { } The data in the column will be as follows (R = Repetition Level, D = Definition Level) To write the column we iterate through the record data for this column: contacts.phoneNumber: “555 987 6543” new record: R = 0 value is defined: D = maximum (2) contacts.phoneNumber: null repeated contacts: R = 1 only defined up to contacts: D = 1 contacts: null new record: R = 0 only defined up to AddressBook: D = 0 The columns contains the following data: Note that NULL values are represented here for clarity but are not stored at all. A definition level strictly lower than the maximum (here 2) indicates a NULL value. To reconstruct the records from the column, we iterate through the column: R=0, D=2, Value = “555 987 6543” : R = 0 means a new record. We recreate the nested records from the root until the definition level (here 2) D = 2 which is the maximum. The value is defined and is inserted. R=1, D=1 : R = 1 means a new entry in the contacts list at level 1. D = 1 means contacts is defined but not phoneNumber, so we just create an empty contacts. R=0, D=0 : R = 0 means a new record. we create the nested records from the root until the definition level D = 0 => contacts is actually null, so we only have an empty AddressBook Storing definition levels and repetition levels efficiently In regards to storage, this effectively boils down to creating three sub columns for each primitive type. However, the overhead for storing these sub columns is low thanks to the columnar representation. That’s because levels are bound by the depth of the schema and can be stored efficiently using only a few bits per value (A single bit stores levels up to 1, 2 bits store levels up to 3, 3 bits can store 7 levels of nesting). In the address book example above, the column owner has a depth of one and the column contacts.name has a depth of two. The levels will always have zero as a lower bound and the depth of the column as an upper bound. Even better, fields that are not repeated do not need a repetition level and required fields do not need a definition level, bringing down the upper bound. In the special case of a flat schema with all fields required (equivalent of NOT NULL in SQL), the repetition levels and definition levels are omitted completely (they would always be zero) and we only store the values of the columns. This is effectively the same representation we would choose if we had to support only flat tables. These characteristics make for a very compact representation of nesting that can be efficiently encoded using a combination of Run Length Encoding and bit packing . A sparse column with a lot of null values will compress to almost nothing, similarly an optional column which is actually always set will cost very little overhead to store millions of 1s. In practice, space occupied by levels is negligible. This representation is a generalization of how we would represent the simple case of a flat schema: writing all values of a column sequentially and using a bitfield for storing nulls when a field is optional. Get Involved Parquet is still a young project; to learn more about the project see our README or look for the “ pick me up! ” label on GitHub. We do our best to review pull requests in a timely manner and give thorough and constructive reviews. You can also join our mailing list and tweet at @ApacheParquet to join the discussion.", "date": "2013-09-11"},
{"website": "Twitter-Engineering", "title": "Bootstrap 3.0", "author": ["‎@cra‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013/bootstrap-30.html", "abstract": "We are thrilled to see the Bootstrap community announce the 3.0 release : the @twbootstrap team released 3.0, check it out http://t.co/11DcMjnRCF — Twitter Open Source ( @TwitterOSS ) August 19, 2013 Since initially releasing Bootstrap two years ago on August 19th 2011, the project has grown outside of Twitter and built a community on its own –– with nearly 400 contributors , 55,000 stargazers and nearly 20,000 forks , it’s currently the most popular project on GitHub. To reflect this success and encourage further growth and community involvement, we worked closely with the Bootstrap team to move the project to its own independent organization on GitHub as part of the v3 release. Bootstrap (initially called Blueprint internally) started as a way to address a pain point around crafting user interfaces for internal applications –– Twitter engineers used to use whatever library they were familiar with to meet front-end requirements, which not only led to inconsistencies among applications, but also made it difficult to scale and maintain them in the long term. During our first Hack Week , engineers across the company started using an early, internal version of Bootstrap. As more and more teams adopted it, and given the importance of open source culture at Twitter, we decided to open source Bootstrap on GitHub and share the work with the greater open source community. We love to see the community blossom and look forward to the continued success of Bootstrap. How about trying the v3 release ?", "date": "2013-08-19"},
{"website": "Twitter-Engineering", "title": "Observability at Twitter", "author": ["‎@gphat‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013/observability-at-twitter.html", "abstract": "As Twitter has moved from a monolithic to a distributed architecture, our scalability has increased dramatically. Because of this, the overall complexity of systems and their interactions has also escalated. This decomposition has led to Twitter managing hundreds of services across our datacenters. Visibility into the health and performance of our diverse service topology has become an important driver for quickly determining the root cause of issues, as well as increasing Twitter’s overall reliability and efficiency. Debugging a complex program might involve instrumenting certain code paths or running special utilities; similarly Twitter needs a way to perform this sort of debugging for its distributed systems. One important metric we track is the overall success rate of the Twitter API. When problems arise, determining the cause requires a system capable of handling metrics from our heterogenous service stack. To understand the health and performance of our services, many things need to be considered together: operating systems, in-house and open-source JVM-based applications, core libraries such as Finagle , storage dependencies such as caches and databases, and finally, application-level metrics. Engineers at Twitter need to determine the performance characteristics of their services, the impact on upstream and downstream services, and get notified when services are not operating as expected. It is the Observability team’s mission to analyze such problems with our unified platform for collecting, storing, and presenting metrics. Creating a system to handle this job at Twitter scale is really difficult. Below, we explain how we capture, store, query, visualize and automate this entire process. Architecture Collection A key component to allowing performant yet very fine grained instrumentation of code and machines is to optimize for the lowest cost aggregation and rollup possible. This aggregation is is generally implemented via in-memory counters and approximate histograms in a service or application memory space, and then exported to the Observability stack over a consistent interface. Such metrics are collected from tens of thousands of endpoints representing approximately 170 million individual metrics every minute. In the vast majority of cases, data is pulled by the Observability stack from endpoints, however a hybrid aggregation and application-push model is used for specific applications which cannot or do not provide in-memory rollups. An endpoint is generally an HTTP server which serves a consistent view of all metrics it exports. For our applications and services, these metrics are exported by in-JVM stats libraries — such as the open-source Twitter-Server framework — which provide convenient functions for adding instrumentation. Depending on the level of instrumentation and which internal libraries are used (such as Finagle, which exports rich datasets), applications commonly export anywhere from 50 to over 10,000 individual metrics per instance. For applications and metrics which do not use our core common libraries, data is made available via a host-agent over the same HTTP interface. This includes machine and operating system statistics, such as disk health, CPU, memory, and overall network traffic. All metrics are identified by multiple dimensions, including an underlying service name, and the origin of the data: such as a host, dynamically scheduled application instance identifier, or other identifier specific to the service, and a metric name. Numeric metrics are written to a time series database, which we will cover in more detail in the next section. For batch-processing and non-numeric values, the data is routed to HDFS using Scribe. Scalding and Pig jobs can be run to produce reports for situations that are not time-sensitive. Determining the network location of applications running in a multi-tenant scheduled environment such as Mesos adds additional complexity to metric collection when compared to ones deployed on statically allocated hosts. Applications running in Mesos may be co-located with other applications and are dynamically assigned network addresses, so we leverage Zookeeper to provide dynamic service discovery in order to determine where an application is running. This data is centralized for use in collector configurations and is queried around 15,000 times per minute by users and automated systems. In some cases, the default collection period is insufficient for surfacing emergent application behavior. To supplement the existing pipeline, Observability also supports a self-service feature to collect data at a user-specified interval, down to one second, and serve it from an ephemeral store. This enables engineers to focus on key metrics during an application deployment or other event where resolution is important but durability and long term storage are less critical. Storage Collected metrics are stored in and queried from a time series database developed at Twitter. As the quantity and dimension of time series data for low-latency monitoring at Twitter grew, existing solutions were no longer able to provide the features or performance required. As such, the Observability team developed a time series storage and query service which served as the abstraction layer for a multitude of Observability products. The database is responsible for filtering, validating, aggregating, and reliably writing the collected metrics to durable storage clusters, as well as hosting the query interface. There are separate online clusters for different data sets: application and operating system metrics, performance critical write-time aggregates, long term archives, and temporal indexes. A typical production instance of the time series database is based on four distinct Cassandra clusters, each responsible for a different dimension (real-time, historical, aggregate, index) due to different performance constraints. These clusters are amongst the largest Cassandra clusters deployed in production today and account for over 500 million individual metric writes per minute. Archival data is stored at a lower resolution for trending and long term analysis, whereas higher resolution data is periodically expired. Aggregation is generally performed at write-time to avoid extra storage operations for metrics that are expected to be immediately consumed. Indexing occurs along several dimensions–service, source, and metric names–to give users some flexibility in finding relevant data. Query Language An important characteristic of any database is the ability to locate relevant information. For Observability, query functionality is exposed as a service by our time series database over HTTP and Thrift. Queries are written using a declarative, functional inspired language. The language allows for cross-correlation of multiple metrics from multiple sources across multiple databases spread across geographical boundaries. Using this query interface and language provide a unified interface to time series data that all processing and visualization tools at Twitter use. This consistency means a single technology for engineers to learn to debug, visualize and alert on performance data. Example Queries Show the slow query count summed for all machines in the role. This can be used in CLI tools or visualizations. The arguments are aggregate function, service name, sources and metric name: Metrics can also be correlated; for instance, the percentage of queries which were slow: Alerts use the same language. The first value is for warning and second is for critical. 7 of 10 minutes means the value exceeded either threshold for any 7 minutes out of a 10 minute window. Note it is identical to the earlier query except for the addition of a threshold and time window: On average, the Observability stack processes 400,000 queries per minute between ad-hoc user requests, dashboards and monitoring. The vast majority of these queries come from the monitoring system. Visualization While collecting and storing the data is important, it is of no use to our engineers unless it is visualized in a way that can immediately tell a relevant story. Engineers use the unified query language to retrieve and plot time series data on charts using a web application called Viz. A chart is the most basic visualization unit in Observability products. Charts are often created ad hoc in order to quickly share information within a team during a deploy or an incident, but they can also be created and saved in dashboards. A command line tool for dashboard creation, libraries of reusable components for common metrics, and an API for automation are available to engineers. Dashboards are equipped with many tools to help engineers analyze the data. They can toggle between chart types (stacked and/or filled), chart scales (linear or logarithmic), and intervals (per-minute, per-hour, per-day). Additionally, they can choose between live near real-time data and historical data dating back to the beginning of when the service began collection. The average dashboard at Twitter contains 47 charts. It’s common to see these dashboards on big screens or on engineer’s monitors if you stroll through the offices. Engineers at Twitter live in these dashboards! Visualization use cases include hundreds of charts per dashboard and thousands of data points per chart. To provide the required chart performance, an in-house charting library was developed. Monitoring Our monitoring system allows users to define alert conditions and notifications in the same query language they use for ad hoc queries and building dashboards. The addition of a predicate (e.g. > 50 for 20 minutes) to a query expresses the condition the user wants to be notified about. The evaluation of these queries is partitioned across multiple boxes for scalability and redundancy with failover in the case of node failure. This system evaluates over 10,000 queries each minute across Twitter’s data centers. Notifications are sent out via email, voice call, or text message. The monitoring system also provides an API that is used to create a rich UI for teams to actively monitor the state of their service. Users can view the historical metrics for an alert as well as snooze an alert, specific rules, or specific machines during maintenance. Related Systems In addition to conventional per-minute metric data, we have two other significant internal systems for learning about the health and performance of services. Zipkin , our distributed tracing system, does not contribute to monitoring but is integral in many debugging situations. Finally, an in-house exception logging and reporting application is an important tool for investigating the health of many systems and is often the first thing checked when Twitter’s success rate fluctuates. Future Work Twitter’s Observability stack is a distributed system just like the systems it provides visibility into. About 1.5% of the machines in a data center are used for collection, storage, query processing, visualization, and monitoring (0.3% if storage is excluded). It is common to see a dashboard or chart open within Twitter’s engineering areas. Our charts update as new data is available and many charts contain multiple queries. We serve around 200 million queries a day that end up in charts. This gives our engineers access to a huge amount of data for diagnosing, monitoring or just checking on health. This architecture has enabled us to keep up with the breakneck pace of growth and the tremendous scale of Twitter. Our challenges are not over. As Twitter continues to grow, it is becoming more complex and services are becoming more numerous. Thousands of service instances with millions of data points require high performance visualizations and automation for intelligently surfacing interesting or anomalous signals to the user. We seek to continually improve the stability and efficiency of our stack while giving users more flexible ways of interacting with the entire corpus of data that Observability manages. These are some of the complex problems that are being solved at Twitter. It is our hope that we have provided a thorough overview of the problems faced when monitoring distributed systems and how Twitter works to solve them. In future posts, we’ll dive deeper into parts of the stack to give more technical detail and discussion of the implementations. We’ll also discuss ways the system can be improved and what we are doing to make that happen. Acknowledgements The entire Observability team contributed to creating this overview: Charles Aylward ( @charmoto ), Brian Degenhardt ( @bmdhacks ), Micheal Benedict ( @micheal ), Zhigang Chen ( @zhigangc ), Jonathan Cao ( @jonathancao ), Stephanie Guo ( @stephanieguo ), Franklin Hu ( @thisisfranklin ), Megan Kanne ( @megankanne ), Justin Nguyen ( @JustANguyen ), Ryan O’Neill ( @rynonl ), Steven Parkes ( @smparkes ), Kamil Pawlowski ( @oo00o0o00oo ), Krishnan Raman ( @dxbydt_jasq ), Yann Ramin ( @theatrus ), Nik Shkrob ( @nshkrob ), Daniel Sotolongo ( @sortalongo ), Chang Su ( @changsmi ), Michael Suzuki ( @michaelsuzuki ), Tung Vo ( @tungv0 ), Daniel Wang, Cory Watson ( @gphat ), Alex Yarmula ( @twalex ), and Jennifer Yip ( @lunchbag ).", "date": "2013-09-09"},
{"website": "Twitter-Engineering", "title": "Improving accessibility of twitter.com", "author": ["‎@todd‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013/improving-accessibility-of-twittercom.html", "abstract": "Timelines are the core content of Twitter. And with device independence being one of the core principles of accessibility, we’ve spent the past quarter tuning and enhancing keyboard access for timelines. Our goal was to provide a first-class user experience for consuming and interacting with timelines using the keyboard. The need for keyboard access isn’t always obvious. It’s sometimes considered a power or pro user feature, but for users who are unable to use the mouse, keyboard access is a necessity. For example, visually impaired users often rely entirely on the keyboard as the mouse requires the user to be able to see the screen. Other users suffer physical injuries which make it either impossible or painful to use the mouse. In The Pointerless Web , @slicknet tells a personal story about how the keyboard became essential for him as a result of his RSI. The list of use cases is endless. What’s most important is to remember that humans are all different, and because of our differences, we all have different needs. The more options we have as users, the better. Through that lens, the importance of the keyboard is elevated as it offers another option to the user. Keyboard shortcuts Since the introduction of our keyboard shortcuts in 2010, users have been able to navigate through timelines using the j and k keys to move the selection cursor up or down. With a clearer understanding of the people who benefit from keyboard shortcuts, we were able to identify and fix gaps in our implementation. Our goal was to provide a first-class user experience for consuming and interacting with timelines using the keyboard. A lack of focus The first and most egregious problem when we started to tackle this was that the shortcuts for timeline navigation didn’t manipulate DOM focus. Specifically, when the user pressed j or k, a Tweet would only be rendered visually selected through the application of a class. This meant the timeline’s selection model was out of sync with the default navigational mechanism provided by the browser, and for practical purposes, limited keyboard access to actions defined by our keyboard shortcuts. For example, you could favorite a selected Tweet by pressing f, but couldn’t easily navigate to a link within the selected Tweet as the subsequent Tab keypress would end up moving focus to some other control in the page. To remedy this issue, the navigational shortcuts now set the tabIndex of the selected item to -1 and focus it. This enables j and k to function as macro-level navigational shortcuts between items in a timeline, and Tab / Shift + Tab to facilitate micro-level navigation between all of the various focusable controls within a Tweet. In other words, our shortcuts function as highways, and the Tab key as a local street. Further, browser-provided tab navigation is more robust in that it guarantees the user complete access to all of the actions within a Tweet. Any shortcuts we add for actions are just sugar. Here’s a video illustrating the difference between macro- and micro-navigation. Screen reader support As an added benefit to focusing the selected Tweet, those with screen readers can use our shortcuts to both browse and interact with Tweets in timelines. This is because screen readers announce the content of a DOM element when it receives focus. And while screen readers provide their own set of keyboard shortcuts for navigating the web pages, they’re generic (necessarily) and therefore not as optimized as those designed exclusively for twitter.com. Below is a video demonstrating how the content of Tweets is announced while navigating through a timeline using the j and k shortcuts. As you’ll see, we use three different combinations of browsers and screen readers: VoiceOver and Safari for the Mac NVDA and Firefox for Windows JAWS and Internet Explorer A more robust selected state An audit of the the various states of Tweets revealed that the previous visual rendering of selection (a gray background) within timelines was not robust enough to support all of the necessary use cases. As a result, this limited our ability to enable keyboard navigation for certain timelines as the user would be unable to perceive an item in a timeline as being selected (e.g. the default state for Tweets in expanded conversations is a gray background). Further, the selected state for keyboard navigation shared the the same color with the hover state. This meant users switching between the mouse and the keyboard could end up with two visually selected tweets in their timeline. To address these issues we’ve introduced a new selection style for items in timelines that is both distinct from the hover state and robust enough to render consistently well across of all of the various states of Tweets. Previous selection style New selection style With this change, you’ll notice the j and k shortcuts have an increased range (for example, navigation between Tweets in expanded conversations in timelines), and should see a visual rendering of selection where j and k were previously supported, but selection was not perceivable (e.g. the Discover timeline). Moving forward If you haven’t already tried using twitter.com with the keyboard, give it a try. (The full list of shortcuts is available by typing “?”) If you’ve been a longtime keyboard-centric twitter.com user, we hope you find these enhancements improve your experience consuming and interacting with timelines. While this posts details some of the accessibility enhancements we’ve made, it by no means captures them all. We’ll post again about more enhancements that are on their way. Lastly, we’ve found user feedback is one of the most valuable means of capturing and prioritizing enhancements and fixes. To that end we’ve created the @a11yteam account for the purpose of helping us both capture your feedback as well as broadcast the availability of improvements.", "date": "2013-08-22"},
{"website": "Twitter-Engineering", "title": "Twitter University: Building a world-class engineering organization", "author": ["‎@chfry‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013/twitter-university-building-a-world-class-engineering-organization.html", "abstract": "As Twitter has scaled, so too has our engineering organization. To help our engineers grow, it’s important for them to have access to world-class technical training, along with opportunities to teach the skills they’ve mastered. To that end, we’re establishing Twitter University. We want Twitter to be the best place in the world for engineers to work. Twitter University builds on several existing efforts at Twitter. We currently offer employees a whole swath of technical trainings, from orientation classes for new engineers to iOS Bootcamp, JVM Fundamentals, Distributed Systems, Scala School, and more for those who want to develop new skills. Most of these classes are taught by our own team members, and many of them have been organized during our quarterly Hack Weeks –– a testament to our engineers’ passion for learning and education. I’ve been inspired by these efforts. Being able to continually learn on the job and develop a sense of expertise or mastery is a fundamental factor in success in the technology industry and long term happiness at a company. Twitter University will be a vital foundation for our engineering organization. To lead the program, we’ve acquired Marakana , a company dedicated to open source training. We’ve been working with them for several months. The founders, Marko and Sasa Gargenta, have impressed us with their entrepreneurial leadership, commitment to learning and technical expertise. The Marakana team has cultivated a tremendous community of engineers in the Bay Area, and we look forward to engaging with all of you at meet-ups and technical events. Additionally, we’ll continue to contribute to open source software , and we aim to release some of the Twitter University content online to anyone who’d like to learn. You can keep up with Twitter University by following @university . We want Twitter to be the best place in the world for engineers to work. Join us . Update : Changed Twitter University’s username to @university .", "date": "2013-08-13"},
{"website": "Twitter-Engineering", "title": "Visualizing Epics and Dependencies in JIRA", "author": ["‎@njm‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013/visualizing-epics-and-dependencies-in-jira.html", "abstract": "The @TwitterTPM team has been exploring ways to visualize work across a number of teams and programs within engineering. One of our key goals is to highlight work that has dependencies and understand whether those dependencies have been met. We’ve arrived at a solution that combines JIRA , GreenHopper and Confluence . Ancient TPM proverb: “a watched JIRA never moves” — Twitter TPM Team ( @TwitterTPM ) August 12, 2013 First, some context: across Twitter, we set quarterly goals for the company, each department (e.g. Engineering), and each team within a department. The company goals are shared by everyone, and every team is focused on supporting them. The name for this practice of setting quarterly goals is called Gulls (it’s a bird , get it?). Recently, we set out to make it easier for employees to update their Gulls and provide greater transparency across teams. In doing so, a positive side effect has been increased visibility into dependencies. So how did we approach this? Organizing JIRA Each engineering team has its own project in JIRA within which they prioritize the backlog, plan sprints, and measure progress. Each quarter, a team identifies its Key Results and records those using the Epic issue type in JIRA. Since teams plan one or two quarters ahead, we needed a way to track what the team was targeting for the current quarter –– we chose JIRA Fix Versions for the quarters - 2013-Q3, 2013-Q4, etc. Finally, we added a Text Custom Field to JIRA called ‘Key Result’ and added it to the Epic issue type. The one other key change we made to JIRA to support tracking progress of these Epics was to create a specific workflow for Gulls. We now have four steps in the workflow for an Epic: Open, Accepted, In Progress and Resolved. Finally, we order the Epics in JIRA to ensure that the order is correct, focusing on delivering the highest value items first. Whenever we report on the goals we always use the JQL ( JIRA Query Language ) clause ORDER BY Rank ASC to ensure they are always displayed in the correct order. Visualizing progress To visualize progress toward the Epics’ completion, we selected two options that fit the needs of particular audiences: Kanban for engineering and Confluence for non-engineering. Engineering In engineering, we tend to want to look at details and dependencies, so we selected Kanban boards in GreenHopper to visualize Epics and their current status. Here’s an example: We leverage the Quick Filters, Swimlanes and Card Colours provided by JIRA to get the visualization we want. For instance, we’re only getting the Epics related to the Gulls practice, teams can continue to use Epics for managing other work so long as it does not have a Fix Version for one of the quarters, e.g. 2013-Q3 . Note: The Work Sub-Filter removes the Epics completed in the last quarter off the Work mode of the board. They are still available on the Report mode for Cycle Time metrics (more on this below). Non-Engineering Teams outside of engineering tend to prefer to see a high-level overview of the projects that engineers are working on. For those teams, we use a page in Confluence to show show progress. Objectives, which may span multiple quarters, are captured on the Confluence page, and Key Results, which are in Epics in JIRA, are introduced using the JIRA Issues macro . Dependencies We manage dependencies in two ways. First, we identify the instances where a team has a dependency on a particular service that a different team still has to build. If, for example, Team A has a goal that depends on Service X, which Team B is still working on, then Team A should defer that goal until the next quarter. Only if Team B delivers Service X early should Team A consider pulling that goal in to the current quarter. Second, we leverage tools to visualize incoming and outgoing dependencies for a team so it’s easy to track them. We want to highlight those goals that are blocked. The Script Runner add-on for JIRA saves the day when it comes to helping find dependencies and making sure teams are aware that other teams depend on them. Script Runner provides some fancy JQL clauses , and we use the following Quick Filter on the Kanban board (for Engineering): (issueFunction in hasLinks(\"depends on\") OR issueFunction in hasLinks(\"is a dependent of\")) AND status in (Accepted, \"In Progress\") This query shows all Epics that are Accepted or In Progress that have an incoming or outgoing dependency. And voila, we can visualize Epics across the company and highlight the blockers. You can even turn this into a nifty JIRA Wallboard and put on screens around the office. Cycle time Cycle time is the amount of time that an issue spends marked as In Progress –– in this case, that’s the amount of time an Epic takes to be completed from when we start working on it. We use the Control Chart to visualize this. You can see in the control chart above the amount of time taken to complete an Epic increased during the 4th July week and following HackWeek . Next steps We’re going to drive this further. One of the pieces we’re exploring is using the open source Foresight add-on for JIRA to visualize dependencies on an aggregate level. And you’ll want to stay tuned for a future blog on our use of the Presentation add-on for Confluence . Providing clear and early visibility for management and partners is a great way for engineering teams to plan a backlog, tie it to the roadmap, simplify dependency tracking, and minimize disruptions. If all the information is there in JIRA, we can package it up as something that makes sense for people throughout the company. What else could we be doing? What are we missing? How do you do it? We’d love to hear from you. Reach out to us @TwitterTPM .", "date": "2013-08-12"},
{"website": "Twitter-Engineering", "title": "Login verification on Twitter for iPhone and Android", "author": ["‎@alsmola‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013/login-verification-on-twitter-for-iphone-and-android.html", "abstract": "At Twitter, we want to make it easy as possible to secure your account. Designing a secure authentication protocol is tough; designing one that is also simple and intuitive is even harder. We think our new login verification feature is an improvement in both security and usability, and we’re excited to share it with you. More secure, you say? Traditional two-factor authentication protocols require a shared secret between the user and the service. For instance, OTP protocols use a shared secret modulated by a counter ( HOTP ) or timer ( TOTP ). A weakness of these protocols is that the shared secret can be compromised if the server is compromised. We chose a design that is resilient to a compromise of the server-side data’s confidentiality: Twitter doesn’t persistently store secrets, and the private key material needed for approving login requests never leaves your phone. Other previous attacks against two-factor authentication have taken advantage of compromised SMS delivery channels. This solution avoids that because the key necessary to approve requests never leaves your phone. Also, our updated login verification feature provides additional information about the request to help you determine if the login request you see is the one you’re making. And easier to use, as well? Now you can enroll in login verification and approve login requests right from the Twitter app on iOS and Android. Simply tap a button on your phone, and you’re good to go. This means you don’t have to wait for a text message and then type in the code each time you sign in on twitter.com. So how does it work? When you enroll, your phone generates an asymmetric 2048-bit RSA keypair, which stores the private key locally on the device and sends the public key, which Twitter stores as part of your user object in our backend store, to the server. Whenever you initiate a login request by sending your username and password, Twitter will generate a challenge and request ID –– each of which is a 190-bit (32 alphanumerics) random nonce –– and store them in memcached. The request ID nonce is returned to the browser or client attempting to authenticate, and then a push notification is sent to your phone, letting you know you have a login verification request. Within your Twitter app, you can then view the outstanding request, which includes several key pieces of information: time, geographical location, browser, and the login request’s challenge nonce. At that point, you can choose to approve or deny the request. If you approve the request, the client will use its private key to respond by signing the challenge. If the signature is correct, the login request will be marked as verified. In the meantime, the original browser will poll the server with the request ID nonce. When the request is verified, the polling will return a session token and the user will be signed in. Login verification is more secure and easier to use. And you can still sign in even if you lose your phone. What happens if I don’t have my phone? The private key is only stored on the phone. However, there’s still a way to sign in to Twitter even if you don’t have your phone or can’t connect to Twitter –– by using your backup code. We encourage you to store it somewhere safe. To make the backup code work without sharing secrets, we use an algorithm inspired by S/KEY . During enrollment, your phone generates a 64-bit random seed, SHA256 hashes it 10,000 times, and turns it into a 60-bit (12 characters of readable base32) string. It sends this string to our servers. The phone then asks you to write down the next backup code, which is the same seed hashed 9,999 times. Later, when you send us the backup code to sign in, we hash it one time, and then verify that the resulting value matches the value we initially stored. Then, we store the value you sent us, and the next time you generate a backup code it will hash the seed 9,998 times. This means that you don’t have to be connected to Twitter to generate a valid backup code. And, due to the one-way property of the hash algorithm, if ever an attacker could read the data on our servers, he/she won’t be able to generate one. In addition to storing your backup codes safely, we encourage you to backup your phone. If you have an iPhone, you should make an encrypted backup , which stores the cryptographic material necessary to recover your account easily in case you lose your phone or upgrade to a new phone. Also if you’re upgrading, you can simply un-enroll on your old phone and re-enroll on your new phone. What if I want to sign in to Twitter on a service that doesn’t support login verification? Twitter clients that use XAuth authentication , which expects a username and a password, don’t always support login verification directly. Instead, you’ll need to go to twitter.com on the web, navigate to your password settings, and generate a temporary password. You can use this password instead of your regular password to sign in over XAuth. What’s next? We’ll continue to make improvements so signing in to Twitter is even easier and more secure. For example, we’re working on building login verification into our clients and exposing a login verification API for other XAuth clients so people who don’t have access to the web also have a seamless login experience.", "date": "2013-08-06"},
{"website": "Twitter-Engineering", "title": "New Tweets per second record, and how!", "author": ["‎@raffi‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013/new-tweets-per-second-record-and-how.html", "abstract": "Recently, something remarkable happened on Twitter: On Saturday, August 3 in Japan, people watched an airing of Castle in the Sky , and at one moment they took to Twitter so much that we hit a one-second peak of 143,199 Tweets per second. (August 2 at 7:21:50 PDT; August 3 at 11:21:50 JST) To give you some context of how that compares to typical numbers, we normally take in more than 500 million Tweets a day which means about 5,700 Tweets a second, on average. This particular spike was around 25 times greater than our steady state. During this spike, our users didn’t experience a blip on Twitter. That’s one of our goals: to make sure Twitter is always available no matter what is happening around the world. New Tweets per second (TPS) record: 143,199 TPS. Typical day: more than 500 million Tweets sent; average 5,700 TPS. This goal felt unattainable three years ago, when the 2010 World Cup put Twitter squarely in the center of a real-time, global conversation . The influx of Tweets –– from every shot on goal, penalty kick and yellow or red card –– repeatedly took its toll and made Twitter unavailable for short periods of time. Engineering worked throughout the nights during this time, desperately trying to find and implement order-of-magnitudes of efficiency gains. Unfortunately, those gains were quickly swamped by Twitter’s rapid growth, and engineering had started to run out of low-hanging fruit to fix. After that experience, we determined we needed to step back. We then determined we needed to re-architect the site to support the continued growth of Twitter and to keep it running smoothly. Since then we’ve worked hard to make sure that the service is resilient to the world’s impulses. We’re now able to withstand events like Castle in the Sky viewings, the Super Bowl, and the global New Year’s Eve celebration. This re-architecture has not only made the service more resilient when traffic spikes to record highs, but also provides a more flexible platform on which to build more features faster, including synchronizing direct messages across devices, Twitter cards that allow Tweets to become richer and contain more content, and a rich search experience that includes stories and users. And more features are coming. Below, we detail how we did this. We learned a lot. We changed our engineering organization. And, over the next few weeks, we’ll be publishing additional posts that go into more detail about some of the topics we cover here. Starting to re-architect After the 2010 World Cup dust settled, we surveyed the state of our engineering. Our findings: We were running one of the world’s largest Ruby on Rails installations, and we had pushed it pretty far –– at the time, about 200 engineers were contributing to it and it had gotten Twitter through some explosive growth, both in terms of new users as well as the sheer amount of traffic that it was handling. This system was also monolithic where everything we did, from managing raw database and memcache connections through to rendering the site and presenting the public APIs, was in one codebase. Not only was it increasingly difficult for an engineer to be an expert in how it was put together, but also it was organizationally challenging for us to manage and parallelize our engineering team. We had reached the limit of throughput on our storage systems –– we were relying on a MySQL storage system that was temporally sharded and had a single master. That system was having trouble ingesting tweets at the rate that they were showing up, and we were operationally having to create new databases at an ever increasing rate. We were experiencing read and write hot spots throughout our databases. We were “throwing machines at the problem” instead of engineering thorough solutions –– our front-end Ruby machines were not handling the number of transactions per second that we thought was reasonable, given their horsepower. From previous experiences, we knew that those machines could do a lot more. Finally, from a software standpoint, we found ourselves pushed into an “optimization corner” where we had started to trade off readability and flexibility of the codebase for performance and efficiency. We concluded that we needed to start a project to re-envision our system. We set three goals and challenges for ourselves: We wanted big infrastructure wins in performance, efficiency, and reliability –– we wanted to improve the median latency that users experience on Twitter as well as bring in the outliers to give a uniform experience to Twitter. We wanted to reduce the number of machines needed to run Twitter by 10x. We also wanted to isolate failures across our infrastructure to prevent large outages –– this is especially important as the number of machines we use go up, because it means that the chance of any single machine failing is higher. Failures are also inevitable, so we wanted to have them happen in a much more controllable manner. We wanted cleaner boundaries with “related” logic being in one place –– we felt the downsides of running our particular monolithic codebase, so we wanted to experiment with a loosely coupled services oriented model. Our goal was to encourage the best practices of encapsulation and modularity, but this time at the systems level rather than at the class, module, or package level. Most importantly, we wanted to launch features faster. We wanted to be able to run small and empowered engineering teams that could make local decisions and ship user-facing changes, independent of other teams. We prototyped the building blocks for a proof of concept re-architecture. Not everything we tried worked and not everything we tried, in the end, met the above goals. But we were able to settle on a set of principles, tools, and an infrastructure that has gotten us to a much more desirable and reliable state today. The JVM vs the Ruby VM First, we evaluated our front-end serving tier across three dimensions: CPU, RAM, and network. Our Ruby-based machinery was being pushed to the limit on the CPU and RAM dimensions –– but we weren’t serving that many requests per machine nor were we coming close to saturating our network bandwidth. Our Rails servers, at the time, had to be effectively single threaded and handle only one request at a time. Each Rails host was running a number of Unicorn processes to provide host-level concurrency, but the duplication there translated to wasteful resource utilization. When it came down to it, our Rails servers were only capable of serving 200 - 300 requests / sec / host. Twitter’s usage is always growing rapidly, and doing the math there, it would take a lot of machines to keep up with the growth curve. At the time, Twitter had experience deploying fairly large scale JVM-based services –– our search engine was written in Java, and our Streaming Api infrastructure as well as Flock, our social graph system , was written in Scala. We were enamored by the level of performance that the JVM gave us. It wasn’t going to be easy to get our performance, reliability, and efficiency goals out of the Ruby VM, so we embarked on writing code to be run on the JVM instead. We estimated that rewriting our codebase could get us > 10x performance improvement, on the same hardware –– and now, today, we push on the order of 10 - 20K requests / sec / host. There was a level of trust that we all had in the JVM. A lot of us had come from companies where we had experience working with, tuning, and operating large scale JVM installations. We were confident we could pull off a sea change for Twitter in the world of the JVM. Now, we had to decompose our architecture and figure out how these different services would interact. Programming model In Twitter’s Ruby systems, concurrency is managed at the process level: a single network request is queued up for a process to handle. That process is completely consumed until the network request is fulfilled. Adding to the complexity, architecturally, we were taking Twitter in the direction of having one service compose the responses of other services. Given that the Ruby process is single-threaded, Twitter’s “response time” would be additive and extremely sensitive to the variances in the back-end systems’ latencies. There were a few Ruby options that gave us concurrency; however, there wasn’t one standard way to do it across all the different VM options. The JVM had constructs and primitives that supported concurrency and would let us build a real concurrent programming platform. It became evident that we needed a single and uniform way to think about concurrency in our systems and, specifically, in the way we think about networking. As we all know, writing concurrent code (and concurrent networking code) is hard and can take many forms. In fact, we began to experience this. As we started to decompose the system into services, each team took slightly different approaches. For example, the failure semantics from clients to services didn’t interact well: we had no consistent back-pressure mechanism for servers to signal back to clients and we experienced “thundering herds” from clients aggressively retrying latent services. These failure domains informed us of the importance of having a unified, and complementary, client and server library that would bundle in notions of connection pools, failover strategies, and load balancing. To help us all get in the same mindset, we put together both Futures and Finagle . Now, not only did we have a uniform way to do things, but we also baked into our core libraries everything that all our systems needed so we could get off the ground faster. And rather than worry too much about how each and every system operated, we could focus on the application and service interfaces. Independent systems The largest architectural change we made was to move from our monolithic Ruby application to one that is more services oriented. We focused first on creating Tweet, timeline, and user services –– our “core nouns”. This move afforded us cleaner abstraction boundaries and team-level ownership and independence. In our monolithic world, we either needed experts who understood the entire codebase or clear owners at the module or class level. Sadly, the codebase was getting too large to have global experts and, in practice, having clear owners at the module or class level wasn’t working. Our codebase was becoming harder to maintain, and teams constantly spent time going on “archeology digs” to understand certain functionality. Or we’d organize “whale hunting expeditions” to try to understand large scale failures that occurred. At the end of the day, we’d spend more time on this than on shipping features, which we weren’t happy with. Our theory was, and still is, that a services oriented architecture allows us to develop the system in parallel –– we agree on networking RPC interfaces, and then go develop the system internals independently –– but, it also meant that the logic for each system was self-contained within itself. If we needed to change something about Tweets, we could make that change in one location, the Tweet service, and then that change would flow throughout our architecture. In practice, however, we find that not all teams plan for change in the same way: for example, a change in the Tweet service may require other services to do an update if the Tweet representation changed. On balance, though, this works out more times than not. This system architecture also mirrored the way we wanted, and now do, run the Twitter engineering organization. Engineering is set up with (mostly) self-contained teams that can run independently and very quickly. This means that we bias toward teams spinning up and running their own services that can call the back end systems. This has huge implications on operations, however. Storage Even if we broke apart our monolithic application into services, a huge bottleneck that remained was storage. Twitter, at the time, was storing tweets in a single master MySQL database. We had taken the strategy of storing data temporally –– each row in the database was a single tweet, we stored the tweets in order in the database, and when the database filled up we spun up another one and reconfigured the software to start populating the next database. This strategy had bought us some time, but, we were still having issues ingesting massive tweet spikes because they would all be serialized into a single database master so we were experiencing read load concentration on a small number of database machines. We needed a different partitioning strategy for Tweet storage. We took Gizzard, our framework to create sharded and fault-tolerant distributed databases, and applied it to tweets. We created T-Bird. In this case, Gizzard was fronting a series of MySQL databases –– every time a tweet comes into the system, Gizzard hashes it, and then chooses an appropriate database. Of course, this means we lose the ability to rely on MySQL for unique ID generation. Snowflake was born to solve that problem. Snowflake allows us to create an almost-guaranteed globally unique identifier. We rely on it to create new tweet IDs, at the tradeoff of no longer having “increment by 1” identifiers. Once we have an identifier, we can rely on Gizzard then to store it. Assuming our hashing algorithm works and our tweets are close to uniformly distributed, we increase our throughput by the number of destination databases. Our reads are also then distributed across the entire cluster, rather than being pinned to the “most recent” database, allowing us to increase throughput there too. Observability and statistics We’ve traded our fragile monolithic application for a more robust and encapsulated, but also complex, services oriented application. We had to invest in tools to make managing this beast possible. Given the speed with which we were creating new services, we needed to make it incredibly easy to gather data on how well each service was doing. By default, we wanted to make data-driven decisions, so we needed to make it trivial and frictionless to get that data. As we were going to be spinning up more and more services in an increasingly large system, we had to make this easier for everybody. Our Runtime Systems team created two tools for engineering: Viz and Zipkin . Both of these tools are exposed and integrated with Finagle, so all services that are built using Finagle get access to them automatically. stats.timeFuture(\"request_latency_ms\") { // dispatch to do work } The above code block is all that is needed for a service to report statistics into Viz. From there, anybody using Viz can write a query that will generate a timeseries and graph of interesting data like the 50th and 99th percentile of request_latency_ms. Runtime configuration and testing Finally, as we were putting this all together, we hit two seemingly unrelated snags: launches had to be coordinated across a series of different services, and we didn’t have a place to stage services that ran at “Twitter scale”. We could no longer rely on deployment as the vehicle to get new user-facing code out there, and coordination was going to be required across the application. In addition, given the relative size of Twitter, it was becoming difficult for us to run meaningful tests in a fully isolated environment. We had, relatively, no issues testing for correctness in our isolated systems –– we needed a way to test for large scale iterations. We embraced runtime configuration. We integrated a system we call Decider across all our services. It allows us to flip a single switch and have multiple systems across our infrastructure all react to that change in near-real time. This means software and multiple systems can go into production when teams are ready, but a particular feature doesn’t need to be “active”. Decider also allows us to have the flexibility to do binary and percentage based switching such as having a feature available for x% of traffic or users. We can deploy code in the fully “off” and safe setting, and then gradually turn it up and down until we are confident it’s operating correctly and systems can handle the new load. All this alleviates our need to do any coordination at the team level, and instead we can do it at runtime. Today Twitter is more performant, efficient and reliable than ever before. We’ve sped up the site incredibly across the 50th (p50) through 99th (p99) percentile distributions and the number of machines involved in serving the site itself has been decreased anywhere from 5x-12x. Over the last six months, Twitter has flirted with four 9s of availability. Twitter engineering is now set up to mimic our software stack. We have teams that are ready for long term ownership and to be experts on their part of the Twitter infrastructure. Those teams own their interfaces and their problem domains. Not every team at Twitter needs to worry about scaling Tweets, for example. Only a few teams –– those that are involved in the running of the Tweet subsystem (the Tweet service team, the storage team, the caching team, etc.) –– have to scale the writes and reads of Tweets, and the rest of Twitter engineering gets APIs to help them use it. Two goals drive us as we did all this work: Twitter should always be available for our users, and we should spend our time making Twitter more engaging, more useful and simply better for our users. Our systems and our engineering team now enable us to launch new features faster and in parallel. We can dedicate different teams to work on improvements simultaneously and have minimal logjams for when those features collide. Services can be launched and deployed independently from each other (in the last week, for example, we had more than 50 deploys across all Twitter services), and we can defer putting everything together until we’re ready to make a new build for iOS or Android. Keep an eye on this blog and @twittereng for more posts that will dive into details on some of the topics mentioned above. Thanks goes to Jonathan Reichhold ( @jreichhold ), David Helder ( @dhelder ), Arya Asemanfar ( @a_a ), Marcel Molina ( @noradio ), and Matt Harris ( @themattharris ) for helping contribute to this blog post.", "date": "2013-08-16"},
{"website": "Twitter-Engineering", "title": "Announcing Parquet 1.0: Columnar Storage for Hadoop", "author": ["‎@squarecog‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013/announcing-parquet-10-columnar-storage-for-hadoop.html", "abstract": "In March we announced the Parquet project, the result of a collaboration between Twitter and Cloudera intended to create an open-source columnar storage format library for Apache Hadoop . We’re happy to release Parquet 1.0.0, more at: https://t.co/xKilQU22a5 90+ merged pull requests since announcement: https://t.co/lrKdrNiUQA — Parquet Format ( @ParquetFormat ) July 30, 2013 Today, we’re happy to tell you about a significant Parquet milestone: a 1.0 release, which includes major features and improvements made since the initial announcement. But first, we’ll revisit why columnar storage is so important for the Hadoop ecosystem. What is Parquet and columnar storage? Parquet is an open-source columnar storage format for Hadoop . Its goal is to provide a state of the art columnar storage layer that can be taken advantage of by existing Hadoop frameworks, and can enable a new generation of Hadoop data processing architectures such as Impala, Drill, and parts of the Hive ‘Stinger’ initiative. Parquet does not tie its users to any existing processing framework or serialization library. The idea behind columnar storage is simple: instead of storing millions of records row by row (employee name, employee age, employee address, employee salary…) store the records column by column (all the names, all the ages, all the addresses, all the salaries). This reorganization provides significant benefits for analytical processing: Since all the values in a given column have the same type, generic compression tends to work better and type-specific compression can be applied. Since column values are stored consecutively, a query engine can skip loading columns whose values it doesn’t need to answer a query, and use vectorized operators on the values it does load. These effects combine to make columnar storage a very attractive option for analytical processing. Implementing a columnar storage format that can be used by the many various Hadoop-based processing engines is tricky. Not all data people store in Hadoop is a simple table — complex nested structures abound. For example, one of Twitter’s common internal datasets has a schema nested seven levels deep, with over 80 leaf nodes. Slicing such objects into a columnar structure is non-trivial, and we chose to use the approach described by Google engineers in their paper Dremel: Interactive Analysis of Web-Scale Datasets . Another complexity derives from the fact that we want it to be relatively easy to plug new processing frameworks into Parquet. Despite these challenges, we are pleased with the results so far: our approach has resulted in integration with Hive, Pig, Cascading, Impala, and an in-progress implementation with Drill, and is currently in production at Twitter. What’s in the Parquet 1.0 release Parquet 1.0 is available for download on GitHub and via Maven Central . It provides the following features: Apache Hadoop Map-Reduce Input and Output formats Apache Pig Loaders and Storers Apache Hive SerDes Cascading Schemes Impala support Self-tuning dictionary encoding Dynamic Bit-Packing / RLE encoding Ability to work directly with Avro records Ability to work directly with Thrift records Support for both Hadoop 1 and Hadoop 2 APIs Eighteen contributors from multiple organizations (Twitter, Cloudera, Criteo, UC Berkeley AMPLab, Stripe and others) contributed to this release. Improvements since initial Parquet release When we announced Parquet, we encouraged the greater Hadoop community to contribute to the design and implementation of the format. Parquet 1.0 features many contributions from the community as well as the initial core team of committers; we will highlight two of these improvements below. Dictionary encoding The ability to efficiently encode columns in which the number of unique values is fairly small (10s of thousands) can lead to a significant compression and processing speed boost. Nong Li and Marcel Kornacker of Cloudera teamed up with Julien Le Dem of Twitter to define a dictionary encoding specification, and implemented it both in Java and C++ (for Impala). Parquet’s dictionary encoding is automatic, so users do not have to specify it — Parquet will dynamically turn it on and off as applicable, given the data it is compressing. Hybrid bit packing and RLE encoding Columns of numerical values can often be efficiently stored using two approaches: bit packing and run-length encoding (RLE): Bit packing uses the fact that small integers do not need a full 32 or 64 bits to be represented, and packs multiple values into the space normally occupied by a single value. There are multiple ways to do this, but we use a modified version of Daniel Lemire’s JavaFastPFOR library (read more about it here ). Run-length encoding turns “runs” of the same value, meaning multiple occurrences of the same value in a row, into just a pair of numbers: the value, and the number of times it is repeated. Our hybrid implementation of bit-packing and RLE monitors the data stream, and dynamically switches between the two types of encoding, depending on what gives us the best compression. This is extremely effective for certain kinds of integer data and combines particularly well with dictionary encoding. A Growing community One of the major goals of Parquet is to provide a columnar storage format for Hadoop that can be used by many projects and companies, rather than being tied to a specific tool. We are heartened to find that so far, the bet on open-source collaboration and tool independence has paid off. This release includes quality contributions from 18 developers, who are affiliated with a number of different companies and institutions. Looking forward We are far from done, of course; there are many improvements we would like to make and features we would like to add. To learn more about these items, you can see our roadmap on the README or look for the “pick me up!” label on GitHub. You can also join our mailing list at parquet-dev@googlegroups.com and tweet at us @ParquetFormat to join the discussion.", "date": "2013-07-30"},
{"website": "Twitter-Engineering", "title": "Mesos Graduates from Apache Incubation", "author": ["‎@davelester‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013/mesos-graduates-from-apache-incubation.html", "abstract": "The Apache Software Foundation (ASF) has announced the graduation of Apache Mesos , the open source cluster manager that is used and heavily supported by Twitter, from its incubator. we’ve become a Top-Level Project at #Apache ! https://t.co/qk7CxCOLfi #mesos — Apache Mesos ( @ApacheMesos ) July 24, 2013 Mesos, which was released last year , acts as a layer of abstraction between applications and pools of servers, helping avoid the necessity of creating separate clusters to run individual frameworks and instead making it possible to optimize how jobs are executed across shared machines. What’s new During incubation, Mesos had four releases and has continued to mature within Twitter’s production environment. Some of its improvements include: increased flexibility to support new application frameworks in several programming languages and cgroups isolation, in order to decrease interference across tasks and guarantee cpu/memory availability. The Mesos community now includes new core committers, an active developer mailing list where future releases and patches are discussed, and increasing interest in running additional frameworks on Mesos. Currently, Mesos can run multiple frameworks, including Apache Hadoop , MPI, Jenkins, Storm , and Spark as well as other applications and custom frameworks. How Twitter uses Mesos Twitter actively contributes to and uses Mesos –– it is the cornerstone of our elastic compute infrastructure. A number of key Twitter services run in production on Mesos including: analytics, typeahead, ads and more, and we rely on it to build all our new services. By using Mesos, these services can scale and leverage a shared pool of servers across data centers efficiently. Furthermore, Mesos has transformed the way developers think about launching new services at Twitter. Instead of thinking about static machines, engineers think about resources like CPU, memory and disk. The result has been a reduction in the time between prototyping and launching new services efficiently running on Mesos. Future Work Graduation is a fantastic milestone, but only the beginning for Mesos. We look forward to continuing to help grow the Mesos community and fostering an ecosystem around the project. In addition to continuing to contribute to the Mesos core, we are preparing to open source Aurora, a service scheduler used to schedule jobs onto Mesos within Twitter. Aurora provides many of the primitives that allow stateless services to quickly deploy and scale, while providing good assurances for fault tolerance. We hope Aurora will make it easier to adopt and use Mesos in the broader ecosystem. How to get involved Mesos is still a young project and there’s plenty of areas to contribute. If you’re interested in test driving Mesos, visit the recently-redesigned Mesos website where you’ll find a getting started guide, documentation , and community section. We look towards the community to help improving the documentation as we continue to grow. You can also create and contribute to Mesos frameworks. For example, Airbnb built their distributed and fault-tolerant scheduler Chronos on top of Mesos . As a replacement for cron, Chronos is a great example of the types of frameworks we hope to see built by the community. And, this Thursday, Twitter is hosting a meetup event focused on building and running frameworks on Mesos, featuring talks by Twitter’s Vinod Kone on “Building new frameworks: Jenkins as a Case Study” and Brenden Matthews from Airbnb, presenting “Hadoop on Mesos.” Videos from the meetup will be posted online after the event and future meetups are being planned. Acknowledgements Mesos was originally created at the University of California at Berkeley’s RAD Lab (the research center also responsible for the original development of Apache Spark). Apache Mesos contributors include Ali Ghodsi, Benjamin Hindman ( @benh ), Vinod Kone ( @vinodkone ), Ben Mahler ( @bmahler ), Andy Konwinski ( @andykonwinski ), Dave Lester ( @davelester ), Thomas Marshall, Brenden Matthews, Charles Reiss, Ross Allen, Chris Mattmann, Tom White, Brian McCallister and Matei Zaharia ( @matei_zaharia ).", "date": "2013-07-24"},
{"website": "Twitter-Engineering", "title": "hRaven and the @HadoopSummit", "author": ["‎@twitterhadoop‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013/hraven-and-the-hadoopsummit.html", "abstract": "Today marks the start of the Hadoop Summit , and we are thrilled to be a part of it. A few of our engineers will be participating in talks about our Hadoop usage at the summit: Day 1, 4:05pm: Parquet : Columnar storage for the People Day 1, 4:55pm: A cluster is only as strong as its weakest link Day 2, 11:00am: A Birds-Eye View of Pig and Scalding Jobs with hRaven Day 2, 1:40pm: Hadoop Hardware at Twitter: Size does matter! As Twitter’s use of Hadoop and MapReduce rapidly expands, tracking usage on our clusters grows correspondingly more difficult. With an ever-increasing job load (tens of thousands of jobs per day), and a reliance on higher level abstractions such as Apache Pig and Scalding, the utility of existing tools for viewing job history decreases rapidly. Note how paging through a very large number of jobs becomes unrealistic, especially when newly finished jobs push jobs rapidly through pages before you can navigate there. Extracting insights and browsing thousands of jobs becomes a challenge using the existing JobTracker user interface. We created hRaven to improve this situation and are open sourcing the code on GitHub today at the Hadoop Summit under the Apache Public License 2.0 to share with the greater Hadoop community. Why hRaven? There were many questions we wanted to answer when we were created hRaven. For example, how many Pig versus Scalding jobs do we run? What cluster capacity do jobs in my pool take? How many jobs do we run each day? What percentage of jobs have more than 30,000 tasks? Why do I need to hand-tune these (hundreds) of jobs, can’t the cluster learn and do it? We found that the existing tools were unable to start answering these questions at our scale. How does it work? hRaven archives the full history and metrics from all MapReduce jobs on clusters and strings together each job from a Pig or Scalding script execution into a combined flow. From this archive, we can easily derive aggregate resource utilization by user, pool, or application. Historical trending of an individual application allows us to perform runtime optimization of resource scheduling. The key concepts in hRaven are: cluster : each cluster has a unique name mapping to the JobTracker user : MapReduce jobs are run as a given user application : a Pig or Scalding script (or plain MapReduce job) flow : the combined DAG of jobs from a single execution of an application version : changes impacting the DAG are recorded as a new version of the same application hRaven stores statistics, job configuration, timing and counters for every MapReduce job on every cluster. The key metrics stored are: Submit, launch and finish timestamps Total map and reduce tasks HDFS bytes read and written File bytes read and written Total map slot milliseconds Total reduce slot milliseconds This structured data around the full DAG of MapReduce jobs allows you to query for historical trending information or better yet, job optimization based on historical execution information. A concrete example is a custom Pig parallelism estimator querying hRaven that we use to automatically adjust reducer count. Data is loaded into hRaven into three steps, coordinated through ProcessRecords which record processing state in HBase: JobFilePreprocessor JobFileRawLoader JobFileProcessor First, the HDFS JobHistory location is scanned and the JobHistory and JobConfiguration file names of newly completed jobs are added to a sequence file. Then a mapreduce job runs on the source cluster to load the JobHistory and JobConfiguration files into HBase in parallel. Then in the third step a mapreduce job runs on the HBase cluster to parse the JobHistory and store individual stats and indexes. hRaven provides access to all of its stored data via a REST API, allowing auxiliary services such as web UIs and other tools to be built on it with ease. Below is a screenshot of an Twitter internal reporting application based on hRaven data showing overall cluster growth. Similarly we can visualize spikes in load over time, changes in reads and writes by application and by pool, as well as aspects such as pool usage vs. allocation. We also use hRaven data to calculate compute cost along varying dimensions. Future work In the near future, we want to add real time data loading from JobTracker and come up with a full flow-centric replacement for the JobTracker user interface (on top of integrating with the Ambrose project). We would also like hRaven be enhanced to capture flow information from jobs run by frameworks other than Pig and Cascading, for instance Hive. Furthermore, we are in the process of supporting Hadoop 2.0 and want to focus on building a community around hRaven. The project is still young, so if you’d like to help work on any features or have any bug fixes, we’re always looking for contributions or people to join the flock to expand our @corestorage team. In particular, we are looking for engineers with Hadoop and HBase experience. To say hello, just submit a pull request, follow @TwitterHadoop or reach out to us on the mailing list . If you find something broken or have feature request ideas, report it in the issue tracker . Acknowledgements hRaven was primarily authored by Gary Helmling ( @gario ), Joep Rottinghuis ( @joep ), Vrushali Channapattan ( @vrushalivc ) and Chris Trezzo ( @ctrezzo ) from the Twitter @corestorage Hadoop team. In addition, we’d like to acknowledge the following folks who contributed to the project either directly or indirectly: Bill Graham ( @billgraham ), Chandler Abraham ( @cba ), Chris Aniszczyk ( @cra ), Michael Lin ( @mlin ) and Dmitriy Ryaboy ( @squarecog ).", "date": "2013-06-26"},
{"website": "Twitter-Engineering", "title": "Zippy Traces with Zipkin in your Browser", "author": ["‎@IceCreamYou‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013/zippy-traces-zipkin-your-browser.html", "abstract": "Last summer, we open-sourced Zipkin , a distributed tracing system that helps us gather timing and dependency data for the many services involved in managing requests to Twitter. As we continually improve Zipkin, today we’re adding a Firefox extension to Zipkin that makes it easy to see trace visualizations in your browser as you navigate your website. Get Zippy Traces with Zipkin in your browser! https://t.co/v75OcKry32 — Zipkin project ( @zipkinproject ) June 18, 2013 This helps us see a holistic view of how an entire Twitter web page is generated from the many requests behind it. Zipkin’s web interface looks like this: By default, you choose a service you’re interested in, and Zipkin shows you the requests it traced that involved that service. Choose one and it will show you a waterfall visualization and other information. But a full web page load encompasses multiple HTTP requests and thus multiple Zipkin traces, and as a result it can be difficult to get a complete view of all the requests of a single page load in Zipkin. That’s where the extension comes in: The extension intercepts every HTTP request to a site you specify in the extension’s settings and adds special HTTP headers. Normally Zipkin selects a sample of requests to trace, but the special headers tell Zipkin that this specific request must be traced. The extension then retrieves the trace data from the Zipkin REST API and uses it to display a lightweight waterfall visualization for each request in a Firebug panel. Hovering over a segment of the waterfall shows the name of the service it represents. Acknowledgements and Future Work Zipkin was originally authored by Johan Oskarsson ( @skr ), Franklin Hu ( @thisisfranklin ), and Brian Degenhardt ( @bmdhacks ). This particular Firefox extension was authored by Isaac Sukin ( @IceCreamYou ). Although Zipkin was built for Twitter initially, it was designed to be used and extended by everyone. On top of that, we are working harder to make Zipkin more consumable and look forward to any contributions from the community. If you want to get involved by adding features or support browsers, please let us know on GitHub . If you find any issues, pull requests are always welcome! To stay in touch, follow @zipkinproject and join the Zipkin mailing list .", "date": "2013-06-19"},
{"website": "Twitter-Engineering", "title": "Libcrunch and CRUSH Maps", "author": ["‎@jerryxu‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013/libcrunch-and-crush-maps.html", "abstract": "When we introduced our in-house photo storage system Blobstore to the world , we discussed a mapping framework called libcrunch for Blobstore that maps virtual buckets to storage nodes. The libcrunch implementation was heavily inspired by the seminal paper on the CRUSH algorithm. Today, we are open-sourcing libcrunch on GitHub under the Apache Public License 2.0 to share some of our code with the greater community. Why libcrunch? In developing Blobstore, we knew we wanted a mapping framework that can: support a flexible topology definition support placement rules such as rack diversity handle replication factor (RF) distribute data with a good balance cause stable data movement as a result of topology change (when a small change in the topology occurs, the resulting data movement should be similarly small) exhibit a good mean time to recovery when a node fails In addition to these fairly standard requirements, we zeroed in on another critical factor, which we call Replica Distribution Factor (RDF). In our initial research, we didn’t find any open source libraries that met our needs on the JVM, so we developed our own. There are mapping algorithms that may satisfy some of these criteria (such as consistent hashing) but none satisfies all of them, especially RDF. What is RDF? RDF is defined as the number of data nodes that share any data with a node. To understand RDF, you can look at one data node and how many other data nodes share any data with that node. In an extreme case, you can imagine data mapping where each data node is completely replicated by another data node. In this case, RDF would be 1. In another extreme case, every single data node may participate in having replicas of every other node. In that case, RDF would be as large as the size of the entire cluster. The key concern RDF seeks to address is the (permanent) loss of any data. It would be useful to think about the following scenario. Suppose the replication factor is 2 (2 replicas for each datum). And suppose that we lost one data node for any reason (disk failures and loss of racks). Then the number of replicas for any data that was on that lost data node is down to 1. At this point, if we lose any of those replicas, that piece of data is permanently lost. Assuming that the probability of losing one data node is small and independent (a crude but useful approximation), one can recognize that the probability of losing any data increases proportionally with the number of data nodes that share data with the lost node. And that is the definition of RDF. The bigger the RDF the bigger the probability of losing any data in case of data node loss. By tuning RDF down to a smaller number, one can mitigate the probability of permanent data loss to an acceptable level. As you can imagine, RDF becomes much more relevant if the replication factor (RF) is small. One can adopt a larger RF to address the risk of data loss but it would come at a cost, and a prohibitively expensive one if the data size is large. Libcunch is designed to deliver these functionalities, including RDF. How does it work? The libcrunch implementation uses the basic CRUSH algorithm as the building block of how it computes mapping. The CRUSH algorithm provides a number of functionalities that are mentioned in the paper. By using this algorithm to store and retrieve data, we can avoid a single point of failure and scale easily. To be able to limit the size of the RDF, we use a two-pass approach. In the first pass, we compute what we call the RDF mapping using the same cluster topology but using each data node (or its identifier) as the data. This way, we can come up with a fairly well-defined RDF set from which data mapping can be handled later. In the second pass, we compute the actual data mapping. But for a given data object, we don’t use the full cluster to select the data nodes. Instead, we limit the selection to one of those RDF set we computed in the first pass. Example code A mapping function is needed when you have a number of data objects you want to distribute to a number of nodes or containers. For example, you may want to distribute files to a number of storage machines. But it may not need to be limited to physical storage. Any time logical data is mapped to a logical container, you can use a mapping function. Creating and using the libcrunch mapping functions are pretty straightforward. The key part is to implement the placement rules you desire (such as rack isolation rules), and set up your cluster topology in terms of the type Node provided by libcrunch. Then you get the mapping result via the MappingFunction.computeMapping() method. For example: // set up the placement rules PlacementRules rules = createPlacementRules(); // instantiate the mapping function MappingFunction mappingFunction = new RDFMapping(rdf, rf, rules, targetBalance); // prepare your data List<Long> data = prepareYourDataIds(); // set up the topology Node root = createTopology(); // compute the mapping Map<Long,List<Node>> mapping = mappingFunction.computeMapping(data, root); Future work In the near future, we look forward to improving documentation and nurturing a community around libcrunch. We are also constantly looking for ways to improve various aspects of the algorithm such as balance and stability. We are also planning to adopt libcrunch in other storage systems we are developing at Twitter. If you’d like to help work on any features or have any bug fixes, we’re always looking for contributions or people to join the flock to build out our core storage technology. Just submit a pull request to say hello or reach out to us on the mailing list . If you find something broken or have feature request ideas, report it in the issue tracker . Acknowledgements Libcrunch is a team effort by Twitter’s Core Storage team ( @corestorage ). It was primarily authored by Jerry Xu ( @jerryxu ) and Sangjin Lee ( @sjlee ). The idea for libcrunch came out of discussions by Peter Schuller ( @scode ), Boaz Avital ( @bx ) and Chris Goffinet ( @lenn0x ), who also has made a number of direct contributions to the current manifestation. We’d also like to thank Stu Hood ( @stuhood ) for his invaluable feedback and contributions.", "date": "2013-06-19"},
{"website": "Twitter-Engineering", "title": "CSP to the Rescue: Leveraging the Browser for Security", "author": ["‎@ndm‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013/csp-to-the-rescue-leveraging-the-browser-for-security.html", "abstract": "Programming is difficult — and difficult things generally don’t have a perfect solution. As an example, cross-site scripting (XSS) is still very much unsolved. It’s very easy to think you’re doing the right thing at the right time, but there are two opportunities to fail here: the fix might not be correct, and it might not be applied correctly. Escaping content (while still the most effective way to mitigate XSS) has a lot of “gotchas” (such as contextual differences and browser quirks) that show up time and time again. Content Security Policy (CSP) is an additional layer of security on top existing controls. Twitter has recently expanded our use of response headers in order to leverage the protection they provide via the browser. Headers like X-Frame-Options (for clickjacking protection) and Strict Transport Security (for enforcing SSL) are somewhat common these days, but we’re here to discuss and recommend Content Security Policy. What is Content Security Policy? Content Security Policy (CSP) is a whitelisting mechanism that allows you to declare what behavior is allowed on a given page. This includes where assets are loaded from, where forms can send data, and most importantly, what JavaScript is allowed to execute on a page. This is not the first time we’ve blogged about CSP or have dealt with CSP related vulnerabilities: http://t.co/V1r84QSL のjavascript:のリンクを埋め込めたXSSなおった。ちなみにFirefoxからではCSPによって実行をブロックされた。実際のXSSでCSPが反応したのを見たのはこれが初めて。 pic.twitter.com/ytFYeR4w — Masato Kinugawa ( @kinugawamasato ) May 8, 2012 CSP empowers you to disallow inline JavaScript including onclick and other DOM events, links with “JavaScript:” values, and <script> blocks in the HTML content of a page. This feature effectively eliminates all stored and reflected XSS. Here’s an example of using CSP to disable the content inside a script tag: But wait! There’s more. CSP can be used to ensure that you don’t cause any mixed-content issues that can lead to session token theft or other tampering with user experience (see “replace all pictures with lolcats” for resources loaded over HTTP). You can also ensure that content is only loaded from hosts that you trust. In a sense, the CSP header documents the integrity of your web page inside a response header. I know what you’re thinking: This sounds disruptive, potentially flawed, and could really break my site without even realizing it! CSP violations will cause the browser to POST json content to the specified report-uri. There’s also a “report-only” mode in which violations are reported without actually changing any behavior. You can analyze the values of these reports by inspecting the browser console as well as aggregating the reports that are sent to the “report-uri” value. The reports contain useful information such as the type of violation that occurred, the action that was blocked, the part of the policy that was violated, and other environmental information that can help describe what happened. You can use these reports to tune the policies over time and look for anomalous behavior (which might indicate a successful exploit). How CSP was implemented at Twitter Application of the CSP header was previously done inconsistently. Differences in browser implementations compounded the issue. That problem was solved by providing easy-to-use solutions that abstract browser-specific header implementations. This involved augmenting frameworks or providing libraries to manage these headers. Our ruby implementation ( secure_headers ) is open sourced and this has Ruby on Rails support to make the application of CSP and other headers minimally painful. We have built similar, but not identical, functionality in the framework which all Scala web projects are built upon. That was just the technical challenge. We decided to fully invest in the technology and doing so would require a moratorium on inline JavaScript and eventually inline styles. We did receive some pushback, but the support was overwhelming and coming from all directions and all levels. This allowed us to discuss CSP as a company-wide development guideline which in turn led to the almost sitewide application of this header. A side effect of this process was that we realized contextual encoding of dynamic content is an anti-pattern. Contextual encoding promotes using dynamic content in any situation so long as it is properly encoded. This complexity is unnecessary and error-prone. By removing all inline JavaScript and inline event handlers, we completely eliminated the need to encode for the JavaScript context. Dynamic values can be html entity encoded and placed in element attributes or in the body of tags which is read by the external JavaScript. The techniques used to apply this strategy can be found here . This can be adopted as a company wide standard because all frameworks have html entity encoding built in. We were really surprised with what kinds of reports we were seeing. By examining the CSP reports we found foreign domains pointing at our hosts, potential indicators of infected browsers/computers, non-security-related bugs in site functionality, as well as how popular certain browser extensions are among our visitors. Most plugins aren’t CSP-friendly so this can generate a lot of noise. Don’t worry, we don’t aggregate the data with identifying information, all reports are treated as anonymous. What about CSP and Twitter today and the future? As you probably can tell, you don’t see CSP on Twitter.com today. As one of the oldest projects, there is a wealth of inline JavaScript which is delaying our rollout for all of our public properties. We have also encountered a few cases which require inline script for functionality or performance issues that are prolonging the process which necessitates exceptions for endpoints that are not ready for CSP. However, CSP is applied to all other Twitter properties including Twitter Mobile , Twitter Ads Center , Translation Center , Twitter for Business , Tweetdeck and more. In fact, this post is hosted on our new blogging platform, which applies a strict policy too. In some situations, like high latency connections, inline scripts/styles perform better due to the lower number of connections required to satisfy a window load. In these cases, we remove inline script/styles for users with fast connections, and leave the scripts/styles in for the rest. There are a few browser quirks and workarounds that require inline script but we’re confident these things will be phased out over time. As these blockers fade away, the header will continue to proliferate until we have 100% coverage. We track the application of CSP, as well as other “security headers” on a dashboard - full coverage means all cells are showing green. CSP 1.1 is proposing important changes that will make changes that will help make CSP a viable option for a wider audience. In addition to making the existing specification easier to use, new features are also being worked on. Currently, there is a discussion related to how to can use CSP Javascript execution protection without being required to remove all inline script blocks. While removing the inline content is still your best option due to wider adoption and reduced complexity, CSP 1.1 allows you to whitelist individual script blocks. This is accomplished by providing a fingerprint and whitelisting the values in the header. Another interesting proposal is the concept of a DOM API for interacting with CSP. The ability to interact with CSP will provide the ability to toggle whether client-side capabilities/workarounds should be enabled/disabled based on how the features would interact with CSP protection. Conversely, 1.1 also specifies the ability to disable the eval function via the API. In the case where a library will only do an eval on initialization, we can disable eval after initialization to keep the policy very strict. As noted in the Github Content Security Policy post , plugin and bookmarklet implementation in browsers differs with the spec text in that violations from these sources break functionality and generate reports. This is definitely a pain point when reviewing reports. Unfortunately, this isn’t a trivial fix for the browser implementers. The usefulness of CSP reports will skyrocket once this is implemented. Applying CSP sitewide is an effort that involved every web development team in the company. We could not have done this without the support of the developers on the ground, who sometimes before getting a full blessing from their peers, would investigate and/or submit a patch or shepherd my attempt to apply the policy. We have been practicing the “if you build it, they will come” strategy, and so far it seems to work very well for response headers, especially CSP. Further reading CSP 1.0 Spec: http://www.w3.org/TR/CSP/ CSP 1.1 Spec (work in progress): https://dvcs.w3.org/hg/content-security-policy/raw-file/tip/csp-specification.dev.html Web Application Security Working Group: http://www.w3.org/2011/webappsec/ secure_headers library: https://github.com/twitter/secureheaders Introduction to CSP - Mike West: http://www.html5rocks.com/en/tutorials/security/content-security-policy/ Security Headers on the Top 1,000,000 Websites - Veracode: http://www.veracode.com/blog/2013/03/security-headers-on-the-top-1000000-websites-march-2013-report/ Removing inline JavaScript (for CSP): http://nmatatal.blogspot.com/2013/01/removing-inline-javascript-for-csp.html", "date": "2013-06-06"},
{"website": "Twitter-Engineering", "title": "2017", "author": ["‎@‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2017.html", "abstract": "", "date": "1970-01-01"},
{"website": "Twitter-Engineering", "title": "How Twitter deploys its widgets JavaScript", "author": ["Aravind Ramanathan"], "link": "https://blog.twitter.com/engineering/en_us/a/2016/how-twitter-deploys-its-widgets-javascript.html", "abstract": "Deploys are hard and it can be frustrating to do them. Many bugs manifest themselves during deploys, especially when there are a large number of code changes. Now what if a deploy also goes out to millions of people at once? Here’s the story of how my team makes a deploy of that scale safely and with ease. Publishers on the web use a single JavaScript file, widgets.js , to embed Twitter content on their website. Embedded Tweets, embedded timelines, and Tweet buttons are powered by the same JavaScript file, making it easy for web publishers to integrate widgets into their websites. We update widgets.js weekly, deploying bug fixes and adding features without requiring our publishers to do any work to receive the updates. But to ensure things stay simple for our customers, we need to take on some of the complexity. Specifically, we need to deploy a single, unversioned, well-known, static asset which publishers trust to run within their page. This code is executed roughly 300,000 times a second by over a billion visitors every month. We know this is a big responsibility for us and that’s why we recently invested in upgrading the widgets.js deploy process to catch mistakes early, and avoid negative customer impact. A safe deploy We began this project by cementing what an ideal deploy would look like. Specifically, we identified three qualities of a safe deploy that we were after: Reversibility: ‘Rollback first, debug later’ is our motto. Rollback should be fast, easy, and simple. Ideally, it’s a giant red button that can get our heart rates down. Incremental release: All code has bugs and deploys have an uncanny way of surfacing them. That’s why we wanted the ability to release new code in phases. Visibility: We need to have graphs to show how both versions of widgets.js are doing at all times. We also need the ability to drill down by country, browser type, and widget type. These graphs should be real time so we can quickly tell how a deploy is going and take action as necessary. These were the goals we set for ourselves. Now, let’s dive into the details of how we accomplished them. How the deploy works Because widgets.js is a well-known asset (platform.twitter.com/widgets.js), it has no versioning in its file name which makes it harder to control the progress of its deploy. The way we decided to control the release of a new version of this file is by controlling how our domain, platform.twitter.com, gets resolved at the DNS level. This way, we can set rules to resolve the file to either the new version, or the old version, during the deploy. Deploy architecture To implement such control over our DNS, we had to configure three components: DNS Management service: This is a service that lets us control how platform.twitter.com gets resolved to an IP address. We use geographic regions to control the roll-out, based on the following three rules, which correspond to each phase of the deploy: Phase 1: 5% of traffic from Region A gets IP2 and others get IP1. Phase 2: 100% of traffic from Region A gets IP2 and others get IP1. Repeat for increasingly larger regions. Phase 3: 100% of all traffic gets IP2. This includes TOR traffic and any requests that we could not identify what region it is coming from. CDN (Content Delivery Network): This is a service that helps serve our static assets in a performant manner. Ours is configured so that if a request is made through IP1 it’ll serve the asset from ORIGIN 1, otherwise ORIGIN 2. Origin: A storage service, like Amazon S3, where widgets.js is uploaded. CDN asks the origin for the latest version to serve. The default state is that all requests are served by the asset in ORIGIN 1. A deploy starts with uploading a new version of widgets.js to ORIGIN 2. Then we start moving traffic to ORIGIN 2 by going from Phase 1 to 3, as described above. If the deploy is successful, we copy assets from ORIGIN 2 to ORIGIN 1 and reset all traffic to ORIGIN 1. Scoring the new deploy process Our goal was to execute a safe deploy, so let’s evaluate how we did. By having 2 origins, we were able to rollback instantly – a rollback here is moving all traffic back to ORIGIN 1 where we have the previous version of widgets.js . The geography-based deploy gave us a way to incrementally rollout the new version and only move forward if it was safe to do so. Lastly, our client code logs the release version, so we were able to build real-time graphs that told us if the deploy was successful or not. A successful deploy looks like this today: Traffic changes of the new and old version during a successful deploy We have used this deploy process for nearly a year now, and detected several regressions earlier than we would have previously. For example, we recently had a bug in our code where a lazy loaded JavaScript file was incorrectly addressed which resulted in our widgets not rendering completely. But thanks to this deploy process, we quickly saw the impact and were able to address it before it affected customers widely. Next steps We have big ideas on what we can improve for the next iteration. One thing we have learned is that our DNS rules could be better. We would like Phase 1 to be a small but significant number of users to give us quick insight into critical regressions. In Phase 2, we would like the sample of users to be bigger to catch more subtle bugs that show up only at scale. Matching our phases to these goals requires some tuning of DNS rules which is an area we want to invest going forward. Another area we would like to improve is the total deploy time. With so many moving pieces, our deploy time has increased from a few minutes to a couple of hours, and this is mostly because we have to wait for all intermediate caches to invalidate each time we move traffic from one phase to another. We would also like to add performance metrics in the future, so we can expand our release verification from raw successes/failures to deeper performance insights such as render times in different locations around the world. We use external vendors for CDN and DNS management and all the configurations that I describe here at the DNS level use publicly documented APIs that you can use for your deploys today. Overall, we are very happy with how the new deploy process works for us today because it has emboldened us to ship updates more often while still keeping things simple for publishers.", "date": "2016-09-21"},
{"website": "Twitter-Engineering", "title": "SuperRoot: Launching a High-SLA Production Service at Twitter", "author": ["‎@sam‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2016/superroot-launching-a-high-sla-production-service-at-twitter.html", "abstract": "Our Search Infrastructure team is building a new information retrieval system called Omnisearch to power Twitter’s next generation of relevance-based, personalized products. We recently launched the first major architectural component of Omnisearch, the SuperRoot. We thought it would be interesting to share what’s involved with building, productionizing, and launching a new high-scale, high-SLA distributed system at Twitter. For a person using Twitter’s consumer search product, it might look like Twitter has one search engine, but we actually have five indexes working together to serve many different products: A high-scale in-memory index serving recent, public Tweets A separate in-memory index to securely serve protected Tweets A large SSD-based archival index serving all public Tweets ever An in-memory typeahead index, for autocompleting queries An in-memory user search index We maintain several separate search indexes because each differs in terms of scale, latency requirements, usage patterns, index size and content. While they are all Finagle-based Thrift services that implement a shared API and query language, interfacing with multiple indexes is inconvenient for internal customers because they must understand which index to query for their use case and how to correctly merge results from different indexes. For example, Twitter’s consumer search product displays a variety of types of content based on the user’s intent; for example, recent Tweets, high-quality older Tweets, relevant accounts, popular images, etc. As shown in Figure 1, we query multiple indexes to get all of the necessary content to build the page. Once we have results from each type of index, we merge them, removing duplicates. This isn’t as easy as it sounds: after removing duplicates, there may not be enough results to fill the page. With a static index, we could simply retry the query, asking for more results. However, some of our indexes are updated in realtime, meaning a retry of the same query may return different (fresher) results, which should be considered. Correctly implementing pagination across the boundaries of rapidly changing realtime indexes is even more challenging. With the architecture in Figure 1, the code to manage merging and pagination had to be replicated for each product, slowing product development. Finally, consider the challenges of modifying the system shown in Figure 1: to add another index, merge two indexes, or change our API, we would have to work with every customer individually to avoid breaking their product. This overhead was slowing down the Search Infrastructure team’s progress on Omnisearch, which we believe is an important technology for increasing the pace of development of relevance-based products over the coming months. Enter SuperRoot “All problems in computer science can be solved by another level of indirection, except of course for the problem of too many indirections.” David J. Wheeler Recently, we launched SuperRoot, a scatter-gather Thrift aggregation service that sits between our customers and our indexes: SuperRoot adds a layer of indirection to our architecture, presenting a consistent, logical view of the underlying decomposed, physical indices. By choosing to use the same API as each individual index’s root service, we were able to migrate existing customers seamlessly. It also adds less than 5ms of overhead to each request, which we think is a fair price given the functionality, usability and flexibility it affords. The new functionality available in SuperRoot includes: Efficient query execution, by only hitting the necessary indexes for a query Merging across indexes of the same document type (e.g., Tweets) Precise pagination across temporally tiered indexes (i.e., real-time to archive) Per-customer quotas and rate limiting Search-wide feature degradation for better availability (e.g. reduced quality under heavy load) Improved access control, monitoring, security and privacy protections For our internal customers, the SuperRoot means faster and easier development of new products. They no longer have to understand which indexes are available, write code to talk to each of them, reconcile duplicates, and implement complex pagination logic. Prototyping and experimentation using our search indexes is closer to self-serve. Debugging is also easier, both because their code is simpler and because they interface with fewer downstream services. For the Search Infrastructure team, it creates a much needed layer of abstraction that will allow us to iterate faster on Omnisearch. We can now introduce a new, simple API to search in a single place. We can add or remove indexes without coordinating with every internal customer. We can change index boundaries (e.g., increasing or decreasing the depth of the real-time Tweet index) without breaking consumer products. In the extreme, we can even experiment with new architectures and search technologies behind the scenes. The Road to Production Now that you understand what SuperRoot is and why we wanted it, you might be thinking that adding an additional layer of indirection to a distributed system isn’t a new idea. This is true! This project was not about novel architectures, creative data structures or complex distributed algorithms. What was interesting about the SuperRoot project was how we used the technologies available at Twitter to develop and deploy a new high-scale distributed system without impacting our customers. SuperRoot has capacity to serve over 85,000 queries per second (QPS) per datacenter. It services not only Twitter’s Search product, but also our Home Timeline, our data products, and many other core Twitter features. With this much traffic, small mistakes in implementation, scaling issues, or deploy problems would have an outsized impact on our users. Yet, we were able to launch SuperRoot to 100% of customers without a single production incident or unhappy internal customer. This is delicate work and it’s a major aspect of what we do as Platform engineers at Twitter. SLAs: Latency, Throughput, and Success Rate In addition to functionality, our internal customers will usually specify basic requirements for serving their product, which we commit to in the form of a Service-Level Agreement (SLA). For SuperRoot customers, the most important aspects of the SLA are query latency (measured in milliseconds), query throughput (measured in QPS) and success rate (the percentage of queries that succeed per second). Minimizing Overhead with Finagle and Thrift SuperRoot aggregates the results from different indexes, some of which serve from RAM with very low latencies. For example, the in-memory index that serves recent, public Tweets has p50 latencies of under 10ms (for simpler queries) and p999 latencies under 100ms (for expensive queries). Since low latencies are required for many of our products, it was important that SuperRoot not add a lot of overhead. SuperRoot relies on Finagle’s RPC system and the Thrift protocol to ensure low overhead. One way that Finagle minimizes overhead is through RPC multiplexing, meaning there is only one network connection per client-server session, regardless of the number of requests from the client. This minimizes use of bandwidth and reduces the need to open and close network sockets. Finagle clients also implement load balancing, meaning requests are distributed across the many SuperRoot instances in an attempt to maximize success rate and minimize tail latencies. For SuperRoot, we chose the Power of Two Choices (P2C) + Peak EWMA load balancing algorithm, which is designed to quickly move traffic off of slow endpoints. Finally, the choice of Thrift reduces overhead in terms of bandwidth and CPU for serialization and deserialization, compared to a non-binary protocol like JSON. Ensuring Sufficient Throughput Not all queries are created equal: simple realtime queries take only a few milliseconds while hard tail queries can take hundreds of milliseconds and hit multiple indexes. At Twitter, we do “redline testing,” using real traffic to accurately measure the real-world throughput of a service. These tests work by incrementally increasing load balancer weights for instances under test until key metrics (e.g. latency, CPU or error rate) hit a predetermined limit. Redline tests at Twitter can be run via a self-serve UI and are available to all teams. Using these tests, we determined (and later adjusted) the number of instances required for SuperRoot to serve 85,000 real-world QPS. Reliability Once SuperRoot had the required functionality and was provisioned to handle the estimated load, we needed to make sure it was stable. We measure the stability of our systems with a metric called “success rate,” which is simply the number of successful requests divided by the total number of requests, expressed as a percentage. For SuperRoot, we were looking for a 99.97% success rate, which means we tolerate no more than 3 failures per 10,000 queries. During normal operation, our success rate is usually significantly higher, but we consider anything lower an “incident.” A shared cloud is a critical tool for reliably operating large distributed systems. Twitter uses Apache Mesos and Apache Aurora to schedule and run stateless services like SuperRoot. Mesos abstracts CPU, memory, storage, and other compute resources away from the actual hardware. Aurora is responsible for scheduling (and rescheduling) jobs onto healthy machines and keeping them running indefinitely. Given number of SuperRoot instances we need, hardware failures are expected on a daily basis. We would not be able to attain a 99.97% success rate if for every hardware failure, we had to manually take corrective action. Thankfully, Finagle routes traffic away from bad instances while Mesos and Aurora reschedule jobs quickly and automatically. One interesting and unexpected issue we encountered when building SuperRoot manifested itself as a periodic success rate dip, occurring every few minutes. On the surface, it looked like the dips were caused multiple sequential timeouts when querying in-memory Earlybird indexes. We naturally suspected an issue with Earlybird, but Earlybird’s graphs showed consistent response times of under 10ms. On deeper inspection, we found that the requests were timing out before they were sent to Earlybird, implicating SuperRoot itself. Often times, when debugging distributed systems, it is hard to tell the difference between cause and effect without some experimentation. For example, we noticed slight CPU throttling by Mesos around the timeout events, so we tried allocating more CPU. This did not help, indicating it was an effect, not the cause. With our long-running JVM-based services, we often suspect garbage collection (GC), but we didn’t see any correlation between GC events and timeout events in our logs. However, when inspecting the logs, we did notice that the logs themselves were being printed during the events! From this observation, we were able to trace the issue back to a release of a new Twitter-specific JVM. With the smoking gun in hand, we worked with our VM team to identify synchronous GC logging in the JVM as the culprit. The VM team implemented asynchronous logging and the issue disappeared, clearing the SuperRoot for launch. Getting to Perfect For SuperRoot to reach its full potential, we needed every customer of the search infrastructure to use it. For this to happen, we needed to guarantee that the results from SuperRoot exactly matched what each customer expected. Before SuperRoot, most customers were directly hitting the roots of individual indexes (see Figure 1). With the introduction of SuperRoot, the responsibility of hitting multiple indexes and merging their results moved to it, meaning that any mistake made in the merging logic would directly manifest itself as a bug or quality regression in one of our products. The process of getting to feature parity started with understanding our customers’ needs, typically by reading their code and talking to them. We then wrote unit tests, implemented an initial version, and deployed it. To verify the correctness of a new implementation of an existing system, we used a technique we call “tap-compare.” Our tap-compare tool replays a sample of production traffic against the new system and compares the responses to the old system. Using the output of the tap-compare tool, we found and fixed bugs in our implementation without exposing end customers to the bugs. This allowed us to migrate customers one-by-one to SuperRoot without incident. In one case, in an effort to reduce complexity, we didn’t want to recreate the exact logic in the customer’s system. We suspected that the simplification we had in mind would have a subtle impact on the Home timeline’s filtering algorithm, but we didn’t know if the effect would be positive or negative. Tap-compare techniques don’t help when you’re not looking for exact feature parity, so we instead chose to A/B test the effect on the Home timeline. Given the high-stakes nature of changing the Home timeline, we felt the added time and complexity of running a proper A/B test was prudent. Ultimately, we found that our simplification reduced end-user engagement, and so we abandoned it in favor of a more complex implementation. #ShipIt Due to the incremental nature of our development process, there was no single day when we launched the SuperRoot. Instead, we shipped each request type one at a time, ensuring quality and correctness along the way. In our retrospective, the team was particularly proud by how smoothly this project went. There were no incidents, no unhappy customers, we cleared our backlog, and there were plenty of learning opportunities for the team members, many of whom had never built a distributed system from the ground up. Acknowledgements The core SuperRoot team was Dumitru Daniliuc ( @twdumi ), Bogdan Gaza ( @hurrycane ), and Jane Wang ( @jane12345689 ). Other contributors included Paul Burstein ( @pasha407 ), Hao Wu, Tian Wang ( @wangtian ), Xiaobing Xue ( @xuexb ), Vikram Rao Sudarshan ( @raosvikram ), Wei Li ( @alexweili ), Patrick Lok ( @plok ), Yi Zhuang ( @yz ), Lei Wang ( @wonlay ), Stephen Bezek ( @SteveBezek ), Sergey Serebryakov ( @megaserg ), Yan Zhao ( @zhaoyan1117 ), Joseph Barker ( @seph_barker ), Maer Melo ( @maerdot ), Mark Sparhawk ( @sparhawk ), Dean Hiller ( @spack_jarrow ), Sean Smith ( @seanachai ), and Sam Luckenbill ( @sam ).", "date": "2016-07-22"},
{"website": "Twitter-Engineering", "title": "Bug Bounty, 2 years in", "author": ["‎@arkadiyt‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2016/bug-bounty-2-years-in.html", "abstract": "Security on a global platform like Twitter is a 24/7 job – we are constantly evolving to respond to new threats and attacks against our users and our systems. In order to stay ahead of the game we staff dedicated account-, network-, enterprise-, corporate-, and application-security teams, as well as an incident detection and response team. We also maintain a secure development lifecycle that includes secure development training to everyone that ships code, security review processes, hardened security libraries and robust testing through internal and external services – all to maximize the security we provide to our users. On top of these measures we also engage the broader infosec community through our bug bounty program, allowing security researchers to responsibly disclose vulnerabilities to us so that we can respond and address these issues before they are exploited by others. We’ve been running our program on HackerOne since May 2014 and have found the program to be an invaluable resource for finding and fixing security vulnerabilities ranging from the mundane to severe. In the two years since launch we’ve received 5,171 submissions to our program from 1,662 researchers. 20% of our resolved bugs have been publically disclosed (we allow bugs to be publically disclosed after they’ve been fixed, at the request of the researcher) We’ve paid out a total of $322,420 (USD) to researchers Our average payout is $835 Our minimum payout is $140, and our highest payout to date was $12,040 (our payouts are always a multiple of 140) In 2015 alone, a single researcher made over $54,000 for reporting vulnerabilities At the time that we made our $12,040 payout we set a record on Hackerone: Congrats @filedescriptor for record $12k award from @twittersecurity , and thanks for making the Internet safer! pic.twitter.com/z6RLUBOIlr — HackerOne ( @Hacker0x01 ) December 22, 2015 We also offer a minimum of $15,000 for remote code execution vulnerabilities, but we have yet to receive such a report. Since launching the program we’ve seen impressive growth in both the number of vulnerabilities reported and our payout amounts, reflecting our rising payout minimums and also the growing community of ethical hackers participating in the program: Notable bugs We’ve had many great bugs exposed through the program. For example: XSS inside Crashlytics Android app: The Android Crashlytics application renders part of its content inside a webview, which did not have adequate protection against cross site scripting attacks. By creating an application with a malicious name like ‘”><img src=x>’ it was possible to trigger an XSS inside the application. HTTP response splitting with header overflow: sending a long (just under 8192 bytes) payload to a number of endpoints would cause a failure to occur, and repeating the same request would return a valid page with attacker-controlled headers based on the payload sent. Read the full blog post from the reporter or the disclosed report on HackerOne. IDOR allowing credit card deletion: a simple insecure direct object reference bug on the credit card deletion endpoint allowed an attacker to delete, but not view, credit cards not belonging to them. Additionally, the ids were auto-incrementing integers and there was no rate-limiting on the endpoint, so it was possible for someone to delete all credit cards on Twitter. This bug has also been publicly disclosed and is one of the many bugs covered in our Secure Coding class taught to all new hires during orientation. We’re thankful to all the security researchers who have worked hard to find and report vulnerabilities in Twitter, and we look forward to continuing our good faith relationship in 2016 and beyond. If you’re interested in helping keep Twitter safe & secure too then head on over to our bug bounty program , or apply to one of our open security positions !", "date": "2016-05-27"},
{"website": "Twitter-Engineering", "title": "Introducing Omnisearch", "author": ["‎@sam‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2016/introducing-omnisearch.html", "abstract": "Twitter has more than 310 million monthly active users who send hundreds of millions of Tweets per day, from all over the world. To make sure everyone sees the Tweets that matter most to them, we’ve been working on features that bring the best content to the forefront. We’ve refreshed the Home timeline to highlight the best Tweets first , introduced tailored content via Highlights for Android , and personalized the search results and trends pages. The performance of these products depends on finding the most relevant Tweets from a large set of candidates, based on a product-specific definition of “relevant.” From an engineering point of view, we view these as information retrieval problems where the documents are Tweets and the product is defined by a query. For example, to show you the best Tweets first in your Home timeline, we might first find candidate Tweets from accounts you follow with a search query like this one: In our search index, we rank the candidate Tweets and select the best few using signals that correlate to the likelihood you’ll engage with them. By building products based on search infrastructure, we can speed up development while simultaneously reducing the inherent risk of developing custom new infrastructure. Furthermore, it allows product teams to experiment quickly — changing a search query is easy! Omnisearch: an information retrieval system To meet the needs of these (and future) products, we’re building a new information retrieval system called Omnisearch . Like databases, information retrieval systems match documents (e.g., Tweets or web pages) to users’ queries. However, unlike databases, the documents are usually ranked and may not exactly match the query. While the most common application of an information retrieval system is web search, their power and flexibility make them useful for many other products. Earlybird , our Lucene -based search indexing technology, has been rock-solid for years. We index new Tweets within seconds and have a large archival index of all public Tweets . These systems can handle upwards of 60K queries per second while simultaneously indexing up to 80K Tweets per second. We maintain a service level agreement (SLA) of 99.98% uptime and (for most queries) our latencies are under 100ms. However, Earlybird was primarily built to serve Twitter search and trends landing pages, so it lacks some of the flexibility we need for building the next generation of Twitter’s products. The new products we want to build will require fields, operators, and ranking signals to be added to our indexes. For example, to build a new media product we might want a new operator to find Tweets with GIFs, or to use content as a ranking signal. When we started Omnisearch, this was difficult or impossible. Our in-memory Earlybird indexes were already using most of their available memory, and our archival indexes required a slow rebuild before new fields and signals became available. After several months of work, these hard limitations have been removed. When we’re done with Omnisearch, product engineering teams will be able to do this work on their own. Different products also have different reliability and scaling requirements. This is challenging in both directions. For small applications, a technology that can serve 100K QPS may be overly complex. On the other hand, we’re nearing the scaling limits of the current architecture: we can scale what we have by 2x but not by 10x. Twitter’s Home timeline requires higher uptime than some of our other products because when it’s down, Twitter is down. Ultimately, we’d like Omnisearch to scale by an order of magnitude beyond our current systems in several dimensions: success rate, indexing latency, query latency, indexing throughput, and query throughput. In the next phase of development for Omnisearch, we will be tackling these challenges via a series of architectural projects. Finally, new products may require completely new indexes. For example, we may want to build indexes of Moments, Vines, and Periscope broadcasts. Currently, the core search infrastructure team only maintains indexes of Tweets and users. While bringing up new Earlybird indexes for other types of documents is possible, it requires custom work and the engineering cost often outweighs the value. Our ultimate vision for Omnisearch is to provide search as a service, allowing us to build entirely new kinds of products. Over the coming months, our search infrastructure team will be posting a series of blog posts detailing the transformation of our existing infrastructure into Omnisearch. Stay tuned!", "date": "2016-05-05"},
{"website": "Twitter-Engineering", "title": "The release of Pants 1.0", "author": ["‎@digwanderlust‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2016/the-release-of-pants-10.html", "abstract": "Today, Twitter is excited to announce participation in the first major release of the Pants open source project: 1.0.0 , an open source build tool for monorepo-style source repositories. After more than five years of development, this significant milestone has greatly improved the project, and we invite you to test out this new and improved experience. Pants 1.0.0 provides a better user experience with: Easier installation , allowing Pants to be setup with an empty pants.ini file Stable public APIs and options with a clear deprecation policy Regular, vetted, stable releases from release branches Pants at Twitter Before 2010, Twitter experienced a growing codebase that existing open source tools like ant and maven were unable to scale up to. In 2010, former Twitter engineer John Sirois began to develop Pants in order to address those scalability problems. Pants was open-sourced in 2012 under the Apache 2.0 license. Currently, Twitter uses Pants in its internal monorepo for building and testing internal code. Twitter remains committed to developing Pants and will continue to work with the community to improve performance, enhance support for mobile development, and solidify Pants as a best-in-class build tool. In addition to having a distributed cache, great subtarget incremental build performance, and support for a diverse set of languages, Pants also integrates well with Twitter developers’ workflow. Many developers at Twitter use IntelliJ and the IntelliJ Pants plugin imports Pants projects in a way IntelliJ understands and also provides compile/test assistance based on IntelliJ’s understanding of the project structure. Why we use Pants There are several monorepo-style tools available today, including Buck, Bazel, and more. When Pants was first developed, these tools were still closed source. Because there were no other tools available that fit Twitter’s needs, we wrote our own. Now, a vibrant open source community has grown around Pants, with more than 100 contributors . Pants has since added support for several languages including Scala, Java, Python, JavaScript, Go, C/C++, Thrift, protobuf and more, many of which are used at Twitter. Additionally, there is added support for a rich plugin API and fast, reproducible builds. With this 1.0.0 release, we are stabilizing the plugin API, enabling caching by default, and removing some unnecessary configuration to make setup easier. Today, Pants is an excellent choice for a monorepo-style build tool. We have a great open source community with an active Slack channel and mailing list, that includes participants across several companies. Pants also scales favorably when compared to other tools for larger targets because of its support for partial-target incremental builds for the JVM, which can make Pants significantly easier to adopt when compared to tools with strict requirements on target size. The development continues We’re dedicated to continually enhancing the Pants user experience, performance and build reproducibility. To learn more and get involved, join the conversation . Pants documentation: http://www.pantsbuild.org/ Github repo: https://github.com/pantsbuild/pants", "date": "2016-05-02"},
{"website": "Twitter-Engineering", "title": "Overview of the Twitter Cloud Platform: Compute", "author": ["‎@micheal‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2016/overview-of-the-twitter-cloud-platform-compute.html", "abstract": "On March 31, we hosted the first ever #compute event at our headquarters in San Francisco. Over 150 people attended the event and heard our engineers talk about the present and future of how we build and operate the compute infrastructure as part of the Twitter Cloud Platform. We also gained insights from many in the community on the challenges they face and how they are addressing similar problems. Overall, the #compute event served as a great forum for all us to connect, learn, and share ideas. With that, we are excited to announce the availability of all the technical talks from the event on the Twitter University YouTube channel . About the Twitter Cloud Platform: Compute Twitter Cloud Platform: Compute powers over 95% of all stateless services at Twitter. It is built atop of open source technologies including Apache Mesos , Apache Aurora , and a suite of internal services that address both operator and user needs. The platform has grown from managing a few hundred containers to over 100,000 containers across tens of thousands of hosts. It has not only become a critical piece of Twitter’s underlying infrastructure, but our experience in building and operating it are contributing to the future of cloud infrastructure management in the industry. Our vision for the Twitter Cloud Platform: Compute is around the following tenets: Reliability : provide an always-available system with strong resource and performance guarantees Developer agility : make it simple for developers to build, deploy, manage, and scale their services Efficiency : run a well-utilized and cost-effective platform Scalability : accommodate growing business needs without sacrificing reliability, developer agility, and efficiency Given Twitter’s growing and diverse computing needs, we are building the next generation compute infrastructure that leverages both private and public clouds in a way that is reliable, scalable, developer-friendly, and cost-effective. The #compute event https://twitter.com/TwitterEng/status/710609684890189825 We had six technical talks, each focussed on specific challenges from the bottom-up in building and operating the Twitter Cloud Platform: Compute. The first two talks by Eric Danielson and David Robinson are operator-focused and deep dive on how we provision and manage servers and how a small team of SREs perform operational procedures on a large scale compute cluster. These serve as the building blocks of the platform and has an impact on two of our core tenets, reliability and scalability. Server provisioning and management at scale Tech lead of provisioning engineering, Eric Danielson talks about two specific systems: Audubon - Machine database and Wilson API/Lifecycle Manager - Machine Lifecycle Manager that are used to provision, track, and manage a fleet of tens of thousands of hosts at Twitter. Managing a large scale compute platform David Robinson , site reliability engineer on Compute, talks about how they leverage Audubon, Wilson, and other tooling to manage (configuration, deployment, and other operational procedures) across tens of thousands of hosts in the Compute cluster. He also shares the challenges when operating at this scale and things that can potentially go wrong that impact the reliability of the cluster. Chargeback for multi-tenant infrastructure systems Software engineers on Cloud Infrastructure Management, Vinu Charanya and Jessica Yuen , talk about how they built a generic system that helps define chargeable resources, collects utilization metrics, resolves owners, and generates billing and utilization reports to improve resource utilization and cost effectiveness of every multi-tenant infrastructure service at Twitter. Aurora Workflows One of the biggest cognitive overheads for a developer is to shepherd a deployment from development to production. Software engineer David Mclaughlin from the Compute team talks about our internal project called “Workflows” which aims to reduce this overhead and become a key building block for continuous deployment automation in the future. Twitter Heron on the Aurora Heron is the next generation real-time streaming analytics platform used heavily in Twitter. Heron runs diverse real-time analytics applications ranging from counting to real-time machine learning. Heron tech lead Maosong Fu talks about how Heron leverages Aurora extensively to run its topologies. AWS + Aurora/Mesos In 2014, TellApart (now part of Twitter) adopted Mesos/Aurora as the solution to run their infrastructure in AWS . Engineering Manager David Hagar shares the team’s experiences, the problems encountered, and how they addressed them. He also gives a glimpse into the work to extend the capabilities of the Compute infrastructure across private and public clouds. Acknowledgements We’d like to thank vice president of platform engineering Chris Pinkham , all the speakers, attendees, and folks who worked behind the scenes to make the #compute event possible. A special note of acknowledgement to Megan Carlisle , Holly Dyche , J.J. Jeyappragash , Ian Brown , Ian Downes , Derek Lyon , and Karthik Ramasamy . We look forward to hosting the next one! All photos courtesy Twitter, Inc.", "date": "2016-04-28"},
{"website": "Twitter-Engineering", "title": "Resilient ad serving at Twitter-scale", "author": ["‎@iamsridhar‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2016/resilient-ad-serving-at-twitter-scale.html", "abstract": "Introduction Popular events, breaking news, and other happenings around the world drive hundreds of millions of visitors to Twitter, and they generate a huge amount of traffic, often in an unpredictable manner. Advertisers seize these opportunities and react quickly to reach their target audience in real time, resulting in demand surges in the marketplace. In the midst of such variability, Twitter’s ad server — our revenue engine — performs ad matching, scoring, and serving at an immense scale. The goal for our ads serving system is to serve queries at Twitter-scale without buckling under load spikes, find the best possible ad for every query, and utilize our resources optimally at all times. Let’s discuss one of the techniques we use to achieve our goal: Operate a highly available service (four-nines) at Twitter-scale query loads (be resilient, and degrade gracefully with increase in QPS or demand.) Serve the highest quality ad possible, for every query, given current resource constraints. Use resources optimally. We would like to provision such that we are at a high level of average CPU utilization while sustaining business continuity in the event of a datacenter failure (Disaster Recovery, or ‘DR’, compliance). A brief overview of the ad serving pipeline A brief introduction to the ad serving pipeline (henceforth called serving pipeline) is in order before discussing the technique in detail. The serving pipeline can be visualized as a funnel with the following stages: Selection The selection phase sifts through the entire corpus of potential ad candidates for a query and comes up with a set of matches whose targeting criteria match the user in question. Selection may result in as little as a few tens to as many as several thousands of candidates being selected. All these candidates are eligible participants in subsequent stages. The number of ad candidates selected for a particular query is essentially the result of a match between user attributes and targeting criteria across the corpus of ads in the index. Hence, not all ads are relevant for all users. Engagement rate prediction The engagement rate for an ad is defined as the ratio of the number of engagements (e.g., click, follow, Retweet) on an ad impression to the total number of impressions served. Engagement rate is a critical predictor that determines the relevancy of an ad for a particular user (this score can be used to answer the question, “How likely is user U to engage with ad A?”). This rate changes in real time, and is evaluated by machine-learned models based on a number of user and advertiser features. Evaluating the engagement rate is one of the most computationally expensive phases of the serving pipeline, and therefore, we run multiple rounds of this predictor to progressively thin down the number of candidates that are ultimately run through the auction. We first run a light version of the predictor over the entire set of selected auction candidates. We use this to limit the number of candidates to some top- k (order of hundreds) that we then run through the full auction. Since the best ad can be found by running all the selected candidates through the auction, it stands to reason that by decreasing the value of k , we make the auction cheaper in terms of both CPU utilization and latency, but at the same time find a slightly lower-quality ad. Therefore, k serves as a knob that can be used to tradeoff latency against ad quality. Auction Typically, a standard second-price auction is run for every request on the expected cost per impression (computed as bid times the engagement rate ). Additional rules and logic apply if the bidding happens on our ad exchange, Mopub marketplace . Ad server latencies and success rate Queries hitting the ad server are not all the same in terms of how valuable they are; some queries are more monetizable than others, thereby making the cost of a failed query variable. Requests also have high variance in compute, depending upon the ad match. We observe two strong correlations: Revenue per request correlates with the number of candidates in the auction Query latency correlates with the number of candidates in the auction High latency requests — the ones that influence success rate — therefore contribute disproportionately to revenue. Simply put, the more work we expend for a query, the more revenue we stand to make. Hence it follows that timing-out the higher latency requests has a disproportionately negative impact on revenue. We can conclude from the above observation that it is very important to maintain a high success rate. Using k to scale the ad server As you will recall, k is a knob that can be used to control CPU utilization and latency. This provides us with an interesting insight — we could simply use k as a means to scale the ad server, as long as we have a good way to pick the right value for k for every query. One strategy to pick k is to predictively determine its value based on a set of observable features for every query (e.g., current load, available CPU, current success rate, user features, etc.). While this approach is promising, it is expensive and hard to model precisely. Our model(s) for predicting k would have to be complex to react quickly to external parameters (e.g., load spikes), and such prediction itself can prove to be computationally expensive. Another strategy is to continually learn the value for k . To do this, we should pick a metric to center the system around that’s both fundamentally important to the system as well as influenced directly by this knob, k . Since we know that k directly influences latency, an adaptive learning strategy that learns k by tracking success rate is a viable approach. We build this adaptive learner into our ad server, which essentially functions as a control system that learns k . For quick reference, a basic controller (see figure below) keeps a system at a desired set point (expectation) by continuously calculating the deviation of the process output against the set point through a feedback loop, and minimizes this error by the use of a control variable. Mapping this to the ad server’s goal of operating at the right k value, we fix our set-point to the target success rate we desire (say, 99.9%), and build a controller that constantly tracks towards this success-rate by adjusting k . QF: adaptive quality factor The controller we build outputs a control variable called quality factor ( q ), which performs the function of keeping the success rate (SR) at the expectation. q varies by following a few simple rules. q reduces by some δ for every failure, and increases by f (δ) for every successful query. For example, if the target SR is to be kept at, 99.9%, every failure will reduce q by δ, and every successful query will increase q by δ/999. Hence, if the system stays at the target success rate of 99.9%, the variation of q within the span of 1,000 requests would be negligible. δ and f will determine the rate at which q will adapt, and can be set with proper tuning. Any target success rate can be translated into a target latency requirement since latency and success rate are inversely related. If our target SR translates to a target latency requirement of T, we can say that in effect, q stays constant as long as we are at the target latency. It increases when the latencies are lower than T, and decreases when the latencies exceed T. When the ad server starts, it picks some default value for q which then adjusts to a level commensurate with the current load and latency profile by following the aforementioned rules. How do we use q? With q defined as above, we select the top q*k candidates after our light prediction stage instead of k , q converges when query latencies are around T. During times of high QPS or failover, q automatically adapts down, thereby reducing the number of candidates entering the full auction and reducing the load on a computationally expensive step (while suffering a loss in ad quality). This consequently reduces query latency, and keeps our success rate to upstream clients on target. Importantly, during regular operation, we can operate at a q value of greater than 1.0 with current provisioning (since we provision for DR, and have extra capacity available most of the time). The figure below shows how q adapts to variation in load (both are normalized by some factor to show the interplay more clearly). During times of low qps, q trends up, thereby effectively increasing the amount of work done per query, and when qps peaks, q trends down, reducing the work done per query. This has the effect of using our CPU optimally at all times. Another interesting aspect of this design is that each ad server instance maintains its own view of an optimal q , thereby ensuring that we have resiliency at a local, per-instance level, requiring no central coordination. In practice, the ad server uses several tunable parameters to control the performance characteristics of various parts of the system. The k we saw before (candidates after light prediction) is only one such knob. We can now use q as a parameter to tune each of these other parameters further, thereby achieving efficiencies across the whole of the ad server. You might recollect that at the beginning of this blog, we stated that our goal was around effectively utilizing CPU, but our technique of using the quality factor tried to achieve this goal by ultimately controlling latency. In order for this to improve CPU utilization, we would have to first ensure that the ad server is CPU bound, and not latency bound. We achieve this by making all operations asynchronous, reducing lock contention, etc. How does q help with provisioning? The typical approach to provisioning is to allocate resources at a level such that comfortably allows for temporary spikes in load and maintain business continuity during failovers (disaster recovery). It is easy to see why this is wasteful, since we end up underutilizing resources during the normal course of operation. Ideally, we would like for our utilization to always be close to our provisioning, while still being able to absorb load spikes (as shown in the green line in the curve below): Quality factor helps us understand and maintain optimal provisioning levels. With experimentation, we are able to measure the impact of varying q on key performance indicators such as RPMq*, and also on the impact on downstream services (during query execution, the ad server calls out to several downstream components such as user-data services and other key-value stores. The impact on these downstream components should, therefore, be taken into account for any provisioning changes in the ad server). Thus, we’re able to increase or decrease our provisioning levels based on desired operating points in our system. By directly controlling utilization, q allows us to use our provisioning optimally at all levels. This benefit, however, does not come without cost. As alluded to before, we trade off ad quality for this ability to always optimally utilize our resources. Since q basically tracks ad quality, we see a temporary dip in ad quality during periods of high load. We see in practice that this is a very fair tradeoff to make. Since q semantically represents the revenue made per unit core time, it also serves as a ready metric for us to get a sense of current performance from a revenue (or more precisely, an RPMq) standpoint. This graph shows the strong correlation: Wrapping up The technique we’ve outlined uses concepts from control theory to craft a control variable called quality factor , which is then used by the ad server in achieving the stated goals around resiliency (availability), scalability, resource-utilization, and revenue-optimality. Quality factor has benefited our ad serving system enormously, and is a critical metric that is now used to tune several parameters across the ad server besides the auction depth. It also allows us to evaluate the cost of incremental capacity increases against the revenue gains they drive. The ads serving team at Twitter takes on challenges posed by such enormous scale on a continual basis. If building such world-class systems excites you, we invite you to join the flock ! Acknowledgements Ads Serving Team: Sridhar Iyer , Rohith Menon , Ken Kawamoto , Gopal Rajpurohit , Venu Kasturi , Pankaj Gupta , Roman Chen , Jun Erh , James Gao , Sandy Strong , Brian Weber . Parag Agrawal was instrumental in conceiving and designing the adaptive quality factor. *RPMq = Revenue per thousand queries.", "date": "2016-03-30"},
{"website": "Twitter-Engineering", "title": "Observability at Twitter: technical overview, part II", "author": ["‎@anthonyjasta‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2016/observability-at-twitter-technical-overview-part-ii.html", "abstract": "This is the second post of two part series on observability engineering at Twitter. In this post, we discuss visualization, alerting, distributed tracing systems, log aggregation/analytics platform, utilization, and lessons learned. Visualization While collecting and storing the data is important, it is of no use to our engineers unless it is visualized in a way that can immediately tell a relevant story. Engineers use the CQL query language to plot time series data on charts inside a browser. A chart is the most basic, fundamental visualization unit in observability products. Charts are often embedded and organized into dashboards, but can also be created ad hoc in order to quickly share information while performing a deploy or diagnosing an incident. Also available to engineers are a command line tool for dashboard creation, libraries of reusable monitoring components, and an API for automation. We improved the user’s cognitive model of monitoring data by unifying visualization and alerting configurations. Alerts, described in the next section, are simply predicates applied to the same time series data used for visualization and diagnosis. This makes it easier for the engineers to reason about the state of their service because all the related data is in one place. Dashboards and charts are equipped with many tools to help engineers drill down into their metrics. They can change the arrangement and presentation of their data with stack and fill options, they can toggle between linear and logarithmic chart scales, they can select different time granularities (per-minute, per-hour, or per-day). Additionally, engineers can choose to view live, near real-time data as it comes into the pipeline or dive back into historical data. When strolling through the offices, it’s common to see these dashboards on big screens or an engineer’s monitor. Engineers at Twitter live in these dashboards! Visualization use cases include hundreds of charts per dashboard and thousands of data points per chart. To meet the required browser chart performance, an in-house charting library was developed. Alerting Our alerting system tells our engineers when their service is degraded or broken. To use alerting, the engineer sets conditions on their metrics and we notify them when those conditions are met. The alerting system can handle over 25,000 alerts, evaluated minutely. Alert evaluation is partitioned across multiple boxes for scalability and redundancy with failover in the case of node failure. While our legacy system has served Twitter well, we have migrated to a new distributed alerting system that has additional benefits: Inter-data center alert failover in the case of zone failures Alert evaluation catchup in the case of node failures Alert execution isolation so one bad alert won’t take down others Non-impacting deployments so users don’t lose visibility Unified object model for alerting and visualization configurations The visualization service allows engineers to interact with the alerting system and provides a UI for actions such as viewing alert state, referencing runbook, and snoozing alerts. Dynamic configuration As our system becomes more complex, we need a lightweight mechanism to deploy configuration changes to a large number of servers so that we can iterate quickly as part of the development process without restarting the service. Our dynamic configuration library provides a standard way of deploying and updating configuration for both Mesos/Aurora services and services deployed on dedicated machines. The library uses Zookeeper as a source of truth for the configuration. We use a command line tool to parse the configuration files and update the configuration data in Zookeeper. Services relying on this data receive a notification of the changes within a few seconds: Distributed tracing system (Zipkin) Because of the limited number of engineers on the team, we wanted to tap into the growing Zipkin open source community, which has been working on the OSS Twitter Zipkin , to accelerate our development velocity. As a result, the observability team decided to completely open source Zipkin through the Open Zipkin project . We have since worked with the open source community to establish governance and infrastructure models to ensure change is regularly reviewed, merged and released. These models have proven to work well: 380 pull requests have been merged into 70 community-driven releases in 8 months. All documentation and communication originates from the Open Zipkin community. Going forward, Twitter will deploy zipkin builds directly from the Open Zipkin project into our production environments. Log aggregation/analytics platform LogLens is a service that provides indexing, search, visualization, and analytics of service logs. It was motivated by two specific gaps in developer experience when running services on Aurora/Mesos. The coupling between the lifetime of service logs and the lifetime of the transient resource containers the task was scheduled on caused a lot of uncertainty in our ability to triage recent incidents because of lost logs. The difficulty in quickly searching through all of the distinct logs generated by the many components that comprised a service increased the response time for live incidents. The LogLens service was designed around the following prioritizations — ease of onboarding, prioritizing availability of “live” logs over cost, prioritizing cost over availability for older logs, and the ability to operate the service reliably with limited developer investment. Customers can onboard their services through a self-service portal that provisions an index for their service logs with reserved capacity and burst headroom. Logs are retained on HDFS for 7 days and a cache tier serves the last 24 hours of logs in real time and older logs on demand. Utilization As Twitter and observability grow, service owners want visibility into their usage of our platform. We track all the read and write requests to Cuckoo, and use them to calculate a simple utilization metric, defined as the read/write ratio. This tracking data is also useful for our growth projection and capacity planning. Our data pipeline aggregates event data on a daily basis, and we store the output in both HDFS and Vertica. Our users can access the data in three different ways. First, we send out periodic utilization and usage reports to individual teams. Second, users can visualize the Vertica data with Tableau, allowing them to do deep analysis of the data. Finally, we also provide our users with a Utilization API with detailed actionable suggestions. This API, beyond just showing the basic utilization and usage numbers, is also designed to help users drill down into which specific groups of metrics are not used. Since this initiative came into play, these tools have allowed users to close the gap between their reads and writes in two ways: by simply reducing the number of unused metrics they write, or by replacing individual metrics with aggregate metrics. As a result, some teams have been able to reduce their metric footprint by an order of magnitude. Lessons learned “Pull” vs “push” in metrics collection: At the time of our previous blog post , all our metrics were collected by “pulling” from our collection agents. We discovered two main issues: There is no easy way to differentiate service failures from collection agent failures. Service response time out and missed collection request are both manifested as empty time series. There is a lack of service quality insulation in our collection pipeline. It is very difficult to set an optimal collection time out for various services. A long collection time from one single service can cause a delay for other services that share the same collection agent. In light of these issues, we switched our collection model from “pull” to “push” and increased our service isolation. Our collection agent on each host only collects metrics from services running on that specific host. Additionally, each collection agent sends separate collection status tracking metrics in addition to the metrics emitted by the services. We have seen a significant improvement in collection reliability with these changes. However, as we moved to self service push model, it becomes harder to project the request growth. In order to solve this problem, we plan to implement service quota to address unpredictable/unbounded growth. Fault tolerance: As one of the most critical services at Twitter, we bear the responsibility of providing high available observability services even in the event of catastrophic failures, such as a complete DC outage. In order to achieve that, we followed two principles Cross-DC redundancy : Some of our most critical metrics are sent to more than one DC for redundancy. This makes us resistant to a single DC failure. Eliminate/decouple unnecessary dependencies on other libraries/services : In some cases of our development, we intentionally remove dependencies on some widely used internal infrastructures, such as the Twitter Front End, TFE, to avoid downtime event caused by failures of those systems. In other cases, we use dedicated clusters and instances, like Manhattan and ZooKeeper, to decouple our failure from that of the services we monitor. Learn more Want to know more about some of the challenges faced building Twitter’s observability stack? check out the following: Twitter Flight 2015 talk by Caitie McCaffrey Acknowledgements Observability Engineering team: Anthony Asta , Jonathan Cao , Hao Huang , Megan Kanne , Caitie McCaffrey , Mike Moreno , Sundaram Narayanan , Justin Nguyen , Aras Saulys , Dan Sotolongo , Ning Wang , Si Wang", "date": "2016-03-22"},
{"website": "Twitter-Engineering", "title": "Observability at Twitter: technical overview, part I", "author": ["‎@anthonyjasta‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2016/observability-at-twitter-technical-overview-part-i.html", "abstract": "The Observability Engineering team at Twitter provides full-stack libraries and multiple services to our internal engineering teams to monitor service health, alert on issues, support root cause investigation by providing distributed systems call traces, and support diagnosis by creating a searchable index of aggregated application/system logs. These are the four pillars of the Observability Engineering team’s charter: Monitoring Alerting/visualization Distributed systems tracing infrastructure Log aggregation/analytics We fulfill this charter for all applications and services running in our owned and operated data center as well as acquired properties that are deployed in external public clouds, such as Amazon AWS and Google Compute. Twitter services handle a large number of tweets every day. Responsible for providing visibility into the services powering Twitter, our observability services are some of the largest scale internal systems. The request volume and data size on disk have grown tremendously since we last discussed this in a blog post two years ago. Today, our time series metric ingestion service handles more than 2.8 billion write requests per minute, stores 4.5 petabytes of time series data, and handles 25,000 query requests per minute. This blog is the first in a two-part series. It covers our architecture, metrics ingestion, time series database, and indexing services. Architecture Our system architecture has also changed quite a bit over the past two years . Here is our current architecture: Metric ingestion Metrics are pushed into the observability infrastructure in three ways. Most Twitter services run a Python collection agent to push metrics to the Time Series Database & HDFS. The Observability team also supports a slightly modified version of a common StatsD server, Statsite , that sends metrics to our main time series metrics ingestion service, Cuckoo-Write, or other ingestion services of choice like Carbon. Finally an HTTP API is provided for scenarios where ease of ingestion is more important than performance. With this breadth of options we are able to support a wide variety of customers from Twitter servers running in Twitter data-centers to acquired companies running in external data-centers like AWS that want to take advantage of the centralized observability stack. Time series database All Twitter metrics are sent and stored in Twitter’s time series database, Cuckoo. Cuckoo is actually two services, Cuckoo-Read and Cuckoo-Write, and backed by Twitter’s distributed key-value storage system, Manhattan . Initially Cuckoo-Read and Cuckoo-Write were one service, but they were split into two due to the different nature of the read and write workload, so that each service could be tuned for their specific task. Cuckoo-Write is the ingestion point for metrics, and exposes an API to which metrics are written. In addition to storing these metrics in Manhattan, Cuckoo-Write also ensures the metrics are sent to the appropriate services for indexing. Data is stored at minutely granularity for two weeks, and at hourly granularity forever. The time series database query engine, Cuckoo-Read, handles time series data queries from our alerting and dashboarding systems as well as user initiated ad-hoc query traffic. Queries are specified in the Cuckoo query language, CQL. CQL query engine Given the scale of our data set, serving all queries with low latency and high availability is technically very challenging. More than 36 million queries are executed every day in real time, and our engineers rely on these queries and the monitoring system to meet their service SLA. CQL queries are supported natively in Cuckoo-Read. The query engine is composed of three components: parser, rewriter, and executor. The parser is responsible for parsing query strings into internal Abstract Syntax Trees (ASTs); the rewriter then processes AST nodes and replaces some of them with simpler calculations to improve performance; finally the executor fetches data from downstream services and calculates the output. In CQL, time series are uniquely identified by tuple of service, source, and metric. This structure not only maps directly to how services are organized but also allows us to simplify data partitioning in storage. Time-based aggregation Our time series database supports data aggregation based on service host group or time granularity of hour and day. In the past, we used Manhattan counters to accomplish time-based aggregation. We observed two common access patterns in hourly/daily data that helped us redesign our aggregation pipeline. Most notably two are: Data access typically happens after the roll of the hour/day boundary (e.g., most hourly data reads happened after 10pm for data collected between 9pm to 10pm). Latency requirement for hourly data is much lower than that for minutely data. Users usually have higher tolerance for hourly data delay. Given these observations, we made a few design choices for the new aggregation pipeline, deciding to: Replace relatively expensive Manhattan-based counter aggregation with low-cost high latency Hadoop batch process pipeline Utilize high-density storage (spindle disk vs SSD) for aggregated results Synthesize hourly/daily data from minutely data for infrequent high recency requests before data is available from Hadoop pipeline With the new pipeline, we achieved substantial efficiency gain in lower hardware cost. Furthermore, we improved our system reliability by reducing the load on our time series database. Temporal set indexing service Temporal set indexing service, Nuthatch, keeps track of metrics metadata and maintains a map of metric keys to members sets (e.g., the map from host group to individual host) as well as timestamps. The browsing tool below shows one use case of the data. Engineers can quickly navigate the services, sources, and metrics available in services. More importantly, Nuthatch provides membership resolution and glob query support for CQL query engine so that engineers can query membership for a given key and then use functions, like sum() and avg(), to aggregate all time series together. In the following query example, Nuthatch is responsible for identifying all the sources for nuthatch.router service as well as the metric keys in the scope of “/update/request/” and providing the data for CQL query engine to fetch specific time series data set from storage. The challenges for the Nuthatch service come from the huge amount of the incoming metric sets to be indexed and the members requests from CQL query engine. A general purpose indexing or caching engine would not work well in this scenario because the temporal set data has a few unique properties: Time series data writes generate a very high volume of member set updates. The number of requests is too high to be handled by Manhattan storage system effectively. Almost all of the temporal sets are updated frequently, which makes caching based on most recent updates less efficient. Most members in a member set remain relatively stable even though some members may join and leave the set at high frequency. Nuthatch uses an in-process cache to reduce the number of storage operations in order to improve performance and reduce cost. In this design, a stateless router service is responsible for accepting incoming requests and deciding which worker shards the requests should be forwarded to using consistent hashing. A set of dedicated worker shards, each with its own in-memory cache, handles requests by reading from either cache or Manhattan storage. Acknowledgements Observability Engineering team: Anthony Asta , Jonathan Cao , Hao Huang , Megan Kanne , Caitie McCaffrey , Mike Moreno , Sundaram Narayanan , Justin Nguyen , Aras Saulys , Dan Sotolongo , Ning Wang , Si Wang", "date": "2016-03-18"},
{"website": "Twitter-Engineering", "title": "Strong consistency in Manhattan", "author": ["‎@twalex‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2016/strong-consistency-in-manhattan.html", "abstract": "Introduction To learn more about Manhattan, Twitter’s distributed storage system, a great place to start is our first blog post . While we’ll cover the basics of Manhattan, it’s recommended that you read the post first to understand the bigger picture. Manhattan is a general-purpose distributed key-value storage system that’s designed for small and medium-sized objects and fast response time. It’s one of the primary data stores at Twitter, serving Tweets, Direct Messages, and advertisements, among others. The primary goals behind building Manhattan were achieving predictability of runtime behavior, stability, and operational simplicity. There are two main parts to Manhattan: A stateless coordination layer, responsible for accepting requests and routing them. We call each instance of this layer a coordinator . A stateful data persistence layer. Each instance is referred to as a storage node . Coordinators route requests to storage nodes and wait for responses. They do this directly by sending messages to storage nodes, or indirectly by placing request messages in replicated logs and waiting for a callback from any consumer of those logs. Each key that our customers write gets stored several times for redundancy, and there’s a well-known set of storage nodes where a given piece of data lives. Performance of a storage node is largely dependent on the storage engine used. We have support for several storage engines to fit a variety of use cases. Each key belongs to a single shard. A shard is responsible for a part of the keyspace. Under normal circumstances, a key always resolves to the same shard, regardless of where that shard is physically placed. We use a topology management service to identify physical locations of each shard. A storage node is typically responsible for hundreds or thousands of shards. Besides the main components, there is a large ecosystem of supporting services around Manhattan: topology management service, self-service web portal, log management service, metadata service, and many more. Eventual Consistency Let’s start with a summary of what eventual consistency is and what are its limitations. Eventually consistent systems in the style of Amazon Dynamo are designed to be fast and highly available during failures, including node failures and network partitions. We’re guaranteed that in the absence of any changes, all copies of data in a distributed database will converge to the same state at some future point. However, this puts the burden of handling temporary inconsistencies on the application logic. Here are some examples of guarantees that we cannot provide with eventual consistency: Key uniqueness guarantees . If we can’t guarantee uniqueness of, for example, email addresses registered with Twitter accounts, one email address may get attached to two accounts. This violates our business logic and we need to ensure this cannot happen. With eventual consistency, two clients operating in parallel may see that a given key doesn’t exist and request to insert it with a different value at the same time. If they operate concurrently, they can both get an acknowledgement of a successful insert. Check-and-set. A more general case of the prior point, if check-and-set (CAS) in an eventually consistent system is done by reading and then writing, the order of updates between competing writers that contend on the same key may not be the same on all storage nodes. This means that there will be a temporary disagreement between storage nodes, even though both writes would be acknowledged as successful. A possible outcome is demonstrated in the diagram below, where both clients issue writes after learning that A is equal to 5, but the order of updates is intertwined. One can imagine these sorts of inconsistencies accumulating if a client application needs to implement a frequently updated state machine or a counter. All-or-nothing updates. Let’s say the client uses our default consistency level and requires a quorum of updates to succeed. This means if there are three storage nodes then two out of the three storage nodes have to acknowledge the update as successful in order for the write to be successful. If there’s a network partition and the coordinator’s write request only succeeds on one out of three storage nodes of a given key, the client will receive a response indicating partial success. The one storage node that succeeded will hold on to the key, which means that whenever it is consulted for that key (as a part of quorum reads, for instance), it will return it. The future data repair mechanisms will copy that key to other storage nodes. Read-your-write consistency (causality) . Low response time is necessary for most use cases. To accomplish it, we chose to only wait for the writes to succeed in the zone that accepts the write, while replication to the other zones and data centers happens asynchronously. However, as a result, readers in other zones and data centers are not guaranteed to see the results of prior writes. This can result in an unintuitive behavior for the client (i.e., getting authenticated by token in one data center and being unable to use that token when trying to make requests to another data center). Therefore, while eventually consistent systems have their place in data storage, they don’t cover all of the needs of our customers. Consistency in Manhattan To address these requirements and shortcomings, we added stronger consistency guarantees into Manhattan. Our goal was to provide ways to treat keys as single units, no matter how they are distributed geographically. Next, we’ll describe how that works under the hood. The strongest consistency model that we now support is the sequential ordering of per-object updates. This means that clients can issue writes to individual keys that will take place atomically, and all subsequent reads (globally) will observe the previously written or newer versions of the key, in the same order. For example, suppose the writer writes “X=1” and then “X=2”. If a reader reads X twice, it will receive either “1, 1”, “2, 2” or “1, 2”, but never “2, 1”. In this post, we’ll refer to this concept as “strong consistency” for simplicity. Not all data needs to be strongly consistent. To understand how consistency applies to some keys and not others, it’s important to understand Manhattan datasets. A dataset is analogous to a RDBMS table: it’s a group of keys that belong to a single use case, such as “staging data for Moments Service” or “production data for notification history”. Datasets provide logical separation of the data and allow services to coexist in a shared environment. The consistency model is configurable per dataset, which means an application can use multiple consistency models if it uses more than one dataset. As low latency and strong consistency are trade-offs, many of our customers inside the company prefer the flexibility to mix-and-match consistency types. Another useful property is that the reads against strongly consistent datasets can be either eventually consistent or strongly consistent: In terms of latency numbers, the 99th percentile is typically low tens of milliseconds for local strongly consistent operations (in one data center) and low hundreds of milliseconds for global operations (all data centers are included). The higher latency for global operations is due to latencies traversing large geographical distances. Median latencies are much lower. Architecture To explain how keys are updated, we will first explain the key structure. Our keys are hierarchical, and under one top-level key (we call it a “partitioning key”) there can be many sub-keys (we call them “local keys”): Therefore, a dataset may look like this: When we talk about atomically updating a single key, we’re actually talking about a partitioning key. In the example above, we can atomically update all of the keys under 437698567. We considered two options for the scope of our strong consistency support. We could have done either full distributed transactions (where operations can span any number of keys in our system) or addressed a simpler scenario (where strong consistency applies per key). After talking to our internal customers and reviewing the use cases, we decided that starting with the latter model satisfied a majority of use cases. Because each key belongs to a single shard, we don’t have to pay a performance penalty of coordinating a multi-key distributed transaction. In our design, to provide strong order of updates for each key, all strongly consistent operations go through a per-shard log. A typical system has tens of thousands of shards and a large number of logs (we support multiplexing multiple shards on the same log). Each shard is independent from others, so when Twitter engineers design their applications, they must choose their keys in a way that strongly consistent operations are confined to individual partitioning keys and don’t span multiple keys. We rely on DistributedLog , a replicated log service built by Twitter. Manhattan coordinators map keys to shards, create messages that represent requested operations and submit them to per-shard logs. Each such message is a record in the log. Each storage node shares responsibility for a given shard and subscribes to that shard’s log, as shown in the diagram below: Each storage node subscribes to many logs. This allows us to achieve a high degree of parallelism and makes the unit of failure smaller. Each storage node consumes its logs in parallel. Within each log, however, all operations have to be applied sequentially to preserve the order. Effectively, all storage nodes maintain a state machine per shard, and after applying N operations, each shard’s state machine will be in the exact same state on all storage nodes. In DistributedLog, each log has an elected single writer that accepts and sequences writes. This allows us to establish consensus on the order of operations. Operations on any two distinct keys are likely going to go to different logs and therefore have different writers, which mitigates the global impact from a single writer failure. There’s a failover mechanism based on ZooKeeper for when a writer fails, — for example, during network partitions, hardware failures, or planned maintenances. Workflow Every client request has to go through a Manhattan coordinator. A coordinator will group keys in each request by shard, and write the messages corresponding to per-shard operations into corresponding logs (e.g., “read key A” for shard 2, “check-and-set key B from 5 to 9” for shard 8). The coordinator also places its own callback address into the message to inform storage nodes about where to respond. The responses are necessary to provide results for operations like check-and-set, increment and read. For strongly consistent writes, we only need to ensure that the operation is written to the log. Next, storage nodes subscribing to particular logs will consume these messages, execute the operations one at a time, and respond back to the coordinator. When consuming logs, storage nodes always keep the position of the current operation on the log. They also have to atomically write that position to disk storage with the results of the corresponding operation. Otherwise, consistency could be violated during crashes, because some operations could be applied twice or not applied at all. These per-log positions are also useful to check whether storage nodes that consume a given log are in sync with each other. When they respond to a coordinator, the coordinator can check whether the positions for a given operation match. If they don’t, this is an indication of data corruption having happened to this operation or its predecessor. Then we can find when the positions matched in the past and determine when a corruption happened. Truncating the logs Logs provide a nice guarantee that no matter how long a storage node has been down, it can catch up with the latest state by consuming all of the operations since the last-saved position. However, this approach poses a problem: do we have to keep all log entries forever, or can we safely remove some of them? Let’s say we make coordinators responsible for truncation. Coordinators cannot truncate unless the information about log progress is sent back to them periodically. That could result in many messages being sent. Also, if coordinators had to keep track of progress and truncations of all logs, how can we avoid many coordinators truncating the same log at the same time? If instead we make individual storage nodes do the truncations, how do we distribute the information about positions of other nodes that subscribe to the same logs? To safely truncate, all readers need to agree on last-saved position. This is a distributed consensus problem that we can also solve with a log. We opted for a simple solution that didn’t require any additional communication channels: every node periodically publishes information about its own position in a log into that log. Eventually, every storage node learns the positions of others in the logs. If the position of a given node on some log is unknown, we don’t truncate that log. This can happen during temporary failures. Otherwise, we select the earliest position and truncate up to it. The following diagram shows node 3 publishing its own position to the log. When a storage node replaces another storage node completely (for example, due to hardware failures), we update the topology membership accordingly and no longer wait for the old node to report its truncation position. This allows other storage nodes to proceed with truncation. Time in strongly consistent systems Manhattan supports key expiration: a key can have a limited lifespan and will be erased after a configured time-to-live, or TTL. Our customers use TTLs to implement application-level expiration logic or store temporary data. Implementing expiring keys in an eventually consistent model means guaranteeing that at some point in time after the expiration time, the key will indeed expire. It may not happen atomically: different storage nodes may respond with different results until they reconcile on a decision. For example, a small difference in local clocks may result in such inconsistencies. However, in case of strongly consistent systems, we have to provide strong ordering guarantees for key expirations. A key expiration needs to be consistent across all nodes, so determinism is important. One source of non-determinism is time. If storage nodes disagree about the current time, they’ll disagree about whether a key has expired or not, and therefore make different decisions about whether a subsequent CAS operation succeeded or failed. Here’s an example of ordering of regular operations A and B and an expiration of key X on different storage nodes of the same data. The timeline below shows three possible ways these events can be ordered. Note that we don’t care about the absolute time of each operation, as long as the order is maintained. In the following diagram, the bottom timeline is inconsistent with the first two (key X expires after B): To guarantee correctness, we need all operations for any given key to have monotonically increasing timestamps. We could have kept track of time per key, per shard, or globally for the entire cluster. Since our model already uses logs to sequence all per-shard operations, we decided to keep track of time per shard and use logs as the source of truth for time. When a record is submitted to DistributedLog, the log’s single writer assigns it a timestamp. The single writer ensures that the timestamp is monotonically increasing, and therefore will be correct from the external point of view. To provide this guarantee, it ensures that every new value of the timestamp is equal or higher than the previous one. Upon ownership changes of log writers, the timestamp used by the new owner is always greater or equal to the timestamp of the last successful record written by the old owner. Now that the storage nodes have consensus on current time, they also agree on when each key expires. Note that they don’t need to immediately delete expired keys. For example, if a given key it set to expire at time T, and a strongly consistent read operation comes from the log with the exact timestamp T+1, we can filter out the expired key from results. Comprehensive rate limiting As a multi-tenant database, Manhattan needs to provide high quality of service to each customer. A client can affect latencies of all operations on a shard by overwhelming the log with repeated operations for a hot key. Due to having a single log per shard, a hot key can affect latencies of all operations in the log by overwhelming the log with repeated operations. In an eventually consistent model, this can be mitigated by having the storage node arbitrarily skip records when under high load. In strongly consistent model, this is not possible: storage nodes cannot arbitrarily skip records under high load unless they achieve consensus on which records get skipped. We introduced rate limiting to protect the shards against this scenario. Since our clusters are multi-tenant, we rate-limit by client. We will talk more about this and other types of rate limiting that we’re using in our next post. Summary We discussed our solution to adding strong guarantees to an eventually consistent system. Over the last year, we started off with a few initial internal customers, and later opened it up to all of Twitter Engineering. Our customers include the URL shortener, authentication service and profile service. Currently, it takes a few minutes to provision a strongly consistent dataset at Twitter. Readers may also be interested in exploring Datomic , which followed a very different approach. Datomic added transactions as a layer on top of an eventually consistent data store, but without modifying the underlying store. We also recommend VoltDB’s whitepaper on distributed transactions at scale. Team We would like to thank Peter Schuller , Melvin Wang , Robin Dhamankar , Bin Zhang , Sijie Guo , Boaz Avital , David Helder , Hareesha Tamatam , Istvan Marko , Jerric Gao , Johan Harjono , Karthik Katooru , Kunal Naik , Leigh Stewart , Pascal Borghino , Ravi Sharma , Spencer Fang , Sree Kuchibhotla , Sumeet Lahorani , Tugrul Bingol , Tyler Bulut , Unmesh Jagtap , Vishnu Challam , Yalei Wang for their contributions to the project.", "date": "2016-03-17"},
{"website": "Twitter-Engineering", "title": "When seconds really do matter", "author": ["‎@maxmichaels‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2016/when-seconds-really-do-matter.html", "abstract": "Behind unmarked doors in office buildings across the globe, engineers sit in front of walls covered in 60-inch LCD panels displaying graphs of many colors. Fueled mostly by caffeine, candy, and a wary eye, these teams are tasked with a complex job: ensuring all the services supporting Twitter are available, responsive, and functioning as expected — 24 hours a day, 365 days a year. This is the Twitter Command Center (referred to as “the TCC”) and this team is responsible for identifying and triaging any problems with the Twitter platform for our users, partners, and advertisers. Of course, the definition of a “problem” can be very broad when operating such a dynamic and high traffic service. From the simplest latency spike due to the failure of a hard drive in a cluster, to a complex linux kernel timing bug to an undersea fiber optic cable being severed, the tools used to identify and resolve these and other issues have remained mostly the same: a minutely time series storage and graphing engine. From MRTG / RRD , Graphite , and Ganglia to Twitter’s own internal observability tool , the minutely time series has kept us in the know for 99% of the questions we’ve needed answered. But for Twitter, that 1% of information that minutely metrics miss is a very, very important 1% and we needed to address this gap. On Twitter, the real world happens in sub minutely intervals and activity on the platform follows. Whether it’s a retirement announcement from a member of a boy band, a government coup, a world wide soccer tournament goal, or simply the start of a new year, traffic patterns change quickly and unpredictably as these events unfold. These are the moments when the minutely model becomes insufficient. Let’s imagine that a celebrity couple announces the birth of a child at two seconds after the nearest minute — for this example, let’s say 2:00:02. With this announcement, a spike of traffic causes undue pressure on one of the many micro services that powers Twitter, causing service errors to some users. With the standard monitoring platforms, it would be another 58 seconds (at 2:01) before we even had data on the problem, at which point, we could turn one of the many knobs at our disposal to attempt to resolve this problem, and then wait another 60 seconds to see if the knob did what we hoped it would. We’re now at 2:02. So that would be basically two minutes to resolve an issue if all things go perfectly. At Twitter, we need to do better. The metric used at Twitter to measure the health of a service is its success rate, or the percentage of requests to the service that were served successfully. This is simply expressed as (1-(failed_requests/total_requests))*100. To render this metric in real time, we need to aggregate our metrics on several dimensions: the time of the request on secondly boundaries, name of the cluster servicing the request, the category of the response code (2xx,3xx,4xx,5xx), and the zone from which the request was served. This sounds relatively simple, but when you consider the volume of requests Twitter receives, it becomes a bit trickier. Thankfully, the Twitter Data Platform team has developed tools to make solving this tricky problem a reality. Using Twitter’s own TSAR and Heron services, we were able to create a job that consumes every request Twitter receives and aggregates on the dimensions mentioned above. With additional improvement, we were able to capture a statistically significant data sample size at a 5 second delay with a surprisingly small amount of compute resources. Leveraging TSAR’s HTTP API, we whipped up a simple front end using the simple yet amazing Smoothie Charts library . With this relatively easy implementation, we were able to knock down the best effort resolution time from 2 minutes to 10 seconds, a remarkable 12x improvement in problem identification and resolution. Once we tackled the success rate problem, it made sense to add in other data aggregations that allow the team to identify a larger variety of problems much faster. Fields such as source IPs, user agents, user IDs, URIs requested, and application IDs are aggregated every second to form more specific secondly success rates. These metrics allow us to detect which users, networks, and applications are impacting our services and act accordingly to block and/or correct the behavior before it becomes an issue that impacts our users’ experience. Shoutouts to Mansur Ashraf , Anirudh Todi , Allen Chen , Sanjeev Kulkarni , Karthik Ramasamy , and the rest of the Twitter Data Platform team for creating the frameworks that made building these tools possible. This post was co-authored by Harry Kantas , Staff Reliability Engineer.", "date": "2016-03-11"},
{"website": "Twitter-Engineering", "title": "Power, minimal detectable effect, and bucket size estimation in A/B tests", "author": ["Lucile Lu"], "link": "https://blog.twitter.com/engineering/en_us/a/2016/power-minimal-detectable-effect-and-bucket-size-estimation-in-ab-tests.html", "abstract": "Bucket size estimation for A/B experiments In previous posts, we discussed how to detect potentially biased experiments , and explored the implications of using multiple control groups . This post describes how Twitter’s A/B testing framework, DDG, addresses one of the most common questions we hear from experimenters, product managers, and engineers: how many users do we need to sample in order to run an informative experiment? From a statistical standpoint, the larger the buckets, the better. If all we cared about was how reliably and quickly we could observe a change, we would run all tests at a 50/50 split, which is not always advisable. New features have risk, and we want to limit risk exposure. Also, if experimental treatments change as we iterate on designs and approaches, we might confuse a lot of users. Additionally, if all experiments ran at a 50/50 split, our users could wind up in multiple experiments simultaneously, making it difficult to reason what anyone should be experiencing at any given time. Thus, if 1% of the traffic is sufficient to measure effect of changes, we would prefer to use only 1%. However, if we pick too small a bucket, we might not be able to observe a real change, which will affect our decision-making and/or slow down iteration while we re-run the experiment with larger buckets. To address this problem, we created a special tool to provide guidance for experimenters on sizing their experiment appropriately, and to alert them when experiments are likely to be underpowered. Prior art There are many different experiment sizing and power calculation tools available online. They tend to either explicitly ask the experimenter the sample mean and standard deviation (in addition to overall population size) or they address “click through rate” (CTR) or “conversion” — binary-valued metrics (either 1 or 0) for which variance is easy to calculate given the ratio of 1s to 0s. None of these were straightforward to incorporate into our experiment creation workflow, for two reasons: Non-binary metrics : Many metrics we track are not ones that can be described in terms of CTR. For example “proportion of people who ‘like’ a Tweet” is a binary metric, a user either hits the ‘like’ button at some point, or she does not. “Average number of Tweets people ‘like’” is a non-binary metric. A user can like many Tweets. Variance of such metrics can’t be calculated directly from the metric value, and isn’t something an experimenter is likely to have at hand. Targeted Experiments : When observing an experiment, a naive estimate of variance of a metric would be calculating the metric across the full population, and recording the observed variance. However, as described in an earlier post , Twitter experiments use “trigger analysis” — only subsets of users who trigger specific features are included in an experiment. These sub-populations frequently have different values for important metrics than does the overall Twitter population (this is intuitive: people who tap the “New Tweet” icon are a-priori more likely to send a Tweet than those who do not!). Since an experimenter can instrument the code to trigger an experiment at practically any place in the app, pre-calculating variances for each metric at each possible trigger point is unrealistic. A quick review of statistical power Let’s first define a few terms. Null hypothesis is the hypothesis that the treatment has no effect. An A/B test seeks to measures a statistic for control and treatment, and calculates the probability that the difference would be as extreme as the observed difference under the null hypothesis. This probability is called the p-value . We reject the null hypothesis if p-value is very low. The conventional threshold for rejecting the null hypothesis is 5%. Power is the probability that an experiment will flag a real change as statistically significant. The convention is to require 80% power. Power depends on magnitude of the change, and variance among samples. True effect is the actual difference of mean between the buckets that would have been observed if we had infinitely large sample sizes. It’s a fixed but unknown parameter, which we are trying to infer. The difference we compute from the samples is called the observed effect and is an estimate of the true effect. Given sample size and sample variance, we can calculate the smallest real effect size which we would be able to detect at 80% power. This value is called the minimal detectable effect with 80% power , or 0.8 MDE . The graph below — which applies to a one-sided two sample t-test — helps us visualize all this. The two bell-shaped curves represent the sampling distribution of the standardized mean difference , under the null hypothesis (red) and the alternative (green) respectively. The umber-colored area illustrates false positives — the 5% chance of getting a measurement under the null hypothesis that is so far from the mean that we call it statistically significant. The green area illustrates the 80% of time we call statistical significance under the alternative. Note there’s a lot of red and green to the left of the MDE — it is possible to observe a statistically significant change that is less than “minimal detectable effect” under the conventional cutoff values of 0.05 p-value and 0.8 MDE (you can’t observe it as often as 4 out of 5 times, but you can still happen to observe it). We can represent false positive rate and power (of a two-sided test) in the following way: As experimenters, we want to increase our power and decrease the false positive rate. Low statistical power means we can miss a positive change and dismiss an experiment that would have made a difference. It also means we can miss a negative change, and pass through as harmless an experiment that in fact hurts our metrics. Ensuring we can get sufficient power is a critical step in experiment design. Sizing an experiment There is one big lever we can use to get desired power: the bucket size. We want to allocate enough traffic that we can detect the effect, but not so much that we’re practically shipping the feature. To figure out how large our bucket sizes need to be, let’s take another look at the power formula. Assuming a false positive rate of 5% and a two-tailed test, we see that power can be represented by: Here, Z is the distribution of the standardized mean difference. The difference of mus is the true effect size, and sigmas represent the standard deviation of the metrics in control and treatment buckets, respectively. Sizes of each bucket are represented by n and m. Finally, 1.96 is the C from above when we set false positive rate to 5% (the curious can find derivation in any statistics textbook). Given this equation, we can make a few assertions on how to increase power: The larger the true effect size , the larger the power. The smaller the variance , the smaller the sigma, the larger the power. The larger the sample size , the larger the power. If we use the convention of requiring power to be at least 80%, and make the simplifying assumptions that we have equal bucket sizes (n = m) and that the sigmas are the same, we can derive a nice formula for sample size n, as a function of true effect size and variance (via Kohavi et al [1]). With this simplified formula in place, we need to get a handle on delta and sigma — the true effect size and standard deviation, respectively. Bucket size estimation tool Since we can’t rely on collecting metric variance or standard deviation for the specific sampled populations directly from the experimenters, we created a tool that can make suggestions based on basic data the experimenter can get more easily. The key insight is that while in theory, an experiment can be instrumented anywhere in the app, many product teams at Twitter tend to instrument experiments in relatively few decision points. Stable products also tend to reuse their custom metrics between experiments. In these cases, we have a rich set of experiment records and historical data that we can use to estimate the sigmas for an upcoming but similar experiment. Our tool simply asks the experimenter to provide an ID of a similar experiment from the past, and loads up observed statistics from that experiment for all metrics the experiment tracked. The experimenter can now choose any of the previously tracked metrics, specify how much of a lift they expect to see with new changes, and get accurate estimates on the amount of traffic they need to allocate to reliably observe the expected change. In cases where no prior experiments have been instrumented in the same location, or when brand new metrics are necessary, an experiment can be started in “dry-run” mode, just bucketing into control. The regular experiment pipeline will collect all the statistics necessary for buckets to be estimated. Calling attention to underpowered metrics Sometimes the bucket estimation tool isn’t enough. Experimenters may discover that their experiment targets an audience with different behavior from the audience of the experiment they provided for bucket size estimation. They may also want to assess impact on metrics that were not taken into account when using the bucket estimation tool. In such cases, it’s possible that a given metric has insufficient power to make a strong claim regarding experiment results. We call attention to metrics for which no significant effect is detected, and the minimal detectable effect is large. Consider a hypothetical experiment that attempts to improve the “Who To Follow” suggestion algorithm. Let’s imagine that historical experiments of this sort tend to produce a gain of 1% more follows. In the current experiment, we measure a 1.5% observed effect but it isn’t statistically significant. We also calculate that based on the current sample size, MDE is 5%. Based on past experience, we know that if there is a true effect, it’s unlikely to be above 5%. With the MDE we measured, it’s unlikely we would call a “real” 1.5% change statistically significant. In fact, the graph below demonstrates that only 30% of experiments of this size would detect a 1.5% change as statistically significant. Considering the high MDE, instead of concluding that the proposed improvement has no effect, it’s better to increase the sample size and re-run the experiment with higher power. The Twitter experimentation framework, DDG, provides visual feedback to experimenters by coloring statistically significant positive changes green, and statistically significant negative changes red. We color likely-underpowered metrics a shade of yellow. The intensity of the color is used to call experimenters’ attention to the actual p-values when we see statistical significance, and to potential power problems when we do not. The intensity of red and green colors depends on the p-value (the more intense, the lower the p-value). The intensity of the yellow depends on the MDE (the more intense, the higher the MDE). Summary Providing the right amount of traffic is critical for successful experimentation. Too little traffic, and the experimenter does not have enough information to make a decision. Too much traffic, and you expose a lot of users to a treatment that might not fully ship. Power analysis is a critical tool for determining how much traffic is required. DDG uses data collected from past experiments to guide experimenters in selecting optimal bucket sizes. Furthermore, DDG provides visual feedback to help experimenters detect whether their experiments are underpowered, allowing them to rerun with larger sample sizes if needed. If your A/B testing tool does not have similar capabilities, there’s a decent chance you are missing “real” changes due to being underpowered — both good changes that improve your product, and bad effects that are affecting you negatively. Call your data scientist today, and ask them about power analysis. Acknowledgements Robert Chang and Dmitriy Ryaboy co-authored this blog post. [1] R. Kohavi, R. Longbotham, D. Sommerfield, and R. M. Henne. Controlled experiments on the web: Survey and practical guide. Data Mining and Knowledge Discovery, 18, no. 1:140–181, July 2008.", "date": "2016-03-08"},
{"website": "Twitter-Engineering", "title": "Fixing a recent password recovery issue", "author": ["‎@_mwc‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2016/fixing-a-recent-password-recovery-issue.html", "abstract": "We recently learned about — and immediately fixed — a bug that affected our password recovery systems for about 24 hours last week. The bug had the potential to expose the email address and phone number associated with a small number of accounts (less than 10,000 active accounts). We’ve notified those account holders today, so if you weren’t notified, you weren’t affected. We take these incidents very seriously, and we’re sorry this occurred. Any user that we find to have exploited the bug to access another account’s information will be permanently suspended, and we will also be engaging law enforcement as appropriate so they may conduct a thorough investigation and bring charges as warranted. While this issue did not expose passwords or information that could be used directly to access an account, it serves as a reminder to us all about the importance of good account security hygiene. Some suggestions: Require additional information be entered in order to initiate a password reset. This feature will require that you enter your account email address or mobile number, in addition to your username, in order to send a password reset email or SMS/text. Be sure to use a strong password – at least 10 (but more is better) characters and a mixture of upper and lowercase letters, numbers, and symbols – that you are not using for any other accounts or sites. Consider using login verification. Instead of relying on just a password, login verification introduces a second check to make sure that you and only you can access your Twitter account. Check the Applications tab at http://twitter.com/settings/applications and revoke the access privileges of any third party applications that you do not recognize. If you’d like to review logins for your account you can do that at the Twitter data dashboard in your settings. For more information about making your Twitter and other Internet accounts more secure, read our Help Center and the FTC’s guide on passwords .", "date": "2016-02-17"},
{"website": "Twitter-Engineering", "title": "Implications of use of multiple controls in an A/B test", "author": ["Lucile Lu"], "link": "https://blog.twitter.com/engineering/en_us/a/2016/implications-of-use-of-multiple-controls-in-an-ab-test.html", "abstract": "In previous posts, we discussed how A/B testing helps us innovate , how DDG, our A/B testing framework , is implemented, and a technique for detecting bucketing imbalance . In this post, we explore and dismiss the idea of using a second control to reduce false positives. The trouble with probabilities A/B testing is a methodology fundamentally built on a framework of statistics and probability; as such, the conclusions we reach from experiments may not always be correct. A false positive, or a type I error, is rejecting the null hypothesis when it is actually correct — or, roughly, claiming that a behavior change is seen when in reality there is no change between the sampled populations. A false negative, or type II error, is accepting the null hypothesis when it is actually not correct — in other words, claiming that we don’t see a change between the buckets when a difference exists between sampled populations. A well designed statistical experiment aims to minimize both types of errors. It is impossible to completely avoid them. Naturally, this uncertainty can be very concerning when you are making ship/no-ship decisions, and you are not sure if the changes you observe are “real.” This post will examine the idea of using two control buckets in order to guard against these errors. We will demonstrate this causes significant problems, and that creating a single large control is a superior and unbiased way to achieve the same goal using the same amount of data. Testing with a second control An A/A test is one in which no changes are applied to the “treatment” bucket, effectively resulting in having two controls. A/A tests are a very useful tool for validating the testing framework. We regularly use A/A tests to verify the overall correctness of our tooling: to see if the statistical methods are implemented correctly, check that there are no obvious issues with outlier detection, and spammer removal. A/A tests can also be used to determine the amount of traffic necessary to detect predicted metric changes. Some experimenters at Twitter proposed running A/A/B instead of A/B tests, to reduce the false positive rate due to “biased” buckets. An A/A/B test is one in which in addition to the provided control (A1), an experimenter would add another control group (A2) to their experiment. There are two approaches to using the two control buckets. The first approach is to use A2 for validation — if treatment shows significant differences against both control A and control A2, it is “real”; otherwise, it’s discarded as a false positive. Another approach is to use A2 as a replacement control when A1 bucket is believed to be a poor representation of the underlying population. Our analysis shows that both of these approaches are inferior to pooling the traffic from the two controls, and treating it as a single large control — in other words, if you have the traffic to spare, just make your control bucket twice as big. Statistics behind the second control Let us consider the possible outcomes of an A/A/B test in which the reality is that the null hypothesis is true and treatment (B) has no effect. Below, we present all possible combinations of significance outcomes between the three buckets. At significance level of 0.05, there is 95% chance of seeing non-significance between A1 and A2 (the left diagram). Among the remaining 5%, half of the time A1 will be significantly smaller than A2, and half of the time the reverse will hold (the middle and right diagrams). The number in each cell is the percent of the time a particular combination of A1 vs Treatment and A2 vs Treatment occurs, given the A1-A2-B relationship. These values are analytically derived; you can find the derivation in the appendix . We will now examine both approaches to using two controls, and show why they are suboptimal compared to combining both control buckets into a single large control. Analysis of Approach 1: rejecting upon any disagreement Recall that the first approach states that we accept the null hypothesis unless it is rejected when comparing against each of the controls. We also discard all results in which controls disagree. Controls will disagree in 5% of the cases by construction, and those are thrown out immediately. Per the leftmost table in Figure 1, the chance of observing both controls being significantly different from treatment under the null hypothesis (two false positives in a row) is merely 0.49% + 0.49% = 0.98% of 95%, or 0.93% of the total. Thus the rate of false rejections of the null hypothesis is significantly reduced, from 5% to under 1%. This comes at a price. Using two controls, either of which can cause the experimenter to accept the null hypothesis, is a much stronger requirement that will cause false negatives in cases where B does indeed diverge from control. We drop power by introducing this constraint, which results in a sharp increase to the false negative rate. The specific false negative rates depend on how much a treatment moves the metric. By way of illustration, we consider a normal distribution with mean of 0 and standard deviation of 1 in both controls, and mean of 0.04 and standard deviation of 1 in treatment. The table below presents the false negative % that can be analytically derived using the same methodology as presented above. We compare it to simply using a single control and to using a pooled control. P-value is set to 0.0093 in both cases in order to match Approach 1’s false positive rate. In all cases, one pays a very high false negative cost for such a low false positive rate. While Approach 1 outperforms using a single control, it produces many more false negatives than does combining the traffic from both controls into a single large pooled control. Analysis of Approach 2: pick the “best” control The second approach for using two controls is to try to identify which of them is the best representative of the underlying population. Since we would be identifying the “best” control after seeing experiment results, this approach suffers from severe temptation to cherry-pick and make biased choices. This is a significant danger, and the biggest reason to completely avoid using two controls. Let us be charitable, however, and imagine that instead of “going with our gut” to pick between A1 and A2, we perform analysis to determine which of the two controls is a better representative of the underlying data. Let’s further assume (unrealistically) that our analyst has access to the true distributions and can always correctly identify the best control. We simulate 50,000 pairs of (A1, A2) buckets each with 10,000 samples from the standard normal distribution. In our simulation, 2492 (4.98%) turn out to have statistically significantly different means. Since we know the distribution from which we drew the simulated data points, it’s easy for us to to simulate a perfect “best control” oracle. For each pair of controls, we simply pick the one whose mean is closer to 0. We compare it to the mean of the pooled control, which is of course the average of the two control means — we will call it “average.” The cases where two controls do not exhibit statistically significant differences are not interesting, since they do not cause the experimenter to choose a “best” control. Let us consider only the cases when we observe divergence between A1 and A2. From the plot below we see that “best of 2” correctly centered at zero. However, while the pooled plot appears to fit the expected unimodal shape, the “best of 2” strategy results in a bimodal back of a two-humped camel. Variance of means from the pooled approach is 4.81e-5, while variance of “best of 2” is a much higher to 1.45e-4. The shape exhibited by “Best of 2” is not surprising if we consider what must happen in order for two controls to be seen having a statistically significant difference. Statistical significance is measured based on the difference between the two controls’ means. The further apart they are, the more different we observe them to be. Given a normal distribution of sample means (cf. Central Limit Theorem), for any difference d, the probability of seeing difference d due to two samples coming from either side of the true mean is higher than the probability of both means coming from the same side of the true mean. In other words, if you see a large difference between two controls, chances are it’s because one is (much) larger than the true mean, and the other is (much) smaller than the true mean. Averaging them will tend to give you something closer to the true mean than picking either of the extremes. We can clearly see from this simulation that when the two control buckets diverge, pooling them provides a better estimation of the mean of the distribution with lower variance, and therefore has much higher power, than even a perfect oracle that correctly identifies the “best” control 100% of the time. Pooled control will perform even better in the real world, since correctly identifying the “best” of two controls 100% of the time is impossible. Conclusion Doubling the size of a single control bucket is preferred over implementing two separate t-tests using two control buckets. Using two controls exposes the experimenter to a high risk of confirmation bias. If the experimenter resists the bias and only uses the second control as a validation tool, the experiment suffers from higher rate of false negatives than if the controls were pooled. Pooling will also tend to result in a better estimate of the true mean than using just one of controls, and does not require defining a methodology for choosing the “best” control. Experimenters should be discouraged from setting up a “second control” in a misguided attempt to protect themselves against errors. Acknowledgements Dmitriy Ryaboy co-authored this blog post. We’d like to thank Chris Said , Robert Chang , Yi Liu , Milan Shen , Zhen Zhu, and Mila Goetz , as well as data scientists across Twitter, for their feedback.", "date": "2016-01-13"},
{"website": "Twitter-Engineering", "title": "Visually explore funnels of user activities", "author": ["‎@kristw‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2016/visually-explore-funnels-of-user-activities.html", "abstract": "Funnel analysis is a concept that has been around for a while and there are many systems that support counting funnels, or sequences of events. In this post, we describe our experimental visual analytics approach that not only counts the specified sequence, but also aggregates and visualizes information between steps within the sequences to provide broader perspectives. This novel approach helps us learn how users interact with the user interfaces during Tweeting and signing up, and leads to insights for improving user engagement with Twitter. The challenges of funnel analysis We’re constantly analyzing user interaction logs to ensure we are delivering product features that users find truly useful and engaging. The simplest way to analyze logs is to count a single log event , such as the Click on Tweet button. However, such click only opens the Tweet composer and does not necessarily mean the user successfully Tweeted. Counting funnels can provide a broader picture. For example, to check how often users abandon a Tweet after they start composing, we can count the funnels “ Click on Tweet button then Send Tweet ” and compare it with the number of Click on Tweet button. Seeing the composing success rate leads to more exploratory questions such as “is abandonment consistently associated with certain composing activities such as typing or attaching images?”, which tend to push the limits of the ad-hoc counting approach. One workaround is to count all funnels with this template: Click on Tweet button => ? => NOT Send Tweet The placeholder “?” is replaced with events of interest such as Enter text and Attach image , leading to two funnels to be counted. As the question gets more complicated, the number of funnels to count starts to skyrocket, multiplying by the number of events of interest every time we add in another placeholder. For example, a template with two placeholders “ Click on Tweet button => ? => Attach image => ? => Send Tweet ” leads to four funnels. More ambiguous question such as “what are the common sequences of actions before Click on the Tweet button ?” will require a variable number of placeholder and quickly become difficult to enumerate. Once all the counts are computed, a data scientist also has to process these counts for interpretations. The scale of Twitter data complicates the problem even more. To give you some ideas, there are more than 10K types of events and hundreds of millions of users. Sequences with five user activities have 10K^5 possible patterns and these raw data will not fit on any laptop. Nevertheless, these challenging tasks are beneficial to our understanding of user behaviors. Therefore, we decided to tackle this problem and develop a better solution. In this post, we will tell you about Flying Sessions [1], our experimental visual analytics tool for funnel analysis. Our approach We will explain our approach by walking through a fictional use case describing how Piper, a data scientist, analyzes how users find people on Twitter to Tickle , a new fictional feature. Note that all numbers in this post are made up for demonstration and do not reflect actual Twitter usage. To start the analysis, Piper needs to run a data analysis job with two parameters below: 1) Alignment point(s) : At least one or more alignment points are required. This is similar to specifying an event or a sequence of events for counting. Piper wants to focus on the users who find people to tickle by opening the “Find people” page, so she defines her alignment points as: Open ‘Find people’ page => Tickle The key benefit of our approach is that we not only provide the count for the funnel above, but also aggregate information between the steps and before/after the funnel to provide more context. The contextual information can be from the alignment points themselves, such as time between Open ‘Find people’ page and Tickle , or from other events that are not the alignment points. The second parameter is needed for this purpose. 2) Events of interest : In addition to the alignment points, Piper can include more events to provide more context. These can be events that are relevant to Twitter users’ tickling behaviors such as View profile popup or Open profile page . An event can also be defined as a combination of multiple raw events. For example, Retweet, Like and Reply can be combined and treated as a custom event Tweet engagement . Figure 1. Funnel view with alignment points Open ‘Find people’ page => Tickle. The visualization shows that 70.02% of users tickled another user after opening the “Find people” page. After the data processing job with specified parameters above is completed, the results can be explored via the user interface shown in Figure 1. The funnel view shows how user sessions bifurcate after each alignment point. Looking at this view, Piper can tell at a glance that 70.02% of the users tickled at least one person after opening the “Find people” page. The regions behind the number 70.02% show that the majority of users have done another action (darker gray region) during these two steps while a small proportion have done nothing else (light gray region). To get a sense of what users do after landing on the “Find people” page and before tickling someone, Piper clicks on the segment (entire gray region) between the two alignment points. This brings up additional segment information in the bottom panel (Segment 1), which Piper can choose from different types of aggregated information. Piper chooses the “Average count per session” view, which shows the most frequently performed actions in this segment and their average counts. Piper notices that users on average look at the profile popups of recommended users 3.5 times before deciding to tickle. It also seems common for users to look at the profile page and the photo and videos tab. Piper switches to “Forward patterns” view (so the bar chart in Figure 1 is replaced with this visualization below in Figure 2) and sees that many users tickle immediately while many open the popup before tickle. Figure 2. Visualization of common patterns between alignment points, which can be read from top to bottom. Each rectangle encodes an event and its width is proportional to the number of sessions. For example 100% of the patterns start with Open ‘Find people’ page (pink) and 70% of the time are followed by Tickle (gray) immediately. Piper can also select another segment (blue region in Figure 1) that represents users who did not tickle to compare side-by-side (Segment 2). This feature is usually useful for comparing users who are in the funnel and users who dropped out to spot different behaviors. In this case, none of the users who dropped out tried to preview the user profile or visit profile page. Iterative analysis Flying Sessions was designed to encourage iterative analysis, where the data scientist gradually modifies the set of alignment points to accommodate newly emerged analysis questions. Seeing that users frequently look at target users’ profile pages before tickling, Piper decides to look more into this and adds “ Open profile page ” as a new alignment point: Open ‘Find people’ page => Open profile page => Tickle She then clicks on the segment between Open profile page and Tickle and chooses the “Time between” option in the aggregate view. She notes that the majority of the users spend around ten seconds scanning the profile of a user they might want to tickle. Figure 3. The new results after adding more alignment point Open profile page How does it work? We use Scalding to filter and summarize raw log events in Hadoop, which is often on the order of GBs or TBs, into a much smaller JSON file (a few MBs) that can be visualized in a web-based front-end, built with D3 and d3Kit . The data pipeline consists of three major steps: sessionization, segmentation, and aggregation. Sessionization Events are sorted by timestamp and grouped by user ID, then added into a session in order until the time difference between two consecutive events is more than 30 minutes, at which point a new session is created. In other words, each session contains consecutive events that belong to a single user and does not contain any period of inactivity that is longer than 30 minutes. Figure 4. During sessionization, events that are more than 30 minutes apart are split into separate sessions. Segmentation The segmentation stage extracts subsequences of events relevant to the analyst-specified alignment points from sessions, and groups the subsequences into segments based on the alignment points they contain. This stage contains two steps: Step 1: identify session fragments — A set of alignment points may appear multiple times throughout a session, and ideally we want to capture all those occurrences. Therefore, we first identify session fragments from sessions, in which each alignment point occurs at most once. To do so, we find all occurrences of the first alignment point. For each occurrence, we apply a window which extends in both backward and forward directions and is cut off at the neighboring occurrences of the first alignment points or session boundaries. All events covered by the window form a session fragment. Figure 5. A new session fragment is identified with every occurrence of the initial alignment point (Find people). We then identify the other alignment points contained in each session fragment. To find those alignment points, we scan forward from the initial alignment point, looking for the second alignment point. Once the second alignment point is found, we continue the forward scan but this time looking for the third alignment point. We repeat this process until all alignment points have been identified or the end of the session fragment is reached. Step 2: extract and group subsequences between alignment points — We then iterate through all the session fragments, extracting subsequences between each pair of neighboring alignment points. Each subsequence is labeled with two properties: the number of alignment points matched by the corresponding session fragment and the indices of surrounding alignment points. Finally, we group subsequences with the same labels into same segment. Figure 6. From each session fragment, we extract and group subsequences of events that come between each pair of neighboring alignment points (Find people and Tickle). Eight subsequences shown above are grouped into four segments. Subsequences from fragment 1 and 2 are not grouped with fragment 3 and 4 because the latter fragments contain only one alignment point (Find people) while the former contains both (Find people and Tickle). Aggregation The aggregation stage pipes the segments from the previous stage through a variety of aggregators in parallel to produce summaries (e.g., average count of events) that can be visualized in the front end. This stage is highly extensible. New aggregators can be added when demands for new types of summaries arise. Conclusions and future work Flying Sessions was designed to support funnel exploration with less required effort and provide more information than simple counting. The project is still in early stages and we envision a few design improvements that will enable more flexible analysis, for example, to extend support for optional alignment points. We also include only a modest set of aggregation methods, but it’s possible to add new types of aggregation that employ pattern mining or more sophisticated algorithms. The segments are also just sequences of events and can be exported for other purposes or recursively analyzed. Acknowledgement The main contributor of this project was Hua Guo , a PhD student at Brown University, who worked with Krist during her summer internship at Twitter HQ. She also co-wrote this blog post. We appreciate the help and feedback from Gillian Chin , Charlie Croom , Jesse Bridgewater , Joy Ding , Nodira Khoussainova , Joshua Lande , Raghav Ramesh , Dmitriy Ryaboy , and Richard Whitcomb during the development of Flying Sessions. Footnote [1] Flying Sessions mentioned in this blog post is actually the second version. The earlier version [2], only supported single alignment point and focused on identifying common patterns. These eventually became the aggregated views: forward patterns and backward patterns (Figure 2). [2] Krist Wongsuphasawat and Jimmy Lin. Using Visualizations to Monitor Changes and Harvest Insights from a Global-Scale Logging Infrastructure at Twitter . in Proc. IEEE Conference on Visual Analytics Science and Technology (VAST), pages 113-122, Nov 2014. [3] George Lee, Jimmy Lin, Chuang Liu, Andrew Lorek, and Dmitriy Ryaboy. The Unified Logging Infrastructure for Data Analytics at Twitter . in Proc. International Conference on Very Large Data Bases (VLDB), pages 1771-1780, Aug 2012.", "date": "2016-01-06"},
{"website": "Twitter-Engineering", "title": "2016", "author": ["‎@‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2016.html", "abstract": "", "date": "1970-01-01"},
{"website": "Twitter-Engineering", "title": "Sunsetting SHA-1", "author": ["‎@_mwc‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2015/sunsetting-sha-1.html", "abstract": "Twitter takes the responsibility of protecting our users and their data seriously. Over the years, we’ve implemented many security defenses such as content security policy , perfect forward security , and privacy protecting email security . We’ve also been consistently recognized by organizations such as the Online Trust Alliance for our commitment to security. As a result of ongoing security research into the weaknesses of SHA-1 we believe it’s time to move on from SHA-1 certificates. We’re doing our part by implementing SHA-256 certificates on our Twitter endpoints, and using cert switching to only serve SHA-1 certificates if we detect older clients without SHA-256 support. However, we are still concerned that the overall CA/B forum migration plans don’t provide a sensible path forward for users whose low-end devices cannot support SHA-256 certificates . In our testing, we calculate that between 3% and 6% of Twitter users are on older devices that would be unable to access websites via HTTPS after the SHA-256 migration is complete. Many of these people are in parts of the world where it is prohibitively expensive to buy a new device. This fact puts these users in a difficult situation, faced with only two options: One, have their traffic trivially monitored as it passed over unencrypted HTTP; or two, have no access at all to the numerous websites that are only accessible over HTTPS. Facebook and CloudFlare have also discussed their concerns on this, and presented an amended proposal to the CA/B forum to address the issue of older devices. This proposal ensures that the SHA-256 migration moves forward for the vast majority of modern web devices that are regularly updated. For the small percentage of old devices that cannot support SHA-256 (which does represent millions of people), their proposal outlines a reasonable path that provides continued access for a temporary period with SHA-1 certificates issued – with strict additional controls. We support the amended proposal that has been put forth by Facebook and CloudFlare to the CA/Browser forum for several reasons: The proposal ensures the continued migration to SHA-256 for all mainstream devices. It only allows legacy validated SHA-1 certificates when a domain also provides SHA-256 support. Legacy validated SHA-1 certificates are only available per specific requirements and will still sunset in March, 2019. Increased randomization of serial numbers in legacy validated certificates results in less probable SHA-1 collisions. In the event an attack is discovered that forces the improper use of SHA-1 certificates, the owners would terminate use of these legacy validated certificates. Balancing security and accessibility is challenging, especially when considering older devices and transitions in technology. Twitter supports a path that provides maximum security for the majority of users while also ensuring those with low-end devices are not forced to choose between losing access or being vulnerable to privacy-invading options over HTTP.", "date": "2015-12-22"},
{"website": "Twitter-Engineering", "title": "Detecting and avoiding bucket imbalance in A/B tests", "author": ["‎@_rchang‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2015/detecting-and-avoiding-bucket-imbalance-in-ab-tests.html", "abstract": "In previous posts, we discussed the motivation for doing A/B testing on a product like Twitter, how A/B testing helps us innovate , and how DDG, our A/B testing framework, is implemented . Here we describe a simple technique for auto-detecting potentially buggy experiments: testing for unbalanced entry rates of users into experiment buckets. Trigger analysis An A/B test maps users to treatment “buckets.” The control bucket (“A”) is the current production experience; the treatment bucket (“B”) implements the change being tested. You can create multiple treatment buckets. Choosing which bucket to show seems simple: randomly and deterministically distribute all user IDs over some integer space, and assign a mapping from that space to buckets. But consider that many experiments change something only a subset of users will see. For example, we might want to change something about the photo editing experience on the iOS app — but not all of our users use iOS, and not all of the Twitter iOS users edit photos. Including all users, regardless of whether they “triggered” the experiment or not, leads to dilution. Even if your feature changed something in the behavior of users who saw it, most did not, so the effect is hard to observe. It’s desirable to narrow the analysis by looking only at users who triggered the change. Doing so can be dangerous, however; conditionally opting people into the experiment makes us vulnerable to bias. Populations of different experiment buckets might not be comparable due to subtleties of the experiment setup, making results invalid. A simple example Let’s imagine an engineer needs to implement a US-specific experiment. She made sure that the treatment is not shown to users outside the US, using the following (pseudo-) code: // bucket the user and record that the user is in the experiment\nbucket = getExperimentAndLogBucket(user, experiment); \n\nif (user.country == “US” &&  bucket == “treatment”) {\n  /** do treatment stuff **/\n} else {\n  /** do normal stuff **/\n} This results in a diluted experiment, since users from all countries get recorded as being in either control and treatment, but only the US “treatment” users actually got to see the treatment. At this point, there is no bias — just dilution. When this is pointed out in code review, the code is changed as follows: // bucket the user, but do not record triggering.\nbucket = peekAtExperimentBucket(user, experiment);\n\nif (user.country == “US”  && bucket == “treatment”) {\n  recordExperimentTrigger(user, experiment, “treatment”)\n  /** do treatment stuff **/\n} else {\n  if (bucket == “control”) {\n    recordExperimentTrigger(user, experiment, “control”)\n  }\n  /** do normal stuff **/\n} This new version looks like it will avoid the dilution problem, since it doesn’t record a bunch of people who didn’t see the treatment as having triggered it. It has a bug, however — there is bias in the experiment. Treatment only gets the US users, while control is seen as getting all the users. Nothing will break in the app, but the two buckets are not comparable. An unbiased solution would look as follows: // bucket the user, but do not record triggering.\nbucket = peekAtExperimentBucket(user, experiment);\n\nif (user.country == “US”  && bucket == “treatment”) {\n  recordExperimentTrigger(user, experiment, “treatment”)\n  /** do treatment stuff **/\n} else {\n  // do not record trigger for non-US visitors, even in control\n  if (user.country == “US” && bucket == “control”) {\n    recordExperimentTrigger(user, experiment, “control”)\n  }\n  /** do normal stuff **/\n} Not all bucketing imbalances have such obvious causes. We recently encountered an experiment which triggered the user bucketing log call asynchronously, from client-side Javascript. The experimental treatment required loading certain additional assets and making other calls, which made the bucketing log call slightly less likely to succeed for users on slow connections. This resulted in bias: users on slow connections were more likely to show up in control than in treatment, subtly skewing the results. Identifying bucket imbalance An effective way to detect bucketing bias is to test for imbalanced bucket sizes. Our experimentation framework automatically checks if buckets are roughly the expected size, using two methods. First, we perform an overall health check using the multinomial goodness of fit test. This test checks if observed bucket allocations collectively matched expected traffic allocations. If overall health is bad, we also perform binomial tests on each bucket to pinpoint which buckets might have problems, and show a time series of newly bucketed users in case experimenters want to do a deep dive. Overall health check using multinomial test Imagine an experiment with k buckets, where bucket i receives p percent of the traffic ( p can be different across buckets). Given that we know the total number of users who are bucketed into the experiment, we can model the number of users in each bucket jointly as a multinomial distribution. Furthermore, we can perform a goodness of fit test to see if the actual observed count deviates from the expected count. If we see that the traffic allocation deviates significantly from what it is supposed to be, this is a good indicator that split of traffic is problematic or that the splitting is biased. Mathematically, the multinomial goodness of test is quite intuitive: Here, O denotes the observed number of users bucketed in bucket i , and E denotes the expected number of users bucketed in each bucket. The statistics capture how much each bucket deviates from its expected value, and the summation captures the overall deviation. The test statistics follow a chi-square distribution (with k-1 degrees of freedom), so we can determine the likelihood of observing a deviation as large as in the current sample had the bucket allocation were to be defined by p ’s. It is worth noting that the multinomial test is a generalization of the binomial test (i.e. often used when there is only 1 control vs. 1 treatment group). In the past, we leveraged the binomial test so that each bucket was compared to its expected bucket count. However, in situations where an experiment has many buckets, we can quickly run into the problems of multiple hypothesis testing. Below, we plot the probability of getting at least one false positive as a function of the number of independent buckets, which is simply 1 - (1 - p)^k where p is the probability of a getting a false positive (often set at 5%). The increase in false positives rate in a properly designed experiment when multiple buckets are involved is fairly high: Under the null hypothesis, with 5 buckets, one would expect to see at least one of them be imbalanced more than 20 percent of the time. False positives lead to a lot of time spent in investigation, and loss of trust in the tool. To address this problem, we evaluated the improvement of switching from binomial test to multinomial. We ran a meta-analysis using 179 experiments, and compared the bucket health results of multinomial and binomial tests. In the case of binomial, an experiment was considered unhealthy if at least one of the bucket has p-value < 0.05, while in the multinomial case, the experiment was labeled unhealthy if the overall p-value is < 0.05. We found a subset of experiments where the binomial test concluded that the experiment is unhealthy while the multinomial test did not. We examined all such experiments, and observed that they shared some characteristics: For all experiments except one, there was only 1 bucket that has a binomial test < 5%. The rest of the buckets tend to have consistently healthy p-values. All of the flagged experiments have 4 or more buckets. These characteristics suggested that the binomial results are likely to be false positives. We then verified that all of these experiments were set up correctly through manual validation. Switching to the multinomial test for overall health of the experiment reduced our false positive rate by an estimate of 25%. Flagging individual buckets The multinomial test can protect us from the woes of multiple hypothesis testing, but it has a disadvantage: it does not tell us which buckets are problematic. To provide more guidance, we run additional binomial tests in cases when the multinomial test flags an experiment. DDG performs a two-sided binomial test using normal approximation of the binomial distribution. The binomial test allows us to check if the traffic is roughly balanced at time t , and it flags abnormal buckets when the actual traffic is outside of the 95% confidence interval of the expected traffic. Time series of bucket count At the lowest level of granularity, our tool also presents the time series of bucket count in an 8 hour batch time window. Below are two examples: a healthy time series, and an unhealthy one. Example 1: healthy time series In the above example, the time series indicates that the number of users in each bucket are balanced across buckets, except a few batches (the color labeling again is determined based on binomial test, at each time t). The two unbalanced batches are not concerning: with the significance level set at 5%, we would expect a false positive 1 out of every 20 tests. The multinomial test is not significant. Overall, we see no evidence for biased bucketing. Example 2: unhealthy time series In this example, we see that a few days after the experiment started, the number of new users started to deviate from the expected traffic, indicating bucketing bias. Looking at the overall bucket health (top right), we also see that the multinomial test indicates that the test is unhealthy. In this case, the tool warns the user to investigate the design before moving to analysis. This combination of batch-level and global testing allows us to detect more subtle problems than either type of test would detect individually. Test only on users, not impressions A properly designed and implemented experiment can have the total number of bucketed impressions vary across buckets due to experiment effects or implementation details. Comparing bucket imbalance based on unique bucketed users is a better test than looking at total triggers or total visits. It is important to note that in our time series tests, we check bucket imbalance on users bucketed for the first time instead of all bucketing events. The experiment itself can cause users to continue triggering the experiment, or to do so less than control. This makes post-exposure impression data inappropriate to compare. In the overall test, we also only compared total number of users bucketed, rather than total impressions scribed across buckets. Conclusion The bucket imbalance check is a powerful, yet simple and convenient way to determine if an experiment might not be set up correctly. This is the very first thing we check to validate experiment results, and building it into our toolchain helped save many hours of investigation and analysis. By automatically checking experiments for clear evidence of bias, we drastically reduce time required to detect a problem and increase the experimenters’ confidence in experiment results.", "date": "2015-12-28"},
{"website": "Twitter-Engineering", "title": "How we break things at Twitter: failure testing", "author": ["Mazdak Hashemi"], "link": "https://blog.twitter.com/engineering/en_us/a/2015/how-we-break-things-at-twitter-failure-testing.html", "abstract": "When you run an infrastructure at Twitter scale, hardware and network failures become unavoidable. Each of these failures has the potential to negatively impact the user experience, so it’s important that we design our systems to be as resilient to failure as possible. To test how our services react to unexpected failures, we created a framework that can inject controlled failure conditions into the Twitter infrastructure. This allows us to discover vulnerabilities so that we’re better prepared to handle a site-wide incident. By causing failures in our own system, we’re able to build more resilient services. Framework Our framework consists of a Python library and a command-line executable which is designed to directly introduce failure conditions into production infrastructure, monitor site-wide health metrics, and notify systems with status updates during the testing procedure. It provides a flexible, YAML-based configuration syntax and is trivially extensible with additional functionality, based around a modular design. The framework consists of three module types: Mischief modules introduce and reverse failure conditions within production services and infrastructure. Monitors modules are designed to check sources of health metrics for Twitter services, looking for states that may lead to a site-wide incident. If at least one of these states is detected, the framework will immediately reverse the failure conditions which have been introduced. Notifiers modules are designed to interface with systems such as HipChat and JIRA to provide live status updates during the execution of a failure test. Targets for failure testing within Twitter’s infrastructure are selected by querying Twitter’s asset inventory database, which is the source of information for all data center hardware and associated attributes. The framework combines these queries, allowing tests to be precisely targeted against machines supporting a single Twitter service (if running on our internal cloud) or machine role. At the moment, the framework supports the introduction of the following failure conditions: Power loss via issuing commands to IPMI controllers and PDUs Partial to full loss of a Twitter service cluster running in Mesos Network loss via pushing new switch configurations which includes switch port failure, partial packet loss, denial of all IP traffic, all traffic to one or more TCP/UDP ports, and more To demonstrate how an engineer might use the framework to test a failure condition, here’s an example of a configuration which will introduce a power loss for 30 minutes across all Hadoop DataNodes in rack abc, monitor health metrics during the test’s execution, and send status updates to a chat room: failure_test:\n  name: Power loss within rack abc in datacenter abcd\n  duration_mins: 30\n\n  mischief:\n    - power_loss:\n        datacenter: abcd\n        selectors:\n          - group:\n              type: role\n              name: hadoop.datanode\n          - group:\n              type: rack\n              name: abc\n\n  notifiers:\n    - chat:\n        rooms:\n          - Failure Testing\n\n  monitors:\n    - observability:\n        datacenter: abcd\n        queries: !snippet Upon passing this configuration to the framework’s command-line tool, it will gather all information necessary to introduce the failure conditions, such as the target machine hostnames and the addresses of their IPMI BMC interfaces. If this mischief preparation step succeeds, the framework will ensure that all systems under test are healthy via checking the configured monitors. It will then attempt to introduce the requested failure conditions. If these conditions are introduced successfully, the framework will pause for the configured test length, checking the configured monitors on a minute-by-minute basis to ensure that all remain in a healthy state. If any monitor detects an unhealthy infrastructure condition, the framework will consider the failure test to be unsuccessful. Otherwise, if monitors have reported healthy conditions throughout the test’s duration, the framework will consider the test to be successful. In both cases, the framework will then immediately reverse all induced failure conditions, terminating the test. A visualization of the testing process: Challenges Introducing a failure condition that targets a specific service requires careful design and planning due to the heterogeneous dynamic infrastructure we run at Twitter. Services hosted in Apache Aurora are different from those that run directly on hardware as they are dynamically scheduled. To determine the root cause of unexpected service behavior, we need to capture the full testing environment, which can include rack profiles, service types, service diversity stats, traffic volume, and more. In some cases, the anomaly we detect is simply a side effect of upstream or downstream issues or a service’s recovery behavior. Usage and lessons learned This framework has driven all failure testing at Twitter over the past six months and has helped us discover numerous vulnerabilities in our stack. In addition, it’s given us confidence in the failure resiliency of several of our primary systems, such as Apache Mesos and Apache Aurora , where we have tested large-scale failures which resulted in no user-facing impact. An example of a common failure condition seen in our infrastructure is when a Top of Rack (TOR) switch goes bad. When this situation occurs, services running within this rack will experience either a total network loss or a partial packet loss. We learned that our services handle the total network loss case well whereas a partial packet loss will almost always cause some internal impact. We additionally found that the severity of this impact varies based on both the rack’s profile and what services are running on Mesos slaves in that rack. The framework is able to identify how Twitter services respond to a wide variety of common hardware failure scenarios, providing site reliability engineers with substantial practical data. Much of this data surrounds how inter-service RPCs, mediated by the powerful Finagle library, behave in the face of failure conditions. Finagle imbues all inter-service RPCs with logic to ensure that each is facilitated as successfully as possible; however, to take full advantage of this logic, downstream service clients need to be properly tuned with correct request timeout and retry values. Using data precipitated by the framework failure testing, our engineers can provide tuning values and graceful degradation logic that can better maintain SLAs and prevent user-facing incidents. Future work Of course, we’ll continue to test our infrastructure’s ability to withstand random failures. This framework has been used to drive exploratory failure testing to help us find and push the limits of Twitter infrastructure. As the scope of our failure testing program increases over the next few months, the framework will run continuously during work hours. By doing so, this framework will help us ensure that our resiliency to hardware failure conditions does not regress from scenarios where it has been repeatedly demonstrated. In addition, we are investigating the injection of failure conditions at the layer of our extensible RPC system, Finagle . Through injecting controlled failure conditions into Twitter infrastructure, we can ensure that our systems will better tolerate unintended failures in production. This allows us to write services that better endure failures and gracefully degrade when needed. And ultimately by running these tests, we’re able to locate and address these vulnerabilities in our infrastructure. If failure and reliability testing sounds like an interesting challenge, we could use your help — join the flock ! Special thanks to Steven Salevan for his contributions to this framework and blog post. Many thanks to Adam Crane , Davin Bogan , Kyle Laplante , Jay Holler , J.P. Doherty , Justin Shields , Jeff Hodge , Maciej Drobny , and Ramya Krishnan .", "date": "2015-12-15"},
{"website": "Twitter-Engineering", "title": "Finatra 2.0: the fast, testable Scala services framework that powers Twitter", "author": ["Christopher Coco"], "link": "https://blog.twitter.com/engineering/en_us/a/2015/finatra-20-the-fast-testable-scala-services-framework-that-powers-twitter.html", "abstract": "Finatra is a framework for easily building API services on top of Finagle and TwitterServer . We run hundreds of HTTP services, many of which are now running Finatra 2.0. And now, we’re pleased to formally announce the general availability of the new Finatra 2.0 framework to the open source community. At Twitter, Finagle provides the building blocks for most of the code we write on the JVM. It has long-served as our extensible, protocol-agnostic, highly-scalable RPC framework. Since Finagle itself is a fairly low-level library, we created TwitterServer — which is how we define an application server at Twitter. It provides elegant integration with twitter/util Flags for parameterizing external server configuration, an HTTP admin interface, tracing functionality, lifecycle management, and stats. However, TwitterServer does not provide any routing semantics for your server. Thus we created Finatra 2.0 which builds on-top of both Finagle and TwitterServer, and allows you to create both HTTP and Thrift services in a consistent, testable framework. What’s new? Finatra 2.0 represents a complete rewrite of the codebase from v1.x. In this release, we set out to significantly increase the modularity, testability, and performance of the framework. We want to make it easy to work with the codebase as well as intuitive to write really powerful tests for your API. New features and improvements include: ~50x speed improvement over the previous v1.6 release in many benchmarks Powerful integration testing support (details in the following section) Optional JSR-330 style dependency injection using Google Guice Jackson JSON enhancements supporting required fields, default values, and custom validations Logback MDC -integration with com.twitter.util.Local for contextual logging across Twitter Futures Finatra builds on top of the features of TwitterServer and Finagle by allowing you to easily define a server and (in the case of an HTTP service) controllers — a service -like abstraction which define and handle endpoints of the server. You can also compose filters either per controller, per route in a controller, or across controllers. Please take a look at the main documentation for more detailed information. Testing One of the big improvements in this release of Finatra is the ability to easily write robust and powerful tests for your services. Finatra provides the following testing features the ability to: start a locally running server, issue requests, and assert responses easily replace class implementations throughout the object graph retrieve classes in the object graph to perform assertions on them write tests without deploying test code to production At a high-level, the philosophy of testing in Finatra revolves around the following testing definitions: Feature tests — the most powerful tests enabled by Finatra. These tests allow you to verify feature requirements of the service by exercising its external interface. Finatra supports both “black-box” and “white-box” testing against a locally running version of your server. You can selectively swap out certain classes, insert mocks, and perform assertions on internal state. Take a look at an example feature test . Integration tests — similar to feature tests but the entire service is not started. Instead, a list of modules are loaded and then method calls and assertions are performed at the class-level. You can see an example integration test . Unit tests — these are tests of a single class and since constructor injection is used throughout the framework, Finatra stays out of your way. Getting started To get started we’ll focus on building an HTTP API for posting and getting Tweets. Our example will use Firebase (a cloud storage provider) as a datastore. The main entry point for creating an HTTP service is the finatra/http project which defines the com.twitter.finatra.http.HttpServer trait. Let’s start by creating a few view objects — case classes that represent POST/GET requests to our service. We’ll assume that we’re using previously created domain objects: Tweet and TweetId . case class TweetPostRequest(\n @ Size(min = 1, max = 140) message: String,\n location: Option[TweetLocation],\n nsfw: Boolean = false) {\n\n def toDomain(id: TweetId) = {\n   Tweet(\n     id = id,\n     text = message,\n     location = location map {_.toDomain},\n     nsfw = nsfw)\n }\n}\n\ncase class TweetGetRequest(\n @ RouteParam id: TweetId) Next, let’s create a simple Controller: @ Singleton\nclass TweetsController @ Inject()(\n tweetsService: TweetsService)\n extends Controller {\n\n post(\"/tweet\") { tweetPostRequest: TweetPostRequest =>\n   for {\n     savedTweet <− tweetsService.save(tweetPostRequest)\n     responseTweet = TweetResponse.fromDomain(savedTweet)\n   } yield {\n     response\n       .created(responseTweet)\n       .location(responseTweet.id)\n   }\n }\n\n get(\"/tweet/:id\") { tweetGetRequest: TweetGetRequest =>\n   tweetsService.getResponseTweet(tweetGetRequest.id)\n }\n} The TweetsController defines two routes: GET  /tweet/:id\nPOST /tweet/ Routes are defined in a Sinatra -style syntax which consists of an HTTP method, a URL matching pattern, and an associated callback function. The callback function can accept either a finagle-http Request or a custom case-class that declaratively represents the request you wish to accept. In addition, the callback can return any type that can be converted into a finagle-http Response . When Finatra receives an HTTP request, it will scan all registered controllers (in the order they are added) and dispatch the request to the first matching route starting from the top of each controller invoking the route’s associated callback function. In the TweetsController we handle POST requests using the TweetPostRequest case class which mirrors the structure of the JSON body posted while specifying field validations — in this case ensuring that the message size in the JSON is between 1 and 140 characters. For handling GET requests, we likewise define a TweetGetRequest case class which parses the “id” route param into a TweetId class. And now, we’ll construct an actual server: class TwitterCloneServer extends HttpServer {\n override val modules = Seq(FirebaseHttpClientModule)\n override def jacksonModule = TwitterCloneJacksonModule\n\n override def configureHttp(router: HttpRouter): Unit = {\n   router\n     .filter[CommonFilters]\n     .add[TweetsController]\n }\n\n override def warmup() {\n   run[TwitterCloneWarmup]()\n }\n} Our server is composed of the one controller with a common set of filters. More generically, a server can be thought of as a collection of controllers (or services) composed with filters. Additionally, the server can define what modules to use and how to map exceptions . Modules are mechanism to help you inject Guice-managed components into your application. They can be useful for constructing instances that rely on some type of external configuration which can be set via a com.twitter.app.Flag . And finally we’ll write a FeatureTest — note, we could definitely start with a feature test but for the purpose of introducing the concepts in a concise order, we’ve saved this part (the best) for last. class TwitterCloneFeatureTest extends FeatureTest with Mockito with HttpTest {\n\n override val server = new EmbeddedHttpServer(new TwitterCloneServer)\n\n @ Bind val firebaseClient = smartMock[FirebaseClient]\n @ Bind val idService = smartMock[IdService]\n\n /* Mock GET Request performed in TwitterCloneWarmup */\n firebaseClient.get(\"/tweets/123.json\")(manifest[TweetResponse]) returns Future(None)\n\n \"tweet creation\" in {\n   idService.getId returns Future(TweetId(\"123\"))\n\n   val savedStatus = TweetResponse(\n     id = TweetId(\"123\"),\n     message = \"Hello FinagleCon\",\n     location = Some(TweetLocation(37.7821120598956, -122.400612831116)),\n     nsfw = false)\n\n   firebaseClient.put(\"/tweets/123.json\", savedStatus) returns Future.Unit\n   firebaseClient.get(\"/tweets/123.json\")(manifest[TweetResponse]) returns Future(Option(savedStatus))\n   firebaseClient.get(\"/tweets/124.json\")(manifest[TweetResponse]) returns Future(None)\n   firebaseClient.get(\"/tweets/125.json\")(manifest[TweetResponse]) returns Future(None)\n\n   val result = server.httpPost(\n     path = \"/tweet\",\n     postBody = \"\"\"\n       {\n         \"message\": \"Hello FinagleCon\",\n         \"location\": {\n           \"lat\": \"37.7821120598956\",\n           \"long\": \"-122.400612831116\"\n         },\n         \"nsfw\": false\n       }\"\"\",\n     andExpect = Created,\n     withJsonBody = \"\"\"\n       {\n         \"id\": \"123\",\n         \"message\": \"Hello FinagleCon\",\n         \"location\": {\n           \"lat\": \"37.7821120598956\",\n           \"long\": \"-122.400612831116\"\n         },\n         \"nsfw\": false\n       }\"\"\")\n\n   server.httpGetJson[TweetResponse](\n     path = result.location.get,\n     andExpect = Ok,\n     withJsonBody = result.contentString)\n }\n\n \"Post bad tweet\" in {\n   server.httpPost(\n     path = \"/tweet\",\n     postBody = \"\"\"\n       {\n         \"message\": \"\",\n         \"location\": {\n           \"lat\": \"9999\"\n         },\n         \"nsfw\": \"abc\"\n       }\"\"\",\n     andExpect = BadRequest,\n     withJsonBody = \"\"\"\n       {\n         \"errors\" : [\n           \"message: size [0] is not between 1 and 140\",\n           \"location.lat: [9999.0] is not between -85 and 85\",\n           \"location.long: field is required\",\n           \"nsfw: 'abc' is not a valid boolean\"\n         ]\n       }\n       \"\"\")\n }\n} In the test, we first create an embedded server . This is an actual instance of the server under test (running locally on ephemeral ports) at which we’ll issue requests and assert expected responses. A quick note here — you do not have to use Guice when using Finatra . You can create a server, route to controllers, and apply filters all without using any dependency injection. However, you won’t be able to take full advantage of all of the testing features that Finatra offers. Having Guice manage the object-graph allows us to selectively replace instances in the graph on an per-test basis, giving us a lot of flexibility in terms of defining or restricting the surface area of the test. Next you’ll see that we bind different implementations to the FirebaseClient and IdService types. Here you see the power of object-graph manipulation. In creating the server in production, the “real” versions of these classes are instantiated. In the test, however, we replace FirebaseClient and IdService with mock instantiations to which we hold references in order to mock responses to expected method calls. Finally, we test specific features of the service: create a new Tweet verify we can read the newly created Tweet back from the service and what happens when we post an “invalid” Tweet to the service and assert that we get back an expected error We recommend taking a look at the full Twitter Clone example project on GitHub for more information. We also have a Typesafe Activator seed template that is available for quickly getting a new Finatra project started. Future work We’re excited about the future of the Finatra framework and are actively working on new features such as improving the Thrift server support. Stay tuned! In the interim you can checkout our public backlog or browse our issues list. Getting involved Finatra is an open source project that welcomes contributions from the greater community. We’re thankful for the many people who have already contributed and if you’re interested, please read the contributing guidelines . For support, follow and/or Tweet at our @finatra account, post questions to the Gitter chat room , or email the finatra-users Google group: finatra-users@googlegroups.com. Acknowledgements We would like to thank Steve Cosenza , Christopher Coco , Jason Carey , Eugene Ma , Nikolaj Nielsen , and Alex Leong . Additionally, we’d like to thank Christopher Burnett ( @twoism ) and Julio Capote ( @capotej ), originators of Finatra v1 for graciously letting us tinker with their creation. Many thanks also to the Twitter OSS group — particularly former members Chris Aniszczyk ( @cra ) and Travis Brown ( @travisbrown ) for all of their help in getting Finatra 2.0 open-sourced. And lastly, we would like to thank the Finatra community for all their contributions .", "date": "2015-12-09"},
{"website": "Twitter-Engineering", "title": "Behind the scenes of enhancements to MoPub data ", "author": ["‎@elfin_ova‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2015/behind-the-scenes-of-enhancements-to-mopub-data-0.html", "abstract": "As a leading monetization solution for mobile app publishers, we’re excited to share some news about major infrastructure improvements on MoPub . We’ve been listening to the developer community and based on feedback we’ve received over the months, we’ve made MoPub better by equipping publishers with faster access to data and faster updates between our UI and ad server. This was an enormous technical feat, so we want to give you a deep dive on how we rebuilt two key pillars of our platform — the data pipeline and propagation pipeline. DATA PIPELINE MoPub’s data pipeline is the backbone of core data tools and features including Publisher UI statistics and offline reports. Our data team has been invested in rebuilding our pipeline from the ground up to ensure it could scale as we grew, handle high traffic volumes, and give users faster access to more consistent data, available by 5am PT. To make all this possible, we needed to overcome the challenges of our previous system, which was made up of multiple components (Hive data processing jobs, multiple Python based steps in post-processing, and migration of data to multiple sources that powered different features in our Publisher UI). Stronger, more scalable systems Leveraging the strength of Twitter’s infrastructure was foundational to overcome these challenges. Migrating into Twitter’s data center provided the scalable resources needed for our systems to be performant. Twitter’s technology stack includes tools for monitoring, job scheduling, and most importantly, access to Tsar — Twitter’s time series aggregation framework. Integrating with Tsar required rewriting our entire pipeline from Python and Hive to Scalding , which allowed us to combine all post-processing and data movement into one single pipeline. This migration to a single pipeline ensures consistency in our data. Faster data availability To reduce and resolve customer issues faster, we put in place a single batch job that runs hourly and outputs to multiple sources. Extensive analysis showed that the majority of our incoming log events are covered in a much shorter window than what we had running at the time. This gave us the flexibility to shorten our window, dramatically speeding up data availability and significantly reducing cost. Speeding up reporting To speed up report generation, we migrated from Postgres to a Vertica which is a data warehouse that helped scale our platform by supporting automatic sharding, compression, fast batch write, and fast SELECTs. By layering advanced data partitioning and segments on top of Vertica’s functionality, we were able to generate reports twice as fast. PROPAGATION PIPELINE MoPub’s propagation pipeline is the caching hierarchy that connects our Publisher UI and ad server at a massive scale. We migrated to a brand new architecture that makes Publisher UI changes take effect faster (in less than one minute), ensures consistency between the Publisher UI and ad server, and eliminates ad server warm-up, in addition to simplifying our system and making it easier to maintain. MoPub’s ad server processes about 335 billion monthly incoming ad requests. To make the required data more readily available to the ad server while minimizing both bandwidth and cost, we pre-package data in compressed morsels that contain all the information the ad server needs and store them in a distributed cache that supports very high reads per second. Stronger, more responsive systems We designed the new architecture so that publishers can serve ads from the first ad request by solving cache “warm up,” an industry-wide problem that causes ad serving failures for low-volume ad units. To do this, we migrated into Twitter’s distributed key value store, Manhattan . Manhattan doesn’t lose data, so the cache is always populated and is built to not drop an ad request. Manhattan also presents a holistic view of the data so we can propagate changes quickly with a single write. Rock-solid cache consistency To monitor and ensure rock-solid cache consistency, we built a self-repairing system that constantly compares data stored in our database with Manhattan. Inconsistencies are automatically detected, logged, and repaired. And since we simplified our system to one layer of cache with a single consistent view, there are no other copies of the data that could become inconsistent. To ensure eventual consistency of replicated data, developers building distributed systems should consider implementing a self-healing mechanism. Publisher UI changes take effect faster Finally, to propagate publisher UI changes to the ad server much faster, we optimized the Postgres queries involved so this now takes 10x fewer queries, and we achieved about a 100x increase in system throughput. This means that publisher UI changes take effect nearly instantaneously. Looking ahead As a rapidly growing platform, we continue to improve, and in some cases, redesign parts of our architecture and frameworks to be more performant, scalable, and reliable. These projects are foundational and enable us to ship products that help make our publisher partners more successful. We’re eager to continue improving our core technology with more updates to come. This post was co-written by two MoPub engineers: Simon Radford (Sr. Software Engineer, propagation pipeline) and Meng Lay (Sr. Software Engineer, data pipeline).", "date": "2015-11-17"},
{"website": "Twitter-Engineering", "title": "Twitter experimentation: technical overview", "author": ["‎@squarecog‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2015/twitter-experimentation-technical-overview.html", "abstract": "In our previous post , we discussed the motivation for doing A/B testing at Twitter, and how A/B testing helps us innovate. We will now describe how the backend of Twitter’s A/B system is implemented. Overview The Twitter experimentation tool, Duck Duck Goose (DDG for short), was first created in 2010. It has evolved into a system that is capable of aggregating many terabytes of data such as Tweets, social graph changes, server logs, and records of user interactions with web and mobile clients, to measure and analyze a large amount of flexible metrics. At a high level, the flow of data is fairly straightforward. Data processing pipeline An engineer creates an experiment via a web UI, and specifies a few details: Who is eligible for this experiment? We might want to restrict this experiment to specific languages, countries, operating systems, and so on. What are the treatment buckets? We might have several alternative implementations or designs to test against the current production build, the “control.” What is the experiment hypothesis? How will this change affect user behavior? Which metrics are expected to move? What metrics should be tracked? In addition to metrics directly relating to the experiment hypothesis, a number of other metrics may make sense to track. Some metrics are tracked for all experiments; others can be created on the fly during experiment creation, or picked up from a library of other, existing metrics. DDG then gives the engineer a bit of code to use to check which treatment should be shown to a user. To a feature developer, this is simply a kind of “Feature Switch,” a generic mechanism used to control feature availability. We log an “ab test impression” any time the application decides whether a user is an experiment. Delaying such decisions until a user is going to be affected by the experiment increases statistical power. Data about Twitter app usage is sent to the event ingest service. Some lightweight statistics are computed in a streaming job using TSAR running on Heron . The bulk of the work is done offline, using a Scalding pipeline that combines client event interaction logs, internal user models, and other datasets. The Scalding pipeline can be thought of as having three distinct stages. First, we aggregate the raw sources to generate a dataset of metric values on a per-user, per-hour basis. The result looks something like this: This creates input data for the next stage, as well as a resource for analyses other than experimentation — top-level metric calculations, ad-hoc cohorting, and so on. We then join these per-user metrics with information about experiment ab test impressions, and calculate aggregate values per metric, per user, during the experiment’s runtime. Since a user might enter different experiments at different times, such an aggregate can have different values in different experiments. We also record the time the user first entered the experiment, whether they were a new, casual, or frequent user at that time, and other metadata. This allows for experiment result segmentation and measurement of changes in attribute values during the course of the experiment. Results of this second stage are great for deep dives into experiments and research into alternative analysis approaches — they let us iterate on different aggregation techniques, stratification approaches, various algorithms for dealing with outliers, and so on. Finally, a third stage of aggregation runs which rolls up all experiment data: This is the final experiment result data that gets loaded into Manhattan and served to our product teams via internal dashboards. Defining metrics DDG is a platform meant to allow measurement of very different features, some of them not invented yet. This means we need to balance predictability and stability of metric definitions with a large amount of flexibility. We offer three types of metrics, in descending order of centralized control and specification: Built-in metrics are defined and owned, for the most part, by the experimentation team; these core metrics like “number of Tweets” or “number of logins.” They get automatically tracked for all experiments. Experimenter-defined, configured metrics are created by specifying, using a lightweight DSL, what “events” should be counted. Many metrics can be defined as simply counting all rows in a general client event log that match some feature-specific predicate. The DDG pipeline evaluates these predicates and does all the computation for the experimenter. Imported metrics are completely owned and generated by Twitter engineers. Experimenters can create their own aggregates in the form of Table 1 above, and get them imported into the system. Generating one’s own aggregates is more work than the other two options, but allows for maximal flexibility with regards to data sources and transformation and aggregation logic. To help experimenters find the right sets of metrics, and keep metric definitions correct and current, metrics are collected and organized into “metric groups.” Each metric group is owned and curated by teams that create them. Version history, ownership, and other attributes are tracked for all metric groups. This encourages sharing and communication among experimenters. As the number of interesting combinations of tracked events, and number of experimenters, grows over time, redundant metrics sometimes get created. This can cause confusion (“What’s the difference between the built-in “Foobar Quality” metric, and the “Quality of Foobar” metric that Bob defined?”). An interesting project on our “TODO” list is creating a way to automatically identify metrics that appear to measure mostly the same thing, and suggest metric reconciliation. Scaling the pipeline Getting the aggregation pipeline to run efficiently is one of the biggest challenges in this system. The interactions data source alone is responsible for hundreds of billions of events on a daily basis; relatively small inefficiencies can significantly impact total runtime and processing cost. We found that lightweight, constant profiling of Hadoop Map-Reduce jobs is important for quick analysis of performance problems. To that effect, we worked with the Hadoop team to make on-demand task jvm profiling available in our Hadoop build (by implementing YARN-445 and a number of follow-up items), as well as instrumenting one-click thread dumps and turning on automated XProf profiling for all tasks. Through profiling we found a number of opportunities to improve efficiency. For example, we found a few places to memoize results of custom metric event matching. We made a pass to replace strings with generated numeric ids when possible. We also used a number of tricks specific to Hadoop Map-Reduce: sort and spill buffer tuning for map and reduce stages, data sorting to achieve maximal map-side collapse during aggregations, early projection, and so on. During a Hack Week in early 2015, we noticed that a large amount of time was spent inside Hadoop map tasks’ SpillThread, which is responsible for sorting partial map outputs and writing them to disk. A large fraction of the SpillThread was spent deserializing output keys and sorting them. Hadoop provides a RawComparator interface to let Hadoop power users avoid this, but it wasn’t implemented for Thrift objects we were using. We built a prototype that implemented a generic RawComparator for Thrift-serialized structures, and benchmarked the gains. Our prototype cut a few corners, and the benchmark tested a worst-case scenario, but the resulting 80% gain was significant enough that we recruited a couple of engineers from the Scalding team to really implement this idea for Thrift, Scala Tuples, and case classes. This turned into the OrderedSerialization feature released in Scalding 0.15. Turning this on for the DDG jobs resulted in 30% savings of overall compute time! More details about this work can be found in the “ Performance Optimization At Scale ” talk that the Scalding team delivered at Hadoop Summit 2015. Finally, we have two levels of defense to ensure that we do not introduce performance regressions: prevention and detection. To prevent regressions, in addition to regular unit tests, we have automation that allows us to run a full end to end pipeline in a staging environment, and compare both the results (to ensure correctness) and all Hadoop counters (to check for performance regressions). To detect performance problems if they do happen in production, we created a Scala trait which allows Scalding jobs to export all Hadoop counters to Twitter’s internal observability infrastructure . This means that we can easily generate dashboards for our Scalding jobs using common templates, create alerts for problems like not running or running for too long, check on certain classes of permissible errors happening at too-high a rate, and more. Conclusion The infrastructure required to power the Twitter experimentation platform is quite extensive, largely due to the sheer volume of data that needs to be processed to analyze the experiments. The system must balance flexibility of available metrics with predictability and ease of analysis; the pipeline is designed to produce data at multiple granularities to enable different kinds of analysis. A great amount of effort goes into making processing efficient, including automated full-scale testing and continuous improvements to profiling and monitoring capabilities in Hadoop. Acknowledgements A number of Twitter engineers worked on these tools over the years; we want to particularly acknowledge contributions by Chuang Liu , Jimmy Chen , Peter Seibel , Zachary Taylor , Nodira Khoussainova , Richard Whitcomb , Luca Clementi , Gera Shegalov , Ian O’Connell , Oscar Boykin , Mansur Ashraf , and others from PIE, Scalding, and Hadoop teams.", "date": "2015-11-06"},
{"website": "Twitter-Engineering", "title": "Evaluating language identification performance", "author": ["‎@tm‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2015/evaluating-language-identification-performance.html", "abstract": "Understanding the content of Tweets is important for many reasons: grasping a user’s interests (which in turn lets us show more relevant content), improving search, and fighting spam. There are many steps involved in a typical natural language processing pipeline, but one of the first and most fundamental steps is language identification — determining the language in which a piece of text is written. This is generally not a hard problem. Even with a small and simple model (e.g., list of most common words in each language), we can achieve near-perfect accuracy when classifying news articles [1], for example. However, Tweets are different from the average news article or web page: they’re very short and use informal language. In practice, they’re different enough that we don’t want to evaluate language classifiers on news articles, because that doesn’t tell us much about their performance on Tweets. What we need, then, is a golden set of language-annotated Tweets to evaluate on. So, how did we go about constructing it? Measuring accuracy on all of Twitter To measure performance on Twitter overall, we can simply take a uniform sample of all Tweets and manually annotate it. The problem is that each annotator can recognize only one or two languages, and it’s prohibitively inefficient and expensive to have every annotator look at every Tweet. Therefore, we annotated every Tweet with three independent software packages (see Semi-automatic annotation, below) and then kept the majority language as “likely.” All Tweets with likely language X were given to native speakers of X to annotate. The annotators were instructed to either enter the true language (if they recognized it) or skip the Tweet. For all skipped Tweets, we then determined the second most-likely language and gave the Tweet to annotators again. For the Tweets that got skipped even in the second round, we manually investigated the user’s profile and used internet resources (dictionaries, search, etc.) to determine the true language. For 0.18% of Tweets, we were unable to determine the language; because it’s a tiny number, we simply discarded them. In total, we ended up with 120,575 annotated Tweets. This uniformly sampled dataset is available for download (see below). Measuring classifier performance for rare languages The uniformly sampled dataset theoretically allows us to measure overall accuracy, as well as precision and recall for each language. However, we can detect over 60 languages, and only 17 of them have at least 100 Tweets in this dataset. The fewer Tweets we have in a certain language, the less we can say about that language’s performance. For example, based on just five Czech Tweets, it is impossible to say with any reasonable confidence what the actual precision and recall are of our Czech classifier. We therefore built two more datasets. Recall We construct a dataset for each language separately. Let’s take German as an example; we follow the same procedure for all languages. To measure recall of the German classifier, we need an unbiased [2] sample of all Tweets in German. We cannot simply task a human annotator to go through all Tweets, as they would need to read hundreds of thousands of English Tweets to gather 1000 German ones [3]. Instead, we use the following heuristic procedure: Determine the set of users U who Tweeted in German (according to our classifier) at least 10% of the time between March and May 2014 Randomly sample 2000 Tweets created by users from U in July 2014 Use human annotators to discard all non-German Tweets from the set of 2000 We call this the recall-oriented dataset . The data collection strategy is obviously not uniform; how certain can we be that it provides an accurate estimate of recall? For the 10 biggest languages, we measured our classifier’s recall on three datasets: the uniformly sampled dataset dataset of geo-tagged Tweets from Germany the recall-oriented dataset as described above. The differences in recall estimates were statistically indistinguishable using a 95% confidence interval [4]. For smaller languages, we don’t always have enough data to rely on geo tags or the uniform stream, which is why we used the “10% heuristic” outlined above for all languages. While our 3-way comparison test on the 10 big languages does not strictly guarantee that the recall-oriented dataset is a good recall estimator for small languages as well, it did assuage our concerns significantly. To get even better recall estimates for the large languages, we suggest using the union of the uniformly sampled dataset and the recall-oriented dataset. Precision Measuring precision (on the full-Twitter language distribution) is trivial. Grab, say, one thousand random Tweets for which your classifier (let’s call it v1) triggers, and have them human-annotated with two labels: German or non-German. We call this the precision-oriented dataset , and we can use it on its own or union it with the uniformly sampled dataset (to get even better estimates for the large languages). A major caveat is that if we later want to evaluate a different algorithm (let’s call it v2; it could be an improved variant of v1, or a completely different algorithm), no amount of tweaking our existing data can give us the true performance numbers. Consider the following Venn diagram: Our precision-oriented dataset samples the dotted area (i.e. the overlap between “predicted German v1” and “true German” well). But in the shaded area, (almost) all Tweets are not human-annotated, so we cannot say much about the precision there. We have a sample of all true German Tweets in our recall-oriented dataset, but the sample is so small (<0.0001%) that its intersection with “predicted German v2” is not informative. A rough estimate of v2’s performance is to ignore the shaded part of the Venn diagram, and simply evaluate on the precision-oriented dataset created with v1. The more dissimilar v1 and v2 are, the rougher the estimate. Still, it’s better than nothing, which is why we’re also releasing the prediction-oriented dataset created with an old version of Twitter’s language identifier. A note on classifier comparison and distribution skew Another use of the recall-oriented dataset is to measure precision on hypothetical data with a balanced language distribution (i.e., the same number of Tweets for every language). While not a commonly encountered setting in real life [5], it is the “fairest,” most use-case agnostic way of comparing disparate classifiers. It is used for example by Mike McCandless and the author of the “language-detection” java package. However, in practice it is more common to evaluate precision on datasets with a real-world skew from some specific use case; our equivalent of that is the uniformly sampled dataset. A good overview paper is Cross-domain Feature Selection for Language Identification (2011) by Marco Lui ( @saffsd ) and Timothy Baldwin ( @eltimster ); see also Evaluation of Language Identification Methods (2005) by Simon Kranig for older experiments. Note that at world scale (all Tweets, all web pages, etc.), the language distribution is extremely skewed, and measuring precision on the balanced (i.e., recall-oriented) dataset can give very deceiving results. For example, our internal classifier at Twitter labels English Tweets with 99% precision, but on the recall-oriented dataset, its precision is 70%! That’s because the classifier over-triggers on some smaller languages (e.g., Dutch). Those errors are negligible in the wild, because we see very few Dutch Tweets compared to English ones; but in the recall-oriented dataset, Dutch Tweets represent roughly 1/68 of all data, just like English and the other 66 languages. Language distribution skew is also the reason we cannot reuse the recall-oriented dataset to measure precision even with reweighting. For example, if English, German, and Slovenian Tweets appeared at a ratio of 10:3:1, we could create 10 virtual copies of every golden English Tweet, and similarly for German, then evaluate. However, this approach breaks down when we realise that the ratio English:Slovenian is closer to 500:1. Imagine that in our recall-oriented dataset, which has about 1500 Tweets per language, the Slovenian classifier triggers on 1200 Slovenian Tweets and a single English Tweet. Taking the 95% confidence interval , we can conclude that between 0.007% and 0.311% of all English Tweets get labeled as Slovenian. Now let’s apply the 500:1 weighting: the Slovenian classifier is expected to trigger on 1200 Slovenian Tweets and 0.007%*500*1500=52 to 2325 English Tweets. So Slovenian precision is estimated to be between 1200/(1200+52)=95.8% and 1200/(1200+2325)=34% — hardly useful. We neglected and simplified several details in this example, but the core problem remains the same. The curse of vague Tweets Human annotation is always a hairy task with some inconsistency involved. For the majority of Tweets, it is not questionable what the main language is. But there are also a number of Tweets that are linguistically ambiguous or contain more than one language. To keep the complexity of the annotation task reasonable, we decided to use a single label for all such cases: “und” for “undefined” [6]. To make the labeling process as predictable and consistent as possible, the annotators were given the following instructions: Please help us determine the language in which Tweets are written. Possible answers: A language code \"xx\" (choose from list of possible codes). Choose this if a person HAS to speak xx to understand most of the Tweet or all of it, and speaking ONLY xx is enough to understand most or all of the Tweet. Undefined. Choose this if any of the following applies: • the Tweet can be interpreted in multiple languages (words used by multiple languages, interjections (\"haha!\", \"yay\"), proper names, emoticons, ...) • the Tweet strongly mixes languages and does not have a clear \"main\" language • the Tweet is gibberish, not written in any language (e.g. \"#HarryStyles alskdfbasfd\") • the Tweet is written in an actual language not available on our list. In this case, please mark this in the Comments column. Leave empty. If you are unable to provide an answer (i.e. you think the Tweet is written in an actual language, but do not recognize the language), do not enter anything. It's OK to leave rows empty if you don't recognize the language -- you should not need to spend more than 10 seconds on a Tweet, and usually much less. We also presented annotators with the following borderline examples that were intended to calibrate them: We replaced all at-mentions in Tweets with “ @xzy ” prior to presenting them to users, because usernames cannot be translated and do not inherently have a language. For example, we felt that “ @common_squirrel Wow!” should not be labeled as English, because “wow” could come from a number of languages, and “ @common_squirrel ” will always be “ @common_squirrel ”, even in a German Tweet. In addition, usernames are limited to ASCII only. Conversely, we did not obfuscate hashtags as they are free-form (can be translated/adapted to other languages) and support all scripts. We ran out of resources to evaluate inter-annotator agreement, and solicited only one label per Tweet. Because each language was annotated by a different set of annotators, and because languages vary in how unique/confusable they are, inter-annotator agreement would have to be measured for each language separately. Semi-automatic annotation The more Tweets we can annotate, the smaller the error bars on our performance estimates. We therefore tried to expend human annotation resource on only those Tweets where it was not possible to very reliably determine the language automatically. Luckily, this turns out to be the minority of all Tweets. For every Tweet that needed to be language-annotated, we first ran three independent langid algorithms on it: Twitter’s internal algorithm, Google’s CLD2 ( https://code.google.com/p/cld2/ ), and langid.py ( https://github.com/saffsd/langid.py ). If they all assign the same language label, we assume this is the true label, without consulting human annotators. Informal evaluation shows this “triple-agreement” method has <1% error rate (no errors detected in a few minutes’ scanning of output). The three algorithms agree on about two-thirds of all Tweets. Data Download The annotated Tweets are available for download for anyone to evaluate their language identification algorithms: uniformly_sampled.tsv — a uniform sample of all Twitter data; 120575 rows recall_oriented.tsv — about 1500 Tweets per true language precision_oriented.tsv — about 1000 Tweets per language as determined by an old version of Twitter’s internal language identifier In addition, precision_oriented.tsv contains language codes like not-en , which indicates this tweet is not English, though we don’t know its actual language. All Tweets are from July 2014 and cover 70 languages: am (Amharic), ar (Arabic), bg (Bulgarian), bn (Bengali), bo (Tibetan), bs (Bosnian), ca (Catalan), ckb (Sorani Kurdish), cs (Czech), cy (Welsh), da (Danish), de (German), dv (Maldivian), el (Greek), en (English), es (Spanish), et (Estonian), eu (Basque), fa (Persian), fi (Finnish), fr (French), gu (Gujarati), he (Hebrew), hi (Hindi), hi-Latn (Latinized Hindi), hr (Croatian), ht (Haitian Creole), hu (Hungarian), hy (Armenian), id (Indonesian), is (Icelandic), it (Italian), ja (Japanese), ka (Georgian), km (Khmer), kn (Kannada), ko (Korean), lo (Lao), lt (Lithuanian), lv (Latvian), ml (Malayalam), mr (Marathi), ms (Malay), my (Burmese), ne (Nepali), nl (Dutch), no (Norwegian), pa (Panjabi), pl (Polish), ps (Pashto), pt (Portuguese), ro (Romanian), ru (Russian), sd (Sindhi), si (Sinhala), sk (Slovak), sl (Slovenian), sr (Serbian), sv (Swedish), ta (Tamil), te (Telugu), th (Thai), tl (Tagalog), tr (Turkish), ug (Uyghur), uk (Ukrainian), ur (Urdu), vi (Vietnamese), zh-CN (Simplified Chinese), zh-TW (Traditional Chinese). There is a smattering of other language codes present in the data as an artifact of our labeling process, but Tweets in those languages were not collected systematically. To retrieve the text, use the Twitter API. An example of efficiently fetching the Tweets 100 at a time using the statuses/lookup API endpoint, the twurl command-line utility, and jq for JSON parsing: cat >/tmp/fetch.sh <<EOF #!/bin/bash sleep 5 twurl \"/1.1/statuses/lookup.json?id=$(echo $@ | tr ' ' ,)&trim_user=true\" | jq -c \".[]|[.id_str, .text]\" EOF cat uniformly_sampled.tsv | cut -f2 | xargs -n100 /tmp/fetch.sh > hydrated.json Make sure you’ve completed the OAuth setup for twurl (see “Getting started” on their github page) before running the above commands. The code above observes the API rate limits by sleeping between requests. It silently skips Tweets that have been removed, and you will need to join the hydrated.json file with original language labels in uniformly_sampled.tsv. Acknowledgements Everyone on our team contributed to handling the annotation process, helped work out the methodology kinks, and occasionally labeled a tweet or two themselves. Big thanks to Gianna Badiali ( @gianna ), Eden Golshani ( @edeng ), Hohyon Ryu ( @nlpenguin ), Nathaniel Okun ( @natoparkway ), and Sumit Shah ( @bigloser ). Footnotes [1] An early paper by Grefenstette (1995) evaluates this simple technique on newspaper articles and for a limited set of European languages. They achieve near-perfect accuracy for the big European languages on sentences with 20+ words. [2] Unbiased in the sense that German Tweets in our sample should be statistically indistinguishable from all German Tweets. They should have the same distribution of character ngram frequencies, word frequencies, emoticon and emoji usage, etc. [3] And literally billions of English Tweets to gather 1,000 Tibetan ones. [4] Interesting detail: We were originally afraid that the recall-oriented dataset might overestimate recall because it’s based on users that our own algorithm originally recognized as German, creating a possible positive feedback loop. Against our expectations, the recall as measured on the recall-oriented dataset is consistently slightly lower. We hypothesize that this is because of how Tweets were presented to annotators: when constructing the German recall-oriented dataset, all candidate Tweets were presented only to German annotators. For a borderline German/Swedish Tweet, it’s easy to imagine a sloppy German annotator to have a bias to yes and label it as German. Conversely, when constructing the uniformly sampled dataset, a borderline German/Swedish Tweet had a good chance of having been looked at by a Swedish annotator. [5] A possible exception are environments with very few languages, e.g. a Canadian blog with a 60:40 English:French split in content. The recall-oriented dataset is well suited to estimating performance there. [6] ISO-693-3 suggests more expressive special labels , and uses “und” to essentially mean “unlabeled”. This is the only place where we deviate slightly from the BCP-47 standard.", "date": "2015-11-16"},
{"website": "Twitter-Engineering", "title": "The what and why of product experimentation at Twitter", "author": ["‎@squarecog‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2015/the-what-and-why-of-product-experimentation-at-twitter-0.html", "abstract": "Experimentation is at the heart of Twitter’s product development cycle. This culture of experimentation is possible because Twitter invests heavily in tools, research, and training to ensure that feature teams can test and validate their ideas seamlessly and rigorously. The scale of Twitter experiments is vast both in quantity and variety — from subtle UI/UX changes, to new features, to improvements in machine learning models. We like to view experimentation as an endless learning loop: Build a hypothesis: Come up with an idea for a new feature or an improvement to an existing one. Define success metrics: Estimate the “opportunity size” — number of users who will be affected by the change. Formally define success and failure metrics for your experiment; consider what trade-offs are acceptable. Test hypothesis: Implement the proposed change, instrument appropriate logging, and perform sanity checks to make sure an experiment is set-up correctly. Learn: Examine the data collected in the experiment to see what we can learn from it, and share with other Twitter teams. Ship : Having gathered the data, determine if the experiment validated the hypothesis, and make the ship or no-ship decision. Build another hypothesis: Generate more hypotheses for more improvements, incorporating new ideas from the experiment. A/B Testing, decision-making, and innovation The Product Instrumentation and Experimentation (PIE) team at Twitter thinks about philosophy of experimentation a lot. A/B testing can provide many benefits, but it has well-known, easy-to-hit pitfalls . Its results are often unexpected and counter-intuitive . How do we avoid the pitfalls? When should we recommend running an A/B test to try out a feature or proposed change? How do we stay nimble and take big risks while also staying rigorous in our decision-making process? The benefits of testing, and a little about incrementalism The big concern about a culture of A/B testing feature changes is that it leads to small incremental gains, and that most experiments only move the needle by single-digit percentages, or even fractions of a percent. So, the argument goes, what’s the point? Why not work on something more impactful and revolutionary? It’s true: by far the majority of experiments move metrics in a minimal way, if at all; an experiment that moves a core metric by a few percentage points for a large fraction of users is considered an exceptional success. This is not due to something fundamental about A/B testing. This is because a mature product is hard to change in a way that moves the metrics by a large margin. A lot of ideas people think are home-runs simply do not move the needle: humans turn out to be pretty bad at predicting what will work (for more on this, see “ Seven Rules of Thumb for Web Site Experimenters ”). Most of the time, poor A/B test results allow us to find out early that an idea that sounds good may not work. We’d rather get the bad news and go back to the drawing board, as early as possible; so we experiment. A/B testing is a way to make sure good ideas don’t die young, and are given the opportunity to develop fully. When we really believe in an idea, and initial experiment results don’t fulfill our expectations, we can make further changes to the product, and continue improving things until they are ready to be shipped to the hundreds of millions of people. The alternative is that you create something that feels good, you ship it, move on to some new idea, and a year later someone realizes no one is using the feature, and it gets sunset quietly. Quickly iterating and measuring the effects of proposed changes lets our teams incorporate implicit user feedback into the product early on, as we work through various prototypes. We are able to ship a change, look at what’s working and what isn’t, create a hypothesis for what further changes would improve the product, ship those, and keep going until we have something that’s ready to launch widely. Some may look at incremental changes as insufficient. Certainly, shipping a “big idea” sounds far better than a small improvement. Consider, however, that small changes add up, and have a compounding effect. Shying away from incremental changes that materially improve the product is simply not a good policy. A good financial portfolio balances safe bets that give predictable, albeit less than astronomical, return, and some higher-risk, higher-reward bets. Product portfolio management is not very different in that regard. That said, there are lots of things we cannot, or perhaps should not, test. Some changes are designed to result in network effects which user-bucketed A/B testing won’t capture (although other techniques do exist to quantify such effects). Some features do not work when given only to a random percentage of people. For example, Group DMs are not a feature to use in a plain A/B test, because chances are the lucky people who get the feature would want to message those who don’t have it, which makes the feature essentially useless. Others might just be totally orthogonal — e.g., rolling out a new app like Periscope is not a Twitter application experiment. But once it’s out, A/B testing becomes an important way to drive measurable incremental and not-so-incremental changes within that app. Yet another class of changes is major new features that are tested in internal builds and via user research, but released in a big-bang moment to all the customers in a given market for strategic reasons. Such a decision is made when, as an organization, we believe it’s the right thing to do for the product and the customer. We believe there are greater gains to be had from a large release than there are from incremental changes that lead, perhaps, to a better initial release that might get more customers trying it and using it. It’s a trade-off the product leadership chooses to make. And are we going to A/B test incremental changes to such a new features after they launch? You bet! As ideas mature, we guide their evolution using well-established scientific principles — and experimentation is a key part of the process. Experimenting responsibly Now that we’ve made the case for running experiments, let’s discuss what one can do to avoid the pitfalls. Experiment setup and analysis is complex. It is very easy for normal human behavior to lead to bias and misinterpretation of the results. There are several practices one can implement to mitigate the risks. Requiring a Hypothesis Experimentation tools generally expose a lot of data, and frequently allow experimenters to design their own custom metrics to measure effects of their changes. This can lead to one of the most insidious pitfalls in A/B testing: “cherry-picking” and “ HARKing ” — choosing from among many data points just the metrics that support your hypothesis, or adjusting your hypothesis after looking at the data, so that it matches experiment results. At Twitter, it’s common for an experiment to collect hundreds of metrics, which can be broken down by a large number of dimensions (user attributes, device types, countries, etc.), resulting in thousands of observations — plenty to choose from if you are looking to fit data to just about any story. One way we guide experimenters away from cherry-picking is by requiring them to explicitly specify the metrics they expect to move during the set-up phase. Experimenters can track as many metrics as they like, but only a few can be explicitly marked in this way. The tool then displays those metrics prominently in the result page. An experimenter is free to explore all the other collected data and make new hypotheses, but the initial claim is set and can be easily examined. Process No matter how good the tools are, a poorly set up experiment will still deliver poor results. At Twitter, we have invested in creating an experimentation process that improves one’s chances of running a successful, correct experiment. Most of the steps in this process are optional — but we find that having them available and explicitly documented greatly decreases time lost to re-running experiments to collect more data, waiting on app store release cycles, and so on. All experimenters are required to document their experiments. What are you changing? What do you expect the outcomes to be? What’s the expected “audience size” (fraction of people who will see the feature)? Collecting this data not only ensures that the experimenter considered these questions, but also allows us to build up a corpus of institutional learning — a formal record of what’s been tried, and what the outcomes were, including negative outcomes. We can use this to inform future experiments. Experimenters can also take advantage of Experiment Shepherds. Experiment Shepherds are experienced engineers and data scientists who review experiment hypotheses and proposed metrics to minimize the chances of experiments going awry. This is optional, and recommendations are non-binding. The program has received great feedback from people who participate in it, as they have much more confidence that their experiment is set up correctly, that they are tracking the right metrics, and that they will be able to correctly analyze their experimental results. Some teams also have weekly launch meetings, in which they review experimental results to determine what should and should not launch to a wider audience. This helps with addressing issues like cherry-picking and misunderstanding of what statistical significance is saying. It’s important to note this is not a “give me a reason to say no” meeting — we’ve definitely had “red” experiments ship, and “green” experiments not ship. The important thing here is to be honest and clear about the expectations and results of the changes we are introducing, not to tolerate stagnation and reward short-term gains. Introducing these reviews has significantly raised the overall quality of the changes we ship. It’s also an interesting meeting, because we get to see all the work that’s occurring on the team, and how people are thinking about the product. Another practice we employ frequently is using “holdbacks” when possible — rolling out a feature to 99% (or some other high percentage) of users, and observing how key metrics diverge from the 1% that was held back over time. This allows us to iterate and ship quickly, while keeping an eye on the long-term impact of the experiment. This is also a good way to validate that gains expected from the experiment actually materialize. Education One of the most effective ways to ensure that experimenters watch out for pitfalls is simply to teach them. Twitter Data Scientists teaches several classes on experimentation and statistical intuition, one of which is in the list of classes all new engineers take in their first few weeks at the company. The goal there is to familiarize engineers, PMs, EMs and other roles with the experimentation process, caveats, pitfalls, and best practices. Increasing the awareness of the power and pitfalls of experimentation helps us avoid losing time on preventable mistakes and misinterpretations, getting people to insights faster and improving cadence as well as quality. Coming soon In upcoming posts, we will describe how DDG, our experimentation tool, works; we will then jump straight into several interesting statistical problems we have encountered — detecting biased bucketing, using (or not using) a second control as a sanity check, automatically determining the right bucket size, session-based metrics, and dealing with outliers. Acknowledgements Thanks to Lucile Lu , Robert Chang , Nodira Khoussainova , and Joshua Lande for their feedback on this post. Many people contributed to the philosophy and tooling behind experimentation at Twitter. We’d particularly like to thank Cayley Torgeson , Chuang Liu , Madhu Muthukumar , Parag Agrawal , and Utkarsh Srivastava .", "date": "2015-10-23"},
{"website": "Twitter-Engineering", "title": "Twitter goes to #GHC15", "author": ["‎@amyt‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2015/twitter-goes-to-ghc15.html", "abstract": "Every year, the Grace Hopper Celebration of Women in Computing is a wonderful gathering, and this year’s was another wildly successful turnout. Some 12,000 attendees came – a 50% increase over last year. The vibe at #GHC15 was exciting, especially for people who had never been before. A team of us from Twitter @WomEng and Recruiting started a #WhyIAttendGHC campaign to get some insight into what resonates with conference-goers. Tweets with this hashtag cited connecting with amazing women engineers, learning from inspiring leaders, and discovering new career opportunities. For a second time, Sheryl Sandberg was invited to give another motivational chat at Thursday’s main tent session. Reshma Saujani , Founder and CEO of Girls Who Code, led an executive panel with our CEO Jack Dorsey , Chelsea Clinton , and Maxine Williams , Facebook’s Global Director of Diversity, to discuss the all-too-thin pipeline for technical women. These leaders discussed their companies’ policies and strategies for increasing diversity in tech. Our #TwitterGHC team had a strong presence at this year’s gathering; we sent 30+ Tweeps to represent. Our #GHC15 Twitter Team A few days earlier, we held our very first @WomEng Grace Hopper Fellowship program , which included 10 computer science and engineering students together with six Twitter mentors. We hosted these 10 students at #GHC15 (covering their registration, travel, food, and lodging), and organized lots of workshops and activities like tech talks, mock interviews, mentoring sessions, and team bonding fun, including as a cooking class and a trip to the NASA Johnson Space Center. By the end of the week, these 10 remarkable young women had become good friends and a part of our Twitter family. Our #GHC15 @WomEng Fellows At #GHC15 itself, we led six talks with our own Tweeps, covering topics from data science (in two separate talks presented by Ayse Naz Erkan and Rowan Vasquez ), natural language processing ( Gianna Badiali ) and RESTful APIs ( Beth Andres-Beck ) to reliability at scale ( Elif Dede ) and green data centers ( Jennifer Fraser ). We also held several Twitter events, including an industry brunch with Jack, Nandini Ramani (VP of Engineering), and Sharon Ly (Engineering Manager and @WomEng Lead); hosted a dinner for candidates with Nandini and Jeremy Rishel (VP of Engineering) and Kevin Weil (SVP of Product); and held a networking event with a diversity panel discussion that featured Pinterest, Airbnb, Uber, Pandora, and Twitter. This panel of women shared their ideas on addressing the diversity gap at these companies and attracted some 150 women engineers. Throughout the three day conference, our Twitter booth saw consistently high traffic. This year, we organized giveaway games that involved the Twitter platform. For example, if a conference attendee could show us that their #GHC15 Tweet got Retweeted at least 25 times, they could win an iPad mini or a Star Wars LEGO™set. The most inspiring Tweet for this particular contest: https://twitter.com/geekettebits/status/654724402991394817 What’s more, to show support for the moms and dads in the crowd, a new giveaway this year was a Twitter onesie. These went fast! Related: #GHC15 wisely continued to offer childcare and on-site nursing rooms at no charge to conference attendees. In short, #GHC15 was a huge success. It was so inspiring to see so many women come together to support each other in our field. We were very glad to be part of the action, and can’t wait to go again next year!", "date": "2015-10-22"},
{"website": "Twitter-Engineering", "title": "Improving communication across distributed teams", "author": ["‎@vijayp‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2015/improving-communication-across-distributed-teams.html", "abstract": "As a global company, Twitter has offices in numerous locations around the world. Optimizing collaboration and improving productivity between employees across offices is a challenge faced by many companies and something we’ve spent a great deal of time on. For our part, we’ve learned a few lessons about what it takes to make distributed engineering offices successful. Over the next few months, we’ll share what we’ve learned. We’ll also periodically offer workshops about how to do distributed engineering well. The first of these will be on Sept. 24 at General Assembly in San Francisco (shameless plug!). In this post, we’ll describe the value, modes, and importance of communications between employees in different offices. We’ll share some data about how we do things, describe some of the common pitfalls that we identified, and how we’ve improved efficiency by addressing them. In a TechCrunch interview, Alex Roetter, our SVP of Engineering, recently said: “We try to build teams that thrive in an environment with a clear direction, but the details are unspecified. Teams are able to try a bunch of things and fail fast, and hold themselves accountable and measure themselves rigorously. We find that smaller teams are faster…we try to build them in that sort of environment.” This kind of rapid iteration with small teams that Alex describes requires clear and constant communication between engineers, product managers, designers, researchers, and other functions. When everyone is in the same location, this is a lot easier. “Water-cooler” conversations or chats at offsites can lead to novel ideas, or foster cross-team collaboration. When employees work remotely, we have to do more work to ensure that they are able to have the kind of ad hoc conversations necessary to allow for innovation and ensure that different groups stay in sync. Building teams that are as geographically co-located as possible is one easy way to address some of this. In a future post, our New York Engineering Operations Manager Valerie Kuo ( @vyk ) will talk about the ways in which Twitter organizes teams against various constraints, and how we deal with them. In a modern-day technology company like Twitter, how often do meetings span locations? When I first started working here about a year ago, I was really curious about this – but it turns out relevant data are hard to find. To understand how pervasive cross-office collaboration really was, I analyzed data on every meeting held at Twitter between May 1, 2015 and July 25, 2015. There are a number of interesting conclusions: Over 20% of all meetings company-wide involve more than one city. Nearly 40% of meetings originating in our San Francisco HQ involve more than one city. Some cities work very closely together due to shared resources, strategies, market needs. For instance, Tokyo, Singapore, Seoul, and Sydney interact with each other far more than with they do with San Francisco. Despite the fact that most meetings are held in one city, meetings with multiple cities generally have more participants; so that 50% of meeting time at Twitter is spent in multi-city meetings . If we could increase the efficiency of multi-city meetings by 10%, we would save about 5 weeks of aggregate work every day . These data only include meetings that take place in conference rooms; since ad hoc video conferences between co-workers are excluded, we are almost certainly underestimating the scale of remote collaboration. Given how critical interoffice communication is to a smooth operation, it’s clear that having a videoconferencing (VC) infrastructure is key to productivity. Recent upgrades to our VC systems have made this kind of collaboration substantially easier. The rest of this post will describe where we were a year ago, how we evaluated systems, and how changes we’ve made improve the working life of our employees. How we started A year ago, most Twitter HQ conference rooms were not outfitted with VC equipment. In distributed offices, penetration was higher, though by no means widespread. At that time, two different types of VC systems were deployed: Full VC rooms : These had enterprise-level VC systems including a hardware codec, table mics, built-in speakers and a touch screen controller. These rooms were fairly easy to use but relatively expensive to build. VC-light rooms : These were outfitted with an LCD TV, a USB camera, and a USB speaker speaker/mic pod on the table. Calls were driven by the user’s laptop via cables to connect to VC calls. These rooms were very cost efficient, but also required the user to dedicate their laptop to the call. We primarily used a SaaS service that provides video conferencing from a number of devices and allows interconnectivity to a variety of established VC systems. To use this system: The host or meeting organizer was responsible for creating a meeting in the app. The host then had to paste dial-in information into the meeting invite. In Full-VC rooms, users entered a 12-digit number into a the touch panel controller. In VC-light rooms, users connected their laptops to USB connectors, clicked on the invite, logged in to the service, then edited their audio and video sources. Users could also use mobile apps or the website, or dial in using a phone number. Although we used this system extensively, there were some drawbacks: it regularly took over 5 minutes to initiate a VC connection. For a 30-minute meeting, this was over 15% of the meeting’s duration! And the quality of videoconferencing was poor because of several factors, including poor egress network bandwidth from our old New York engineering office, packet loss on the network, and the app’s prioritization of video quality over audio quality. After noting these issues, I worked with key members of our IT team to make things better. We proposed these requirements to fix multi-city meetings: 80%+ of rooms should be integrated with VC equipment in all offices. Our calendaring system (Google Calendar) should be seamlessly integrated with video conferencing; i.e., VC equipment in each room should know the room’s schedule. One-button entry into a video conference upon entering the meeting room. Support for third-party phone dial-in. Ability to join calls without a Google account. Ability to support large number of dial-ins. After a number of pilots, our IT team decided to go with a company-wide deployment of Google Hangouts plus Chrome for Meetings (CfM) boxes in conference rooms. Deployment of Google Hangouts and CfM in a company that already uses Google Apps, as we do, makes adoption much easier — Google Calendar integration is built into the hardware. CfM boxes are not complicated, and can be connected to an LCD screen or integrated into full room systems. Our standard Hangouts room has an LCD screen mounted on the wall and Google’s standard mic/speaker pod and camera. A local HDMI connection with auto-input switching is included to enable low-friction presentations for local meetings. Currently, CfM does not support 20+ person rooms. Since support for these rooms is important for larger meetings, we worked with AV vendors to develop a way to integrate higher-fidelity cameras and microphones typically found in these larger rooms. We’ve made it a standard across our offices, installing it in over 100 rooms globally. We will describe our approach in greater detail in a future post. At this point, we have over 570 rooms set up with CfM, and most VC meetings are conducted using Hangouts and CfM. We’ve easily saved 3-5 minutes per meeting in VC setup time, and people are notably happier. Unfortunately, CfM still does not support (4), (5), and (6), and so we have maintained a few rooms to be compatible with our legacy setup. We hope that these issues will be addressed soon. A Twitter conference room. Photo courtesy Alex Stillings. Video conferencing is one of the most important pieces of internal infrastructure at Twitter, and CfM does its job well. While this setup is massive strides ahead of our initial deployment, I still fly to San Francisco once a month for in-person meetings, because the best VC available at a reasonable price doesn’t come close to substituting for in-person conversations. In larger meetings, being remote still feels a bit too much like watching a meeting on TV. And, we often have to mute mics when we’re not speaking and have periodics blips in A/V quality. The more we reduce the cost and pain of remote collaboration, the more efficiently we – or any company – can run. We’re already a highly distributed company that takes advantage of a great deal of cross-office collaboration. Further improvements in VC technology will really unlock our ability to efficiently interact with the best employees – current and future – wherever they might be.", "date": "2015-09-04"},
{"website": "Twitter-Engineering", "title": "Hadoop filesystem at Twitter", "author": ["‎@gerashegalov‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2015/hadoop-filesystem-at-twitter.html", "abstract": "Twitter runs multiple large Hadoop clusters that are among the biggest in the world. Hadoop is at the core of our data platform and provides vast storage for analytics of user actions on Twitter. In this post, we will highlight our contributions to ViewFs, the client-side Hadoop filesystem view, and its versatile usage here. ViewFs makes the interaction with our HDFS infrastructure as simple as a single namespace spanning all datacenters and clusters. HDFS Federation helps with scaling the filesystem to our needs for number of files and directories while NameNode High Availability helps with reliability within a namespace. These features combined add significant complexity to managing and using our several large Hadoop clusters with varying versions. ViewFs removes the need for us to remember complicated URLs by using simple paths. Configuring ViewFs itself is a complex task at our scale. Thus, we run TwitterViewFs , a ViewFs extension we developed, that dynamically generates a new configuration so we have a simple holistic filesystem view. Hadoop at Twitter: scalability and interoperability Our Hadoop filesystems host over 300PB of data on tens of thousands of servers. We scale HDFS by federating multiple namespaces. This approach allows us to sustain a high HDFS object count (inodes and blocks) without resorting to a single large Java heap size that would suffer from long GC pauses and the inability to use compressed oops . While this approach is great for scaling, it is not easy for us to use because each member namespace in the federation has its own URI. We use ViewFs to provide an illusion of a single namespace within a single cluster. As seen in Figure 1, under the main logical URI we create a ViewFs mount table with links to the appropriate mount point namespaces for paths beginning with /user, /tmp, and /logs , correspondingly. The configuration of the view depicted in Figure 1 translates to a lengthy configuration of a mount table named clusterA . Logically, you can think of this as a set of symbolic links. We abbreviate such links simply as /logs->hdfs://logNameSpace/logs . Here you can find more details about our TwitterViewFs extension to ViewFs that handles both hdfs:// and viewfs:// URI’s on the client side to onboard hundreds of Hadoop 1 applications without code changes. Twitter’s Hadoop client and server nodes store configurations of all clusters. At Twitter, we don’t invoke the hadoop command directly. Instead we use a multiversion wrapper hadoop that dispatches to different hadoop installs based on a mapping from the configuration directory to the appropriate version. We store the configuration of cluster C in the datacenter DC abbreviated as C@DC in a local directory /etc/hadoop/hadoop-conf-C-DC , and we symlink the main configuration directory for the given node as /etc/hadoop/conf . Consider a DistCp from source to destination . Given a Hadoop 2 destination cluster (which is very common during migration), the source cluster has to be referenced via read-only Hftp regardless of the version of the source cluster. In case of a Hadoop 1 source, Hftp is used because the Hadoop 1 client is not wire-compatible with Hadoop 2. In case of a Hadoop 2 source, Hftp is used as there is no single HDFS URI because of federation. Moreover, with DistCp we have to use the destination cluster configuration to submit the job. However, the destination configuration does not contain information about HA and federation on the source side. Our previous solution implementing a series of redirects to the right NameNode is insufficient to cover all scenarios encountered in production so we merge all cluster configurations on the client side to generate one valid configuration for HDFS HA and ViewFs for all Twitter datacenters as described in the next section. User-friendly paths instead of long URIs We developed user-friendly paths instead of long URIs and enabled native access to HDFS. This removes the overwhelming number of different URIs and greatly increases the availability of the data. When we use multi-cluster applications, we have to cope with the full URIs that sometimes have a long authority part represented by a NameNode CNAME. Furthermore, if the cluster mix includes both Hadoop 1 and Hadoop 2, which are not wire-compatible, we unfortunately have to remember which cluster to address via the interoperable Hftp filesystem URI. The volume of questions around this area on our internal Twitter employee mailing lists, chat channels and office hours motivated us to solve this URI problem for good on the Hadoop 2 side. We realized that since we already present multiple namespaces as a single view within a cluster, we should do the same across all all clusters within a datacenter, or even across all datacenters. The idea is that a path /path/file at the cluster C1 in the datacenter DC1 should be mounted by the ViewFs in each cluster as /DC1/C1/path/file as shown Figure 3. This way we will never have to specify a full URI, nor remember whether Hftp is needed because we can transparently link via Hftp within ViewFs. With our growing number of clusters and number of namespaces per cluster, it would be very cumbersome if we had to maintain additional mount table entries in each cluster configuration manually as it turns into a O(n2) configuration problem. In other words, if we change the configuration of just one cluster we need to touch all n cluster configurations just for ViewFs. We also need to handle the HDFS client configuration for nameservices because otherwise mount point URIs cannot be resolved by the DFSClient . It’s quite common that we have the same logical cluster in multiple datacenters for load balancing and availability: C1@DC1, C1@DC2 , etc. Thus, we decided to add some more features to TwitterViewFs . Instead of populating the configurations administratively, our code adds the configuration keys needed for the global cross-datacenter view at the runtime during the filesystem initialization automatically. This allows us to change existing namespaces in one cluster, or add more clusters without touching the configuration of the other clusters. By default our filesystem scans the glob file:/etc/hadoop/hadoop-conf- *. The following steps construct the TwitterViewFs namespace. When the Hadoop client is started with a specific C-DC cluster configuration directory, the following keys are added from all other C’-DC’ directories during the TwitterViewFs initialization: If there is a ViewFs mount point link like /path->hdfs://nameservice/path in C’-DC’ , then we will add a link /DC’/C’/path->hdfs://nameservice/path . For the Figure 1 example above, we would add to all cluster configurations: /dc/a/user=hdfs://dc-A-user-ns/user Similarly, for consistency, we duplicate all conventional links /path->hdfs://nameservice/path for C-DC as /DC/C/path->hdfs://nameservice/path . This allows us to use the same notation regardless of whether we work with the default C-DC cluster or a remote cluster. We can easily detect whether the configuration C’-DC’ that we are about to merge dynamically is a legacy Hadoop 1 cluster. For Hadoop 1, the key fs.defaultFS points to an hdfs:// URI, whereas for Hadoop 2, it points to a viewfs:// URI. Our Hadoop 1 clusters consist of a single namespace/NameNode, so we can transparently substitute the hftp scheme for the hdfs scheme and simply add the link: /DC/C’/->hft p ://hadoop1nn/ Now the TwitterViewFs namespace is defined. However, at this stage ViewFs links pointing to hdfs nameservices cannot be used by the DFSClient yet. In order to make HA nameservice URIs resolvable, we need to merge the relevant HDFS client configuration from all hdfs-site.xml files in C’-DC’ directories. Here’s how we do this: HDFS uses the key dfs.nameservices to store a comma-separated list of all the nameservices DFSClient needs to resolve. We append the values of all C’-DC’ to the dfs.nameservices value of the current cluster. We typically have 3-4 namespaces per cluster. All namespace-specific parameters in HDFS carry the namespace somewhere in the suffix. Twitter namespace names are unique and mnemonic enough that a simple heuristic of copying all key-value pairs from C’-DC’ where the key name begins with “dfs” and contains one of the nameservices from Step 1 is sufficient. Now we have a working TwitterViewFs with all clusters accessible via the /DC/C/path convention regardless of whether a specific C is a Hadoop 1 or a Hadoop 2 cluster. A powerful example of this scheme is to check the quota of home directories on all clusters in one single command: hadoop fs -count ‘/{dc1,dc2}/*/user/gera’ We can also easily run fsck on any of the namespaces without remembering the exact complex URI: hadoop fsck /dc1/c1/user/gera We want a consistent experience when working with the local filesystem and HDFS. It is much easier to remember conventional commands such as cp than “syntactic-sugar commands” such as copyFrom/ToLocal, put, get, etc. A regular hadoop cp command requires a full file:/// URI and that is what the syntactic sugar commands try to simplify. When mounted with ViewFs even this is not necessary. Similar to how we add ViewFs links for the cluster /DC/cluster , we add ViewFS links to the TwitterViewFs configuration such as: /local/user/<user>->file:/home/<user> /local/tmp->file:/${hadoop.tmp.dir} Then, copying a file from a cluster to a local directory looks like: hadoop fs -cp /user/laurent/debug.log /local/user/laurent/ The simple, unified cross-DC view on an otherwise fragmented Hadoop namespace has pleased internal users and sparked public interest . High availability for multi-datacenter environment Beyond this, we created a project code-named Nfly (N as in N datacenters), where we implement much of the HA and multi-dc functionality in ViewFs itself in order to avoid unnecessary code duplication. Nfly is able to link a single ViewFs path to multiple clusters. When using Nfly one appears to interact with a single filesystem while in reality in the background each write is applied to all linked clusters and a read is performed from either the closest cluster (according to NetworkTopology ) or the one with the most recent available copy. Nfly makes cross-datacenter HA very easy. Fusion of multiple physical paths to one logical more available path is achieved with a new replication multi-URI Inode . This is tailored to a common HDFS usage pattern in our highly available Twitter services. Our services host their data on some logical cluster C. New service data versions are created periodically to relatively infrequently and read by many different servers. There is a corresponding HDFS cluster in multiple datacenters. When the service runs in datacenter DC1 it prefers to read from /DC1/C for lower latency. However, when data under /DC1/C is unavailable the service wants to failover its reads to the higher latency path /DC2/C instead of exposing the outage to its users. A conventional ViewFs mount direct inode points to a single URI via ChRootedFileSystem , as you can see there is one arrow between nodes in Figure 3 above. The user namespace (which is green above) of ClusterA in datacenter DC1 is mounted using the mount point entry /DC1/clusterA/user ->hdfs://dc1-A-user/user . When the application passes the path /DC1/clusterA/user /lohit it will be resolved as follows. The root portion of the path marked bold between the root / and the mount point inode user (top of the namespace tree in Figure 3) is replaced by the link target value hdfs://dc1-A-user/user . Then the result hdfs://dc1-A-user/user/lohit is used to access the physical FileSystem. Replacing of root portion is called chrooting in this context, hence the name ChRootedFileSystem . Thus, if we had multiple URI’s in the inode, we could back a single logical path by multiple physical filesystems typically residing in different datacenters. Consequently, we introduce a new type of link pointing to a list of URIs each wrapped in a ChRootedFileSystem . The basic principle that a write call is propagated to each filesystem represented by the URIs synchronously. On the read path, the FileSystem client picks the URI pointing to the closest destination, such as in the same datacenter. A typical usage is /nfly/C/user->/DC1/C/user,/DC2/C/user,… The message sequence diagram in Figure 4 illustrates this scenario. This collection of ChRootedFileSystem instances is fronted by the Nfly filesystem object that is used for the mount point inode. The Nfly filesystem backs a single logical path /nfly/C/user/<user>/path by multiple physical paths. It supports setting minReplication. As long as the number of URIs on which an update has succeeded is greater than or equal to minReplication, exceptions are merely logged but not thrown. Each update operation is currently executed serially. However, we do plan to add a feature to use parallel writes from the client as far as its bandwidth permits. With Nfly a file create or write is executed as follows: Creates a temporary invisible _nfly_tmp_file in the intended chrooted filesystem. Returns a FSDataOutputStream that wraps output streams returned by A. All writes are forwarded to each output stream. On close of stream created in B, all n streams are closed, and the files are renamed from _nfly_tmp_file to file. All files receive the same mtime corresponding to the client system time as of beginning of this step. If at least minReplication destinations have gone through steps 1 to 5 without failures the filesystem considers the transaction logically committed; Otherwise it tries to clean up the temporary files in a best-effort attempt. As for reads, we support a notion of locality similar to HDFS /DC/rack/node . We sort URIs using NetworkTopology by their authorities. These are typically host names in simple HDFS URIs. If the authority is missing as is the case with the local file:/// the local host name is assumed InetAddress.getLocalHost() . This ensures that the local file system is always considered to be the closest one to the reader. For our Hadoop 2 hdfs URIs that are based on nameservice ids instead of hostnames it is very easy to adjust the topology script since our nameservice ids already contain the datacenter reference. As for rack and node we can simply output any string such as /DC/rack-nsid/node-nsid , because we are concerned with only datacenter-locality for such filesystem clients. There are two policies/additions to the read call path that make it more computationally expensive, but improve user experience: readMostRecent - Nfly first checks mtime for the path under all URIs and sorts them from most to least recent. Nfly then sorts the set of URIs with the most recent mtime topologically in the same manner as described above. repairOnRead - Nfly already has to contact all underlying destinations. With repairOnRead , the Nfly filesystem would additionally attempt to refresh destinations with the path missing or a stale version of the path using the nearest available most recent destination. As we pointed out before, managing ViewFs configurations can already be quite cumbersome, and Nfly mounts make it even more complicated. Luckily, TwitterViewFs provides mechanisms with sufficient flexibility to add more code in order to generate useful Nfly configurations “on the fly”. If a Twitter employee wants their home directories on the logical cluster C across all DC’s nflied under /nfly/C/user/<user> , she simply specifies -Dfs.nfly.mount=C. If she then additionally wants to cache the files locally under /local/user/<user>/C , she specifies -Dfs.nfly.local=true . Future work The multi-URI inode introduced for Nfly lays the groundwork for the read-only Merge FileSystem that transparently merges inodes from the underlying filesystems. This is something we’re currently working on implementing. It will allow us to cut the number of mount table entries dramatically in comparison to single-URI inode approach. The target use case for the Merge FileSystem is to split an existing namespace, for example the user namespace, into two namespaces without the need for users to adjust code, and without bloating configuration. To see this illustrated, you can compare Figures 5 and 6. In this post we shared our approach to managing Hadoop filesystems at Twitter: scaling to meet our needs for vast storage using federated namespaces while maintaining simplicity through ViewFs. We extended ViewFs to simplify its operation in face of ever growing number of clusters and namespaces in multiple datacenters and added Nfly for cross-datacenter availability of HDFS data. We believe that the broader Hadoop user community will benefit from our experience. Acknowledgements We would like to thank Laurent Goujon , Lohit VijayaRenu , Siqi Li , Joep Rottinghuis , the Hadoop team at Twitter and the wider Hadoop community for helping us scale Hadoop at Twitter.", "date": "2015-09-29"},
{"website": "Twitter-Engineering", "title": "Diffy: Testing services without writing tests", "author": ["‎@pzdk‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2015/diffy-testing-services-without-writing-tests.html", "abstract": "Today, we’re excited to release Diffy , an open-source tool that automatically catches bugs in Apache Thrift and HTTP-based services. It needs minimal setup and is able to catch bugs without requiring developers to write many tests. Service-oriented architectures like our platform see a large number of services evolve at a very fast pace. As new features are added with each commit, existing code is inevitably modified daily – and the developer may wonder if they might have broken something. Unit tests offer some confidence, but writing good tests can take more time than writing the code itself. What’s more, unit tests offer coverage for tightly-scoped small segments of code, but don’t address the aggregate behavior of a system composed of multiple code segments. Each independent code path requires its own test. As the complexity of a system grows, it very quickly becomes impossible to get adequate coverage using hand-written tests, and there’s a need for more advanced automated techniques that require minimal effort from developers. Diffy is one such approach we use. What is Diffy? Diffy finds potential bugs in your service by running instances of your new and old code side by side. It behaves as a proxy and multicasts whatever requests it receives to each of the running instances. It then compares the responses, and reports any regressions that surface from these comparisons. The premise for Diffy is that if two implementations of the service return “similar” responses for a sufficiently large and diverse set of requests, then the two implementations can be treated as equivalent and the newer implementation is regression-free. We use the language “similar” instead of “same” because responses may be prone to a good deal of noise that can make some parts of the response data structure non-deterministic. For example: Server-generated timestamps embedded in the response Use of random generators in the code Race conditions in live data served by downstream services All of these create a strong need for noise to be automatically eliminated. Noisy results are useless for developers, because trying to manually distinguish real regressions from noise is like looking for a needle in a haystack. Diffy’s novel noise cancellation technique distinguishes it from other comparison-based regression analysis tools. How Diffy works Diffy acts as a proxy which accepts requests drawn from any source you provide and multicasts each of these requests to three different service instances: A candidate instance running your new code A primary instance running your last known-good code A secondary instance running the same known-good code as the primary instance Here’s a diagram illustrating how Diffy operates: As Diffy receives a request, it sends the same request to candidate, primary and secondary instances. When those services send responses back, Diffy compares these responses and looks for two things: Raw differences observed between the candidate and primary instances. Non-deterministic noise observed between the primary and secondary instances. Since both of these instances are running known-good code, we would ideally expect responses to be identical. For most real services, however, we observe that some parts of the responses end up being different and exhibit nondeterministic behavior. These differences may not show up consistently on a per-request basis. Imagine a random boolean embedded in the response. There is a 50% chance that the boolean will be the same across primary and secondary and a 50% chance that candidate will have a different value than primary. This means that 25% of the requests will trigger a false error and result in noise. For this reason, Diffy looks at the aggregate frequency of each type of error across all the requests it has seen to date. Diffy measures how often primary and secondary disagree with each other versus how often primary and candidate disagree with each other. If these measurements are roughly the same, then it determines that there is nothing wrong and that the error can be ignored. Getting started Here’s how you can start using Diffy to compare three instances of your service: 1. Deploy your old code to localhost:9990. This is your primary. 2. Deploy your old code to localhost:9991. This is your secondary. 3. Deploy your new code to localhost:9992. This is your candidate. 4. Build your diffy jar from the code using the “./sbt assembly” comand. 5. Run the Diffy jar with following command from the diffy directory : java -jar./target/scala-2.11/diffy-server.jar \\ -candidate=\"localhost:9992\" \\ -master.primary=\"localhost:9990\" \\ -master.secondary=\"localhost:9991\" \\ -service.protocol=\"http\" \\ -serviceName=\"My Service\" \\ -proxy.port=:31900 \\ -admin.port=:31159 \\ -http.port=:31149 \\ -rootUrl=’localhost:31149’ 6. Send a few test requests to your Diffy instance: curl localhost:31900/your_application_route 7. Watch the differences show up in your browser at localhost:31149. You should see something like this: 8. You can also see the full request that triggered the behavior and the full responses from primary and candidate: Visit the Github repo for more detailed instructions and examples. As engineers we all want to focus on building and shipping products quickly. Diffy enables us to do that by keeping track of potential bugs for us. We hope you can gain from the project just as we have, and help us to continue improving it over time.", "date": "2015-09-03"},
{"website": "Twitter-Engineering", "title": "Twitter at @MesosCon 2015", "author": ["‎@davelester‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2015/twitter-at-mesoscon-2015.html", "abstract": "Once again, we’re pleased to sponsor and participate in #MesosCon . As heavy users of both Mesos and Apache Aurora to power our cloud infrastructure, we’re excited to be part of this growing community event. The conference, organized by the Apache Mesos community, features talks on the popular open source cluster management software and its ecosystem of software for running distributed applications at the scale of tens of thousands of servers. Conference highlights This year’s #MesosCon will be significantly larger than last year and features simultaneous tracks including beginner talks, the Mesos core, frameworks, and operations. We have a stellar lineup of invited keynote speakers including Adrian Cockcroft ( @adrianco , Battery Ventures), Neha Narula ( @neha , MIT), Peter Bailis ( @pbailis , UC Berkeley), and Benjamin Hindman ( @benh , Mesosphere). We’re also pleased that Twitter will have a strong presence. We’ll be sharing our latest work as well as best practices from the last four-plus years of using Apache Mesos and Apache Aurora. And if you’re interested in learning more about engineering opportunities, stop by our booth. There’s a pre-conference hackathon that several of us Twitter folks will be attending. We’re also hosting a #MesosSocial in our Seattle office on Wednesday, August 19 to kick off the conference. You can follow @TwitterOSS for updates when we announce more details next week. See you at #MesosCon ! Twitter speakers The New Mesos HTTP API - Vinod Kone , Twitter, Isabel Jimenez ( @ijimene ), Mesosphere This session will provide a comprehensive walkthrough of recent advancements with the Mesos API, explaining the design rationale and highlighting specific improvements that simplify writing frameworks to Mesos. Twitter’s Production Scale: Mesos and Aurora Operations - Joe Smith , Twitter This talk will offer an operations perspective on the management of a Mesos + Aurora cluster, and cover many of the cluster management best practices that have evolved here from real-world production experience. Supporting Stateful Services on Mesos using Persistence Primitives - Jie Yu , Twitter, and Michael Park, Mesosphere This talk will cover the persistence primitives recently built into Mesos, which provide native support for running stateful services like Cassandra and MySQL in Mesos. The goal of persistent primitives is to allow a framework to have assured access to its lost state even after task failover or slave restart. Apache Cotton MySQL on Mesos - Yan Xu , Twitter Cotton is a framework for launching and managing MySQL clusters within a Mesos cluster. Recently open-sourced by Twitter as Mysos and later renamed, Cotton dramatically simplifies the management of MySQL instances and is one of the first frameworks that leverages Mesos’ persistent resources API. We’ll share our experience using this framework. It’s our hope that this is helpful to other Mesos framework developers, especially those wanting to leverage Mesos’ persistent resources API. Tactical Mesos: How Internet-Scale Ad Bidding Works on Mesos/Aurora - Dobromir Montauk , TellApart Dobromir will present TellApart’s full stack in detail, which includes Mesos/Aurora, ZK service discovery, Finagle-Mux RPC, and a Lambda architecture with Voldemort as the serving layer. Scaling a Highly-Available Scheduler Using the Mesos Replicated Log: Pitfalls and Lessons Learned - Kevin Sweeney , Twitter This talk will give you tools for writing a framework scheduler for a large-scale Mesos cluster using Apache Aurora as a case study. It will also explore the tools the Aurora scheduler has used to meet these challenges, including Apache Thrift for schema management. Simplifying Maintenance with Mesos - Benjamin Mahler , Twitter Today, individual frameworks are responsible for maintenance which poses challenges when running multiple frameworks (e.g. services, storage, batch compute). We’ll explore a current proposal for adding maintenance primitive in Mesos to address these concerns, enabling tooling for automated maintenance. Generalizing Software Deployment - The Many Meanings of “Update” - Bill Farner , Twitter Bill will present the evolution of how Apache Aurora managed deployments and describe some of the challenges imposed by wide variance in requirements. This talk will also share how deployments on Aurora currently run major services at Twitter. Per Container Network Monitoring and Isolation in Mesos - Jie Yu , Twitter This talk will discuss the per container network monitoring and isolation feature introduced in Mesos 0.21.0. We’ll show you the implications of this approach and lessons we learned during the deployment and use of this feature. Join us! Good news: there’s still time to register for #MesosCon and join us in Seattle on August 20-21. There’s a pre-conference hackathon that several of us Twitter folks will be attending. We’re also hosting a #MesosSocial in our Seattle office on Wednesday, August 19 to kick off the conference. You can follow @TwitterOSS for updates when we announce more details next week. See you at #MesosCon !", "date": "2015-07-31"},
{"website": "Twitter-Engineering", "title": "Flying faster with Twitter Heron", "author": ["Karthik Ramasamy"], "link": "https://blog.twitter.com/engineering/en_us/a/2015/flying-faster-with-twitter-heron.html", "abstract": "We process billions of events on Twitter every day. As you might guess, analyzing these events in real time presents a massive challenge. Our main system for such analysis has been Storm , a distributed stream computation system we’ve open-sourced . But as the scale and diversity of Twitter data has increased, our requirements have evolved. So we’ve designed a new system, Heron — a real-time analytics platform that is fully API-compatible with Storm. We introduced it yesterday at SIGMOD 2015 . Our rationale and approach A real-time streaming system demands certain systemic qualities to analyze data at a large scale. Among other things, it needs to: process of billions of events per minute; have sub-second latency and predictable behavior at scale; in failure scenarios, have high data accuracy, resiliency under temporary traffic spikes and pipeline congestions; be easy to debug; and simple to deploy in a shared infrastructure. To meet these needs we considered several options, including: extending Storm; using an alternative open source system; developing a brand new one. Because several of our requirements demanded changing the core architecture of Storm, extending it would have required longer development cycles. Other open source streaming processing frameworks didn’t perfectly fit our needs with respect to scale, throughput and latency. And these systems aren’t compatible with the Storm API – and adapting a new API would require rewriting several topologies and modifying higher level abstractions, leading to a lengthy migration process. So we decided to build a new system that meets the requirements above and is backward-compatible with the Storm API. The highlights of Heron When developing Heron, our main goals were to increase performance predictability, improve developer productivity and ease manageability. We made strategic decisions about how to architect the various components of the system to operate at Twitter scale. The overall architecture for Heron is shown here in Figure 1 and Figure 2. Users employ the Storm API to create and submit topologies to a scheduler. The scheduler runs each topology as a job consisting of several containers. One of the containers runs the topology master, responsible for managing the topology. The remaining containers each run a stream manager responsible for data routing, a metrics manager that collects and reports various metrics and a number of processes called Heron instances which run the user-defined spout/bolt code. These containers are allocated and scheduled by scheduler based on resource availability across the nodes in the cluster. The metadata for the topology, such as physical plan and execution details, are kept in Zookeeper. Figure 1. Heron Architecture Figure 2. Topology Architecture Specifically, Heron includes these features: Off the shelf scheduler: By abstracting out the scheduling component, we’ve made it easy to deploy on a shared infrastructure running various scheduling frameworks like Mesos, YARN, or a custom environment. Handling spikes and congestion: Heron has a back pressure mechanism that dynamically adjusts the rate of data flow in a topology during execution, without compromising data accuracy. This is particularly useful under traffic spikes and pipeline congestions. Easy debugging: Every task runs in process-level isolation, which makes it easy to understand its behavior, performance and profile. Furthermore, the sophisticated UI of Heron topologies, shown in Figure 3 below, enables quick and efficient troubleshooting for issues. Figure 3. Heron UI showing logical plan, physical plan and status of a topology Compatibility with Storm : Heron provides full backward compatibility with Storm, so we can preserve our investments with that system. No code changes are required to run existing Storm topologies in Heron, allowing for easy migration. Scalability and latency : Heron is able to handle large-scale topologies with high throughput and low latency requirements. Furthermore, the system can handle a large number of topologies. Heron performance We compared the performance of Heron with Twitter’s production version of Storm, which was forked from an open source version in October 2013, using word count topology. This topology counts the distinct words in a stream generated from a set of 150,000 words. Figure 4. Throughput with acks enabled Figure 5. Latency with acks enabled As shown in Figure 4, the topology throughput increases linearly for both Storm and Heron. However for Heron, the throughput is 10–14x higher than that of Storm in all experiments. Similarly, the end-to-end latency, shown in Figure 5, increases far more gradually for Heron than it does for Storm. Heron latency is 5-15x lower than Storm’s latency. Beyond this, we have run topologies which scale to hundreds of machines, many of which handle sources that generate millions of events per second, without issues. Also with Heron, numerous topologies that aggregate data every second are able to achieve sub-second latencies. In these cases, Heron is able to achieve this with less resource consumption than Storm. Heron at Twitter At Twitter, Heron is used as our primary streaming system, running hundreds of development and production topologies. Since Heron is efficient in terms of resource usage, after migrating all Twitter’s topologies to it we’ve seen an overall 3x reduction in hardware, causing a significant improvement in our infrastructure efficiency. What’s next? We would like to collaborate and share lessons learned with the Storm community as well as other real-time stream processing system communities in order to further develop these programs. Our first step towards doing this was sharing our research paper on Heron at SIGMOD 2015. In this paper, you’ll find more details about our motivations for designing Heron, the system’s features and performance, and how we’re using it on Twitter. Acknowledgements Heron would not have been possible without the work of Sanjeev Kulkarni , Maosong Fu , Nikunj Bhagat , Sailesh Mittal , Vikas R. Kedigehalli , Siddarth Taneja ( @staneja ), Zhilan Zweiger , Christopher Kellogg , Mengdie Hu ( @MengdieH ) and Michael Barry . We would also like to thank the Storm community for teaching us numerous lessons and for moving the state of distributed real-time processing systems forward. References [1] Twitter Heron: Streaming at Scale , Proceedings of ACM SIGMOD Conference, Melbourne, Australia, June 2015 [2] Storm@Twitter , Proceedings of ACM SIGMOD Conference, Snowbird, Utah, June 2014", "date": "2015-06-02"},
{"website": "Twitter-Engineering", "title": "Building a new trends experience", "author": ["‎@zhenghuali‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2015/building-a-new-trends-experience.html", "abstract": "We recently launched a new trends experience on mobile and web. Certain users will now see additional context with their trends: a short description and at times an accompanying image. While building this new product experience, we also switched the entire trends backend system to a brand new platform. This is the largest engineering undertaking of the trends system since 2008. Every second on Twitter, thousands of Tweets are sent. Since 2008, trends have been the one-stop window into the Twitter universe, providing users with a great tool to keep up with breaking news , entertainment and social movements . In last week’s release, we built new mechanisms that enrich trends with additional information to simplify their consumption. In particular, we introduced an online news clustering service that detects and groups breaking news on Twitter. We also replaced the legacy system for detecting text terms that are irregular in volume with a generic, distributed system for identifying anomalies in categorized streams of content. The new system can detect trending terms, URLs, users, videos, images or any application-specific entity of interest. Adding context to trends Until recently, trends have only been presented as a simple list of phrases or hashtags, occasionally leaving our users puzzled as to why something like “Alps,” “Top Gear” or “East Village” is trending. Learning more required clicking on the trend and going through related Tweets. With this update, Twitter now algorithmically provides trend context for certain tailored trends users. Not only does this improve usability, but according to our experimental data, it also motivates users to engage even more. Context for the new trends experience may include the following pieces of data: A description: Short text descriptions are presented to explain the meaning of a trend. Currently, they are mainly retrieved from popular articles shared on Twitter. An image: Surfacing a relevant image provides our users with direct visual knowledge. Trend images can come either from popular images or news articles shared on Twitter. Twitter-specific information: The volume of Tweets and Retweets of a trend, or the time in which it started trending, helps users understand the magnitude of conversation surrounding the topic. When users click on a trend, Tweets that provided the image or description source are shown at the top of the corresponding timeline. This gives tribute to the original source of context and ensures a consistent browsing experience for the user. The trend context is recomputed and refreshed every few minutes, providing users with a summary of the current status of affairs. News clustering system Surfacing high quality, relevant and real-time contextual information can be challenging, given the massive amount of Tweets we process. Currently, one major source for descriptions and images are news URLs shared on Twitter. The news clustering system detects breaking news in real time, based on engagement signals from users. It groups similar news stories into clusters along different news verticals and surfaces real-time conversations and content like top images and Tweets for each cluster. The following diagram provides details about components of this system. The news clusterer consumes streams of Tweets containing URLs from a set of news domains. Twitter’s URL crawler is then used to fetch the content from the links embedded in each Tweet. The clusterer then builds feature vectors out of crawled article content. Using these feature vectors, an online clustering algorithm is employed to cluster related stories together. The resulting clusters are ranked using various criteria including recency and size. Each cluster maintains a ranked list of metadata, such as top images, top Tweets, top articles and keywords. The ranked clusters are persisted to Manhattan periodically for serving purposes. The news clustering service polls for updated clusters from Manhattan. An inverted index is built from keywords computed by the clusterer to corresponding clusters. Given a query, this service returns matching clusters, or the top-n ranked clusters available, along with their metadata, which is used to add context to trends. Trends computation In addition to making trends more self-explanatory, we have also replaced the entire underlying trends computation system itself with a new real-time distributed solution. Since the old system ran on a single JVM, it could only process a small window of Tweets at a time. It also lacked stable recovery mechanism. The new system is built on a scalable, distributed Storm architecture for streaming MapReduce. It maintains state in both memcached and Manhattan. Tweet input passes through durable Kafka queue for proper recovery on restarts. The new system consists of two major components. The first component is trends detection. It is built on top of Summingbird , responsible for processing Firehose data, detecting anomalies and surfacing trends candidates . The other component is trends postprocessing, which selects the best trends and decorates them with relevant context data. Trends detection Based on Firehose Tweets input, a trends detection job computes trending entities in domains related to languages, geo locations and interesting topics. As shown in the diagram below, it has the following main phases: Data preparation Entity, domain, attribute extraction and aggregation Scoring and ranking Data preparation includes filtering and throttling. Basic filtering removes replies, Tweets with low text quality or containing sensitive content. Anti-spam filtering takes advantage of real-time spam signal available from BotMaker . Throttling removes similar Tweets and ensures contribution to a trend from a single user is limited. After filtering and throttling, the trending algorithm is where the decision of what domain-entity pairs are trending is made. For this, domain-entity pairs are extracted along with related metadata, and then aggregated into counter objects. Additional pair attributes like entity co-occurrence and top URLs are collected and persisted separately, which are later used for scoring and post-processing.The scorer computes score for entity-domain pairs based on the main counter objects and their associated attribute counters. The tracker then ranks these pairs and saves top ranked results with scores onto Manhattan. These results are trends candidates ready for postprocessing and human evaluation. Trends postprocessor Trends postprocessor has the following main functionalities: Periodically retrieves trends generated by the trends detection job Performs trends backfill and folding Collects context metadata including description, image, Tweet volume The following diagram shows how the postprocessor works: The scanner periodically loads all available domains and initiates the post-processing operation for each domain. Depending on the granularity, a domain may be expanded to form a proper sequence in ascending order of scope. For example, a city level domain [San Francisco] will be expanded to List[San Francisco, California, USA, en] that contains the full domain hierarchy, with language en as the most general one. For domains without sufficient organic trending entities, a backfill process is used to compensate them with data from their ancestors’ domains, after domain expansion. The folding process is responsible for combining textually different, semantically similar trending entities into a single cluster and selecting the best representative to display to the end user. Metadata fetcher retrieves data from multiple sources, including the search-blender and news clustering service described earlier to decorate each tending entity with context information. These decorated entities are then persisted in batch for the trends serving layer to pick up. Acknowledgement Looking ahead, we are working hard to improve the quality of trends in multiple ways. Stay tuned! The following people contributed to these updates: Alex Cebrian , Amit Shukla , Brian Larson , Chang Su , Dumitru Daniliuc , Eric Rosenberg , Fabio Resende , Fei Ma , Gabor Cselle , Gilad Mishne , Jay Han , Jerry Marino , Jess Myra , Jingwei Wu , Jinsong Lin , Justin Trobec , Keh-Li Sheng , Kevin Zhao , Kris Merrill , Maer Melo , Mike Cvet , Nipoon Malhotra , Royce Cheng-Yue , Stanislav Nikolov , Suchit Agarwal , Tal Stramer , Todd Jackson , Veljko Skarich , Venu Kasturi , Zhenghua Li , Zijiao Liu .", "date": "2015-04-24"},
{"website": "Twitter-Engineering", "title": "Graduating Apache Parquet", "author": ["‎@J_‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2015/graduating-apache-parquet.html", "abstract": "ASF, the Apache Software Foundation , recently announced the graduation of Apache Parquet , a columnar storage format for the Apache Hadoop ecosystem. At Twitter, we’re excited to be a founding member of the project. https://twitter.com/TheASF/status/592644433813884929 Apache Parquet is built to work across programming languages, processing frameworks, data models and query engines including Apache Hive, Apache Drill, Impala and Presto. At Twitter, Parquet has helped us scale by reducing storage requirements by at least one-third on large datasets, as well as improving scan and deserialization time. This has translated into hardware savings and reduced latency for accessing data. Furthermore, Parquet’s integration with so many tools creates opportunities and flexibility for query engines to help optimize performance. Since we announced Parquet , these open source communities have integrated the project: Apache Crunch, Apache Drill, Apache Hive, Apache Pig, Apache Spark, Apache Tajo, Kite, Impala, Presto and Scalding. What’s new? The Parquet community just released version 1.7.0 with several new features and bug fixes. This update includes: A new filter API for Java and DSL for Scala that uses statistics metadata to filter large batches of records without reading them A memory manager that will scale down memory consumption to help avoid crashes Improved MR and Spark job startup time Better support for evolving schemas with type promotion when reading More logical types for storing dates, times, and more Improved compatibility between Hive, Avro and other object models As usual, this release also includes many other bug fixes. We’d like to thank the community for reporting these and contributing fixes. Parquet 1.7.0 is now available for download . Future work Although Parquet has graduated, there’s still plenty to do, and the Parquet community is planning some major changes to enable even more improvements. First is updating the internals to work with the zero-copy read path in Hadoop, making reads even faster by not copying data into Parquet’s memory space. This will also enable Parquet to take advantage of HDFS read caching and should pave the way for significant performance improvements. After moving to zero-copy reads, we plan to add a vectorized read API that will enable processing engines like Drill, Presto and Hive to save time by processing column data in batches before reconstructing records in memory, if at all. We also plan to add more advanced statistics based record filtering to Parquet. Statistics based record filtering allows us to drop entire batches of data with only reading a small amount of metadata). For example, we’ll take advantage of dictionary encoded columns and apply filters to batches of data by examining a column’s dictionary, and in cases where no dictionary is available, we plan to store a bloom filter in the metadata. Aside from performance, we’re working on adding POJO support in the Parquet Avro object model that works the same way Avro handles POJOs in avro-reflect. This will make it easier to use existing Java classes that aren’t based on one of the already-supported object models and enable applications that rely on avro-reflect to use Parquet as their data format. Getting involved Parquet is an independent open source project at the ASF. To get involved, join the community mailing lists and any of the community hangouts the project holds. We welcome everyone to participate to make Parquet better and look forward to working with you in the open. Acknowledgements We would like to thank Ryan Blue from Cloudera for helping craft parts of this post and the wider Parquet community for contributing to the project. Specifically, contributors from a number of organizations (Twitter, Netflix, Criteo, MaPR, Stripe, Cloudera, AmpLab) contributed to this release. We’d also like to thank these people: Daniel Weeks, Zhenxiao Luo, Nezih Yigitbasi, Tongjie Chen, Mickael Lacour, Jacques Nadeau, Jason Altekruse, Parth Chandra, Colin Marc ( @colinmarc ), Avi Bryant ( @avibryant ), Ryan Blue ( @6d352b5d3028e4b ), Marcel Kornacker, Nong Li ( @nongli ), Tom White ( @tom_e_white ), Sergio Pena, Matt Massie ( @matt_massie ), Tianshuo Deng , Julien Le Dem , Alex Levenson , Chris Aniszczyk and Lukas Nalezenec.", "date": "2015-05-21"},
{"website": "Twitter-Engineering", "title": "Another look at MySQL at Twitter and incubating Mysos", "author": ["‎@marsanfra‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2015/another-look-at-mysql-at-twitter-and-incubating-mysos.html", "abstract": "While we’re at the Percona Live MySQL Conference , we’d like to discuss updates on how Twitter uses MySQL, as well as share our plans to open source Mysos, a new MySQL on Apache Mesos framework. https://twitter.com/TwitterDBA/status/588748702216359937 MySQL at Twitter Since Twitter was founded, MySQL has been one of our key data storage technologies. We store data in hundreds of schemas and our largest cluster is thousands of nodes serving millions of queries per second. At the scale of Twitter, we are pushing MySQL to its limits. At Twitter, MySQL is used in two ways: As part of data services: MySQL is used as storage nodes of a distributed data store within Twitter’s own sharding framework. Here we leverage the reliability and high performance of MySQL on individual storage nodes while the sharding framework manages the distribution and high availability of our data. Some of our biggest MySQL clusters are over a thousand servers. As relational data stores: We use MySQL replication for fault-tolerance and read-scalability. We scale to a large volume of reads using clusters with standard MySQL replication. These clusters store a wide variety of data from commerce and ads to authentication, trends, internal services and more. As Twitter has grown as a service, so has our need to scale storage technologies like MySQL, PostgreSQL, Vertica and Manhattan in production. To make this possible, we’re actively hiring for positions including database administrators, site reliability engineers and software engineers working on distributed storage technology. Contributing to the open source community Twitter has benefited greatly from the MySQL community and we’ve contributed many patches back upstream. Examples of patches contributed to MySQL include: Bug #75298 : Purge thread should check purge_sys->state after every batch Bug #74512 : excessive split/merge for unique sec index Bug #74511 : adaptive_hash_searches_btree not updated Bug #72520 : os_event_wait_time_low(): wait time calculation is messed up Bug #71411 : buf_flush_LRU() does not return correct number in case of compressed pages Twitter is also part of the WebScaleSQL initiative, which just won the 2015 Corporate Contributor Award to the MySQL community. The goal of WebScaleSQL is to enable the scale-oriented members of the MySQL community to work closer to build and add more features to MySQL that are specific to deployments in large scale environments. You can check out some of our examples of patches contributed to WebScaleSQL here . Introducing and incubating Mysos In an effort to improve the scalability and management of our MySQL clusters, we’ve begun work on a new framework called Mysos. The Mysos project leverages Apache Mesos to build a scalable database service for MySQL. Mesos provides primitives to allow Mysos to reliably schedule, monitor and communicate with MySQL instances. As a storage framework, Mysos will be able to use recently-added persistent storage primitives within Mesos. It dramatically simplifies the management of a MySQL cluster and is designed to offer: Efficient hardware utilization through multi-tenancy (in performance-isolated containers) High reliability through preserving the MySQL state during failure and automatic backing up to/restoring from HDFS An automated self-service option for bringing up new MySQL clusters High availability through automatic MySQL master failover An elastic solution that allows users to easily scale up and down a MySQL cluster by changing the number of slave instances Having initially developed Mysos, we’re now announcing our plans to open source the project and our intention to build a strong, independent open source community around Mysos. We are still in the early stages and the code isn’t meant for production usage yet. However, we are starting to seed the code and a proposal at the Apache Incubator with engineers from Twitter, Mesosphere and the Apache Mesos community. We invite anyone interested in scaling MySQL on Mesos to reach out to the Mysos community — we’d love to have you involved. At the moment, we’re best reached by visiting the #mysos IRC channel on Freenode. Acknowledgements The Mysos project was born out of a collaboration between members of the Cloud Infrastructure team at Twitter ( Yan Xu , Chris Lambert , Vinod Kone , Dominic Hamon , Jie Yu , Ben Mahler , Brian Wickman ), members of the @TwitterOSS team ( Chris Aniszczyk , Dave Lester ), members of the @TwitterDBA team ( Chuck Sumner , Jonah Berquist , Pascal Borghino ), and the MySQL team ( Calvin Sun , Inaam Rana , Tugrul Bingol ).", "date": "2015-04-16"},
{"website": "Twitter-Engineering", "title": "Handling five billion sessions a day – in real time ", "author": ["‎@edsolovey‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2015/handling-five-billion-sessions-a-day-in-real-time.html", "abstract": "Since we first released Answers seven months ago, we’ve been thrilled by tremendous adoption from the mobile community. We now see about five billion sessions per day , and growing. Hundreds of millions of devices send millions of events every second to the Answers endpoint. During the time that it took you to read to here, the Answers back-end will have received and processed about 10,000,000 analytics events. The challenge for us is to use this information to provide app developers with reliable, real-time and actionable insights into their mobile apps. At a high level, we guide our architectural decisions on the principles of decoupled components, asynchronous communication and graceful service degradation in response to catastrophic failures. We make use of the Lambda Architecture to combine data integrity with real-time data updates. In practice, we need to design a system that receives events, archives them, performs offline and real-time computations, and merges the results of those computations into coherent information. All of this needs to happen at the scale of millions events per second. Let’s start with our first challenge: receiving and handling these events. Event reception When designing our device-server communication, our goals were: reducing impact on battery and network usage; ensuring data reliability; and getting the data over as close to real time as possible. To reduce impact on the device, we send analytics events in batches and compress them before sending. To ensure that valuable data always gets to our servers, devices retry failed data transfers after a randomized back-off and up to a disk size limit on the device. To get events over to the servers as soon as possible, there are several triggers that cause the device to attempt a transfer: a time trigger that fires every few minutes when the app is foregrounded, a number of events trigger and an app going into background trigger. This communication protocol results in devices sending us hundreds of thousands of compressed payloads every second. Each of these payloads may contain tens of events. To handle this load reliably and in a way that permits for easy linear scaling, we wanted to make the service that accepts the events be dead simple. This service is written in GOLANG, fronted by Amazon Elastic Load Balancer (ELB), and simply enqueues every payload that it receives into a durable Kafka queue . Archival Because Kafka writes the messages it receives to disk and supports keeping multiple copies of each message, it is a durable store. Thus, once the information is in it we know that we can tolerate downstream delays or failures by processing, or reprocessing, the messages later. However, Kafka is not the permanent source of truth for our historic data — at the incoming rate of information that we see, we’d need hundreds of boxes to store all of the data just for a few days. So we configure our Kafka cluster to retain information for a few hours (enough time for us to respond to any unexpected, major failures) and get the data to our permanent store, Amazon Simple Storage Service (Amazon S3), as soon as possible. We extensively utilize Storm for real-time data processing, and the first relevant topology is one that reads the information from Kafka and writes it to Amazon S3. Batch computation Once the data is in Amazon S3, we’ve set ourselves up for being able to compute anything that our data will allow us to via Amazon Elastic MapReduce (Amazon EMR). This includes batch jobs for all of the data that our customers see in their dashboards, as well as experimental jobs as we work on new features. We write our MapReduce in Cascading and run them via Amazon EMR. Amazon EMR reads the data that we’ve archived in Amazon S3 as input and writes the results back out to Amazon S3 once processing is complete. We detect the jobs’ completion via a scheduler topology running in Storm and pump the output from Amazon S3 into a Cassandra cluster in order to make it available for sub-second API querying. Speed computation What we have described so far is a durable and fault-tolerant framework for performing analytic computations. There is one glaring problem however — it’s not real time. Some computations run hourly, while others require a full day’s of data as input. The computation times range from minutes to hours, as does the time it takes to get the output from Amazon S3 to a serving layer. Thus, at best, our data would always be a few hours behind, and wouldn’t meet our goals of being real time and actionable . To address this, in parallel to archiving the data as it comes in, we perform stream computations on it. An independent Storm topology consumes the same Kafka topic as our archival topology and performs the same computations that our MapReduce jobs do, but in real time. The outputs of these computations are written to a different independent Cassandra cluster for real-time querying. To compensate for the fact that we have less time, and potentially fewer resources, in the speed layer than the batch, we use probabilistic algorithms like Bloom Filters and HyperLogLog (as well as a few home grown ones). These algorithms enable us to make order-of-magnitude gains in space and time complexity over their brute force alternatives, at the price of a negligible loss of accuracy. Fitting it together So now that we have two independently produced sets of data (batch and speed), how do we combine them to present a single coherent answer? We combine them with logic in our API that utilizes each data set under specific conditions. Because batch computations are repeatable and more fault-tolerant than speed, our API’s always favor batch produced data. So, for example, if our API receives a request for data for a thirty-day, time-series DAU graph, it will first request the full range from the batch-serving Cassandra cluster. If this is a historic query, all of the data will be satisfied there. However, in the more likely case that the query includes the current day, the query will be satisfied mostly by batch produced data, and just the most recent day or two will be satisfied by speed data. Handling of failure scenarios Let’s go over a few different failure scenarios and see how this architecture allows us to gracefully degrade instead of go down or lose data when faced with them. We already discussed the on-device retry-after-back-off strategy. The retry ensures that data eventually gets to our servers in the presence of client-side network unavailability, or brief server outages on the back-end. A randomized back-off ensures that devices don’t overwhelm (DDos) our servers after a brief network outage in a single region or a brief period of unavailability of our back-end servers. What happens if our speed (real-time) processing layer goes down? Our on-call engineers will get paged and address the problem. Since the input to the speed processing layer is a durable Kafka cluster, no data will have been lost and once the speed layer is back and functioning, it will catch up on the data that it should have processed during its downtime. Since the speed layer is completely decoupled from the batch layer, batch layer processing will go-on unimpacted. Thus the only impact is delay in real-time updates to data points for the duration of the speed layer outage. What happens if there are issues or severe delays in the batch layer? Our APIs will seamlessly query for more data from the speed layer. A time-series query that may have previously received one day of data from the speed layer will now query it for two or three days of data. Since the speed layer is completely decoupled from the batch layer, speed layer processing will go-on unimpacted. At the same time, our on-call engineers will get paged and address the batch layer issues. Once the batch layer is back up, it will catch up on delayed data processing, and our APIs will once again seamlessly utilize the batch produced data that is now available. Our back-end architecture consists of four major components: event reception, event archival, speed computation, and batch computation. Durable queues between each of these components ensure that an outage in one of the components does not spill over to others and that we can later recover from the outage. Query logic in our APIs allows us to seamlessly gracefully degrade and then recover when one of the computations layers is delayed or down and then comes back up. Our goal for Answers is to create a dashboard that makes understanding your user base dead simple so you can spend your time building amazing experiences, not digging through data. Learn more about Answers here and get started today. Big thanks to the Answers team for all their efforts in making this architecture a reality. Also to Nathan Marz for his Big Data book . Contributors Andrew Jorgensen , Brian Swift , Brian Hatfield , Michael Furtak , Mark Pirri , Cory Dolphin , Jamie Rothfeder , Jeff Seibert , Justin Starry , Kevin Robinson , Kristen Johnson , Marc Richards , Patrick McGee , Rich Paret , Wayne Chang .", "date": "2015-02-17"},
{"website": "Twitter-Engineering", "title": "All about Apache Aurora", "author": ["‎@davelester‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2015/all-about-apache-aurora.html", "abstract": "Today we’re excited to see the Apache Aurora community announce the 0.7.0 release . Since we began development on this project, Aurora has become a critical part of how we run services at Twitter. Now a fledgling open source project, Aurora is actively developed by a community of developers running it in production. This is Aurora’s third release since joining the Apache Incubator . New features include beta integration with Docker – which allows developers to deploy their pre-packaged applications as lightweight containers – as well as full support for a new command-line client that makes it even simpler to deploy services via Aurora. A full changelog is available documenting more updates. https://twitter.com/ApacheAurora/status/565914721165918208 What is Aurora? Platforms like Twitter operate across tens of thousands of machines, with hundreds of engineers deploying software daily. In this type of environment, automation is critical. Aurora is software that keeps services running in the face of many types of failure, and provides engineers a convenient, automated way to create and update these services. To accomplish this, Aurora leverages the Apache Mesos cluster manager, which provides information about the state of the cluster. Aurora uses that knowledge to make scheduling decisions. For example, when a machine experiences failure Aurora automatically reschedules those previously-running services onto a healthy machine in order to keep them running. History of Aurora at Twitter Development on Aurora began in 2010 when Bill Farner , an engineer on Twitter’s research team, started a project to simplify operations of the many services making up Twitter’s architecture. Bill formerly worked at Google and wanted the project to achieve similar goals as Borg, Google’s cluster manager. Over time, a growing team of Twitter engineers began to contribute directly to and rely on Aurora. Additionally, Aurora began to host key Twitter services such as Twitter’s ad-serving platform. Today, all new stateless services – and most existing services – run on Aurora. Not only does Aurora assist with keeping the platform running, Aurora has helped push utilization of our cluster. This has allowed us to run more applications on fewer servers and substantially save costs while increasing efficiency. A growing open source community In late 2013, we shared our work with the open source community by releasing Aurora into the Apache Incubator . Since then, Aurora has been cultivating a vibrant and growing community. The @ApacheAurora community holds weekly IRC meetings open to the public for participation. To get involved, you can join #aurora on irc.freenode.net every Monday at 11 a.m. PT. If you’d like to learn more about Aurora and how it’s used at huge scale in production, we’re hosting an Aurora meetup at Twitter HQ on Thursday, Feb. 19. This meetup will feature talks from Steve Niemitz ( @steveniemitz ) from TellApart, who contributed to the Docker integration in the latest release, well as Joe Smith , one of the small number of SREs at Twitter who run Aurora. We’d love to see you there . Besides Twitter, many companies are currently using Aurora. Acknowledgements Many people have worked to make Aurora possible both across Twitter and the greater community. Special thanks to the release manager for the 0.7.0 release, Maxim Khutornenko , and the many other contributors for this release.", "date": "2015-02-12"},
{"website": "Twitter-Engineering", "title": "Introducing practical and robust anomaly detection in a time series", "author": ["‎@arun_kejariwal‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2015/introducing-practical-and-robust-anomaly-detection-in-a-time-series.html", "abstract": "Both last year and this year, we saw a spike in the number of photos uploaded to Twitter on Christmas Eve, Christmas and New Year’s Eve (in other words, an anomaly occurred in the corresponding time series). Today, we’re announcing AnomalyDetection , our open-source R package that automatically detects anomalies like these in big data in a practical and robust way. Time series from Christmas Eve 2014 Time series from Christmas Eve 2013 Early detection of anomalies plays a key role in ensuring high-fidelity data is available to our own product teams and those of our data partners. This package helps us monitor spikes in user engagement on the platform surrounding holidays, major sporting events or during breaking news. Beyond surges in social engagement, exogenic factors – such as bots or spammers – may cause an anomaly in number of favorites or followers. The package can be used to find such bots or spam, as well as detect anomalies in system metrics after a new software release. We’re open-sourcing AnomalyDetection because we’d like the public community to evolve the package and learn from it as we have. Recently , we open-sourced BreakoutDetection , a complementary R package for automatic detection of one or more breakouts in time series. While anomalies are point-in-time anomalous data points, breakouts are characterized by a ramp up from one steady state to another. Despite prior research in anomaly detection [1], these techniques are not applicable in the context of social network data because of its inherent seasonal and trend components. Also, as pointed out by Chandola et al. [2], anomalies are contextual in nature and hence, techniques developed for anomaly detection in one domain can rarely be used ‘as is’ in another domain. Broadly, an anomaly can be characterized in the following ways: Global/Local: At Twitter, we observe distinct seasonal patterns in most of the time series we monitor in production. Furthermore, we monitor multiple modes in a given time period. The seasonal nature can be ascribed to a multitude of reasons such as different user behavior across different geographies. Additionally, over longer periods of time, we observe an underlying trend. This can be explained, in part, by organic growth. As the figure below shows, global anomalies typically extend above or below expected seasonality and are therefore not subject to seasonality and underlying trend. On the other hand, local anomalies, or anomalies which occur inside seasonal patterns, are masked and thus are much more difficult to detect in a robust fashion. Illustrates positive/negative, global/local anomalies detected in real data Positive/Negative: An anomaly can be positive or negative. An example of a positive anomaly is a point-in-time increase in number of Tweets during the Super Bowl. An example of a negative anomaly is a point-in-time decrease in QPS (queries per second). Robust detection of positive anomalies serves a key role in efficient capacity planning. Detection of negative anomalies helps discover potential hardware and data collection issues. How does the package work? The primary algorithm, Seasonal Hybrid ESD (S-H-ESD), builds upon the Generalized ESD test [3] for detecting anomalies. S-H-ESD can be used to detect both global and local anomalies. This is achieved by employing time series decomposition and using robust statistical metrics , viz., median together with ESD. In addition, for long time series such as 6 months of minutely data, the algorithm employs piecewise approximation. This is rooted to the fact that trend extraction in the presence of anomalies is non-trivial for anomaly detection [4]. The figure below shows large global anomalies present in the raw data and the local (intra-day) anomalies that S-H-ESD exposes in the residual component via our statistically robust decomposition technique. Besides time series, the package can also be used to detect anomalies in a vector of numerical values. We have found this very useful as many times the corresponding timestamps are not available. The package provides rich visualization support. The user can specify the direction of anomalies, the window of interest (such as last day, last hour) and enable or disable piecewise approximation. Additionally, the x- and y-axis are annotated in a way to assist with visual data analysis. Getting started To begin, install the R package using the commands below on the R console: install.packages(\"devtools\")\ndevtools::install_github(\"twitter/AnomalyDetection\")\nlibrary(AnomalyDetection) The function AnomalyDetectionTs is used to discover statistically meaningful anomalies in the input time series. The documentation of the function AnomalyDetectionTs details the input arguments and output of the function AnomalyDetectionTs, which can be seen by using the command below. help(AnomalyDetectionTs) An example The user is recommended to use the example dataset which comes with the packages. Execute the following commands: data(raw_data)\nres = AnomalyDetectionTs(raw_data, max_anoms=0.02, direction='both', plot=TRUE)\nres$plot This yields the following plot: From the plot, we can tell that the input time series experiences both positive and negative anomalies. Furthermore, many of the anomalies in the time series are local anomalies within the bounds of the time series’ seasonality. Therefore, these anomalies can’t be detected using the traditional methods. The anomalies detected using the proposed technique are annotated on the plot. In case the timestamps for the plot above were not available, anomaly detection could then be carried out using the AnomalyDetectionVec function. Specifically, you can use the following command: AnomalyDetectionVec(raw_data[,2], max_anoms=0.02, period=1440, direction='both', only_last=FALSE, plot=TRUE) Often, anomaly detection is carried out on a periodic basis. For instance, you may be interested in determining whether there were any anomalies yesterday. To this end, we support a flag only_last where one can subset the anomalies that occurred during the last day or last hour. The following command res = AnomalyDetectionTs(raw_data, max_anoms=0.02, direction='both', only_last=\"day\", plot=TRUE)\nres$plot yields the following plot: From the above plot, we observe that only the anomalies that occurred during the last day have been annotated. Additionally, the prior six days are included to expose the seasonal nature of the time series but are put in the background as the window of primary interest is the last day. Anomaly detection for long duration time series can be carried out by setting the longterm argument to T. An example plot corresponding to this (for a different data set) is shown below: Acknowledgements Our thanks to James Tsiamis and Scott Wong for their assistance, and Owen Vallis ( @OwenVallis ) and Jordan Hochenbaum ( @jnatanh ) for this research. References [1] Charu C. Aggarwal. “ Outlier analysis ”. Springer, 2013. [2] Varun Chandola, Arindam Banerjee, and Vipin Kumar. “ Anomaly detection: A survey ”. ACM Computing Surveys, 41(3):15:1{15:58, July 2009. [3] Rosner, B., (May 1983), “ Percentage Points for a Generalized ESD Many-Outlier Procedure ”, Technometrics, 25(2), pp. 165-172. [4] Vallis, O., Hochenbaum, J. and Kejariwal, A., (2014) “ A Novel Technique for Long-Term Anomaly Detection in the Cloud ”, 6th USENIX Workshop on Hot Topics in Cloud Computing, Philadelphia, PA.", "date": "2015-01-06"},
{"website": "Twitter-Engineering", "title": "2015", "author": ["‎@‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2015.html", "abstract": "", "date": "1970-01-01"},
{"website": "Twitter-Engineering", "title": "Building a complete Tweet index  ", "author": ["‎@yz‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2014/building-a-complete-tweet-index.html", "abstract": "Today, we are pleased to announce that Twitter now indexes every public Tweet since 2006. Since that first simple Tweet over eight years ago, hundreds of billions of Tweets have captured everyday human experiences and major historical events. Our search engine excelled at surfacing breaking news and events in real time, and our search index infrastructure reflected this strong emphasis on recency. But our long-standing goal has been to let people search through every Tweet ever published. This new infrastructure enables many use cases, providing comprehensive results for entire TV and sports seasons, conferences ( #TEDGlobal ), industry discussions ( #MobilePayments ), places, businesses and long-lived hashtag conversations across topics, such as #JapanEarthquake , #Election2012 , #ScotlandDecides , #HongKong , #Ferguson and many more. This change will be rolling out to users over the next few days. In this post, we describe how we built a search service that efficiently indexes roughly half a trillion documents and serves queries with an average latency of under 100ms. The most important factors in our design were: Modularity : Twitter already had a real-time index (an inverted index containing about a week’s worth of recent Tweets). We shared source code and tests between the two indices where possible, which created a cleaner system in less time. Scalability : The full index is more than 100 times larger than our real-time index and grows by several billion Tweets a week. Our fixed-size real-time index clusters are non-trivial to expand; adding capacity requires re-partitioning and significant operational overhead. We needed a system that expands in place gracefully. Cost effectiveness : Our real-time index is fully stored in RAM for low latency and fast updates. However, using the same RAM technology for the full index would have been prohibitively expensive. Simple interface : Partitioning is unavoidable at this scale. But we wanted a simple interface that hides the underlying partitions so that internal clients can treat the cluster as a single endpoint. I ncremental development: The goal of “indexing every Tweet” was not achieved in one quarter. The full index builds on previous foundational projects. In 2012, we built a small historical index of approximately two billion top Tweets, developing an offline data aggregation and preprocessing pipeline. In 2013, we expanded that index by an order of magnitude, evaluating and tuning SSD performance. In 2014, we built the full index with a multi-tier architecture, focusing on scalability and operability. Overview The system consists four main parts: a batched data aggregation and preprocess pipeline; an inverted index builder; Earlybird shards; and Earlybird roots. Read on for a high-level overview of each component. Batched data aggregation and preprocessing The ingestion pipeline for our real-time index processes individual Tweets one at a time. In contrast, the full index uses a batch processing pipeline, where each batch is a day of Tweets. We wanted our offline batch processing jobs to share as much code as possible with our real-time ingestion pipeline, while still remaining efficient. To do this, we packaged the relevant real-time ingestion code into Pig User-Defined Functions so that we could reuse it in Pig jobs (soon, moving to Scalding), and created a pipeline of Hadoop jobs to aggregate data and preprocess Tweets on Hadoop. The pipeline is shown in this diagram: The daily data aggregation and preprocess pipeline consists of these components: Engagement aggregator: Counts the number of engagements for each Tweet in a given day. These engagement counts are used later as an input in scoring each Tweet. Aggregation: Joins multiple data sources together based on Tweet ID. Ingestion: Performs different types of preprocessing — language identification, tokenization, text feature extraction, URL resolution and more. Scorer: Computes a score based on features extracted during Ingestion. For the smaller historical indices, this score determined which Tweets were selected into the index. Partitioner: Divides the data into smaller chunks through our hashing algorithm. The final output is stored into HDFS. This pipeline was designed to run against a single day of Tweets. We set up the pipeline to run every day to process data incrementally. This setup had two main benefits. It allowed us to incrementally update the index with new data without having to fully rebuild too frequently. And because processing for each day is set up to be fully independent, the pipeline could be massively parallelizable on Hadoop. This allowed us to efficiently rebuild the full index periodically (e.g. to add new indexed fields or change tokenization) Inverted index building The daily data aggregation and preprocess job outputs one record per Tweet. That output is already tokenized, but not yet inverted. So our next step was to set up single-threaded, stateless inverted index builders that run on Mesos . The inverted index builder consists of the following components: Segment partitioner: Groups multiple batches of preprocessed daily Tweet data from the same partition into bundles. We call these bundles “segments.” Segment indexer: Inverts each Tweet in a segment, builds an inverted index and stores the inverted index into HDFS. The beauty of these inverted index builders is that they are very simple. They are single-threaded and stateless, and these small builders can be massively parallelized on Mesos (we have launched well over a thousand parallel builders in some cases). These inverted index builders can coordinate with each other by placing locks on ZooKeeper, which ensures that two builders don’t build the same segment. Using this approach, we rebuilt inverted indices for nearly half a trillion Tweets in only about two days (fun fact: our bottleneck is actually the Hadoop namenode). Earlybirds shards The inverted index builders produced hundreds of inverted index segments. These segments were then distributed to machines called Earlybirds . Since each Earlybird machine could only serve a small portion of the full Tweet corpus, we had to introduce sharding. In the past, we distributed segments into different hosts using a hash function. This works well with our real-time index, which remains a constant size over time. However, our full index clusters needed to grow continuously. With simple hash partitioning, expanding clusters in place involves a non-trivial amount of operational work – data needs to be shuffled around as the number of hash partitions increases. Instead, we created a two-dimensional sharding scheme to distribute index segments onto serving Earlybirds. With this two-dimensional sharding, we can expand our cluster without modifying existing hosts in the cluster: Temporal sharding: The Tweet corpus was first divided into multiple time tiers. Hash partitioning: Within each time tier, data was divided into partitions based on a hash function. Earlybird: Within each hash partition, data was further divided into chunks called Segments. Segments were grouped together based on how many could fit on each Earlybird machine. Replicas: Each Earlybird machine is replicated to increase serving capacity and resilience. The sharding is shown in this diagram: This setup makes cluster expansion simple: To grow data capacity over time, we will add time tiers. Existing time tiers will remain unchanged. This allows us to expand the cluster in place. To grow serving capacity (QPS) over time, we can add more replicas. This setup allowed us to avoid adding hash partitions, which is non-trivial if we want to perform data shuffling without taking the cluster offline. A larger number of Earlybird machines per cluster translates to more operational overhead. We reduced cluster size by: Packing more segments onto each Earlybird (reducing hash partition count). Increasing the amount of QPS each Earlybird could serve (reducing replicas). In order to pack more segments onto each Earlybird, we needed to find a different storage medium. RAM was too expensive. Even worse, our ability to plug large amounts of RAM into each machine would have been physically limited by the number of DIMM slots per machine. SSDs were significantly less expensive ($/terabyte) than RAM. SSDs also provided much higher read/write performance compared to regular spindle disks. However, SSDs were still orders of magnitude slower than RAM. Switching from RAM to SSD, our Earlybird QPS capacity took a major hit. To increase serving capacity, we made multiple optimizations such as tuning kernel parameters to optimize SSD performance, packing multiple DocValues fields together to reduce SSD random access, loading frequently accessed fields directly in-process and more. These optimizations are not covered in detail in this blog post. Earlybird roots This two-dimensional sharding addressed cluster scaling and expansion. However, we did not want API clients to have to scatter gather from the hash partitions and time tiers in order to serve a single query. To keep the client API simple, we introduced roots to abstract away the internal details of tiering and partitioning in the full index. The roots perform a two level scatter-gather as shown in the below diagram, merging search results and term statistics histograms. This results in a simple API, and it appears to our clients that they are hitting a single endpoint. In addition, this two level merging setup allows us to perform additional optimizations, such as avoiding forwarding requests to time tiers not relevant to the search query. Looking ahead For now, complete results from the full index will appear in the “All” tab of search results on the Twitter web client and Twitter for iOS & Twitter for Android apps. Over time, you’ll see more Tweets from this index appearing in the “Top” tab of search results and in new product experiences powered by this index. Try it out : you can search for the first Tweets about New Years between Dec. 30, 2006 and Jan. 2, 2007. The full index is a major infrastructure investment and part of ongoing improvements to the search and discovery experience on Twitter. There is still more exciting work ahead, such as optimizations for smart caching. If this project sounds interesting to you, we could use your help – join the flock ! Acknowledgments The full index project described in this post was led by Yi Zhuang and Paul Burstein . However, it builds on multiple years of related work. Many thanks to the team members that made this project possible. Contributors : Forrest Bennett , Steve Bezek , Paul Burstein , Michael Busch , Chris Dudte , Keita Fujii , Vibhav Garg , Mila Hardt , Justin Hurley , Aaron Hook , Nik Johnson , Brian Larson , Aaron Lewis , Zhenghua Li , Patrick Lok , Sam Luckenbill, Gilad Mishne , Yatharth Saraf, Jayarama Shenoy , Thomas Snider , Haixin Tie , Owen Vallis , Jane Wang , John Wang , Lei Wang , Tian Wang , Bryce Yan , Jim Youll , Min Zeng , Kevin Zhao , Yi Zhuang", "date": "2014-11-18"},
{"website": "Twitter-Engineering", "title": "Breakout detection in the wild", "author": ["‎@arun_kejariwal‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2014/breakout-detection-in-the-wild.html", "abstract": "Nowadays, BigData is leveraged in every sphere of business: decision making for new products, gauging user engagement, making recommendations for products, health care, data center efficiency and more. A common form of BigData is time series data. With the progressively decreasing costs of collecting and mining large data sets, it’s become increasingly common that companies – including Twitter – collect millions of metrics on a daily basis [ 1 , 2 , 3 ]. Exogenic and/or endogenic factors often give rise to breakouts in a time series. Breakouts can potentially have ramifications on the user experience and/or on a business’ bottom line. For example, in the context of cloud infrastructure, breakouts in time series data of system metrics – that may happen due to a hardware issues – could impact availability and performance of a service. Given the real-time nature of Twitter, and that high performance is key for delivering the best experience to our users, early detection of breakouts is of paramount importance. Breakout detection has also been used to detect change in user engagement during popular live events such as the Oscars, Super Bowl and World Cup. A breakout is typically characterized by two steady states and an intermediate transition period. Broadly speaking, breakouts have two flavors: Mean shift: A sudden jump in the time series corresponds to a mean shift. A sudden jump in CPU utilization from 40% to 60% would exemplify a mean shift. Ramp up: A gradual increase in the value of the metric from one steady state to another constitutes a ramp up. A gradual increase in CPU utilization from 40% to 60% would exemplify a ramp up. The figure below illustrates multiple mean shifts in real data. Given the ever-growing number of metrics being collected, it’s imperative to automatically detect breakouts. Although a large body of research already exists on breakout detection, existing techniques are not suitable for detecting breakouts in cloud data. This can be ascribed to the fact that existing techniques are not robust in the presence of anomalies (which are not uncommon in cloud data). Today, we’re excited to announce the release of BreakoutDetection, an open-source R package that makes breakout detection simple and fast. With its release, we hope that the community can benefit from the package as we have at Twitter and improve it over time. Our main motivation behind creating the package has been to develop a technique to detect breakouts which are robust, from a statistical standpoint, in the presence of anomalies. The BreakoutDetection package can be used in wide variety of contexts. For example, detecting breakout in user engagement post an A/B test, detecting behavioral change , or for problems in econometrics, financial engineering, political and social sciences. How the package works The underlying algorithm – referred to as E-Divisive with Medians (EDM) – employs energy statistics to detect divergence in mean. Note that EDM can also be used detect change in distribution in a given time series. EDM uses robust statistical metrics , viz., median, and estimates the statistical significance of a breakout through a permutation test. In addition, EDM is non-parametric. This is important since the distribution of production data seldom (if at all) follows the commonly assumed normal distribution or any other widely accepted model. Our experience has been that time series often contain more than one breakout. To this end, the package can also be used to detect multiple breakouts in a given time series. How to get started Install the R package using the following commands on the R console: install.packages(\"devtools\")\ndevtools::install_github(\"twitter/BreakoutDetection\")\nlibrary(BreakoutDetection) The function breakout is called to detect one or more statistically significant breakouts in the input time series. The documentation of the function breakout, which can be seen by using the following command, details the input arguments and the output of the function breakout. help(breakout) A simple example To get started, the user is recommended to use the example dataset which comes with the packages. Execute the following commands: data(Scribe)\nres = breakout(Scribe, min.size=24, method='multi', beta=.001, degree=1, plot=TRUE)\nres$plot The above yields the following plot: From the above plot, we observe that the input time series experiences a breakout and also has quite a few anomalies. The two red vertical lines denote the locations of the breakouts detected by the EDM algorithm. Unlike the existing approaches mentioned earlier, EDM is robust in the presence of anomalies. The change in mean in the time series can be better viewed with the following annotated plot: The horizontal lines in the annotated plot above correspond to the approximate (i.e., filtering out the effect of anomalies) mean for each window. Acknowledgements We thank James Tsiamis and Scott Wong for their support and Nicholas James as the primary researcher behind this research.", "date": "2014-10-24"},
{"website": "Twitter-Engineering", "title": "Investing in MIT’s new Laboratory for Social Machines", "author": ["‎@mjgillis‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2014/investing-in-mit-s-new-laboratory-for-social-machines.html", "abstract": "Today, @MIT announced the creation of the Laboratory for Social Machines, funded by a five-year, $10 million commitment from Twitter. Through @Gnip , MIT will also have access to our full stream of public Tweets and complete corpus of historical public Tweets, starting with Jack Dorsey’s first Tweet in 2006. This is an exciting step for all of us at Twitter as we continue to develop new ways to support the research community. Building on the success of the Twitter Data Grants program, which attracted more than 1,300 applications, we remain committed to making public Twitter data available to researchers, instructors and students. We’ve already seen Twitter data being used in everything from epidemiology to natural disaster response . The Laboratory for Social Machines anticipates using Twitter data to investigate the rapidly changing and intersecting worlds of news, government and collective action. The hope is that their research team will be able to understand how movements are started by better understanding how information spreads on Twitter. We look forward to the innovative research generated by the Laboratory for Social Machines and to further increasing Twitter’s footprint in the research community. For more information about the Laboratory for Social Machines, please visit socialmachines.media.mit.edu .", "date": "2014-10-01"},
{"website": "Twitter-Engineering", "title": "Celebrating over a year of @FlightJS", "author": ["‎@tgvashworth‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2014/celebrating-over-a-year-of-flightjs.html", "abstract": "Over a year ago, we open-sourced FlightJS , a lightweight JavaScript framework for structuring web applications which was designed based on our experience scaling Twitter front-end projects. Since then, we have seen an independent community grow with over 45 contributors who created new projects like a Yeoman generator and an easier way to find new components . FlightJS was also adopted by other companies like @Airbnb and @gumroad . Easier to get started Over the pear year, we made it easier to get started with FlightJS due to the generator. Assuming you have NPM installed, simply start with this command to install the generator: npm install -g generator-flight Once that is done, you can bootstrap your FlightJS application using Yeoman and a variety of generator commands : yo flight hello-world-flight yo flight:component hello-word This will scaffold your application file structure, install the necessary library code and configure your test setup. It’s as simple as that. The withChildComponents mixin Grown out of TweetDeck’s codebase, withChildComponents is a mixin for FlightJS that we recently open sourced. It offers a way to nest components, automatically managing component lifecycles to avoid memory leaks and hard-to-maintain code. The mixin binds two or more components’ lifecycles in a parent-child relationship using events, matching the FlightJS philosophy that no component should ever have a reference to any other. A child component’s lifecycle is bound to its parent’s, and a series of child components can be joined in this way to form a tree. To host children, a component should use the attachChild method exposed by this mixin. The attachChild method, using the FlightJS late-binding mixin method, initializes the child up so that it will teardown when the parent tears down. An event, signifying that the parent is tearing down, is is passed by attachChild to the child component as its teardownOn attribute. By default, this childTeardownEvent is unique to the parent but can be overwritten to group components or define other teardown behaviors. Before the parent tears down, the childTeardownEvent is triggered. Child components, listening for this event, will teardown. If a child is hosting its own children, it will then trigger its own childTeardownEvent so that any children it attached will teardown. The teardown order is therefore equivalent to depth first traversal of the component tree. For example, a Tweet composition pane might be structured like this: The darker blue components mix-in withChildComponents , and use attachChild to attach a new component and bind the child’s lifecycle to their own using their childTeardownEvent . The Compose component might look something like this. It attaches the ComposeBox when it initializes, and invokes teardown when it detects a Tweet being sent. function Compose() {\n    this.after('initialize', function () {\n        this.attachChild(ComposeBox, '.compose-box', {\n            initialText: 'Some initial text...'\n        });\n\n        this.on('sendTweet', this.teardown);\n    });\n} The ComposeBox also attaches its children during initialization, as well as attaching some behaviour to its own teardown using advice. function ComposeBox() {\n    this.after('initialize', function () {\n        this.attachChild(TweetButton, ...);\n        this.attachChild(CharacterCount, ...);\n    });\n\n    this.before('teardown', function () {\n        this.select('composeTextareaSelector')\n            .attr('disabled', true);\n    });\n} If the Compose pane were torn down – perhaps because a Tweet was sent – the first event to fire would be childTeardownEvent-1 , which would cause the ComposeBox and AccountPicker components to teardown. The ComposeBox would fire childTeardownEvent-2 , causing the TweetButton and CharacterCount to teardown. Of course, if the ComposeBox was torn down on its own, only the TweetButton and the CharacterCount components would teardown with it – you can teardown only part of a component tree if you need to. TweetDeck and the withChildComponents mixin TweetDeck uses withChildComponents to tie logical groups of UI components together into pages or screens. For example, our login UI has a top level component named Startflow that nests components to look after the login form. When a user successfully logs in, the Startflow component tears down, and that brings the login forms with it. This centralises the logic for a successful login, and changes to this flow can be made without looking at all files concerned with login. We also don’t have to worry about removing DOM nodes and forgetting to rip out the component too! The withChildComponents mixin helps TweetDeck manage nested components in a simple way, abstracting away memory management and a good deal of complexity. Feedback encouraged FlightJS is an ongoing and evolving project. We’re planning to make more of the utilities and mixins used on the Twitter website and in TweetDeck available over time and look forward to your contributions and comments. If you’re interested in learning more, check out the FlightJS mailing list , #flightjs on Freenode and follow @flightjs for updates.", "date": "2014-09-19"},
{"website": "Twitter-Engineering", "title": "Hello Pants build", "author": ["‎@Ity‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2014/hello-pants-build.html", "abstract": "As codebases grow, they become increasingly difficult to work with. Builds get ever slower and existing tooling doesn’t scale. One solution is to keep splitting the code into more and more independent repositories — but you end up with hundreds of free-floating codebases with hard-to-manage dependencies. This makes it hard to discover, navigate and share code, which can affect developer productivity. Another solution is to have a single large, unified codebase. We’ve found that this promotes better engineering team cohesion and collaboration, which results in greater productivity and happiness. But tooling for such structured codebases has been lacking. That’s why we developed Pants , an open source build system written in Python. https://twitter.com/TwitterOSS/status/465934496475271168 Pants models code modules (known as “targets”) and their dependencies in BUILD files — in a manner similar to Google’s internal build system . This allows it to only build the parts of the codebase you actually need, ignoring the rest of the code. That’s a key requirement for scaling large, unified codebases. Pants started out in 2010 as an internal tool here, and was originally just a frontend to generate build.xml files for the Ant build tool, hence the name (a contraction of “Python Ant”). Pants grew in capability and complexity, and became the build tool for the twitter/commons open source libraries , and hence became open source itself. In 2012, Foursquare began using Pants internally, and Foursquare engineers picked up the Pants development mantle, adding Scala support, build artifact caching and many other features. Since then, several more engineering teams, including those at Urban Compass and Oscar, have integrated Pants into their codebases. Most recently, Square began to use Pants, and has contributed significantly to its development. As a result, Pants is a true independent open source project with collaborators across companies and a growing development community . It now lives in a standalone GitHub repo at github.com/pantsbuild/pants and we’ve welcomed more committers to the project. Among Pants’ current strengths: Builds Java, Scala and Python. Adding support for new languages is straightforward. Supports code generation: thrift, protocol buffers, custom code generators. Resolves external JVM and Python dependencies. Runs tests. Spawns Python and Scala REPLs with appropriate load paths. Creates deployable packages. Scales to large repos with many interdependent modules. Designed for incremental builds. Support for local and distributed caching. Especially fast for Scala builds, compared to alternatives. Builds standalone python executables (PEX files). Has a plugin system to add custom features and override stock behavior. Runs on Linux and Mac OS X. If your codebase is growing beyond your toolchain’s ability to scale, but you’re reluctant to split it up, you might want to give Pants a try. It may be of particular interest if you have complex dependencies, generated code and custom build steps. Pants is still a young and evolving open source project. We constantly strive to make it easier to use. If you’re interested in using or learning from Pants, reach out to the community on the developer mailing list and follow @pantsbuild for updates.", "date": "2014-09-16"},
{"website": "Twitter-Engineering", "title": "All-pairs similarity via DIMSUM", "author": ["‎@Reza_Zadeh‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2014/all-pairs-similarity-via-dimsum.html", "abstract": "We are often interested in finding users, hashtags and ads that are very similar to one another, so they may be recommended and shown to users and advertisers. To do this, we must consider many pairs of items, and evaluate how “similar” they are to one another. We call this the “all-pairs similarity” problem, sometimes known as a “similarity join.” We have developed a new efficient algorithm to solve the similarity join called “Dimension Independent Matrix Square using MapReduce,” or DIMSUM for short, which made one of our most expensive computations 40% more efficient. Introduction To describe the problem we’re trying to solve more formally, when given a dataset of sparse vector data, the all-pairs similarity problem is to find all similar vector pairs according to a similarity function such as cosine similarity , and a given similarity score threshold. Not all pairs of items are similar to one another, and yet a naive algorithm will spend computational effort to consider even those pairs of items that are not very similar. The brute force approach of considering all pairs of items quickly breaks, since its computational effort scales quadratically. For example, for a million vectors, it is not feasible to check all roughly trillion pairs to see if they’re above the similarity threshold. Having said that, there exist clever sampling techniques to focus the computational effort on only those pairs that are above the similarity threshold, thereby making the problem feasible. We’ve developed the DIMSUM sampling scheme to focus the computational effort on only those pairs that are highly similar, thus making the problem feasible. In November 2012, we reported the DISCO algorithm to solve the similarity join problem using MapReduce. More recently, we have started using a new version called DIMSUMv2, and the purpose of this blog post is to report experiments and contributions of the new algorithm to two open-source projects. We have contributed DIMSUMv2 to the Spark and Scalding open-source projects. The algorithm First, let’s lay down some notation: we’re looking for all pairs of similar columns in an m x n matrix whose entries are denoted a_ij, with the i’th row denoted r_i and the j’th column denoted c_j. There is an oversampling parameter labeled ɣ that should be set to 4 log(n)/s to get provably correct results (with high probability), where s is the similarity threshold. The algorithm is stated with a Map and Reduce , with proofs of correctness and efficiency in published papers [1] [2]. The reducer is simply the summation reducer. The mapper is more interesting, and is also the heart of the scheme. As an exercise, you should try to see why in expectation, the map-reduce below outputs cosine similarities. The mapper above is more computationally efficient than the mapper presented in [1], in that it tosses fewer coins than the one presented in [1]. Nonetheless, its proof of correctness is the same as Theorem 1 mentioned in [1]. It is also more general than the algorithm presented in [2] since it can handle real-valued vectors, as opposed to only {0,1}-valued vectors. Lastly, this version of DIMSUM is suited to handle rows that may be skewed and have many nonzeros. Experiments We run DIMSUM daily on a production-scale ads dataset. Upon replacing the traditional cosine similarity computation in late June, we observed 40% improvement in several performance measures, plotted below. Open source code We have contributed an implementation of DIMSUM to two open source projects: Scalding and Spark. Scalding github pull-request: https://github.com/twitter/scalding/pull/833 Spark github pull-request: https://github.com/apache/spark/pull/336 Collaborators Thanks to Kevin Lin, Oscar Boykin, Ashish Goel and Gunnar Carlsson. References [1] Bosagh-Zadeh, Reza and Carlsson, Gunnar (2013), Dimension Independent Matrix Square using MapReduce, arXiv:1304.1467 [2] Bosagh-Zadeh, Reza and Goel, Ashish (2012), Dimension Independent Similarity Computation, arXiv:1206.2082", "date": "2014-08-29"},
{"website": "Twitter-Engineering", "title": "Push our limits - reliability testing at Twitter", "author": ["Mazdak Hashemi"], "link": "https://blog.twitter.com/engineering/en_us/a/2014/push-our-limits-reliability-testing-at-twitter.html", "abstract": "At Twitter, we strive to prepare for sustained traffic as well as spikes - some of which we can plan for, some of which comes at unexpected times or in unexpected ways. To help us prepare for these varied types of traffic, we continuously run tests against our infrastructure to ensure it remains a scalable and highly available system. Our Site Reliability Engineering (SRE) team has created a framework to perform different types of load and stress tests. We test different stages of a service life cycle in different environments (e.g., a release candidate service in a staging environment). These tests help us anticipate how our services will handle traffic spikes and ensure we are ready for such events . Additionally, these tests help us to be more confident that the loosely coupled distributed services that power Twitter’s products are highly available and responsive at all times and under any circumstance. As part of our deploy process before releasing a new version of a service, we run a load test to check and validate the performance regressions of the service to estimate how many requests a single instance can handle. While load testing a service in a staging environment is a good release practice, it does not provide insight into how the overall system behaves when it’s overloaded. Services under load fail due to a variety of causes including GC pressure, thread safety violations and system bottlenecks (CPU, network). Below are the typical steps we follow to evaluate a service’s performance. Performance evaluation We evaluate performance in several ways for different purposes; these might be broadly categorized: In staging Load testing: Performing load tests against few instances of a service in non-production environment to identify a new service’s performance baseline or compare a specific build’s performance to the existing baseline for that service. Tap compare: Sending production requests to instances of a service in both production and staging environments and comparing the results for correctness and evaluating performance characteristics. Dark traffic testing: Sending production traffic to a new service to monitor its health and performance characteristics. In this case, the response(s) won’t be sent to the requester(s). In production Canarying: Sending small percentage of production traffic to some number of instances in a cluster which are running a different build (newer in most cases). The goal is to measure the performance characteristics and compare the results to the existing/older versions. Assuming the performance is in an acceptable range, the new version will be pushed to the rest of the cluster. Stress testing: Sending traffic (with specific flags) to the production site to simulate unexpected load spikes or expected organic growth. In this blog, we are primarily focusing on our stress testing framework, challenges, lessons learned, and future work. Framework We usually don’t face the typical performance testing problems such as collecting services’ metrics , allocating resources to generate the load or implementing a load generator . Obviously, any part of your system at scale could get impacted, but some are more resilient and some require more testing. Even though we are still focusing on the items mentioned above, in regards to this blog and this type of work, we are focusing on system complexity and scalability. As part of our reliability testing, we generate distributed multi-datacenter load to analyze the impact and determine the bottlenecks. Our stress-test framework is written in Scala and leverages Iago to create load generators that run on Mesos . Its load generators send requests to the Twitter APIs to simulate Tweet creation, message creation, timeline reading and other types of traffic. We simulate patterns from past events such as New Year’s Eve, the Super Bowl, the Grammys, the State of the Union, NBA Finals, etc. The framework is flexible and integrated with the core services of Twitter infrastructure. We can easily launch jobs that are capable of generating large traffic spikes or a high volume of sustained traffic with minor configuration changes. The configuration file defines the required computational resources, transactions rate, transactions logs, the test module to use, and the targeted service. Figure 1 below shows an example of a launcher configuration file: new ParrotLauncherConfig {\n\n  // Aurora Specification\n  role = \"limittesting\"\n  jobName = \"limittesting\"\n  serverDiskInMb = 4096\n  feederDiskInMb = 4096\n  mesosServerNumCpus = 4.0\n  mesosFeederNumCpus = 4.0\n  mesosServerRamInMb = 4096\n  mesosFeederRamInMb = 4096\n  numInstances = 2\n\n  // Target systems address\n  victimClusterType = \"sdzk\"\n  victims = \"/service/web\"\n\n  // Test specification\n  hadoopConfig = \"/etc/hadoop/conf\"\n  log = \"hdfs://hadoop-nn/limittesting/logs.txt\"\n  duration = 10\n  timeUnit = \"MINUTES\"\n  requestRate = 1000\n  \n  // Testing Module\n  imports = \"import com.twitter.loadtest.Eagle\"\n  responseType = \"HttpResponse\"\n  loadTest = \"new EagleService(service.get)\"\n} The framework starts by launching a job in Aurora, the job scheduler for Mesos. It registers itself in Zookeeper, and publishes into the Twitter observability stack (Viz) and distributed tracing system (Zipkin). This seamless integration lets us monitor the test execution. We can measure test resource usage, transaction volume, transaction time, etc. If we want to increase the traffic volume, we only need a few clicks to change a variable. Figure 2: Load generated during a stress test Challenges Comparing the performance characteristics of test runs is complicated. As we continuously integrate and deliver changes across all services, it gets harder to identify a baseline to compare against. The test’s environment changes many times between test runs due to many factors such as new service builds and releases. The inconsistency in test environments makes it difficult to determine the change that introduced the bottlenecks. If a regression is identified, we study what could contribute to it including, but not limited to, how services behave under upstream and downstream failures, and changes in traffic patterns. In some cases, detecting the root cause can be challenging. The anomaly we detect might not be the root cause, but rather a side effect of upstream or downstream issues. Finding the root cause between thousands of changes across many services is a time consuming process and might require lots of experiments and analysis. Generating the test traffic against a single or multiple data centers requires careful planning and a test case design. Many factors need to be taken into consideration (like cache hit ratio). A cache miss for a tweet read can trigger a cache fill which in turn triggers multiple backend read requests to fill the data. Because things like a cache miss is much more expensive than a cache hit, the generated test traffic must respect these factors to get accurate tests results that match production traffic patterns. Since our platform is real-time, it’s expected for us to observe extra surges of traffic at any point. The two more frequent kinds of load we have seen: heavy traffic during a special event for a few minutes or hours, and spikes that happen in a second or two when users share a moment . Simulating the spikes that last for a few seconds while monitoring the infrastructure to detect anomalies in real time is a complicated problem, and we are actively working on improving our approach. Lesson learned Our initial focus was on overloading the entire Tweet creation path to find limits in specific internal services, verify capacity plans, and understand the overall behavior under stress. We expected to identify weaknesses in the stack, adjust capacity and implement safety checks to protect minor services from upstream problems. However, we quickly learned this approach wasn’t comprehensive. Many of our services have unique architectures that make load testing complicated. We had to focus on prioritizing our efforts, review the main call paths, then design and cover the major scenarios. An example is our internal web crawler service that assigns site crawling tasks to a specific set of machines in a cluster. The service does this for performance reasons since those machines have higher probability of having an already-established connection to the target site. This same service replicates its already-crawled sites to the other data centers to save computing resources and outbound internet bandwidth. Addressing all of these steps complicated the collection of links, their types and their distribution during the test modeling. The distribution of links among load generators throughout the test was a problem because these were real production websites. In response to those challenges, we designed a system that distributes links across all our load generators in a way that guarantees no more than N links of any website are crawled per second across the cluster. We had to specify the link types and distribution carefully. We might have overwhelmed the internal systems if most of the links were invalid, spammy or contain malware. Additionally, we could have overwhelmed the external systems if all links were for a single website. The stack’s overall behavior changes as the percentage of each category changes. We had to find the right balance to design a test that covered all possible scenarios. These custom procedures repeat every time we model new tests. We started our testings methodologies by focusing on specific site features such as tweet write and read paths. Our first approach was to simulate high volume of sustained tweets creation and reads. Due to the real-time nature of our platform, variation of spikes, and types of traffic we observe, we continuously expand our framework to cover additional features such as users retweeted Tweets, favorited Tweets, conversations, etc. The variety of our features (Tweets, Tweet with media, searches, Discover, timeline views, etc) requires diversity in our approach in order to ensure the results of our test simulations are complete and accurate. Twitter’s internal services have mechanisms to protect themselves and their downstreams. For example, a service will start doing in-process caching to prevent cache overwhelming or will raise special exceptions to trigger upstream retry/backoff logic. This complicates test execution because the cache shields downstream services from the load. In fact, when in-process cache kicks in, the service’s overall latency decreases since it no longer requires a round trip to the distributed caches. We had to work around such defence mechanisms by creating multiple tests models around a single test scenario. One test verifies the in-process cache has kicked in; another test simulates the service’s behavior without the in-process cache. This process required changes across the stack to pass and respect special testing flags. After going through that process, we learned to design and architect services with reliability testing in mind to simplify and speed up future tests’ modeling and design. Future work Since Twitter is growing rapidly and our services are changing continuously, our strategies and framework should as well. We continue to improve and in some case redesign our performance testing strategy and frameworks. We are automating the modeling, design, execution of our stress tests, and making the stress-testing framework context-aware so it’s self driven and capable of targeting a specific or N datacenters. If this sounds interesting to you, we could use your help — join the flock ! Special thanks to Ali Alzabarah for leading the efforts to improve and expand our stress-test framework and his hard work and dedication to this blog. Many thanks to a number of folks across @twittereng , specifically — James Waldrop , Tom Howland , Steven Salevan , Dmitriy Ryaboy and Niranjan Baiju . In addition, thanks to Joseph Smith , Michael Leinartas , Larry Hosken , Stephanie Dean and David Bar r for their contributions to this blog post.", "date": "2014-09-02"},
{"website": "Twitter-Engineering", "title": "Bringing more design principles to security", "author": ["‎@neildaswani‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2014/bringing-more-design-principles-to-security.html", "abstract": "To date, much of the web and mobile security focus has been on security bugs such as cross-site-scripting and SQL injection. Due to the number of those issues and the fact that the number of bugs in general increases in proportion to the number of lines of code, it’s clear that if we hope to address software security problems as a community, we also need to invest in designing software securely to eliminate entire classes of bugs. To that end, we are participating in the founding of the IEEE Center for Secure Design, which was announced today, and contributed to the Center’s in-depth report on “Avoiding the top ten software security design flaws.” We hope it serves as a useful resource to help software professionals as well as the community at large build more secure systems. We’ve been using these secure design principles in some form at Twitter, and with their codification by the IEEE, we’ll be further leveraging them in our own internal documentation and processes. As we continue to scale the mobile and web services that we provide, it will be increasingly important to continue taking a holistic, proactive approach to designing secure software to protect our users. Our participation in the IEEE Center for Secure Design is one way we are glad to contribute back to the community while furthering our own approach to secure software design. To learn more about the IEEE Center for Secure Design and download the report, visit cybersecurity.ieee.org .", "date": "2014-08-27"},
{"website": "Twitter-Engineering", "title": "Outreach Program for Women and GSoC 2014 results", "author": ["‎@cra‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2014/outreach-program-for-women-and-gsoc-2014-results.html", "abstract": "We had the opportunity to participate in the Google Summer of Code (GSoC) for the third time and would like to share the resulting open source activities. While many GSoC participating organizations focus on a single ecosystem, we have a variety of projects that span multiple programming languages and communities. And for the first time, we participated in Outreach Program for Women (OPW) — an organization that focuses on helping women get involved in open source. In total, we worked on nine successful projects with nine amazing students over the summer. Outreach Program for Women projects Apache Mesos CLI improvements Isabel Jimenez worked with her mentor Ben Hindman to add new functionality to the Mesos CLI interface. You can read about the work via her blog posts over the summer and review commits associated with the project. Apache Mesos slave unregistration support Alexandra Sava worked with mentor Ben Mahler to add the ability to unregister a slave and have it drain all tasks instead of leaving tasks running underneath it. Check out ReviewBoard to look at the commits that have been merged already, and take a look at her blog posts that summarize her experience. Summer of Code projects Use zero-copy read path in @ApacheParquet Sunyu Duan worked with mentors Julien Le Dem and Gera Shegalov on improving performance in Parquet by using the new ByteBuffer based APIs in Hadoop . As a result of their efforts, performance has improved up to 40% based on initial testing and the work will make its way into the next Parquet release. A pluggable algorithm to choose next EventLoop in Netty Jakob Buchgraber worked with mentor Norman Maurer to add pluggable algorithm support to Netty’s event loop (see pull request ). https://twitter.com/jakobbuchgraber/status/501561485626449920 Currently when a new EventLoop is needed to register a Channel, EventLoopGroup implementations use a round-robin-like algorithm to choose the next EventLoop. Since different different EventLoops might become busier than others over time, the support for pluggable algorithms to increase performance was needed. Various compression codecs for Netty Idel Pivnitskiy worked with mentor Trustin Lee to add multiple compression codes (LZ4, FastLZ and BZip2) to the Netty project. https://twitter.com/idelpivnitskiy/status/501834437059821568 Compression codecs will allow cutting traffic and creating applications, which are able to transfer large amounts of data faster and more effectively. Android support for Pants build Mateo Rodriguez added Android support to the Pants build system (see commits ) so Pants can build Android applications (APKs) on top of the many other languages and tools it supports. https://twitter.com/mateornaut/status/501448396733829122 A pure ZooKeeper client for Finagle Pierre-Antoine Ganaye was mentored by Evan Meagher to add a pure ZooKeeper client to Finagle to improve performance (see project ). https://twitter.com/trypag/status/501552546763534336 An SMTP client for Finagle Lera Dymbitska worked with mentors Selvin George and Travis Brown to add SMTP protocol support to Finagle to improve performance (see pull request ). https://twitter.com/Suncelesta/status/501715527979765760 Since Finagle strives to provide fully asynchronous protocol support, baking in SMTP support was required instead of using third-party libraries (such as javamail and commons-email) which are synchronous by design. Analyze Wikipedia using Cassovary Szymon Matejczyk worked with mentors Pankaj Gupta and Ajeet Grewal to enable Cassovary to analyze Wikipedia data. https://twitter.com/szymonmatejczyk/status/501976468856246273 The result of this work improved the performance of Cassovary when dealing with large graphs. See the commits associated with the project to see how it was done. Summary As part of GSoC, students and mentoring organizations receive a stipend. We are donating our portion of the stipend to the Outreach Program for Women to support their mission of involving more women in open source communities. In the end, we really enjoyed the opportunity to take part in Google Summer of Code and Outreach Program for Women. Thanks again to our nine students and mentors. We look forward to the next time.", "date": "2014-08-25"},
{"website": "Twitter-Engineering", "title": "Fighting spam with BotMaker ", "author": ["‎@raghavj‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2014/fighting-spam-with-botmaker.html", "abstract": "Spam on Twitter is different from traditional spam primarily because of two aspects of our platform: Twitter exposes developer APIs to make it easy to interact with the platform and real-time content is fundamental to our user’s experience. These constraints mean that spammers know (almost) everything Twitter’s anti-spam systems know through the APIs, and anti-spam systems must avoid adding latency to user-visible operations. These operating conditions are a stark contrast to the constraints placed upon more traditional systems, like email, where data is private and adding latency of tens of seconds goes unnoticed. So, to fight spam on Twitter, we built BotMaker, a system that we designed and implemented from the ground up that forms a solid foundation for our principled defense against unsolicited content. The system handles billions of events every day in production, and we have seen a 40% reduction in key spam metrics since launching BotMaker. In this post we introduce BotMaker and discuss our overall architecture. All of the examples in this post are used to illustrate the use of BotMaker, not actual rules running in production. BotMaker architecture Goals, challenges and BotMaker overview The goal of any anti-spam system is to reduce spam that the user sees while having nearly zero false positives. Three key principles guided our design of Botmaker: Prevent spam content from being created . By making it as hard as possible to create spam, we reduce the amount of spam the user sees. Reduce the amount of time spam is visible on Twitter. For the spam content that does get through, we try to clean it up as soon as possible. Reduce the reaction time to new spam attacks. Spam evolves constantly. Spammers respond to the system defenses and the cycle never stops. In order to be effective, we have to be able to collect data, and evaluate and deploy rules and models quickly. BotMaker achieves these goals by receiving events from Twitter’s distributed systems, inspecting the data according to a set of rules, and then acting accordingly. BotMaker rules, or bots as they are known internally, are decomposed into two parts: conditions for deciding whether or not to act on an event, and actions that dictate what the caller should do with this particular event. For example, a simple rule for denying any Tweets that contain a spam url would be: Condition:\nHasSpamUrl(GetUrls(tweetText))\n\nAction:\nDeny() The net effect of this rule is that BotMaker will deny any Tweets that match this condition. The main challenges in supporting this type of system are evaluating rules with low enough latency that they can run on the write path for Twitter’s main features (i.e., Tweets, Retweets, favorites, follows and messages), supporting computationally intense machine learning based rules, and providing Twitter engineers with the ability to modify and create new rules instantaneously. For the remainder of this blog post, we discuss how we solve these challenges. When we run BotMaker The ideal spam defense would detect spam at the time of creation, but in practice this is difficult due to the latency requirements of Twitter. We have a combination of systems (see figure above) that detect spam at various stages. Real time (Scarecrow): Scarecrow detects spam in real time and prevents spam content from getting into the system, and it must run with low latency. Being in the synchronous path of all actions enables Scarecrow to deny writes and to challenge suspicious actions with countermeasures like captchas. Near real time (Sniper): For the spam that gets through Scarecrow’s real time checks, Sniper continuously classifies users and content off the write path. Some machine learning models cannot be evaluated in real time due to the nature of the features that they depend on. These models get evaluated in Sniper. Since Sniper is asynchronous, we can also afford to lookup features that have high latency. Periodic jobs: Models that have to look at user behavior over extended periods of time and extract features from massive amounts of data can be run periodically in offline jobs since latency is not a constraint. While we do use offline jobs for models that need data over a large time window, doing all spam detection by periodically running offline jobs is neither scalable nor effective. The BotMaker rule language In addition to when BotMaker runs, we have put considerable time into designing an intuitive and powerful interface for guiding how BotMaker runs. Specifically: our BotMaker language is type safe, all data structures are immutable, all functions are pure except for a few well marked functions for storing data atomically, and our runtime supports common functional programming idioms. Some of the language highlights include: Human readable syntax. Functions that can be combined to compose complex derived functions. New rules can be added without any code changes or recompilation. Edits to production rules get deployed in seconds. Sample bot Here is a bot that demonstrates some of the above features. Lets say we want to get all users that are receiving blocks due to mentions that they have posted in the last 24 hours. Here is what the rule would look like: Condition: Count(\n    Intersection(\n      UsersBlocking(spammerId, 1day),\n      UsersMentionedBy(spammerId, 1day)\n    )\n  ) >= 1 Actions: Record(spammerId) UsersBlocking and UsersMentionedBy are functions that return lists of users, which the bot intersects and gets a count of the result. If the count is more than one, then the user is recorded for analysis. Impact and lessons learned This figure shows the amount of spam we saw on Twitter before enabling spam checks on the write path for Twitter events. This graph spans 30 days with time on the x-axis and spam volume on the y-axis. After turning on spam checking on the write paths, we saw a 55% drop in spam on the system as a direct result of preventing spam content from being written. BotMaker has also helped us reduce our response time to spam attacks significantly. Before BotMaker, it took hours or days to make a code change, test and deploy, whereas using BotMaker it takes minutes to react. This faster reaction time has dramatically improved developer and operational efficiency, and it has allowed us to rapidly iterate and refine our rules and models, thus reducing the amount of spam on Twitter. Once we launched BotMaker and started using it to fight spam, we saw a 40% reduction in a metric that we use to track spam. Conclusion BotMaker has ushered in a new era of fighting spam at Twitter. With BotMaker, Twitter engineers now have the ability to create new models and rules quickly that can prevent spam before it even enters the system. We designed BotMaker to handle the stringent latency requirements of Twitter’s real-time products, while still supporting more computationally intensive spam rules. BotMaker is already being used in production at Twitter as our main spam-fighting engine. Because of the success we have had handling the massive load of events, and the ease of writing new rules that hit production systems immediately, other groups at Twitter have started using BotMaker for non-spam purposes. BotMaker acts as a fundamental interposition layer in our distributed system. Moving forward, the principles learned from BotMaker can help guide the design and implementation of systems responsible for managing, maintaining and protecting the distributed systems of today and the future.", "date": "2014-08-20"},
{"website": "Twitter-Engineering", "title": "#ScalingTwitter in Dublin and the UK", "author": ["‎@chtemplin‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2014/scalingtwitter-in-dublin-and-the-uk.html", "abstract": "As Twitter continues to expand internationally, our engineering teams are growing around the world. We recently held two #ScalingTwitter tech talks in our Dublin and London engineering hubs to highlight the work of these teams, the unique challenges of operating at scale and how Twitter is addressing these issues. Over 100 guests attended these events that featured a series of lightning tech talks from some of our most senior engineers across the company. Sharon Ly , Lucy Cunningham and Esteban Kuber showcased the technology stack required to process our traffic volume while K.G. Nesbit discussed the evolution of the Twitter network. In addition, Andy Hume , Harry Kantas and Lukasz Szemis presented on using tooling to measure and manage both infrastructure and software at scale. London Tweets: https://twitter.com/EleonoreMayola/status/494984311716974594 https://twitter.com/WWCLondon/status/495095865217122304 https://twitter.com/GirlMeetsCode/status/494983761373954048 https://twitter.com/eurydice13/status/494932714957836288 Dublin Tweets: https://twitter.com/tomwoolway/status/492378638117924864 https://twitter.com/womeng/status/492353321101832192 https://twitter.com/chtemplin/status/491589893076680705 The #ScalingTwitter discussions in Dublin and London were our first engineering focused events, as well as our first WomEng hosted event, outside of the U.S. They provided a fantastic opportunity to bring engineers together who are passionate about leveraging technology to face the challenges of operating at global scale. Learn more about engineering at Twitter and come join the flock !", "date": "2014-08-08"},
{"website": "Twitter-Engineering", "title": "TSAR, a TimeSeries AggregatoR", "author": ["‎@anirudhtodi‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2014/tsar-a-timeseries-aggregator.html", "abstract": "Twitter is a global real-time communications platform that processes many billions of events every day. Aggregating these events in real time presents a massive challenge of scale. Classic time-series applications include site traffic, service health, and user engagement monitoring; these are increasingly complemented by a range of analytics products and features such as Tweet activity, Followers, and Twitter Cards that surface aggregated time-series data directly to end users, publishers, and advertisers. Services that power such features need to be resilient enough to ensure a consistent user experience, flexible enough to accommodate a rapidly changing product roadmap, and able to scale to keep up with Twitter’s ever growing user base. Our experience demonstrates that truly robust real-time aggregation services are hard to build; that scaling and evolving them gracefully is even harder; and moreover, that many time-series applications call for essentially the same architecture, with only slight variations in the data model. Solving this broad class of problems at Twitter has been a multi-year effort. A previous post introduced Summingbird , a high-level abstraction library for generalized distributed computation, which provides an elegant descriptive framework for complex aggregation problems. In this post, we’ll describe how we built a flexible, reusable, end-to-end service architecture on top of Summingbird, called TSAR (the TimeSeries AggregatoR). We’ll explore the motivations and design choices behind TSAR and illustrate how it solves a particular time-series problem: counting Tweet impressions. The Tweet impressions problem in TSAR Let’s suppose we want to annotate every Tweet with an impression count - that is, a count representing the total number of views of that Tweet, updated in real time. This innocent little feature conceals a massive problem of scale. Although “just” 500 million Tweets are created each day, these Tweets are then viewed tens of billions of times. Counting so many events in real time is already a nontrivial problem, but to harden our service into one that’s fit for production we need to answer questions like: What happens if the service is interrupted? Can we retrieve lost data? How do we coordinate our data schema and keep all of its representations consistent? In this example, we want to store Tweet impressions in several different ways: as log data (for use by downstream analytics pipelines); in a key/value data store (for low latency and high availability persistent data); in a cache (for quick access); and, in certain cases, in a relational database (for internal research and data-quality monitoring). How can we ensure the schema of our data is flexible to change and can gracefully propagate to each of its representations without disrupting the running service? For example, the product team might want to count promoted impressions (paid for by an advertiser) and earned impressions (impressions of a retweet of a promoted Tweet) separately. Or perhaps we want to segment impressions by country, or restrict to impressions just in the user’s home country… Such requirements tend to drift in unforeseeable ways, even after the service is first deployed. How do we update or repair historical data in a way that is relatively painless? In this case, we need to backfill a portion of the time-series history. Most important: How do we avoid having to solve all of these problems again the next time we are faced with a similar application? TSAR addresses these problems by following these essential design principles: Hybrid computation . Process every event twice — in real time, and then again (at a later time) in a batch job. The double processing is orchestrated using Summingbird. This hybrid model confers all the advantages of batch (stability, reproducibility) and streaming (recency) computation. Separation of event production from event aggregation . The first processing stage extracts events from source data; in this example, TSAR parses Tweet impression events out of log files deposited by web and mobile clients. The second processing stage buckets and aggregates events. While the “event production” stage differs from application to application, TSAR standardizes and manages the “aggregation” stage. Unified data schema . The data schema for a TSAR service is specified in a datastore-independent way. TSAR maps the schema onto diverse datastores and transforms the data as necessary when the schema evolves. Integrated service toolkit . TSAR integrates with other essential services that provide data processing, data warehousing, query capability, observability, and alerting, automatically configuring and orchestrating its components. Let’s write some code! Production requirements continually change at Twitter, based on user feedback, experimentation, and customer surveys. Our experience has shown us that keeping up with them is often a demanding process that involves changes at many levels of the stack. Let us walk through a lifecycle of the impression counts product to illustrate the power of the TSAR framework in seamlessly evolving with the product. Here is a minimal example of a TSAR service that counts Tweet impressions and persists the computed aggregates in Manhattan (Twitter’s in-house key-value storage system): aggregate {\n  onKeys( \n    (TweetId)\n  ) produce (\n    Count\n  ) sinkTo (Manhattan)\n } fromProducer { \n   ClientEventSource(“client_events”)\n     .filter { event => isImpressionEvent(event) }\n     .map { event =>\n        val impr = ImpressionAttributes(event.tweetId)\n        (event.timestamp, impr)\n     }\n } The TSAR job is broken into several code sections: The onKeys section declares one or more aggregation templates — the dimensions along which we’re aggregating. In this example, it’s just Tweet ID for now. The produce section tells TSAR which metrics to compute. Here we’re producing a count of the total number of impressions for each Tweet. sinkTo(Manhattan) tells TSAR to send data to the Manhattan key/value datastore. Finally, the fromProducer block specifies preprocessing logic for turning raw events into impressions, in the language of Summingbird. TSAR then takes over and performs the heavy lifting of bucketing and aggregating these events (although under the covers, this step is implemented in Summingbird too). Seamless schema evolution We now wish to change our product to break down impressions by the client application (e.g., Twitter for iPhone, Android, etc.) that was used to view the Tweet. This requires us to evolve our job logic to aggregate along an additional dimension. TSAR simplifies this schema evolution: aggregate {\n  onKeys(\n    (TweetId)\n    (TweetId, ClientApplicationId) // new aggregation dimension\n  ) produce (\n    Count\n  ) sinkTo (Manhattan)\n} Backfill tooling Going forward, the impression counts product will now break down data by client application as well. However, data generated by prior iterations of the job does not reflect our new aggregation dimension. TSAR makes backfilling old data as simple as running one backfill command: $ tsar backfill --start=<start> --end=<end> The backfill then runs in parallel to the production job. Backfills are useful to repair bugs in the aggregated data between a certain time range, or simply to fill in old data in parallel to a production job that is computing present data. Simplify aggregating data on different time granularities Our impression counts TSAR job has been computing daily aggregates so far, but now we wish to compute all-time aggregates. TSAR uses a custom configuration file format, where you can add or remove aggregation granularities with a single line change: Output(sink = Sink.Manhattan, width = 1 * Day)\nOutput(sink = Sink.Manhattan, width = Alltime) // new aggregation granularity The user specifies whether he/she wants minutely, hourly, daily or alltime aggregates and TSAR handles the rest. The computational boilerplate of event aggregation (copying each event into various time buckets) is abstracted away. Automatic metric computation For the next version of the product, we can even compute the number of distinct users who have viewed each Tweet, in addition to the total impression count - that is, we can compute a new metric. Normally, this would require changing the job’s aggregation logic. However, TSAR abstracts away the details of metric computation from the user: aggregate {\n  onKeys(\n    (TweetId),\n    (TweetId, ClientApplicationId)\n  ) produce (\n    Count,\n    Unique(UserId) // new metric\n  ) sinkTo (Manhattan)\n} TSAR provides a built-in set of core metrics that the user can specify via configuration options (such as count, sum, unique count, standard deviation, ranking, variance, max, min). However, if a user wishes to aggregate using a new metric (say exponential backoff) that TSAR does not support as yet, the user can can easily add it. Automatic support for multiple sinks Additionally, we can export aggregated data to new output sinks like MySQL to allow for easy exploration. This is also a one-line configuration change: Output(sink = Sink.Manhattan, width = 1 * Day)\nOutput(sink = Sink.Manhattan, width = Alltime)\nOutput(sink = Sink.MySQL, width = Alltime) // new sink TSAR infers and defines the key-value pair data models and relational database schema descriptions automatically via a job-specific configuration file. TSAR automates Twitter best practices using a general-purpose reusable aggregation framework. Note that TSAR is not tied to any specific sink. Sinks can easily be added to TSAR by the user, and TSAR will transparently begin persisting aggregated data to these sinks. Operational simplicity TSAR provides the user with an end-to-end service infrastructure that you can deploy with a single command: $ tsar deploy In addition to simply writing the business logic of the impression counts job, one has to build infrastructure to deploy the Hadoop and Storm jobs, build a query service that combines the results of the two pipelines, and deploy a process to load data into Manhattan and MySQL. A production pipeline requires monitoring and alerting around its various components along with checks for data quality. In our experience, the operational burden of building an entire analytics pipeline from scratch and managing the data flow is quite cumbersome. These parts of the infrastructure look very similar from pipeline to pipeline. We noticed common patterns between the pipelines we built before TSAR and abstracted it all away from the user into a managed framework. A bird’s eye view of the TSAR pipeline looks like: Now let’s bring the various components of our updated TSAR service together. You will see that the updated TSAR service looks almost exactly like the original. However, the data produced by this version of the TSAR service aggregates along additional event dimensions and along more time granularities and writes to an additional data store. The TSAR toolkit and service infrastructure simplify the operational aspects of this evolution as well. The final TSAR service fits into three small files: ImpressionCounts: Thrift schema enum Client\n{\n  iPhone = 0,\n  Android = 1,\n  ...\n}\n\nstruct ImpressionAttributes\n{\n   1: optional Client client,\n   2: optional i64 user_id,\n   3: optional i64 tweet_id\n} ImpressionCounts: TSAR service object ImpressionJob extends TsarJob[ImpressionAttributes] {\n  aggregate {\n    onKeys(\n      (TweetId),\n      (TweetId, ClientApplicationId)\n    ) produce (\n      Count,\n      Unique(UserId)\n    ) sinkTo (Manhattan, MySQL)\n  } fromProducer {\n    ClientEventSource(“client_events”)\n      .filter { event => isImpressionEvent(event) }\n      .map { event =>\n        val impr = ImpressionAttributes(\n          event.client, event.userId, event.tweetId\n        )\n        (event.timestamp, impr)\n      }\n   }\n} ImpressionCounts: Configuration file Config(\n  base = Base(\n    user            = \"platform-intelligence\",\n    name            = \"impression-counts\",\n    origin          = \"2014-01-01 00:00:00 UTC\",\n    primaryReducers = 1024,\n    outputs =  [\n      Output(sink = Sink.Hdfs, width = 1 * Day),\n      Output(sink = Sink.Manhattan, width = 1 * Day),\n      Output(sink = Sink.Manhattan, width = Alltime),\n      Output(sink = Sink.MySQL, width = Alltime)\n    ],\n    storm = Storm(\n      topologyWorkers = 10,\n      ttlSeconds      = 4.days,\n    ),\n  ),\n) The information contained in these three files (thrift, scala class, configuration) is all that the user needs to specify in order to deploy a fully functional service. TSAR fills in the blanks: How does one represent the aggregated data? How does one represent the schema? How does one actually perform the aggregation (computationally)? Where are the underlying services (Hadoop, Storm, MySQL, Manhattan, …) located, and how does one connect to them? The end-to-end management of the data pipeline is TSAR’s key feature. The user concentrates on the business logic. Looking ahead While we have been running TSAR in production for more than a year, it is still a work in progress. The challenges are increasing and the number of features launching internally on TSAR is growing at rapid pace. Pushing ourselves harder to be better and smarter is what drives us on the Platform Intelligence team. We wish to grow our business in a way that makes us proud and do what we can to make Twitter better and our customers more successful. Acknowledgments Among the many people who have contributed to TSAR (far too many to list here), I especially want to thank Aaron Siegel , Reza Lotun , Ryan King , Dave Loftesness , Dmitriy Ryaboy , Andrew Nguyen , Jeff Sarnat , John Chee , Eric Conlon , Allen Chen , Gabriel Gonzalez , Ali Alzabarah , Zongheng Yang , Zhilan Zweiger , Kevin Lingerfelt , Justin Chen , Ian Chan , Jialie Hu , Max Hansmire , David Marwick , Oscar Boykin , Sam Ritchie , Ian O’Connell … And special thanks to Raffi Krikorian for conjuring the Platform Intelligence team into existence and believing that anything is possible. If this sounds exciting to you and you’re interested in joining the Platform Intelligence team to work on Tsar, we’d love to hear from you!", "date": "2014-06-27"},
{"website": "Twitter-Engineering", "title": "Using Twitter to measure earthquake impact in almost real time", "author": ["‎@Reza_Zadeh‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2014/using-twitter-to-measure-earthquake-impact-in-almost-real-time.html", "abstract": "At Twitter, we know that Tweets can sometimes travel as fast as an earthquake. We were curious to know just how accurate such a correlation might be, so we collaborated with Stanford researchers to model how Tweets can help create more accurate ShakeMaps, which provide near-real-time maps of ground motion and shaking intensity following significant earthquakes. These maps are used by federal, state and local organizations, both public and private, for post-earthquake response and recovery, public and scientific information, as well as for preparedness exercises and disaster planning. Currently, ShakeMaps produced by the U.S. Geological Survey represent the state-of-the-art rapid shaking intensity estimation. When an earthquake happens, a ShakeMap is typically produced in a matter of minutes using a combination of recordings, a simple ground motion prediction equation, and geological site correction factors. As time progresses, the ShakeMap is continually updated as new information becomes available, including “did you feel it?” data — qualitative first-hand accounts of the earthquake collected via online surveys. To help improve the accuracy of ShakeMaps, we used all geo-tagged Tweets around the world containing the keyword “earthquake” or “tsunami” in several languages that occurred in the first 10 minutes following Japanese earthquakes of magnitude 6 or greater from 2011 to 2012. We found that the model with lowest error was based on a combination of earthquake and Tweet-based features, such as local site conditions, source-to-site distance and the number of Tweets within a certain radius. Ground shaking intensity estimates from our model are comparable with historical recordings and conventional estimates provided, for example, by USGS ShakeMaps. Figure 1. For the Tohoku (c0001xgp) earthquake: (a) number of geo-tagged Tweets containing an earthquake keyword per minute after the event, (b) distance between each Tweet and the epicenter as a function of time, and (c) map showing the number of Tweets, where the star represents the epicenter location. Figure 1 shows Tweet activity following a significant earthquake. From these, we generated 8 Tweet-based features, each considered within varying radii of a recording station. They included: count of Tweets, average number of tokens, average number of characters, average index of first mention of an earthquake keyword, average number of appearances of earthquake keywords, average number of exclamation points, average number of dashes and average number of appearances of a selected Japanese keyword. With these features, we trained several models, including a simple linear regression, elastic net regression and k-Nearest-Neighbors regression. For a more thorough description, you can read this paper , which we’ll present in July at the National Conference on Earthquake Engineering . Reference: Rapid Estimate of Ground Shaking Intensity by Combining Simple Earthquake Characteristics with Tweets Mahalia Miller , Lynne Burks , Reza Zadeh Tenth U.S. National Conference on Earthquake Engineering", "date": "2014-05-02"},
{"website": "Twitter-Engineering", "title": "Twitter #DataGrants selections", "author": ["‎@raffi‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2014/twitter-datagrants-selections.html", "abstract": "In February, we introduced the Twitter #DataGrants pilot program, with the goal of giving a handful of research institutions access to Twitter’s public and historical data. We are thrilled with the response from the research community — we received more than 1,300 proposals from more than 60 different countries, with more than half of the proposals coming from outside the U.S. After reviewing all of the proposals, we’ve selected six institutions, spanning four continents, to receive free datasets in order to move forward with their research. Harvard Medical School / Boston Children’s Hospital (US): Foodborne Gastrointestinal Illness Surveillance using Twitter Data NICT (Japan): Disaster Information Analysis System University of Twente (Netherlands): The Diffusion And Effectiveness of Cancer Early Detection Campaigns on Twitter UCSD (US): Do happy people take happy images? Measuring happiness of cities University of Wollongong (Australia): Using GeoSocial Intelligence to Model Urban Flooding in Jakarta, Indonesia University of East London (UK): Exploring the relationship between Tweets and Sports Team Performance Thank you to everyone who took part in this pilot. As we welcome Gnip to Twitter, we look forward to expanding the Twitter #DataGrants program and helping even more institutions and academics access Twitter data in the future. Finally, we’d also like to thank Mark Gillis , Chris Aniszczyk and Jeff Sarnat for their passion in helping create this program.", "date": "2014-04-17"},
{"website": "Twitter-Engineering", "title": "Scalding 0.9: Get it while it’s hot!", "author": ["‎@posco‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2014/scalding-09-get-it-while-it-s-hot.html", "abstract": "It’s been just over two years since we open sourced Scalding and today we are very excited to release the 0.9 version. Scalding at Twitter powers everything from internal and external facing dashboards, to custom relevance and ad targeting algorithms, including many graph algorithms such as PageRank, approximate user cosine similarity and many more. There have been a wide breadth of new features added to Scalding since the last release: Joins An area of particular activity and impact has been around joins. The Fields API already had an API to do left and right joins over multiple streams, but with 0.9 we bring this functionality to the Typed-API. In 0.9, joins followed by reductions followed by more joins are automatically planned as single map reduce jobs, potentially reducing the number of steps in your pipelines. case class UserName(id: Long, handle: String)\n case class UserFavs(byUser: Long, favs: List[Long])\n case class UserTweets(byUser: Long, tweets: List[Long])\n \n def users: TypedSource[UserName]\n def favs: TypedSource[UserFavs]\n def tweets: TypedSource[UserTweets]\n \n def output: TypedSink[(UserName, UserFavs, UserTweets)]\n \n // Do a three-way join in one map-reduce step, with type safety\n users.groupBy(_.id)\n   .join(favs.groupBy(_.byUser))\n   .join(tweets.groupBy(_.byUser))\n   .map { case (uid, ((user, favs), tweets)) =>\n    (user, favs, tweets)\n   }   \n   .write(output) This includes custom co-grouping, not just left and right joins. To handle skewed data there is a new count-min-sketch based algorithm to solve the curse of the last reducer, and a critical bug-fix for skewed joins in the Fields API. Input/output In addition to joins, we’ve added support for new input/output formats: Parquet Format is a columnar storage format which we open sourced in collaboration with Cloudera. Parquet can dramatically accelerate map-reduce jobs that read only a subset of the columns in an dataset, and can similarly reduce storage cost with more efficiently serialization. Avro is an Apache project to standardize serialization with self-describing IDLs. Ebay contributed the scalding-avro module to make it easy to work with Apache Avro serialized data. TemplateTap support eases partitioned writes of data, where the output path depends on the value of the data. Hadoop counters We’re also adding support for incrementing Hadoop counters inside map and reduce functions. For cases where you need to share a medium sized data file across all your tasks, support for Hadoop’s distributed cache was added in this release cycle. Typed API The typed API saw many improvements. When doing data-cubing, partial aggregation should happen before key expansion and sumByLocalKeys enables this. The type-system enforces constraints on sorting and joining that previously would have caused run-time exceptions. When reducing a data-set to a single value, a ValuePipe is returned. Like TypedPipe is analogous to a program to produce a distributed list, a ValuePipe is a like a program to produce a single value, with which we might want to filter or transform some TypedPipe. Matrix API When it comes to linear algebra, Scalding 0.9 introduced a new Matrix API which will replace the former one in our next major release. Due to the associative nature of matrix multiplication we can choose to compute (AB)C or A(BC). One of those orders might create a much smaller intermediate product than the other. The new API includes a dynamic programming optimization of the order of multiplication chains of matrices to minimize realized size along with several other optimizations. We have seen some considerable speedups of matrix operations with this API. In addition to the new optimizing API, we added some functions to efficiently compute all-pair inner-products (A A^T) using DISCO and DIMSUM . These algorithms excel for cases of vectors highly skewed in their support, which is to say most vectors have few non-zero elements, but some are almost completely dense. Upgrading and Acknowledgements Some APIs were deprecated, some were removed entirely, and some added more constraints. We have some sed rules to aid in porting. All changes fixed significant warts. For instance, in the Fields API sum takes a type parameter, and works for any Semigroup or Monoid. Several changes improve the design to aid in using scalding more as a library and less as a framework. This latest release is our biggest to date spanning over 800 commits from 57 contributors It is available today in maven central . We hope Scalding is as useful to you as it is for us and the growing community . Follow us @scalding , join us on IRC ( #scalding ) or via the mailing list .", "date": "2014-04-03"},
{"website": "Twitter-Engineering", "title": "Manhattan, our real-time, multi-tenant distributed database for Twitter scale", "author": ["‎@scode‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2014/manhattan-our-real-time-multi-tenant-distributed-database-for-twitter-scale.html", "abstract": "As Twitter has grown into a global platform for public self-expression and conversation, our storage requirements have grown too. Over the last few years, we found ourselves in need of a storage system that could serve millions of queries per second, with extremely low latency in a real-time environment. Availability and speed of the system became the utmost important factor. Not only did it need to be fast; it needed to be scalable across several regions around the world. Over the years, we have used and made significant contributions to many open source databases. But we found that the real-time nature of Twitter demanded lower latency than the existing open source products were offering. We were spending far too much time firefighting production systems to meet the performance expectations of our various products, and standing up new storage capacity for a use case involved too much manual work and process. Our experience developing and operating production storage at Twitter’s scale made it clear that the situation was simply not sustainable. So we began to scope out and build Twitter’s next generation distributed database, which we call Manhattan . We needed it to take into account our existing needs, as well as put us in a position to leapfrog what exists today. Our holistic view into storage systems at Twitter Different databases today have many capabilities, but through our experience we identified a few requirements that would enable us to grow the way we wanted while covering the majority of use cases and addressing our real-world concerns, such as correctness, operability, visibility, performance and customer support. Our requirements were to build for: Reliability : Twitter services need a durable datastore with predictable performance that they can trust through failures, slowdowns, expansions, hotspots, or anything else we throw at it. Availability : Most of our use cases strongly favor availability over consistency, so an always-on eventually consistent database was a must. Extensibility : The technology we built had to be able to grow as our requirements change, so we had to have a solid, modular foundation on which to build everything from new storage engines to strong consistency. Additionally, a schemaless key-value data model fit most customers’ needs and allowed room to add structure later. Operability : As clusters grow from hundreds to thousands of nodes, the simplest operations can become a pain and a time sink for operators. In order to scale efficiently in manpower, we had to make it easy to operate from day one. With every new feature we think about operational complexity and the ease of diagnosing issues. Low latency : As a real-time service, Twitter’s products require consistent low latency, so we had to make the proper tradeoffs to guarantee low latent performance. Real-world scalability : Scaling challenges are ubiquitous in distributed systems. Twitter needs a database that can scale not just to a certain point, but can continue to grow to new heights in every metric — cluster size, requests per second, data size, geographically, and with number of tenants — without sacrificing cost effectiveness or ease of operations. Developer productivity : Developers in the company should be able to store whatever they need to build their services, with a self service platform that doesn’t require intervention from a storage engineer, on a system that in their view “just works”. Developers should be able to store whatever they need on a system that just works. Reliability at scale When we started building Manhattan, we already had many large storage clusters at Twitter, so we understood the challenges that come from running a system at scale, which informed what kinds of properties we wanted to encourage and avoid in a new system. A reliable storage system is one that can be trusted to perform well under all states of operation, and that kind of predictable performance is difficult to achieve. In a predictable system, worst-case performance is crucial; average performance not so much. In a well implemented, correctly provisioned system, average performance is very rarely a cause of concern. But throughout the company we look at metrics like p999 and p9999 latencies, so we care how slow the 0.01% slowest requests to the system are. We have to design and provision for worst-case throughput. For example, it is irrelevant that steady-state performance is acceptable, if there is a periodic bulk job that degrades performance for an hour every day. Because of this priority to be predictable, we had to plan for good performance during any potential issue or failure mode. The customer is not interested in our implementation details or excuses; either our service works for them and for Twitter or it does not. Even if we have to make an unfavorable trade-off to protect against a very unlikely issue, we must remember that rare events are no longer rare at scale. With scale comes not only large numbers of machines, requests and large amounts of data, but also factors of human scale in the increasing number of people who both use and support the system. We manage this by focusing on a number of concerns: if a customer causes a problem, the problem should be limited to that customer and not spread to others it should be simple, both for us and for the customer, to tell if an issue originates in the storage system or their client for potential issues, we must minimize the time to recovery once the problem has been detected and diagnosed we must be aware of how various failure modes will manifest for the customer an operator should not need deep, comprehensive knowledge of the storage system to complete regular tasks or diagnose and mitigate most issues And finally, we built Manhattan with the experience that when operating at scale, complexity is one of your biggest enemies. Ultimately, simple and working trumps fancy and broken. We prefer something that is simple but works reliably, consistently and provides good visibility, over something that is fancy and ultra-optimal in theory but in practice and implementation doesn’t work well or provides poor visibility, operability, or violates other core requirements. Building a storage system When building our next generation storage system, we decided to break down the system into layers so it would be modular enough to provide a solid base that we can build on top of, and allow us to incrementally roll out features without major changes. We designed with the following goals in mind: Keep the core lean and simple Bring value sooner rather than later (focus on the incremental) Multi-Tenancy, Quality of Service (QoS) and Self-Service are first-class citizens Focus on predictability Storage as a service , not just technology Layers We have separated Manhattan into four layers: interfaces, storage services, storage engines and the core. Core The core is the most critical aspect of the storage system: it is highly stable and robust. It handles failure, eventual consistency, routing, topology management, intra- and inter-datacenter replication, and conflict resolution. Within the core of the system, crucial pieces of architecture are completely pluggable so we can iterate quickly on designs and improvements, as well as unit test effectively. Operators are able to alter the topology at any time for adding or removing capacity, and our visibility and strong coordination for topology management are critical. We store our topology information in Zookeeper because of it’s strong coordination capabilities and because it is a managed component in our infrastructure at Twitter, though Zookeeper is not in the critical path for reads or writes. We also put a lot of effort into making sure we have extreme visibility into the core at all times with an extensive set of Ostrich metrics across all hosts for correctness and performance. Consistency model Many of Twitter’s applications fit very well into the eventually consistent model. We favor high availability over consistency in almost all use cases, so it was natural to build Manhattan as an eventually consistent system at its core. However, there will always be applications that require strong consistency for their data so building such a system was a high priority for adopting more customers. Strong consistency is an opt-in model and developers must be aware of the trade-offs. In a strongly consistent system, one will typically have a form of mastership for a range of partitions. We have many use cases at Twitter where having a hiccup of a few seconds of unavailability is simply not acceptable (due to electing new masters in the event of failures). We provide good defaults for developers and help them understand the trade-offs between both models. Achieving consistency To achieve consistency in an eventually consistent system you need a required mechanism which we call replica reconciliation . This mechanism needs to be incremental, and an always running process that reconciles data across replicas. It helps in the face of bitrot, software bugs, missed writes (nodes going down for long periods of time) and network partitions between datacenters. In addition to having replica reconciliation, there are two other mechanisms we use as an optimization to achieve faster convergence: read-repair, which is a mechanism that allows frequently accessed data to converge faster due to the rate of the data being read, and hinted-handoff, which is a secondary delivery mechanism for failed writes due to a node flapping, or being offline for a period of time. Storage engines One of the lowest levels of a storage system is how data is stored on disk and the data structures kept in memory. To reduce the complexity and risk of managing multiple codebases for multiple storage engines, we made the decision to have our initial storage engines be designed in-house, with the flexibility of plugging in external storage engines in the future if needed. This gives us the benefit of focusing on features we find the most necessary and the control to review which changes go in and which do not. We currently have three storage engines: seadb, our read-only file format for batch processed data from hadoop sstable, our log-structured merge tree based format for heavy write workloads btree, our btree based format for heavy read, light write workloads All of our storage engines support block-based compression. Storage services We have created additional services that sit on top of the core of Manhattan that allow us to enable more robust features that developers might come to expect from traditional databases. Some examples are: Batch Hadoop importing: One of the original use cases of Manhattan was as an efficient serving layer on top of data generated in Hadoop. We built an importing pipeline that allows customers to generate their datasets in a simple format in HDFS and specify that location in a self service interface. Our watchers automatically pick up new datasets and convert them in HDFS into seadb files, so they can then be imported into the cluster for fast serving from SSDs or memory. We focused on making this importing pipeline streamlined and easy so developers can iterate quickly on their evolving datasets. One lesson we learned from our customers was that they tend to produce large, multi-terabyte datasets where each subsequent version typically changes less than 10-20% of their data. We baked in an optimization to reduce network bandwidth by producing binary diffs that can be applied when we download this data to replicas, substantially reducing the overall import time across datacenters. Strong Consistency service: The Strong Consistency service allows customers to have strong consistency when doing certain sets of operations. We use a consensus algorithm paired with a replicated log to guarantee in-order events reach all replicas. This enables us to do operations like Check-And-Set (CAS), strong read, and strong write. We support two modes today called LOCAL_CAS and GLOBAL_CAS. Global CAS enables developers to do strongly consistent operations across a quorum of our datacenters, whereas a Local CAS operation is coordinated only within the datacenter it was issued. Both operations have different tradeoffs when it comes to latency and data modeling for the application. Timeseries Counters service : We developed a very specific service to handle high volume timeseries counters in Manhattan. The customer who drove this requirement was our Observability infrastructure, who needed a system that could handle millions of increments per second. At this level of scale, our engineers went through the exercise of coming up with an agreed upon set of design tradeoffs over things like durability concerns, the delay before increments needed to be visible to our alerting system, and what kind of subsecond traffic patterns we could tolerate from the customer. The result was a thin, efficient counting layer on top of a specially optimized Manhattan cluster that greatly reduced our requirements and increased reliability over the previous system. Interfaces The interface layer is how a customer interacts with our storage system. Currently we expose a key/value interface to our customers, and we are working on additional interfaces such as a graph based interface to interact with edges. Tooling With the easy operability of our clusters as a requirement, we had to put a lot of thought into how to best design our tools for day-to-day operations. We wanted complex operations to be handled by the system as much as possible, and allow commands with high-level semantics to abstract away the details of implementation from the operator. We started with tools that allow us to change the entire topology of the system simply by editing a file with host groups and weights, and do common operations like restarting all nodes with a single command. When even that early tooling started to become too cumbersome, we built an automated agent that accepts simple commands as goals for the state of the cluster, and is able to stack, combine, and execute directives safely and efficiently with no further attention from an operator. Storage as a service A common theme that we saw with existing databases was that they were designed to be setup and administered for a specific set of use-cases. With Twitter’s growth of new internal services, we realized that this wouldn’t be efficient for our business. Our solution is storage as a service. We’ve provided a major productivity improvement for our engineers and operational teams by building a fully self-service storage system that puts engineers in control. Engineers can provision what their application needs (storage size, queries per second, etc) and start using storage in seconds without having to wait for hardware to be installed or for schemas to be set up. Customers within the company run in a multi-tenant environment that our operational teams manage for them. Managing self service and multi-tenant clusters imposes certain challenges, so we treat this service layer as first-class feature: we provide customers with visibility into their data and workloads, we have built-in quota enforcement and rate-limiting so engineers get alerted when they go over their defined thresholds, and all our information is fed directly to our Capacity and Fleet Management teams for analysis and reporting. By making it easier for engineers to launch new features, we saw a rise in experimentation and a proliferation of new use-cases. To better handle these, we developed internal APIs to expose this data for cost analysis which allows us to determine what use cases are costing the business the most, as well as which ones aren’t being used as often. Focus on the customer Even though our customers are our fellow Twitter employees, we are still providing a service, and they are still our customers. We must provide support, be on call, isolate the actions of one application from another, and consider the customer experience in everything we do. Most developers are familiar with the need for adequate documentation of their services, but every change or addition to our storage system requires careful consideration. A feature that should be seamlessly integrated into self-service has different requirements from one that needs intervention by operators. When a customer has a problem, we must make sure to design the service so that we can quickly and correctly identify the root cause, including issues and emergent behaviors that can arise from the many different clients and applications through which engineers access the database. We’ve had a lot success building Manhattan from the ground up as a service and not just a piece of technology. Multi-Tenancy and QoS (Quality of Service) Supporting multi-tenancy — allowing many different applications to share the same resources — was a key requirement from the beginning. In previous systems we managed at Twitter, we were building out clusters for every feature. This was increasing operator burden, wasting resources, and slowing customers from rolling out new features quickly. As mentioned above, allowing multiple customers to use the same cluster increases the challenge of running our systems. We now must think about isolation, management of resources, capacity modeling with multiple customers, rate limiting, QoS, quotas, and more. In addition to giving customers the visibility they need to be good citizens, we designed our own rate limiting service to enforce customers usage of resources and quotas. We monitor and, if needed, throttle resource usage across many metrics to ensure no one application can affect others on the system. Rate limiting happens not at a coarse grain but at a subsecond level and with tolerance for the kinds of spikes that happen with real world usage. We had to consider not just automatic enforcement, but what controls should be available manually to operators to help us recover from issues, and how we can mitigate negative effects to all customers, including the ones going over their capacity. We built the APIs needed to extract the data for every customer and send it to our Capacity teams, who work to ensure we have resources always ready and available for customers who have small to medium requirements (by Twitter standards), so that those engineers can get started without additional help from us. Integrating all of this directly into self-service allows customers to launch new features on our large multi-tenant clusters faster, and allows us to absorb traffic spikes much more easily since most customers don’t use all of their resources at all times. Looking ahead We still have a lot of work ahead of us. The challenges are increasing and the number of features being launched internally on Manhattan is growing at rapid pace. Pushing ourselves harder to be better and smarter is what drives us on the Core Storage team. We take pride in our values: what can we do to make Twitter better, and how do we make our customers more successful? We plan to release a white paper outlining even more technical detail on Manhattan and what we’ve learned after running over two years in production, so stay tuned! Acknowledgments We want to give a special thank you to Armond Bigian , for helping believe in the team along the way and championing us to make the best storage system possible for Twitter. The following people made Manhattan possible: Peter Schuller , Chris Goffinet , Boaz Avital , Spencer Fang , Ying Xu , Kunal Naik , Yalei Wang , Danny Chen , Melvin Wang , Bin Zhang , Peter Beaman , Sree Kuchibhotla , Osama Khan , Victor Yang Ye , Esteban Kuber , Tugrul Bingol , Yi Lin , Deng Liu , Tyler Serdar Bulut , Andy Gross , Anthony Asta , Evert Hoogendoorn , Lin Lee , Alex Peake , Yao Yue , Hyun Kang , Xin Xiang , Sumeet Lahorani , Rachit Arora , Sagar Vemuri , Petch Wannissorn , Mahak Patidar , Ajit Verma , Sean Wang , Dipinder Rekhi , Satish Kotha , Johan Harjono , Alex Young , Kevin Donghua Liu , Pascal Borghino , Istvan Marko , Andres Plaza , Ravi Sharma , Vladimir Vassiliouk , Ning Li , Liang Guo , Inaam Rana . If you’d like to work on Manhattan and enjoy tackling hard problems in distributed storage, apply to the Core Storage Team at jobs.twitter.com!", "date": "2014-04-02"},
{"website": "Twitter-Engineering", "title": "Greater privacy for your Twitter emails with TLS", "author": ["‎@jaberant‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2014/greater-privacy-for-your-twitter-emails-with-tls.html", "abstract": "Protecting users’ privacy is a never-ending process, and we are committed to keeping our users’ information safe. Since mid-January, we have been protecting your emails from Twitter using TLS in the form of StartTLS . StartTLS encrypts emails as they transit between sender and receiver and is designed to prevent snooping. It also ensures that emails you receive from Twitter haven’t been read by other parties on the way to your inbox if your email provider supports TLS. We’re using StartTLS in addition to other email security protocols we’ve previously enabled like DKIM and DMARC , which prevent spoofing and email forgeries by ensuring emails claiming to be from Twitter were indeed sent by us. These email security protocols are part of our commitment to continuous improvement in privacy protections and complement improvements like our securing of web traffic with forward secrecy and always-on HTTPS . While we’ve enabled StartTLS for SMTP, that’s not enough to guarantee delivery over TLS. TLS encryption only works if both the sender and receiver of emails support it. We commend those email providers like Gmail & AOL Mail that have turned on TLS and we ask all other providers that haven’t yet to prioritize it. Together, we can protect the privacy of every user.", "date": "2014-03-12"},
{"website": "Twitter-Engineering", "title": "How To: Objective C Initializer Patterns", "author": ["‎@jakej‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2014/how-to-objective-c-initializer-patterns.html", "abstract": "Initializer patterns are an important part of good Objective-C, but these best practices are often overlooked. It’s the sort of thing that doesn’t cause problems most of the time, but the problems that arise are often difficult to anticipate. By being more rigorous and conforming to some best practices, we can save ourselves a lot of trouble. In this article, we’ll cover initialization topics in-depth, with examples to demonstrate how things can go wrong. Part 1 : Designated and Secondary Initializers Part 2 : Case Studies Part 3 : Designated and Secondary Initializer Cheat Sheet Part 4 : - initWithCoder:, + new, and - awakeFromNib Part 1: Designated and Secondary Initializers Designated initializers define how we structure our initializers when subclassing; they are the “canonical initializer” for your class. A designated initializer does not define what initializer you should use when creating an object, like this common example: [[UIView alloc] initWithFrame:CGRectZero]; It’s not necessary to call the designated initializer in the above case, although it won’t do any harm. If you are conforming to best practices, It is valid to call any designated initializer in the superclass chain, and the designated initializer for every class in the hierarchy is guaranteed to be called. For example: [[UIView alloc] init]; is guaranteed to call [NSObject init] and [UIView initWithFrame:], in that order. The order is guaranteed to be reliable regardless of which designated initializer in the superclass chain you call, and will always go from furthest ancestor to furthest descendant . When subclassing, you have three valid choices: you may choose to reuse your superclass’s designated initializer, to create your own designated initializer, or to not create any initializers (relying on your superclass’s). If you override your superclass’s designated initializer, your work is done. You can feel safe knowing that this initializer will be called. If you choose to create a new designated initializer for your subclass, you must do two things. First, create a new initializer, and document it as the new designated initializer in your header file. Second, you must override your superclass’s designated initializer and call the new one. Here’s an example for a UIView subclass: // Designated initializer\n- (instancetype)initWithFoo:(TwitterFoo *)foo \n{\n    if (self = [super initWithFrame:CGRectZero]) {\n        _foo = foo;\n        // initializer logic\n    }\n    return self;\n}\n\n- (instancetype)initWithFrame:(CGRect)rect\n{\n    return [self initWithFoo:nil];\n} Apple doesn’t mention much about it in the documentation, but all Apple framework classes provide valuable guarantees due to their consistency with these patterns. In the above example, if we did not override our superclass’s designated initializer to call the new one, we would break the guarantee which makes calling any designated initializer in the hierarchy reliable. For example, if we removed our initWithFrame: override, [[TwitterFooView alloc] init]; could not be relied upon to call our designated initializer, initWithFoo:. The initialization would end with initWithFrame:. Finally, not all initializers are designated initializers. Additional initializers are referred to as convenience or secondary initializers. There is one rule here you’ll want to follow: Always call the designated initializer (or another secondary initializer) on self instead of super. Example 1: @interface TwitterFooView : UIView @end @implementation TwitterFooView\n\n// Designated Initializer\n- (instancetype)initWithFoo:(TwitterFoo *)foo\n{\n\tif (self = [super initWithFrame:CGRectZero]) {\n\t\t_foo = foo;\n\t\t// do the majority of initializing things\n\t}\n\treturn self;\n}\n\n// Super override\n- (instancetype)initWithFrame:(CGRect)rect\n{\n\treturn [self initWithFoo:nil];\n}\n\n// Instance secondary initializer\n- (instancetype)initWithBar:(TwitterBar *)bar \n{\n\tif (self = [self initWithFoo:nil]) {\n\t\t_bar = bar;\n\t\t// bar-specific initializing things\n\t}\n\treturn self;\n}\n\n// Class secondary initializer\n+ (instancetype)fooViewWithBaz:(TwitterBaz *)baz\n{\n\tTwitterFooView *fooView = [[TwitterFooView alloc] initWithFoo:nil];\n\tif (fooView) {\n\t\t// baz-specific initialization\n}\n\treturn fooView;\n} @end Again, the key takeaway from this example is that in both - initWithBar: and + fooViewWithBaz:, we call - initWithFoo:, the designated initializer, on self. There’s one more rule to follow to preserve a deterministic designated initializer behavior: When writing initializers, don’t call designated initializers beyond your direct superclass. This can break the order of designated initializer execution. For an example of how this can go wrong, see Part 2, Case 2. Part 2: Case Studies Now that we’ve covered the rules and guarantees relating to designated and secondary initializers, let’s prove these assertions using some concrete examples. Case 1: Designated Initializer Ordering Based on the code from Example 1, let’s prove the following assertion: Calling any designated initializer in the superclass chain is valid, and designated initializers are guaranteed to be executed in order from furthest ancestor ([NSObject init]) to furthest descendant ([TwitterFooView initWithFoo:]). In the following three diagrams, we’ll show the order of initializer execution when calling each designated initializer in the hierarchy: initWithFoo:, initWithFrame:, and init. Case 2: Example bug in UIViewController subclass In the following example, we will analyze what can happen when we violate the following rule: When writing initializers, don’t call designated initializers beyond your immediate superclass. In context, this means we shouldn’t override or call [NSObject init] from a UIViewController subclass. Let’s say we start with a class TwitterGenericViewController, and incorrectly override [NSObject init]: @interface TwitterGenericViewController : UIViewController\n\n// Incorrect\n- (instancetype)init\n{\n    if (self = [super init]) {\n        _textView = [[UITextView alloc] init];\n        _textView.delegate = self;\n    }\n    return self;\n} @end If we instantiate this object using [[TwitterGenericViewController alloc] init], this will work fine. However, if we use [[TwitterGenericViewController alloc] initWithNibName:nil bundle:nil], which should be perfectly valid, this initializer method will never be called. Let’s look at the order of execution for this failure case: Things begin to break even further when subclasses are introduced below this incorrect - init implementation. Consider the following subclass to TwitterGenericViewController which correctly overrides initWithNibName:bundle: @interface TwitterViewController : TwitterGenericViewController\n\n- (instancetype)initWithNibName:(NSString *)nibNameOrNil bundle:(NSBundle *)nibBundleOrNil\n{\n    if (self = [super initWithNibName:nibNameOrNil bundle:nibBundleOrNil]) {\n        _textView = [[UITextView alloc] init];\n        _textView.delegate = self;\n    }\n    return self;\n} @end Now, we have failure no matter which initializer we choose. In this case, there is a failure because TwitterGenericViewController’s initializer is never called. In this case, all initializers are called, but in the wrong order. TwitterViewController will populate _textView and set it’s delegate, and then TwitterGenericViewController (the superclass) will initialize, overriding the _textView configuration. That’s backwards! We always want subclasses to initialize after superclasses, so we can override state properly. Part 3: Designated and Secondary Initializer Cheat Sheet When creating an object Calling any designated initializer in the superclass chain is valid when creating an object. You can rely on all designated initializers being calling in order from furthest ancestor to furthest descendant. When subclassing Option 1: Override immediate superclass’s designated initializer Be sure to call immediate super’s designated initializer first Only override your immediate superclass’s designated initializer Option 2: Create a new designated initializer Be sure to call your immediate superclass’s designated initializer first Define a new designated initializer Document new designated initializer as such in your header Separately, override immediate superclass’s designated initializer and call the new designated initializer on self Option 3: Don’t create any initializers It is valid to omit any initializer definition and rely on your superclass’s In this case, you ‘inherit’ your superclass’s designated initializer as it applies to the last rule in Option 1 Additionally, you may define class or instance secondary initializers Secondary initializers, must always call self, and either call the designated initializer or another secondary initializer. Secondary initializers may be class or instance methods (see Example 1) Part 4: - initWithCoder:, + new, and - awakeFromNib + new The documentation describes the [NSObject new] as “a combination of alloc and init”. There’s nothing wrong with using the method for initialization, since we’ve established that calling any designated initializer in the hierarchy is valid, and all designated initializers will be called in order. However, when contrasted with [[NSObject alloc] init], + new is used less often, and is therefore less familiar. Developers using Xcode’s global search for strings like “MyObject alloc” may perhaps unknowingly overlook uses of [MyObject new]. - initWithCoder: Reading Apple’s documentation on object initialization when using NSCoding is a helpful first step. There are two key things to remember when implementing initWithCoder: First, if your superclass conforms to NSCoding, you should call [super initWithCoder:coder] instead of [super (designated initializer)]. There’s a problem with the example provided in the documentation for initWithCoder:, specifically the call to [super (designated initializer)]. If you’re a direct subclass of NSObject, calling [super (designated initializer)] won’t call your [self (designated initializer)]. If you’re not a direct subclass of NSObject, and one of your ancestors implements a new designated initializer, calling [super (designated initializer)] WILL call your [self (designated initializer)]. This means that apple’s suggestion to call super in initWithCoder encourages non-deterministic initialization behavior, and is not consistent with the solid foundations laid by the designated initializer pattern. Therefore, my recommendation is that you should treat initWithCoder: as a secondary initializer, and call [self (designated initializer)], not [super (designated initializer)], if your superclass does not conform to NSCoding. - awakeFromNib The documentation for - awakeFromNib is straightforward: NSNibAwaking Protocol Reference The key point here is that interface builder outlets will not be set while the designated initializer chain is called. awakeFromNib happens afterwards, and IBOutlets will be set at that point. Documentation NSCell has four designated initializers for different configurations. This is an interesting exception to the standard single designated initializer pattern, so it’s worth checking out: NSCell Class Reference Documentation relating to initialization: Object Initialization Multiple Initializers Encapsulating Data Encoding and Decoding Objects", "date": "2014-02-27"},
{"website": "Twitter-Engineering", "title": "__attribute__ directives in Objective-C", "author": ["Nolan O'Brien"], "link": "https://blog.twitter.com/engineering/en_us/a/2014/attribute-directives-in-objective-c.html", "abstract": "In this post, we’ll examine what __attribute__ directives are and how they can be used in development. The goal is to establish a value to using __attribute__ directives in any codebase and to provide a starting point with some directives that anyone can start using right away. What are __attribute__ directives? When should I use an __attribute__ directive? Recognizing the dangers of misusing an __attribute__ directive Core __attribute__ directives ARC __attribute__ directives More __attribute__ directives in, out and inout __attribute__ directives as a tool __attribute__ resources What are __attribute__ directives? The __attribute__ directive is used to decorate a code declaration in C, C++ and Objective-C programming languages. This gives the declared code additional attributes that would help the compiler incorporate optimizations or elicit useful warnings to the consumer of that code. Better said, __attribute__ directives provide context. The value of providing context to code cannot be overstated. Developers have provided context by way of explicit declarations and comments since before the advent of the integrated circuit, but the value of providing context that can be evaluated by a compiler gives us a whole new level of control. By explicitly providing the confines of how an API behaves to the compiler, a programmer can gain some tangible benefits. The directives can be used to enforce compliance with how other programmers consume that API. In other cases, __attribute__ directives can help the compiler to optimize - sometimes to large performance gains. As Mattt Thompson cogently put it in a blog post : “Context is king when it comes to compiler optimizations. By providing constraints on how to interpret your code, [you’ll increase] the chance that the generated code is as efficient as possible. Meet your compiler half-way, and you’ll always be rewarded… [It] isn’t just for the compiler either: The next person to see the code will appreciate the extra context, too. So go the extra mile for the benefit of your collaborator, successor, or just 2-years-from-now you.” Which leads nicely into wise words from Sir Paul McCartney, “and in the end, the love you take is equal to the love you make.” When should I use an __attribute__ directive? Whenever you have an opportunity to provide additional context to a code declaration (variable, argument, function, method, class, etc), you should. Providing context to code benefits both the compiler and the reader of the API, whether that’s another programmer or yourself at a future point in time. Now let’s be practical for a moment too. There are dozens of __attribute__ directives and knowing every single one of them for every single compiler on every single architecture is just not a reasonable return on investment. Rather, let’s focus on a core set of commonly useful __attribute__ directives any developer can take advantage of. Recognizing the dangers of misusing an __attribute__ directive Just as poorly written comments and documentation can have consequences, providing the wrong __attribute__ can have consequences. In fact, since an __attribute__ affects code compilation, affixing the wrong __attribute__ to code can actually result in a bug that could be incredibly difficult to debug. Let’s take a look at an example of where an __attribute__ directive can be misused. Let’s suppose I have an enum that I use pretty often and frequently want a string version for it, whether it’s for populating a JSON structure or just for logging. I create a simple function to help me convert that enum into an NSString. // Header declarations\n\ntypedef NS_ENUM(char, XPL802_11Protocol) {\n    XPL802_11ProtocolA = 'a',\n    XPL802_11ProtocolB = 'b',\n    XPL802_11ProtocolG = 'g',\n    XPL802_11ProtocolN = 'n'\n};\n\nFOUNDATION_EXPORT NSString *XPL802_11ProtocolToString(XPL802_11Protocol protocol);\n\n// Implementation\n\nNSString *XPL802_11ProtocolToString(XPL802_11Protocol protocol)\n{\n    switch(protocol) {\n        case XPL802_11ProtocolA:\n\t\t     return @\"802.11a\";\n        case XPL802_11ProtocolB:\n\t\t     return @\"802.11b\";\n        case XPL802_11ProtocolG:\n\t\t     return @\"802.11g\";\n        case XPL802_11ProtocolN:\n\t\t     return @\"802.11n\";\n        default:\n           break;\n    }\n    return nil;\n} So I have my great little converting function and I end up using it a lot in my code. I notice that my return values are constant NSString references and are always the same based on the protocol that is provided as a parameter to my function. Aha! A prime candidate for a const __attribute__ directive. So I just update my header’s function declaration like so: FOUNDATION_EXPORT NSString*XPL802_11ProtocolToString(XPL802_11Protocol protocol)__attribute__((const)); And voilà! I have just provided context to any consumer of this function such that they know that the return value is completely based on the provided parameter and won’t change over the course of the process’ life. This change would also provide a performance boost, depending on how often the function is called, since the compiler now knows that it doesn’t actually have to re-execute this function if it already has cached the return value. Now, let’s say one day I notice the enum value is the character of the protocol and I decide to be clever and change my implementation to something like this: NSString *XPL802_11ProtocolToString(XPL802_11Protocol protocol)\n{\nswitch(protocol) {\n    case XPL802_11ProtocolA:\n    case XPL802_11ProtocolB:\n    case XPL802_11ProtocolG:\n    case XPL802_11ProtocolN:\n       return [NSString stringWithFormat:@\"802.11%c\", protocol];\n    default:\n       break;\n  }\n      return nil;\n} Now since I failed to remove the const attribute, I have just introduced a massive bug that could easily crash my app. Why? Well, the key difference is that the return value of my function is no longer a constant reference. When we were returning hard coded strings before, the compiler stored those strings as persistent memory and those NSStrings effectively had a retain count of infinite. Now that we dynamically generate the string based on the protocol’s char value, we are creating a new string every time - and that means memory that changes. The reference returned in one call to the function won’t actually be the same reference as a subsequent identical call. The problem will rear it’s head when the compiler optimizes subsequent calls to that function to just immediately access what the compiler considers the known return value, which would be the reference returned by the original call. Sadly, that reference has likely been deallocated and potentially reallocated by some other memory allocation by now. This will lead to our application crashing on either a bad memory access or an invalid method call on the object that occupies that reference’s memory. The worst of this is that the optimization that would cause this crash will only happen in builds that are highly optimized. Since debug builds often have optimizations turned down, you can run your app in a debugger forever and never reproduce it, making this bug, like most __attribute__ based bugs, very hard to figure out and fix. This is bug effectively boils down to treating a function that returns transient memory as const. The same goes for functions or methods that take transient memory as a parameter. Easy enough to remember is that any function returning a pointer return value must return a constant reference to use the const __attribute__ directive and absolutely no const function can have a pointer (including an Objective-C object) as a parameter. Now this example is merely a precaution for using __attribute__ directives and shouldn’t deter you from using them in your code. If you stick to __attribute__ directives you understand and pay attention to how they are used, you’ll be able to steer clear of these bugs and harness the power __attribute__ directives were meant to provide. Just remember, when in doubt, don’t attribute, because providing the wrong context is worse than providing no context. To point you in the right direction, below is a compiled list of useful attributes that should be more than enough to improve any developer’s tool belt. Core __attribute__ directives __attribute__((availability(…))), NS_AVAILABLE and NS_DEPRECATED Indicate the availability of an API on the platform NS_AVAILABLE: Apple macro for attributing an API as available in a given OS release. NS_AVAILABLE_IOS(available_os_version) NS_DEPRECATED: Apple macro for attributing an API as deprecated in a given OS release. NS_DEPRECATED_IOS(available_os_version,deprecated_os_version) Use NS_AVAILABLE and NS_DEPRECATED macros to hide the complexity of this attribute. Commonly used when deprecating one API and adding a new API as a replacement. When creating a new API for backwards compatibility, immediately attribute the API as deprecated and include a comment on the API that should be used for current OS targets. These directives are tied to the operating system version and cannot be used for framework versioning. For those instances, use __attribute__((deprecated(…)). FOUNDATION_EXPORT NSString * const MyClassNotification NS_AVAILABLE_IOS(3_0);\nFOUNDATION_EXPORT NSString * const MyClassNotificationOldKey NS_DEPRECATED_IOS(3_0, 7_0);\nFOUNDATION_EXPORT NSString * const MyClassNotificaitonNewKey NS_AVAILABLE_IOS(7_0);\n\nNS_AVAILABLE_IOS(3_0) @class MyClass : NSObject\n- (void)oldMethod NS_DEPRECATED_IOS(3_0, 6_0);\n- (void)newMethod:(out NSError * __autoreleasing *)outError NS_AVAILABLE_IOS(6_0); @end __attribute__((deprecated(…))) and __attribute__((unavailable(…))) Indicates that an API is deprecated/unavailable. __attribute__((deprecated(optional_message))) __attribute__((unavailable(optional_message))) In case you don’t want to use the availability attribute for deprecation. - (void)deprecatedMethod __attribute__((deprecated));\n- (void)deprecatedMethodWithMessage __attribute__((deprecated(\"this method was deprecated in MyApp.app version 5.0.2, use newMethod instead\"));\n\n- (void)unavailableMethod __attribute__((unavailable));\n- (void)unavailableMethodWithMessage __attribute__((unavailable(\"this method was removed from MyApp.app version 5.3.0, use newMethod instead\")); __attribute__((format(…))) and NS_FORMAT_FUNCTION Indicates that a function/method contains a format string with format arguments. __attribute__((format(format_type, format_string_index, first_format_argument_index))) format_type: one of printf, scant, strftime, strfmon or __NSString__ Reminder : argument indexes when specified in an attribute are 1 based. When it comes to Objective-C methods, remember they are just C functions whose first 2 arguments are the id self argument and the SEL _cmd argument. NS_FORMAT_FUNCTION: macro provided by Apple for __NSString__ type format string Use NS_FORMAT_FUNCTION macro when attributing a method/function with an objective-c string for formatting. FOUNDATION_EXPORT void NSLog(NSString *format, ...) NS_FORMAT_FUNCTION(1,2);\nvoid MyLog(MyLogLevel lvl, const char *format, ...) __attribute((format(printf, 2, 3)));\n// ...\n- (void)appendFormat:(NSString *)format, ... NS_FORMAT_FUNCTION(3, 4); __attribute__((sentinel(…))) and NS_REQUIRES_NIL_TERMINATION Indicates that a function/method requires a nil (NULL) argument, usually used as a delimiter. You can only use this attribute with variadic functions/methods. __attribute__((sentinel(index)) index: the index offset from the last argument in the variadic list of arguments. __attribute__((sentinel)) is equivalent to __attribute__((sentinel(0))) You’ll almost always want to use the NS_REQUIRES_NIL_TERMINATION macro // Example 1 @interface NSArray\n- (instancetype)arrayWithObjects:... NS_REQUIRES_NIL_TERMINATION; @end // Example 2 - of course you'd never do this...\nNSArray *CreateArrayWithObjectsWithLastArgumentIndicatingIfArrayIsMutable(...) __attribute__((sentinel(1)));\n \nvoid foo(id object1, id object2)\n{\n    NSArray *weirdArray = CreateArrayWithObjectsWithLastArgumentIndicatingIfArrayIsMutable(object1, object2, nil, YES);\n    NSAssert([weirdArrayrespondsToSelector: @selector (addObject:)]);\n    // ...\n} __attribute__((const)) and __attribute__((pure)) __attribute__((const)) is used to indicate that the function/method results are entirely dependent on the provided arguments and the function/method does not mutate state. __attribute__((pure)) is almost the same as its const counterpart, except that the function/method can also take global/static variables into account. Though adding the const or pure attribute to an Objective-C method is not as useful to the compiler due to the dynamic runtime, it is still VERY useful to a programmer reading an interface. It is recommended that all singleton instance accessors use the const attribute. The optimization upside of accurately using this attribute can be an enormous win. If you have an Objective-C class method that is frequently used and is const or pure, consider converting it into a C function to reap some serious benefits. On the flipside, using this attribute incorrectly can lead to a nearly impossible to locate bug as actually seeing the redundant use of the function removed by the compiler requires looking at the assembly! Oh, and this type of bug will rarely show in a debug build since only highly optimized builds will have the bug. // Example 1: Singleton @interface MySingleton : NSObject\n+ (MySingleton *)sharedInstance __attribute__((const)); @end // Example 2: Function overhead optimization\n\n// Get the description of a specified error number\nconst char *StringForErrNo(int errorNumber) __attribute__((const));// strerror(errorNumber)\n\n// Get the description of the global errno error number\nconst char *StringForGlobalErrNo(void) __attribute__((pure)); // strerror(errno)\n\nvoid DoStuffWithGlobalErrNo()\n{\n      NSLog(@\"%@ %s\", [NSStringstringWithUTF8String:StringForGlobalErrNo()],StringForGlobalErrNo());\n      printf(\"%s\\n\", StringForGlobalErrNo());\n      printf(\"%i\\n\", strlen(StringForGlobalErrNo()));\n}\n\n// will compile as something more like this:\n\nvoid DoStuffWithGlobalErrNo()\n{\nconst char *__error = StringForGlobalErrNo();\nNSLog(@\"%@ %s\", [NSString stringWithUTF8String:__error], __error);\n     printf(\"%s\\n\", __error);\n     printf(\"%i\\n\", strlen(__error));\n}\n\n// which effectively eliminates both 1) the overhead of the function call and 2) the internal execution cost of the function\n\n// Example 3: Function execution cost optimization\n\nint nthFibonacci(int n) __attribute__((const)); // naive implementation to get the nth fibonacci number without any caching\n\nvoid TestFibonacci()\n{\n     time_t start = time(NULL);\n     int result1 = nthFibonacci(1000); // execution time of D\n     time_t dur1 = time(NULL) - start; // some large duration D\n     int result2 = nthFibonacci(1000); // execution time of 1\n     time_t dur2 = time(NULL) - start; // same as dur1, duration D\n     int result3 = nthFibonacci(999); // execution time of ~D\n     time_t dur3 = time(NULL) - start; // duration of 2*D\n\n// The __attribute__((const)) directive can effectively eliminate a redundant call to an expensive operation...nice!\n} __attribute__((objc_requires_super)) and NS_REQUIRES_SUPER Indicate that the decorated method must call the super version of it’s implementation if overridden. Existed in LLVM for Xcode 4.5 but had bugs and wasn’t exposed with NS_REQUIRES_SUPER until Xcode 5.0 When creating a class that is expressly purposed to be a base class that is subclassed, any method that is supposed to be overridden but needs to have the super implementation called needs to use this macro. This attribute can be a large codebase win by contextualizing what methods are necessary for a base class to work. Widely adopt this attribute and your codebase will benefit. @interface MyBaseClass : NSObject\n- (void)handleStateTransition NS_REQUIRES_SUPER; @end // ... @interface MyConcreteClass : MyBaseClass @end @implementation MyConcreteClass\n\n- (void)handleStateTransition\n{\n     [super handleStateTransition]; // @end ARC __attribute__ directives __attribute__((objc_precise_lifetime)) and NS_VALID_UNTIL_END_OF_SCOPE Indicate that the given variable should be considered valid for the duration of its scope Though this will rarely come up, it can be a big help combatting a brain scratching crash that only appears in release builds (since the optimization likely doesn’t happen in your debug build) - (void)foo\n{\n    NS_VALID_UNTIL_END_OF_SCOPE MyObject *obj = [[MyObject alloc] init];\n    NSValue *value = [NSValue valueWithPointer:obj];\n\n// do stuff\n\n    MyObject *objAgain = [value pointerValue];\n    NSLog(@\"%@\", objAgain);\n}\n\n/* in ARC, without NS_VALID_UNTIL_END_OF_SCOPE, the compiler will optimize and after the obj pointer is used to create the NSValue the compiler will have no knowledge of the encapsulated use of the object in the NSValue. ARC will release obj and this NSLog line will crash with EXEC_BAD_ACCESS because the reference retrieved from the NSValue and stored in objAgain will now be pointing to the deallocated reference. */ __attribute__((ns_returns_retained)) and NS_RETURNS_RETAINED Indicates to ARC that the method returns a +1 retain count. Per Apple: only use this attribute for extraneous circumstances. Use the Objective-C naming convention of prefixing your method with alloc, new, copy, or mutableCopy to achieve the same result without an attribute. ARC will follow a naming convention and this directive for how to manage the returned value’s retain count. If the implementation is non-ARC, it is up to the implementation to adhere to the rule such that when an ARC file consumes the API the contract is adhered to. Honestly, you should just use the Apple recommend method prefix for methods and reserve this only for cases where you create an object with a +1 retain count with a function. NSString *CreateNewStringWithFormat(NSString *format, ...)NS_FORMAT_FUNCTION(1, 2) NS_RETURNS_RETAINED; __attribute__((ns_returns_not_retained)) and NS_RETURNS_NOT_RETAINED Indicates to ARC that the method returns a +0 retain count. Default behavior of all methods and functions in Objective-C. Per Apple: only use this attribute for extraneous circumstances. Use the Objective-C naming convention of NOT prefixing your method with alloc, new, copy, or mutableCopy to achieve the same result without an attribute. ARC will follow a naming convention and this directive for how to manage the returned value’s retain count. If the implementation is non-ARC, it is up to the implementation to adhere to the rule such that when an ARC file consumes the API the contract is adhered to. The only use for this attribute would be if you prefixed a method with alloc, new, copy, or mutableCopy but you didn’t want a +1 retain count - which is just nonsense. You should never need to use this attribute as it is implied on any method that doesn’t have a +1 keyword prefix. - (NSString *)newUnretainedStringWithFormat:(NSString *)format, ... NS_FORMAT_FUNCTION(3, 4) NS_RETURNS_NOT_RETAINED; __attribute__((objc_returns_inner_pointer)) and NS_RETURNS_INNER_POINTER Indicates that the method will return a pointer that is only valid for the lifetime of the owner. This will prevent ARC from preemptively releasing an object when the internal pointer is still in use. This is actually a very important attribute that few developers do a very good job of using, but really should. If a method returns a non Objective-C reference, then ARC doesn’t know that the returned value is a reference that belongs to the owning object and will go away if the owning object goes away. Without this attribute, after the final use of an object ARC will release it. This can result in a crash if the inner pointer is referenced after the last use of the object since it could have been deallocated. Ordering lines of code is not enough either, since the compiler could easily reorder the execution order as a way to optimize . @interface NSMutableData : NSData\n- (void *)mutableBytes NS_RETURNS_INNER_POINTER; @end void Foo(void)\n{\n     NSMutableData *buffer = [[NSMutableData alloc] initWithLength:8];\n     char* cBuffer = buffer.mutableBytes;\n     memcpy(cBuffer, \"1234567\", 8); // crash if NS_RETURNS_INNER_POINTER doesn't decorate the mutableBytes method\n     printf(\"%s\\n\", cBuffer);\n     (void)buffer; // this will not save us from a crash if the mutableBytes method isn't decorated with an NS_RETURNS_INNER_POINTER\n} __attribute__((ns_consumes_self)) and NS_REPLACES_RECEIVER Indicates that the provided method can replace the receiver with a different object. Presumes a +0 retain count (which can be overridden with NS_RETURNS_RETAINED, but if you do that you really need to be asking yourself “what the heck am I doing?”). By default, all methods prefixed with init are treated as if this attribute were decorating them. ARC makes this behavior really easy to implement. non-ARC implementers of the same behavior still need the decoration but also have to pay closer attention to how they are managing memory in the implementation. (awakeAfterUsingCoder: is regular source of memory leaks in non-ARC code bases) @interface NSObject (NSCoderMethods)\n- (id)awakeAfterUsingCoder:(NSCoder *) NS_REPLACES_RECEIVER; @end __attribute__((objc_arc_weak_reference_unavailable)) and NS_AUTOMATED_REFCOUNT_WEAK_UNAVAILABLE Indicates that the decorated class does not support weak referencing Mac OS X Examples: NSATSTypesetter, NSColorSpace, NSFont, NSMenuView, NSParagraphStyle, NSSimpleHorizontalTypesetter, NSTextView,NSFontManager, NSFontPanel, NSImage, NSTableCellView, NSViewController, NSWindow, and NSWindowController. iOS and Mac OS X Examples: NSHashTable, NSMapTable, or NSPointerArray NS_AUTOMATED_REFCOUNT_WEAK_UNAVAILABLE @interface NSHashTable : NSObject *Protocols*/>\n//... @end NS_AUTOMATED_REFCOUNT_UNAVAILABLE Indicates that the decorated API is unavailable in ARC. Can also use OBJC_ARC_UNAVAILABLE - (oneway void)release NS_AUTOMATED_REFCOUNT_UNAVAILABLE; More __attribute__ directives __attribute__((objc_root_class)) and NS_ROOT_CLASS __attribute__((constructor(…))) and __attribute__((destructor(…))) __attribute__((format_arg(…))) and NS_FORMAT_ARGUMENT __attribute__((nonnull(…))) __attribute__((returns_nonnull)) __attribute__((noreturn)) __attribute__((used)) __attribute__((unused)) __attribute__((warn_unused_result)) __attribute__((error(…))) and __attribute__((warning(…))) in, out and inout While we’re on the topic of providing context to code we should take the briefest of moments to bring up the Objective-C keywords in, out and inout. These little keywords are used to attribute Objective-C method arguments to provide context on whether the parameter is for input, output or both. These keywords came about with distributed objects along with oneway, byref, and bycopy but, in the spirit of providing context to programmers, these keywords can bridge the gap between the consumer of an API presuming how an argument will behave and knowing how that argument will behave. Consider using inout or out the next time you return a value via an argument and consumers of your API will appreciate it. in Indicates that the given argument is used only for input. This is the default behavior for non-pointers and Objective-C objects. Use this keyword for methods that accept a pointer to a primitive that is read but never modified. Not a common case. - (void)configureWithRect:(in CGRect *rect)\n{\n    if (rect) {\n        _configRect = *rect;\n    }\n    [self _innerConfigure];\n} out Indicates that the given argument is used just for output. This is never a default behavior. This keyword doesn’t make sense to apply to non pointers, which are always in arguments. Use this keyword for methods that return a value via an argument but don’t read that argument. - (void)configure:(out NSError **error)\n{\n     NSError *theError = [self _configure];\n     if (error) {\n        *error = theError;\n     }\n} inout Indicates that the given argument is used for both input and output. This is the default behavior for pointers, except for Objective-C objects (which default to in). This keyword doesn’t make sense to be applied to non pointers, which are always in arguments. Use this to provide context when it may not be apparent how the pointer behaves. Always use this to provide context when a method has numerous pointer arguments and at least one is in or out. Basically, when there are multiple pointer arguments and they are not all inout, every pointer argument should specify its behavior. - (void)configureRect:(inout CGRect *rect)\n{\n     if (rect) {\n         if (CGRectIsNull(*rect)) { // where rect acts an \"in\" argument\n             *rect = CGRectMake(_x, _y, _w, _h); // where rect acts as an \"out\" argument\n          }\n      }\n} __attribute__ directives as a tool With such a valuable tool available to the C languages, any team can benefit by using these __attribute__ directives to give context in their code. At Twitter, with a very large code base and many engineers developing on it daily, every bit of context that can be provided helps in maintaining a quality code base for reuse. Adding __attribute__ directives to your toolbelt of code comments and good naming conventions will give you a robust toolset for providing indispensable context to your code base. Don’t shy away from adding __attribute__ directives to your next project. Use them, evangelize them and everyone will benefit. __attribute__ resources GCC __attribute__ documentation Clang __attribute__ reference Clang Objective-C ARC Attributes NSHipster’s __attribute__ blog post", "date": "2014-03-10"},
{"website": "Twitter-Engineering", "title": "Netty at Twitter with Finagle", "author": ["‎@sprsquish‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2014/netty-at-twitter-with-finagle.html", "abstract": "Finagle is our fault tolerant, protocol-agnostic RPC framework built atop Netty . Twitter’s core services are built on Finagle, from backends serving user profile information, Tweets, and timelines to front end API endpoints handling HTTP requests. Part of scaling Twitter was the shift from a monolithic Ruby on Rails application to a service-oriented architecture. In order to build out this new architecture we needed a performant, fault tolerant, protocol-agnostic, asynchronous RPC framework. Within a service-oriented architecture, services spend most of their time waiting for responses from other upstream services. Using an asynchronous library allows services to concurrently process requests and take full advantage of the hardware. While Finagle could have been built directly on top of NIO, Netty had already solved many of the problems we would have encountered as well as provided a clean and clear API. Twitter is built atop several open source protocols: primarily HTTP, Thrift, Memcached, MySQL, and Redis. Our network stack would need to be flexible enough that it could speak any of these protocols and extensible enough that we could easily add more. Netty isn’t tied to any particular protocol. Adding to it is as simple as creating the appropriate event handlers. This extensibility has lead to many community driven protocol implementations including, SPDY , PostrgreSQL, WebSockets, IRC, and AWS. Netty’s connection management and protocol agnosticism provided an excellent base from which Finagle could be built. However we had a few other requirements Netty couldn’t satisfy out of the box as those requirements are more “high-level”. Clients need to connect to and load balance across a cluster of servers. All services need to export metrics (request rates, latencies, etc) that provide valuable insight for debugging service behavior. With a service-oriented architecture a single request may go through dozens of services making debugging performance issues nearly impossible without a tracing framework. Finagle was built to solve these problems. In the end Finagle relies on Netty for IO multiplexing providing a transaction-oriented framework on top of Netty’s connection-oriented model. How Finagle Works Finagle emphasizes modularity by stacking independent components together. Each component can be swapped in or out depending on the provided configuration. For example, tracers all implement the same interface. Thus, a tracer can be created to send tracing data to a local file, hold it in memory and expose a read endpoint, or write out to the network. At the bottom of a Finagle stack is a Transport. A Transport represents a stream of objects that can be asynchronously read from and written to. Transports are implemented as Netty ChannelHandlers and inserted into the end of a ChannelPipeline. Finagle’s ChannelHandlerTransport manages Netty interest ops to propagate back pressure. When Finagle indicates that the service is ready to read, Netty reads data off the wire and runs it through the ChannelPipeline where they’re interpreted by a codec then sent to the Finagle Transport. From there, Finagle sends the message through its own stack. For client connections, Finagle maintains a pool of transports across which it balances load. Depending on the semantics of the provided connection pool Finagle either requests a new connection from Netty or re-uses an existing one if it’s idle. When a new connection is requested, a Netty ChannelPipeline is created based on the client’s codec. Extra ChannelHandlers are added to the ChannelPipeline for stats, logging, and SSL. The connection is then handed to a channel transport which Finagle can write to and read from. If all connections are busy requests will be queued according to configurable queueing policies. On the server side Netty manages the codec, stats, timeouts, and logging via a provided ChannelPipelineFactory. The last ChannelHandler in a server’s ChannelPipeline is a Finagle bridge. The bridge will watch for new incoming connections and create a new Transport for each one. The Transport wraps the new channel before it’s handed to a server implementation. Messages are then read out of the ChannelPipeline and sent to the implemented server instance. 1) Finagle Client which is layered on top of the Finagle Transport. This Transport abstracts Netty away from the user 2) The actual ChannelPipeline of Netty that contains all the ChannelHandler implementations that do the actual work 3) Finagle Server which is created for each connection and provided a transport to read from and write to 4) ChannelHandlers implementing protocol encoding/decoding logic, connection level stats, SSL handling. Bridging Netty and Finagle Finagle clients use ChannelConnector to bridge Finagle and Netty. ChannelConnector is a function that takes a SocketAddress and returns a Future Transport. When a new connection is requested of Netty, Finagle uses a ChannelConnector to request a new Channel and create a new Transport with that Channel. The connection is established asynchronously, fulfilling the Future with the new Transport on success or a failure if the connection cannot be established. A Finagle client can then dispatch requests over the Transport. Finagle servers bind to an interface and port via a Listener. When a new connection is established, the Listener creates a Transport and passes it to a provided function. From there, the Transport is handed to a Dispatcher which dispatches requests from the Transport to the Service according to a given policy. Finagle’s Abstraction Finagle’s core concept is a simple function (functional programming is the key here) from Request to Future of Response. type Service[Req, Rep] = Req => Future[Rep] A future is a container used to hold the result of an asynchronous operation such as a network RPC, timeout, or disk I/O operation. A future is either empty—the result is not yet available; succeeded—the producer has completed and has populated the future with the result of the operation; or failed—the producer failed, and the future contains the resulting exception. This simplicity allows for very powerful composition. Service is a symmetric API representing both the client and the server. Servers implement the service interface. The server can be used concretely for testing or Finagle can expose it on a network interface. Clients are provided an implemented service that is either virtual or a concrete representation of a remote server. For example, we can create a simple HTTP server by implementing a service that takes an HttpReq and returns a Future[HttpRep] representing an eventual response: val s: Service[HttpReq, HttpRep] = new Service[HttpReq, HttpRep] { \n  def apply(req: HttpReq): Future[HttpRep] =\n    Future.value(HttpRep(Status.OK, req.body))\n}\nHttp.serve(\":80\", s) A client is then provided with a symmetric representation of that service: val client: Service[HttpReq, HttpRep] = Http.newService(\"twitter.com:80\")\nval f: Future[HttpRep] = client(HttpReq(\"/\"))\nf map { rep => transformResponse(rep) } This example exposes the server on port 80 of all interfaces and consumes from twitter.com port 80. However we can also choose not to expose the server and instead use it directly: server(HttpReq(\"/\")) map { rep => transformResponse(rep) } Here the client code behaves the same way but doesn’t require a network connection. This makes testing clients and servers very simple and straightforward. Clients and servers provide application-specific functionality. However, there is a need for application agnostic functionality as well. Timeouts, authentication, and statics are a few examples. Filters provide an abstraction for implementing application-agnostic functionality. Filters receive a request and a service with which it is composed. type Filter[Req, Rep] = (Req, Service[Req, Rep]) => Future[Rep] Filters can be chained together before being applied to a service. recordHandletime andThen\ntraceRequest andThen\ncollectJvmStats\nandThen myService This allows for clean abstractions of logic and good separation of concerns. Internally, Finagle heavily uses filters. Filters enhance modularity and reusability. They’ve proved valuable for testing as they can be unit tested in isolation with minimal mocking. Filters can also modify both the data and type of requests and responses. The figure below shows a request making its way through a filter chain into a service and back out. We might use type modification for implementing authentication. val auth: Filter[HttpReq, AuthHttpReq, HttpRes, HttpRes] =\n{ (req, svc) => authReq(req) flatMap { authReq => svc(authReq) } }\n\nval authedService: Service[AuthHttpReq, HttpRes] = ...\nval service: Service[HttpReq, HttpRes] =\nauth andThen authedService Here we have a service that requires and AuthHttpReq. To satisfy the requirement, a filter is created that can receive an HttpReq and authenticate it. The filter is then composed with the service, yielding a new service that can take an HttpReq and produce an HttpRes. This allows us to test the authenticating filter in isolation of the service. Failure Management At scale, failure becomes common rather than exceptional; hardware fails, networks become congested, network links fail. Libraries capable of extremely high throughput and extremely low latency are meaningless if the systems they run on or communicate with fail. To that end, Finagle is set up to manage failures in a principled way. It trades some throughput and latency for better failure management. Finagle can balance load across a cluster of hosts implicitly using latency as a heuristic. A Finagle client locally tracks load on every host it knows about. It does so by counting the number of outstanding requests being dispatched to a single host. Given that, Finagle will dispatch new requests to hosts with the lowest load and implicitly the lowest latency. Failed requests will cause Finagle to close the connection to the failing host and remove it from the load balancer. In the background, Finagle will continuously try to reconnect. The host will be re-added to the load balancer only after Finagle can re-establish a connection. Service owners are then free to shut down individual hosts without negatively impacting downstream clients. Clients also keep per-connection health heuristics and remove the connection if it’s deemed unhealthy. Composing Services Finagle’s service as a function philosophy allows for simple, but expressive code. For example, a user’s request for their home timeline touches several services. The core of these are the authentication service, timeline service, and Tweet service. These relationships can be expressed succinctly. val timelineSvc = Thrift.newIface[TimelineService](...) // #1 \nval tweetSvc = Thrift.newIface[TweetService](...)\nval authSvc = Thrift.newIface[AuthService](...)\n \nval authFilter = Filter.mk[Req, AuthReq, Res, Res] { (req, svc) => // #2 \n  authSvc.authenticate(req) flatMap svc(_)\n}\n \nval apiService = Service.mk[AuthReq, Res] { req => \n  timelineSvc(req.userId) flatMap {tl =>\n    val tweets = tl map tweetSvc.getById(_)\n    Future.collect(tweets) map tweetsToJson(_) }\n    } \n  } //#3 \nHttp.serve(\":80\", authFilter andThen apiService) // #4\n \n// #1 Create a client for each service\n// #2 Create new Filter to authenticate incoming requests\n// #3 Create a service to convert an authenticated timeline request to a json response \n// #4 Start a new HTTP server on port 80 using the authenticating filter and our service Here we create clients for the timeline service, Tweet service, and authentication service. A filter is created for authenticating raw requests. Finally, our service is implemented, combined with the auth filter and exposed on port 80. When a request is received, the auth filter will attempt to authenticate it. A failure will be returned immediately without ever affecting the core service. Upon successful authentication the AuthReq will be sent to the API service. The service will use the attached userId to lookup the user’s timeline via the timeline service. A list of tweet ids is returned then iterated over. Each id is then used to request the associated tweet. Finally, the list of Tweet requests is collected and converted into a JSON response. As you can see, the flow of data is defined and we leave the concurrency to Finagle. We don’t have to manage thread pools or worry about race conditions. The code is clear and safe. Conclusion We’ve been working closely with the Netty committers to improve on parts of Netty that both Finagle and the wider community can benefit from . Recently the internal structure of Finagle has been updated to be more modular, paving the way for an upgrade to Netty 4. Finagle has yielded excellent results. We’ve managed to dramatically increase the amount of traffic we can serve while reducing latencies and hardware requirements. For example, after moving our API endpoints from the Ruby stack onto Finagle, we saw p99 latencies drop from hundreds of milliseconds to tens. Our new stack has enabled us to reach new records in throughput and as of this writing our record tweets per second is 143,199 . Finagle was born out of a need to set Twitter up to scale out to the entire globe at a time when we were struggling with site stability for our users. Using Netty as a base, we could quickly design and build Finagle to manage our scaling challenges. Finagle and Netty handle every request Twitter sees. Thanks This post will also appear as a case study in the Netty in Action book by Norman Maurer. Your Server as a Function by Marius Eriksen provides more insight into Finagle’s philosophy. Many thanks to Trustin Lee and Norman Maurer for their work on Netty. Thanks to Marius Eriksen , Evan Meagher , Moses Nakamura , Steve Gury , Ruben Oanta , Brian Degenhardt for their insights.", "date": "2014-02-13"},
{"website": "Twitter-Engineering", "title": "Introducing Twitter Data Grants", "author": ["‎@raffi‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2014/introducing-twitter-data-grants.html", "abstract": "Today we’re introducing a pilot project we’re calling Twitter Data Grants , through which we’ll give a handful of research institutions access to our public and historical data. With more than 500 million Tweets a day, Twitter has an expansive set of data from which we can glean insights and learn about a variety of topics, from health-related information such as when and where the flu may hit to global events like ringing in the new year . To date, it has been challenging for researchers outside the company who are tackling big questions to collaborate with us to access our public, historical data. Our Data Grants program aims to change that by connecting research institutions and academics with the data they need. Submit a proposal for consideration to our Twitter Data Grants pilot program by March 15. If you’d like to participate, submit a proposal here no later than March 15th. For this initial pilot, we’ll select a small number of proposals to receive free datasets. We can do this thanks to Gnip, one of our certified data reseller partners . They are working with us to give selected institutions free and easy access to Twitter datasets. In addition to the data, we will also be offering opportunities for the selected institutions to collaborate with Twitter engineers and researchers. We encourage those of you at research institutions using Twitter data to send in your best proposals. To get updates and stay in touch with the program: visit research.twitter.com, make sure to follow @TwitterEng , or email data-grants@twitter.com with questions.", "date": "2014-02-05"},
{"website": "Twitter-Engineering", "title": "Mesos 0.15 and Authentication Support", "author": ["‎@vinodkone‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2014/mesos-015-and-authentication-support.html", "abstract": "With the latest Mesos 0.15.0 release , we are pleased to report that we’ve added initial authentication support for frameworks (see MESOS-418 ) connecting to Mesos. In a nutshell, this feature allows only authenticated frameworks to register with Mesos and launch tasks. Authentication is important as it prevents rogue frameworks from causing problems that may impact the usage of resources within a Mesos cluster. How it works Mesos uses the excellent Cyrus SASL library to provide authentication. SASL is a very flexible authentication framework that allows two endpoints to authenticate with each other and also has support for various authentication mechanisms (e.g., ANONYMOUS, PLAIN, CRAM-MD5, GSSAPI). In this release, Mesos uses SASL with the CRAM-MD5 authentication mechanism. The process for enabling authentication begins with the creation of an authentication credential that is unique to the framework. This credential constitutes a principal and secret pair, where principal refers to the identity of the framework. Note that the ‘principal’ is different from the framework user (the Unix user which executors run as) or the resource role (role that has reserved certain resources on a slave). These credentials should be shipped to the Mesos master machines, as well as given access to the framework, meaning that both the framework and the Mesos masters should be started with these credentials. Once authentication is enabled, Mesos masters only allow authenticated frameworks to register. Authentication for frameworks is performed under the hood by the new scheduler driver. For specific instructions on how to do this please read the upgrade instructions . Looking ahead Adding authentication support for slaves Similar to adding authentication support to frameworks, it would be great to add authentication support to the slaves. Currently any node in the network can run a Mesos slave process and register with the Mesos master. Requiring slaves to authenticate with the master before registration would prevent rogue slaves from causing problems like DDoSing the master or getting access to users tasks in the cluster. Integrating with Kerberos Currently the authentication support via shared secrets between frameworks and masters is basic to benefit usability. To improve upon this basic approach, a more powerful solution would be to integrate with an industry standard authentication service like Kerberos . A nice thing about SASL and one of the reasons we picked it is because of its support for integration with GSSAPI/Kerberos. We plan to leverage this support to integrate Kerberos with Mesos. Data encryption Authentication is only part of the puzzle when it comes to deploying and running applications securely in the cloud. Another crucial component is data encryption. Currently all the messages that flow through the Mesos cluster are un-encrypted making it possible for intruders to intercept and potentially control your task. We plan to add encryption support by adding SSL support to libprocess , the low-level communication library that Mesos uses which is responsible for all network communication between Mesos components. Authorization We are also investigating authorizing principals to allow them access to only a specific set of operations like launching tasks or using resources. In fact, you could imagine a world where an authenticated ‘principal’ will be authorized to on behalf of a subset of ‘user’s and ‘role’s for launching tasks and accepting resources respectively. This authorization information could be stored in a directory service like LDAP. Thanks While a lot of people contributed to this feature, we would like to give special thanks to Ilim Igur , our Google Summer of Code intern who started this project and contributed to the initial design and implementation. If you are as excited as us about this feature please go ahead and play with latest release and let us know what you think. You can get in touch with us via @ApacheMesos or via mailing lists and IRC .", "date": "2014-01-09"},
{"website": "Twitter-Engineering", "title": "2014", "author": ["‎@‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2014.html", "abstract": "", "date": "1970-01-01"},
{"website": "Twitter-Engineering", "title": "Hello World", "author": ["‎@sandofsky‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2010/hello-world-0.html", "abstract": "Welcome! I’m Ben, and I’m an engineer at Twitter. We’ve started this blog to show some of the cool things we’re creating and tough problems we’re solving. As a fun way to kick things off, I ran Code Swarm over a few essential production apps. Icons represent developers, and particles represent files added or modified. It doesn’t cover prototypes or contributions to open source, so it isn’t exactly scientific, but it still goes to show Twitter’s explosive growth mirrored in engineering. (The forked version of Code Swarm that adds rounded avatars and particle trails is available on github .)", "date": "2010-02-03"},
{"website": "Twitter-Engineering", "title": "WOEIDs in Twitter's Trends", "author": ["‎@raffi‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2010/woeids-in-twitters-trends.html", "abstract": "How do you represent a “place”? That’s what we were wondering when we were putting together our API for Trends on Twitter . We needed a way to represent a place in a permanent and language-independent way - we didn’t want to be caught in using an identifier that may change over time, and we didn’t want to be caught in internationalization issues. Where we landed was on using Yahoo!’s Where On Earth IDs, or WOEIDs, as our representation. WOEID s are 32-bit identifiers that are “unique and non-repetitive” — if a location is assigned a WOEID, the WOEID assigned is never changed, and that particular WOEID is never given to another location. In addition, a WOEID has certain properties such as an implied hierarchy (a particular city is in a particular state, for example), and a taxonomy of categories (there is a “language” to categorize something as a “town” or a “suburb”). Finally, there is a standard endpoint to query to get more information about a place. A program that wanted to get data about San Francisco, CA would first determine that it has a WOEID of 2487956, and with that query http://where.yahooapis.com/v1/place/2487956 . What this all means is that our Trends API is now interoperable with anybody else who is building a system on top of WOEIDs — you could easily mash-up Flickr’s places with our Trend locations, for example. If you want to give something like that a try, check out the trends/available endpoint, as that will let you know what WOEIDs we are exposing trending information for. With those WOEIDs, you can then hit up trends/location to get the actual trending data. There are two niceties to the API: pass in a latitude/longitude when querying trends/available, and remember that there is a hierarchy of WOEIDs. If you pass in a lat and a long parameter to trends/available, then all the locations that are returned are sorted by their haversine distance from the coordinate passed in. Application developers can use this to help get trends from “around where you are”. And second, like I mentioned above, WOEIDs form a hierarchy (that’s mostly correct). Here is the hierarchy of the locations that we have as of today: 1 (\"Terra\")\n |---- 23424775 (\"Canada\" - Country)\n |---- 23424803 (\"Ireland\" - Country)\n |---- 23424975 (\"United Kingdom\" - Country)\n | \\---- 24554868 (\"England\" - State)\n |   \\---- 23416974 (\"Greater London\" - County)\n |     \\---- 44418 (\"London\" - Town)\n |---- 23424900 (\"Mexico\" - Country)\n |---- 23424768 (\"Brazil\" - Country)\n | \\---- 2344868 (\"Sao Paulo\" - State)\n |   \\---- 12582314 (\"São Paulo\" - County)\n |     \\---- 455827 (\"Sao Paulo\" - Town)\n \\---- 23424977 (\"United States\" - Country)\n   |---- 2347572 (\"Illinois\" - State)\n   | \\---- 12588093 (\"Cook\" - County)\n   |   \\---- 2379574 (\"Chicago\" - Town)\n   |---- 2347567 (\"District of Columbia\" - State)\n   | \\---- 12587802 (\"District of Columbia\" - County)\n   |   \\---- 2514815 (\"Washington\" - Town)\n   |---- 2347606 (\"Washington\" - State)\n   | \\---- 12590456 (\"King\" - County)\n   |   \\---- 2490383 (\"Seattle\" - Town)\n   |---- 2347579 (\"Maryland\" - State)\n   | \\---- 12588679 (\"Baltimore City\" - County)\n   |   \\---- 2358820 (\"Baltimore\" - Town)\n   |---- 2347563 (\"California\" - State)\n   | |---- 12587707 (\"San Francisco\" - County)\n   | | \\---- 2487956 (\"San Francisco\" - Town)\n   | \\---- 12587688 (\"Los Angeles\" - County)\n   |   \\---- 2442047 (\"Los Angeles\" - Town)\n   |---- 2347580 (\"Massachusetts\" - State)\n   | \\---- 12588712 (\"Suffolk\" - County)\n   |   \\---- 2367105 (\"Boston\" - Town)\n   |---- 2347591 (\"New York\" - State)\n   | \\---- 2459115 (\"New York\" - Town)\n   |---- 2347569 (\"Georgia\" - State)\n   | \\---- 12587929 (\"Fulton\" - County)\n   |   \\---- 2357024 (\"Atlanta\" - Town)\n   |---- 2347602 (\"Texas\" - State)\n   | |---- 12590226 (\"Tarrant\" - County)\n   | | \\---- 2406080 (\"Fort Worth\" - Town)\n   | |---- 12590107 (\"Harris\" - County)\n   | | \\---- 2424766 (\"Houston\" - Town)\n   | |---- 12590063 (\"Dallas\" - County)\n   | | \\---- 2388929 (\"Dallas\" - Town)\n   | \\---- 12590021 (\"Bexar\" - County)\n   |   \\---- 2487796 (\"San Antonio\" - Town)\n   \\---- 2347597 (\"Pennsylvania\" - State)\n     \\---- 12589778 (\"Philadelphia\" - County)\n       \\---- 2471217 (\"Philadelphia\" - Town) Right now, even though we don’t expose this information using trends/available, you could ask for any of those WOEIDs, and we’ll choose the nearest trend location (or locations!) that we have data for. And, of course, we have a few more things in the pipeline… — @raffi", "date": "2010-02-04"},
{"website": "Twitter-Engineering", "title": "Introducing the Open Source Twitter Text libraries", "author": ["‎@mzsanford‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2010/introducing-the-open-source-twitter-text-libraries.html", "abstract": "Over time Tweets have acquired a language all their own. Some of these have been around a long time (like @username at the beginning of a Tweet) and some of these are relatively recent (such as lists) but all of them make the language of Tweets unique. Extracting these Tweet-specific components from a Tweet is relatively simple for the majority of Tweets, but like most text parsing issues the devil is in the details. We’ve extracted the code we use to handle Tweet-specific elements and released it as an open source library. This first version is available in Ruby and Java but in the Twitter spirit of openness we’ve also released a conformance test suite so any other implementations can verify they meet the same standards. Tweet-specific Language It all started with the @reply … and then it got complicated. Twitter users started the use of @username at the beginning of a Tweet to indicate a reply, but you’re not here to read about history. In order to talk about the new Twitter Text libraries one needs to understand the Tweet-specific elements we’re interested in. Much of this will be a review of what you already know but a shared vocabulary will help later on. While the Tweet-specific language is always expanding the current elements consist of: @reply This is a Tweet which begins with @username. This is distinct from the presence of @username elsewhere in the Tweet (more on that in a moment). An @reply Tweet is considered directly addressed to the @username and only some of your followers will see the Tweets (notably, those who follow both you and the @username). @mention This is a Tweet which contains one or more @usernames anywhere in the Tweet. Technically an @reply is a type of @mention , which is important from a parsing perspective. An @mention Tweets will be delivered to all of your followers regardless of is the follow the @mentioned user or not. @username/list-name Twitter lists are referenced using the syntax @username/list-name where the list-name portion has to meet some specific rules. #hashtag As long has there has been a way to search Tweets* people have been adding information to make the easy to find. The #hashtag syntax has become the standard for attaching a succinct tag to Tweets. URLs While URLs are not Tweet-specific they are an important part of Tweets and require some special handling. There is a vast array of services based on the URLs in Tweets. In addition to services that extract the URLs most people expect URLs to be automatically converted to links when viewing a Tweet. Twitter Text Libraries For this first version of the Twitter Text libraries we’ve released both Ruby and Java versions. We certainly expect more languages in the future and we’re looking forward to the patches and feedback we’ll get on these first versions. For each library we’ve provided functions for extracting the various Tweet-specific elements. Displaying Tweets in HTML is a very common use case so we’ve also included HTML auto-linking functions. The individual language interfaces differ so they can feel as natural as possible for each individual language. Ruby Library The Ruby library is available as a gem via gemcutter or the source code can be found on github . You can also peruse the rdoc hosted on github . The Ruby library is provided as a set of Ruby modules so they can be included in your own classes and modules. The rdoc is a more complete reference but for a quick taste check out this short example: class MyClass\n  include Twitter::Extractor\n  usernames = extract_mentioned_screen_names(\"Mentioning @twitter and @jack\")\n  # usernames = [\"twitter\", \"jack\"]\nend The interface makes this all seems quite simple but there are some very complicated edge cases. I’ll talk more about that in the next section, Conformance Testing. Java Library The source code for the Java library can be found on github . The library provides an ant file for buildinf the twitter-text.jar file. You can also peruse the javadocs hosted on github . The Java library provides Extractor and Autolink classes that provide object-oriented methods for extraction and auto-linking. The javadoc is a more complete reference but for a quick taste check out this short example: import java.util.List;\nimport com.twitter.Extractor;\n\npublic class Check {\n  public static void main(String[] args) {\n    List names;\n    Extractor extractor = new Extractor();\n\n    names = extractor.extractMentionedScreennames(\"Mentioning @twitter and @jack\");\n    for (String name : names) {\n      System.out.println(\"Mentioned @\" + name);\n    }\n  }\n} The library makes this all seems quite simple but there are some very complicated edge cases. Conformance Testing While working on the Ruby and Java version of the Twitter Text libraries it became pretty clear that porting tests to each language individually wasn’t going to be sustainable. To help keep things in sync we created that Twitter Text Conformance project . This project provides some simple yaml files that define the expected before and after states for testing. The per-language implementation of these tests can vary along with the per-language interface, making it intuitive for programmers in any language. The basic extraction and auto-link test cases are easy to understand but the edge cases about. Many of the largest complications come from handling Tweets written in Japanese and other languages that don’t use spaces. We also try to be lenient with the allowed URL characters, which creates some more headaches. — @mzsanford", "date": "2010-02-04"},
{"website": "Twitter-Engineering", "title": "Links: Relational Algebra and Scala DI", "author": ["‎@evan‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2010/links-relational-algebra-and-scala-di.html", "abstract": "Wondering what other technical concerns we face here at Twitter? Infrastructure engineer Nick Kallen has a pair of posts on his personal blog: one about the new relational algebra system behind the Rails 3 ORM, and one about dependency injection in Scala . — @evan", "date": "2010-02-09"},
{"website": "Twitter-Engineering", "title": "The Anatomy of a Whale", "author": ["‎@asdf‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2010/the-anatomy-of-a-whale.html", "abstract": "Sometimes it’s really hard to figure out what’s causing problems in a web site like Twitter. But over time we have learned some techniques that help us to solve the variety of problems that occur in our complex web site. A few weeks ago, we noticed something unusual: over 100 visitors to Twitter per second saw what is popularly known as “the fail whale”. Normally these whales are rare; 100 per second was cause for alarm. Although even 100 per second is a very small fraction of our overall traffic, it still means that a lot of users had a bad experience when visiting the site. So we mobilized a team to find out the cause of the problem. What Causes Whales? What is the thing that has come to be known as “the fail whale”? It is a visual representation of the HTTP “503: Service Unavailable” error. It means that Twitter does not have enough capacity to serve all of its users. To be precise, we show this error message when a request would wait for more than a few seconds before resources become available to process it. So rather than make users wait forever, we “throw away” their requests by displaying an error. This can sometimes happen because too many users try to use Twitter at once and we don’t have enough computers to handle all of their requests. But much more likely is that some component part of Twitter suddenly breaks and starts slowing down. Discovering the root cause can be very difficult because Whales are an indirect symptom of a root cause that can be one of many components. In other words, the only concrete fact that we knew at the time was that there was some problem, somewhere. We set out to uncover exactly where in the Twitter requests’ lifecycle things were breaking down. Debugging performance issues is really hard. But it’s not hard due to a lack of data; in fact, the difficulty arises because there is too much data. We measure dozens of metrics per individual site request, which, when multiplied by the overall site traffic, is a massive amount of information about Twitter’s performance at any given moment. Investigating performance problems in this world is more of an art than a science. It’s easy to confuse causes with symptoms and even the data recording software itself is untrustworthy. In the analysis below we used a simple strategy that involves proceeding from the most aggregate measures of system as a whole and at each step getting more fine grained, looking at smaller and smaller parts. How is a Web Page Built? Composing a web page for Twitter request often involves two phases. First data is gathered from remote sources called “network services”. For example, on the Twitter homepage your tweets are displayed as well as how many followers you have. These data are pulled respectively from our tweet caches and our social graph database, which keeps track of who follows whom on Twitter. The second phase of the page composition process assembles all this data in an attractive way for the user. We call the first phase the IO phase and the second the CPU phase. In order to discover which phase was causing problems, we checked data that records what amount of time was spent in each phase when composing Twitter’s web pages. The green line in this graph represents the time spent in the IO phase and the blue line represents the CPU phase. This graph represents about 1 day of data. You can see that the relationships change over the course of the day. During non-peak traffic, CPU time is the dominant portion of our request, with our network services responding relatively quickly. However, during peak traffic, IO latency almost doubles and becomes the primary component of total request latency. Understanding Performance Degradation There are two possible interpretations for this ratio changing over the course of the day. One possibility is that the way people use Twitter during one part of the day differs from other parts of the day. The other possibility is that some network service degrades in performance as a function of use. In an ideal world, each network service would have equal performance for equal queries; but in the worst case, the same queries actually get slower as you run more simultaneously. Checking various metrics confirmed that users use Twitter the same way during different parts of the day. So we hypothesize that the problem must be in a network service degrading poorly. We were still unsure; in any good investigation one must constantly remain skeptical. But we decided that we had enough information to transition from this more general analysis of the system into something more specific, so we looked into IO latency data. This graph represents the total amount of time waiting for our network services to deliver data. Since the amount of traffic we get changes over the course of the day, we expect any total to vary proportionally. But this graph is actually traffic independent; that is, we divide the measured latency by the amount of traffic at any given time. If any traffic-independent total latency changes over the course of the day, we know the corresponding network service is degrading with traffic. You can see that the purple line in this graph (which represents Memcached) degrades dramatically as traffic increases during peak hours. Furthermore, because it is at the top of the graph it is also the biggest proportion of time waiting for network services. So this correlates with the previous graph and we now have a stronger hypothesis: Memcached performance degrades dramatically during the course of the day, which leads to slower response times, which leads to whales. This sort of behavior is consistent with insufficient resource capacity. When a service with limited resources, such as Memcached, is taxed to its limits, requests begin contending with each other for Memcached’s computing time. For example, if Memcached can only handle 10 requests at a time but it gets 11 requests at time, the 11th request needs to wait in line to be served. Focus on the Biggest Contributor to the Problem If we can add sufficient Memcached capacity to reduce this sort of resource contention, we could increase the throughput of Twitter.com substantially. If you look at the above graph, you can infer that this optimization could increase twitter performance by 50%. There are two ways to add capacity. We could do this by adding more computers (memcached servers). But we can also change the software that talks to Memcached to be as efficient with its requests as possible. Ideally we do both. We decided to first pursue how we query Memcached to see if there was any easy way to optimize that by reducing the overall number of queries. But, there are many types of queries to memcached and it might be that some may take longer than others. We want to spend our time wisely and focus on optimizing the queries that are most expensive in aggregate. We sampled a live process to record some statistics on which queries take the longest. The following is each type of Memcached query and how long they take on average: get         0.003s\nget_multi   0.008s\nadd         0.003s\ndelete      0.003s\nset         0.003s\nincr        0.003s\nprepend     0.002s You can see that get_multi is a little more expensive than the rest but everything else is the same. But that doesn’t mean it’s the source of the problem. We also need to know how many requests per second there are for each type of query. get         71.44%\nget_multi    8.98%\nset          8.69%\ndelete       5.26%\nincr         3.71%\nadd          1.62%\nprepend      0.30% If you multiply average latency by the percentage of requests you get a measure of the total contribution to slowness. Here, we found that gets were the biggest contributor to slowness. So, we wanted to see if we could reduce the number of gets. Tracing Program Flow Since we make Memcached queries from all over the Twitter software, it was initially unclear where to start looking for optimization opportunities. Our first step was to begin collecting stack traces, which are logs that represent what the program is doing at any given moment in time. We instrumented one of our computers to sample some small percentages of get memcached calls and record what sorts of things caused them. Unfortunately, we collected a huge amount of data and it was hard to understand. Following our precedent of using visualizations in order to gain insight into large sets of data, we took some inspiration from the Google perf-tools project and wrote a small program that generated a cloud graph of the various paths through our code that were resulting in Memcached Gets. Here is a simplified picture: Each circle represents one component/function. The size of the circle represents how big a proportion of Memcached get queries come from that function. The lines between the circles show which function caused the other function to occur. The biggest circle is check_api_rate_limit but it is caused mostly by authenticate_user and attempt_basic_auth . In fact, attempt_basic_auth is the main opportunity for enhancement. It helps us compute who is requesting a given web page so we can serve personalized (and private) information to just the right people. Any Memcached optimizations that we can make here would have a large effect on the overall performance of Twitter. By counting the number of actual get queries made per request, we found that, on average, a single call to attempt_basic_auth was making 17 calls. The next question is: can any of them be removed? To figure this out we need to look very closely at the all of the queries. Here is a “history” of the the most popular web page that calls attempt_basic_auth . This is the API request for http://twitter.com/statuses/friends_timeline.format , the most popular page on Twitter! get([\"User:auth:missionhipster\",                       # maps screen name to user id\nget([\"User:15460619\",                                  # gets user object given user id (used to match passwords)\nget([\"limit:count:login_attempts:...\",                 # prevents dictionary attacks\nset([\"limit:count:login_attempts:...\",                 # unnecessary in most cases, bug\nset([\"limit:timestamp:login_attempts:...\",             # unnecessary in most cases, bug\nget([\"limit:timestamp:login_attempts:...\",\nget([\"limit:count:login_attempts:...\",                 # can be memoized\nget([\"limit:count:login_attempts:...\",                 # can also be memoized\nget([\"user:basicauth:...\",                             # an optimization to avoid calling bcrypt\nget([\"limit:count:api:...\",                            # global API rate limit\nset([\"limit:count:api:...\",                            # unnecessary in most cases, bug\nset([\"limit:timestamp:api:...\",                        # unnecessary in most cases, bug\nget([\"limit:timestamp:api:...\",\nget([\"limit:count:api:...\",                            # can be memoized from previous query\nget([\"home_timeline:15460619\",                         # determine which tweets to display\nget([\"favorites_timeline:15460619\",                    # determine which tweets are favorited\nget_multi([[\"Status:fragment:json:7964736693\",         # load, in parallel, all of the tweets we're gonna display. Note that all of the “limit:” queries above come from attempt_basic_auth . We noticed a few other (relatively minor) unnecessary queries as well. It seems like from this data we can eliminate seven out of seventeen total Memcached calls — a 42% improvement for the most popular page on Twitter. At this point, we need to write some code to make these bad queries go away. Some of them we cache (so we don’t make the exact same query twice), some are just bugs and are easy to fix. Some we might try to parallelize (do more than one query at the same time). But this 42% optimization (especially if combined with new hardware) has the potential to eliminate the performance degradation of our Memcached cluster and also make most page loads that much faster. It is possible we could see a (substantially) greater than 50% increase in the capacity of Twitter with these optimizations. This story presents a couple of the fundamental principles that we use to debug the performance problems that lead to whales. First, always proceed from the general to the specific. Here, we progressed from looking first at I/O and CPU timings to finally focusing on the specific Memcached queries that caused the issue. And second, live by the data, but don’t trust it. Despite the promise of a 50% gain that the data implies, it’s unlikely we’ll see any performance gain anywhere near that. Even still, it’ll hopefully be substantial. — @asdf and @nk", "date": "2010-02-10"},
{"website": "Twitter-Engineering", "title": "Link: Cassandra at Twitter", "author": ["‎@jeanpaul‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2010/link-cassandra-at-twitter.html", "abstract": "Storage Team Lead Ryan King recently spoke to MyNoSQL about how we’re using Cassandra at Twitter. In the interview Ryan talks about the reasons for the switch and how we plan to migrate tweets from MySQL to Cassandra. — @jeanpaul", "date": "2010-02-23"},
{"website": "Twitter-Engineering", "title": "Unicorn Power", "author": ["‎@sandofsky‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2010/unicorn-power.html", "abstract": "Have you found yourself waiting in line at a supermarket and the guy in front decides to pay by check? One person can slow everyone down. We had a similar problem at Twitter. Every server has a fixed number of workers (cashiers) that handle incoming requests. During peak hours, we may get more simultaneous requests than available workers. We respond by putting those requests in a queue. This is unnoticeable to users when the queue is short and we handle requests quickly, but large systems have outliers. Every so often a request will take unusually long, and everyone waiting behind that request suffers. Worse, if an individual worker’s line gets too long, we have to drop requests. You may be presented with an adorable whale just because you landed in the wrong queue at the wrong time. A solution is to stop being a supermarket and start being Fry’s. When you checkout at Fry’s you wait in one long line. In front are thirty cash registers handling one person at a time. When a cashier finishes with a customer, they turn on a light above the register to signal they’re ready to handle the next one. It’s counterintuitive, but one long line can be more efficient than many short lines. For a long Time, twitter.com ran on top of Apache and Mongrel, using mod_proxy_balancer. As in the supermarket, Apache would “push” requests to waiting mongrels for processing, using the ‘bybusyness’ method. Mongrels which had the least number of requests queued received the latest request. Unfortunately, when an incoming request was queued, the balancer process had no way of knowing how far along in each task the mongrels were. Apache would send requests randomly to servers when they were equally loaded, placing fast requests behind slow ones, increasing the latency of each request. In November we started testing Unicorn , an exciting new Ruby server that takes the Mongrel core and adds Fry’s “pull” model. Mobile.twitter.com was our first app to run Unicorn behind Apache, and in January @netik ran the gauntlet to integrate Unicorn into the main Twitter.com infrastructure. During initial tests, we predicted we would maintain CPU usage and only cut request latency 10-15%. Unicorn surprised us by dropping request latency 30% and significantly lowering CPU usage. For automatic recovery and monitoring, we’d relied on monit for mongrel. Monit couldn’t introspect the memory and CPU within the Unicorn process tree, so we developed a new monitoring script, called Stormcloud, to kill Unicorns when they ran out of control. Monit would still monitor the master Unicorn process, but Stormcloud would watch over the Unicorns. With monit, child death during request processing (due to process resource limits or abnormal termination) would cause that request and all requests queued in the mongrel to send 500 “robot” errors until the mongrel had been restarted by monit. Unicorn’s pull model prevents additional errors from firing as the remaining children can still process the incoming workload. Since each Unicorn child is only working on one request at a time, a single error is thrown, allowing us to isolate a failure to an individual request (or sequence of requests). Recovery is fast, as Unicorn immediately restarts children that have died, unlike monit which would wait until the next check cycle. We also took advantage of Unicorn’s zero-downtime deployment by writing a deploy script that would transition Twitter on to new versions of our code base while still accepting requests, ensuring that the new code base was verified and running before switching onto it. It’s a bit like changing the tires on your car while still driving, and it works beautifully. Stay tuned for the implications. — @sandofsky , @netik", "date": "2010-03-30"},
{"website": "Twitter-Engineering", "title": "Timeboxing", "author": ["‎@Stevejenson‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2010/timeboxing.html", "abstract": "When you build a service that talks to external components, you have to worry about the amount of time that a network call will make. The standard technique for protecting yourself is to use timeouts. Most network libraries let you set timeouts to protect yourself but for local computation there are few tools to help you. Timeboxing is the name we’ve given to a technique for setting timeouts on local computation. The name is borrowed from the handy organizational technique . Let’s say you have a method that can take an unbounded amount of time to complete. Normally it’s fast but sometimes it’s horribly slow. If you want to ensure that the work doesn’t take too long, you can box the amount of time it will be allowed to take before it’s aborted. One implementation we use for this in Scala is built on the Futures Java concurrency feature. Futures allow you to compute in a separate thread while using your current thread for whatever else you’re doing. When you need the results of the Future, you call its get() method which blocks until the computation is complete. The trick we use is that you don’t need to do other work in the meantime, you can call get() immediately with a timeout value. Here’s an example (in Scala): import java.util.concurrent.{Executors, Future, TimeUnit}\nval defaultValue = \"Not Found\"\nval executor = Executors.newFixedThreadPool(10)\nval future: Future[String] = executor.submit(new Callable[String]() {\n  def call(): String = {\n    // There's a small chance that this will take longer than you're willing to wait.\n    sometimesSlow()\n  }\n})\ntry {\n  future.get(100L, TimeUnit.MILLISECONDS)\n} catch {\n  case e: TimeoutException => defaultValue\n} If returning a default value isn’t appropriate, you can report an error to the user or handle it in some other custom fashion. It entirely depends on the task. We measure and manage these slow computations with Ostrich , our performance statistics library. Code that frequently times out is a candidate for algorithmic improvement. Even though we’ve described it as a technique for protecting you from runaway local computation, timeboxing can also help with network calls that don’t support timeouts such as DNS resolution . Timeboxing is just one of many techniques we use to keep things humming along here at Twitter. — @stevej", "date": "2010-04-01"},
{"website": "Twitter-Engineering", "title": "Introducing Gizzard, a framework for creating distributed datastores", "author": ["‎@nk‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2010/introducing-gizzard-a-framework-for-creating-distributed-datastores.html", "abstract": "An introduction to sharding Many modern web sites need fast access to an amount of information so large that it cannot be efficiently stored on a single computer. A good way to deal with this problem is to “shard” that information; that is, store it across multiple computers instead of on just one. Sharding strategies often involve two techniques: partitioning and replication. With partitioning , the data is divided into small chunks and stored across many computers. Each of these chunks is small enough that the computer that stores it can efficiently manipulate and query the data. With the other technique of replication , multiple copies of the data are stored across several machines. Since each copy runs on its own machine and can respond to queries, the system can efficiently respond to tons of queries for the same data by adding more copies. Replication also makes the system resilient to failure because if any one copy is broken or corrupt, the system can use another copy for the same task. The problem is: sharding is difficult. Determining smart partitioning schemes for particular kinds of data requires a lot of thought. And even more difficult is ensuring that all of the copies of the data are consistent despite unreliable communication and occasional computer failures. Recently, a lot of open-source distributed databases have emerged to help solve this problem. Unfortunately, as of the time of writing, most of the available open-source projects are either too immature or too limited to deal with the variety of problems that exist on the web. These new databases are hugely promising but for now it is sometimes more practical to build a custom solution. What is a sharding framework? Twitter has built several custom distributed data-stores. Many of these solutions have a lot in common, prompting us to extract the commonalities so that they would be more easily maintainable and reusable. Thus, we have extracted Gizzard, a Scala framework that makes it easy to create custom fault-tolerant, distributed databases. Gizzard is a framework in that it offers a basic template for solving a certain class of problem. This template is not perfect for everyone’s needs but is useful for a wide variety of data storage problems. At a high level, Gizzard is a middleware networking service that manages partitioning data across arbitrary backend datastores (e.g., SQL databases, Lucene, etc.). The partitioning rules are stored in a forwarding table that maps key ranges to partitions. Each partition manages its own replication through a declarative replication tree. Gizzard supports “migrations” (for example, elastically adding machines to the cluster) and gracefully handles failures. The system is made eventually consistent by requiring that all write-operations are idempotent AND commutative and as operations fail (because of, e.g., a network partition) they are retried at a later time. A very simple sample use of Gizzard is Rowz , a distributed key-value store. To get up-and-running with Gizzard quickly, clone Rows and start customizing! But first, let’s examine how Gizzard works in more detail. How does it work? Gizzard is middleware Gizzard operates as a middleware networking service. It sits “in the middle” between clients (typically, web front-ends like PHP and Ruby on Rails applications) and the many partitions and replicas of data. Sitting in the middle, all data querying and manipulation flow through Gizzard. Gizzard instances are stateless so run as many gizzards as are necessary to sustain throughput or manage TCP connection limits. Gizzard, in part because it is runs on the JVM, is quite efficient. One of Twitter’s Gizzard applications (FlockDB, our distributed graph database) can serve 10,000 queries per second per commodity machine. But your mileage may vary. Gizzard supports any datastorage backend Gizzard is designed to replicate data across any network-available data storage service. This could be a relational database, Lucene, Redis, or anything you can imagine. As a general rule, Gizzard requires that all write operations be idempotent AND commutative (see the section on Fault Tolerance and Migrations), so this places some constraints on how you may use the back-end store. In particular, Gizzard does not guarantee that write operations are applied in order. It is therefore imperative that the system is designed to reach a consistent state regardless of the order in which writes are applied. Gizzard handles partitioning through a forwarding table Gizzard handles partitioning (i.e., dividing exclusive ranges of data across many hosts) by mappings ranges of data to particular shards. These mappings are stored in a forwarding table that specifies lower-bound of a numerical range and what shard that data in that range belongs to. To be precise, you provide Gizzard a custom “hashing” function that, given a key for your data (and this key can be application specific), produces a number that belongs to one of the ranges in the forwarding table. These functions are programmable so you can optimize for locality or balance depending on your needs. This tabular approach differs from the “consistent hashing” technique used in many other distributed data-stores. This allows for heterogeneously sized partitions so that you easily manage hotspots , segments of data that are extremely popular. In fact, Gizzard does allows you to implement completely custom forwarding strategies like consistent hashing, but this isn’t the recommended approach. Gizzard handles replication through a replication tree Each shard referenced in the forwarding table can be either a physical shard or a logical shard. A physical shard is a reference to a particular data storage back-end, such as a SQL database. In contrast, A logical shard is just a tree of other shards, where each branch in the tree represents some logical transformation on the data, and each node is a data-storage back-end. These logical transformations at the branches are usually rules about how to propagate read and write operations to the children of that branch. For example, here is a two-level replication tree. Note that this represents just ONE partition (as referenced in the forwarding table): The “Replicate” branches in the figure are simple strategies to repeat write operations to all children and to balance reads across the children according to health and a weighting function. You can create custom branching/logical shards for your particular data storage needs, such as to add additional transaction/coordination primitives or quorum strategies. But Gizzard ships with a few standard strategies of broad utility such as Replicating, Write-Only, Read-Only, and Blocked (allowing neither reads nor writes). The utility of some of the more obscure shard types is discussed in the section on Migrations . The exact nature of the replication topologies can vary per partition. This means you can have a higher replication level for a “hotter” partition and a lower replication level for a “cooler” one. This makes the system highly configurable. For instance, you can specify that the that back-ends mirror one another in a primary-secondary-tertiary-etc. configuration for simplicity. Alternatively, for better fault tolerance (but higher complexity) you can “stripe” partitions across machines so that no machine is a mirror of any other. Gizzard is fault-tolerant Fault-tolerance is one of the biggest concerns of distributed systems. Because such systems involve many computers, there is some likelihood that one (or many) are malfunctioning at any moment. Gizzard is designed to avoid any single points of failure. If a certain replica in a partition has crashed, Gizzard routes requests to the remaining healthy replicas, bearing in mind the weighting function. If all replicas of in a partition are unavailable, Gizzard will be unable to serve read requests to that shard, but all other shards will be unaffected. Writes to an unavailable shard are buffered until the shard again becomes available. In fact, if any number of replicas in a shard are unavailable, Gizzard will try to write to all healthy replicas as quickly as possible and buffer the writes to the unavailable shard, to try again later when the unhealthy shard returns to life. The basic strategy is that all writes are materialized to a durable, transactional journal. Writes are then performed asynchronously (but with manageably low latency) to all replicas in a shard. If a shard is unavailable, the write operation goes into an error queue and is retried later. In order to achieve “eventual consistency”, this “retry later” strategy requires that your write operations are idempotent AND commutative. This is because a retry later strategy can apply operations out-of-order (as, for instance, when newer jobs are applied before older failed jobs are retried). In most cases this is an easy requirement. A demonstration is commutative, idempotent writes is given in the Gizzard demo app, Rowz . Winged migrations It’s sometimes convenient to copy or move data from shards from one computer to another. You might do this to balance load across more or fewer machines, or to deal with hardware failures. It’s interesting to explain some aspect of how migrations work just to illustrate some of the more obscure logical shard types. When migrating from Datastore A to Datastore A' , a Replicating shard is set up between them but a WriteOnly shard is placed in front of Datastore A' . Then data is copied from the old shard to the new shard. The WriteOnly shard ensures that while the new Shard is bootstrapping, no data is read from it (because it has an incomplete picture of the corpus). Because writes will happen out of order (new writes occur before older ones and some writes may happen twice), all writes must be idempotent to ensure data consistency. How to use Gizzard? The source-code to Gizzard is available on GitHub . A sample application that uses Gizzard, called Rowz, is also available . The best way to get started with Gizzard is to clone Rowz and customize. Installation Maven com.twitter gizzard 1.0 Ivy Reporting problems The Github issue tracker is here . Contributors Robey Pointer Nick Kallen Ed Ceaser John Kalucki If you’d like to learn more about the technologies that power Twitter, join us at Chirp , the Twitter Developer Conference, where you will party, hack , chat with and learn from Twitter Engineers and other developers! — @nk , @robey , @asdf , @jkalucki", "date": "2010-04-06"},
{"website": "Twitter-Engineering", "title": "Hadoop at Twitter", "author": ["‎@kevinweil‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2010/hadoop-at-twitter.html", "abstract": "My name is Kevin Weil and I’m a member of the analytics team at Twitter. We’re collectively responsible for Twitter’s data warehousing, for building out an analysis platform that lets us easily and efficiently run large calculations over the Twitter dataset, and ultimately for turning that data into something actionable that helps the business. We’re fortunate to work with great people from teams across the organization for the latter. The former two are largely on our plate though, and as a result we use Hadoop , Pig , and HBase heavily. Today we’re excited to open source some of the core code we rely on for our data analysis. TL;DR We’re releasing a whole bunch of code that we use with Hadoop and Pig specifically around LZO and Protocol Buffers. Use it, fork it, improve upon it. http://github.com/kevinweil/elephant-bird What’s Hadoop? Pig, HBase? Hadoop is a distributed computing framework with two main components: a distributed file system and a map-reduce implementation. It is a top-level Apache project, and as such it is fully open source and has a vibrant community behind it. Imagine you have a cluster of 100 computers. Hadoop’s distributed file system makes it so you can put data “into Hadoop” and pretend that all the hard drives on your machines have coalesced into one gigantic drive. Under the hood, it breaks each file you give it into 64- or 128-MB chunks called blocks and sends them to different machines in the cluster, replicating each block three times along the way. Replication ensures that one or even two of your 100 computers can fail simultaneously, and you’ll never lose data. In fact, Hadoop will even realize that two machines have failed and will begin to re-replicate the data, so your application code never has to care about it! The second main component of Hadoop is its map-reduce framework, which provides a simple way to break analyses over large sets of data into small chunks which can be done in parallel across your 100 machines. You can read more about it here ; it’s quite generic, capable of handling everything from basic analytics through map-tile generation for Google Maps ! Google has a proprietary system which Hadoop itself is modeled after; Hadoop is used at many large companies including Yahoo!, Facebook, and Twitter. We’re happy users of Cloudera’s free Hadoop distribution . Pig is a dataflow language built on top of Hadoop to simplify and speed up common analysis tasks. Instead of writing map-reduce jobs in Java, you write in a higher-level language called PigLatin , and a query compiler turns your statements into an ordered sequence of map-reduce jobs. It enables complex map-reduce job flows to be written in a few easy steps. HBase is a distributed, column-oriented data store built on top of Hadoop and modeled after Google’s BigTable . It allows for structured data storage combined with low-latency data serving. How does Twitter Use Hadoop? Twitter has large data storage and processing requirements, and thus we have worked to implement a set of optimized data storage and workflow solutions within Hadoop. In particular, we store all of our data LZO compressed, because the LZO compression turns out to strike a very good balance between compression ratio and speed for use in Hadoop. Hadoop jobs are generally IO-bound, and typical compression algorithms like gzip or bzip2 are so computationally intensive that jobs quickly become CPU-bound. LZO in contrast was built for speed, so you get 4-5x compression ratio while leaving the CPU available to do real work. For more discussion of LZO, complete with performance comparisons, see this Cloudera blog post we did a while back. We also make heavy use of Google’s protocol buffers for efficient, extensible, backward-compatible data storage. Hadoop does not mandate any particular format on disk, and common formats like CSV are space-inefficient: an integer like 2930533523 takes 10 bytes in ASCII. untyped: is 2930533523 an int, a long, or a string? not robust to versioning changes: adding a new field, or removing an old one, requires you to change your code not hierarchical: you cannot store any nested structure Other solutions like JSON fail fewer of these tests, but protocol buffers retain one key advantage: code generation. You write a quick description of your data structure, and the protobuf library will generate code for working with that data structure in the language of your choice. Because Google designed protobufs for data storage, the serialized format is efficient; integers, for example, are variable-length or zigzag encoded . So… The code we are releasing makes it easy to work with LZO-compressed data of all sorts — JSON, CSV, TSV, line-oriented, and especially protocol buffer-encoded — in Hadoop, Pig, and HBase. It also includes a framework for automatically generating all of this code for protobufs given the protobuf IDL. That is, not only will protoc generate the standard protobuf code for you, it will now generate Hadoop and Pig layers on top of that which you can immediately plug in to start analyzing data written with these formats. Having code automatically generated from a simple data structure definition has helped us move very quickly and make fewer mistakes in our analysis infrastructure at Twitter. You can even hook in and add your own code generators from within the framework. Please do, and submit back! Much more documentation is available at the github page, http://www.github.com/kevinweil/elephant-bird . Thanks to Dmitriy Ryaboy , Chuang Liu , and Florian Leibert for help developing this framework. We welcome contributions, forks, and pull requests. If working on stuff like this every day sounds interesting, we’re hiring ! — @kevinweil", "date": "2010-04-08"},
{"website": "Twitter-Engineering", "title": "Memcached SPOF Mystery", "author": ["‎@wanliyang‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2010/memcached-spof-mystery.html", "abstract": "At Twitter, we use memcached to speed up page loads and alleviate database load. We have many memcached hosts. To make our system robust, our memcached clients use consistent hashing and enable the auto_eject_hosts option. With this many hosts and this kind of configuration, one would assume that it won’t be noticeable if one memcached host goes down, right? Unfortunately, our system will have elevated robots whenever a memcached host dies or is taken out of the system. The system does not recover on its own unless the memcached host is brought back. Essentially, every memcached host is a single point of failure. This is what we observed when a memcached host crashed recently. Web 502/503s spiked and recovered, and web/api 500 errors occur at a sustained elevated rate. Why is this happening? At first, we thought the elevated robots were caused by remapping the keys on the dead host. After all, reloading from databases can be expensive. When other memcached hosts have more data to read from databases than they can handle, they may throw exceptions. But that should only happen if some memcached hosts are near their capacity limit. Whereas the elevated robots can happen even during off-peak hours. There must be something else. A closer look at the source of those exceptions surprised us. It turns out those exceptions are not from the requests sent to the other healthy memcached hosts but from the requests sent to the dead host! Why do the clients keep sending requests to the dead host? This is related to the “auto_eject_hosts” option. The purpose of this option is to let the client temporarily eject dead hosts from the pool. A host is marked as dead if the client has a certain number of consecutive failures with the host. The dead server will be retried after retry timeout. Due to general unpredictable stuff such as network flux, hardware failures, timeouts due to other jobs in the boxes, requests sent to healthy memcached hosts can fail sporadically. The retry timeout is thus set to a very low value to avoid remapping a large number of keys unnecessarily. When a memcached host is really dead, however, this frequent retry is undesirable. The client has to establish connection to the dead host again, and it usually gets one of the following exceptions: Memcached::ATimeoutOccurred, Memcached::UnknownReadFailure, Memcached::SystemError (“Connection refused”), or Memcached::ServerIsMarkedDead. Unfortunately, a client does not share the “the dead host is still dead” information with other clients, so all clients will retry the dead host and get those exceptions at very high frequency. The problem is not difficult to fix once we get better understanding of the problem. Simply retrying a memcached request once or twice on those exceptions usually works. Here is the list of all the memcached runtime exceptions. Ideally, memcached client should have some nice build-in mechanisms (e.g. exponential backoff) to retry some of the exceptions, and optionally log information about what happened. The memcached client shouldn’t transparently swallow all exceptions, which would cause users to lose all visibilities into what’s going on. After we deployed the fix, we don’t see elevated robots any more when a memcached host dies. The memcached SPOF mystery solved! P.S. Two option parameters “exception_retry_limit” and “exceptions_to_retry” have been added to memcached . — @wanliyang", "date": "2010-04-20"},
{"website": "Twitter-Engineering", "title": "Introducing FlockDB", "author": ["‎@robey‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2010/introducing-flockdb.html", "abstract": "Twitter stores many graphs of relationships between people: who you’re following, who’s following you, who you receive phone notifications from, and so on. Some of the features of these graphs have been challenging to store in scalable ways as we’ve grown. For example, instead of requiring each friendship to be requested and confirmed, you can build one-way relationships by just following other people. There’s also no limit to how many people are allowed to follow you, so some people have millions of followers (like @aplusk ), while others have only a few. To deliver a tweet, we need to be able to look up someone’s followers and page through them rapidly. But we also need to handle heavy write traffic, as followers are added or removed, or spammers are caught and put on ice. And for some operations, like delivering a @mention , we need to do set arithmetic like “who’s following both of these users?” These features are difficult to implement in a traditional relational database. A valiant effort We went through several storage layers in the early days, including abusive use of relational tables and key-value storage of denormalized lists. They were either good at handling write operations or good at paging through giant result sets, but never good at both. A little over a year ago, we could see that we needed to try something new. Our goals were: Write the simplest possible thing that could work. Use off-the-shelf MySQL as the storage engine, because we understand its behavior — in normal use as well as under extreme load and unusual failure conditions. Give it enough memory to keep everything in cache. Allow for horizontal partitioning so we can add more database hardware as the corpus grows. Allow write operations to arrive out of order or be processed more than once. (Allow failures to result in redundant work rather than lost work.) FlockDB was the result. We finished migrating to it about 9 months ago and never looked back. A valiant-er effort FlockDB is a database that stores graph data, but it isn’t a database optimized for graph-traversal operations. Instead, it’s optimized for very large adjacency lists, fast reads and writes, and page-able set arithmetic queries. It stores graphs as sets of edges between nodes identified by 64-bit integers. For a social graph, these node IDs will be user IDs, but in a graph storing “favorite” tweets, the destination may be a tweet ID. Each edge is also marked with a 64-bit position, used for sorting. (Twitter puts a timestamp here for the “following” graph, so that your follower list is displayed latest-first.) When an edge is “deleted”, the row isn’t actually deleted from MySQL; it’s just marked as being in the deleted state, which has the effect of moving the primary key (a compound key of the source ID, state, and position). Similarly, users who delete their account can have their edges put into an archived state, allowing them to be restored later (but only for a limited time, according to our terms of service). We keep only a compound primary key and a secondary index for each row, and answer all queries from a single index. This kind of schema optimization allows MySQL to shine and gives us predictable performance. A complex query like “What’s the intersection of people I follow and people who are following President Obama?” can be answered quickly by decomposing it into single-user queries (“Who is following President Obama?”). Data is partitioned by node, so these queries can each be answered by a single partition, using an indexed range query. Similarly, paging through long result sets is done by using the position field as a cursor, rather than using LIMIT/OFFSET , so any page of results for a query is indexed and is equally fast. Write operations are idempotent and commutative , based on the time they enter the system. We can process operations out of order and end up with the same result, so we can paper over temporary network and hardware failures, or even replay lost data from minutes or hours ago. This was especially helpful during the initial roll-out. Commutative writes also simplify the process of bringing up new partitions. A new partition can receive write traffic immediately, and receive a dump of data from the old partitions slowly in the background. Once the dump is over, the partition is immediately “live” and ready to receive reads. The app servers (affectionately called “flapps”) are written in Scala, are stateless, and are horizontally scalable. We can add more as query load increases, independent of the databases. They expose a very small thrift API to clients, though we’ve written a Ruby client with a much richer interface. We use the Gizzard library to handle the partitioning layer. A forwarding layer maps ranges of source IDs to physical databases, and replication is handled by building a tree of such tables under the same forwarding address. Write operations are acknowledged after being journalled locally, so that disruptions in database availability or performance are decoupled from website response times. Each edge is actually stored twice: once in the “forward” direction (indexed and partitioned by the source ID) and once in the “backward” direction (indexed and partitioned by the destination ID). That way a query like “Who follows me?” is just as efficient as “Who do I follow?”, and the answer to each query can be found entirely on a single partition. The end result is a cluster of commodity servers that we can expand as needed. Over the winter, we added 50% database capacity without anyone noticing. We currently store over 13 billion edges and sustain peak traffic of 20k writes/second and 100k reads/second . Lessons learned Some helpful patterns fell out of our experience, even though they weren’t goals originally: Use aggressive timeouts to cut off the long tail. You can’t ever shake out all the unfairness in the system, so some requests will take an unreasonably long time to finish — way over the 99.9th percentile. If there are multiple stateless app servers, you can just cut a client loose when it has passed a “reasonable” amount of time, and let it try its luck with a different app server. Make every case an error case. Or, to put it another way, use the same code path for errors as you use in normal operation. Don’t create rarely-tested modules that only kick in during emergencies, when you’re least likely to feel like trying new things. We queue all write operations locally (using Kestrel as a library), and any that fail are thrown into a separate error queue. This error queue is periodically flushed back into the write queue, so that retries use the same code path as the initial attempt. Do nothing automatically at first. Provide lots of gauges and levers, and automate with scripts once patterns emerge. FlockDB measures the latency distribution of each query type across each service (MySQL, Kestrel, Thrift) so we can tune timeouts, and reports counts of each operation so we can see when a client library suddenly doubles its query load (or we need to add more hardware). Write operations that cycle through the error queue too many times are dumped into a log for manual inspection. If it turns out to be a bug, we can fix it, and re-inject the job. If it’s a client error, we have a good bug report. Check it out The source is in github: http://github.com/twitter/flockdb In particular, check out the demo to get a feel for the kind of data that can be stored and what you can do with it: http://github.com/twitter/flockdb/blob/master/doc/demo.markdown Talk to us on IRC, in #twinfra (irc.freenode.net), or join the mailing list: http://groups.google.com/group/flockdb — @robey , @nk , @asdf , @jkalucki", "date": "2010-05-03"},
{"website": "Twitter-Engineering", "title": "Tracing traffic through our stack", "author": ["‎@raffi‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2010/tracing-traffic-through-our-stack.html", "abstract": "The @twitterapi has had two authentication mechanisms for quite a while now: HTTP Basic Authentication and OAuth . Basic authentication has gotten us so far! You can even use curl from the command line to interact with our API and simply pass a username and a password as a -u command line parameter when calling statuses/update to tweet, for example. However, as times have changed, so have our requirements around authentication — developers will need to take action. Basic Auth support is going away on June 30, 2010 . OAuth has always been part of Twitter’s blood , and soon, we’re going to be using it exclusively. OAuth has many benefits for end users (e.g. protection of their passwords and fine grained control over applications), but what does it mean for Twitter on the engineering front? Quite a lot. Our authentication stack, right now, for basic auth, looks as so: decode the Authorization header that comes in via the HTTP request; check any rate limits that apply for the user or the IP address that request came from (a memcache hit); see if the authorization header is in memcache - and if it is, use it to find the user in cache and verify that the password is correct. If neither the header is in cache, nor the user is in cache, nor the password is correct (in case the user has changed his or her password), then keep going; pull the user out of storage; verify the user hasn’t been locked out of the system; and verify the user’s credentials. Our stack then also logs a lot of information to scribe about that user and login to help us counter harmful activities (whether malicious or simply buggy) — but, the one thing that we don’t have any visibility into, when using basic authentication, is what application is doing all this. To verify an OAuth-signed request, we go through a lot more intensive (both computationally and on our storage systems): decode the Authorization header; validate that the oauth_nonce and the oauth_timestamp pair that were passed in are not present in memcache — if so, then this may be a relay attack, and deny the user access; use the oauth_consumer_key and the oauth_token from the header, look up both the Twitter application and the user’s access token object from cache and fallback to the database if necessary. If, for some reason, neither can be retrieved, then something has gone wrong and proactively deny access; with the application and the access token, verify the oauth_signature. If it doesn’t match, then reject the request; and check any rate limits that may apply for the user at this stage Of course, for all the reject paths up top, we log information — that’s invaluable data for us to turn over to our Trust & Safety team. If the user manages to authenticate, however, then we too have a wealth of information! We can, at this point, for every authenticated call, tie an user and an application to a specific action on our platform. For us, and the entire Twitter ecosystem, its really important to be able to identify, and get visibility into, our users’ traffic. We want to be able to help developers if their software is malfunctioning, and we want to be able to make educated guesses as to whether traffic is malicious or not. And, if everything is functioning normally, then we can use this data to help us provision and plan for growth better and deliver better reliability. But, if all applications are simply using usernames and passwords as their identifiers, then we have no way to distinguish who is sending what traffic on behalf of which users. Phase one of our plan is to remove basic authentication for calls that require authentication — those calls will migrate to a three-legged OAuth scheme. After that, we’ll start migrating all calls to at least begin to use a two-legged OAuth scheme. We also have OAuth 2 in the works. Start firing up dev.twitter.com and creating Twitter applications! As always, the @twitterapi team is here to help out. Just make sure to join the Twitter Development Talk group to ask for questions, follow @twitterapi for announcements, and skim through our docs on dev.twitter.com to help you through this transition. — @raffi", "date": "2010-05-12"},
{"website": "Twitter-Engineering", "title": "Announcing Snowflake", "author": ["‎@rk‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2010/announcing-snowflake.html", "abstract": "A while back we announced on our API developers list that we would change the way we generate unique ID numbers for tweets. While we’re not quite ready to make this change, we’ve been hard at work on Snowflake which is the internal service to generate these ids. To give everyone a chance to familiarize themselves with the techniques we’re employing and how it’ll affect anyone building on top of the Twitter platform we are open sourcing the Snowflake code base today. Before I go further, let me provide some context. The Problem We currently use MySQL to store most of our online data. In the beginning, the data was in one small database instance which in turn became one large database instance and eventually many large database clusters. For various reasons, the details of which merit a whole blog post, we’re working to replace many of these systems with the Cassandra distributed database or horizontally sharded MySQL (using gizzard ). Unlike MySQL, Cassandra has no built-in way of generating unique ids – nor should it, since at the scale where Cassandra becomes interesting, it would be difficult to provide a one-size-fits-all solution for ids. Same goes for sharded MySQL. Our requirements for this system were pretty simple, yet demanding: We needed something that could generate tens of thousands of ids per second in a highly available manner. This naturally led us to choose an uncoordinated approach. These ids need to be roughly sortable , meaning that if tweets A and B are posted around the same time, they should have ids in close proximity to one another since this is how we and most Twitter clients sort tweets.[1] Additionally, these numbers have to fit into 64 bits. We’ve been through the painful process of growing the number of bits used to store tweet ids before . It’s unsurprisingly hard to do when you have over 100,000 different codebases involved . Options We considered a number of approaches: MySQL-based ticket servers ( like flickr uses ), but those didn’t give us the ordering guarantees we needed without building some sort of re-syncing routine. We also considered various UUIDs, but all the schemes we could find required 128 bits. After that we looked at Zookeeper sequential nodes, but were unable to get the performance characteristics we needed and we feared that the coordinated approach would lower our availability for no real payoff. Solution To generate the roughly-sorted 64 bit ids in an uncoordinated manner, we settled on a composition of: timestamp, worker number and sequence number. Sequence numbers are per-thread and worker numbers are chosen at startup via zookeeper (though that’s overridable via a config file). We encourage you to peruse and play with the code: you’ll find it on github . Please remember, however, that it is currently alpha-quality software that we aren’t yet running in production and is very likely to change. Feedback If you find bugs, please report them on github. If you are having trouble understanding something, come ask in the #twinfra IRC channel on freenode. If you find anything that you think may be a security problem, please email security@twitter.com (and cc myself: ryan@twitter.com). [1] In mathematical terms, although the tweets will no longer be sorted, they will be k-sorted . We’re aiming to keep our k below 1 second, meaning that tweets posted within a second of one another will be within a second of one another in the id space too.", "date": "2010-06-01"},
{"website": "Twitter-Engineering", "title": "A Perfect Storm.....of Whales", "author": ["‎@jeanpaul‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2010/a-perfect-stormof-whales.html", "abstract": "Since Saturday, Twitter has experienced several incidences of poor site performance and a high number of errors due to one of our internal sub-networks being over-capacity. We’re working hard to address the core issues causing these problems—more on that below—but in the interests of the open exchange of information, wanted to pull back the curtain and give you deeper insight into what happened and how we’re working to address this week’s poor site performance. What happened? In brief, we made three mistakes: * We put two critical, fast-growing, high-bandwith components on the same segment of our internal network. * Our internal network wasn’t appropriately being monitored. * Our internal network was temporarily misconfigured. What we’re doing to fix it * We’ve doubled the capacity of our internal network. * We’re improving the monitoring of our internal network. * We’re rebalancing the traffic on our internal network to redistribute the load. Onward For much of 2009, Twitter’s biggest challenge was coping with our unprecedented growth (a challenge we happily still face). Our engineering team spent much of 2009 redesigning Twitter’s runtime for scale, and our operations team worked to improve our monitoring and capacity planning so we can quickly identify and find solutions for problems as they occur. Those efforts were well spent; every day, more people use Twitter, yet we serve fewer whales. But as this week’s issues show, there is always room for improvement: we must apply the same diligence & care in the design, planning, and monitoring of our internal network. Based on our experiences this week, we’re working with our hosting partner to deliver improvements on all three fronts. By bringing the monitoring of our internal network in line with the rest of the systems at Twitter, we’ll be able to grow our capacity well ahead of user growth. Furthermore, by doubling our internal network capacity and rebalancing load across the internal network, we’re better prepared to serve today’s tweets and beyond. As more people turn to Twitter to see what’s happening in the world (or in the World Cup), you may still see the whale when there are unprecedented spikes in traffic. For instance, during the World Cup tournament—and particularly during big, closely-watched matches (such as tomorrow’s match between England and the U.S.A.)—we anticipate a significant surge in activity on Twitter. While we are making every effort to prepare for that surge, the whale may surface. Finally, as we think about new ways to communicate with you about Twitter’s performance and availability status, continue reading http://status.twitter.com , http://dev.twitter.com/status , and following @twitterapi for the latest updates. Thanks for your continued patience and enthusiasm. — @jeanpaul", "date": "2010-06-11"},
{"website": "Twitter-Engineering", "title": "Cassandra at Twitter Today", "author": ["‎@rk‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2010/cassandra-at-twitter-today.html", "abstract": "In the past year, we’ve been working with the Apache Cassandra open source distributed database. Much of our work there has been out in the open, since we’re big proponents of open source software . Unfortunately, lately we’ve been less involved in the community because of more pressing concerns and have created some misunderstandings. We’re using Cassandra in production for a bunch of things at Twitter. A few examples: Our geo team uses it to store and query their database of places of interest. The research team uses it to store the results of data mining done over our entire user base. Those results then feed into things like @toptweets and local trends. Our analytics, operations and infrastructure teams are working on a system that uses cassandra for large-scale real time analytics for use both internally and externally. For now, we’re not working on using Cassandra as a store for Tweets. This is a change in strategy. Instead we’re going to continue to maintain our existing Mysql-based storage. We believe that this isn’t the time to make large scale migration to a new technology. We will focus our Cassandra work on new projects that we wouldn’t be able to ship without a large-scale data store. We’re investing in Cassandra every day. It’ll be with us for a long time and our usage of it will only grow.", "date": "2010-07-10"},
{"website": "Twitter-Engineering", "title": "Murder: Fast datacenter code deploys using BitTorrent", "author": ["‎@lg‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2010/murder-fast-datacenter-code-deploys-using-bittorrent.html", "abstract": "Twitter has thousands of servers. What makes having boatloads of servers particularly annoying though is that we need to quickly get multiple iterations of code and binaries onto all of them on a regular basis. We used to have a git-based deploy system where we’d just instruct our front-ends to download the latest code from our main git machine and serve that. Unfortunately, once we got past a few hundred servers, things got ugly. We recognized that this problem was not unlike many of the scaling problems we’ve had and dealt with in the past though—we were suffering the symptoms of a centralized system. Slow deploys By sitting beside a particularly vocal Release Engineer, I received first-hand experience of the frustration caused by slow deploys. We needed a way to dramatically speed things up. I thought of some quick hacks to get this fixed: maybe replicate the git repo or maybe shard it so everyone isn’t hitting the same thing at once. Most of these quasi-centralized solutions will still require re-replicating or re-sharding again in the near future though (especially at our growth). It was time for something completely different, something decentralized, something more like… BitTorrent …running inside of our datacenter to quickly copy files around. Using the file-sharing protocol, we launched a side-project called Murder and after a few days (and especially nights) of nervous full-site tinkering, it turned a 40 minute deploy process into one that lasted just 12 seconds! To the rescue Murder (which by the way is the name for a flock of crows ) is a combination of scripts written in Python and Ruby to easily deploy large binaries throughout your company’s datacenter(s). It takes advantage of the fact that the environment in a datacenter is somewhat different from regular internet connections: low-latency access to servers, high bandwidth, no NAT/Firewall issues, no ISP traffic shaping, only trusted peers, etc. This let us come up with a list of optimizations on top of BitTornado to make BitTorrent not only reasonable, but also effective on our internal network. Since at the time we used Capistrano for signaling our servers to perform tasks, Murder also includes a Capistrano deploy strategy to make it easy for existing users of Capistrano to convert their file distribution to be decentralized. The final component is the work Matt Freels (@ mf ) did in bundling everything into an easy to install ruby gem. This further helped get Murder to be usable for more deploy tasks at Twitter. Where to get it Murder, like many internal Twitter systems, is fully open sourced for your contributions and usage at: http://github.com/lg/murder . I recently did a talk (see video below) at CUSEC 2010 in Montreal, Canada which explains many of the internals. If you have questions for how to use it, feel free to contact me or Matt on Twitter. We’re always looking for talented Systems and Infrastructure engineers to help grow and scale our website. Murder is one of the many projects that really highlights how thinking about decentralized and distributed systems can make huge improvements. If Murder or these kinds of engineering challenges interest you, please visit our jobs page and apply. We’ve got loads of similar projects waiting for staffing. Thanks! Twitter - Murder Bittorrent Deploy System from Larry Gadea on Vimeo .", "date": "2010-07-15"},
{"website": "Twitter-Engineering", "title": "Room to grow: a Twitter data center", "author": ["‎@jeanpaul‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2010/room-to-grow-a-twitter-data-center.html", "abstract": "Later this year, Twitter is moving our technical operations infrastructure into a new, custom-built data center in the Salt Lake City area. We’re excited about the move for several reasons. First, Twitter’s user base has continued to grow steadily in 2010, with over 300,000 people a day signing up for new accounts on an average day. Keeping pace with these users and their Twitter activity presents some unique and complex engineering challenges (as John Adams, our lead engineer for application services, noted in a speech last month at the O’Reilly Velocity conference). Having dedicated data centers will give us more capacity to accommodate this growth in users and activity on Twitter. Second, Twitter will have full control over network and systems configuration, with a much larger footprint in a building designed specifically around our unique power and cooling needs. Twitter will be able to define and manage to a finer grained SLA on the service as we are managing and monitoring at all layers. The data center will house a mixed-vendor environment for servers running open source OS and applications. Importantly, having our own data center will give us the flexibility to more quickly make adjustments as our infrastructure needs change. Finally, Twitter’s custom data center is built for high availability and redundancy in our network and systems infrastructure. This first Twitter managed data center is being designed with a multi-homed network solution for greater reliability and capacity. We will continue to work with NTT America to operate our current footprint, and plan to bring additional Twitter managed data centers online over the next 24 months.", "date": "2010-07-21"},
{"website": "Twitter-Engineering", "title": "Twitter & Performance: An update", "author": ["‎@jeanpaul‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2010/twitter-performance-an-update.html", "abstract": "On Monday, a fault in the database that stores Twitter user records caused problems on both Twitter.com and our API. The short, non-technical explanation is that a mistake led to some problems that we were able to fix without losing any data. While we were able to resolve these issues by Tuesday morning, we want to talk about what happened and use this an opportunity to discuss the recent progress we’ve made in improving Twitter’s performance and availability. We recently covered these topics in a pair of June posts here and on our company blog ). Riding a rocket Making sure Twitter is a stable platform and a reliable service is our number one priority. The bulk of our engineering efforts are currently focused on this effort, and we have moved resources from other important projects to focus on the issue. As we said last month, keeping pace with record growth in Twitter’s user base and activity presents some unique and complex engineering challenges. We frequently compare the tasks of scaling, maintaining, and tweaking Twitter to building a rocket in mid-flight. During the World Cup, Twitter set records for usage . While the event was happening, our operations and infrastructure engineers worked to improve the performance and stability of the service. We have made more than 50 optimizations and improvements to the platform, including: Doubling the capacity of our internal network; Improving the monitoring of our internal network; Rebalancing the traffic on our internal network to redistribute the load; Doubling the throughput to the database that stores tweets; Making a number of improvements to the way we use memcache, improving the speed of Twitter while reducing internal network traffic; and, Improving page caching of the front and profile pages, reducing page load time by 80 percent for some of our most popular pages. So what happened Monday? While we’re continuously improving the performance, stability and scalability of our infrastructure and core services, there are still times when we run into problems unrelated to Twitter’s capacity. That’s what happened this week. On Monday, our users database, where we store millions of user records, got hung up running a long-running query; as a result, most of the table became locked. The locked users table manifested itself in many ways: users were unable to sign-up, sign in, update their profile or background images, and responses from the API were malformed, rendering the response unusable to many of the API clients. In the end, this affected most of the Twitter ecosystem: our mobile, desktop, and web-based clients, the Twitter support and help system, and Twitter.com. To remedy the locked table, we force-restarted the database server in recovery mode, a process that took more than 12 hours (the database covers records for more than 125 million users — that’s a lot of records). During the recovery, the users table and related tables remained unavailable. Unfortunately, even after the recovery process completed, the table remained in an unusable state. Finally, yesterday morning we replaced the partially-locked user db with a copy that was fully available (in the parlance of database admins everywhere, we promoted a slave to master), fixing the database and all of the related issues. We have taken steps to ensure we can more quickly detect and respond to similar issues in the future. For example, we are prepared to more quickly promote a slave db to a master db, and we put additional monitoring in place to catch errant queries like the one that caused Monday’s incidents. Long-term solutions As we said last month, we are working on long-term solutions to make Twitter more reliable (news that we are moving into our own data center this fall, which we announced this afternoon , is just one example). This will take time, and while there has been short-term pain, our capacity has improved over the past month. Finally, despite the rapid growth of our company, we’re still a relatively small crew maintaining a comparatively large (rocket) ship. We’re actively looking for engineering talent, with more than 20 openings currently. If you’re interested in learning more about the problems we’re solving or “ joining the flock ,” check out our jobs page .", "date": "2010-07-21"},
{"website": "Twitter-Engineering", "title": "My Awesome Summer Internship at Twitter", "author": ["‎@jeanpaul‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2010/my-awesome-summer-internship-at-twitter.html", "abstract": "On my second day at Twitter, I was writing documentation for the systems I was going to work on (to understand them better), and I realized that there was a method in the service’s API that should be exposed but wasn’t. I pointed this out to my engineering mentor, Steve Jenson ( @stevej ). I expected him to ignore me, or promise to fix it later. Instead, he said, “Oh, you’re right. What are you waiting for? Go ahead and fix it.” After 4 hours, about 8 lines of code, and a code review with Steve, I committed my first code at Twitter. My name is Siddarth Chandrasekaran ( @sidd ). I’m a rising junior at Harvard studying Computer Science and Philosophy, and I just spent the last 10 weeks at Twitter as an intern with the Infrastructure team. When I started, I had very little real-world experience — I’d never coded professionally before – so, I was really excited and really nervous. I spent the first couple of weeks understanding the existing code base (and being very excited that I sat literally three cubicles away from Jason Goldman! ( @goldman )). I remember my first “teatime” (Twitter’s weekly Friday afternoon company all-hands), when Evan Williams ( @ev ) broke into song in the middle of his presentation, dramatically launching the karaoke session that followed teatime. Over the next few weeks, I worked on a threshold monitoring system: a Scala service that facilitates defining basic “rules” (thresholds for various metrics), and monitors these values using a timeseries database. The goal was to allow engineers to be able to easily define and monitor their own thresholds. I was extremely lucky to have the opportunity to build such a critical piece of infrastructure, with abundant guidance from Ian Ownbey ( @iano ). Writing an entire service from scratch was scary, but as a result, I also learned a lot more than I expected. It was perfect: I was working independently, but could turn to my co-workers for help anytime. There are several things that I’ve loved during my time at Twitter: On my third day at work, I got to see the President of Russia. A few weeks later, Kanye West ( @kanyewest ) “dropped by” for lunch. I was in the amazing “Class of Twitter HQ” recruiting video. Every day at Twitter has given me something to be very excited about: the snack bars, the delicious lunches, teatime, random rockband sessions, the opportunity to work on some really cool stuff with very smart people, and most importantly, being part of a company that is caring and honest. My co-workers have artful, creative, daring, and ingenious approaches to the hard engineering problems that Twitter faces, and the company supports them by providing a culture of trust and openness. As an intern, it has been an overwhelmingly positive experience to be part of such a culture. Needless to say, I will miss Twitter very dearly, and I’m very thankful for this opportunity. What are you waiting for? Join The Flock! ( @jointheflock ) —Siddarth ( @sidd )", "date": "2010-08-27"},
{"website": "Twitter-Engineering", "title": "The Tech Behind the New Twitter.com", "author": ["‎@bs‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2010/the-tech-behind-the-new-twittercom.html", "abstract": "The Twitter.com redesign presented an opportunity to make bold changes to the underlying technology of the website. With this in mind, we began implementing a new architecture almost entirely in JavaScript. We put special emphasis on ease of development, extensibility, and performance. Building the application on the client forced us to come up with unique solutions to bring our product to life, a few of which we’d like to highlight in this overview. API Client One of the most important architectural changes is that Twitter.com is now a client of our own API. It fetches data from the same endpoints that the mobile site, our apps for iPhone, iPad, Android, and every third-party application use. This shift allowed us to allocate more resources to the API team, generating over 40 patches. In the initial page load and every call from the client, all data is now fetched from a highly optimized JSON fragment cache. The Javascript API We built a JavaScript library to access Twitter’s REST API for @anywhere which provided a good starting point for development on this project. The JavaScript API provides API fetching and smart client-side caching, both in-memory and using localStorage, allowing us to minimize the number of network requests made while using Twitter.com. For instance, timeline fetches include associated user data for each Tweet. The resulting user objects are proactively cached, so viewing a profile does not require unnecessary fetches of user data. Another feature of the JavaScript API is that it provides event notifications before and after each API call. This allows components to register interest and respond immediately with appropriate changes to the UI, while letting independent components remain decoupled, even when relying on access to the same data. Page Management One of the goals with this project was to make page navigation easier and faster. Building on the web’s traditional analogy of interlinked documents, our application uses a page routing system that maintains a strong relationship between a URL and its content. This allows us to provide a rich web application that behaves like a traditional web site. Doing so demanded that we develop a rich routing model on the client. To do so we developed a routing system to switch between stateful pages, driven by the URL hash. As the user navigates, the application caches the visited pages in memory. Although the information on those pages can quickly become stale, we’ve alleviated much of this complexity by making pages subscribe to events from the JavaScript API and keep themselves in sync with the overall application state. The Rendering Stack In order to support crawlers and users without JavaScript, we needed a rendering system that runs on both server and client. To meet this need, we’ve built our rendering stack around Mustache , and developed a view object system that generates HTML fragments from API objects. We’ve also extended Mustache to support internationalized string substitution. Much attention was given to optimizing performance in the DOM. For example, we’ve implemented event delegation across the board, which has enabled a low memory profile without worrying about event attachment. Most of our UI is made out of reusable components, so we’ve centralized event handling to a few key root nodes. We also minimize repaints by building full HTML structures before they are inserted and attach relevant data in the HTML rendering step, rather than through DOM manipulation. Inline Media One important product feature was embedding third-party content directly on the website whenever tweet links to one of our content partners. For many of these partners, such as Kiva and Vimeo , we rely on the oEmbed standard, making a simple JSON-P request to the content provider’s domain and embeds content found in the response. For other media partners, like TwitPic and YouTube , we rely on known embed resources that can be predicted from the URL, which reduces network requests and results in a speedier experience. Open Source Twitter has always embraced open-source technology, and the new web client continues in this tradition. We used jQuery , Mustache , LABjs , Modernizr , and numerous other open-source scripts and jQuery plugins. We owe a debt of gratitude to the authors of these libraries and many others in the JavaScript community for their awesome efforts in writing open-source JavaScript. We hope that, through continuing innovations in front-end development here at Twitter, we’ll be able to give back to the open-source community with some of our own technology. Conclusions With #NewTwitter , we’ve officially adopted JavaScript as a core technology in our organization. This project prompted our first internal JavaScript summit, which represents an ongoing effort to exchange knowledge, refine our craft and discover new ways of developing for the web. We’re very excited about the doors this architectural shift will open for us as we continue to invest more deeply in rich client experiences. If you’re passionate about JavaScript, application architecture, and Twitter, now is a very exciting time to @JoinTheFlock ! This application was engineered in four months by seven core engineers: Marcus Phillips , Britt Selvitelle , Patrick Ewing , Ben Cherry , Dustin Diaz , Russ d’Sa , and Sarah Brown , with numerous contributions from around the company.", "date": "2010-09-20"},
{"website": "Twitter-Engineering", "title": "Tool Legit", "author": ["‎@stirman‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2010/tool-legit.html", "abstract": "Hi, I’m @stirman , and I’m a tool. Well, I build tools, along with @jacobthornton , @gbuyitjames and @sm , the Internal Tools team here at Twitter. Build or buy? To build or not to build internal tools is usually a debated topic, especially amongst startups. Investing in internal projects has to be weighed against investing in external-facing features for your product, although at some point the former investment shows greater external returns than the latter. Twitter has made it a priority to invest in internal tools since the early days, and with the growth of the product and the company, our tools have become a necessity. I often hear from friends in the industry about internal tools being a night and weekend additional project for engineers that are already backlogged with “real” work. We have decided to make building tools our “real” work. This decision means we have time to build solid applications, spend the necessary time to make them look great and ensure that they work well. Noble goals Our team’s mission is to increase productivity and transparency throughout the company. We increase productivity by streamlining processes and automating tasks. We increase transparency by building tools and frameworks that allow employees to discover, and be notified of, relevant information in real time. Many companies use the term “transparency” when discussing their company culture, but very few put the right pieces in place to ensure that a transparent environment can be established without exposing too much information. Twitter invests heavily in my team so that we can build the infrastructure to ensure a healthy balance. Example apps We have built tools that track and manage milestones for individual teams, manage code change requests, provide an easy A/B testing framework for twitter.com, create internal short links, get approval for offer letters for new candidates, automate git repository creation, help conduct fun performance reviews and many more. We release a new tool about once every other week. We release a first version as early as possible, and then iterate quickly after observing usage and gathering feedback. Also, with the help of @mdo , we have put together a internal blueprint site that not only contains a style guide for new apps, but also hosts shared stylesheets, javascript libraries and code samples, like our internal user authentication system, to make spinning up a new tool as simple as possible. We put a lot of effort into ensuring our tools are easy to use and making them look great. We have fun with it. Here’s a screenshot of a recent app that tracks who’s on call for various response roles at any given time. Time to play We also have fun learning new technologies. Here’s a screenshot of a real-time Space Invaders Twitter sentiment analysis visualization that is part of a status board displayed on flat screens around the office. @jacobthornton wanted to learn more about node.js for some upcoming projects and he built “Space Tweets” to do just that! If you’re interested in the code, get it on github . Giving back While we’re talking about open source, we would like to mention how much our team values frameworks like Ruby on Rails , MooTools and their respective communities, all of which are very important to our internal development efforts and in which you’ll find us actively participating by submitting patches, debating issues, etc. We are proactively working towards open sourcing some of our own tools in the near future, so keep an eye on this blog. Join us Does this stuff interest you? Are you a tool? Hello? Is this thing on? Is anyone listening? (If you are still here, you passed the test! Apply here to join our team or hit me up at @stirman !)", "date": "2010-09-30"},
{"website": "Twitter-Engineering", "title": "Twitter's New Search Architecture", "author": ["‎@michibusch‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2010/twitters-new-search-architecture.html", "abstract": "If we have done a good job then most of you shouldn’t have noticed that we launched a new backend for search on twitter.com during the last few weeks! One of our main goals, but also biggest challenges, was a smooth switch from the old architecture to the new one, without any downtime or inconsistencies in search results. Read on to find out what we changed and why. Twitter’s real-time search engine was, until very recently, based on the technology that Summize originally developed. This is quite amazing, considering the explosive growth that Twitter has experienced since the Summize acquisition. However, scaling the old MySQL-based system had become increasingly challenging. The new technology About 6 months ago, we decided to develop a new, modern search architecture that is based on a highly efficient inverted index instead of a relational database. Since we love Open Source here at Twitter we chose Lucene , a search engine library written in Java, as a starting point. Our demands on the new system are immense: With over 1,000 TPS (Tweets/sec) and 12,000 QPS (queries/sec) = over 1 billion queries per day (!) we already put a very high load on our machines. As we want the new system to last for several years, the goal was to support at least an order of magnitude more load. Twitter is real-time, so our search engine must be too. In addition to these scalability requirements, we also need to support extremely low indexing latencies (the time it takes between when a Tweet is tweeted and when it becomes searchable) of less than 10 seconds. Since the indexer is only one part of the pipeline a Tweet has to make it through, we needed the indexer itself to have a sub-second latency. Yes, we do like challenges here at Twitter! (btw, if you do too: @JoinTheFlock !) Modified Lucene Lucene is great, but in its current form it has several shortcomings for real-time search. That’s why we rewrote big parts of the core in-memory data structures, especially the posting lists, while still supporting Lucene’s standard APIs. This allows us to use Lucene’s search layer almost unmodified. Some of the highlights of our changes include: significantly improved garbage collection performance lock-free data structures and algorithms posting lists, that are traversable in reverse order efficient early query termination We believe that the architecture behind these changes involves several interesting topics that pertain to software engineering in general (not only search). We hope to continue to share more on these improvements. And, before you ask, we’re planning on contributing all these changes back to Lucene; some of which have already made it into Lucene’s trunk and its new realtime branch. Benefits Now that the system is up and running, we are very excited about the results. We estimate that we’re only using about 5% of the available backend resources, which means we have a lot of headroom. Our new indexer could also index roughly 50 times more Tweets per second than we currently get! And the new system runs extremely smoothly, without any major problems or instabilities (knock on wood). But you might wonder: Fine, it’s faster, and you guys can scale it longer, but will there be any benefits for the users? The answer is definitely yes! The first difference you might notice is the bigger index, which is now twice as long — without making searches any slower. And, maybe most importantly, the new system is extremely versatile and extensible, which will allow us to build cool new features faster and better. Stay tuned! The engineers who implemented the search engine are: Michael Busch , Krishna Gade , Mike Hayes , Abhi Khune , Brian Larson , Patrick Lok , Samuel Luckenbill , Jake Mannix , Jonathan Reichhold .", "date": "2010-10-06"},
{"website": "Twitter-Engineering", "title": "Hack Week", "author": ["‎@bs‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2010/hack-week.html", "abstract": "Here at Twitter, we make things. Over the last five weeks, we’ve launched the new Twitter and made significant changes to the technology behind Twitter.com, deployed a new backend for search, and refined the algorithm for trending topics to make them more real-time. To keep with the spirit of driving innovation in engineering, we’ll be holding our first Hack Week starting today (Oct 22) and running through next Friday (Oct 29). In this light, we’ll all be building things that are separate from our normal work and not part of our day-to-day jobs. Of course, we’ll keep an eye out for whales. There aren’t many rules – basically we’ll work in small teams and share our projects with the company at the end of the week. What will happen with each project will be determined once it’s complete. Some may ship immediately, others may be added to the roadmap and built out in the future, and the remainder may serve as creative inspiration. If you have an idea for one of our teams, send a tweet to @hackweek . We’re always looking for feedback.", "date": "2010-10-22"},
{"website": "Twitter-Engineering", "title": "2010", "author": ["‎@‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2010.html", "abstract": "", "date": "1970-01-01"},
{"website": "Twitter-Engineering", "title": "Building a Faster Ruby Garbage Collector", "author": ["‎@evan‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2011/building-a-faster-ruby-garbage-collector.html", "abstract": "Since late 2009, much of www.twitter.com has run on Ruby Enterprise Edition (REE), a modified version of the standard MRI 1.8.7 Ruby interpreter. At the time, we worked with the REE team to integrate some third-party patches that allowed us to tune the garbage collector for long-lived workloads. We knew this was not a perfect choice, but a switch to a new runtime (even MRI 1.9x) would introduce compatibility problems, and testing indicated that alternative runtimes are not necessarily faster for our workload. Nevertheless, the CPU cost of REE remained too high. To address this problem, we decided to explore options for optimizing the REE runtime. We called this effort Project Kiji, after the Japanese bird. Inefficient garbage collection Our performance measurements revealed that even after our patches, the Ruby runtime uses a significant fraction of the CPU for running the garbage collector on twitter.com. This is largely because MRI’s garbage collector uses a single heap: The garbage collector’s naive stop-the-world mark-and-sweep process accesses the entire memory set several times. It first marks all objects at the “root-set” level as “in-use” and then reexamines all the objects to release the memory of those not in use. Additionally, the collector suspends the system during every sweep, thereby periodically “freezing” some of the programs. The collection process is not generational. That is, the collector does not move objects between heaps; they all stay at the same address for their lifetime. The resulting fragmented memory extracts a penalty in bookkeeping cost because it can neither be consolidated nor discarded. We needed to make the garbage collector more efficient but had limited options. We couldn’t easily change the runtime’s stop-the world-process because internally it relies on being single-threaded. Neither could we implement a real generational collector because our interpreter relies on objects staying at the same address in memory. Two heaps are better than one While we could not change the location of an allocated object, we assumed we could allocate the objects in a different location, depending on their expected lifetime. So, first, we separated the live objects into transient objects and long-lived objects. Next, we added another heap to Ruby and called it “longlife” heap. According to the current implementation, Kiji has two heaps: An ordinary heap that has a variety of objects, including the transient objects A longlife heap for objects that we expect to be long-lived. How the longlife heap works In the new configuration, AST (Abstract Syntax Tree) nodes occur in longlife heaps. They are a parsed representation of the Ruby programs’ source code construct (such as name, type, expression, statement, or declaration). Once loaded, they tend to stick in memory, and are largely immutable. They also occupy a large amount of memory: in twitter.com’s runtime, they account for about 60% of live objects at any given time. By placing the AST nodes in a separate longlife heap and running an infrequent garbage collection only on this heap, we saw a significant performance gain: the time the CPU spends in garbage collection reduced from 18.5% to 14%. With infrequent collection, the system retains some garbage in memory, which increases overall memory use. We experimented with various scheduling strategies for longlife garbage collection that balanced the tradeoff between CPU usage and memory usage. We finally selected a strategy that triggers a collection scheduled to synchronize with the 8th collection cycle of the ordinary heap if an allocation occurs in the longlife heap. If the longlife heap does not receive an allocation, subsequent collections on the longlife heap occur after the 16th, 32nd, 64th collection cycle and so on, with each occurrence increasing exponentially. Improved mark phase A second heap improved garbage collection but we needed to ensure that the objects in the longlife heap continued to keep alive those objects they referenced in the ordinary heap. Due to a separation in heaps, we were now processing the majority of our ordinary heap collections without a longlife collection. Therefore, ordinary objects—reachable only through longlife objects—would not be marked as live and could, mistakenly, be swept as garbage. We needed to maintain a set of “remembered objects” or boundary objects that would live in the ordinary heap but were directly referenced by objects living in the longlife heap. This proved to be a far greater challenge than originally expected. At first we added objects to the remembered set whenever we constructed an AST node. However, the AST nodes are not uniformly immutable. Following a parse, the Ruby interpreter tends to rewrite them immediately to implement small optimizations on them. This frequently rewrites the pointers between objects. We overcame this problem by implementing an algorithm that is similar to the mark phase, except that it is not recursive and only discovers direct references from the longlife to the ordinary heap. We run the algorithm at ordinary collection time when we detect that prior changes have occurred in the longlife heap. The run decreases in frequency over time; the longer the process runs, the more the amount of loaded code that stagnates. In other words, we are consciously optimizing for long-running processes. An additional optimization ensures that if an ordinary object points to a longlife object, the marking never leaves the ordinary heap during the mark phase. This is because all outgoing pointers from the longlife heap to the ordinary heap are already marked as remembered objects. The mark algorithm running through the longlife heap reference chains does not mark any new objects in the ordinary heap. Results The graph below shows the performance curves of the twitter.com webapp on various Ruby interpreter variants on a test machine with a synthetic load (not indicative of our actual throughput). We took out-of-the-box Ruby MRI 1.8.7p248, REE 2010.02 (on which Kiji is based), and Kiji. In addition, we tested REE and Kiji in two modes: one with the default settings, the other with GC_MALLOC_LIMIT tuned to scale back speculative collection. We used httperf to stress the runtimes with increasing numbers of requests per second, and measured the rate of successful responses per second. As you can see, the biggest benefit comes from the GC tuning, but Kiji’s two-heap structure also creates a noticeable edge over standard REE. We have also measured the CPU percentage spent in the GC for these variants, this time with actual production traffic. We warmed up the runtimes first, then explicitly shut off allocations in the longlife heap once it was unlikely that any genuinely long-lived AST nodes would be generated. The results are below. Lessons and future trends With Kiji, the garbage collector now consumes a smaller portion of the CPU for Ruby processes, and more importantly, enables additional improvements. As we identify additional objects to move to the longlife heap, we can further decrease the overall CPU usage. The theoretical floor is about 5% of total CPU time spent in the GC. We will be posting updates with new results. References A major source of inspiration was the patch by Narihiro Nakamura (available here ). Narihiro’s patch is a proof-of-concept; it handles few AST nodes. It is also written as a patch for MRI 1.9, and we needed to cross-port it to REE, which is a derivative of MRI 1.8.7. We substantially extended Narihiro’s work to include algorithmic boundary set calculation and stop the ordinary mark from leaving the ordinary heap. We also ensured our solution integrated well with REE’s strategy for avoiding copy-on-write of objects in forked process in the mark phase. These changes delivered significant gains. Try it! We have released the Kiji REE branch on GitHub, and hope that a future version will be suitable for merging into REE itself. In our case, switching to Kiji brought a 13% increase in the number of requests twitter.com can serve per second. Acknowledgements The following engineers at Twitter contributed to the REE improvements: Rob Benson, Brandon Mitchell, Attila Szegedi, and Evan Weaver. — Attila ( @asz )", "date": "2011-03-04"},
{"website": "Twitter-Engineering", "title": "The Great Migration, the Winter of 2011", "author": ["‎@mabb0tt‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2011/the-great-migration-the-winter-of-2011.html", "abstract": "If you look back at the history of Twitter, our rate of growth has largely outpaced the capacity of our hardware, software, and the company itself. Indeed, in our first five years, Twitter’s biggest challenge was coping with our unprecedented growth and sightings of the infamous Fail Whale. These issues came to a head last June when Twitter experienced more than ten hours of downtime. However, unlike past instances of significant failure, we said at the time that that we had a long-term plan. Last September, we began executing on this plan and undertook the most significant engineering challenge in the history of Twitter. We hope it will have a significant impact the service’s success for many years to come. During this time, the engineers and operations teams moved Twitter’s infrastructure to a new home while making changes to our infrastructure and our organization that will ensure that we can constantly stay abreast of our capacity needs; give users and developers greater reliability; and, allow for new product offerings. This was our season of migration. Redesigning and Rebuilding the Bird Mid-flight Under the hood , Twitter is a complex yet elegant distributed network of queues , daemons, caches , and databases . Today, the feed and care of Twitter requires more than 200 engineers to keep the site growing and running smoothly. What did moving the entirety of Twitter while improving up-time entail? Here’s a simplified version of what we did. First, our engineers extended many of Twitter’s core systems to replicate Tweets to multiple data centers. Simultaneously, our operations engineers divided into new teams and built new processes and software to allow us to qualify, burn-in, deploy, tear-down and monitor the thousands of servers, routers, and switches that are required to build out and operate Twitter. With hardware at a second data center in place, we moved some of our non-runtime systems there – giving us headroom to stay ahead of tweet growth. This second data center also served as a staging laboratory for our replication and migration strategies. Simultaneously, we prepped a third larger data center as our final nesting ground. Next, we set out rewiring the rocket mid-flight by writing Tweets to both our primary data center and the second data center. Once we proved our replication strategy worked, we built out the full Twitter stack, and copied all 20TB of Tweets, from @jack ’s first to @honeybadger ’s latest Tweet to the second data center. Once all the data was in place we began serving live traffic from the second data center for end-to-end testing and to continue to shed load from our primary data center. Confident that our strategy for replicating Twitter was solid, we moved on to the final leg of the migration, building out and moving all of Twitter from the first and second data centers to the final nesting grounds. This essentially required us to move much of Twitter two times. What’s more, during the migration we set a new Tweet per second record , continued to grow , launched new products , while improving the security and up-time of our service. A Flock The effort and planning behind this effort were huge. Vacations were put off, weekends were worked, more than a few strategic midnight oil reserves were burned in this two-stage move. The technical accomplishments by the operations and engineering teams that made this move possible were immense. Equally great, was the organization and alignment of the engineering and operations teams, their ability to create lightweight robust processes where none had existed before. Without this cohesion, this flocking of sorts, none of this would have been possible. Though spring is here, and this particular season of migration is over, it represents more of a beginning than an ending. This move gives us the capacity to deliver Tweets with greater reliability and speed, and creates more runway to focus on the most interesting operations and engineering problems. It’s an immense opportunity to innovate and build the products and technologies that our users request and our talented engineers love to develop. —The Twitter Engineering Team P.S. Twitter is hiring across engineering and operations. If you want to develop novel systems that scale on the order of billions, join the flock .", "date": "2011-03-21"},
{"website": "Twitter-Engineering", "title": "Improving Browser Security with CSP", "author": ["‎@mdp‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2011/improving-browser-security-with-csp.html", "abstract": "If you are using Firefox 4, you now have an extra layer of security when accessing mobile.twitter.com. Over the past few weeks we’ve been testing a new security feature for our mobile site. It is called a Content Security Policy, or CSP. This policy is a standard developed by Mozilla that aims to thwart cross site scripting (XSS) attacks at their point of execution, the browser. The upcoming release of Firefox 4 implements CSP, and while the mobile site may not get a high volume of desktop browser traffic (the desktop users hitting that site typically have low bandwidth connections), it has given us an opportunity to test out a potentially powerful anti-XSS tool in a controlled setting. CSP IN A NUTSHELL In a typical XSS attack, the attacker injects arbitrary Javascript into a page, which is then executed by an end-user. When a website enables CSP, the browser ignores inline Javascript and only loads external assets from a set of whitelisted sites. Enabling CSP on our site was simply a matter of including the policy in the returned headers under the CSP defined key, ‘X-Content-Security-Policy’. The policy also contains a ‘reporting URI’ to which the browser sends JSON reports of any violations. This feature not only assists debugging of the CSP rules, it also has the potential to alert a site’s owner to emerging threats. IMPLEMENTING THE FEATURE Although activating CSP is easy, in order for it to work correctly you may need to modify your site. In our case it meant removing all inline Javascript. While it is good practice to keep inline Javascript out of your HTML, it is sometimes necessary to speed up the load times on slower high-latency mobile phones. We began our explorations by restricting the changes to browsers that support CSP (currently only Firefox 4) in order to lessen the impact on users. Next, we identified all the possible locations of our assets and built a rule set to encompass those; for example, things such as user profile images and stylesheets from our content delivery network. Our initial trials revealed that some libraries were evaluating strings of Javascript and triggering a violation, most notably jQuery 1.4, which tests the ‘eval’ function after load. This wasn’t totally unexpected and we modified some of the libraries to get them to pass. Since jQuery fixed this in 1.5, it is no longer an issue. INITIAL RESULTS After a soft launch, we ran into some unexpected issues. Several common Firefox extensions insert Javascript on page load, thereby triggering a report. However, even more surprising were the number of ISPs who were inadvertently inserting Javascript or altering image tags to point to their caching servers. It was the first example of how CSP gave us visibility into what was happening on the user’s end. We addressed this problem by mandating SSL for Firefox 4 users, which prevents any alteration of our content. Today CSP is one hundred percent live on mobile.twitter.com and we are logging and evaluating incoming violation reports. FINAL THOUGHTS Allowing sites like Twitter to disable inline Javascript and whitelist external assets is a huge step towards neutralizing XSS attacks. However, for many sites it is not going to be as simple as flipping a switch. Most sites will require some work and you may need to alter a few third-party Javascript libraries. Depending on how complex your site is, this could entail the bulk of your effort. We hope other browsers will adopt the CSP standard, especially as more sites depend on client-side code and user-generated content. The simple option of being able to disable inline Javascript and limit external sources gives sites the ability to stop the vast majority of today’s attacks with minimal effort. Over the next couple of months we plan to implement a Content Security Policy across more of Twitter, and we encourage you to request support for this standard in your preferred browser. ACKNOWLEDGEMENTS The following people at Twitter contributed to the CSP effort: John Adams, Jacob Hoffman-Andrews, Kevin Lingerfelt, Bob Lord, Mark Percival, and Marcus Philips FURTHER READING Mozilla CSP announcement Mozilla CSP Doc Center CSP Spec CSP Demo Page —Mark ( @mdp )", "date": "2011-03-22"},
{"website": "Twitter-Engineering", "title": "Twitter Search is Now 3x Faster", "author": ["‎@krishnagade‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2011/twitter-search-is-now-3x-faster.html", "abstract": "In the spring of 2010, the search team at Twitter started to rewrite our search engine in order to serve our ever-growing traffic, improve the end-user latency and availability of our service, and enable rapid development of new search features. As part of the effort, we launched a new real-time search engine , changing our back-end from MySQL to a real-time version of Lucene . Last week, we launched a replacement for our Ruby-on-Rails front-end: a Java server we call Blender. We are pleased to announce that this change has produced a 3x drop in search latencies and will enable us to rapidly iterate on search features in the coming months. PERFORMANCE GAINS Twitter search is one of the most heavily-trafficked search engines in the world, serving over one billion queries per day. The week before we deployed Blender, the #tsunami in Japan contributed to a significant increase in query load and a related spike in search latencies. Following the launch of Blender, our 95th percentile latencies were reduced by 3x from 800ms to 250ms and CPU load on our front-end servers was cut in half. We now have the capacity to serve 10x the number of requests per machine. This means we can support the same number of requests with fewer servers, reducing our front-end service costs. 95th Percentile Search API Latencies Before and After Blender Launch TWITTER’S IMPROVED SEARCH ARCHITECTURE In order to understand the performance gains, you must first understand the inefficiencies of our former Ruby-on-Rails front-end servers. The front ends ran a fixed number of single-threaded rails worker processes, each of which did the following: parsed queries queried index servers synchronously aggregated and rendered results We have long known that the model of synchronous request processing uses our CPUs inefficiently. Over time, we had also accrued significant technical debt in our Ruby code base, making it hard to add features and improve the reliability of our search engine. Blender addresses these issues by: Creating a fully asynchronous aggregation service. No thread waits on network I/O to complete. Aggregating results from back-end services, for example, the real-time, top tweet, and geo indices. Elegantly dealing with dependencies between services. Workflows automatically handle transitive dependencies between back-end services. The following diagram shows the architecture of Twitter’s search engine. Queries from the website, API, or internal clients at Twitter are issued to Blender via a hardware load balancer. Blender parses the query and then issues it to back-end services, using workflows to handle dependencies between the services. Finally, results from the services are merged and rendered in the appropriate language for the client. Twitter Search Architecture with Blender BLENDER OVERVIEW Blender is a Thrift and HTTP service built on Netty , a highly-scalable NIO client server library written in Java that enables the development of a variety of protocol servers and clients quickly and easily. We chose Netty over some of its other competitors, like Mina and Jetty, because it has a cleaner API, better documentation and, more importantly, because several other projects at Twitter are using this framework. To make Netty work with Thrift, we wrote a simple Thrift codec that decodes the incoming Thrift request from Netty’s channel buffer, when it is read from the socket and encodes the outgoing Thrift response, when it is written to the socket. Netty defines a key abstraction, called a Channel, to encapsulate a connection to a network socket that provides an interface to do a set of I/O operations like read, write, connect, and bind. All channel I/O operations are asynchronous in nature. This means any I/O call returns immediately with a ChannelFuture instance that notifies whether the requested I/O operations succeed, fail, or are canceled. When a Netty server accepts a new connection, it creates a new channel pipeline to process it. A channel pipeline is nothing but a sequence of channel handlers that implements the business logic needed to process the request. In the next section, we show how Blender maps these pipelines to query processing workflows. WORKFLOW FRAMEWORK In Blender, a workflow is a set of back-end services with dependencies between them, which must be processed to serve an incoming request. Blender automatically resolves dependencies between services, for example, if service A depends on service B, A is queried first and its results are passed to B. It is convenient to represent workflows as directed acyclic graphs (see below). Sample Blender Workflow with 6 Back-end Services In the sample workflow above, we have 6 services {s1, s2, s3, s4, s5, s6} with dependencies between them. The directed edge from s3 to s1 means that s3 must be called before calling s1 because s1 needs the results from s3. Given such a workflow, the Blender framework performs a topological sort on the DAG to determine the total ordering of services, which is the order in which they must be called. The execution order of the above workflow would be {(s3, s4), (s1, s5, s6), (s2)}. This means s3 and s4 can be called in parallel in the first batch, and once their responses are returned, s1, s5, and s6 can be called in parallel in the next batch, before finally calling s2. Once Blender determines the execution order of a workflow, it is mapped to a Netty pipeline. This pipeline is a sequence of handlers that the request needs to pass through for processing. MULTIPLEXING INCOMING REQUESTS Because workflows are mapped to Netty pipelines in Blender, we needed to route incoming client requests to the appropriate pipeline. For this, we built a proxy layer that multiplexes and routes client requests to pipelines as follows: When a remote Thrift client opens a persistent connection to Blender, the proxy layer creates a map of local clients, one for each of the local workflow servers. Note that all local workflow servers are running inside Blender’s JVM process and are instantiated when the Blender process starts. When the request arrives at the socket, the proxy layer reads it, figures out which workflow is requested, and routes it to the appropriate workflow server. Similarly, when the response arrives from the local workflow server, the proxy reads it and writes the response back to the remote client. We made use of Netty’s event-driven model to accomplish all the above tasks asynchronously so that no thread waits on I/O. DISPATCHING BACK-END REQUESTS Once the query arrives at a workflow pipeline, it passes through the sequence of service handlers as defined by the workflow. Each service handler constructs the appropriate back-end request for that query and issues it to the remote server. For example, the real-time service handler constructs a realtime search request and issues it to one or more realtime index servers asynchronously. We are using the twitter commons library (recently open-sourced!) to provide connection-pool management, load-balancing, and dead host detection. The I/O thread that is processing the query is freed when all the back-end requests have been dispatched. A timer thread checks every few milliseconds to see if any of the back-end responses have returned from remote servers and sets a flag indicating if the request succeeded, timed out, or failed. We maintain one object over the lifetime of the search query to manage this type of data. Successful responses are aggregated and passed to the next batch of service handlers in the workflow pipeline. When all responses from the first batch have arrived, the second batch of asynchronous requests are made. This process is repeated until we have completed the workflow or the workflow’s timeout has elapsed. As you can see, throughout the execution of a workflow, no thread busy-waits on I/O. This allows us to efficiently use the CPU on our Blender machines and handle a large number of concurrent requests. We also save on latency as we can execute most requests to back-end services in parallel. BLENDER DEPLOYMENT AND FUTURE WORK To ensure a high quality of service while introducing Blender into our system, we are using the old Ruby on Rails front-end servers as proxies for routing thrift requests to our Blender cluster. Using the old front-end servers as proxies allows us to provide a consistent user experience while making significant changes to the underlying technology. In the next phase of our deploy, we will eliminate Ruby on Rails entirely from the search stack, connecting users directly to Blender and potentially reducing latencies even further. — @twittersearch ACKNOWLEDGEMENTS The following Twitter engineers worked on Blender: Abhi Khune, Aneesh Sharma, Brian Larson, Frost Li, Gilad Mishne, Krishna Gade, Michael Busch, Mike Hayes, Patrick Lok, Raghavendra Prabhu, Sam Luckenbill, Tian Wang, Yi Zhuang, Zhenghua Li.", "date": "2011-04-06"},
{"website": "Twitter-Engineering", "title": "Faster Ruby: Kiji Update", "author": ["‎@AttilaSzegedi‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2011/faster-ruby-kiji-update.html", "abstract": "In March 2011, we shared Kiji , an improved Ruby runtime. The initial performance gains were relatively modest, but laid the foundation for future improvements. We continued the work and now have some excellent results. FASTER REMEMBERED SET CALCULATIONS In Kiji 0.10, every change to the longlife heap required full recalculation of the “remembered set,” the boundary objects referenced from the longlife to the eden heap. For Kiji 0.11, we changed the calculation to an incremental model that only includes newly-allocated objects. We made this easier by disabling garbage collection during source code parsing, which has a tendency to mutate references in place. Now, if the parser needs more memory, it merely allocates a new heap chunk. This lets us allocate all AST nodes, including those created in instance_eval , on the longlife heap. The result is a big performance boost for applications like template engines that use lots of instance_eval . MORE OBJECTS IN LONGLIFE For Kiji 0.11, we now allocate non-transient strings in the longlife heap, along with the AST nodes. This includes strings allocated during parsing, assigned to constants (or members of a constant hash or array), and those that are members of frozen objects. With Ruby’s Kernel.freeze method, big parts of frozen objects are now evicted from the ordinary heap and moved to the longlife heap. This change is significant. When the twitter.com web application ran Kiji 0.10, it had 450,000 live objects after garbage collection in its ordinary heap. Kiji 0.11 places over 300,000 string objects in the longlife heap, reducing the number of live objects in the ordinary heap to under 150,000. The nearly 66 percent reduction allows the heap to collect much less frequently. SIMPLIFIED HEAP GROWTH STRATEGY Ruby Enterprise Edition has a set of environment variables that govern when to run the garbage collector and how to grow and shrink the heaps. After evaluating Ruby’s heap growth strategy, we replaced it with one that is much simpler to configure and works better for server workloads. As a first step, we eliminated GC_MALLOC_LIMIT . This environment variable prescribes when to force a garbage collection, following a set of C-level malloc() calls. We found this setting to be capricious; it performed best when it was set so high as to be effectively off. By eliminating the malloc limit entirely, the Kiji 0.11 garbage collector runs only when heaps are full, or when no more memory can be allocated from the operating system. This also means that under UNIX-like systems, you can more effectively size the process with ulimit -u . 0.11 now has only these three GC-tuning environment variables: The first parameter is RUBY_GC_HEAP_SIZE . This parameter determines the number of objects in a heap slab. The value is specified in numbers of objects. Its default value is 32768. The next parameter is RUBY_GC_EDEN_HEAPS . This parameter specifies the target number of heap slabs for the ordinary heap. Its default value is 24. The runtime starts out with a single heap slab, and when it fills up, it collects the garbage and allocates a new slab until it reaches the target number. This gradual strategy keeps fragmentation in the heaps low, as it tends to concentrate longer-lived objects in the earlier heap slabs. If the heap is forced to grow beyond the target number of slabs, the runtime releases vacated slabs after each garbage collection in order to restore the target size. Once the application reaches the target size of ordinary heap, it does not go below it. Since performance is tightly bound to the rate of eden collections (a classic memory for speed tradeoff), this makes the behavior of a long-lived process very predictable. We have had very good results with settings as high as 64. The final parameter is RUBY_GC_LONGLIFE_LAZINESS , a decimal between 0 and 1, with a default of 0.05. This parameter governs a different heap growth strategy for longlife heap slabs. The runtime releases vacant longlife heap slabs when the ratio of free longlife heap slots to all longlife heap slots after the collection is higher than this parameter. Also, if the ratio is lower after collection, a new heap slab is allocated. The default value is well-tuned for our typical workload and prevents memory bloat. We also reversed the order of adding the freed slots onto the free list. Now, new allocations are fulfilled with free slots from older (presumably, more densely-populated) heap slabs first, allowing recently allocated heap slabs to become completely vacant in a subsequent GC run. This may slightly impact locality of reference, but works well for us. ADDITIONAL CHANGES We replaced the old profiling methods that no longer applied with our improved memory debugging. We also removed the “fastmarktable” mode, where the collector used a mark bit in the object slots. Kiji 0.11 uses only the copy-on-write friendly mark table. This lets us reset the mark bits after collection by zeroing out the entire mark table, instead of flipping a bit in every live object. IT’S IN THE NUMBERS We updated the performance chart from the first blog post about Kiji with the 0.11 data. As you can see, the new data shows a dramatic improvement for our example intensive workload. While Kiji 0.9 responded to all requests until 90 requests/sec and peaked at 95 responses out of 100 requests/sec, Kiji 0.11 responds to all requests until 120 requests/sec. This is a 30% improvement in throughput across the board, and 2.7x the speed of standard Ruby 1.8. FULL ALLOCATION TRACING We found that in order to effectively develop Kiji 0.11, we needed to add more sophisticated memory instrumentation than is currently available for Ruby. As a result, we ended up with some really useful debugging additions that you can turn on as well. The first tool is a summary of memory stats after GC. It lets you cheaply measure the impact of memory-related changes: The second tool is an allocation tracer (a replacement for BleakHouse and similar tools). After each GC, the runtime writes files containing full stack traces for the allocation points of all freed and surviving objects. You can easily parse this with AWK to list common object types, allocation sites, and number of objects allocated. This makes it easy to identify allocation hotspots, memory leaks, or objects that persist on the eden and should be manually moved to the longlife. A sample output for allocation tracing, obtained by running RubySpec under Kiji: For more information, refer to the README-kiji file in the distribution. FUTURE DIRECTIONS 0.11 is a much more performant and operable runtime than Kiji 0.10. However, through this work we identified a practical strategy for making an even better, fully-generational version that would apply well to Ruby 1.9. Time will tell if we get to implement it. We also would like to investigate the relative performance of JRuby. TRY IT! We have released the Kiji REE branch on GitHub. ACKNOWLEDGEMENTS The following engineers at Twitter contributed to the REE improvements: Rob Benson, Brandon Mitchell, Attila Szegedi, and Evan Weaver. If you want to work on projects like this, join the flock! — Attila ( @asz )", "date": "2011-05-19"},
{"website": "Twitter-Engineering", "title": "The Engineering Behind Twitter’s New Search Experience", "author": ["‎@133824534‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2011/the-engineering-behind-twitter-s-new-search-experience.html", "abstract": "Today, Twitter launched a personalized search experience to help our users find the most relevant Tweets, images, and videos. To build this product, our infrastructure needed to support two major features: relevance-filtering of search results and the identification of relevant images and photos. Both features leverage a ground-up rewrite of the search infrastructure, with Blender and Earlybird at the core. Investment in Search Since the acquisition of Summize in 2008, Twitter has invested heavily in search. We’ve grown our search team from three to 15 engineers and scaled our real-time search engine by two orders of magnitude — all this, while we replaced the search infrastructure in flight, with no major service interruptions. The engineering story behind the evolution of search is compelling. The Summize infrastructure used Ruby on Rails for the front-end and MySQL for the back-end (the same architecture as the one used by Twitter and many other start-ups). At the time, Lucene and other open-source search technology did not support real-time search. As a result, we constructed our reverse indexes in MySQL, leveraging its concurrent transactions and B-tree data structures to support concurrent indexing and searching. We were able to scale our MySQL-based solution surprisingly far by partitioning the index across multiple databases and replicating the Rails front-end. In 2008, Twitter search handled an average of 20 TPS and 200 QPS. By October 2010, when we replaced MySQL with Earlybird, the system was handling 1,000 TPS and 12,000 QPS on average. Earlybird, a real-time, reverse index based on Lucene , not only gave us an order of magnitude better performance than MySQL for real-time search, it doubled our memory efficiency and provided the flexibility to add relevance filtering. However, we still needed to replace the Ruby on Rails front-end, which was only capable of synchronous calls to Earlybird and had accrued significant technical debt through years of scaling and transition to Earlybird. In April 2011, we launched a replacement, called Blender, which improved our search latencies by 3x, gave us 10x throughput, and allowed us to remove Ruby on Rails from the search infrastructure. Today, we are indexing an average of 2,200 TPS while serving 18,000 QPS (1.6B queries per day!). More importantly, Blender completed the infrastructure necessary to make the most significant user-facing change to Twitter search since the acquisition of Summize. From Hack-Week Project to Production When the team launched Earlybird, we were all excited by its potential — it was fast and the code was clean and easy to extend. While on vacation in Germany, Michael Busch, one of our search engineers, implemented a demo of image and video search. A few weeks later, during Twitter’s first Hack Week , the search team, along with some members of other teams, completed the first demo of our new search experience. Feedback from the company was so positive that the demo became part of our product roadmap. Surfacing Relevant Tweets There is a lot of information on Twitter — on average, more than 2,200 new Tweets every second! During large events, for example the #tsunami in Japan, this rate can increase by 3 to 4x. Often, users are interested in only the most memorable Tweets or those that other users engage with. In our new search experience, we show search results that are most relevant to a particular user. So search results are personalized, and we filter out the Tweets that do not resonate with other users. To support relevance filtering and personalization, we needed three types of signals: Static signals, added at indexing time Resonance signals, dynamically updated over time Information about the searcher, provided at search time Getting all of these signals into our index required changes to our ingestion pipeline, Earlybird (our reverse index), and Blender (our front-ends). We also created a new updater component that continually pushes resonance signals to Earlybird. In the ingestion pipeline, we added a pipeline stage that annotates Tweets with static information, for example, information about the user and the language of the Tweet’s text. The Tweets are then replicated to the Earlybird indexes (in real time), where we have extended Lucene’s internal data structures to support dynamic updates to arbitrary annotations. Dynamic updates, for example, the users’ interactions with Tweets, arrive over time from the updater. Together, Earlybird and the updater support a high and irregular rate of updates without requiring locks or slowing down searches. At query time, a Blender server parses the user’s query and passes it along with the user’s social graph to multiple Earlybird servers. These servers use a specialized ranking function that combines relevance signals and the social graph to compute a personalized relevance score for each Tweet. The highest-ranking, most-recent Tweets are returned to the Blender, which merges and re-ranks the results before returning them to the user. Twitter search architecture with support for relevance Removing Duplicates Duplicate and near-duplicate Tweets are often not particularly helpful in Twitter search results. During popular and important events, when search should be most helpful to our users, nearly identical Tweets increase in number. Even when the quality of the duplicates is high, the searcher would benefit from a more diverse set of results. To remove duplicates we use a technique based on MinHashing , where several signatures are computed per Tweet and two Tweets sharing the same set of signatures are considered duplicates. The twist? Like everything at Twitter, brevity is key: We have a very small memory budget to store the signatures. Our algorithm compresses each Tweet to just 4 bytes while still identifying the vast majority of duplicates with very low computational requirements. Personalization Twitter is most powerful when you personalize it by choosing interesting accounts to follow, so why shouldn’t your search results be more personalized too? They are now! Our ranking function accesses the social graph and uses knowledge about the relationship between the searcher and the author of a Tweet during ranking. Although the social graph is very large, we compress the meaningful part for each user into a Bloom filter , which gives us space-efficient constant-time set membership operations. As Earlybird scans candidate search results, it uses the presence of the Tweet’s author in the user’s social graph as a relevance signal in its ranking function. Even users that follow few or no accounts will benefit from other personalization mechanisms; for example, we now automatically detect the searcher’s preferred language and location. Images and Videos in Search Images and videos have an amazing ability to describe people, places, and real-time events as they unfold. Take for example @ jkrums ’ Twitpic of US Airways Flight 1549 Hudson river landing , and @ stefmara ’s photos and videos of space shuttle Endeavour’s final launch . There is a fundamental difference between searching for Tweets and searching for entities in Tweets, such as images and videos. In the former case, the decision about whether a Tweet matches a query can be made by looking at the text of the Tweet, with no other outside information. Additionally, per-Tweet relevance signals can be used to rank and compare matching Tweets to find the best ones. The situation is different when searching for images or videos. For example, the same image may be tweeted many times, with each Tweet containing different keywords that all describe the image. Consider the following Tweets: “This is my Australian Shepherd: http://bit.ly/kQvYGp ” “What a cute dog! RT This is my Australian Shepherd: http://bit.ly/kQvYGp ”. One possible description of the image is formed from the union of keywords in the Tweets’ text; that is, “dog”, “Australian”, and “shepherd” all describe the image. If an image is repeatedly described by a term in the Tweet’s text, it is likely to be about that term. So what makes this a difficult problem? Twitter allows you to search Tweets within seconds; images and photos in tweets should be available in realtime too! Earlybird uses inverted indexes for search. While these data structures are extremely efficient, they do not support inline updates, which makes it nearly impossible to append additional keywords to indexed documents. If timeliness was not important, we could use MapReduce jobs that periodically aggregate keyword unions and produce inverted indexes. In these offline indexes, each link to an image or photo link would be a document, with the aggregated keywords as the document’s text. However, to meet our indexing latency goals, we would have to run these MapReduce jobs every few seconds, an impractical solution. Instead, we extended Earlybird’s data structures to support efficient lookups of entities contained in Tweets. At query time, we look up the images and videos for matching Tweets and and store them in a custom hash map. The keys of the map are URLs and the values are score counters. Each time the same URL is added to the map, its corresponding score counter is incremented. After this aggregation is complete, the map is sorted and the best images and photos are returned for rendering. What’s next? The search team is excited to build innovative search products that drive discovery and help our users. While the new search experience is a huge improvement over pure real-time search, we are just getting started. In the coming months, we will improve quality, scale our infrastructure, expand our indexes, and bring relevance to mobile. If you are a talented engineer and want to work on the largest real-time search engine in the world, Twitter search is hiring for search quality and search infrastructure ! Acknowledgements The following people contributed to the launch: Abhi Khune, Abdur Chowdhury, Aneesh Sharma, Ashok Banerjee, Ben Cherry, Brian Larson, Coleen Baik, David Chen, Frost Li, Gilad Mishne, Isaac Hepworth, Jon Boulle, Josh Brewer, Krishna Gade, Michael Busch, Mike Hayes, Nate Agrin, Patrick Lok, Raghavendra Prabu, Sarah Brown, Sam Luckenbill, Stephen Fedele, Tian Wang, Yi Zhuang, Zhenghua Li. We would also like to thank the original Summize team, former team members, hack-week contributors, and management for their contributions and support. — @twittersearch", "date": "2011-05-31"},
{"website": "Twitter-Engineering", "title": "Join the Flock!", "author": ["‎@mabb0tt‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2011/join-the-flock.html", "abstract": "Engineering Open House Twitter’s engineering team is growing quickly. Two-thirds of our engineers were hired in the last 12 months. Those engineers joined us from cities and countries around the world and from companies of various sizes. As part of our effort to find and hire great people to build great products and solve complicated problems, last Thursday we invited several dozen engineers to Twitter HQ for our first engineering open house. Presentations from @wfarner , @michibusch , @mracus and @esbie showcased the depth and range of the effort required to present twitter.com to the world. The topics covered some of these key areas for development: Dynamic deployment and resource management with Mesos - @wfarner Using Mesos as a platform, we have built a private cloud system on which we hope to eventually run most, if not all, of our services. We expect this to simplify deployment and improve the reliability of our systems, while making more efficient use of our compute resources. Real-time search at Twitter - @michibusch Since 2008, Twitter has made dramatic enhancements to our real-time search engine, scaling it from 200 QPS to 18,000 QPS. At the core of our infrastructure is Earlybird , a version of Lucene modified for real-time search. This work, combined with other key infrastructure components, led to our recent revamp of the search experience and will enable future innovation in real-time search. The client-side architecture of # NewTwitter - @mracus and @esbie Client-side applications for desktop and mobile environments have access to a class of well-rounded tools and framework components that aren’t as yet widely available for the browser. Therefore, a fully in-browser app like # NewTwitter requires investment in solid architecture in order to remain clean and extensible as it grows. At Twitter, we’re constantly iterating on the in-house and open source JavaScript tools we use to address this need. This was Twitter’s first engineering open house, but it certainly won’t be our last. We plan to hold these regularly - every couple months or so. In the meantime, if you’re interested in keeping up with our engineering team, you can follow @twittereng or check out our jobs page . - Mike Abbott (@ mabb0tt ), VP Engineering", "date": "2011-06-22"},
{"website": "Twitter-Engineering", "title": "Fast Core Animation UI for the Mac", "author": ["Revenue Platform"], "link": "https://blog.twitter.com/engineering/en_us/a/2011/fast-core-animation-ui-for-the-mac.html", "abstract": "Starting today, Twitter is offering TwUI as an open-source framework (https://github.com/twitter/twui) for developing interfaces on the Mac. Until now, there was not a simple and effective way to design interactive, hardware-accelerated interfaces on the Mac. Core Animation can create hardware-accelerated drawings, but doesn’t provide interaction mechanisms. AppKit and NSView have excellent interaction mechanisms, but the drawings operations are CPU-bound, which makes fluid scrolling, animations, and other effects difficult – if not impossible – to accomplish. UIKit on Apple’s iOS platform has offered developers a fresh start. While UIKit borrows many ideas from AppKit regarding interaction, it can offload compositing to the GPU because it is built on top of Core Animation. This architecture has enabled developers to create many applications that were, until this time, impossible to build. TwUI as a solution TwUI brings the philosophy of UIKit to the desktop. It is built on top of Core Animation, and it borrows interaction ideas from AppKit. It allows for all the things Mac users expect, including drag & drop, mouse events, tooltips, Mac-like text selection, and so on. And, since TwUI isn’t bound by the constraints of an existing API, developers can experiment with new features like block-based drawRect and layout. How TwUI works You will recognize the fundamentals of TwUI if you are familiar with UIKit. For example, a “TUIView” is a simple, lightweight wrapper around a Core Animation layer – much like UIView on iOS. TUIView offers useful subclasses for operations such as scroll views, table views, buttons, and so on. More importantly, TwUI makes it easy to build your own custom interface components. And because all of these views are backed by layers, composited by Core Animation, your UI is rendered at optimal speed. Xcode running the TwUI example project Ongoing development Since TwUI forms the basis of Twitter for the Mac, it is an integral part of our shipping code. Going forward, we need to stress test it in several implementations. We’ll continue to develop additional features and make improvements. And, we encourage you to experiment, as that will help us build a robust and exciting UI framework for the Mac. Acknowledgements The following engineers were mainly responsible for the TwUI development: -Loren Brichter (@ lorenb ), Ben Sandofsky (@ sandofsky )", "date": "2011-07-01"},
{"website": "Twitter-Engineering", "title": "Finagle: A Protocol-Agnostic RPC System", "author": ["Revenue Platform"], "link": "https://blog.twitter.com/engineering/en_us/a/2011/finagle-a-protocol-agnostic-rpc-system.html", "abstract": "Finagle is a protocol-agnostic, asynchronous RPC system for the JVM that makes it easy to build robust clients and servers in Java, Scala, or any JVM-hosted language. Rendering even the simplest web page on twitter.com requires the collaboration of dozens of network services speaking many different protocols. For example, in order to render the home page, the application issues requests to the Social Graph Service, Memcached, databases, and many other network services. Each of these speaks a different protocol: Thrift, Memcached, MySQL, and so on. Additionally, many of these services speak to other services — they are both servers and clients. The Social Graph Service, for instance, provides a Thrift interface but consumes from a cluster of MySQL databases. In such systems, a frequent cause of outages is poor interaction between components in the presence of failures; common failures include crashed hosts and extreme latency variance. These failures can cascade through the system by causing work queues to back up, TCP connections to churn, or memory and file descriptors to become exhausted. In the worst case, the user sees a Fail Whale. Challenges of building a stable distributed system Sophisticated network servers and clients have many moving parts: failure detectors, load-balancers, failover strategies, and so on. These parts need to work together in a delicate balance to be resilient to the varieties of failure that occur in a large production system. This is made especially difficult by the many different implementations of failure detectors, load-balancers, and so on, per protocol. For example, the implementation of the back-pressure strategies for Thrift differ from those for HTTP. Ensuring that heterogeneous systems converge to a stable state during an incident is extremely challenging. Our approach We set out to develop a single implementation of the basic components of network servers and clients that could be used for all of our protocols. Finagle is a protocol-agnostic, asynchronous Remote Procedure Call (RPC) system for the Java Virtual Machine (JVM) that makes it easy to build robust clients and servers in Java, Scala, or any JVM-hosted language. Finagle supports a wide variety of request/response- oriented RPC protocols and many classes of streaming protocols. Finagle provides a robust implementation of: connection pools, with throttling to avoid TCP connection churn; failure detectors, to identify slow or crashed hosts; failover strategies, to direct traffic away from unhealthy hosts; load-balancers, including “least-connections” and other strategies; and back-pressure techniques, to defend servers against abusive clients and dogpiling. Additionally, Finagle makes it easier to build and deploy a service that publishes standard statistics, logs, and exception reports; supports distributed tracing (a la Dapper) across protocols; optionally uses ZooKeeper for cluster management; and supports common sharding strategies. We believe our work has paid off — we can now write and deploy a network service with much greater ease and safety. Finagle at Twitter Today, Finagle is deployed in production at Twitter in several front- and back-end serving systems, including our URL crawler and HTTP Proxy. We plan to continue deploying Finagle more widely. A Finagle-based architecture (under development) The diagram illustrates a future architecture that uses Finagle pervasively. For example, the User Service is a Finagle server that uses a Finagle memcached client, and speaks to a Finagle Kestrel service. How Finagle works Finagle is flexible and easy to use because it is designed around a few simple, composable primitives: Futures , Services , and Filters . Future objects In Finagle, Future objects are the unifying abstraction for all asynchronous computation. A Future represents a computation that may not yet have completed and that can either succeed or fail. The two most basic ways to use a Future are to: block and wait for the computation to return register a callback to be invoked when the computation eventually succeeds or fails Future callbacks In cases where execution should continue asynchronously upon completion of a computation, you can specify a success and a failure callback. Callbacks are registered via the onSuccess and onFailure methods: val request: HttpRequest =\n  new DefaultHttpRequest(HTTP_1_1, GET, \"/\")\nval responseFuture: Future[HttpResponse] = client(request)\n\nresponseFuture onSuccess { responseFuture =>\n  println(responseFuture)\n} onFailure { exception =>\n  println(exception)\n} Composing Futures Futures can be combined and transformed in interesting ways, leading to the kind of compositional behavior commonly seen in functional programming languages. For instance, you can convert a Future[String] to a Future[Int] by using map : val stringFuture: Future[String] = Future(\"1\")\n  val intFuture: Future[Int] = stringFuture map { string =>\n    string.toInt\n  } Similarly, you can use flatMap to easily pipeline a sequence of Futures : val authenticatedUser: Future[User] =\n    User.authenticate(email, password)\n\nval lookupTweets: Future[Seq[Tweet]] =\n    authenticatedUser flatMap { user =>\n      Tweet.findAllByUser(user)\n    } In this example, User.authenticate() is performed asynchronously; Tweet.findAllByUser() is invoked on its eventual result. This is alternatively expressed in Scala, using the for statement: for {\n   user <- User.authenticate(email, password)\n   tweets <- Tweet.findAllByUser(user)\n} yield tweets Handling errors and exceptions is very easy when Futures are pipelined using flatMap or the for statement. In the above example, if User.authenticate() asynchronously raises an exception, the subsequent call to Tweet.findAllByUser() never happens. Instead, the result of the pipelined expression is still of the type Future[Seq[Tweet]] , but it contains the exceptional value rather than tweets. You can respond to the exception using the onFailure callback or other compositional techniques. A nice property of Futures , as compared to other asynchronous programming techniques (such as the continuation passing style), is that you an easily write clear and robust asynchronous code, even with more sophisticated operations such as scatter/gather: val severalFutures = Seq[Future[Int]] =\n   Seq(Tweet.find(1), Tweet.find(2), …)\nval combinedFuture: Future[Seq[Int]] =\n   Future.collect(severalFutures) Service objects A Service is a function that receives a request and returns a Future object as a response. Note that both clients and servers are represented as Service objects. To create a Server , you extend the abstract Service class and listen on a port. Here is a simple HTTP server listening on port 10000: val service = new Service[HttpRequest, HttpResponse] {\n  def apply(request: HttpRequest) =\n    Future(new DefaultHttpResponse(HTTP_1_1, OK))\n}\n\nval address = new InetSocketAddress(10000)\n\nval server: Server[HttpRequest, HttpResponse] = ServerBuilder()\n  .name(\"MyWebServer\")\n  .codec(Http())\n  .bindTo(address)\n  .build(service) Building an HTTP client is even easier: val client: Service[HttpRequest, HttpResponse] = ClientBuilder()\n  .codec(Http())\n  .hosts(address)\n  .build()\n\n// Issue a request, get a response:\nval request: HttpRequest =\n  new DefaultHttpRequest(HTTP_1_1, GET, \"/\")\n\nclient(request) onSuccess { response =>\n  println(\"Received response: \" + response)\n} Filter objects Filters are a useful way to isolate distinct phases of your application into a pipeline. For example, you may need to handle exceptions, authorization, and so forth before your Service responds to a request. A Filter wraps a Service and, potentially, converts the input and output types of the Service to other types. In other words, a Filter is a Service transformer. Here is a filter that ensures an HTTP request has valid OAuth credentials that uses an asynchronous authenticator service: class RequireAuthentication(a: Authenticator) extends Filter[...] {\n  def apply(\n    request: Request,\n    continue: Service[AuthenticatedRequest, HttpResponse]\n  ) = {\n      a.authenticate(request) flatMap {\n        case AuthResult(OK, passport) =>\n          continue(AuthenticatedRequest(request, passport))\n        case ar: AuthResult(Error(code)) =>\n          Future.exception(new RequestUnauthenticated(code))\n    }\n  }\n} A Filter then decorates a Service , as in this example: val baseService = new Service[HttpRequest, HttpResponse] {\n  def apply(request: HttpRequest) =\n    Future(new DefaultHttpResponse(HTTP_1_1, OK))\n}\n\nval authorize = new RequireAuthorization(…)\nval handleExceptions = new HandleExceptions(...)\n\nval decoratedService: Service[HttpRequest, HttpResponse] =\n  handleExceptions andThen authorize andThen baseService Finagle is an open source project, available under the Apache License, Version 2.0. Source code and documentation are available on GitHub. Acknowledgements Finagle was originally conceived by Marius Eriksen and Nick Kallen. Other key contributors are Arya Asemanfar, David Helder, Evan Meagher, Gary McCue, Glen Sanford, Grant Monroe, Ian Ownbey, Jake Donham, James Waldrop, Jeremy Cloud, Johan Oskarsson, Justin Zhu, Raghavendra Prabhu, Robey Pointer, Ryan King, Sam Whitlock, Steve Jenson, Wanli Yang, Wilhelm Bierbaum, William Morgan, Abhi Khune, and Srini Rajagopal.", "date": "2011-08-19"},
{"website": "Twitter-Engineering", "title": "A Storm is coming: more details and plans for release", "author": ["Revenue Platform"], "link": "https://blog.twitter.com/engineering/en_us/a/2011/a-storm-is-coming-more-details-and-plans-for-release.html", "abstract": "We’ve received a lot of questions about what’s going to happen to Storm now that BackType has been acquired by Twitter. I’m pleased to announce that I will be releasing Storm at Strange Loop on September 19th! Check out the session info for more details. In my preview post about Storm, I discussed how Storm can be applied to a huge variety of realtime computation problems. In this post, I’ll give more details on Storm and what it’s like to use. Here’s a recap of the three broad use cases for Storm: Stream processing: Storm can be used to process a stream of new data and update databases in realtime. Unlike the standard approach of doing stream processing with a network of queues and workers, Storm is fault-tolerant and scalable. Continuous computation: Storm can do a continuous query and stream the results to clients in realtime. An example is streaming trending topics on Twitter into browsers. The browsers will have a realtime view on what the trending topics are as they happen. Distributed RPC: Storm can be used to parallelize an intense query on the fly. The idea is that your Storm topology is a distributed function that waits for invocation messages. When it receives an invocation, it computes the query and sends back the results. Examples of Distributed RPC are parallelizing search queries or doing set operations on large numbers of large sets. The beauty of Storm is that it’s able to solve such a wide variety of use cases with just a simple set of primitives. Components of a Storm cluster A Storm cluster is superficially similar to a Hadoop cluster. Whereas on Hadoop you run “MapReduce jobs”, on Storm you run “topologies”. “Jobs” and “topologies” themselves are very different — one key difference is that a MapReduce job eventually finishes, whereas a topology processes messages forever (or until you kill it). There are two kinds of nodes on a Storm cluster: the master node and the worker nodes. The master node runs a daemon called “Nimbus” that is similar to Hadoop’s “JobTracker”. Nimbus is responsible for distributing code around the cluster, assigning tasks to machines, and monitoring for failures. Each worker node runs a daemon called the “Supervisor”. The supervisor listens for work assigned to its machine and starts and stops worker processes as necessary based on what Nimbus has assigned to it. Each worker process executes a subset of a topology; a running topology consists of many worker processes spread across many machines. All coordination between Nimbus and the Supervisors is done through a Zookeeper cluster. Additionally, the Nimbus daemon and Supervisor daemons are fail-fast and stateless; all state is kept in Zookeeper or on local disk. This means you can kill -9 Nimbus or the Supervisors and they’ll start back up like nothing happened. This design leads to Storm clusters being incredibly stable. We’ve had topologies running for months without requiring any maintenance. Running a Storm topology Running a topology is straightforward. First, you package all your code and dependencies into a single jar. Then, you run a command like the following: storm jar all-my-code.jar backtype.storm.MyTopology arg1 arg2 This runs the class backtype.storm.MyTopology with the arguments arg1 and arg2. The main function of the class defines the topology and submits it to Nimbus. The storm jar part takes care of connecting to Nimbus and uploading the jar. Since topology definitions are just Thrift structs, and Nimbus is a Thrift service, you can create and submit topologies using any programming language. The above example is the easiest way to do it from a JVM-based language. Streams and Topologies Let’s dig into the abstractions Storm exposes for doing scalable realtime computation. After I go over the main abstractions, I’ll tie everything together with a concrete example of a Storm topology. The core abstraction in Storm is the “stream”. A stream is an unbounded sequence of tuples. Storm provides the primitives for transforming a stream into a new stream in a distributed and reliable way. For example, you may transform a stream of tweets into a stream of trending topics. The basic primitives Storm provides for doing stream transformations are “spouts” and “bolts”. Spouts and bolts have interfaces that you implement to run your application-specific logic. A spout is a source of streams. For example, a spout may read tuples off of a Kestrel queue and emit them as a stream. Or a spout may connect to the Twitter API and emit a stream of tweets. A bolt does single-step stream transformations. It creates new streams based on its input streams. Complex stream transformations, like computing a stream of trending topics from a stream of tweets, require multiple steps and thus multiple bolts. Multi-step stream transformations are packaged into a “topology” which is the top-level abstraction that you submit to Storm clusters for execution. A topology is a graph of stream transformations where each node is a spout or bolt. Edges in the graph indicate which bolts are subscribing to which streams. When a spout or bolt emits a tuple to a stream, it sends the tuple to every bolt that subscribed to that stream. Everything in Storm runs in parallel in a distributed way. Spouts and bolts execute as many threads across the cluster, and they pass messages to each other in a distributed way. Messages never pass through any sort of central router, and there are no intermediate queues. A tuple is passed directly from the thread who created it to the threads that need to consume it. Storm guarantees that every message flowing through a topology will be processed, even if a machine goes down and the messages it was processing get dropped. How Storm accomplishes this without any intermediate queuing is the key to how it works and what makes it so fast. Let’s look at a concrete example of spouts, bolts, and topologies to solidify the concepts. A simple example topology The example topology I’m going to show is “streaming word count”. The topology contains a spout that emits sentences, and the final bolt emits the number of times each word has appeared across all sentences. Every time the count for a word is updated, a new count is emitted for it. The topology looks like this: Here’s how you define this topology in Java: The spout for this topology reads sentences off of the “sentence_queue” on a Kestrel server located at kestrel.backtype.com on port 22133. The spout is inserted into the topology with a unique id using the setSpout method. Every node in the topology must be given an id, and the id is used by other bolts to subscribe to that node’s output streams. The KestrelSpout is given the id “1” in this topology. setBolt is used to insert bolts in the topology. The first bolt defined in this topology is the SplitSentence bolt. This bolt transforms a stream of sentences into a stream of words. Let’s take a look at the implementation of SplitSentence: The key method is the execute method. As you can see, it splits the sentence into words and emits each word as a new tuple. Another important method is declareOutputFields, which declares the schema for the bolt’s output tuples. Here it declares that it emits 1-tuples with a field called “word”. Bolts can be implemented in any language. Here is the same bolt implemented in Python: The last parameter to setBolt is the amount of parallelism you want for the bolt. The SplitSentence bolt is given a parallelism of 10 which will result in 10 threads executing the bolt in parallel across the Storm cluster. To scale a topology, all you have to do is increase the parallelism for the bolts at the bottleneck of the topology. The setBolt method returns an object that you use to declare the inputs for the bolt. Continuing with the example, the SplitSentence bolt subscribes to the output stream of component “1” using a shuffle grouping. “1” refers to the KestrelSpout that was already defined. I’ll explain the shuffle grouping part in a moment. What matters so far is that the SplitSentence bolt will consume every tuple emitted by the KestrelSpout. A bolt can subscribe to multiple input streams by chaining input declarations, like so: You would use this functionality to implement a streaming join, for instance. The final bolt in the streaming word count topology, WordCount, reads in the words emitted by SplitSentence and emits updated counts for each word. Here’s the implementation of WordCount: WordCount maintains a map in memory from word to count. Whenever it sees a word, it updates the count for the word in its internal map and then emits the updated count as a new tuple. Finally, in declareOutputFields the bolt declares that it emits a stream of 2-tuples named “word” and “count”. The internal map kept in memory will be lost if the task dies. If it’s important that the bolt’s state persist even if a task dies, you can use an external database like Riak, Cassandra, or Memcached to store the state for the word counts. An in-memory HashMap is used here for simplicity purposes. Finally, the WordCount bolt declares its input as coming from component 2, the SplitSentence bolt. It consumes that stream using a “fields grouping” on the “word” field. “Fields grouping”, like the “shuffle grouping” that I glossed over before, is a type of “stream grouping”. “Stream groupings” are the final piece that ties topologies together. Stream groupings A stream grouping tells a topology how to send tuples between two components. Remember, spouts and bolts execute in parallel as many tasks across the cluster. If you look at how a topology is executing at the task level, it looks something like this: When a task for Bolt A emits a tuple to Bolt B, which task should it send the tuple to? A “stream grouping” answers this question by telling Storm how to send tuples between sets of tasks. There’s a few different kinds of stream groupings. The simplest kind of grouping is called a “shuffle grouping” which sends the tuple to a random task. A shuffle grouping is used in the streaming word count topology to send tuples from KestrelSpout to the SplitSentence bolt. It has the effect of evenly distributing the work of processing the tuples across all of SplitSentence bolt’s tasks. A more interesting kind of grouping is the “fields grouping”. A fields grouping is used between the SplitSentence bolt and the WordCount bolt. It is critical for the functioning of the WordCount bolt that the same word always go to the same task. Otherwise, more than one task will see the same word, and they’ll each emit incorrect values for the count since each has incomplete information. A fields grouping lets you group a stream by a subset of its fields. This causes equal values for that subset of fields to go to the same task. Since WordCount subscribes to SplitSentence’s output stream using a fields grouping on the “word” field, the same word always goes to the same task and the bolt produces the correct output. Fields groupings are the basis of implementing streaming joins and streaming aggregations as well as a plethora of other use cases. Underneath the hood, fields groupings are implemented using consistent hashing. There are a few other kinds of groupings, but talking about those is beyond the scope of this post. With that, you should now have everything you need to understand the streaming word count topology. The topology doesn’t require that much code, and it’s completely scalable and fault-tolerant. Whether you’re processing 10 messages per second or 100K messages per second, this topology can scale up or down as necessary by just tweaking the amount of parallelism for each component. The complexity that Storm hides The abstractions that Storm provides are ultimately pretty simple. A topology is composed of spouts and bolts that you connect together with stream groupings to get data flowing. You specify how much parallelism you want for each component, package everything into a jar, submit the topology and code to Nimbus, and Storm keeps your topology running forever. Here’s a glimpse at what Storm does underneath the hood to implement these abstractions in an extremely robust way. Guaranteed message processing: Storm guarantees that each tuple coming off a spout will be fully processed by the topology. To do this, Storm tracks the tree of messages that a tuple triggers. If a tuple fails to be fully processed, Storm will replay the tuple from the Spout. Storm incorporates some clever tricks to track the tree of messages in an efficient way. Robust process management: One of Storm’s main tasks is managing processes around the cluster. When a new worker is assigned to a supervisor, that worker should be started as quickly as possible. When that worker is no longer assigned to that supervisor, it should be killed and cleaned up. An example of a system that does this poorly is Hadoop. When Hadoop launches a task, the burden for the task to exit is on the task itself. Unfortunately, tasks sometimes fail to exit and become orphan processes, sucking up memory and resources from other tasks. In Storm, the burden of killing a worker process is on the supervisor that launched it. Orphaned tasks simply cannot happen with Storm, no matter how much you stress the machine or how many errors there are. Accomplishing this is tricky because Storm needs to track not just the worker processes it launches, but also subprocesses launched by the workers (a subprocess is launched when a bolt is written in another language). The nimbus daemon and supervisor daemons are stateless and fail-fast. If they die, the running topologies aren’t affected. The daemons just start back up like nothing happened. This is again in contrast to how Hadoop works. Fault detection and automatic reassignment: Tasks in a running topology heartbeat to Nimbus to indicate that they are running smoothly. Nimbus monitors heartbeats and will reassign tasks that have timed out. Additionally, all the tasks throughout the cluster that were sending messages to the failed tasks quickly reconnect to the new location of the tasks. Efficient message passing: No intermediate queuing is used for message passing between tasks. Instead, messages are passed directly between tasks using ZeroMQ . This is simpler and way more efficient than using intermediate queuing. ZeroMQ is a clever “super-socket” library that employs a number of tricks for maximizing the throughput of messages. For example, it will detect if the network is busy and automatically batch messages to the destination. Another important part of message passing between processes is serializing and deserializing messages in an efficient way. Again, Storm automates this for you. By default, you can use any primitive type, strings, or binary records within tuples. If you want to be able to use another type, you just need to implement a simple interface to tell Storm how to serialize it. Then, whenever Storm encounters that type, it will automatically use that serializer. Local mode and distributed mode: Storm has a “local mode” where it simulates a Storm cluster completely in-process. This lets you iterate on your topologies quickly and write unit tests for your topologies. You can run the same code in local mode as you run on the cluster. Storm is easy to use, configure, and operate. It is accessible for everyone from the individual developer processing a few hundred messages per second to the large company processing hundreds of thousands of messages per second. Relation to “Complex Event Processing” Storm exists in the same space as “Complex Event Processing” systems like Esper , Streambase , and S4 . Among these, the most closely comparable system is S4. The biggest difference between Storm and S4 is that Storm guarantees messages will be processed even in the face of failures whereas S4 will sometimes lose messages. Some CEP systems have a built-in data storage layer. With Storm, you would use an external database like Cassandra or Riak alongside your topologies. It’s impossible for one data storage system to satisfy all applications since different applications have different data models and access patterns. Storm is a computation system and not a storage system. However, Storm does have some powerful facilities for achieving data locality even when using an external database. Summary I’ve only scratched the surface on Storm. The “stream” concept at the core of Storm can be taken so much further than what I’ve shown here — I didn’t talk about things like multi-streams, implicit streams, or direct groupings. I showed two of Storm’s main abstractions, spouts and bolts, but I didn’t talk about Storm’s third, and possibly most powerful abstraction, the “state spout”. I didn’t show how you do distributed RPC over Storm, and I didn’t discuss Storm’s awesome automated deploy that lets you create a Storm cluster on EC2 with just the click of a button. For all that, you’re going to have to wait until September 19th. Until then, I will be working on adding documentation to Storm so that you can get up and running with it quickly once it’s released. We’re excited to release Storm, and I hope to see you there at Strange Loop when it happens. - Nathan Marz ( @nathanmarz )", "date": "2011-08-04"},
{"website": "Twitter-Engineering", "title": "Twitter’s mobile web app delivers performance", "author": ["Revenue Platform"], "link": "https://blog.twitter.com/engineering/en_us/a/2011/twitter-s-mobile-web-app-delivers-performance.html", "abstract": "As the number of people using Twitter has grown, we’ve wanted to make sure that we deliver the best possible experience to users, regardless of platform or device. Since twitter.com is not optimized for smaller screens or touch interactions familiar to many smart phones, we decided to build a cross-platform web application that felt native in its responsiveness and speed for those who prefer accessing Twitter on their phone’s or the tablet’s browser. A better mobile user experience When building mobile.twitter.com as a web client, we used many of the tools offered in HTML5, CSS3, and JavaScript to develop an application that has the same look, feel, and performance of a native mobile application. This post focuses on four primary areas of the mobile app architecture that enabled us to meet our performance and usability goals: event listeners scroll views templates storage Twitter’s mobile app architecture Event listener For the Twitter application to feel native, responses have to be immediate. The web application delivers this experience by using event listeners in its code. Traditionally, Javascript uses DOM-only events such as onclick, mouseover, mouseout, focus, and blur to render a page. However, because Twitter has so many unique points of interaction, we decided to optimize the resources presented to us with mobile devices. The web application we developed uses event listeners throughout the code. These syntactic events, loaded with the JavaScript on the client, listen for unique triggers that are fired, following the users’ interactions. When users retweet or favorite Tweets, the JavaScript listens for those events and responds accordingly throughout the application, updating screen views where necessary. The client-side JavaScript on the mobile application handles communication with Twitter through the Twitter API. To illustrate the use of event listeners, let’s look at how a Retweet works. When a user clicks the Retweet button on the UI, the system fires a click event that fires a Retweet request through the API. The web client application listens for an event like a Retweet and updates the rest of the application when it receives it. When that Retweet event is successful, a return event fires off a signal and the web app listens for a successful Retweet notification. When it receives the notification, the rest of the application updates appropriately. The web app’s architecture ensures that while the user-facing layer for the various web apps may differ, the app reuses the custom event listeners throughout, thus making it possible to scale across all devices. For instance, both the iPhone and the iPad use the same views and modules, but in different navigation contexts, while the event architecture drives the rest of the application. ScrollViews Mobile browsers use a viewport on top of the browser’s window to allow the user to zoom and scroll the content of an entire page. As helpful as this is, the viewport prevents the web pages from using fixed positioned elements and scrolling gestures. Both of these provide a better user experience because the app header is fixed and you can fit more content in a smaller area. We worked around the limitations of native scrolling by writing a ScrollView component that allows users to scroll the content using JavaScript and CSS Transforms and Transitions. The CSS Transforms uses the device’s GPU to mimic the browser’s viewport. ScrollView adds a scrolling element and a wrapper container to the element that you wish to scroll. The wrapper container has a fixed width and height so that the inner contents can overflow. The JavaScript calculates the amount of pixels that overflow and moves the scroll element, using CSS Transforms. ScrollView listens for three events, onTouchStart , onTouchMove , and onTouchEnd to render a smooth animation: onTouchStart The mobile site stores the initial touch position, timestamp and other variables that it will use later to calculate the distance and velocity of the scroll. onTouchMove Next, the web app simply moves the scroll element by the delta between the start and the current positions. onTouchEnd Finally, the web app confirms if the scroll element has moved. If there was no movement, the application fires a click event that stops the scrolling action. If the scroll element moved, it calculates the distance and the speed to generate inertial scrolling, which fires a timer. When the timer fires, the application uses CSS Transforms to move the scroll element to the new position while it decreases the velocity logarithmically. Once the velocity reaches a minimum speed, the application cancels the timer and completes the animation. During this process, it takes into account important coordinates to calculate the elasticity when the user scrolls past the lower or the upper boundary of the scroll element. ScrollView is used to specify which content is scrollable. It can also be used to fix the navigation header to the top of the window to implement Pull-To-Refresh and infinite Tweet timelines. Templates One of the many customized solutions unique to Twitter and its user experience is a templating system. Templating is a two-pass process. During the first pass, the app expands the templates and marks the places in those resulting strings where dynamic data needs to go. The app then caches the results of the first pass. When it does a second pass to add dynamic data, the app references the cache, delivering a substantial performance benefit. Efficient storage In addition to custom events, we reexamined the use of storage available to the web app from the native browser. Since 15 percent of all mobile applications are launched when the device is offline, the solution needed to cover both online and offline instances. Twitter’s new mobile web app makes use of the HTML5’s app cache, which allows you to specify which files the browser should cache and make available to offline users. Using app cache also helps limit the amount of network activity. You can specify in your manifest file what to store; these include items such as the master index file, sprites, and other assets. When a user loads a page, the web app shows the assets from app cache; it stores the new assets when the manifest gets updated. This ensures the web app can be used even when it is offline, since an updated manifest is always waiting in the cache. The web app also uses local storage for simple items, such as user settings, user information, and strings, that are persistent throughout the application for immediate access. It uses a SQL database to handle Tweets and Profiles. Within the schema, each storage database gets a name based on the user, allowing for very quick joins between tables. Separate user tables allow for encapsulation and provide the ability to bundle data securely, by user. Given the growing use cases for devices like an iPad, especially in a multilingual setting, this innovation allows for two people using separate languages to receive all the translated strings cached per user on the same device. In addition to using storage elements of the HTML5 spec, Twitter’s mobile application also makes use of some of the best tools of CSS3. This list includes Flex box model Gradients Shadows 3D transforms Transitions Animations Future direction The event framework gives us a scalable way to grow this product over time. Our goal is to add support for new devices as well as build new user-facing features and elements. We will invest in both native applications and the web. In cases where we can or should go native, we will, but in many cases we believe our web app provides an optimal approach for serving a broad set of users. Acknowledgements Twitter’s HTML5 mobile application was developed by Manuel Deschamps (@ manuel ) and designed by Bryan Haggerty (@ bhaggs ). Mark Percival (@ mdp ) contributed to the coding of the mobile architecture.", "date": "2011-09-14"},
{"website": "Twitter-Engineering", "title": "2011", "author": ["‎@‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2011.html", "abstract": "", "date": "1970-01-01"},
{"website": "Twitter-Engineering", "title": "SpiderDuck: Twitter's Real-time URL Fetcher", "author": ["Revenue Platform"], "link": "https://blog.twitter.com/engineering/en_us/a/2011/spiderduck-twitters-real-time-url-fetcher.html", "abstract": "Tweets often contain URLs or links to a variety of content on the web, including images, videos, news articles and blog posts. SpiderDuck is a service at Twitter that fetches all URLs shared in Tweets in real-time, parses the downloaded content to extract metadata of interest and makes that metadata available for other Twitter services to consume within seconds. Several teams at Twitter need to access the linked content, typically in real-time, to improve Twitter products. For example: Search to index resolved URLs and improve relevance Clients to display certain types of media, such as photos, next to the Tweet Tweet Button to count how many times each URL has been shared on Twitter Trust & Safety to aid in detecting malware and spam Analytics to surface a variety of aggregated statistics about links shared on Twitter Background Prior to SpiderDuck, Twitter had a service that resolved all URLs shared in Tweets by issuing HEAD requests and following redirects. While this service was simple and met the needs of the company at the time, it had a few limitations: It resolved the URLs but did not actually download the content. The resolution information was stored in an in-memory cache but not persisted durably to disk. This meant that if the in-memory cache instance was restarted, data would be lost. It did not implement politeness rules typical of modern bots, for example, rate limiting and following robots.txt directives. Clearly, we needed to build a real URL fetcher that overcame the above limitations and would meet the company’s needs in the long term. Our first thought was to use or build on top of an existing open source URL crawler. We realized though that almost all of the available crawlers have two properties that we didn’t need: They are recursive crawlers. That is, they are designed to fetch pages and then recursively crawl the links extracted from those pages. Recursive crawling involves significant complexity in crawl scheduling and long term queuing, which isn’t relevant to our use case. They are optimized for large batch crawls. What we needed was a fast, real-time URL fetcher. Therefore, we decided to design a new system that could meet Twitter’s real-time needs and scale horizontally with its growth. Rather than reinvent the wheel, we built the new system largely on top of open source building blocks, thus still leveraging the contributions of the open source community. This is typical of many engineering problems at Twitter – while they resemble problems at other large Internet companies, the requirement that everything work in real-time introduces unique and interesting challenges. System Overview Here’s an overview of how SpiderDuck works. The following diagram illustrates its main components. The SpiderDuck architecture Kestrel: This is message queuing system widely used at Twitter for queuing incoming Tweets. Schedulers: These jobs determine whether to fetch a URL, schedule the fetch, follow redirect hops if any. After the fetch, they parse the downloaded content, extract metadata, and write the metadata to the Metadata Store and the raw content to the Content Store. Each scheduler performs its work independently of the others; that is, any number of schedulers can be added to horizontally scale the system as Tweet and URL volume grows. Fetchers: These are Thrift servers that maintain short-term fetch queues of URLs, issue the actual HTTP fetch requests and implement rate limiting and robots.txt processing. Like the Schedulers, Fetchers scale horizontally with fetch rate. Memcached: This is a distributed cache used by the fetchers to temporarily store robots.txt files. Metadata Store: This is a Cassandra -based distributed hash table that stores page metadata and resolution information keyed by URL, as well as fetch status for every URL recently encountered by the system. This store serves clients across Twitter that need real-time access to URL metadata. Content Store: This is an HDFS cluster for archiving downloaded content and all fetch information. We will now describe the two main components of SpiderDuck — the URL Scheduler and the URL Fetcher — in more detail. The URL Scheduler The following diagram illustrates the various stages of processing in the SpiderDuck Scheduler. The URL Scheduler Like most of SpiderDuck, the Scheduler is built on top of an open source asynchronous RPC framework developed at Twitter called Finagle . (In fact, this was one of the earliest projects to utilize Finagle.) Each box in the diagram above, except for the Kestrel Reader, is a Finagle Filter – an abstraction that allows a sequence of processing stages to be easily composed into a fully asynchronous pipeline. Being fully asynchronous allows SpiderDuck to handle high throughput with a small, fixed number of threads. The Kestrel Reader continuously polls for new Tweets. As Tweets come in, they are sent to the Tweet Processor, which extracts URLs from them. Each URL is then sent to the Crawl Decider stage. This stage reads the Fetch Status of the URL from the Metadata Store to check if and when SpiderDuck has seen the URL before. The Crawl Decider then decides whether the URL should be fetched based on a pre-defined fetch policy (that is, do not fetch if SpiderDuck has fetched it in the past X days). If the Decider determines to not fetch the URL, it logs the status to indicate that processing is complete. If it determines to fetch the URL, it sends the URL to the Fetcher Client stage. The Fetcher Client stage uses a client library to talk to the Fetchers. The client library implements the logic that determines which Fetcher will fetch a given URL; it also handles the processing of redirect hops. (It is typical to have a chain of redirects because URLs posted on Twitter are often shortened.) A context object is associated with each URL flowing through the Scheduler. The Fetcher Client adds all fetch information including status, downloaded headers, and content into the context object and passes it on to the Post Processor. The Post Processor runs the extracted page content through a metadata extractor library, which detects page encoding and parses the page with an open-source HTML5 parser. The extractor library implements a set of heuristics to retrieve page metadata such as title, description, and representative image. The Post Processor then writes all the metadata and fetch information into the Metadata Store. If necessary, the Post Processor can also schedule a set of dependent fetches. An example of dependent fetches is embedded media, such as images. After post-processing is complete, the URL context object is forwarded to the next stage that logs all the information, including full content, to the Content Store (HDFS) using an open source log aggregator called Scribe . This stage also notifies interested listeners that the URL processing is complete. The notification uses a simple Publish-Subscribe model, which is implemented using Kestrel’s fanout queues. All processing steps are executed asynchronously – no thread ever waits for a step to complete. All state related to each URL in flight is stored in the context object associated with it, which makes the threading model very simple. The asynchronous implementation also benefits from the convenient abstractions and constructs provided by Finagle and the Twitter Util libraries. The URL Fetcher Let’s take a look at how a Fetcher processes a URL. The URL Fetcher The Fetcher receives the URL through its Thrift interface. After basic validation, the Thrift handler passes the URL to a Request Queue Manager, which assigns it to the appropriate Request Queue. A scheduled task drains each Request Queue at a fixed rate. Once the URL is pulled off of its queue, it is sent to the HTTP Service for processing. The HTTP service, built on top of Finagle, first checks if the host associated with the URL is already in its cache. If not, it creates a Finagle client for it and schedules a robots.txt fetch. After the robots.txt is downloaded, the HTTP service fetches the permitted URL. The robots.txt file itself is cached, both in the in-process Host Cache as well as in Memcached to prevent its re-fetch for every new URL that the Fetcher encounters from that host. Tasks called Vultures periodically examine the Request Queues and Host Cache to find queues and hosts that haven’t been used for a period of time; when found, they are deleted. The Vultures also report useful stats through logs and the Twitter Commons stats exporting library. The Fetcher’s Request Queue serves an important purpose: rate limiting. SpiderDuck rate limits outgoing HTTP fetch requests per-domain so as not to overload web servers receiving requests. For accurate rate limiting, SpiderDuck ensures each Request Queue is assigned to exactly one Fetcher at any point of time, with automatic failover to a different Fetcher in case the assigned Fetcher fails. A cluster suite called Pacemaker assigns Request Queues to Fetchers and manages failover. URLs are assigned to Request Queues based on their domains by a Fetcher client library. The default rate limit used for all web sites can be overriden on a per-domain basis, as needed. The Fetchers also implement queue backoff logic. That is, if URLs are coming in faster than they can be drained, they reject requests to indicate to the client to backoff or take other suitable action. For security purposes, the Fetchers are deployed in a special zone in Twitter data centers called a DMZ. This means that the Fetchers cannot access Twitter’s production clusters and services. Hence, it is all the more important to keep them lightweight and self contained, a principle which guided many aspects of the design. How Twitter uses SpiderDuck Twitter services consume SpiderDuck data in a number of ways. Most query the Metadata Store directly to retrieve URL metadata (for example, page title) and resolution information (that is, the canonical URL after redirects). The Metadata Store is populated in real-time, typically seconds after the URL is tweeted. These services do not talk directly to Cassandra, but instead to SpiderDuck Thrift servers that proxy the requests. This intermediate layer provides SpiderDuck the flexibility to transparently switch storage systems, if necessary. It also supports an avenue for higher level API abstractions than what would be possible if the services interacted directly with Cassandra. Other services periodically process SpiderDuck logs in HDFS to generate aggregate stats for Twitter’s internal metrics dashboards or conduct other types of batch analyses. The dashboards help us answer questions like “How many images are shared on Twitter each day?” “What news sites do Twitter users most often link to?” and “How many URLs did we fetch yesterday from this specific website?” Note that services don’t typically tell SpiderDuck what to fetch; SpiderDuck fetches all URLs from incoming Tweets. Instead, services query information related to URLs after it becomes available. SpiderDuck also allows services to make requests directly to the Fetchers to fetch arbitrary content via HTTP (thus benefiting from our data center setup, rate limiting, robots.txt support and so on), but this use case is not common. Performance numbers SpiderDuck processes several hundred URLs every second. A majority of these are unique over the time window defined by SpiderDuck’s fetch policy, and hence get fetched. For URLs that get fetched, SpiderDuck’s median processing latency is under two seconds, and the 99th percentile processing latency is under five seconds. This latency is measured from Tweet creation time, which means that in under five seconds after a user clicked “Tweet,” the URL in that Tweet is extracted, prepared for fetch, all redirect hops are retrieved, the content is downloaded and parsed, and the metadata is extracted and made available to clients via the Metadata Store. Most of that time is spent either in the Fetcher Request Queues (due to rate limiting) or in actually fetching from the external web server. SpiderDuck itself adds no more than a few hundred milliseconds of processing overhead, most of which is spent in HTML parsing. SpiderDuck’s Cassandra-based Metadata Store handles close to 10,000 requests per second. Each request is typically for a single URL or a small batch (around 20 URLs), but it also processes large batch requests (200-300 URLs). The store’s median latency for reads is 4-5 milliseconds, and its 99th percentile is 50-60 milliseconds. Acknowledgements The SpiderDuck core team consisted of the following folks: Abhi Khune, Michael Busch, Paul Burstein, Raghavendra Prabhu, Tian Wang and Yi Zhuang. In addition, we’d like to acknowledge the following folks, spanning many teams across the company, who contributed to the project either directly, by helping with components SpiderDuck relies on (for example, Cassandra, Finagle, Pacemaker and Scribe) or with its unique data center setup: Alan Liang, Brady Catherman, Chris Goffinet, Dmitriy Ryaboy, Gilad Mishne, John Corwin, John Sirois, Jonathan Boulle, Jonathan Reichhold, Marius Eriksen, Nick Kallen, Ryan King, Samuel Luckenbill, Steve Jiang, Stu Hood and Travis Crawford. Thanks also to the entire Twitter Search team for their invaluable design feedback and support. If you want to work on projects like this, join the flock! - Raghavendra Prabhu (@ omrvp ), Software Engineer", "date": "2011-11-14"},
{"website": "Twitter-Engineering", "title": "Join the Flock: Twitter's International Engineering Open House", "author": ["Revenue Platform"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/join-the-flock-twitters-international-engineering-open-house.html", "abstract": "Next week, we will host our first Open House to present the achievements of the Twitter Translation Center , a community platform for translating Twitter’s products. Since it’s launch in early 2011, we have released Twitter in 22 languages, up from just six in 2010. The amazing level of activity in the Twitter Translation Center has driven us to explore new avenues to scale and deliver quality translation to our international users. For instance, we’ve learned how to work with a community of 425,000 translators, who have collectively produced more than one million separate translations and voted close to five million times on those translations. Recognizing this growth, we’ve added a lot of features to make the Translation Center a better platform for community translation; these innovations include forums, translation search, translation memory, glossary management, moderation tools, and spam and cross-site scripting prevention tools. We’ve also learnt that introducing a new language requires more than just translations. For each language, we had to ensure that we supported the appropriate date and number formats, that hashtags and URLs could be properly extracted from Tweets in those languages, and that we correctly counted the number of characters in each Tweet. In order to deliver quality, we built tools to test localized versions of our applications before a launch. Given that all of our translators are volunteers, we wanted to give our translator community a chance to review and test their output before they released it to our users. What’s next? Twitter’s impact on the world inspires us to look for new ways to connect and interact with our international users. We continue to address the challenges and lessons on how best to serve community localization at Twitter. To hear more about these tools and topics, join us for an evening of technical discussions and networking during our first International Engineering Open House. When Thursday, February 2, 2012; 7 pm - 9 pm Where Twitter HQ, 795 Folsom St, Suite 600, San Francisco, CA 94107 Speakers Nico Sallembien (@ nsallembien ): A look at Unicode character distribution in Tweets Laura Gomez (@ laura ): Community Localization and Twitter: Experience, Engagement, and Scalability Yoshimasa Niwa (@ niw ): Software development for Japanese mobile phones -@ international", "date": "2012-01-27"},
{"website": "Twitter-Engineering", "title": "Twitter NYC Open House", "author": ["Revenue Platform"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/twitter-nyc-open-house.html", "abstract": "When we opened an office in New York City last October, we were excited to become a part of the city’s growing tech community, with all of its energy and innovation. Since then, we’ve been building out an engineering team in New York. Focused on search and discovery, the team works to find ways to extract value out of the more than 250 million Tweets people send every day. We want to share some of the exciting projects that we’ve been working on, so we’re holding the first #TwitterNYC Engineering Open House. Come by to meet our engineering team, see some of our work, and learn about opportunities to #jointheflock! When Thursday, February 16, 2012; 7 pm - 9 pm Where Twitter NYC Speakers Daniel Loreto (@ DanielLoreto ): The Life of a Tweet Adam Schuck (@ AllSchuckUp ): Realtime Search at Twitter If you’re interested in attending, please send your name and company affiliation to openhouse@twitter.com. Space is very limited. If we have space for you, you’ll get a confirmation with more details, including the office address. If you don’t get in this time, we’ll notify you about future events.", "date": "2012-02-08"},
{"website": "Twitter-Engineering", "title": "Simple Strategies for Smooth Animation on the iPhone", "author": ["Revenue Platform"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/simple-strategies-for-smooth-animation-on-the-iphone.html", "abstract": "The iPhone was revolutionary for its use of direct manipulation – the feeling that you’re really holding content in your hands and manipulating it with your fingertips. While many mobile platforms have touch, it is the realistic physics and fluid animation of the iPhone that sets it apart from its competitors. However, jerky scrolling ruins the experience. The new UI of Twitter for iPhone 4.0 contains many details that could impact performance, so we had to treat 60 frame-per-second animation as a priority. If you are troubleshooting animation performance, this post should provide some useful pointers. A review of layers Animation on iOS is powered by Core Animation layers. Layers are a simple abstraction for working with the GPU. When animating layers, the GPU just transforms surfaces as an extended function of the hardware itself. However, the GPU is not optimized for drawing. Everything in your view’s drawRect: is handled by the CPU, then handed off to the GPU as a texture. Animation problems fall into one of those two phases in the pipeline. Either the GPU is being taxed by expensive operations, or the CPU is spending too much time preparing the cell before handing it off to the GPU. The following sections contain simple directives, based on how we addressed each of these challenges. GPU bottlenecks When the GPU is overburdened, it manifests with low, but consistent, framerates. The most common reasons may be excessive compositing, blending, or pixel misalignment. Consider the following Tweet: Use direct drawing A naive implementation of a Tweet cell might include a UILabel for the username, a UILabel for the tweet text, a UIImageView for the avatar, and so on. Unfortunately, each view burdens Core Animation with extra compositing. Instead, our Tweet cells contain a single view with no subviews; a single drawRect: draws everything. We institutionalized direct drawing by creating a generic table view cell class that accepts a block for its drawRect: method. This is, by far, the most commonly used cell in the app. Avoid blending You’ll notice that Tweets in Twitter for iPhone 4.0 have a drop shadow on top of a subtle textured background. This presented a challenge, as blending is expensive. We solved this by reducing the area Core Animation has to consider non-opaque, by splitting the shadow areas from content area of the cell. To quickly spot blending, select the Color Blended Layers option under Instruments in the Core Animation instrument. The green area indicates opaque; the red areas point to blended surfaces. Check pixel alignment Spot the danger in the following code: CGRect subframe = CGRectMake(x, y, width / 2.0, height / 2.0); If width is an odd number, then subFrame will have a fractional width. Core Animation will accept this, but it will require anti-aliasing, which is expensive. Instead, run floor or ceil on computed values. In Instruments, check Color Misaligned Images to hunt for accidental anti-aliasing. Cell preparation bottlenecks The second class of animation problem is called a “pop” and occurs when new cells scroll into view. When a cell is about to appear on screen, it only has 17 milliseconds to provide content before you’ve dropped a frame. Recycle cells As described in the table view documentation, instead of creating and destroying cell objects whenever they appear or disappear, you should recycle cells with the help of dequeueReusableCellWithIdentifier: Optimize your drawRect: If you are direct drawing and recycling cells, and you still see a pop, check the time of your drawRect: under Instruments in Core Animation. If needed, eliminate “nice to have” details, like subtle gradients. Pre-render if necessary Sometimes, you can’t simplify drawing. The new #Discover tab in Twitter for iPhone 4.0 displays large images in cells. No matter how simple the treatment, scaling and cropping a large image is expensive. We knew #Discover had an upper bound of ten stories, so we decided to trade memory for CPU. When we receive a trending story image we pre-render the cell on a low-priority background queue, and store it in a cache. When the cell scrolls into view, we set the cell’s layer.contents to the prebaked CGImage, which requires no drawing. Conclusion All of these optimizations come at the cost of code complexity and developer productivity. So long as you don’t paint yourself into a corner in architecture, you can always apply these optimizations after you’ve written the simplest thing that works and collected actual measurements on hardware. Remember: Premature optimization is the root of all evil. Acknowledgements -Ben Sandofsky (@ sandofsky ), Ryan Perry (@ ryfar ) for technical review, and the Twitter mobile team for their input.", "date": "2012-02-21"},
{"website": "Twitter-Engineering", "title": "Generating Recommendations with MapReduce and Scalding", "author": ["Revenue Platform"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/generating-recommendations-with-mapreduce-and-scalding.html", "abstract": "Scalding is an in-house MapReduce framework that Twitter recently open-sourced. Like Pig , it provides an abstraction on top of MapReduce that makes it easy to write big data jobs in a syntax that’s simple and concise. Unlike Pig, Scalding is written in pure Scala — which means all the power of Scala and the JVM is already built-in. No more UDFs, folks! At Twitter, our mission is to instantly connect people everywhere to what’s most meaningful to them. With over a hundred million active users creating more than 250 million tweets every day, this means we need to quickly analyze massive amounts of data at scale. That’s why we recently open-sourced Scalding, an in-house MapReduce framework built on top of Scala and Cascading. In 140: Instead of forcing you to write raw map and reduce functions, Scalding allows you to write natural code like: Simple to read, and just as easily run over a 10 line test file as a 10 terabyte data source in Hadoop! Like Twitter, Scalding has a powerful simplicity that we love, and in this post we’ll use the example of building a basic recommendation engine to show you why. A couple of notes before we begin: Scalding is open-source and lives here on Github . For a longer, tutorial-based version of this post (which goes more in-depth into the code and mathematics), see the original blog entry . We use Scalding hard and we use it often, for everything from custom ad targeting algorithms to PageRank on the Twitter graph, and we hope you will too. Let’s dive in! Movie similarities Imagine you run an online movie business. You have a rating system in place (people can rate movies with 1 to 5 stars) and you want to calculate similarities between pairs of movies, so that if someone watches The Lion King, you can recommend films like Toy Story. One way to define the similarity between two movies is to use their correlation: For every pair of movies A and B, find all the people who rated both A and B. Use these ratings to form a Movie A vector and a Movie B vector. Calculate the correlation between these two vectors. Whenever someone watches a movie, you can then recommend the movies most correlated with it. Here’s a snippet illustrating the code. Notice that Scalding provides higher-level functions like group for you (and many others, too, like join and filter ), so that you don’t have to continually rewrite these patterns yourself. What’s more, if there are other abstractions you’d like to add, go ahead! It’s easy to add new functions. Rotten Tomatoes Let’s run this code over some real data. What dataset of movie ratings should we use? My review for ‘How to Train Your Dragon’ on Rotten Tomatoes: 4 1/2 stars > bit.ly/xtw3d3 — Benjamin West ( @BenTheWest ) February 21, 2012 People love to tweet whenever they rate a movie on Rotten Tomatoes, so let’s use these ratings to generate our recommendations! After grabbing and parsing these tweets, we can run a quick command using the handy scald.rb script that Scalding provides. And minutes later, we’re done! As we’d expect, we see that Lord of the Rings, Harry Potter, and Star Wars movies are similar to other Lord of the Rings, Harry Potter, and Star Wars movies Big science fiction blockbusters (Avatar) are similar to big science fiction blockbusters (Inception) People who like one Justin Timberlake movie (Bad Teacher) also like other Justin Timberlake Movies (In Time). Similarly with Michael Fassbender (A Dangerous Method, Shame) Art house movies (The Tree of Life) stick together (Tinker Tailor Soldier Spy) Just for fun, let’s also look at the movies with the most negative correlation: The more you like loud and dirty popcorn movies (Thor) and vamp romance (Twilight), the less you like arthouse? Sounds good to me. Check-in similarities with Foursquare Scalding also makes it easy to abstract away our input format, so that we can grab data from wherever we want. Tweets, TSVs, MySQL tables, HDFS — no problem! And there’s no reason our code needs to be tied to movie recommendations in particular, so let’s switch it up. For example, let’s say we want to generate restaurant or tourist recommendations, and we have a bunch of information on who visits each location. I’m at Empire State Building (350 5th Ave., btwn 33rd & 34th St., New York) 4sq.com/zZ5xGd — Simon Ackerman ( @SimonAckerman ) February 8, 2012 Here, we simply create a new class that scrapes tweets for Foursquare check-in information… …and bam! Here are locations similar to the Empire State Building: Here are places you might want to check out, if you check-in at Bergdorf Goodman: And here’s where to go after the Statue of Liberty: Learn more about Scalding Hopefully this post gave you a taste of the awesomeness of Scalding. To learn more: Check out Scalding on Github . Read this Getting Started Guide on the Scalding wiki. See a longer version of this post here . Follow @Scalding on Twitter! Acknowledgements -Edwin Chen(@ edchedch ). A huge shoutout to Argyris Zymnis (@ argyris ), Avi Bryant (@ avibryant ), and Oscar Boykin (@ posco ), the mastermind hackers who have spent (and continue spending) unimaginable hours making Scalding a joy to use.", "date": "2012-03-02"},
{"website": "Twitter-Engineering", "title": "Cassovary: A Big Graph-Processing Library", "author": ["Revenue Platform"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/cassovary-a-big-graph-processing-library.html", "abstract": "We are open sourcing Cassovary , a big graph-processing library for the Java Virtual Machine (JVM) written in Scala. Cassovary is designed from the ground up to efficiently handle graphs with billions of edges. It comes with some common node and graph data structures and traversal algorithms. A typical usage is to do large-scale graph mining and analysis. At Twitter, Cassovary forms the bottom layer of a stack that we use to power many of our graph-based features, including “Who to Follow” and “Similar to.” We also use it for relevance in Twitter Search and the algorithms that determine which Promoted Products users will see. Over time, we hope to bring more non-proprietary logic from some of those product features into Cassovary. Please use, fork, and contribute to Cassovary if you can. If you have any questions, ask on the mailing list or file issues on GitHub. Also, follow @cassovary for updates. -Pankaj Gupta (@ pankaj )", "date": "2012-03-08"},
{"website": "Twitter-Engineering", "title": "Security Open House March 29", "author": ["Revenue Platform"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/security-open-house-march-29.html", "abstract": "The past few months have been busy for the Twitter security team: we’ve turned on HTTPS by default for everyone, added great engineers from Whisper Systems and Dasient, and had some stimulating internal discussions about how we can continue to better protect users. We want to share what we’ve been up to and discuss the world of online security, so we’ll be hosting a Security Open House on March 29 here at Twitter HQ. We’ve got a great lineup of speakers to get the conversations going: Neil Daswani ( @neildaswani ): Online fraud and mobile application abuse Jason Wiley ( @capnwiley ) & Dino Fekaris ( @dino ): Twitter phishing vectors and the fallout Neil Matatall ( @nilematotle ): Brakeman: detecting security vulnerabilities in Ruby on Rails applications via static analysis Come by to meet our Security team, hear about some of our work, and learn about opportunities to join the flock at the first #TwitterSec . Here’s what you need to know to get yourself signed up: When: Thursday, March 29, 2012; 5:30pm - 9:00pm Where: Twitter HQ - 795 Folsom Street, San Francisco, CA Who: Security and privacy engineers RSVP: Space is limited, so reserve your spot now . Hope to see you here!", "date": "2012-03-22"},
{"website": "Twitter-Engineering", "title": "MySQL at Twitter", "author": ["‎@Twitter‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/mysql-at-twitter.html", "abstract": "MySQL is the persistent storage technology behind most Twitter data: the interest graph, timelines, user data and the Tweets themselves. Due to our scale, we push MySQL a lot further than most companies. Of course, MySQL is open source software, so we have the ability to change it to suit our needs. Since we believe in sharing knowledge and that open source software facilitates innovation, we have decided to open source our MySQL work on GitHub under the BSD New license. The objectives of our work thus far has primarily been to improve the predictability of our services and make our lives easier. Some of the work we’ve done includes: Add additional status variables, particularly from the internals of InnoDB. This allows us to monitor our systems more effectively and understand their behavior better when handling production workloads. Optimize memory allocation on large NUMA systems: Allocate InnoDB’s buffer pool fully on startup, fail fast if memory is not available, ensure performance over time even when server is under memory pressure. Reduce unnecessary work through improved server-side statement timeout support. This allows the server to proactively cancel queries that run longer than a millisecond-granularity timeout. Export and restore InnoDB buffer pool in using a safe and lightweight method. This enables us to build tools to support rolling restarts of our services with minimal pain. Optimize MySQL for SSD-based machines, including page-flushing behavior and reduction in writes to disk to improve lifespan. We look forward sharing our work with upstream and other downstream MySQL vendors, with a goal to improve the MySQL community. For a more complete look at our work, please see the change history and documentation . If you want to learn more about our usage of MySQL, we will be speaking about Gizzard , our sharding and replication framework on top of MySQL, at the Percona Live MySQL Conference and Expo on April 12th. Finally, contact us on GitHub or file an issue if you have questions. On behalf of the Twitter DBA and DB development teams, - Jeremy Cole ( @jeremycole ) - Davi Arnaut ( @darnaut )", "date": "2012-04-09"},
{"website": "Twitter-Engineering", "title": "Introducing the Innovator’s Patent Agreement", "author": ["‎@Twitter‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/introducing-the-innovator-s-patent-agreement-0.html", "abstract": "Cross-posted on the Twitter Blog . One of the great things about Twitter is working with so many talented folks who dream up and build incredible products day in and day out. Like many companies, we apply for patents on a bunch of these inventions. However, we also think a lot about how those patents may be used in the future; we sometimes worry that they may be used to impede the innovation of others. For that reason, we are publishing a draft of the Innovator’s Patent Agreement, which we informally call the “IPA”. The IPA is a new way to do patent assignment that keeps control in the hands of engineers and designers. It is a commitment from Twitter to our employees that patents can only be used for defensive purposes. We will not use the patents from employees’ inventions in offensive litigation without their permission. What’s more, this control flows with the patents, so if we sold them to others, they could only use them as the inventor intended. This is a significant departure from the current state of affairs in the industry. Typically, engineers and designers sign an agreement with their company that irrevocably gives that company any patents filed related to the employee’s work. The company then has control over the patents and can use them however they want, which may include selling them to others who can also use them however they want. With the IPA, employees can be assured that their patents will be used only as a shield rather than as a weapon. We will implement the IPA later this year, and it will apply to all patents issued to our engineers, both past and present. We are still in early stages, and have just started to reach out to other companies to discuss the IPA and whether it might make sense for them too. In the meantime, we’ve posted the IPA on GitHub with the hope that you will take a look, share your feedback and discuss with your companies. And, of course, you can #jointheflock and have the IPA apply to you. Today is the second day of our quarterly Hack Week, which means employees – engineers, designers, and folks all across the company – are working on projects and tools outside their regular day-to-day work. The goal of this week is to give rise to the most audacious and creative ideas. These ideas will have the greatest impact in a world that fosters innovation, rather than dampening it, and we hope the IPA will play an important part in making that vision a reality. - Adam Messinger, VP of Engineering ( @adam_messinger )", "date": "2012-04-17"},
{"website": "Twitter-Engineering", "title": "Sponsoring the Apache Foundation", "author": ["‎@Twitter‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/sponsoring-the-apache-foundation.html", "abstract": "Open source is a pervasive part of our culture. Many projects at Twitter rely on open source technologies, and as we evolve as a company, our commitment to open source continues to increase. Today, we are becoming an official sponsor of the Apache Software Foundation (ASF), a non-profit and volunteer-run open source foundation. Starting today, we are sponsoring The Apache Foundation. We look forward to contributing more and increasing our commitment to @ TheASF — Twitter Open Source ( @TwitterOSS ) April 19, 2012 The ASF provides organizational, legal, and financial support for a broad range of open source software projects that Twitter consumes and contributes to. One example is the Mesos project, which is now being developed inside the ASF Incubator and is nearing its first official release. Within Twitter, Mesos runs on hundreds of production machines and makes it easier to execute clustered jobs that do everything from running services to handling our analytics workload. Sponsoring the ASF is not only the right thing to do, it will help us sustain our existing projects at the ASF by supporting the foundation’s infrastructure. We have a long history of contributing to Apache projects, including not only Mesos, but also Cassandra, Hadoop, Mahout, Pig and more. As Twitter grows, we look to further our commitment to the success of the ASF and other open source organizations. On behalf of the Twitter Open Source Office, - Chris Aniszczyk ( @cra )", "date": "2012-04-19"},
{"website": "Twitter-Engineering", "title": "Discover: Improved personalization algorithms and real-time indexing", "author": ["‎@Twitter‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/discover-improved-personalization-algorithms-and-real-time-indexing.html", "abstract": "We are beginning to roll out a new version of the Discover tab that is even more personalized for you. We’ve improved our personalization algorithms to incorporate several new signals including the accounts you follow and whom they follow. All of this social data is used to understand your interests and display stories that are relevant to you in real-time. Behind the scenes, the new Discover tab is powered by Earlybird , Twitter’s real-time search technology. When a user tweets, that Tweet is indexed and becomes searchable in seconds. Every Tweet with a link also goes through some additional processing: we extract and expand any URLs available in Tweets, and then fetch the contents of those URLs via SpiderDuck , our real-time URL fetcher. To generate the stories that are based on your social graph and that we believe are most interesting to you, we first use Cassovary , our graph processing library, to identify your connections and rank them according to how strong and important those connections are to you. Once we have that network, we use Twitter’s flexible search engine to find URLs that have been shared by that circle of people. Those links are converted into stories that we’ll display, alongside other stories, in the Discover tab. Before displaying them, a final ranking pass re-ranks stories according to how many people have tweeted about them and how important those people are in relation to you. All of this happens in near-real time, which means breaking and relevant stories appear in the new Discover tab almost as soon as people start talking about them. Our NYC engineering team, led by Daniel Loreto ( @DanielLoreto ), along with Julian Marinus ( @fooljulian ), Alec Thomas ( @alecthomas ), Dave Landau ( @landau ), and Ugo Di Girolamo ( @ugodiggi ), is working hard on Discover to create new ways to bring you instantly closer to the things you care about. This update is just the beginning of this ongoing effort. - Ori Allon, Director of Engineering ( @oriallon )", "date": "2012-05-01"},
{"website": "Twitter-Engineering", "title": "Incubating Apache Mesos", "author": ["‎@cra‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/incubating-apache-mesos.html", "abstract": "At Twitter, Apache Mesos runs on hundreds of production machines and makes it easier to execute jobs that do everything from running services to handling our analytics workload. For those not familiar with it, the Mesos project originally started as a UC Berkeley research effort. It is now being developed at the Apache Software Foundation (ASF), where it just reached its first release inside the Apache Incubator . Mesos aims to make it easier to build distributed applications and frameworks that share clustered resources like, CPU, RAM or hard disk space. There are Java, Python and C++ APIs for developing new parallel applications. Specifically, you can use Mesos to: Run Hadoop , Spark and other frameworks concurrently on a shared pool of nodes Run multiple instances of Hadoop on the same cluster to isolate production and experimental jobs, or even multiple versions of Hadoop Scale to 10,000s of nodes using fast, event-driven C++ implementation Run long-lived services (e.g., Hypertable and HBase) on the same nodes as batch applications and share resources between them Build new cluster computing frameworks without reinventing low-level facilities for farming out tasks, and have them coexist with existing ones View cluster status and information using a web user interface Mesos is being used at Conviva , UC Berkeley and UC San Francisco , as well as here. Some of our runtime systems engineers, specifically Benjamin Hindman ( @benh ), Bill Farner ( @wfarner ), Vinod Kone ( @vinodkone ), John Sirois ( @johnsirois ), Brian Wickman ( @wickman ), and Sathya Hariesh ( @sathya ) have worked hard to evolve Mesos and make it useful for our scalable engineering challenges. If you’re interested in Mesos, we invite you to try it out , follow @ApacheMesos , join the mailing list and help us develop a Mesos community within the ASF. — Chris Aniszczyk, Manager of Open Source ( @cra )", "date": "2012-05-10"},
{"website": "Twitter-Engineering", "title": "Related Queries and Spelling Corrections in Search", "author": ["‎@Twitter‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/related-queries-and-spelling-corrections-in-search.html", "abstract": "As you may have noticed, searches on twitter.com, Twitter for iOS, and Twitter for Android now have spelling corrections and related queries next to the search results. At the core of our related queries and spelling correction service is a simple mechanism: if we see query A in some context, and then see query B in the same context, we think they’re related. If A and B are similar, B may be a spell-corrected version of A; if they’re not, it may be interesting to searchers who find A interesting. We use both query sessions and tweets for context; if we observe a user typing [justin beiber] and then, within the same session, typing [justin bieber], we’ll consider the second query as a possible spelling correction to the first — and if the same session will also contain [selena gomez], we may consider this as a related query to the previous queries. The data we process is anonymized — we don’t track which queries are issued by a given user, only that the same (unknown) user has issued several queries in a row, or continuously tweeted. To measure the similarity between queries, we use a variant of Edit Distance tailored to Twitter queries; for example, in our variant we treat the beginning and end characters of a query differently from the inner characters, as spelling mistakes tend to be concentrated in those. Our variant also treats special Twitter characters (such as @ and #) differently from other characters, and has other differences from the vanilla Edit Distance. To measure the quality of the suggestions, we use a variety of signals including query frequencies (of the original query and the suggestion), statistical correlation measures such as log-likelihood , the quality of the search results for the suggestion, and others. Twitter’s spelling correction has a number of unique challenges: searchers frequently type in usernames or hashtags that are not well-formed English words; there is a real-time constancy of new lingo and terms supplied by our own users; and we want to help people find those in order to join in the conversation. To address all of these issues, on top of our context-based mechanism, we also index dictionaries of trending queries and popular users that are likely to be misspelled, and use Lucene’s built-in spelling correction library (tweaked to better serve our needs) to identify misspelling and retrieve corrections for queries. Initially, we started computing-related queries and spelling correction in a batch service, periodically updating our user-facing service with the latest data. But we’ve noticed that the lag this process introduced resulted in a less-than-optimal experience — it would take several hours for the models to adapt to new search trends. We then rewrote the entire service, this time as an online, real-time one. Queries and tweets are tracked as they come, and our models are continuously updated, just like the search results themselves. To account for the longer tail of queries that has less context from recent hours, we combine the real-time, up-to-date model with a background model computed in the same manner, but over several months of data (and updated daily). Within the first two weeks of launching our related queries and spelling corrections in late April, we’ve corrected 5 million queries and provided suggestions to 100 million more. We’re very encouraged by the high engagement rates we’re seeing so far on both features. We’re working on more ways to help you find and discover the most relevant and engaging content in real time, so stay tuned. There are other big improvements we’ll be rolling out to Twitter search over the coming weeks and months. Acknowledgments The system was built by Gilad Mishne ( @gilad ), Zhenghua Li ( @zhenghuali ) and Tian Wang ( @wangtian ) with help from the entire Twitter Search team. Thanks also to Jeff Dalton ( @jeffd ) for initial explorations and to Aneesh Sharma ( @aneeshs ) for help with the design.", "date": "2012-05-11"},
{"website": "Twitter-Engineering", "title": "Visualize Data Workflows with Ambrose", "author": ["‎@Twitter‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/visualize-data-workflows-with-ambrose.html", "abstract": "Last Friday at our Apache Pig Hackathon, we open-sourced Twitter Ambrose , a tool which helps authors of large-scale data workflows keep track of the overall status of a workflow and visualize its progress. hey #hadoop folks, we open sourced @ ambrose today, a tool for visualization and real-time monitoring of data workflows github.com/twitter/ambrose — Twitter Open Source ( @TwitterOSS ) May 11, 2012 Ambrose was hatched at our last Hack Week by Bill Graham ( @billonahill ) and Andy Schlaikjer ( @sagemintblue ), which focused on internal tools and developer efficiency. At Twitter, we develop complex workflows to analyze massive data sets generated by our platform. Our engineers create these workflows using a variety of tools and languages, including Pig and Scalding . One difficulty many of us face when using these tools is observability: when a Pig script is executed, multiple MapReduce jobs might be launched, either in parallel or in a serial fashion if one job depends on the output of another. As these jobs run, the status of individual jobs can be monitored with the Hadoop Job Tracker UI, but overall progress of the script can be difficult to keep track of. With Ambrose, the real-time status of a complex series of MapReduce jobs can be visualized succinctly, so that we can quickly understand how far computation has progressed and diagnose failures in context. In this screenshot, we see the Ambrose UI for a workflow compiled from a single Pig script. The circular chord diagram in the upper left highlights dependencies between jobs. As a job’s status changes, the color of its arc in the diagram changes. Statistics for the job most recently started are displayed to the right of the chord diagram. Summary information and status of all jobs is displayed in the table beneath these two views. At the moment it only works with Pig; however, the framework is extensible and allows support for other other runtimes. We plan to support Cascading and Scalding , but we welcome patches for other runtimes as well. Ambrose also relies on a number of other great open-source projects including Jetty , D3.js , and Twitter Bootstrap . In its current form Ambrose is still early in development and has a growing list of features we’d love to add, but we’ve open sourced it to develop Ambrose in the open and get community feedback. We encourage you to download it and let us know what you think. If you’re interested in working on and evolving data visualization tools like Ambrose, join the flock . In the end, we’d love to hear your feedback — Tweet us at @Ambrose or file an issue . - Chris Aniszczyk, Manager of Open Source ( @cra )", "date": "2012-05-16"},
{"website": "Twitter-Engineering", "title": "Improving performance on twitter.com", "author": ["‎@Twitter‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/improving-performance-on-twittercom.html", "abstract": "To connect you to information in real time, it’s important for Twitter to be fast. That’s why we’ve been reviewing our entire technology stack to optimize for speed. When we shipped #NewTwitter in September 2010, we built it around a web application architecture that pushed all of the UI rendering and logic to JavaScript running on our users’ browsers and consumed the Twitter REST API directly, in a similar way to our mobile clients. That architecture broke new ground by offering a number of advantages over a more traditional approach, but it lacked support for various optimizations available only on the server. To improve the twitter.com experience for everyone, we’ve been working to take back control of our front-end performance by moving the rendering to the server. This has allowed us to drop our initial page load times to 1/5th of what they were previously and reduce differences in performance across browsers. On top of the rendered pages, we asynchronously bootstrap a new modular JavaScript application to provide the fully-featured interactive experience our users expect. This new framework will help us rapidly develop new Twitter features, take advantage of new browser technology, and ultimately provide the best experience to as many people as possible. This week, we rolled out the re-architected version of one of our most visited pages, the Tweet permalink page . We’ll continue to roll out this new framework to the rest of the site in the coming weeks, so we’d like to take you on a tour of some of the improvements. No more #! The first thing that you might notice is that permalink URLs are now simpler: they no longer use the hashbang (#!). While hashbang-style URLs have a handful of limitations , our primary reason for this change is to improve initial page-load performance. When you come to twitter.com, we want you to see content as soon as possible. With hashbang URLs, the browser needs to download an HTML page, download and execute some JavaScript, recognize the hashbang path (which is only visible to the browser), then fetch and render the content for that URL. By removing the need to handle routing on the client, we remove many of these steps and reduce the time it takes for you to find out what’s happening on twitter.com. Reducing time to first tweet Before starting any of this work we added instrumentation to find the performance pain points and identify which categories of users we could serve better. The most important metric we used was “time to first Tweet”. This is a measurement we took from a sample of users, (using the Navigation Timing API ) of the amount of time it takes from navigation (clicking the link) to viewing the first Tweet on each page’s timeline. The metric gives us a good idea of how snappy the site feels. Looking at the components that make up this measurement, we discovered that the raw parsing and execution of JavaScript caused massive outliers in perceived rendering speed. In our fully client-side architecture, you don’t see anything until our JavaScript is downloaded and executed. The problem is further exacerbated if you do not have a high-specification machine or if you’re running an older browser. The bottom line is that a client-side architecture leads to slower performance because most of the code is being executed on our users’ machines rather than our own. There are a variety of options for improving the performance of our JavaScript, but we wanted to do even better. We took the execution of JavaScript completely out of our render path. By rendering our page content on the server and deferring all JavaScript execution until well after that content has been rendered, we’ve dropped the time to first Tweet to one-fifth of what it was. Loading only what we need Now that we’re delivering page content faster, the next step is to ensure that our JavaScript is loaded and the application is interactive as soon as possible. To do that, we need to minimize the amount of JavaScript we use: smaller payload over the wire, fewer lines of code to parse, faster to execute. To make sure we only download the JavaScript necessary for the page to work, we needed to get a firm grip on our dependencies. To do this, we opted to arrange all our code as CommonJS modules, delivered via AMD. This means that each piece of our code explicitly declares what it needs to execute which, firstly, is a win for developer productivity. When working on any one module, we can easily understand what dependencies it relies on, rather than the typical browser JavaScript situation in which code depends on an implicit load order and globally accessible properties. Modules let us separate the loading and the evaluation of our code. This means that we can bundle our code in the most efficient manner for delivery and leave the evaluation order up to the dependency loader. We can tune how we bundle our code, lazily load parts of it, download pieces in parallel, separate it into any number of files, and more — all without the author of the code having to know or care about this. Our JavaScript bundles are built programmatically by a tool, similar to the RequireJS optimizer, that crawls each file to build a dependency tree. This dependency tree lets us design how we bundle our code, and rather than downloading the kitchen sink every time, we only download the code we need — and then only execute that code when required by the application. What’s next? We’re currently rolling out this new architecture across the site. Once our pages are running on this new foundation, we will do more to further improve performance. For example, we will implement the History API to allow partial page reloads in browsers that support it, and begin to overhaul the server side of the application. If you want to know more about these changes, come and see us at the Fluent Conference next week. We’ll speak about the details behind our rebuild of twitter.com and host a JavaScript Happy Hour at Twitter HQ on May 31. @ danwrong yey! The future is coming and it looks just like the past, but more good underneath. — Tom Lea ( @cwninja ) May 23, 2012 -Dan Webb, Engineering Manager, Web Core team ( @danwrong )", "date": "2012-05-29"},
{"website": "Twitter-Engineering", "title": "Studying rapidly evolving user interests", "author": ["‎@lintool‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/studying-rapidly-evolving-user-interests.html", "abstract": "Twitter is an amazing real-time information dissemination platform. We’ve seen events of historical importance such as the Arab Spring unfold via Tweets. We even know that Twitter is faster than earthquakes ! However, can we more scientifically characterize the real-time nature of Twitter? One way to measure the dynamics of a content system is to test how quickly the distribution of terms and phrases appearing in it changes. A recent study we’ve done does exactly this: looking at terms and phrases in Tweets and in real-time search queries, we see that the most frequent terms in one hour or day tend to be very different from those in the next — significantly more so than in other content on the web. Informally, we call this phenomenon churn. This week, we are presenting a short paper at the International Conference on Weblogs and Social Media (ICWSM 2012) , in which @gilad and I examine this phenomenon. An extended version of the paper, titled “A Study of ‘Churn’ in Tweets and Real-Time Search Queries”, is available here . Some highlights: Examining all search queries from October 2011, we see that, on average, about 17% of the top 1000 query terms from one hour are no longer in the top 1000 during the next hour. In other words, 17% of the top 1000 query terms “churn over” on an hourly basis. Repeating this at a granularity of days instead of hours, we still find that about 13% of the top 1000 query terms from one day are no longer in the top 1000 during the next day. During major events, the frequency of queries spike dramatically. For example, on October 5, immediately following news of the death of Apple co-founder and CEO Steve Jobs, the query “steve jobs” spiked from a negligible fraction of query volume to 15% of the query stream — almost one in six of all queries issued! Check it out: the query volume is literally off the charts! Notice that related queries such as “apple” and “stay foolish” spiked as well. What does this mean? News breaks on Twitter, whether local or global, of narrow or broad interest. When news breaks, Twitter users flock to the service to find out what’s happening. Our goal is to instantly connect people everywhere to what’s most meaningful to them; the speed at which our content (and the relevance signals stemming from it) evolves make this more technically challenging, and we are hard at work continuously refining our relevance algorithms to address this. Just to give one example: search, boiled down to its basics, is about computing term statistics such as term frequency and inverse document frequency. Most algorithms assume some static notion of underlying distributions — which surely isn’t the case here! In addition, we’re presenting a paper at the co-located workshop on Social Media Visualization , where @miguelrios and I share some of our experiences in using data visualization techniques to generate insights from the petabytes of data in our data warehouse. You’ve seen some of these visualizations before, for example, about the 2010 World Cup and 2011 Japan earthquake . In the paper, we present another visualization, of seasonal variation of tweeting patterns for users in four different cities (New York City, Tokyo, Istanbul, and Sao Paulo). The gradient from white to yellow to red indicates amount of activity (light to heavy). Each tile in the heatmap represents five minutes of a given day and colors are normalized by day. This was developed internally to understand why growth patterns in Tweet-production experience seasonal variations. We see different patterns of activity between the four cities. For example, waking/sleeping times are relatively constant throughout the year in Tokyo, but the other cities exhibit seasonal variations. We see that Japanese users’ activities are concentrated in the evening, whereas in the other cities there is more usage during the day. In Istanbul, nights get shorter during August; Sao Paulo shows a time interval during the afternoon when Tweet volume goes down, and also longer nights during the entire year compared to the other three cities. Finally, we’re also giving a keynote at the co-located workshop on Real-Time Analysis and Mining of Social Streams (RAMSS) , fitting very much into the theme of our study. We’ll be reviewing many of the challenges of handling real-time data, including many of the issues described above. Interested in real-time systems that deliver relevant information to users? Interested in data visualization and data science? We’re hiring ! Join the flock! - Jimmy Lin, Research Scientist, Analytics ( @lintool )", "date": "2012-06-04"},
{"website": "Twitter-Engineering", "title": "Distributed Systems Tracing with Zipkin", "author": ["‎@cra‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/distributed-systems-tracing-with-zipkin.html", "abstract": "Zipkin is a distributed tracing system that we created to help us gather timing data for all the disparate services involved in managing a request to the Twitter API. As an analogy, think of it as a performance profiler, like Firebug , but tailored for a website backend instead of a browser. In short, it makes Twitter faster. Today we’re open sourcing Zipkin under the APLv2 license to share a useful piece of our infrastructure with the open source community and gather feedback. What can Zipkin do for me? Here’s the Zipkin web user interface. This example displays the trace view for a web request. You can see the time spent in each service compared to the scale on top and all the services involved in the request on the left. You can click on those for more detailed information. Zipkin has helped us find a whole slew of untapped performance optimizations, such as removing memcache requests, rewriting slow MySQL SELECTs, and fixing incorrect service timeouts. Finding and correcting these types of performance bottlenecks helps make Twitter faster. How does Zipkin work? Whenever a request reaches Twitter, we decide if the request should be sampled. We attach a few lightweight trace identifiers and pass them along to all the services used in that request. By only sampling a portion of all the requests we reduce the overhead of tracing, allowing us to always have it enabled in production. The Zipkin collector receives the data via Scribe and stores it in Cassandra along with a few indexes. The indexes are used by the Zipkin query daemon to find interesting traces to display in the web UI. Zipkin started out as a project during our first Hack Week. During that week we implemented a basic version of the Google Dapper paper for Thrift. Today it has grown to include support for tracing Http, Thrift, Memcache, SQL and Redis requests. These are mainly done via our Finagle library in Scala and Java, but we also have a gem for Ruby that includes basic tracing support. It should be reasonably straightforward to add tracing support for other protocols and in other libraries. Acknowledgements Zipkin was primarily authored by Johan Oskarsson ( @skr ) and Franklin Hu ( @thisisfranklin ). The project relies on a bunch of Twitter libraries such as Finagle and Scrooge but also on Cassandra for storage, ZooKeeper for configuration, Scribe for transport, Bootstrap and D3 for the UI. Thanks to the authors of those projects, the authors of the Dapper paper as well as the numerous people at Twitter involved in making Zipkin a reality. A special thanks to @iano , @couch , @zed , @dmcg , @marius and @a_a for their involvement. Last but not least we’d like to thank @jeajea for designing the Zipkin logo. On the whole, Zipkin was initially targeted to support Twitter’s infrastructure of libraries and protocols, but can be extended to support more systems that can be used within your infrastructure. Please let us know on Github if you find any issues and pull requests are always welcome. If you want to stay in touch, follow @ZipkinProject and check out the upcoming talk at Strange Loop 2012. If distributed systems tracing interests you, consider joining the flock to make things better. - Chris Aniszczyk, Manager of Open Source ( @cra )", "date": "2012-06-07"},
{"website": "Twitter-Engineering", "title": "Twitter at the Hadoop Summit", "author": ["‎@Twitter‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/twitter-at-the-hadoop-summit.html", "abstract": "Apache Hadoop is a fundamental part of Twitter infrastructure. The massive computational and storage capacity it provides us is invaluable for analyzing our data sets, continuously improving user experience, and powering features such as “who to follow” recommendations , tailored follow suggestions for new users and “best of Twitter” emails . We developed and open-sourced a number of technologies, including the recent Elephant Twin project that help our engineers be productive with Hadoop. We will be talking about some of them at the Hadoop Summit this week: Real-time analytics with Storm and Hadoop ( @nathanmarz ) Storm is a distributed and fault-tolerant real-time computation system, doing for real-time computation what Hadoop did for batch computation. Storm can be used together with Hadoop to make a potent realtime analytics stack; Nathan will discuss how we’ve combined the two technologies at Twitter to do complex analytics in real-time. Training a Smarter Pig: Large-Scale Machine Learning at Twitter ( @lintool ) We’ll present a case study of Twitter`s integration of machine learning tools into its existing Hadoop-based, Pig-centric analytics platform. In our deployed solution, common machine learning tasks such as data sampling, feature generation, training, and testing can be accomplished directly in Pig , via carefully crafted loaders, storage functions, and user-defined functions. This means that machine learning is just another Pig script, which allows seamless integration with existing infrastructure for data management, scheduling, and monitoring in a production environment. This talk is based on a paper we presented at SIGMOD 2012. Scalding: Twitter`s new DSL for Hadoop ( @posco ) Hadoop uses a functional programming model to represent large-scale distributed computation. Scala is thus a very natural match for Hadoop. We will present Scalding , which is built on top of Cascading. Scalding brings an API very similar to Scala`s collection API to allow users to write jobs as they might locally and run those Jobs at scale. This talk will present the Scalding DSL and show some example jobs for common use cases. Hadoop and Vertica: The Data Analytics Platform at Twitter ( @billgraham ) Our data analytics platform uses a number of technologies, including Hadoop, Pig, Vertica, MySQL and ZooKeeper, to process hundreds of terabytes of data per day. Hadoop and Vertica are key components of the platform. The two systems are complementary, but their inherent differences create integration challenges. This talk is an overview of the overall system architecture focusing on integration details, job coordination and resource management. Flexible In-Situ Indexing for Hadoop via Elephant Twin ( @squarecog ) Hadoop workloads can be broadly divided into two types: large aggregation queries that involve scans through massive amounts of data, and selective “needle in a haystack” queries that significantly restrict the number of records under consideration. Secondary indexes can greatly increase processing speed for queries of the second type. We will present Twitter`s generic, extensible in-situ indexing framework Elephant Twin which was just open sourced: unlike “trojan layouts,” no data copying is necessary, and unlike Hive, our integration at the Hadoop API level means that all layers in the stack above can benefit from indexes. As you can tell, our uses of Hadoop are wide and varied. We are looking forward to exchanging notes with other practitioners and learning about upcoming developments in the Hadoop ecosystem. Hope to see you there and if this sort of thing gets you excited, reach out to us, as we are hiring ! - Dmitriy Ryaboy, Engineering Manager, Analytics ( @squarecog )", "date": "2012-06-13"},
{"website": "Twitter-Engineering", "title": "Building and profiling high performance systems with Iago", "author": ["‎@cra‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/building-and-profiling-high-performance-systems-with-iago.html", "abstract": "Iago is a load generator that we created to help us test services before they encounter production traffic. While there are many load generators available in the open source and commercial software worlds, Iago provides us with capabilities that are uniquely suited for Twitter’s environment and the precise degree to which we need to test our services. There are three main properties that make Iago a good fit for Twitter: High performance: In order to reach the highest levels of performance, your load generator must be equally performant. It must generate traffic in a very precise and predictable way to minimize variance between test runs and allow comparisons to be made between development iterations. Additionally, testing systems to failure is an important part of capacity planning, and it requires you to generate load significantly in excess of expected production traffic. Multi-protocol: Modelling a system as complex as Twitter can be difficult, but it’s made easier by decomposing it into component services. Once decomposed, each piece can be tested in isolation; this requires your load generator to speak each service’s protocol. Twitter has in excess of 100 such services, and Iago can and has tested most of them due to its built-in support for the protocols we use, including HTTP, Thrift and several others. Extensible: Iago is designed first and foremost for engineers. It assumes that the person building the system will also be interested in validating its performance and will know best how to do so. As such, it’s designed from the ground up to be extensible – making it easy to generate new traffic types, over new protocols and with individualized traffic sources. It is also provides sensible defaults for common use cases, while allowing for extensive configuration without writing code if that’s your preference. Iago is the load generator we always wished we had. Now that we’ve built it, we want to share it with others who might need it to solve similar problems. Iago is now open sourced at GitHub under the Apache Public License 2.0 and we are happy to accept any feedback (or pull requests) the open source community might have. How does Iago work? Iago’s documentation goes into more detail, but it is written in Scala and is designed to be extended by anyone writing code for the JVM platform. Non-blocking requests are generated at a specified rate, using an underlying, configurable statistical distribution (the default is to model a Poisson Process ). The request rate can be varied as appropriate – for instance to warm up caches before handling full production load. In general the focus is on the arrival rate aspect of Little’s Law , instead of concurrent users, which is allowed to float as appropriate given service latency. This greatly enhances the ability to compare multiple test runs and protects against service regressions inducing load generator slow down. In short, Iago strives to model a system where requests arrive independently of your service’s ability to handle them. This is as opposed to load generators which model closed systems where users will patiently handle whatever latency you give them. This distinction allows us to closely mimic failure modes that we would encounter in production. Part of achieving high performance is the ability to scale horizontally. Unsurprisingly, Iago is no different from the systems we test with it. A single instance of Iago is composed of cooperating processes that can generate ~10K RPS provided a number of requirements are met including factors such as size of payload, the response time of the system under test, the number of ephemeral sockets available, and the rate you can actually generate messages your protocol requires. Despite this complexity, with horizontal scaling Iago is used to routinely test systems at Twitter with well over 200K RPS. We do this internally using our Apache Mesos grid computing infrastructure, but Iago can adapt to any system that supports creating multiple JVM processes that can discover each other using Apache Zookeeper . Iago at Twitter Iago has been used at Twitter throughout our stack, from our core database interfaces, storage sub-systems and domain logic, up to the systems accepting front end web requests. We routinely evaluate new hardware with it, have extended it to support correctness testing at scale and use it to test highly specific endpoints such as the new tailored trends , personalized search, and Discovery releases. We’ve used it to model anticipated load for large events as well as the overall growth of our system over time. It’s also good for providing background traffic while other tests are running, simply to provide the correct mix of usage that we will encounter in production. Acknowledgements & Future Work Iago was primarily authored by James Waldrop ( @hivetheory ), but as with any such engineering effort a large number of people have contributed. A special thanks go out to the Finagle team, Marius Eriksen ( @marius ), Arya Asemanfar ( @a_a ), Evan Meagher ( @evanm ), Trisha Quan ( @trisha ) and Stephan Zuercher ( @zuercher ) for being tireless consumers as well as contributors to the project. Furthermore, we’d like to thank Raffi Krikorian ( @raffi ) and Dave Loftesness ( @dloft ) for originally envisioning and spearheading the effort to create Iago. To view the Iago source code and participate in the creation and development of our roadmap, please visit Iago on GitHub. If you have any further questions, we suggest joining the mailing list and following @iagoloadgen . If you’re at the Velocity Conference this week in San Francisco, please swing by our office hours to learn more about Iago. - Chris Aniszczyk, Manager of Open Source ( @cra )", "date": "2012-06-25"},
{"website": "Twitter-Engineering", "title": "Caching with Twemcache", "author": ["‎@cra‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/caching-with-twemcache.html", "abstract": "Update - July 11, 2012, 9:45am We want to correct an error regarding the slab calcification problem we mentioned in the original post. This problem only applied to our v1.4.4 fork of Memcached; this correction is reflected below. The recent Memcached version has addressed some of these problems. We built Twemcache because we needed a more robust and manageable version of Memcached, suitable for our large-scale production environment. Today, we are open-sourcing Twemcache under the New BSD license. As one of the largest adopters of Memcached , a popular open source caching system, we have used Memcached over the years to help us scale our ever-growing traffic. Today, we have hundreds of dedicated cache servers keeping over 20TB of data from over 30 services in-memory, including crucial data such as user information and Tweets. Collectively these servers handle almost 2 trillion queries on any given day (that’s more than 23 million queries per second). As we continued to grow, we needed a more robust and manageable version of Memcached suitable for our large scale production environment. We have been running Twemcache in production for more than a year and a half. Twemcache is based on a fork of Memcached v1.4.4 that is heavily modified to improve maintainability and help us monitor our cache servers better. We improved performance, removed code that we didn’t find necessary, refactored large source files and added observability related features. The following sections will provide more details on why we did this and what those new features are. Motivation Almost all of our cache use cases fall into two categories: as an optimization for disk where cache is used as the in-memory serving layer to shed load from databases. as an optimization for cpu where cache is used as a buffer to store items that are expensive to recompute. An example of these two optimizations is “caching of Tweets”. All Tweets are persisted to disk when they are created, but most Tweets requested by users need to be served out of memory for performance reasons. We use Twemcache to store recent and frequently accessed Tweets, as an optimization for disk. When a Tweet shows up in a particular client, it takes a particular presentation - rendered Tweet - which has other metadata like number of retweets, favorites etc. We also use Twemcache to store the recently rendered Tweets, as an optimization for cpu. To effectively address the use cases mentioned above, it’s extremely important that caches are always available and have predictable performance with respect to item hit rate even when operating at full capacity. Caches should also be able to adapt to changing item sizes on-the-fly as application data size grows or shrinks over time. Finally, it is critical to have observability into caches to monitor the health and effectiveness of our cache clusters. It turns out that all these problems are interrelated because adapting to changing item sizes usually requires a cache reconfiguration — which impacts availability and predictability. Twemcache tries to address these needs with the help of the following features: Random Eviction The v1.4.4 implementation of Memcached, which Twemcache is based on, suffers from a problem we call slab calcification. In Memcached, a slab can only store items of a given maximum size and once a slab has been allocated to a slab class, it cannot be reassigned to another slab class. In other words, slabs once allocated are locked to their respective slab classes. This is the crux of the slab calcification problem. When items grow or shrink in size, new slabs must be to allocated to store them. Over time, when caches reach full memory capacity, to store newer items we must rely on evicting existing items in the same slab class. If the newer items are of a size with no slabs allocated, write requests may fail completely. Meanwhile, slabs allocated to a different slab class may sit idle. Slab calcification leads to loss of capacity and efficiency. To solve this problem without resorting to periodically restarting the server instances, we introduced a new eviction strategy called random eviction. In this strategy, when a new item needs to be inserted and it cannot be accommodated by the space occupied by an expired item or the available free memory, we’ll simply pick a random slab from the list of all allocated slabs, evict all items within that slab, and reallocate it to the slab class that fits the new item. It turns out that this feature is quite powerful for two reasons: Cache servers can now gracefully move on-the-fly from one slab size to another for a given application. This enables our cache servers to adapt to changing item sizes and have a predictable long term hit rate by caching an application’s active working set of items. Application developers don’t have to worry about reconfiguring their cache server when they add or delete fields from their cache item structures or if their item size grows over time. By providing a stable hit rate, random eviction prevents performance degradation due to data pattern change and system instability associated with restarts. The video below illustrates how over time Twemcache is able to adapt to a shifting size pattern and still remain effective. Lock-less Stats Collection Cache observability enables us to monitor the health of our cache clusters and ensure that applications are using them effectively. To address this need, we redesigned the Memcached stats module. Similar to the findings in Facebook’s attempt to scale Memcached , we found that the global statistics lock was a main contention point. This motivated us to use an updater-aggregator model of thread synchronization, in which worker threads always update thread-local metrics, and a background aggregator thread asynchronously collects metrics from all threads periodically holding only one thread-local lock at a time. Once aggregated, stats polling comes for free. Removing a global lock reduces the time Twemcache spends in a unresponsive state. There is a slight trade-off between how up-to-date stats are and how much burden stats collection puts on the system. However, the difference in total mutex wait time between aggregating once and 100 times per second is under 20%, and the impact on performance is totally predictable and thread-local. On top of making stats collection scalable, we also systematically reviewed the metrics, and came up with a more comprehensive list of metrics: Memcached provides 48 global metrics, 18 slab metrics and 10 item stats; Twemcache, on the other hand, provides 74 global metrics and 39 slab metrics. We merged item metrics into slab metrics to further simplify stats collection. Asynchronous Command Logger When using Memcached, one of the hardest problems we faced was the hit-rate and memory-footprint trade-off - the sweet spot for achieving the desired performance gain with reasonable resources, as it is typically not possible to keep the entire data set in memory. To pinpoint the minimum memory requirement for a given hit rate, we needed a way to systematically analyze an application’s data access pattern. To address this need, we implemented a new feature called command logger in Twemcache. When turned on, each worker thread will record a time stamped command header as well as return status, as shown below: Each line of the command log gives precise information on the client, the time when a request was received, the command header including the command, key, flags and data length, a return code, and reply message length. In fact, the only thing missing is the item value itself, which turns out to be unimportant for our analysis. The command logger supports lockless read/write into ring buffers. Each worker thread logs into a thread-local buffer as they process incoming queries, and a background thread asynchronously dumps buffer contents to either a file or a socket. Thus the overhead on worker threads is minimal and so would not affect the service availability. The logger has been tested to log at about 100k requests-per-second. To control the speed of log generation, the command logger also supports sampling. Once we know what keys are accessed, the way they are accessed, and their return status, we can perform offline data analysis to estimate optimal working set size, item heat map, etc. Future work Twemcache is the result of our effort to turn Memcached into a reliable building block in Twitter’s data infrastructure. We kept the simplicity of the Memcached protocol intact, but made the service more dependable and more informative with Twemcache, without sacrificing performance. While we initially focused on the challenging goal of making Memcached work extremely well within the Twitter infrastructure, we look forward to sharing our code and ideas with the Memcached community in the long term. In the near future, we plan to evolve Twemcache in the open, address the hashtable lock contention issue that would further improve scalability, support more eviction strategies, support bootstrapping the cache from disk and provide a complete set of real-time data analysis tools. To view the source code and share feedback, please visit the Twemcache GitHub page . You can also follow Twemcache’s Twitter account (@ Twemcache ) for updates. We would love to hear any ideas you have in improving Twemcache via pull requests or issues. Or better yet, why not consider joining the flock (@ jointheflock ) if you want to help build a world class caching system? Other work: Twemproxy Twemcache is one of the building blocks that comprise the caching system at Twitter. Another fundamental building block in our caching system is Twemproxy , a proxy for memcached protocol that we recently open sourced. Twemproxy minimizes the connections to our backend caching servers and enables us to scale horizontally. Furthermore, we are also actively developing the client side of our caching system on top of the Twitter Finagle stack. Acknowledgements Twemcache was primarily engineered by Manju Rajashekhar (@ manju ) and Yao Yue (@ thinkingfish ). In addition, we’d like to acknowledge the following folks who contributed to the project either directly or indirectly and its deployment and maintenance in our datacenters: Anirudh Srinivas (@ asrin ), David Lam (@ kkdlam ), Krishna Gade (@ krishnagade ), Joshua Coats (@ shu ), Owen Vallis (@ o_e_bert ), Rob Benson (@ rgbenson ), Brandon Mitchell (@ bitbckt ) and Xin Xiang (@ xiangxin72 ). - Chris Aniszczyk, Manager of Open Source (@ cra )", "date": "2012-07-10"},
{"website": "Twitter-Engineering", "title": "TwitterCLDR: Improving Internationalization Support in Ruby", "author": ["‎@cra‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/twittercldr-improving-internationalization-support-in-ruby.html", "abstract": "We recently open sourced TwitterCLDR under the Apache Public License 2.0. TwitterCLDR is an “ICU level” internationalization library for Ruby that supports dates, times, numbers, currencies, world languages, sorting, text normalization, time spans, plurals, and unicode code point data. By sharing our code with the community we hope to collaborate together and improve internationalization support for websites all over the world. If your company is considering supporting multiple languages, then you can try TwitterCLDR to help your internationalization efforts. Motivation Here’s a test. Say this date out loud: 2/1/2012 If you said, “February first, 2012”, you’re probably an American. If you said, “January second, 2012”, you’re probably of European or possibly Asian descent. If you said, “January 12, 1902”, you’re probably a computer. The point is that as humans, we almost never think about formatting dates, plurals, lists, and the like. If you’re creating a platform available around the world, however, these kinds of minutiae make a big difference to users. The Unicode Consortium publishes and maintains a bunch of data regarding formatting dates, numbers, lists, and more, called the Common Locale Data Repository (CLDR) . IBM maintains International Components for Unicode (ICU), a library that uses the Unicode Consortium’s data to make it easier for programmers to use. However, this library is targeted at Java and C/C++ developers and not Ruby programmers, which is one of the programming languages used at Twitter. For example, Ruby and TwitterCLDR helps power our Translation Center . TwitterCLDR provides a way to use the same CLDR data that Java uses, but in a Ruby environment. Hence, formatting dates, times, numbers, currencies and plurals should now be much easier for the typical Rubyist. Let’s go over some real world examples. Example Code Dates, Numbers, and Currencies Let’s format a date in Spanish (es): $> DateTime.now.localize(:es).to_full_s\n$> \"lunes, 12 de diciembre de 2011 21:44:57 UTC -0800\" Too long? Make it shorter: $> DateTime.now.localize(:es).to_short_s\n$> \"12/12/11 21:44\" Built in support for relative times lets you do this: $> (DateTime.now - 1).localize(:en).ago.to_s\n$> \"1 day ago\"\n$> (DateTime.now + 1).localize(:en).until.to_s\n$> \"In 1 day\" Number formatting is easy: $> 1337.localize(:en).to_s\n$> \"1,337\"\n$> 1337.localize(:fr).to_s\n$> \"1 337\" We’ve got you covered for currencies and decimals too: $> 1337.localize(:es).to_currency.to_s(:currency => \"EUR\")\n$> \"1.337,00 €\"\n$> 1337.localize(:es).to_decimal.to_s(:precision => 3)\n$> \"1.337,000\" Currency data? Absolutely: $> TwitterCldr::Shared::Currencies.for_country(\"Canada\")\n$> { :currency => \"Dollar\", :symbol => \"$\", :code => \"CAD\" } Plurals Get the plural rule for a number: $> TwitterCldr::Formatters::Plurals::Rules.rule_for(1, :ru)\n$> :one\n$> TwitterCldr::Formatters::Plurals::Rules.rule_for(3, :ru)\n$> :few\n$> TwitterCldr::Formatters::Plurals::Rules.rule_for(10, :ru)\n$> :many Embed plurals right in your translatable phrases using JSON syntax: $> str = 'there % in the barn'\n$> str.localize % { :horse_count => 3 }\n$> \"there are 3 horses in the barn\" Unicode Data Get attributes for any Unicode code point: $> code_point = TwitterCldr::Shared::CodePoint.for_hex(\"1F3E9\")\n$> code_point.name\n$> \"LOVE HOTEL\"\n$> code_point.category\n$> \"So\" Normalize strings using Unicode’s standard algorithms (NFD, NFKD, NFC, or NFKC): $> \"español\".localize.code_points\n$> [\"0065\", \"0073\", \"0070\", \"0061\", \"00F1\", \"006F\", \"006C\"]\n$> \"español\".localize.normalize(:using => :NFKD).code_points\n$> [\"0065\", \"0073\", \"0070\", \"0061\", \"006E\", \"0303\", \"006F\", \"006C\"] Sorting (Collation) TwitterCLDR includes a pure Ruby, from-scratch implementation of the Unicode Collation Algorithm (with tailoring) that enables locale-aware sorting capabilities. Alphabetize a list using regular Ruby sort: $> [\"Art\", \"Wasa\", \"Älg\", \"Ved\"].sort\n$> [\"Art\", \"Ved\", \"Wasa\", \"Älg\"] Alphabetize a list using TwitterCLDR’s locale-aware sort: $> [\"Art\", \"Wasa\", \"Älg\", \"Ved\"].localize(:de).sort.to_a\n$> [\"Älg\", \"Art\", \"Ved\", \"Wasa\"] NOTE: Most of these methods can be customized to your liking. JavaScript Support What good is all this internationalization support in Ruby if I can’t expect the same output on the client side too? To bridge the gap between the client and server sides, TwitterCLDR also contains a JavaScript implementation (known as twitter-cldr-js) whose compiled files are maintained in a separate GitHub repo . At the moment, twitter-cldr-js supports dates, times, relative times, and plural rules. We’re working on expanding its capabilities, so stay tuned. Future Work In the future, we hope to add even more internationalization capabilities to TwitterCLDR, including Rails integration, phone number and postal code validation, support for Unicode characters in Ruby 1.8 strings and regular expressions, and the ability to translate timezone names via the TZInfo gem and ActiveSupport. We would love to have the community use TwitterCLDR and help us improve the code to reach everyone in the world. Acknowledgements Twitter CLDR was primarily authored by Cameron Dutro ( @camertron ). In addition, we’d like to acknowledge the following folks who contributed to the project either directly or indirectly: Kirill Lashuk ( @kl_7 ), Nico Sallembien ( @nsallembien ), Sumit Shah ( @omnidactyl ), Katsuya Noguchi, Engineer ( @kn ), Timothy Andrew ( @timothyandrew ) and Kristian Freeman ( @imkmf ). - Chris Aniszczyk, Manager of Open Source ( @cra )", "date": "2012-08-01"},
{"website": "Twitter-Engineering", "title": "Trident: a high-level abstraction for realtime computation", "author": ["‎@Twitter‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/trident-a-high-level-abstraction-for-realtime-computation.html", "abstract": "Trident is a new high-level abstraction for doing realtime computing on top of Twitter Storm , available in Storm 0.8.0 (released today). It allows you to seamlessly mix high throughput (millions of messages per second), stateful stream processing with low latency distributed querying. If you’re familiar with high level batch processing tools like Pig or Cascading , the concepts of Trident will be very familiar - Trident has joins, aggregations, grouping, functions, and filters. In addition to these, Trident adds primitives for doing stateful, incremental processing on top of any database or persistence store. Trident has consistent, exactly-once semantics, so it is easy to reason about Trident topologies. We’re really excited about Trident and believe it is a major step forward in Big Data processing. It builds upon Storm’s foundation to make realtime computation as easy as batch computation. Example Let’s look at an illustrative example of Trident. This example will do two things: Compute streaming word count from an input stream of sentences Implement queries to get the sum of the counts for a list of words For the purposes of illustration, this example will read an infinite stream of sentences from the following source: This spout cycles through that set of sentences over and over to produce the sentence stream. Here’s the code to do the streaming word count part of the computation: Let’s go through the code line by line. First a TridentTopology object is created, which exposes the interface for constructing Trident computations. TridentTopology has a method called newStream that creates a new stream of data in the topology reading from an input source. In this case, the input source is just the FixedBatchSpout defined from before. Input sources can also be queue brokers like Kestrel or Kafka. Trident keeps track of a small amount of state for each input source (metadata about what it has consumed) in Zookeeper, and the “spout1” string here specifies the node in Zookeeper where Trident should keep that metadata. Trident processes the stream as small batches of tuples. For example, the incoming stream of sentences might be divided into batches like so: Generally the size of those small batches will be on the order of thousands or millions of tuples, depending on your incoming throughput. Trident provides a fully fledged batch processing API to process those small batches. The API is very similar to what you see in high level abstractions for Hadoop like Pig or Cascading: you can do group by’s, joins, aggregations, run functions, run filters, and so on. Of course, processing each small batch in isolation isn’t that interesting, so Trident provides functions for doing aggregations across batches and persistently storing those aggregations - whether in memory, in Memcached, in Cassandra, or some other store. Finally, Trident has first-class functions for querying sources of realtime state. That state could be updated by Trident (like in this example), or it could be an independent source of state. Back to the example, the spout emits a stream containing one field called “sentence”. The next line of the topology definition applies the Split function to each tuple in the stream, taking the “sentence” field and splitting it into words. Each sentence tuple creates potentially many word tuples - for instance, the sentence “the cow jumped over the moon” creates six “word” tuples. Here’s the definition of Split: As you can see, it’s really simple. It simply grabs the sentence, splits it on whitespace, and emits a tuple for each word. The rest of the topology computes word count and keeps the results persistently stored. First the stream is grouped by the “word” field. Then, each group is persistently aggregated using the Count aggregator. The persistentAggregate function knows how to store and update the results of the aggregation in a source of state. In this example, the word counts are kept in memory, but this can be trivially swapped to use Memcached, Cassandra, or any other persistent store. Swapping this topology to store counts in Memcached is as simple as replacing the persistentAggregate line with this (using trident-memcached ), where the “serverLocations” variable is a list of host/ports for the Memcached cluster: The values stored by persistentAggregate represents the aggregation of all batches ever emitted by the stream. One of the cool things about Trident is that it has fully fault-tolerant, exactly-once processing semantics. This makes it easy to reason about your realtime processing. Trident persists state in a way so that if failures occur and retries are necessary, it won’t perform multiple updates to the database for the same source data. The persistentAggregate method transforms a Stream into a TridentState object. In this case the TridentState object represents all the word counts. We will use this TridentState object to implement the distributed query portion of the computation. The next part of the topology implements a low latency distributed query on the word counts. The query takes as input a whitespace separated list of words and return the sum of the counts for those words. These queries are executed just like normal RPC calls, except they are parallelized in the background. Here’s an example of how you might invoke one of these queries: As you can see, it looks just like a regular remote procedure call (RPC), except it’s executing in parallel across a Storm cluster. The latency for small queries like this are typically around 10ms. More intense DRPC queries can take longer of course, although the latency largely depends on how many resources you have allocated for the computation. The implementation of the distributed query portion of the topology looks like this: The same TridentTopology object is used to create the DRPC stream, and the function is named “words”. The function name corresponds to the function name given in the first argument of execute when using a DRPCClient. Each DRPC request is treated as its own little batch processing job that takes as input a single tuple representing the request. The tuple contains one field called “args” that contains the argument provided by the client. In this case, the argument is a whitespace separated list of words. First, the Split function is used to split the arguments for the request into its constituent words. The stream is grouped by “word”, and the stateQuery operator is used to query the TridentState object that the first part of the topology generated. stateQuery takes in a source of state - in this case, the word counts computed by the other portion of the topology - and a function for querying that state. In this case, the MapGet function is invoked, which gets the count for each word. Since the DRPC stream is grouped the exact same way as the TridentState was (by the “word” field), each word query is routed to the exact partition of the TridentState object that manages updates for that word. Next, words that didn’t have a count are filtered out via the FilterNull filter and the counts are summed using the Sum aggregator to get the result. Then, Trident automatically sends the result back to the waiting client. Trident is intelligent about how it executes a topology to maximize performance. There’s two interesting things happening automatically in this topology: Operations that read from or write to state (like persistentAggregate and stateQuery) automatically batch operations to that state. So if there’s 20 updates that need to be made to the database for the current batch of processing, rather than do 20 read requests and 20 write requests to the database, Trident will automatically batch up the reads and writes, doing only 1 read request and 1 write request (and in many cases, you can use caching in your State implementation to eliminate the read request). So you get the best of both words of convenience - being able to express your computation in terms of what should be done with each tuple - and performance. Trident aggregators are heavily optimized. Rather than transfer all tuples for a group to the same machine and then run the aggregator, Trident will do partial aggregations when possible before sending tuples over the network. For example, the Count aggregator computes the count on each partition, sends the partial count over the network, and then sums together all the partial counts to get the total count. This technique is similar to the use of combiners in MapReduce. Let’s look at another example of Trident. Reach The next example is a pure DRPC topology that computes the reach of a URL on Twitter on demand. Reach is the number of unique people exposed to a URL on Twitter. To compute reach, you need to fetch all the people who ever tweeted a URL, fetch all the followers of all those people, unique that set of followers, and that count that uniqued set. Computing reach is too intense for a single machine - it can require thousands of database calls and tens of millions of tuples. With Storm and Trident, it’s easy to parallelize the computation of each step across a cluster. This topology will read from two sources of state. One database maps URLs to a list of people who tweeted that URL. The other database maps a person to a list of followers for that person. The topology definition looks like this: The topology creates TridentState objects representing each external database using the newStaticState method. These can then be queried in the topology. Like all sources of state, queries to these databases will be automatically batched for maximum efficiency. The topology definition is straightforward - it’s just a simple batch processing job. First, the urlToTweeters database is queried to get the list of people who tweeted the URL for this request. That returns a list, so the ExpandList function is invoked to create a tuple for each tweeter. Next, the followers for each tweeter must be fetched. It’s important that this step be parallelized, so shuffle is invoked to evenly distribute the tweeters among all workers for the topology. Then, the followers database is queried to get the list of followers for each tweeter. You can see that this portion of the topology is given a large parallelism since this is the most intense portion of the computation. Next, the set of followers is uniqued and counted. This is done in two steps. First a “group by” is done on the batch by “follower”, running the “One” aggregator on each group. The “One” aggregator simply emits a single tuple containing the number one for each group. Then, the ones are summed together to get the unique count of the followers set. Here’s the definition of the “One” aggregator: This is a “combiner aggregator”, which knows how to do partial aggregations before transferring tuples over the network to maximize efficiency. Sum is also defined as a combiner aggregator, so the global sum done at the end of the topology will be very efficient. Let’s now look at Trident in more detail. Fields and tuples The Trident data model is the TridentTuple which is a named list of values. During a topology, tuples are incrementally built up through a sequence of operations. Operations generally take in a set of input fields and emit a set of “function fields”. The input fields are used to select a subset of the tuple as input to the operation, while the “function fields” name the fields emitted by the operation. Consider this example. Suppose you have a stream called “stream” that contains the fields “x”, “y”, and “z”. To run a filter MyFilter that takes in “y” as input, you would say: Suppose the implementation of MyFilter is this: This will keep all tuples whose “y” field is less than 10. The TridentTuple given as input to MyFilter will only contain the “y” field. Note that Trident is able to project a subset of a tuple extremely efficiently when selecting the input fields: the projection is essentially free. Let’s now look at how “function fields” work. Suppose you had this function: This function takes two numbers as input and emits two new values: the addition of the numbers and the multiplication of the numbers. Suppose you had a stream with the fields “x”, “y”, and “z”. You would use this function like this: The output of functions is additive: the fields are added to the input tuple. So the output of this each call would contain tuples with the five fields “x”, “y”, “z”, “added”, and “multiplied”. “added” corresponds to the first value emitted by AddAndMultiply, while “multiplied” corresponds to the second value. With aggregators, on the other hand, the function fields replace the input tuples. So if you had a stream containing the fields “val1” and “val2”, and you did this: The output stream would only contain a single tuple with a single field called “sum”, representing the sum of all “val2” fields in that batch. With grouped streams, the output will contain the grouping fields followed by the fields emitted by the aggregator. For example: In this example, the output will contain the fields “val1” and “sum”. State A key problem to solve with realtime computation is how to manage state so that updates are idempotent in the face of failures and retries. It’s impossible to eliminate failures, so when a node dies or something else goes wrong, batches need to be retried. The question is - how do you do state updates (whether external databases or state internal to the topology) so that it’s like each message was processed only once? This is a tricky problem, and can be illustrated with the following example. Suppose that you’re doing a count aggregation of your stream and want to store the running count in a database. If you store only the count in the database and it’s time to apply a state update for a batch, there’s no way to know if you applied that state update before. The batch could have been attempted before, succeeded in updating the database, and then failed at a later step. Or the batch could have been attempted before and failed to update the database. You just don’t know. Trident solves this problem by doing two things: Each batch is given a unique id called the “transaction id”. If a batch is retried it will have the exact same transaction id. State updates are ordered among batches. That is, the state updates for batch 3 won’t be applied until the state updates for batch 2 have succeeded. With these two primitives, you can achieve exactly-once semantics with your state updates. Rather than store just the count in the database, what you can do instead is store the transaction id with the count in the database as an atomic value. Then, when updating the count, you can just compare the transaction id in the database with the transaction id for the current batch. If they’re the same, you skip the update - because of the strong ordering, you know for sure that the value in the database incorporates the current batch. If they’re different, you increment the count. Of course, you don’t have to do this logic manually in your topologies. This logic is wrapped by the State abstraction and done automatically. Nor is your State object required to implement the transaction id trick: if you don’t want to pay the cost of storing the transaction id in the database, you don’t have to. In that case the State will have at-least-once-processing semantics in the case of failures (which may be fine for your application). You can read more about how to implement a State and the various fault-tolerance tradeoffs possible in this doc . A State is allowed to use whatever strategy it wants to store state. So it could store state in an external database or it could keep the state in-memory but backed by HDFS (like how HBase works). State’s are not required to hold onto state forever. For example, you could have an in-memory State implementation that only keeps the last X hours of data available and drops anything older. Take a look at the implementation of the Memcached integration for an example State implementation. Execution of Trident topologies Trident topologies compile down into as efficient of a Storm topology as possible. Tuples are only sent over the network when a repartitioning of the data is required, such as if you do a groupBy or a shuffle. So if you had this Trident topology: It would compile into Storm spouts/bolts like this: As you can see, Trident colocates operations within the same bolt as much as possible. Conclusion Trident makes realtime computation elegant. You’ve seen how high throughput stream processing, state manipulation, and low-latency querying can be seamlessly intermixed via Trident’s API. Trident lets you express your realtime computations in a natural way while still getting maximal performance. To get started with Trident, take a look at these sample Trident topologies and the Trident documentation . - Nathan Marz, Software Engineer ( @nathanmarz )", "date": "2012-08-02"},
{"website": "Twitter-Engineering", "title": "Visualizing Hadoop with HDFS-DU", "author": ["‎@cra‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/visualizing-hadoop-with-hdfs-du.html", "abstract": "We are a heavy adopter of Apache Hadoop with a large set of data that resides in its clusters, so it’s important for us to understand how these resources are utilized. At our July Hack Week , we experimented with developing HDFS-DU to provide us an interactive visualization of the underlying Hadoop Distributed File System (HDFS). The project aims to monitor different snapshots for the entire HDFS system in an interactive way, showing the size of the folders and the rate at which the size changes. It can also effectively identify efficient and inefficient file storage and highlight nodes in the file system where this is happening. HDFS-DU provides the following in a web user interface: A TreeMap visualization where each node is a folder in HDFS. The area of each node can be relative to the size or number of descendents A tree visualization showing the topology of the file system HDFS-DU is built using the following front-end technologies: D3.js : for tree visualization JavaScript InfoVis Toolkit : for TreeMap visualization Details Below is a screenshot of the HDFS-DU user interface (directory names scrubbed). The user interface is made up of two linked visualizations. The left visualization is a TreeMap and shows parent-child relationships through containment. The right visualization is a tree layout, which displays two levels of depth from the current selected node in the file system. The tree visualization displays extra information for each node on hover. You can drill down on the TreeMap by clicking on a node, this would create the same effect as clicking on any tree node. There are two possible layouts for the TreeMap. The default one encodes file size in the area of each node. The second one encodes number of descendents in the area of each node. In the second view it’s interesting to spot nodes where storage is inefficient. Future Work This project was created at our July Hack Week and we still consider it beta but useful software. In the future, we would love to improve the front-end client and create a new back-end for a different runtime environment. On the front end, the directory browser, currently on the right, is poorly suited to the task of showing the directory structure. A view which looks more like a traditional filesystem browser would be more immediately recognizable and make better use of space (it is likely that a javascript file browser exists and could be used instead). Also, the integration between the current file browser and the TreeMap needs improvement. We initially envisioned the TreeMap as a Voronoi TreeMap , however our current implementation of that code ran too slowly to be practical. We would love to get the Voronoi TreeMap code to work fast enough. We would also like to add the option to use different values to size and color the TreeMap areas. For example, change in size, creation time, last access time, frequency of access. Acknowledgements HDFS-DU was primarily authored by Travis Crawford ( @tc ), Nicolas Garcia Belmonte ( @philogb ) and Robert Harris ( @trebor ). Given that this is a young project, we always appreciate bug fixes, features and documentation improvements. Feel free to fork the project and send us a pull request on GitHub to say hello. Finally, if you’re interested in visualization and distributed file systems like Hadoop, we’re always looking for engineers to join the flock. Follow @hdfsdu on Twitter to stay in touch! - Chris Aniszczyk, Manager of Open Source ( @cra )", "date": "2012-08-07"},
{"website": "Twitter-Engineering", "title": "Crowdsourced data analysis with Clockwork Raven", "author": ["‎@cra‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/crowdsourced-data-analysis-with-clockwork-raven.html", "abstract": "Today, we’re excited to open source Clockwork Raven , a web application that allows users to easily submit data to Mechanical Turk for manual review and then analyze that data. Clockwork Raven steps in to do what algorithms cannot: it sends your data analysis tasks to real people and gets fast, cheap and accurate results. We use Clockwork Raven to gather tens of thousands of judgments from Mechanical Turk users every week. Motivation We’re huge fans of human evaluation at Twitter and how it can aid data analysis. In the past, we’ve used systems like Mechanical Turk and CrowdFlower, as well as an internal system where we train dedicated reviewers and have them come in to our offices. However, as we scale up our usage of human evaluation, we needed a better system. This is why we built Clockwork Raven and designed it with several important goals in mind: Requires little technical skill to use: The current Mechanical Turk web interface requires knowledge of HTML to do anything beyond very basic tasks. Uniquely suited for our needs: Many of our evaluations require us to embed tweets and timelines in the task. We wanted to create reusable components that would allow us to easily add these widgets to our tasks. Scalable: Manually training reviews doesn’t scale as well as a system that crowd sources the work through Mechanical Turk. Reliable: We wanted controls over who’s allowed to complete our evaluations, so we can ensure we’re getting top-notch results. Low barrier of entry: We wanted a tool that everyone in the company could use to launch evaluations. Integrated analysis: We wanted a tool that would analyze the data we gather, in addition to provide the option to export a JSON or CSV to import into tools like R or a simple spreadsheet. Features In Clockwork Raven, you create an evaluation by submitting a table of data (CSV or JSON). Each row of this table corresponds to a task that a human will complete. We build a template for the tasks in the Template Builder, then submit them to Mechanical Turk and Clockwork Raven tracks how many responses we’ve gotten. Once all the tasks are complete, we can import the results into Clockwork Raven where they’re presented in a configurable bar chart and can be exported to a number of data formats. Here’s the features we’ve built into Clockwork Raven to address the goals above: Clockwork Raven has a simple drag-and-drop builder not unlike the form builder in Google Docs. We can create headers and text sections, add multiple-choice and free-response questions, and insert data from a column in the uploaded data. The template builder has pre-built components for common items we need to put in our evaluations, like users and Tweets. It’s easy to build new components, so you can design your own. In the template builder, we can pass parameters (like the identifier of the Tweet we’re embedding) into the component. Here’s how we insert a tweet: Clockwork Raven submits jobs to Mechanical Turk. We can get back thousands of judgements in an hour or less. And because Mechanical Turk workers come from all over the world, we get results whenever we want them. Clockwork Raven allows you to manage a list of Trusted Workers. We’ve found that having a hand-picked list of workers is the best way to get great results. We can expand our pool by opening up our tasks beyond our hand-picked set and choosing workers who are doing a great job with our tasks. Clockwork Raven authenticates against any LDAP directory (or you can manage user accounts manually). That means that you can give a particular LDAP group at your organization access to Clockwork Raven, and they can log in with their own username and password. No shared accounts, and full accountability for who’s spending what. You can also give “unprivileged” access to some users, allowing them to try Clockwork Raven out and submit evaluations to the Mechanical Turk sandbox (which is free), but not allowing them to submit tasks that cost money without getting approval. Clockwork Raven has a built-in data analysis tool that lets you chart your results across multiple dimensions of data and view individual results: Future Work We’re actively developing Clockwork Raven and improving it over time. Our target for the next release is a comprehensive REST API that works with JSON (possibly Thrift as well). We’re hoping this will allow us to build Clockwork Raven into our workflows, as well as enable its use for real-time human evaluation. We’re also working on better ways of managing workers, by automatically managing the group of trusted workers through qualification tasks and automated analysis of untrusted users’ work. If you’d like to help work on these features, or have any bug fixes, other features, or documentation improvements, we’re always looking for contributions. Just submit a pull request to say hello or reach out to us on the mailing list . If you find something missing or broken, report it in the issue tracker . Acknowledgements Clockwork Raven was primarily authored by Ben Weissmann ( @benweissmann ). In addition, we’d like to acknowledge the following folks who contributed to the project: Edwin Chen ( @echen ) and Dave Buchfuhrer ( @daveFNbuck ). Follow @clockworkraven on Twitter to stay in touch! - Chris Aniszczyk, Manager of Open Source ( @cra )", "date": "2012-08-16"},
{"website": "Twitter-Engineering", "title": "How we spent our Summer of Code", "author": ["‎@cra‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/how-we-spent-our-summer-of-code.html", "abstract": "For the first time, Twitter participated in the Google Summer of Code (GSoC) and we want to share news on the resulting open source activities. Unlike many GSoC participating organizations that focus on a single ecosystem, we have a variety of projects spanning multiple programming languages and communities. it’s “pencils down” for @gsoc , thank you so much to our mentors and student interns @KL_7 @fbru02 @rubeydoo for hacking with us this summer — Twitter Open Source ( @TwitterOSS ) August 23, 2012 We accepted three students to work on a few projects: Kirill Lashuk ( @KL_7 ) mentored by Cameron Dutro ( @camertron ) added more internationalization and localization capabilities to Ruby via the TwitterCLDR project. Among other things, he added the NFKD normalization algorithm, better access to Unicode code points and Unicode Collation Algorithm support, contributed over 6100 total lines of code, countless resource files and prepared the project for a new version of Ruby. His work should help anyone in the Ruby community needing robust internationalization support for their application. Federico Brubacher ( @fbru02 ) mentored by Nathan Marz ( @nathanmarz ) spent time kick starting and adding machine learning capabilities ( see the code ) to Twitter Storm . Ruben Oanta ( @rubeydoo ) mentored by Marius Eriksen ( @marius ) wrote a MySQL client ( see the code ) for our RPC system, Twitter Finagle . It supports both the binary and text protocols, allowing us to use both regular queries as well as prepared statements. His work provides a great foundation for our database clients to make better use of all of our shared infrastructure. We were also thrilled to collaborate with Tumblr’s Blake Matheny ( @bmatheny ) on mentoring this project. As part of GSoC, students and mentoring organizations receive a stipend. We are donating our portion of the stipend to Girls Who Code , an organization we support that introduces high school girls to software development. We really enjoyed the opportunity to take part in Google Summer of Code. Thank you to our three students, mentors and to Google for the program, we look forward to next year.", "date": "2012-08-24"},
{"website": "Twitter-Engineering", "title": "Joining the Linux Foundation", "author": ["‎@cra‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/joining-the-linux-foundation.html", "abstract": "Today Twitter officially joins the Linux Foundation ( @linuxfoundation ), the nonprofit consortium dedicated to protecting and promoting the growth of the Linux operating system. happy 21st birthday to # linux , we’re proud to support the @ linuxfoundation wired.com/thisdayintech/… — Twitter Open Source ( @TwitterOSS ) August 25, 2012 We have tens of thousands machines running all types of services that run a tweaked version of Linux. One reason we support Linux is that open source development lets us control our destiny — we are able to innovate faster given the flexibility to customize based on our needs. We also love the large, stable and mature development community with whom we can collaborate to move the state of the kernel forward. If you look at the latest Linux Development Report , there are more than 7,800 developers from 800 different companies contributing to make Linux better for everyone. We use a couple different kernel versions of Linux and tend to favor stability. As of today, we are mainly on the 2.6.39 kernel release. We tweak the kernel by adding some patches such as enhanced core dump functionality, UnionFS support and the ability to allow TCP congestion window to be set on a socket basis. We expect to further customize the kernel to optimize it for our production environment and contribute some of the work upstream. If you’re interested in hacking on kernel performance at the scale of Twitter and participating in the Linux community, we’d love to hear from you and speak with you at LinuxCon this week. And of course, we wish Linux a happy 21st birthday . Posted by Chris Aniszczyk ( @cra ), Manager of Open Source", "date": "2012-08-27"},
{"website": "Twitter-Engineering", "title": "Scalding 0.8.0 and Algebird", "author": ["‎@cra‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/scalding-080-and-algebird.html", "abstract": "Earlier this year we open sourced Scalding , a Scala API for Cascading that makes it easy to write big data jobs in a syntax that’s simple and concise. We use Scalding heavily — for everything from custom ad targeting algorithms to PageRank on the Twitter graph. Since open sourcing Scalding, we’ve been improving our documentation by adding a Getting Started guide and a Rosetta Code page that contains several MapReduce tasks translated from other frameworks (e.g., Pig and Hadoop Streaming) into Scalding. Today we are excited to tell you about the 0.8.0 release of Scalding. What’s new There are a lot of new features , for example, Scalding now includes a type-safe Matrix API . The Matrix API makes expressing matrix sums, products, and simple algorithms like cosine similarity trivial. The type-safe Pipe API has some new functions and a few bug fixes. In the familiar Fields API , we’ve added the ability to add type information to fields which allows scalding to pick up Ordering instances so that grouping on almost any scala collection becomes easy. There is now a function to estimate set size in groupBy: approxUniques (a naive implementation requires two groupBys, but this function uses HyperLogLog ). Since many aggregations are simple transformations of existing Monoids (associative operations with a zero), we added mapPlusMap to simplify implementation of many reducing operations ( count how many functions are implemented in terms of mapPlusMap ). Cascading and scalding try to optimize your job to some degree, but in some cases for optimal performance, some hand-tuning is needed. This release adds three features to make that easier: forceToDisk forces a materialization and helps when you know the prior operation filters almost all data and should not be limited to just before a join or merge. Map-side aggregation in Cascading is done in memory with a threshold on when to spill and poor tuning can result in performance issues or out of memory errors. To help alleviate these issues, we now expose a function in groupBy to specify the spillThreshold. We make it easy for Scalding Jobs to control the Hadoop configuration by allowing overriding of the config. Algebird Algebird is our lightweight abstract algebra library for Scala and is targeted for building aggregation systems (such as Storm ). It was originally developed as part of Scalding’s Matrix API, but almost all of the common reduce operations we care about in Scalding turn out to be instances of Monoids. This common library gives Map-merge, Set-union, List-concatenation, primitive-type algebra, and some fancy Monoids such as HyperLogLog for set cardinality estimation. Algebird has no dependencies and should be easy to use from any scala project that is doing aggregation of data or data-structures. For instance in the Algebird repo, type “sbt console” and then: scala> import com.twitter.algebird.Operators._\nimport com.twitter.algebird.Operators._ scala> Map(1 -> 3, 2 -> 5, 3 -> 7, 5 -> 1) + Map(1 -> 1, 2 -> 1)\nres0: scala.collection.immutable.Map[Int,Int] = Map(1 -> 4, 2 -> 6, 3 -> 7, 5 -> 1) scala> Set(1,2,3) + Set(3,4,5)\nres1: scala.collection.immutable.Set[Int] = Set(5, 1, 2, 3, 4) scala> List(1,2,3) + List(3,4,5)\nres2: List[Int] = List(1, 2, 3, 3, 4, 5) scala> Map(1 -> 3, 2 -> 4, 3 -> 1) * Map(2 -> 2)\nres3: scala.collection.immutable.Map[Int,Int] = Map(2 -> 8) scala> Map(1 -> Set(2,3), 2 -> Set(1)) + Map(2 -> Set(2,3))\nres4: scala.collection.immutable.Map[Int,scala.collection.immutable.Set[Int]] = Map(1 -> Set(2, 3), 2 -> Set(1, 2, 3)) Future work We are thrilled to see industry recognition of Scalding; the project has received a Bossie Award and there’s a community building around Scalding, with adopters like Etsy and eBay using it in production. In the near future, we are looking at adding optimized skew joins, refactoring the code base into smaller components and using Thrift and Protobuf lzo compressed data. On the whole, we look forward to improving documentation and nurturing a community around Scalding as we approach a 1.0 release. If you’d like to help work on any features or have any bug fixes, we’re always looking for contributions. Just submit a pull request to say hello or reach out to us on the mailing list . If you find something missing or broken, report it in the issue tracker . Acknowledgements Scalding and Algebird are built by a community. We’d like to acknowledge the following folks who contributed to the project: Oscar Boykin ( @posco ), Avi Bryant ( @avibryant ), Edwin Chen ( @echen ), Sam Ritchie ( @sritchie ), Flavian Vasile ( @flavianv ) and Argyris Zymnis ( @argyris ). Follow @scalding on Twitter to stay in touch! Posted by Chris Aniszczyk @cra Manager, Open Source", "date": "2012-09-24"},
{"website": "Twitter-Engineering", "title": "Open Sourcing Clutch.IO", "author": ["‎@cra‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/open-sourcing-clutchio.html", "abstract": "Clutch is an easy-to-integrate library for native iOS applications designed to help you develop faster, deploy instantly and run A/B tests. When Clutch co-founders Eric Florenzano ( @ericflo ) and Eric Maguire ( @etmaguire ) recently joined the flock, they promised that everything you need to run Clutch on your own infrastructure would be available. We are incredibly excited to announce that Twitter has acquired the IP of Clutch.io and we start work there today! blog.clutch.io/post/293407962… — Clutch IO ( @clutchio ) August 13, 2012 Today, we’ve open-sourced the code on GitHub under the Apache Public License 2.0, now with improved documentation to help get you started. As a reminder the hosted service will be active and supported until November 1. After that you can use the open-sourced code to run it on your own and modify it to your needs. Read more about how to set up your instance on GitHub and ask any questions on the mailing list or on Twitter via @clutchio . Releasing this code as an open source project is just the beginning. There are still plenty of areas for improvement from better documentation to an easier setup process. Now that the project is publicly available, we look forward to seeing a community blossom and grow the project into something greater. - Chris Aniszczyk, Manager of Open Source ( @cra )", "date": "2012-10-11"},
{"website": "Twitter-Engineering", "title": "Bolstering our infrastructure", "author": ["‎@mazenra‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/bolstering-our-infrastructure.html", "abstract": "Last night, the world tuned in to Twitter to share the election results as U.S. voters chose a president and settled many other campaigns. Throughout the day, people sent more than 31 million election-related Tweets (which contained certain key terms and relevant hashtags). And as results rolled in, we tracked the surge in election-related Tweets at 327,452 Tweets per minute (TPM). These numbers reflect the largest election-related Twitter conversation during our 6 years of existence, though they don’t capture the total volume of all Tweets yesterday. As an engineering team, we keep an eye on all of the activity across the platform –– in particular, on the number of Tweets per second (TPS). Last night, Twitter averaged about 9,965 TPS from 8:11pm to 9:11pm PT, with a one-second peak of 15,107 TPS at 8:20pm PT and a one-minute peak of 874,560 TPM. Seeing a sustained peak over the course of an entire event is a change from the way people have previously turned to Twitter during live events. In the past, we’ve generally experienced short-lived roars related to the clock striking midnight on New Year’s Eve (6,939 Tweets per second, or TPS), the end of a soccer game (7,196 TPS), or Beyonce’s pregnancy announcement (8,868 TPS). Those spikes tended to last seconds, maybe minutes at most. Now, rather than brief spikes, we are seeing sustained peaks for hours. Last night is just another example of the traffic pattern we’ve experienced this year –– we also saw this during the NBA Finals , Olympics Closing Ceremonies , VMAs , and Hip-Hop Awards . Last night’s numbers demonstrate that as Twitter usage patterns change, Twitter the service can remain resilient. Over time, we have been working to build an infrastructure that can withstand an ever-increasing load. For example, we’ve been steadily optimizing the Ruby runtime . And, as part of our ongoing migration away from Ruby, we’ve reconfigured the service so traffic from our mobile clients hits the Java Virtual Machine (JVM) stack, avoiding the Ruby stack altogether. Of course, we still have plenty more to do. We’ll continue to measure and evaluate event-based traffic spikes, including their size and duration. We’ll continue studying the best ways to accommodate expected and unexpected traffic surges and high volume conversation during planned real-time events such as elections and championship games, as well as unplanned events such as natural disasters. The bottom line: No matter when, where or how people use Twitter, we need to remain accessible 24/7, around the world. We’re hard at work delivering on that vision. - Mazen Rawashdeh, VP of Infrastructure Operations Engineering ( @mazenra )", "date": "2012-11-07"},
{"website": "Twitter-Engineering", "title": "Dimension Independent Similarity Computation (DISCO)", "author": ["‎@Twitter‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/dimension-independent-similarity-computation-disco.html", "abstract": "MapReduce is a programming model for processing large data sets, typically used to do distributed computing on clusters of commodity computers. With large amount of processing power at hand, it’s very tempting to solve problems by brute force. However, we often combine clever sampling techniques with the power of MapReduce to extend its utility. Consider the problem of finding all pairs of similarities between D indicator (0/1 entries) vectors, each of dimension N. In particular we focus on cosine similarities between all pairs of D vectors in R^N. Further assume that each dimension is L-sparse, meaning each dimension has at most L non-zeros across all points. For example, typical values to compute similarities between all pairs of a subset of Twitter users can be: D = 10M N = 1B L = 1000 Since the dimensions are sparse, it is natural to store the points dimension by dimension. To compute cosine similarities, we can easily feed each dimension t into MapReduce by using the following Mapper and Reducer combination Where #(w) counts the number of dimensions in which point w occurs, and #(w1, w2) counts the number of dimensions in which w1 and w2 co-occur, i.e., the dot product between w1 and w2. The steps above compute all dot products, which will then be scaled by the cosine normalization factor. There are two main complexity measures for MapReduce: “shuffle size”, and “reduce-key complexity”, defined shortly (Ashish Goel and Kamesh Munagala 2012). It can be easily shown that the above mappers will output on the order of O(NL^2) emissions, which for the example parameters we gave is infeasible. The number of emissions in the map phase is called the “shuffle size”, since that data needs to be shuffled around the network to reach the correct reducer. Furthermore, the maximum number of items reduced to a single key is at most #(w1, w2), which can be as large as N. Thus the “reduce-key complexity” for the above scheme is N. We can drastically reduce the shuffle size and reduce-key complexity by some clever sampling: Notation: p and ε are oversampling parameters. In this case, the output of the reducers are random variables whose expectations are the cosine similarities. Two proofs are needed to justify the effectiveness of this scheme. First, that the expectations are indeed correct and obtained with high probability, and second, that the shuffle size is greatly reduced. We prove both of these claims in (Reza Bosagh-Zadeh and Ashish Goel 2012). In particular, in addition to correctness, we prove that the shuffle size of the above scheme is only O(DL log(D)/ε), with no dependence on the “dimension” N, hence the name. This means as long as you have enough mappers to read your data, you can use the DISCO sampling scheme to make the shuffle size tractable. Furthermore, each reduce key gets at most O(log(D)/ε) values, thus making the reduce-key complexity tractable too. Within Twitter, we use the DISCO sampling scheme to compute similar users. We have also used the scheme to find highly similar pairs of words, by taking each dimension to be the indicator vector that signals in which Tweets the word appears. We further empirically verify the claims and observe large reductions in shuffle size, with details in the paper . Finally, this sampling scheme can be used to implement many other similarity measures. For Jaccard Similarity, we improve the implementation of the well-known MinHash ( http://en.wikipedia.org/wiki/MinHash ) scheme on Map-Reduce. Posted by Reza Zadeh ( @Reza_Zadeh ) and Ashish Goel ( @ashishgoel ) - Personalization & Recommendation Systems Group and Revenue Group Bosagh-Zadeh, Reza and Goel, Ashish (2012), Dimension Independent Similarity Computation, arXiv:1206.2082 Goel, Ashish and Munagala, Kamesh (2012), Complexity Measures for Map-Reduce, and Comparison to Parallel Computing , http://www.stanford.edu/~ashishg/papers/mapreducecomplexity.pdf", "date": "2012-11-12"},
{"website": "Twitter-Engineering", "title": "Discover with a new lens: Twitter cards", "author": ["‎@Twitter‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/discover-with-a-new-lens-twitter-cards.html", "abstract": "As you already know, there’s a myriad of things shared on Twitter every day, and not just 140 characters of text. There are links to breaking news stories, images from current events, and the latest activity from those you follow. We want Discover to be the place where you find the best of that content relevant to you, even if you don’t necessarily know everyone involved. This is why we’ve introduced several improvements to Discover on twitter.com over the last few months. For example we redesigned it to show a continuous stream of Tweets with photos and links to websites, in which you can also now see Tweets from activity, based on what your network favorites. We also added new signals to better blend together all the most relevant Tweets for you, and implemented polling so you know whenever there are fresh Tweets to see. Have you checked Discover lately? A new notification at the top of your stream shows when new Tweets are available. twitter.com/twitter/status… — Twitter ( @twitter ) November 8, 2012 Today we’re introducing a new version of Discover for mobile that brings many of these features to your iPhone or Android phone. For this new release, we’ve completely re-done the backend and user interface to take advantage of Twitter cards . Using Twitter cards, you’ll now see Tweets in Discover with links to news and photos rather than the former story previews which were not interactive. And supporting Twitter cards on the backend means we can more directly improve the user experience in our native apps, too. You’ll see content from cards partners display as a previews in the stream, so that you’ll get headlines and publication names for story summaries and photo previews rather than shortened URLs. All of this adds up to fewer taps, fewer screen views and more content for you to enjoy, faster. Of course, you can tap through to the details view for richer story summaries, bigger photos and the ability to reply, favorite or retweet Tweets. We think this set of updates delivers the most engaging Discover experience yet, and we hope you enjoy the experience as much as we’ve enjoyed creating it. Posted by Daniel Loreto - @DanielLoreto Engineering Manager, Search and Relevance", "date": "2012-11-15"},
{"website": "Twitter-Engineering", "title": "Twitter and SMS Spoofing", "author": ["‎@Twitter‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/twitter-and-sms-spoofing.html", "abstract": "Over the past two days, a few articles have been published about a potential problem concerning the ability to post false updates to another user’s SMS-enabled Twitter account, and it has been misreported that US-based Twitter users are currently vulnerable to this type of attack. The general concern is that if a user has a Twitter account configured for SMS updates , and an attacker knows that user’s phone number, it could be possible for the attacker to send a fake SMS message to Twitter that looks like it’s coming from that user’s phone number, which would result in a fake post to that user’s timeline. Most Twitter users interact over the SMS channel using a “shortcode.” In the US, for instance, this shortcode is 40404.  Because of the way that shortcodes work, it is not possible to send an SMS message with a fake source addressed to them, which eliminates the possibility of an SMS spoofing attack to those numbers. However, in some countries a Twitter shortcode is not yet available , and in those cases Twitter users interact over the SMS channel using a “longcode.” A longcode is basically just a normal looking phone number.  Given that it is possible to send an SMS message with a fake source address to these numbers, we have offered PIN protection to users who sign up with a longcode since 2007.  As of August of this year, we have additionally disallowed posting through longcodes for users that have an available shortcode. It has been misreported that US-based Twitter users are currently vulnerable to a spoofing attack because PIN protection is unavailable for them.  By having a shortcode, PIN protection isn’t necessary for US-based Twitter users, because they are not vulnerable to SMS spoofing.  We only provide the option for PIN protection in cases where a user could have registered with a longcode that is susceptible to SMS spoofing. We work hard to protect our users from these kinds of threats and many others, and will continue to keep Twitter a site deserving of your trust. Posted by Moxie Marlinspike - @moxie Engineering Manager, Product Security", "date": "2012-12-05"},
{"website": "Twitter-Engineering", "title": "Implementing pushState for twitter.com", "author": ["‎@todd‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/implementing-pushstate-for-twittercom.html", "abstract": "As part of our continuing effort to improve the performance of twitter.com , we’ve recently implemented pushState . With this change, users experience a perceivable decrease in latency when navigating between sections of twitter.com; in some cases near zero latency, as we’re now caching responses on the client. This post provides an overview of the pushState API, a summary of our implementation, and details some of the pitfalls and gotchas we experienced along the way. API Overview pushState is part of the HTML 5 History API — a set of tools for managing state on the client. The pushState() method enables mapping of a state object to a URL. The address bar is updated to match the specified URL without actually loading the page. history.pushState([page data], [page title], [page URL]) While the pushState() method is used when navigating forward from A to B, the History API also provides a “popstate” event—used to mange back/forward button navigation. The event’s “state” property maps to the data passed as the first argument to pushState(). If the user presses the back button to return to the initial point from which he/she first navigated via pushState, the “state” property of the “popstate” event will be undefined. To set the state for the initial, full-page load use the replaceState() method. It accepts the same arguments as the pushState() method. history.replaceState([page data], [page title], [page URL]) The following diagram illustrates how usage of the History API comes together. Progressive Enhancement Our pushState implementation is a progressive enhancement on top of our previous work , and could be described as Hijax + server-side rendering. By maintaining view logic on the server, we keep the client light, and maintain support for browsers that don’t support pushState with the same URLs. This approach provides the additional benefit of enabling us to disable pushState at any time without jeopardizing any functionality. On the Server On the server, we configured each endpoint to return either full-page responses, or a JSON payload containing a partial, server-side rendered view, along with its corresponding JavaScript components. The decision of what response to send is determined by checking the Accept header and looking for “application/json.” The same views are used to render both types of requests; to support pushState the views format the pieces used for the full-page responses into JSON. Here are two example responses for the Interactions page to illustrate the point: pushState response {\n  // Server-rendered HTML for the view\n  page: \" … \",\n  // Path to the JavaScript module for the associated view\n  module: \"app/pages/connect/interactions\",\n  // Initialization data for the current view\n  init_data: {…},\n  title: \"Twitter / Interactions\"\n} Full page response <b>{{title}}</b> {{page}} Client Architecture Several aspects of our existing client architecture made it particularly easy to enhance twitter.com with pushState. By contract, our components attach themselves to a single DOM node, listen to events via delegation, fire events on the DOM, and those events are broadcast to other components via DOM event bubbling. This allows our components to be even more loosely coupled—a component doesn’t need a reference to another component in order to listen for its events. Secondly, all of our components are defined using AMD, enabling the client to make decisions about what components to load. With this client architecture we implemented pushState by adding two components: one responsible for managing the UI, the other data. Both are attached to the document, listen for events across the entire page, and broadcast events available to all components. UI Component Manages the decision to pushState URLs by listening for document-wide clicks, and keyboard shortcuts Broadcasts an event to initiate pushState navigation Updates the UI in response to events from the data component DATA Component Only included if we’re using pushState Manages XHRs and caching of responses Provides eventing around the HTML 5 history API to provide a single interface for UI components Example pushState() Navigation LifeCycle The user clicks on link with a specialized class (we choose “js-nav”), the click is caught by the UI component which prevents the default behavior and triggers a custom event to initiate pushState navigation. The data component listens for that event and… Writes the current view to cache and, only before initial pushState navigation, calls replaceState() to set the state data for the view Fetches the JSON payload for the requested URL (either via XHR or from cache) Update the cache for the URL Call pushState() to update the URL Trigger an event indicating the UI should be updated The UI component resumes control by handling the event from the data component and… JavaScript components for the current view are torn down (event listeners detached, associated state is cleaned up) The HTML for the current view is replaced with the new HTML The script loader only fetches modules not already loaded The JavaScript components for the current view are initialized An event is triggered to alert all components that the view is rendered and initialized Pitfalls, Gotchas, etc. It’ll come as no surprise to any experienced frontend engineers that the majority of the problems and annoyances with implementing pushState stem from either 1) inconsistencies in browser implementations of the HTML 5 History API, or 2) having to replicate behaviors or functionality you would otherwise get for free with full-page reloads. Don’t believe the API, title updates are manual All browsers currently disregard the title attribute passed to the pushState() and replaceState() methods. Any updates to the page title need to be done manually. popstate Event Inconsistencies At the time of this writing, WebKit (and only WebKit) fires an extraneous popstate event after initial page load. This appears to be a known bug in WebKit , and is easy to work around by ignoring popstate events if the “state” property is undefined. State Object Size Limits Firefox imposes 640KB character limit on the serialized state object passed to pushState(), and will throw an exception if that limit is exceeded. We hit this limit in the early days of our implementation, and moved to storing state in memory. We limit the size of the serialized JSON we cache on the client per URL, and can adjust that number via a server-owned config. It’s worth noting that due to the aforementioned popstate bug in WebKit, we pass an empty object as the first argument to pushState() to distinguish WebKit’s extraneous popstate events from those triggered in response to back/forward navigation. Thoughtful State Management Around Caching The bulk of the work implementing pushState went into designing a simple client framework that would facilitate caching and provide the right events to enable components to both prepare themselves to be cached, and restore themselves from cache. This was solved through a few simple design decisions: All events that trigger navigation (clicks on links, keyboard shortcuts, and back/forward button presses) are abstracted by the pushState UI component, routed through the same path in the data component, and subsequently fire the same events. This allows the UI to be both cached and handle updates in a uniform way. The pushState UI component fires events around the rendering of updates: one before the DOM is updated, and another after the update is complete. The former enables UI components such as dialogs and menus to be collapsed in advance of the page being cached; the later enables UI components like timelines to update their timestamps when rendered from cache. POST & DELETE operations bust the client-side cache. Re-implementing Browser Functionality As is often the case, changing the browser’s default behavior in an effort to make the experience faster or simpler for the end-user typically requires more work on behalf of developers and designers. Here are some pieces of browser functionality that we had to re-implement: Managing the position of the scrollbar as the user navigates forward and backward. Preserving context menu functionality when preventing a link’s default click behavior. Accounting for especially fast, indecisive user clicks by ensuring the response you’re rendering is in sync with the last requested URL. Canceling outbound XHRs when the user requests a new page to avoid unnecessary UI updates. Implementing the canonical AJAX spinner , so the user knows the page is loading. Final Thoughts Despite the usual browser inconsistencies and other gotchas, we’re pretty happy with the HTML 5 History API. Our implementation has enabled us to deliver the fast initial page rendering times and robustness we associate with traditional, server-side rendered sites and the lightening quick in-app navigation and state changes associate with client-side rendered web applications. Helpful Resources Mozilla’s ( @Mozilla ) HTML 5 History API documentation Chris Wanstrath’s ( @defunkt ) pjax (pushState + ajax = pjax) plugin for jQuery project on GitHub Benjamin Lupton’s ( @balupton ) history.js project on GitHub Modernizr’s ( @Modernizr ) pushState capability detection —Todd Kloots, Engineer, Web Core team ( @todd )", "date": "2012-12-07"},
{"website": "Twitter-Engineering", "title": "Blobstore: Twitter’s in-house photo storage system ", "author": ["‎@Twitter‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/blobstore-twitter-s-in-house-photo-storage-system.html", "abstract": "Millions of people turn to Twitter to share and discover photos. To make it possible to upload a photo and attach it to your Tweet directly from Twitter, we partnered with Photobucket in 2011. As soon as photos became a more native part of the Twitter experience, more and more people began using this feature to share photos. In order to introduce new features and functionality, such as filters , and continue to improve the photos experience, Twitter’s Core Storage team began building an in-house photo storage system. In September, we began to use this new system, called Blobstore. What is Blobstore? Blobstore is Twitter’s low-cost and scalable storage system built to store photos and other binary large objects, also known as blobs. When we set out to build Blobstore, we had three design goals in mind: Low Cost: Reduce the amount of money and time Twitter spent on storing Tweets with photos. High Performance: Serve images in the low tens of milliseconds, while maintaining a throughput of hundreds of thousands of requests per second. Easy to Operate: Be able to scale operational overhead with Twitter’s continuously growing infrastructure. How does it work? When a user tweets a photo, we send the photo off to one of a set of Blobstore front-end servers. The front-end understands where a given photo needs to be written, and forwards it on to the servers responsible for actually storing the data. These storage servers, which we call storage nodes, write the photo to a disk and then inform a Metadata store that the image has been written and instruct it to record the information required to retrieve the photo. This Metadata store, which is a non-relational key-value store cluster with automatic multi-DC synchronization capabilities, spans across all of Twitter’s data centers providing a consistent view of the data that is in Blobstore. The brain of Blobstore, the blob manager, runs alongside the front-ends, storage nodes, and index cluster. The blob manager acts as a central coordinator for the management of the cluster. It is the source of all of the front-ends’ knowledge of where files should be stored, and it is responsible for updating this mapping and coordinating data movement when storage nodes are added, or when they are removed due to failures. Finally, we rely on Kestrel, Twitter’s existing asynchronous queue server, to handle tasks such as replicating images and ensuring data integrity across our data centers. We guarantee that when an image is successfully uploaded to Twitter, it is immediately retrievable from the data center that initially received the image. Within a short period of time, the image is replicated to all of our other data centers, and is retrievable from those as well. Because we rely on a multi-data-center Metadata store for the central index of files within Blobstore, we are aware in a very short amount of time whether an image has been written to its original data center; we can route requests there until the Kestrel queues are able to replicate the data. Blobstore Components How is the data found? When an image is requested from Blobstore, we need to determine its location in order to access the data. There are a few approaches to solving this problem, each with its own pros and cons. One such approach is to map or hash each image individually to a given server by some method. This method has a fairly major downside in that it makes managing the movement of images much more complicated. For example, if we were to add or remove a server from Blobstore, we would need to recompute a new location for each individual image affected by the change. This adds operational complexity, as it would necessitate a rather large amount of bookkeeping to perform the data movement. We instead created a fixed-sized container for individual blobs of data, called a “virtual bucket”. We map images to these containers, and then we map the containers to the individual storage nodes. We keep the total number of virtual buckets unchanged for the entire lifespan of our cluster. In order to determine which virtual bucket a given image is stored in, we perform a simple hash on the image’s unique ID. As long as the number of virtual buckets remains the same, this hashing will remain stable. The advantage of this stability is that we can reason about the movement of data at a much more coarsely grained level than the individual image. How do we place the data? When mapping virtual buckets to physical storage nodes, we keep some rules in mind to make sure that we don’t lose data when we lose servers or hard drives. For example, if we were to put all copies of a given image on a single rack of servers, losing that rack would mean that particular image would be unavailable. If we were to completely mirror the data on a given storage node on another storage node, it would be unlikely that we would ever have unavailable data, as the likelihood of losing both nodes at once is fairly low. However, whenever we were to lose a node, we would only have a single node to source from to re-replicate the data. We would have to recover slowly, so as to not impact the performance of the single remaining node. If we were to take the opposite approach and allow any server in the cluster to share a range of data on all servers, then we would avoid a bottleneck when recovering lost replicas, as we would essentially be able to read from the entire cluster in order to re-replicate data. However, we would also have a very high likelihood of data loss if we were to lose more than the replication factor of the cluster (two) per data center, as the chance that any two nodes would share some piece of data would be high. So, the optimal approach would be somewhere in the middle: for a given piece of data, there would be a limited number of machines that could share the range of data of its replica - more than one but less than the entire cluster. We took all of these things into account when we determined the mapping of data to our storage nodes. As a result, we built a library called “libcrunch” which understands the various data placement rules such as rack-awareness, understands how to replicate the data in way that minimizes risk of data loss while also maximizing the throughput of data recovery, and attempts to minimize the amount of data that needs to be moved upon any change in the cluster topology (such as when nodes are added or removed). It also gives us the power to fully map the network topology of our data center, so storage nodes have better data placement and we can take into account rack awareness and placement of replicas across PDU zones and routers. Keep an eye out for a blog post with more information on libcrunch. How is the data stored? Once we know where a given piece of data is located, we need to be able to efficiently store and retrieve it. Because of their relatively high storage density, we are using standard hard drives inside our storage nodes (3.5” 7200 RPM disks). Since this means that disk seeks are very expensive, we attempted to minimize the number of disk seeks per read and write. We pre-allocate ‘fat’ files on each storage node disk using fallocate(), of around 256MB each. We store each blob of data sequentially within a fat file, along with a small header. The offset and length of the data is then stored in the Metadata store, which uses SSDs internally, as the access pattern for index reads and writes is very well-suited for solid state media. Furthermore, splitting the index from the data saves us from needing to scale out memory on our storage nodes because we don’t need to keep any local indexes in RAM for fast lookups. The only time we end up hitting disk on a storage node is once we already have the fat file location and byte offset for a given piece of data. This means that we can generally guarantee a single disk seek for that read. Topology Management As the number of disks and nodes increases, the rate of failure increases. Capacity needs to be added, disks and nodes need to be replaced after failures, servers need to be moved. To make Blobstore operationally easy we put a lot of time and effort into libcrunch and the tooling associated with making cluster changes. When a storage node fails, data that was hosted on that node needs to be copied from a surviving replica to restore the correct replication factor. The failed node is marked as unavailable in the cluster topology, and so libcrunch computes a change in the mapping from the virtual buckets to the storage nodes. From this mapping change, the storage nodes are instructed to copy and migrate virtual buckets to new locations. Zookeeper Topology and placement rules are stored internally in one of our Zookeeper clusters. The Blob Manager deals with this interaction and it uses this information stored in Zookeeper when an operator makes a change to the system. A topology change can consist of adjusting the replication factor, adding, failing, or removing nodes, as well as adjusting other input parameters for libcrunch. Replication across Data centers Kestrel is used for cross data center replication. Because kestrel is a durable queue, we use it to asynchronously replicate our image data across data centers. Data center-aware Routing TFE (Twitter Frontend) is one of Twitter’s core components for routing. We wrote a custom plugin for TFE, that extends the default routing rules. Our Metadata store spans multiple data centers, and because the metadata stored per blob is small (a few bytes), we typically replicate this information much faster than the blob data. If a user tries to access a blob that has not been replicated to the nearest data center they are routed to, we look up this metadata information and proxy requests to the nearest data center that has the blob data stored. This gives us the property that if replication gets delayed, we can still route requests to the data center that stored the original blob, serving the user the image at the cost of a little higher latency until it’s replicated to the closer data center. Future work We have shipped the first version of blobstore internally. Although blobstore started with photos, we are adding other features and use cases that require blob storage to blobstore. And we are also continuously iterating on it to make it more robust, scalable, and easier to maintain. Acknowledgments Blobstore was a group effort. The following folks have contributed to the project: Meher Anand ( @meher_anand ), Ed Ceaser ( @asdf ), Harish Doddi ( @thinkingkiddo ), Chris Goffinet ( @lenn0x ), Jack Gudenkauf ( @_jg ), and Sangjin Lee ( @sjlee ). Posted by Armond Bigian @armondbigian Engineering Director, Core Storage & Database Engineering", "date": "2012-12-11"},
{"website": "Twitter-Engineering", "title": "Class project: “Analyzing Big Data with Twitter” ", "author": ["‎@gilad‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/class-project-analyzing-big-data-with-twitter.html", "abstract": "Twitter partnered with UC Berkeley this past semester to teach Analyzing Big Data with Twitter , a class with Prof. Marti Hearst . In the first half of the semester, Twitter engineers went to UC Berkeley to talk about the technology behind Twitter : from the basics of scaling up a service to the algorithms behind user recommendations and search . These talks are available online, on the course website . In the second half of the course, students applied their knowledge and creativity to build data-driven applications on top of Twitter. They came up with a range of products that included tracking bands or football teams, monitoring Tweets to find calls for help, and identifying communities on Twitter. Each project was mentored by one of our engineers. Last week, 40 of the students came to Twitter HQ to demo their final projects in front of a group of our engineers, designers and engineering leadership team. The students’ enthusiasm and creativity inspired and impressed all of us who were involved. The entire experience was really fun, and we hope to work with Berkeley more in the future. Many thanks to the volunteer Twitter engineers, to Prof. Hearst, and of course to our fantastic students! Cannot believe that I’ll be visiting the Twitter HQ tomorrow for our project presentations. Yay! Yes, had to tweet about it. #ilovetwitter — Priya Iyer ( @myy_precious ) December 6, 2012 @ gilad @ ucbtweeter Truly one of my dreams coming true! Thanks to everyone from Twitter and @ martihearst for making this happen. — Seema Hari ( @SeemaHari ) December 7, 2012 Posted by Gilad Mishne - @gilad Engineering Manager, Search", "date": "2012-12-13"},
{"website": "Twitter-Engineering", "title": "How our photo filters came into focus", "author": ["‎@ryfar‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/how-our-photo-filters-came-into-focus.html", "abstract": "The old adage “a picture is worth a thousand words” is very apt for Twitter: a single photo can express what otherwise might require many Tweets. Photos help capture whatever we’re up to: kids’ birthday parties, having fun with our friends, the world we see when we travel. Like so many of you, lots of us here at Twitter really love sharing filtered photos in our tweets. As we got into doing it more often, we began to wonder if we could make that experience better, easier and faster. After all, the now-familiar process for tweeting a filtered photo has required a few steps: 1. Take the photo (with an app) 2. Filter the photo (probably another app) 3. Finally, tweet it! Constantly needing to switch apps takes time, and results in frustration and wasted photo opportunities. So we challenged ourselves to make the experience as fast and simple as possible. We wanted everyone to be able to easily tweet photos that are beautiful, timeless, and meaningful. With last week’s photo filters release, we think we accomplished that on the latest versions of Twitter for Android and Twitter for iPhone. Now we’d like to tell you a little more about what went on behind the scenes in order to develop this new photo filtering experience. It’s all about the filters Our guiding principle: to create filters that amplify what you want to express, and to help that expression stand the test of time. We began with research, user stories, and sketches. We designed and tested multiple iterations of the photo-taking experience, and relied heavily on user research to make decisions about everything from filters nomenclature and iconography to the overall flow. We refined and distilled until we felt we had the experience right. We spent many hours poring over the design of the filters. Since every photo is different, we did our analyses across a wide range of photos including portraits, scenery, indoor, outdoor and low-light shots. We also calibrated details ranging from color shifts, saturation, and contrast, to the shape and blend of the vignettes before handing the specifications over to Aviary, a company specializing in photo editing. They applied their expertise to build the algorithms that matched our filter specs. Make it fast! Our new photo filtering system is a tight integration of Aviary’s cross-platform GPU-accelerated photo filtering technology with our own user interface and visual specifications for filters. Implementing this new UI presented some unique engineering challenges. The main one was the need to create an experience that feels instant and seamless to use — while working within constraints of memory usage and processing speed available on the wide range of devices our apps support. To make our new filtering experience work, our implementation keeps up to four full-screen photo contexts in memory at once: we keep three full-screen versions of the image for when you’re swiping through photos (the one you’re currently looking at plus the next to the right and the left), and the fourth contains nine small versions of the photo for the grid view. And every time you apply or remove a crop or magic enhance, we update the small images in the grid view to reflect those changes, so it’s always up to date. Without those, you could experience a lag when scrolling between photos — but mobile phones just don’t have a lot of memory. If we weren’t careful about when and how we set up these chunks of memory, one result could be running out of memory and crashing the app. So we worked closely with Aviary’s engineering team to achieve a balance that would work well for many use cases. Test and test some more As soon as engineering kicked off, we rolled out this new feature internally so that we could work out the kinks, sanding down the rough spots in the experience. At first, the team tested it, and then we opened it up to all employees to get lots of feedback. We also engaged people outside the company for user research. All of this was vital to get a good sense about which aspects of the UI would resonate, or wouldn’t. After much testing and feedback, we designed an experience in which you can quickly and easily choose between different filtering options – displayed side by side, and in a grid. Auto-enhancement and cropping are both a single tap away in an easy-to-use interface. Finally, a collaborative team of engineers, designers and product managers were able to ship a set of filters wrapped in a seamless UI that anyone with our Android or iPhone app can enjoy. And over time, we want our filters to evolve so that sharing and connecting become even more delightful. It feels great to be able to share it with all of you at last. Posted by @ryfar Tweet Composer Team", "date": "2012-12-20"},
{"website": "Twitter-Engineering", "title": "Right-to-left support for Twitter Mobile", "author": ["‎@ctieu‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012/right-to-left-support-for-twitter-mobile.html", "abstract": "Thanks to the efforts of our translation volunteers , last week we were able to launch right-to-left language support for our mobile website in Arabic and Farsi. Two interesting challenges came up during development for this feature: 1) We needed to support a timeline that has both right-to-left (RTL) and left-to-right (LTR) tweets. We also needed to make sure that specific parts of each tweet, such as usernames and URLs, are always displayed as LTR. 2) For our touch website, we wanted to flip our UI so that it was truly an RTL experience. But this meant we would need to change a lot of our CSS rules to have reversed values for properties like padding, margins, etc. — both time-consuming and unsustainable for future development. We needed a solution that would let us make changes without having to worry about adding in new CSS rules for RTL every time. In this post, I detail how we handled these two challenges and offer some general RTL tips and other findings we gleaned during development. General RTL tips The basis for supporting RTL lies in the dir element attribute, which can be set to either ltr or rtl. This allows you to set an element’s content direction, so that any text/children nodes would render in the orientation specified. You can see the difference below: In the first row, the text in the LTR column is correct, but in the second it’s the text in the RTL column. Since this attribute can be used on any element, it can a) be used to change the direction of inline elements, such as links (see “Handling bidirectional tweet content” below) and b) if added to the root html node then the browser will flip the order of all the elements on the page automatically (see “Creating a right-to-left UI” below). The other way to change content direction lies in the direction and unicode-bidi CSS properties. Just like the dir attribute, the direction property allows you to specify the direction within an element. However, there is one key difference: while direction will affect any block-level elements, for it to affect inline elements the unicode-bidi property must be set to embed or override. Using the dir attribute acts as if both those properties were applied, and is the preferred method as bidi should be considered a document change, not a styling one. For more on this, see the “W3C directionality specs” section below. Handling bidirectional tweet content One of the things we had to think about was how to properly align each tweet depending on the dominant directionality of the content characters. For example, a tweet with mostly RTL characters should be right-aligned and read from right to left. To figure out which chars were RTL, we used this regex: /[\\u0600-\\u06FF]|[\\u0750-\\u077F]|[\\u0590-\\u05FF]|[\\uFE70-\\uFEFF]/m Then depending on how many chars matched, we could figure out the direction we’d want to apply to the tweet. However, this would also affect the different entities that are in a tweet’s content. Tweet entities are special parts included in the text that has their own context applied to them, such as usernames and hashtags. Usernames and URLs should always be displayed as LTR, while hashtags may be RTL or LTR depending on what the first character is. To solve this, while parsing out entities we also make sure that the correct direction was applied to the element the entities were contained in. If you are looking to add RTL support for your site and you have dynamic text with mixed directionality, besides using the dir attribute or direction property, you could also look into the \\u200e () and the \\u200f () characters. These are invisible control markers that tell the browser how the following text should be displayed. But be careful; conflicts can arise if both the dir / direction and character marker methods are used together. Or if you are using Ruby, Twitter has a great localization gem called TwitterCldr which can take a string and insert these markers appropriately. Creating a right-to-left UI For our mobile touch website, we would first detect what language the user’s browser is set in. When it’s one of our supported RTL languages, we add the dir attribute to our page. The browser will then flip the layout of the site so that everything was rendered on the right-hand side first. This worked fairly well on basic alignment of the page; however, this did not change how all the elements are styled. Properties like padding, margin, text-align, and float will all have the same values, which means that the layout will look just plain wrong in areas where these are applied. This can be the most cumbersome part of adding RTL support to a website, as it usually means adding special rules to your stylesheets to handle this flipped layout. For our mobile touch website, we are using Google Closure as our stylesheet compiler. This has an extremely convenient flag called —output-orientation, which will go through your stylesheets and adjust the rules according to the value (LTR or RTL) you pass in. By running the stylesheet compilation twice, once with this flag set to RTL, we get two stylesheets that are the mirror images of each other. This fixed nearly all styling issues that came from needing to flip CSS values. In the end, there were only two extra rules that we needed to add to the RTL stylesheet - those were put into rtl.css which gets added on as the last input file for the RTL compilation, thusly overriding any previous rules that were generated. After that, it’s just a matter of including the right stylesheet for the user’s language and voila! a very nicely RTL’d site with minimal extra effort on the development side. One last thing that we needed to think about was element manipulation with JS. Since elements will now be pushed as far to the right as possible instead of to the far left, the origin point in which an element starts at may be very different than what you’d expect - possibly even out of the visible area in a container. For example, we had to change the way that the media strip in our photo gallery moved based on the page’s directionality. Besides coordinates changing, an LTR user would drag starting from the right, then ending to the left in order to see more photos. For an RTL user, the natural inclination would be to start at a left point and drag to the right. This is something that can’t be handled automatically as with our stylesheet compiler, so it comes down to good old-fashioned programming to figure out how we wanted elements to move. Improving translations We would like to thank our amazing translations community for helping us get to this point. Without your efforts,we would not have been able to launch this feature onto mobile Twitter. And although we’ve made great strides in supporting RTL, we still have more work to do. We would love to have more translations for other languages that are not complete yet, such as our other two RTL languages Hebrew and Urdu. Visit translate.twitter.com to see how you can help us add more languages to Twitter. Helpful Resources W3C directionality specs: dir direction / unicode-bidi dir versus direction More resources: Mozilla TwitterCLDR Ruby Gem Google Closure Stylesheets XKCD’s example of another control character Posted by Christine Tieu ( @ctieu ) Engineer, Mobile Web Team", "date": "2012-12-21"},
{"website": "Twitter-Engineering", "title": "2012", "author": ["‎@‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2012.html", "abstract": "", "date": "1970-01-01"},
{"website": "Twitter-Engineering", "title": "CocoaSPDY: SPDY for iOS / OS X", "author": ["‎@goaway‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013/cocoaspdy-spdy-for-ios-os-x.html", "abstract": "For over a year now, Twitter has supported the SPDY protocol and today it accounts for a significant percentage of our web traffic. SPDY aims to improve upon a number of HTTP’s shortcomings and one client segment in particular that has a lot of potential to benefit is mobile devices. Cellular networks still suffer from high latency, so reducing client-server roundtrips can have a pronounced impact on a user’s experience. Up until now, the primary clients that supported SPDY were browsers, led by Chrome and Firefox. Today we’re happy to announce that in addition to rolling out SPDY to our iOS app users around the world, we’re also open sourcing a SPDY framework, CocoaSPDY , for iOS and OS X apps under the permissive Apache 2.0 license. The code is written in Objective-C against the CoreFoundation and Cocoa/Cocoa Touch interfaces and has no external dependencies. What exactly is SPDY? SPDY was originally designed at Google as an experimental successor to HTTP. It’s a binary protocol (rather than human-readable like HTTP), but is fully compatible with HTTP. In fact, current draft work on HTTP/2.0 is largely based on the SPDY protocol and its real-world success. At Twitter, we have been participating with other companies in the evolution of the specification and adopting SPDY across our technology stack. We’ve also contributed implementations to open source projects such as Netty . For more details about SPDY, we encourage you to read the whitepaper and the latest specification . Enabling SPDY One of our primary goals with our SPDY implementation was to make integration with an existing application as easy, transparent, and seamless as possible. With that in mind, we created two integration mechanisms—one for NSURLConnection and one for NSURLSession—each of which could begin handling an application’s HTTP calls with as little as a one-line change to the code. While we’ve since added further options for customized behavior and configuration to the interface, it’s still possible to enable SPDY in an iOS or OS X application with a diff as minimal as this: +[SPDYURLConnectionProtocol registerOrigin:@\" https://api.twitter.com:443 \"]; The simplicity of this integration is in part thanks to Apple’s own well-designed NSURLProtocol interface. Performance Improvements We’re still actively experimenting with and tuning our SPDY implementation in order to improve the user’s experience in our app as much as possible. However, we have measured as much as a 30% decrease in latency in the wild for API requests carried over SPDY relative to those carried over HTTP. In particular, we’ve observed SPDY helping more as a user’s network conditions get worse . This can be seen in the long tail of the following charts tracking a sample of cellular API traffic: The x-axis is divided into the 50th percentile (median), 75th, 95th and 99th percentiles of HTTP and SPDY requests. The y-axis represents the raw, unadjusted request latency in milliseconds. Note that latency is a fairly coarse metric that can’t fully demonstrate the benefits of SPDY. Future Work and Getting Involved CocoaSPDY is young project under active development, we welcome feedback and contributions. If you’re interested in getting involved outside of usage, some of our future plans include: Server Push Discretionary Request Dispatch HTTP/2.0 Support Feature requests or bugs should be reported on the GitHub issue tracker . Acknowledgements The CocoaSPDY library and framework was originally authored by Michael Schore ( @goaway ) and Jeff Pinner ( @jpinner ) and draws significantly from the Netty implementation . Testing and deploying the protocol has been an ongoing collaboration between Twitter’s TFE and iOS Platform teams, with many individuals contributing to its success. We would also like to extend a special thanks to Steve Algernon ( @salgernon ) at Apple, for providing his insight and assistance with some of the nuances of the Cocoa networking stack.", "date": "2013-12-19"},
{"website": "Twitter-Engineering", "title": "Forward Secrecy at Twitter", "author": ["‎@j4cob‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013/forward-secrecy-at-twitter.html", "abstract": "As part of our continuing effort to keep our users’ information as secure as possible, we’re happy to announce that we recently enabled forward secrecy for traffic on twitter.com, api.twitter.com, and mobile.twitter.com. On top of the usual confidentiality and integrity properties of HTTPS, forward secrecy adds a new property. If an adversary is currently recording all Twitter users’ encrypted traffic, and they later crack or steal Twitter’s private keys, they should not be able to use those keys to decrypt the recorded traffic. As the Electronic Frontier Foundation points out , this type of protection is increasingly important on today’s Internet. Under traditional HTTPS, the client chooses a random session key, encrypts it using the server’s public key, and sends it over the network. Someone in possession of the server’s private key and some recorded traffic can decrypt the session key and use that to decrypt the entire session. In order to support forward secrecy, we’ve enabled the EC Diffie-Hellman cipher suites. Under those cipher suites, the client and server manage to come up with a shared, random session key without ever sending the key across the network, even under encryption. The details of this remarkable and counterintuitive key exchange are explained at Wikipedia’s excellent article on Diffie-Hellman key exchange . The server’s private key is only used to sign the key exchange, preventing man-in-the-middle attacks. There are two main categories of Diffie-Hellman key exchange. Traditional Diffie-Hellman (DHE) depends on the hardness of the Discrete Logarithm Problem and uses significantly more CPU than RSA, the most common key exchange used in SSL. Elliptic Curve Diffie-Hellman (ECDHE) is only a little more expensive than RSA for an equivalent security level. Vincent Bernat ( @vince2_ ) benchmarked ECDHE at a 15% overhead relative to RSA over 2048-bit keys. DHE, by comparison, used 310% more CPU than RSA. Forward secrecy is just the latest way in which Twitter is trying to defend and protect the user’s voice. In practical deployment, we found that enabling and prioritizing ECDHE cipher suites actually caused negligible increase in CPU usage. HTTP keepalives and session resumption mean that most requests do not require a full handshake, so handshake operations do not dominate our CPU usage. We find 75% of Twitter’s client requests are sent over connections established using ECDHE. The remaining 25% consists mostly of older clients that don’t yet support the ECDHE cipher suites. The last obstacle to correctly implementing forward secrecy was our use of TLS session tickets . We use TLS session tickets to allow clients to reconnect quickly using an abbreviated handshake if they still have a session ticket from a recent connection. Beside the CPU savings, this saves one network round-trip, commonly around 150ms and often much more on mobile networks. Session tickets enable pools of servers to support session resumption without need for a shared session cache. However, as Adam Langley ( @agl__ ) points out in his blog post How to Botch TLS Forward Secrecy : If you run several servers then they all need to share the same session ticket key otherwise they can’t decrypt each other’s session tickets. In this case your forward secrecy is limited by the security of that file on disk. Since your private key is probably kept on the same disk, enabling forward secure cipher suites probably hasn’t actually achieved anything other than changing the file that the attacker needs to steal! You need to generate session ticket keys randomly, distribute them to the servers without ever touching persistent storage and rotate them frequently. We implemented such a key rotation system with a few additional goals: It should be simple to implement, simple to maintain, resistant to failure, and we should be able to restart the frontend process during deploys without waiting to receive new session ticket keys. To do so, we have a set of key generator machines, of which one is the leader. The leader generates a fresh session ticket key every twelve hours and zeroes old keys after thirty-six hours. Keys are stored in tmpfs (a RAM-based filesystem), with no swap partitions configured. It’s important that there be no swap, because tmpfs will use swap if available, which could write keys to long-term disk storage. Every five minutes, our frontends fetch the latest ticket keys from a key generator machine via SSH. This traffic is also forward secret by SSH’s Diffie-Hellman key exchange. Again, the keys are stored on a tmpfs, so they survive a restart of the frontend process without being leaked to disk. When a new ticket key K is available, we don’t want to start encrypting new tickets with it until we expect that each frontend has a copy of K and can decrypt those tickets. We use a timestamp in K’s filename to determine its age; once it is at least twenty minutes old, it is considered current and frontends will start encrypting tickets with it. In general, frontends will have three ticket keys available: the current key, and the two previous keys. They can decrypt session tickets using any of those three. When a client resumes a session using one of the previous keys, a frontend will finish the abbreviated handshake and assign a new session ticket using the current key. This logic is implemented in a callback provided to OpenSSL’s SSL_CTX_set_tlsext_ticket_key_cb . The New Normal At the end of the day, we are writing this not just to discuss an interesting piece of technology, but to present what we believe should be the new normal for web service owners. A year and a half ago, Twitter was first served completely over HTTPS. Since then, it has become clearer and clearer how important that step was to protecting our users’ privacy. If you are a webmaster, we encourage you to implement HTTPS for your site and make it the default. If you already offer HTTPS, ensure your implementation is hardened with HTTP Strict Transport Security , secure cookies , certificate pinning , and Forward Secrecy . The security gains have never been more important to implement. If you don’t run a website, demand that the sites you use implement HTTPS to help protect your privacy, and make sure you are using an up-to-date web browser so you are getting the latest security improvements. Security is an ever-changing world. Our work on deploying forward secrecy is just the latest way in which Twitter is trying to defend and protect the user’s voice in that world. Acknowledgements Like all projects at Twitter, this has been a collaboration. We’d like to thank all involved, including: Jeff Hodges , Jeffrey Pinner , Kyle Randolph , Jan Schaumann , Matt Finifter , the entirety of our Security and Twitter Frontend teams, and the broader security and cryptography communities.", "date": "2013-11-22"},
{"website": "Twitter-Engineering", "title": "Slave Recovery in Apache Mesos", "author": ["‎@vinodkone‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013/slave-recovery-in-apache-mesos.html", "abstract": "Cross-posted from the Apache Mesos blog . With the latest Mesos release, 0.14.1 , we are bringing high availability to the slaves by introducing a new feature called Slave Recovery. In a nutshell, slave recovery enables: Tasks and their executors to keep running when the slave process is down A restarted slave process to reconnect with running executors/tasks on the slave Announcing a new version of #Mesos , 0.14.1! Available for download now: http://t.co/e6UmTdjSrD Release notes online: https://t.co/XtRzj2y7Tt — Apache Mesos ( @ApacheMesos ) October 22, 2013 High availability is one of the key features of Mesos. For example, a typical Mesos cluster in production involves 3 to 5 masters with one acting as leader and the rest on standby. When a leading master fails due to a crash or goes offline for an upgrade, a standby master automatically becomes the leader without causing any disruption to running tasks. Leader election is currently performed by using Apache ZooKeeper . Why it matters? A recoverable slave is critical for running services in production on Mesos for several reasons: Stateful services In a typical production environment there are stateful services (e.g., caches) running in the cluster. It is not uncommon for these services to have a high startup time (e.g., cache warm up time of a few hours). Even in the analytics world, there are cases where a single task is responsible for doing work that takes hours to complete. In such cases a restart of the slave (e.g., crash or upgrade) will have a huge impact on the service. While sharding the service wider mitigates this impact it is not always possible to do so (e.g., legacy services, data locality, application semantics). Such stateful applications would benefit immensely from running under a more resilient slave. Faster cluster upgrades It is important for clusters to frequently upgrade their infrastructure to stay up-to-date with the latest features and bug fixes. A typical Mesos slave update involves stopping it, updating its libraries and starting it back. In production environments there is always tension between upgrading the infrastructure frequently and the need to not impact long running services. With respect to Mesos upgrades, if upgrading the slave binary has no impact on underlying services, then it is a win for both cluster operators and service owners. Resilience against slave failures While upgrading the slaves the most often the reason for restarting slaves, there might be other causes for a slave to fail. A slave crash could happen due to a bug in the slave code or due to external factors like a bug in the kernel or ZooKeeper client library. Typically such crashes are temporary and a restart of the slave is enough to correct the situation. If such slave restarts do not affect applications running on the slave it is a huge win for the applications. How it works? Checkpointing Slave recovery works by having the slave checkpoint enough information (e.g., task information, executor information, status updates) about the running tasks and executors to local disk. Once the slave and the framework(s) enable checkpointing, any subsequent slave restarts would recover the checkpointed information and reconnect with the executors. When a checkpointing slave process goes down, both the leading master and the executors running on the slave host wait for the slave to come back up and reconnect. A nice thing about slave recovery is that frameworks and their executors/tasks are oblivious to a slave restart. Executor Driver Caching As part of this feature, the executor driver has also been improved to make it more resilient in the face of a slave failure. As an example, status updates sent by the executor while the slave is down are cached by the driver and sent to the slave when it reconnects with the restarted slave. Since this is all handled by the executor driver, framework and executor writers do not have to worry about it. The executors can keep sending status updates for their tasks while remaining oblivious to the slave being up or down. Reliable status updates Another benefit of slave checkpointing the status updates is that now updates are more reliably delivered to frameworks in the face of failures. Before slave recovery if the slave fails at the same time that a master is failing over, no TASK_LOST updates for tasks running on the slave were sent to the framework. This is partly because the Mesos master is stateless. A failed over master reconstructs the cluster state from the information given to it by re-registering slaves and frameworks. With slave recovery, status updates and tasks are no longer lost when slaves fail. Rather, the slave recovers tasks, status updates and reconnects with the running executors. Even if an executor does terminate when the slave is down, a recovered slave knows about it from its checkpointed state and reliably sends TASK_LOST updates to the framework. For more information about how to enable slave recovery in your cluster, please refer to the documentation . Looking ahead Easy Executor/Task Upgrades In a similar vein to how slave recovery makes upgrading a Mesos cluster easy, we would like to enable frameworks to upgrade their executors/tasks as well. Currently the only way to upgrade an executor/task is to kill the old executor/task and launch the new upgraded executor/task. For the same reasons as we have discussed earlier this is not ideal for stateful services. We are currently investigating proper primitives for enabling frameworks to do such upgrades, so that not every framework have to (re-)implement that logic. State Reconciliation While slave recovery greatly improves the reliability of delivering status updates, there are still some rare cases where updates could be lost. For example, if a slave crashes when a master is failing over and never comes back then the new leading master doesn’t know about the lost slave and executors/tasks running on it. In addition to status updates, any driver methods (e.g., launchTasks, killTask) invoked when the master is failing over are silently dropped. Currently, frameworks are responsible for reconciling their own task state using the reconciliation API. We are currently investigating ways to provide better guarantees around reconciliation. Self Updates of Mesos Currently, updating Mesos involves a cluster operator to manually upgrade the master and slave binaries and roll them in a specific manner (e.g., masters before slaves). But what if Mesos could update itself? It is not hard to imagine a future where Mesos masters can orchestrate the upgrade of slaves and maybe also upgrade one another! This would also help making rollbacks easy incase an upgrade doesn’t work because it would be much easier for Mesos to check if various components are working as expected. So what are you waiting for? Go ahead and give Mesos a whirl and let us know what you think via Twitter at @ApacheMesos . Also, Mesos is an independent open source community that’s organized by its members, including you! Whether you’re running or writing a framework, or hacking the core, there are opportunities for you to get in touch and ask questions (via the mailing list ), get involved locally or contribute back.", "date": "2013-11-11"},
{"website": "Twitter-Engineering", "title": "Twitter University on YouTube", "author": ["‎@telotype‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013/twitter-university-on-youtube.html", "abstract": "Here at Twitter, we embrace open source and support the ongoing education of our team, as well as the general public. The intersection of these domains is particularly important to us, and led to the establishment of Twitter University , which supports these efforts and helps make Twitter the best place in the world for engineers to work. Let’s start Friday morning off right, with an Introduction to @summingbird with @posco & @sritchie - http://t.co/ho3vexgxtY @TwitterOSS — Twitter University ( @university ) September 27, 2013 We believe a fundamental component of this effort is engaging the tech community at large in meaningful conversations, and sharing these conversations with the world through meetups, workshops and conferences. More and more of this content will be broadcasted via our YouTube channel . “Hadoop on @ApacheMesos at Airbnb” http://t.co/04dS4g3Jcp — Twitter Open Source ( @TwitterOSS ) September 10, 2013 In just our first few months we’ve released presentations on Mesos , Summingbird , Docker and Gradle . We’ve got plenty more to come, and we’re excited to share. Please reach out to us on Twitter if you’re interested in engaging with us and subscribe to our YouTube channel to stay plugged in.", "date": "2013-10-25"},
{"website": "Twitter-Engineering", "title": "Netty 4 at Twitter: Reduced GC Overhead", "author": ["‎@trustin‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013/netty-4-at-twitter-reduced-gc-overhead.html", "abstract": "At Twitter, Netty ( @netty_project ) is used in core places requiring networking functionality. For example: Finagle is our protocol agnostic RPC system whose transport layer is built on top of Netty, and it is used to implement most services internally like Search TFE (Twitter Front End) is our proprietary spoon-feeding reverse proxy which serves most of public-facing HTTP and SPDY traffic using Netty Cloudhopper sends billions of SMS messages every month to hundreds of mobile carriers all around the world using Netty For those who aren’t aware, Netty is an open source Java NIO framework that makes it easier to create high-performing protocol servers. An older version of Netty v3 used Java objects to represent I/O events. This was simple, but could generate a lot of garbage especially at our scale. In the new Netty 4 release, changes were made so that instead of short-lived event objects, methods on long-lived channel objects are used to handle I/O events. There is also a specialized buffer allocator that uses pools. We take the performance, usability, and sustainability of the Netty project seriously, and we have been working closely with the Netty community to improve it in all aspects. In particular, we will discuss our usage of Netty 3 and will aim to show why migrating to Netty 4 has made us more efficient. Reducing GC pressure and memory bandwidth consumption A problem was Netty 3’s reliance on the JVM’s memory management for buffer allocations. Netty 3 creates a new heap buffer whenever a new message is received or a user sends a message to a remote peer. This means a ‘new byte[capacity]’ for each new buffer. These buffers caused GC pressure and consumed memory bandwidth: allocating a new byte array consumes memory bandwidth to fill the array with zeros for safety. However, the zero-filled byte array is very likely to be filled with the actual data, consuming the same amount of memory bandwidth. We could have reduced the consumption of memory bandwidth to 50% if the Java Virtual Machine (JVM) provided a way to create a new byte array which is not necessarily filled with zeros, but there’s no such way at this moment. To address this issue, we made the following changes for Netty 4. Removal of event objects Instead of creating event objects, Netty 4 defines different methods for different event types. In Netty 3, the ChannelHandler has a single method that handles all event objects: class Before implements ChannelUpstreamHandler { void handleUpstream(ctx, ChannelEvent e) { if (e instanceof MessageEvent) { ... } else if (e instanceof ChannelStateEvent) { ... } ... } } Netty 4 has as many handler methods as the number of event types: class After implements ChannelInboundHandler { void channelActive(ctx) { ... } void channelInactive(ctx) { ... } void channelRead(ctx, msg) { ... } void userEventTriggered(ctx, evt) { ... } ... } Note a handler now has a method called ‘ userEventTriggered ’ so that it does not lose the ability to define a custom event object. Buffer pooling Netty 4 also introduced a new interface, ‘ ByteBufAllocator ’. It now provides a buffer pool implementation via that interface and is a pure Java variant of jemalloc , which implements buddy memory allocation and slab allocation . Now that Netty has its own memory allocator for buffers, it doesn’t waste memory bandwidth by filling buffers with zeros. However, this approach opens another can of worms—reference counting. Because we cannot rely on GC to put the unused buffers into the pool, we have to be very careful about leaks. Even a single handler that forgets to release a buffer can make our server’s memory usage grow boundlessly. Was it worthwhile to make such big changes? Because of the changes mentioned above, Netty 4 has no backward compatibility with Netty 3. It means our projects built on top of Netty 3 as well as other community projects have to spend non-trivial amount of time for migration. Is it worth doing that? We compared two echo protocol servers built on top of Netty 3 and 4 respectively. (Echo is simple enough such that any garbage created is Netty’s fault, not the protocol). I let them serve the same distributed echo protocol clients with 16,384 concurrent connections sending 256-byte random payload repetitively, nearly saturating gigabit ethernet. According to our test result, Netty 4 had: 5 times less frequent GC pauses: 45.5 vs. 9.2 times/min 5 times less garbage production: 207.11 vs 41.81 MiB/s I also wanted to make sure our buffer pool is fast enough. Here’s a graph where the X and Y axis denote the size of each allocation and the time taken to allocate a single buffer respectively: As you see, the buffer pool is much faster than JVM as the size of the buffer increases. It is even more noticeable for direct buffers. However, it could not beat JVM for small heap buffers, so we have something to work on here. Moving forward Although some parts of our services already migrated from Netty 3 to 4 successfully, we are performing the migration gradually. We discovered some barriers that slow our adoption that we hope to address in the near future: Buffer leaks : Netty has a simple leak reporting facility but it does not provide information detailed enough to fix the leak easily. Simpler core : Netty is a community driven project with many stakeholders that could benefit from a simpler core set of code. This increases the instability of the core of Netty because those non-core features tend to lead to collateral changes in the core. We want to make sure only the real core features remain in the core and other features stay out of there. We also are thinking of adding more cool features such as: HTTP/2 implementation HTTP and SOCKS proxy support for client side Asynchronous DNS resolution (see pull request ) Native extensions for Linux that works directly with epoll via JNI Prioritization of the connections with strict response time constraints Getting Involved What’s interesting about Netty is that it is used by many different people and companies worldwide, mostly not from Twitter. It is an independent and very healthy open source project with many contributors . If you are interested in building ‘the future of network programming’, why don’t you visit the project web site , follow @netty_project , jump right into the source code at GitHub or even consider joining the flock to help us improve Netty? Acknowledgements Netty project was founded by Trustin Lee ( @trustin ) who joined the flock in 2011 to help build Netty 4. We also like to thank Jeff Pinner ( @jpinner ) from the TFE team who gave many great ideas mentioned in this article and became a guinea pig for Netty 4 without hesitation. Furthermore, Norman Maurer ( @normanmaurer ), one of the core Netty committers, made an enormous amount of effort to help us materialize the great ideas into actually shippable piece of code as part of the Netty project. There are also countless number of individuals who gladly tried a lot of unstable releases catching up all the breaking changes we had to make, in particular we would like to thank: Berk Demir ( @bd ), Charles Yang ( @cmyang ), Evan Meagher ( @evanm ), Larry Hosken ( @lahosken ), Sonja Keserovic ( @thesonjake ), and Stu Hood ( @stuhood ).", "date": "2013-10-15"},
{"website": "Twitter-Engineering", "title": "Mobile app development: Catching crashers", "author": ["‎@sandofsky‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013/mobile-app-development-catching-crashers.html", "abstract": "Before Twitter for iOS code reaches production, we run it through static analysis, automated testing, code review, manual verification, and employee dogfooding. In this last step, we distribute beta builds to our employees to collect real-world feedback on products and monitor code stability through crash reports. Detailed crash data has had a huge impact to our development process, and significantly improved mobile app performance and quality. This post describes two types of bugs, and how we used detailed data from Crashlytics to diagnose and fix them. Micro-bugs The most common crashers are not elaborate: You forgot to nil out delegate properties when the delegate is deallocated You didn’t check whether a block was nil before calling it You released an object in the wrong order These mistakes are solved with rote patterns. Sometimes you haven’t learned a pattern. Sometimes you know them, but lack rigor. We experienced the latter with category prefixing. Objective-C’s Categories allow you to attach new methods to classes you don’t own. For instance, we could attach isTweetable to NSString, which returns YES if the string is less than or equal to 140 characters. It’s part of Cocoa best practices to prefix categories with something unique. Instead of isTweetable, call your method tw_isTweetable. Then your app is safe if iOS adds a method with the same name to that class in the future. In the past, we usually did this, but missed a few categories. Late last year, we noticed several crashes consistently plaguing a small number of users. They were related to innocuous categories, but iOS documentation didn’t point to any name collisions. If we can’t reproduce the crash, we try to isolate the problem with crash environment data. Does it affect users of an older version of iOS? Certain hardware? Is it under a low-memory situation? Crashlytics revealed most of these crashes were on jailbroken devices. It turned out the jailbreak environment added its own unprefixed categories to core classes, and they shared the same names as our own categories. We discovered another set of crashes related to categories, on non-jailbroken devices. Older categories were inconsistently prefixed and collided with private categories used by the system frameworks. Sometimes you do the right thing — and just have bad luck. Macro-bugs You should start with the simplest solution that works, but one day you will outgrow that solution. With a more complex architecture come more complex edge cases and their bugs. For example, Twitter for iPhone originally used NSKeyedArchiver for storage. To get more granular control over what we loaded from disk at launch, we moved to SQLite. Benchmarking on older hardware revealed that if we wanted to keep the main thread responsive, we had to enter the thorny world of SQLite multithreading. In our first implementation, we used a background queue to write incoming Tweets to a staging table. Then we bounced back to the main thread to replace the main table with the staging table. This looked fine during testing, but crash reports revealed database lock errors. SQLite’s write locks are not table level, but global. We had to serialize all write operations, so we rewrote the framework to use one GCD queue for reads and one for writes. Fixing that crash cleared the way for the next one: you should not share the same database connection between threads. You should open the connection on the thread where you’re going to use it. However, GCD makes no promises that items in one operation queue are dispatched to the same thread. We rewrote the framework to use native threads instead of GCD, and watched the graph of crashes dramatically drop. What lingered were database schema lock errors. We traced them back to the SQLite Shared Cache . Disabling it eliminated the remaining crashes. While iTunes Connect collects crash reports from production builds, Crashlytics lets us collect crashes from employee dogfood builds, which dramatically reduces the feedback cycle. Instead of iterating on several public releases, we quickly address the crashes internally, and ship a better version to users. On Crashlytics Last year, our Twitter for iOS team –– Satoshi Nakagawa, Bob Cottrell, Zhen Ma and I –– started using Crashlytics as an internal crash reporting framework. It was clear their analysis was the best available, immediately catching crashes other frameworks missed. Their web front-end was far more mature than what we were building internally. We liked them so much, we welcomed them to the flock . Today, the Crashlytics team released the Android SDK , which our Android engineers have been beta testing. We’d all recommend you give it a try. Posted by Ben Sandofsky ( @sandofsky ) Tech Lead, Twitter for Mac (previously, Twitter for iOS)", "date": "2013-05-30"},
{"website": "Twitter-Engineering", "title": "Braindump", "author": ["‎@Twitter‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013/braindump.html", "abstract": "Cross-posted from @skr’s blog . The Twitter stack For various reasons, including performance and cost, Twitter has poured significant engineering effort into breaking down the site backend into smaller JVM based services. As a nice side effect we’ve been able to open source several of the libraries and other useful tools that came out of this effort. While there is a fair amount of information about these projects available as docs or slides I found no simple, high level introduction to what we can unofficially call the Twitter stack. So here it is. It’s worth noting that all this information is about open source projects, that it is public already and that I am not writing this as part of my job at Twitter or on their behalf. Now, granted these were not all conceived at Twitter and plenty of other companies have similar solutions. However I think the software mentioned below is quite powerful and with most of it released as open source it is a fairly compelling platform to base new services off of. I will describe the projects from a Scala perspective, but quite a few are useful in Java programs as well. See the Twitter Scala school for an intro to the language, although that is not required to understand this post. Finagle At the heart of a service lies the Finagle library. By abstracting away the fundamental underpinnings of an RPC system, Finagle greatly reduces the complexity that service developers have to deal with. It allows us to focus on writing application-specific business logic instead of dwelling on lower level details of distributed systems. Ultimately the website itself uses these services to perform operations or fetch data needed to render the HTML. At Twitter the internal services use the Thrift protocol, but Finagle supports other protocols too such as Protocol buffers and HTTP. Setting up a service using Finagle A quick dive into how you would set up a Thrift service using Finagle. Write a Thrift file defining your API. It should contain the structs, exceptions and methods needed to describe the service functionality. See Thrift Interface Description Language (IDL) docs , in particular the examples at the end for more info. Use the Thrift file as input for a code generator that spits out code in your language. For Scala and Finagle based projects I would recommend Scrooge . Implement the Scala trait generated from your Thrift IDL. This is where the actual functionality of your service goes. Provide the Finagle server builder an instance of the implementation above, a port to bind to and any other settings you might need and start it up. That looks pretty similar to just using plain Thrift without Finagle. However, there are quite a few improvements such as excellent monitoring support, tracing and Finagle makes it easy to write your service in an asynchronous fashion. More about these features later. You can also use Finagle as a client. It takes care of all the boring stuff such as timeouts, retries and load balancing for you. Ostrich So let’s say we have a Finagle Thrift service running. It’s doing very important work. Obviously you want to make sure it keeps doing that work and that it performs well. This is where Ostrich comes in. Metrics Ostrich makes it easy to expose various metrics from your service. Let’s say you want to count how many times a particular piece of code is run. In your service you’d write a line of code that looks something like this: Stats.incr(“some_important_counter”) As simple as that. The counter named some_important_counter will be incremented by 1. In addition to just straight up counters you can get gauges that report on the value of a variable: Stats.addGauge(\"current_temperature\") { myThermometer.temperature } or you can time a snippet of code to track the performance Stats.time(\"translation\") { document.translate(\"de\", \"en\") } Those and other examples can be found in the Ostrich readme . Export metrics Ostrich runs a small http admin interface to expose these metrics and other functionality. To fetch them you would simply hit http://hostname:port/stats.json to get the current snapshot of the metrics as JSON. At Twitter the stats from each service will be ingested from Ostrich by our internal observability stack, providing us with fancy graphs, alerting and so on. To tie this back to our previous section: If you provide a Finagle client or server builder with an Ostrich backed StatsReceiver it’ll happily splurt out tons of metrics about how the service is performing, the latencies for the RPC calls and the number of calls to each method to name a few. Ostrich can also deal with configuring your service, shutting down all the components gracefully and more. This is an example of what a dashboard could look like with stats gathered from Ostrich by our observability stack. Screenshot from @raffi ’s presentation deck. Zipkin Ostrich and Finagle combined gives us good service level metrics. However, one downside of a more service oriented architecture is that it’s hard to get a high level performance overview of a single request throughout the stack. Perhaps you are a developer tasked with improving performance of a particular external api endpoint. With Zipkin you can get a visual representation of where most of the time to fulfill the request was spent. Think Firebug or Chrome developer tools for the back end. Zipkin is a implementation of a tracing system based off of the Google Dapper paper. Finagle-Zipkin So how does it work? There’s a finagle-zipkin module that will hook into the transmission logic of Finagle and time each operation performed by the service. It also passes request identifiers down to any services it relies on, this is how we can tie all the tracing data together. The tracing data is logged to the Zipkin backend and finally we can display and visualize that data in the Zipkin UI. Let’s say we use Zipkin to inspect a request and we see that it spent most of it’s time waiting for a query to a MySQL database. We could then also see the actual SQL query sent and draw some conclusions from it. Other times perhaps a GC in a Scala service was a fault. Either way, the hope is that a glance at the trace view will reveal where the developer should spend effort improving performance. Enabling tracing for Finagle services is often as simple as adding .tracerFactory(ZipkinTracer()) to your ClientBuilder or ServerBuilder. Setting up the whole Zipkin stack is a bit more work though, check out the docs for further assistance. Trace view, taken from my Strange loop talk about Zipkin. Mesos Mesos describes itself as “a cluster manager that provides efficient resource isolation and sharing across distributed applications, or frameworks”. I’ll try to go through this section without using buzzwords such as “private cloud”, although technically I just did. The core Mesos project is an open source Apache incubator project. On top of it you can run schedulers that deal with more specific technologies, for example Storm and Hadoop. The idea being that the same hardware can be used for multiple purposes, reducing wasted resources. In addition to using Storm on top of Mesos we deploy some of our JVM-based services to internal Mesos clusters. With the proper configuration it takes care of concerns such as rack diversity, rescheduling if a machine goes down and so on. The constraints imposed by Mesos have the positive side effect of enforcing adherence to various good distributed systems practices. For example: Service owners shouldn’t make any assumptions about jobs’ lifetimes, as the Mesos scheduler can move jobs to new hosts at any time. Jobs shouldn’t write to local disk, since persistence is not guaranteed. Deploy tooling and configs shouldn’t use static server lists, since Mesos implies deployment to a dynamic environment. Iago Before putting your new service into production you might want to check how it performs under load. That’s where Iago (formerly Parrot) comes in handy. It’s a load testing framework that is pretty easy to use. The process might look something like this: Collect relevant traffic logs that you want to use as the basis for your load test. Write a configuration file for the test. It contains the hostnames to send load to, the number of requests per second, the load pattern and so on. Write the actual load test. It receives a log line, you transform that into a request to a client. Run the load test. At Twitter this will start up a few tasks in a Mesos cluster, send the traffic and log metrics. Example A load test class could be as simple as this: class LoadTest(parrotService: ParrotService[ParrotRequest, Array[Byte]]) extends ThriftRecordProcessor(parrotService) { val client = new YourService.FinagledClient(service, new TBinaryProtocol.Factory()) def processLines(job: ParrotJob, lines: Seq[String]) { lines foreach {line =>client.doSomething(line) } } } This class will feed each log line to your service’s doSomething method, according to the parameters defined in the configuration of parrotService. ZooKeeper ZooKeeper is an Apache project that is handy for all kinds of distributed systems coordination. One use case for ZooKeeper within Twitter is service discovery. Finagle services register themselves in ZooKeeper using our ServerSet library, see finagle-serversets . This allows clients to simply say they’d like to communicate with “the production cluster for service a in data centre b” and the ServerSet implementation will ensure an up-to-date host list is available. Whenever new capacity is added the client will automatically be aware and will start load balancing across all servers. Scalding From the Scalding github page: “Scalding is a Scala library that makes it easy to write MapReduce jobs in Hadoop. Instead of forcing you to write raw map and reduce functions, Scalding allows you to write code that looks like natural Scala”. As it turns out services that receive a lot of traffic generate tons of log entries. These can provide useful insights into user behavior or perhaps you need to transform them to be suitable as Iago load test input. I have to admit I was a bit sceptical about Scalding at first. It seemed there were already plenty of ways to write Hadoop jobs. Pig, Hive, plain MapReduce, Cascading and so on. However, when the rest of your project is in Scala it is very handy to be able to write Hadoop jobs in the same language. The syntax is often very close to the one used by Scala’s collection library, so you feel right at home, the difference being that with Scalding you might process terabytes of data with the same lines of code. A simple word count example from their tutorial: TextLine(args(\"input\")) .read .flatMap('line -> 'word){ line : String => line.split(\"\\\\s\")} .groupBy('word){group => group.size} .write(Tsv(args(\"output\"))) jvmgcprof One of the well known downsides of relying on the JVM for time sensitive requests is that garbage collection pauses could ruin your day. If you’re unlucky a GC pause might hit at the wrong time, causing some requests to perform poorly or even timeout. Worst case that might have knock on effects that leads to downtime. As a first line of defence against GC issues you should of course tweak your JVM startup parameters to suit the kind of work the service is undertaking. I’ve found these slides from Twitter alumni Attila Szegedi extremely helpful. Of course, you could minimize GC issues by reducing the amount of garbage your service generates. Start your service with jvmgcprof and it’ll help you reach that goal. If you already use Ostrich to track metrics in your service you can tell jvmgcprof which metric represents the work completed. For example you might want to know how many kilobytes of garbage is generated per incoming Thrift request. The jvmgcprof output for that could look something like this. 2797MB w=101223 (231MB/s 28kB/w) 50.00%  8   297 90.00%  14  542 95.00%  15  572 99.00%  61  2237 99.90%  2620    94821 99.99%  2652    95974 On the first line you can see that the number requests or work were 101223 for the period monitored, with 231MB/s of garbage or 28kB per request. The garbage per request can easily be compared after changes has been made to see if they had a positive or negative impact on garbage generation. See the jvmgcprof readme for more information. Summary It’s no surprise, but it turns out that having a common stack is very beneficial. Improvements and bug fixes made by one team will benefit others. There is of course another side to that coin, sometimes bugs are introduced that might just be triggered in your service. However, as an example, when developing Zipkin it was immensely helpful to be able to assume that everyone used Finagle. That way they would get tracing for free once we were done. I have left out some of the benefits of the Twitter stack and how we use Scala, such as the very convenient way Futures allow you to deal with results from asynchronous requests. I hope to write a more in depth post on how to set up a Twitter style service that would deal with the details omitted in this article. In the meantime you can check out the Scala school for more information. Thanks to everyone who worked on the projects mentioned in this article, too many to name but you know who you are. Posted by Johan Oskarsson", "date": "2013-01-28"},
{"website": "Twitter-Engineering", "title": "Introducing Flight: a web application framework", "author": ["‎@angustweets‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013/introducing-flight-a-web-application-framework.html", "abstract": "Last year we rolled out a major reimplementation of the Twitter website. In addition to shifting the rendering of our page content to the server (which achieved significant performance gains), we re-envisioned the entire client-side infrastructure with a clean, robust and easy-to-learn framework which we call Flight . Today we’re making Flight available to the open source community under the liberal MIT license as a framework for structuring web applications. Whether you use Flight as the JavaScript framework for your next web project, or just as source for new ideas, we look forward to learning from diverse perspectives via community feedback and contributions on GitHub . Why Flight? Flight is distinct from existing frameworks in that it doesn’t prescribe or provide any particular approach to rendering or providing data to a web application. It’s agnostic on how requests are routed, which templating language you use, or even if you render your HTML on the client or the server. While some web frameworks encourage developers to arrange their code around a prescribed model layer, Flight is organized around the existing DOM model with functionality mapped directly to DOM nodes. Not only does this obviate the need for additional data structures that will inevitably influence the broader architecture, but by mapping our functionality directly onto the native web we get to take advantage of native features. For example, we get custom event propagation for free by piggybacking off DOM event bubbling, and our event handling infrastructure works equally well with both native and custom events. How does it work? Flight enforces strict separation of concerns. When you create a component you don’t get a handle to it. Consequently, components cannot be referenced by other components and cannot become properties of the global object tree. This is by design. Components do not engage each other directly; instead, they broadcast their actions as events which are subscribed to by other components. Why events? Events are open-ended. When a component triggers an event, it has no knowledge of how its request will be satisfied or by whom. This enforced decoupling of functionality allows the engineer to consider each component in isolation rather than having to reason about the growing complexity of the application as a whole. By making DOM node events proxies for component events, we let the web work for us: we get event propagation for free a component can subscribe to a given event type at the document level or it can choose to listen only those events originating from within a specified DOM Node subscribing components do not distinguish between custom events from other components (e.g. ‘dataMailItemsServed’) and native DOM node events (e.g. ‘click’), and process both types of event in an identical fashion. Mobility and testing Each component is a module that, aside from a minimal set of standard dependencies (relevant Flight utilities and mixins), has no reference to the outside world. Thus a given component will respond to a given event in the same way, regardless of environment. This makes testing simple and reliable — events are essentially the only variable, and a production event is easy to replicate in testing. You can even debug a component by triggering events in the console. Mixins A mixin defines a set of functionality that is useful to more than one object. Flight comes with built-in support for functional mixins , including protection against unintentional overrides and duplicate mixins. While classical JavaScript patterns support only single inheritance, a component prototype (or other object) can have multiple mixins applied to it. Moreover, mixins requires a fraction of the boilerplate required to form traditional classical hierarchies out of constructor-prototypes hybrids, and don’t suffer the leaky abstractions of the latter (‘super’, ‘static’, ‘const’ etc.) Documentation and demo Our GitHub page includes full documentation as well as a sample app in the form of an email client : Future work Flight is an ongoing and evolving project. We’re planning to add a full testing framework and make available more of the utilities that we use for the Twitter website frontend. We also look forward to your contributions and comments. We know we haven’t thought of everything, and with your help we can continue to improve Flight for the benefit of everyone. Acknowledgments Flight was a group effort. These folks have contributed to the project: Angus Croll , Dan Webb , Kenneth Kufluk , along with other members the Twitter web team. A special thank you to folks in the web community who took the time to review the code.", "date": "2013-01-31"},
{"website": "Twitter-Engineering", "title": "New Twitter search results ", "author": ["‎@glassyocean‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013/new-twitter-search-results.html", "abstract": "We just shipped a new version of the Twitter app with a brand new search experience that blends the most relevant content - Tweets, user accounts, images, news, related searches, and more - into a single stream of results. This is a major shift from how we have previously partitioned results by type (for instance, Tweet search vs. people search). We think this simplified experience makes it easier to find great content on Twitter using your mobile device. A typical search scores items of the same type and picks the top-scoring results. In a blended search experience, this is not straightforward. The scores of different content types are computed by different services, and thus not directly comparable for blending. Another challenge is to decide which type of content to mix, as not all content types are always desirable to display. This post discusses our approach to solving these challenges. Ranking When a user searches, different types of content are searched separately, returning a sequence of candidate results for each content type with a type-specific score for each. For certain content types that are displayed as a single group or gallery unit, such as users or images, we assign the maximum score of results as the representative score of this content type. The result sequences for some content types may be trimmed or discarded entirely at this point. Once results of different content types are prepared, each type-specific score is converted into a universally compatible score, called a “uniscore”. Uniscores of different modules are used as a means to blend content types as in a merge-sort, except for the penalization of content type transition. This is to avoid over-diversification of content types in the blended result. Fig. 1: Search ranker chose News1 followed by Tweet1 so far and is presented with three candidatesTweet2, User Group, and News2 to pick the content after Tweet1. News2 has the highest uniscore but search ranker picks Tweet2, instead of News2 as we penalize change in type between consecutive content by decreasing the score of News2 from 0.65 to 0.55, for instance. Score unification Individual content is assigned a type-specific score, which is called a “raw” score, by its corresponding service. To facilitate blending and ranking content of different types as described above, raw scores are converted into uniscores using type-specific log-linear score conversion functions – where the chance of a converted score to take its value in [0, 1] is at least 95%, as estimated from observed dataset. Content selection and boosting Certain types of content may not have many relevant items to show for a particular input query, in which case we may choose not to include this type of content in search results. In other cases, for instance if query volume or matched item counts have an unusual spike (what we call a “burst”), we show this type and may also boost it to appear at a higher place in the results. To facilitate this, we represent trends in searches or matching result counts as a single number that is proportional to the level of “burstiness”. For example, consider measuring “burstiness” for the number of images and news content matching the query “photos”. We first obtain three sequences of term frequencies, e.g. : Fig. 2 : Three sequences of number of Tweets over eight 15 minute buckets from bucket 1 (2 hours ago) to 8 (most recent). Tweet : counts of Tweets that match query “photos”. Image : counts of Tweets that match query “photos” and contain image links. News : counts of Tweets that match query “photos” and contain news links. Query “photos” is shown not only to match Tweets with image links more than those with news links but also is increasing over time. Our approach to compute the burstiness of image and news facets is an extension of original work by Jon Kleinberg on bursty structure detection, which is in essence matching current level of burst to one of a predefined set of bursty states, while minimizing too diverse a change in matched states for smooth estimation [1]. In our extension, burstiness of mixable content types including images, users, news, and tweets are computed simultaneously to reflect relative difference in bursty levels between different types and used the distance of observed rate from each state’s bursty level as state cost. This is because accurately estimating probability of occurrence is infeasible for real-time estimation due to expensive computational cost and possible introduction of zero intervals between probability states due to numerical approximation. Optimal state sequences for images and news are estimated as shown in Fig 3. Fig. 3 : Normalized image and news counts are matched to one of n=5 states : 1 average, 2 above, and 2 below. Matched states curves show a more stable quantization of original sequence which has the effect of removal of small noisy peaks. Finally, burstiness of each content type is computed as an exponential moving average of state IDs in the optimal state sequence. As shown in Fig. 3, jointly optimizing the sum of state cost and transition cost yields a smooth quantization of original sequence, which automatically filters out small noisy peaks in original counts. Also, this maps both trending (bursty) and steadily high sequences to a high burstiness value. Burstiness computed this way is used to filter out content types with low or no bursts. It’s also used to boost the score of corresponding content types, as a feature for a multi-class classifier that predicts the most likely content type for a query, and in additional components of the ranking system. References [1] J. Kleinberg, Bursty and Hierarchical Structure in Streams, Proc. 8th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining, 2002. ( PDF ) Posted by Youngin Shin Search-Quality Team", "date": "2013-02-06"},
{"website": "Twitter-Engineering", "title": "Twitter Typeahead.js: You Autocomplete Me", "author": ["‎@cra‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013/twitter-typeaheadjs-you-autocomplete-me.html", "abstract": "Twitter typeahead.js is a fast and battle-tested jQuery plugin for auto completion. Today we’re open sourcing the code on GitHub under the MIT license . By sharing a piece of our infrastructure with the open source community, we hope to evolve typeahead.js further with community input. If your web application needs a fully-featured queryable search box, typeahead.js can help. Some of its capabilities and features include: Search data on the client, server, or both Handle multiple inputs on a single page with shared data and caching Suggest multiple types of data (e.g. searches and accounts) in a single input Support for international languages, including right-to-left (RTL) and input method editors (IME) Define custom matching and ranking functions Grey text hints that help explain what hitting tab will do It’s also optimized for large local datasets, so it’s fast for high-latency networks. Examples We recommend you take a look at our examples page. There are three ways to get data: Using local, hard-coded data passed on page render: $('#input').typeahead([ { name: 'planets', local: [ \"Mercury\", \"Venus\", \"Earth\", \"Mars\", \"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\" ] } ]); Using a prefetch URL that will be hit to grab data on pageload and then stored in localStorage: $('#input').typeahead([ { name: 'countries', prefetch: '/countries.json', } ]); Or using a queryable API that returns results as-you-type (with the query being passed in the ?q= parameter): $('#input').typeahead([ { name: 'countries', remote: '/countries.json', } ]); You can also combine local or prefetch with a remote fallback for the performance of local data combined with the coverage of a remote query API (e.g. quickly search your friends but be able to find anyone on your site). There are lots of options for configuring everything from ranking, matching, rendering, templating engines, and more; check out the README for those details. If you want to use this with a project like Bootstrap , all you have to do is include the JavaScript file for typeahead.js after Bootstrap’s JavaScript file and use our configuration options. We initially built typeahead.js to support our needs; now we look forward to improvements and suggestions from the community. To learn more about how typeahead.js works, check out our detailed documentation . To stay in touch, follow @typeahead and submit issues on GitHub. Also, if building web application frameworks like typeahead.js interests you, why not consider joining the flock ? Acknowledgements Typeahead.js was primarily authored by Tim Trueman ( @timtrueman ), Veljko Skarich ( @vskarich ) and Jake Harding ( @jakeharding ).", "date": "2013-02-19"},
{"website": "Twitter-Engineering", "title": "Drinking from the Streaming API", "author": ["‎@cra‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013/drinking-from-the-streaming-api.html", "abstract": "Today we’re open-sourcing the Hosebird Client (hbc) under the ALv2 license to provide a robust Java HTTP library for consuming Twitter’s Streaming API . The client is full featured: it offers support for GZip, OAuth and partitioning; automatic reconnections with appropriate backfill counts; access to raw bytes payload; proper retry schemes, and relevant statistics. Even better, it’s been battle-tested in production by our internal teams. We highly recommend you take advantage of the Hosebird Client if you plan on working with the Streaming API. Using Hosebird The Hosebird Client is broken into two main modules: hbc-core and hbc-twitter4j. The hbc-core module uses a simple message queue that a consumer can poll for messages. The hbc-twitter4j module lets you use the superb Twitter4J project and its data model on top of the message queue to provide a parsing layer. The first step to use Hosebird is to setup the client using the ClientBuilder API: // Create an appropriately sized blocking queue BlockingQueueString> queue = new LinkedBlockingQueueString>(10000); // Authenticate via OAuth Authentication auth = new OAuth1(consumerKey, consumerSecret, token, secret); // Build a hosebird client ClientBuilder builder = new ClientBuilder() .hosts(Constants.STREAM_HOST) .authentication(auth) .endpoint(new StatusesSampleEndpoint()) .processor(new StringDelimitedProcessor(queue)) .eventMessageQueue(queue); Client hosebirdClient = builder.build(); After we have created a Client, we can connect and process messages: client.connect(); while (!client.isDone()) { String message = queue.take(); System.out.println(message); // print the message} Hosebird Examples We recommend you learn from the examples on GitHub or contribute your own. If you want a quick example, set these properties in hbc-example/pom.xml: SECRET SECRET SECRET SECRET Then you can run this command on the command line: mvn exec:java -pl hbc-example This will connect to the sample stream API and print 1000 JSON items from the API. Acknowledgements The Hosebird Client was primarily authored by Steven Liu ( @steven ) and Kevin Oliver ( @kevino ). We’d also like to thank the @TwitterAPI team for their thoughtful suggestions and help. On behalf of the Hosebird team, - Chris Aniszczyk, Manager of Open Source ( @cra )", "date": "2013-02-28"},
{"website": "Twitter-Engineering", "title": "Students: Apply now for Summer of Code", "author": ["‎@cra‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013/students-apply-now-for-summer-of-code.html", "abstract": "We are thrilled to have an opportunity again to participate and support the Summer of Code program, especially since we enjoyed being involved so much last year for the first time . Unlike many Summer of Code participating organizations that focus on a single ecosystem, we have a variety of projects spanning multiple programming languages and open source communities. Here are a few of our project ideas for this year: Finagle Finagle is a protocol-agnostic, asynchronous RPC system for the JVM that makes it easy to build robust clients and servers in Java, Scala or any JVM-hosted language. It is extensively used within Twitter and other companies to run their backend services. This summer, we’re offering these project ideas: Distributed debugging: DTrace is a very powerful and versatile tool for debugging local application. We would like to employ similar types of instrumentation on a cluster of machines that form a distributed system, tracing requests based on specific conditions like the state of the server. Pure Finagle-based ZooKeeper client: ZooKeeper is the open sourced library of cluster coordination that we use at Twitter. We would like to implement a ZooKeeper client purely in Finagle. If you’re new to Scala, we recommend you check out our Scala School and Effective Scala guides on GitHub. Mesos Apache Mesos is a cluster manager that provides efficient resource isolation and sharing across distributed applications (or frameworks). It is extensively used at Twitter to run all sorts of jobs and applications. We are looking for a student to help us add security and authentication support to Mesos (including integration with LDAP). We recommend signing up on the Mesos mailing list and if you want to learn more about Mesos, you might enjoy this article in Wired. Scalding Scalding is a Scala library that makes it easy to specify Hadoop MapReduce jobs. Scalding is built on top of Cascading , a Java library that abstracts away low-level Hadoop details. Scalding is comparable to Pig , but offers tight integration with Scala, bringing advantages of Scala to your MapReduce jobs. This summer, we’re looking for students to help with: Scalding Read-eval-print-loop (REPL): Make a REPL to allow playing with scalding in local and remote mode with a REPL. The challenge here is scheduling which portions of the items can be scheduled to run, and which portions are not yet ready to run. You will build a DAG and when one is materialized, you schedule the part of the job that is dependent on that output. Integrate Algebird and Spire: Spire is a scala library modeling many algebraic concepts. Algebird is a Twitter library that is very similar and has a subset of the objects in Spire. We would like to use the type-classes of Spire in Algebird. Algebird is focused on streaming/aggregation algorithms, which are a subset of Spire’s use case. You can view all of our project ideas on our wiki . We strongly recommend that you submit your application early and discuss your ideas with respective project mentors. The deadline is May 03 at 19:00 UTC and late applications cannot be accepted for any reason. You can always update your application and answer our questions after you submit it. If you have any questions not covered in the wiki , ask them on our Summer of Code mailing list . We look forward to reading your applications and working with you on open source projects over the summer. Good luck! - Chris Aniszczyk, Manager of Open Source ( @cra )", "date": "2013-04-30"},
{"website": "Twitter-Engineering", "title": "2013", "author": ["‎@‎"], "link": "https://blog.twitter.com/engineering/en_us/a/2013.html", "abstract": "", "date": "1970-01-01"},
{"website": "Twitter-Engineering", "title": "Open Sourcing Twitter Heron", "author": ["Karthik Ramasamy"], "link": "https://blog.twitter.com/engineering/en_us/topics/open-source/2016/open-sourcing-twitter-heron.html", "abstract": "", "date": "2016-05-25"},
{"website": "Twitter-Engineering", "title": "Reinforcement Learning for Torch: Introducing torch-twrl", "author": ["Kory Mathewson"], "link": "https://blog.twitter.com/engineering/en_us/topics/open-source/2016/reinforcement-learning-for-torch-introducing-torch-twrl.html", "abstract": "", "date": "2016-09-16"},
{"website": "Twitter-Engineering", "title": "Introducing Torch Decision Trees", "author": ["Nicholas Léonard"], "link": "https://blog.twitter.com/engineering/en_us/topics/open-source/2017/Introducing-Torch-Decision-Trees.html", "abstract": "", "date": "2017-10-09"},
{"website": "Twitter-Engineering", "title": "How we built Twitter Lite", "author": ["Nicolas Gallagher"], "link": "https://blog.twitter.com/engineering/en_us/topics/open-source/2017/how-we-built-twitter-lite.html", "abstract": "", "date": "2017-04-06"},
{"website": "Twitter-Engineering", "title": "Distributed learning in Torch", "author": ["Zak Taylor"], "link": "https://blog.twitter.com/engineering/en_us/topics/infrastructure/2016/distributed-learning-in-torch.html", "abstract": "", "date": "2016-01-25"},
{"website": "Twitter-Engineering", "title": "Introducing Twitter Image Pipeline iOS framework for open source", "author": ["Nolan O'Brien"], "link": "https://blog.twitter.com/engineering/en_us/topics/open-source/2017/introducing-twitter-image-pipeline-ios-framework-for-open-source.html", "abstract": "", "date": "2017-03-01"},
{"website": "Twitter-Engineering", "title": "The infrastructure behind Twitter: efficiency and optimization", "author": ["Mazdak Hashemi"], "link": "https://blog.twitter.com/engineering/en_us/topics/infrastructure/2016/the-infrastructure-behind-twitter-efficiency-and-optimization.html", "abstract": "", "date": "2016-08-23"},
{"website": "Twitter-Engineering", "title": "Introducing Vireo: a lightweight and versatile video processing library", "author": ["Can Bal"], "link": "https://blog.twitter.com/engineering/en_us/topics/open-source/2017/introducing-vireo.html", "abstract": "", "date": "2017-12-15"},
{"website": "Twitter-Engineering", "title": "Introducing Serial: improved data serialization on Android", "author": ["Ali Fauci"], "link": "https://blog.twitter.com/engineering/en_us/topics/open-source/2017/introducing-serial.html", "abstract": "", "date": "2017-11-06"},
{"website": "Twitter-Engineering", "title": "Optimizing Twitter Heron", "author": ["Karthik Ramasamy"], "link": "https://blog.twitter.com/engineering/en_us/topics/open-source/2017/optimizing-twitter-heron.html", "abstract": "", "date": "2017-03-16"},
{"website": "Twitter-Engineering", "title": "Search Relevance Infrastructure at Twitter", "author": ["Yatharth Saraf"], "link": "https://blog.twitter.com/engineering/en_us/topics/infrastructure/2016/search-relevance-infrastructure-at-twitter.html", "abstract": "", "date": "2016-08-15"},
{"website": "Twitter-Engineering", "title": "Simplify Service Dependencies with Nodes", "author": ["Tian Wang"], "link": "https://blog.twitter.com/engineering/en_us/topics/open-source/2016/simplify-service-dependencies-with-nodes.html", "abstract": "", "date": "2016-11-11"},
{"website": "Twitter-Engineering", "title": "Twitter’s Vote on JSR 376 Public Review Reconsideration Ballot", "author": ["Tony Printezis"], "link": "https://blog.twitter.com/engineering/en_us/topics/infrastructure/2017/Twitters-Vote-on-JSR-376-Public-Review-Reconsideration-Ballot.html", "abstract": "", "date": "2017-06-27"},
{"website": "Twitter-Engineering", "title": "Twitter’s Vote on JSR 376 (Java Platform Module System)", "author": ["Tony Printezis"], "link": "https://blog.twitter.com/engineering/en_us/topics/infrastructure/2017/twitters-vote-on-jsr-376-java-platform-module-system.html", "abstract": "", "date": "2017-05-10"},
{"website": "Twitter-Engineering", "title": "Omnisearch index formats", "author": ["Yan Zhao "], "link": "https://blog.twitter.com/engineering/en_us/topics/infrastructure/2016/omnisearch-index-formats.html", "abstract": "", "date": "2016-11-04"},
{"website": "Twitter-Engineering", "title": "The Infrastructure Behind Twitter: Scale", "author": ["Mazdak Hashemi"], "link": "https://blog.twitter.com/engineering/en_us/topics/infrastructure/2017/the-infrastructure-behind-twitter-scale.html", "abstract": "", "date": "2017-01-19"},
{"website": "Twitter-Engineering", "title": "Building and Serving Conversations on Twitter", "author": ["Allen Chen"], "link": "https://blog.twitter.com/engineering/en_us/topics/infrastructure/2017/building-and-serving-conversations-on-twitter.html", "abstract": "", "date": "2017-03-25"},
{"website": "Twitter-Engineering", "title": "Twitter goes to #NSBE42", "author": ["Christopher Coco"], "link": "https://blog.twitter.com/engineering/en_us/topics/insights/2016/twitter-goes-to-nsbe42.html", "abstract": "", "date": "2016-03-31"},
{"website": "Twitter-Engineering", "title": "Discovery and Consumption of Analytics Data at Twitter", "author": ["Sriram Krishnan"], "link": "https://blog.twitter.com/engineering/en_us/topics/insights/2016/discovery-and-consumption-of-analytics-data-at-twitter.html", "abstract": "", "date": "2016-06-29"},
{"website": "Twitter-Engineering", "title": "How Twitter deploys its widgets JavaScript", "author": ["Aravind Ramanathan"], "link": "https://blog.twitter.com/engineering/en_us/topics/insights/2016/how-twitter-deploys-its-widgets-javascript.html", "abstract": "", "date": "2016-09-21"},
{"website": "Twitter-Engineering", "title": "Experimenting to solve cramming", "author": ["Lucile Lu"], "link": "https://blog.twitter.com/engineering/en_us/topics/insights/2017/Experimenting-To-Solve-Cramming.html", "abstract": "", "date": "2017-11-07"},
{"website": "Twitter-Engineering", "title": "The testing renaissance", "author": "Unknown", "link": "https://blog.twitter.com/engineering/en_us/topics/insights/2017/the-testing-renaissance.html", "abstract": "", "date": "2017-10-30"},
{"website": "Twitter-Engineering", "title": "Manhattan software deployments: how we deploy Twitter’s large scale", "author": ["Ameet Kotian"], "link": "https://blog.twitter.com/engineering/en_us/topics/insights/2016/manhattan-software-deployments-how-we-deploy-twitter-s-large-scale-distributed-database.html", "abstract": "", "date": "2016-05-09"},
{"website": "Twitter-Engineering", "title": "Moving Top Tweet Search Results from Reverse Chronological Order", "author": ["Lisa Huang"], "link": "https://blog.twitter.com/engineering/en_us/topics/insights/2016/moving-top-tweet-search-results-from-reverse-chronological-order-to-relevance-order.html", "abstract": "", "date": "2016-12-19"},
{"website": "Twitter-Engineering", "title": "Our discovery of cramming", "author": ["Ikuhiro Ihara"], "link": "https://blog.twitter.com/engineering/en_us/topics/insights/2017/Our-Discovery-of-Cramming.html", "abstract": "", "date": "2017-11-07"},
{"website": "Twitter-Engineering", "title": "Using Deep Learning at Scale in Twitter’s Timelines", "author": "Unknown", "link": "https://blog.twitter.com/engineering/en_us/topics/insights/2017/using-deep-learning-at-scale-in-twitters-timelines.html", "abstract": "", "date": "2017-05-09"},
{"website": "Twitter-Engineering", "title": "Reflecting on Power and Physics in Technology", "author": ["Jen Fraser"], "link": "https://blog.twitter.com/engineering/en_us/topics/insights/2017/Reflecting-on-Power-and-Physics-in-Technology.html", "abstract": "", "date": "2017-07-06"}
]