[
{"website": "Hash-Rocket", "title": "Responsifying with Viewport Units", "author": ["\nRye Mas"], "link": "https://hashrocket.com/blog/posts/responsifying-with-viewport-units", "abstract": "Design Responsifying with Viewport Units by\nRye Mason\n\non\nMarch  2, 2015 Not long ago, I designed what seemed like a tricky heading to implement. For our upcoming Ancient City Ruby conference, I placed two lines of text on either side of “450” (the number of years St. Augustine has been a city), taking up the full width of the containing column. Oh, yeah, and I wanted this heading to be responsive. CSS all the way If you’re not using Javascript to determine browser width — which I wasn’t, for the sake of finding a CSS-only solution — you might think you could use percentages or rems to make this work. But those solutions don’t cause the text to resize correctly, because percentages and rems are relative to their parents and can’t calculate the size of the browser window. Viewport units are nice because they let us create elements relative to the viewport instead of a container. That’s why, within a container that has a max-width , you can still set elements to take up space according to the viewport size, something you can’t do with percentages or rems. This layout was a good test case for viewport units, which I’d heard of but never used before. Viewport units — vw (viewport width), vh (viewport height), vmax (the larger of the two), and vmin (the smaller of the two) — represent 1% of the viewport size, and are fully supported by the big three (Firefox, Chrome, and Safari). IE 9+ has partial support (because it uses vm instead of vmin ), and all modern mobile browsers except Opera support viewport units. There’s some bugginess in iOS 7 related to vh , but there are workarounds if you need them. We won’t be using vh in this example, though. Let’s get started First I created a containing element called .column , with a span for each of the elements inside: .column %span .opening St. Augustine celebrates %span .number 450 %span .closing years of history in 2015 I positioned .column relatively, with a width of 80% and max-width of 1000px. (Again, this wrapper won’t affect child elements that have viewport units. This is because viewport units remain relative to the viewport instead of parent containers.) .column text-align : center position : relative width : 80% max-width : 1000px I absolutely positioned .first and .last on the left and right edges of .column and added a transition effect to smooth resizing motion. span transition : .5s & .opening left : 0 & .closing right : 0 & .opening , & .closing position : absolute I styled each of the span elements, and added a tiny bit of left margin to .number because it isn’t actually dead center. “St. Augustine celebrates,” the preceding span , is a bit longer than the last, “years of history in 2015,” so .number needed to be nudged a bit. span transition : .5s & .opening left : 0 & .closing right : 0 & .opening , & .closing border-top : 1px solid $red border-bottom : 1px solid $red display : inline-block position : absolute top : 28% padding : .8rem & .number color : $red font-weight : 700 letter-spacing : -2px margin : 0 0 0 3% Add some magic Finally, I implemented vw units for the text, with rem sizes for fallbacks (looking at you, IE 8). The font size is determined by the current viewport width, so as you resize your browser, the letters fill the amount of space specified — and at most sizes, this works really well. span transition : .5s & .opening left : 0 & .closing right : 0 & .opening , & .closing font-size : 1 .3rem font-size : 2vw border-top : 1px solid $red border-bottom : 1px solid $red display : inline-block position : absolute top : 28% padding : .8rem & .number color : $red font-size : 3rem font-size : 8 .5vw font-weight : 700 letter-spacing : -2px text-align : center margin : 0 0 0 3% Up and down Of course, I wanted this layout to work at different breakpoints. I wrote rules to create a pixel-based font size when the viewport reaches the largest comfortable width, because I didn’t want the text to grow to infinity, and since the wrapper has a max-width the text could eventually outgrow it. To account for this, I set the font size for .open and .close to 24px and .number to 129px beyond a browser width of 1300px. When sizing down, there wasn’t a whole lot to change. I dragged my browser window back and forth to eyeball top margins for .opening and .closing , which I changed at different breakpoints to keep the content vertically centered on either side of .number . I set rules at max-widths of 480px and 680px to keep the text at a smaller viewport width on mobile devices. For viewports under 680px, the text stacks for readability instead of centering. span transition : .5s & .opening left : 0 & .closing right : 0 & .opening , & .closing font-size : 1 .3rem font-size : 2vw border-top : 1px solid $red border-bottom : 1px solid $red display : inline-block position : absolute top : 28% padding : .8rem +min-width ( 1300 ) font-size : 24px top : 32% +max-width ( 840 ) top : 24% +max-width ( 700 ) top : 18% +max-width ( 680 ) font-size : 1 .8rem font-size : 4 .3vw display : block position : relative top : 0 & .number color : $red font-size : 3rem font-size : 8 .5vw font-weight : 700 letter-spacing : -2px text-align : center margin : 0 0 0 3% +min-width ( 1500 ) font-size : 129px +max-width ( 480 ) font-size : 4rem font-size : 12vw For those of you following along at home, below are the mixins I’m referencing to set my breakpoints: = min-width ($ width : 900 ) @media screen and ( min-width : #{ $width } px ) @content transition : all .5s ease-in-out = max-width ($ width : 900 ) @media screen and ( max-width : #{ $width } px ) @content transition : all .5s ease-in-out Done! I was pretty happy with how the final heading turned out. (The live version is no longer available, but try it for yourself!) You can use viewport units on anything, not just text. For my purposes, I found viewport units to be really helpful with font-size, and knowing it’s pretty cross-browser friendly is a compelling reason to use it in the future. Try implementing viewport units in your next project and see where it takes you. Was this post helpful? Share it with others. Tweet Share Post", "date": "2015-03-02"},
{"website": "Hash-Rocket", "title": "Ruby on the Command Line", "author": ["\nJack Christensen\n\n"], "link": "https://hashrocket.com/blog/posts/ruby-on-the-command-line", "abstract": "Ruby Ruby on the Command Line by\nJack Christensen\n\non\nAugust 15, 2017 Ruby is strongly identified with Rails and web development. But Ruby is far more than a web language. Ruby has a rich set of tools for shell scripting and text processing. Let's start by executing a Ruby command. The -e argument tells Ruby to run the provided code and exit. $ ruby -e 'puts \"Hello, world.\"' Hello, world. Multiple statements can be separated by a ; . $ ruby -e 'puts \"Hello, world.\"; puts \"It is now #{Time.now}.\"' Hello, world.\nIt is now 2017-08-08 16:43:56 -0500 . Or multiple -e arguments can be provided. $ ruby -e 'puts \"Hello, world.\"' -e 'puts \"It is now #{Time.now}.\"' Hello, world.\nIt is now 2017-08-08 16:45:10 -0500 . Reading Files Where this really becomes valuable is to easily process text. Ruby is a Swiss army knife of text processing tools that can be used in place of sed and awk . For the next examples we will use the text of Ulysses by Alfred Lord Tennyson. We'll start by printing the nth line of a file. STDIN is an IO object. It has a readlines method that will return the entire file as an array of lines. Remember that arrays are zero-indexed in Ruby so this actually prints the 59th line. $ ruby -e 'puts STDIN.readlines[58]' < ulysses.txt\n‘Tis not too late to seek a newer world. Sometimes it would be useful if our script could read from STDIN or files passed as arguments. ARGF merges these concepts. We can read from ARGF instead and Ruby will actually read from STDIN or files passed on the command line as necessary. $ ruby -e 'puts ARGF.readlines[58]' < ulysses.txt\n‘Tis not too late to seek a newer world. Note there is no redirection of STDIN below. $ ruby -e 'puts ARGF.readlines[58]' ulysses.txt\n‘Tis not too late to seek a newer world. Extracting the First Lines of a File We can duplicate the functionality of head by using first(n) . $ ruby -e 'puts ARGF.readlines.first(5)' ulysses.txt\nIt little profits that an idle king,\nBy this still hearth, among these barren crags,\nMatched with an aged wife, I mete and dole\nUnequal laws unto a savage race,\nThat hoard, and sleep , and feed, and know not me. Extracting the Last Lines of a File tail can also be duplicated by using last(n) . $ ruby -e 'puts ARGF.readlines.last(5)' ulysses.txt\nWe are not now that strength which in old days\nMoved earth and heaven, that which we are, we are,\nOne equal temper of heroic hearts,\nMade weak by time and fate, but strong in will\nTo strive, to seek, to find, and not to yield. Printing Matches to a Regular Expression Oftentimes we need to process arguments line by line such as done by grep . This is facilited by the -n flag. It wraps a loop around your script like so: while gets; <your script here>; end . Ruby sets the magic global $_ to the last line returned from gets . Combining these gives us a one-liner similar to grep . $ ruby -n -e 'puts $_ if $_ =~ /\\bold\\b/i' ulysses.txt\nFree hearts, free foreheads—you and I are old ; Old age hath yet his honor and his toil.\nWe are not now that strength which in old days The conditional can further be shortened by omitting $_ =~ . This works because Ruby considers a lone regex in a conditional to be a match against $_ . $ ruby -n -e 'puts $_ if /\\bold\\b/i' ulysses.txt\nFree hearts, free foreheads—you and I are old ; Old age hath yet his honor and his toil.\nWe are not now that strength which in old days Finding and Replacing Text The -p flag works the same as -n except it also prints $_ at the end of the loop. This can be used to find and replace. Note that gsub! is used as we need to mutate $_ . $ ruby -p -e '$_.gsub!(/\\bking\\b/, \"monarch\")' ulysses.txt\nIt little profits that an idle monarch,\nBy this still hearth, among these barren crags,\n... Processing Tabular Data The -a flag can be used with -n or -p to auto-split each line. It calls $F = $_.split before each loop. This can be used to easily extract columnar data. In this example we will print the 3rd word of each line. $ ruby -n -a -e 'puts $F[2]' ulysses.txt\nprofits\nstill\nan\n... Extracting Lines Between Starting and Ending Delimiters Sometimes we need to extract lines from a file starting with one delimiter and ending with another. The little-known flip-flop operator is the solution. While it appears to be a range, the flip-flop is Ruby syntax that becomes true when the first condition is triggered and stays true until after the last condition is triggered. $ ruby -n -e 'puts $_ if /^Old/../^Not/' ulysses.txt\nOld age hath yet his honor and his toil.\nDeath closes all ; but something ere the end,\nSome work of noble note, may yet be done ,\nNot unbecoming men that strove with gods. Counting Words What if we wanted to do some sort of aggregate computation such counting the number of words? If the file is small we could read and process the entire file at once. $ ruby -e 'puts ARGF.read.split.size' ulysses.txt\n564 However, the previous solution would be impractical if the source file was exceptionally large. In that case we would need to process the file line by line. But we need to do some work before all lines and after all lines. This can be accomplished with BEGIN and END . BEGIN and END run the code contained in braces at the beginning and end of the program respectively. ruby -n -a -e 'BEGIN { wc = 0 }' -e 'wc += $F.size' -e 'END { puts wc }' ulysses.txt Computing Word Frequency Now let's try something even more advanced. Let's compute the frequency of the words used in Ulysses and print only the words that appear more than 10 times. In the BEGIN block we create a hash that will hold the count of words. We give it a default value of 0. In the body of the implicit loop we add one for each occurance of a word. In the END block we filter, sort, and print the word frequency data. $ ruby -n -a -e 'BEGIN { words = Hash.new(0) }' \\ -e '$F.each { |w| words[w] += 1 }' \\ -e 'END { words.select {|k,v| v > 10}.sort_by { |k,v| [-v, k] }.each { |k,v| puts \"#{k} - #{v}\" } }' \\ ulysses.txt\nand - 27\nthe - 24\nto - 17\nI - 14\nof - 12 Conclusion Ruby is an expressive, full-featured programming language, but it also has a lot of (Perl-inspired) shorthand that make it convenient to use at the shell. Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-08-15"},
{"website": "Hash-Rocket", "title": "The Tale of the Color-Shifting Dress", "author": ["\nCameron Daigle\n\n"], "link": "https://hashrocket.com/blog/posts/the-tale-of-the-color-shifting-dress", "abstract": "Design The Tale of the Color-Shifting Dress by\nCameron Daigle\n\non\nFebruary 27, 2015 Last night, a mysterious dress appeared on Tumblr that caused everyone on the internet to lose their minds. Then, everyone on the internet decided to colorsplain the situation. This is one of those. Friends, this is a case of where your brain is perceiving the light source. The reason this picture is causing so much confusion is that the light source ON the dress is unclear. The background behind the dress is blown out, overexposed fluorescent light, while the light ON the dress is one of two things. Lie down for a second on the nearest comfy couch and let's talk. If you see the dress as gold and white Your brain is seeing the dress as gold and white because your brain is seeing the dress as in shadow, i.e. there is not light on the viewer's side of the dress. At this point, a hastily-drawn Photoshop image should explain things: So the bluish tint of the dress is interpreted by your brain as a consequence of the shadows. If you see the dress as blue and black Your brain is seeing the dress as blue and black because it's assuming the dress is being lit by a warm light, hence the gold or brown overtones on the \"black\" area of the dress: However, this light source isn't actually shown in the photo (and definitely isn't affecting the background, which is overexposed and cold-toned), so people who see the dress as gold and white are SUPER confused by this approach. Here's the thing Light temperature and light source is everything. The side of the dress facing the Confused Person is composed of the EXACT SAME COLORS in both diagrams. So show this post to your gold-and-white-truther friends and help them see the light. Sorry for the pun. It's been a long morning. Was this post helpful? Share it with others. Tweet Share Post", "date": "2015-02-27"},
{"website": "Hash-Rocket", "title": "Vim Magic with Abolish.vim", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/vim-magic-with-abolish-vim", "abstract": "Vim Vim Magic with Abolish.vim by\nJake Worth\n\non\nJanuary 19, 2016 Occasionally, I discover a Vim plugin that blows my mind. Abolish.vim is just such a plugin. The creator Tim Pope describes Abolish.vim as follows: It's three superficially unrelated plugins in one that share a common theme: working with variants of a word. He's right. The features of this plugin— abbreviation, substitution, and coercion— seem unrelated. The first can be used to preempt chronic typos, the second can be used to beat Vim's limited string substitution, and the third can be used to transform all the unique cases we use as programmers. Why combine them? It doesn't matter why. This plugin is amazing. Add it to your Vim configuration, learn a few commands, and prepare to look like a magician at the keyboard. In this blog post, we will explore the setup and three main features of Abolish.vim. Setup With Pathogen.vim , installation is as easy as: $ cd ~/.vim/bundle\n$ git clone git://github.com/tpope/vim-abolish.git Abbreviation Do you often type 'teh' when you meant 'the'? What about 'fucntion' instead of 'function'? These mistakes are easy to make, and sometimes for historical or cultural reasons you just can't seem to break the habit. Keystrokes are valuable, so stop fixing the same typos. Abolish.vim can help. If you don't have an after/ directory in your '.vim' directory, create one. It's the default for the plugin. Then add a config file: $ mkdir ~/.vim/after/plugin\n$ touch ~/.vim/after/plugin/abolish.vim The after/ location tells this code to be run after all plugins, including Abolish.vim, have been loaded. Inside your new file, add this: \" ~/.vim/after/plugin/abolish.vim\nAbolish teh the Restart Vim and try typing 'teh', then a space. Vim fixes it. Failure neutralized! Here are four common misspellings of 'function' you might address: \" ~/.vim/after/plugin/abolish.vim\nAbolish functoin function\nAbolish fucnton function\nAbolish fucntion function\nAbolish fuction function It even supports curly brackets, preempting multiple typos on one line: \" ~/.vim/after/plugin/abolish.vim\nAbolish functoin{ally,ed} function{} This fixes the stem of the words functoinally and functoined by changing them to function , while preserving the affixes ally and ed . Tangentially related to this feature is the :Abolish -search foo command, which will find foo , Foo , and FOO . Substitution Do you like Vim's string substitution? Sort of? Me too. But it's an essential tool. A weakness of this tool is how it handles case variants. :%s/FOO/BAR/g will replace FOO with BAR , and :%s/foo/bar/g will replace foo with bar , but doing both at once? That would be useful. Abolish.vim does this with the following command, adding curly bracket support along the way: :%Subvert/run{ning,ner}/walk{ing,er}/g In one line: running becomes walking RUNNING becomes WALKING Running becomes Walking runner becomes walker RUNNER becomes WALKER Runner becomes Walker This is crucial when changing the name of a concept that is used in a lot of places. Coercion When I've been working in JavaScript and switch to Ruby, the first variable I write will almost always be something like notificationEmail . And vice versa when switching the other way; that first JavaScript variable will be a Matz-approved admin_user . These mistakes violate language convention and require wasteful keystrokes to fix. Coercion is the killer feature of Abolish.vim. crs (coerce to snake case) fixes the first example: # some/ruby/file.rb noticationEmail # crs (visual mode) notification_email crc (coerce to camel case) fixes the second: // some/javascript/file.js notification_email // crc (visual mode) noticationEmail crm (coerce to mixed case), cru (coerce to upper case), cr- (coerce to dash case), and cr. (coerce to dot case) are all useful in various situations. Conclusion Once the plugin is loaded, run :h abolish to learn more. If you've just added it, this may require running :Helptags (Pathogen users) or :helptags {dir} (everyone else) first. In this blog post, we looked at Abolish.vim's setup and key features. We have only scratched the surface of what this plugin can do. Add it to your Vim stack. It's one of those workhorse plugins that makes Vim more fun and powerful. Thanks to the Rocketeers of yore for adding this to the Hashrocket Dotmatrix , and to Tim Pope for publishing it to the world. header photo via Ian Kobylanski on Flickr Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-01-19"},
{"website": "Hash-Rocket", "title": "RubyJax Recap - OpenHax - February 20, 2015", "author": ["\nMicah Cooper\n\n"], "link": "https://hashrocket.com/blog/posts/rubyjax-recap-openhax-february-20-2015", "abstract": "Community RubyJax Recap - OpenHax - February 20, 2015 by\nMicah Cooper\n\non\nFebruary 23, 2015 This is a recap of the most recent RubyJax meetup. We had a great meetup at the latest RubyJax OpenHax with new people, new projects and interesting learnings. The group was 8 strong, and everyone was working on something different: Shaun Hubbard started on the design of an iOS app that interacts with a Sinatra API to return global tide data. We successfully helped Julian Parrish switch to Postgresql on his development machine instead of Sqlite3. A veteran member, Patrick Canfield , rejoined our meetup after being out of the country for a few months. He was learning GO by created a Go backend and Javascript frontend. There was a new face this meetup: Mike Yenny . He's just getting into Ruby on Rails, and also walked us through an automotive development platform called Jetson Pro . Between these projects, some testimonials, and comparisons between Active Model Serializers and Jbuilder , we had a very productive and interesting meetup. Here's hoping the next one is as fun-filled (I'm sure it will be). RubyJax is a meetup group hosted in Jacksonville, FL & sponsored by Starfield TMS and Hashrocket . We usually meet the first three Thursdays of every month with the following agenda: First Thursday: Book Club Second Thursday: Lecture/Presendation Third Thursday: OpenHax We'd love for you to come out and join us! Was this post helpful? Share it with others. Tweet Share Post", "date": "2015-02-23"},
{"website": "Hash-Rocket", "title": "A Compendium of Hooks in EmberCLI", "author": ["\nJonathan Jacks"], "link": "https://hashrocket.com/blog/posts/a-compendium-of-hooks-in-embercli", "abstract": "Ember A Compendium of Hooks in EmberCLI by\nJonathan Jackson\n\non\nDecember 22, 2014 Below you'll find information relevant to creating addons in EmberCLI.  Addons are a powerful abstraction in EmberCLI.  These addons help enable the extension of Ember applications built with the framework.  EmberCLI has several generators that make creating addons simple, but knowing where to put your addon specific customizations can be difficult.  This post will attempt to list all known addon hooks as of EmberCLI version 0.1.4 . Table of Contents: config bluprintsPath includedCommands serverMiddleware postBuild preBuild included postProcess treeFor treeForApp treeForStyles treeForTemplates treeForAddon treeForVendor treeForTestSupport treeForPublic For each hook we'll cover the following (if applicable): Received arguments Source Default implementation Uses Examples Compendium is largely based of a talk by @rwjblue which can be found here Config Augments the applications configuration settings.  Object returned from this hook is merged with the application's configuration object.  Application's configuration always take precedence. Received arguments: env - name of current environment (ie \"developement\") baseConfig - Initial application config Source: lib/models/addon.js:312 Default implementation: Addon . prototype . config = function ( env , baseConfig ) { var configPath = path . join ( this . root , 'config' , 'environment.js' ); if ( fs . existsSync ( configPath )) { var configGenerator = require ( configPath ); return configGenerator ( env , baseConfig ); } }; Uses: Modifying configuration options (see list of defaults here ) For example minifyJS storeConfigInMeta es3Safe et, al Examples: Setting storeConfigInMeta to false in ember-cli-rails-addon blueprintsPath Tells the application where your blueprints exist. Received arguments: None Source: lib/models/addon.js:304 Default implementation: Addon . prototype . blueprintsPath = function () { var blueprintPath = path . join ( this . root , 'blueprints' ); if ( fs . existsSync ( blueprintPath )) { return blueprintPath ; } }; Uses: Let application know where blueprints exists. Examples: ember-cli-coffeescript includedCommands Allows the specification of custom addon commands.  Expects you to return an object whose key is the name of the command and value is the command instance. Received arguments: None Source: lib/models/project.js:234 Default implementation: None Uses: Include custom commands into consuming application Examples: ember-cli-cordova // https://github.com/rwjblue/ember-cli-divshot/blob/v0.1.6/index.js includedCommands : function () { return { 'divshot' : require ( './lib/commands/divshot' ) } } serverMiddleware Designed to manipulate requests in development mode. Received arguments: - options (eg express_instance, project, watcher, environment) Source: lib/tasks/server/express-server.js:63 Default implementation: None Uses: Tacking on headers to each request Modifying the request object Note: that this should only be used in development, and if you need the same behavior in production you'll need to configure your server. Examples: ember-cli-content-security-policy history-support-addon postBuild Gives access to the result of the tree, and the location of the output. Received arguments: Result object from broccoli build result.directory - final output path Source: lib/models/builder.js:111 Default implementation: None Uses: Slow tree listing May be used to manipulate your project after build has happened Opportunity to symlink or copy files elsewhere. Examples: ember-cli-rails-addon In this case we are using this in tandem with a rails middleware to remove a lock file.  This allows our ruby gem to block incoming requests until after the build happens reliably. preBuild Hook called before build takes place. Received arguments: Source: lib/models/builder.js:114 Default implementation: None Uses: Examples: ember-cli-rails-addon In this case we are using this in tandem with a rails middleware to create a lock file. [See postBuild] included Usually used to import assets into the application. Received arguments: EmberApp instance see ember-app.js Source: lib/broccoi/ember-app.js:216 Default implementation: None Uses: including vendor files setting configuration options Note: Any options set in the consuming application will override the addon. Examples: // https://github.com/yapplabs/ember-colpick/blob/master/index.js included : function colpick_included ( app ) { this . _super . included ( app ); var colpickPath = path . join ( app . bowerDirectory , 'colpick' ); this . app . import ( path . join ( colpickPath , 'js' , 'colpick.js' )); this . app . import ( path . join ( colpickPath , 'css' , 'colpick.css' )); } ember-cli-rails-addon postProcess Received arguments: post processing type (eg all) receives tree after build Source: lib/broccoli/ember-app.js:251 Default implementation: None Uses: fingerprint assets running processes after build but before toTree Examples: broccoli-asset-rev treeFor Return value is merged with application tree of same type Received arguments: returns given type of tree (eg app, vendor, bower) Source: lib/broccoli/ember-app.js:240 Default implementation: Addon . prototype . treeFor = function treeFor ( name ) { this . _requireBuildPackages (); var tree ; var trees = []; if ( tree = this . _treeFor ( name )) { trees . push ( tree ); } if ( this . isDevelopingAddon () && this . app . hinting && name === 'app' ) { trees . push ( this . jshintAddonTree ()); } return this . mergeTrees ( trees . filter ( Boolean )); }; Uses: manipulating trees at build time Examples: treeFor (cont...) Instead of overriding treeFor and acting only if the tree you receive matches the one you need EmberCLI has custom hooks for the following Broccoli trees treeForApp treeForStyles treeForTemplates treeForAddon treeForVendor treeForTestSupport treeForPublic See more here Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-12-22"},
{"website": "Hash-Rocket", "title": "Using SASS To Be Responsive and Retina-Ready", "author": ["\nCameron Daigle\n\n"], "link": "https://hashrocket.com/blog/posts/using-sass-to-be-responsive-and-retina-ready", "abstract": "Design Using SASS To Be Responsive and Retina-Ready by\nCameron Daigle\n\non\nApril 26, 2013 Responsive design is much more than just a buzzword and a loosely-organized set of principles – it's a design technique that requires you to approach almost every problem differently than a traditional website. It's also a pain to code. We're neck-deep in a new Hashrocket site design that's fully responsive and uses retina assets – and from that project (and others like it) we've ended up with a pretty solid starting point for tackling those requirements: Get iOS to Behave Responsively There are two crucial lines of code you'll need to make pretty much anything responsive work properly: a meta tag and a webkit tag. Put this in your <head> : %meta ( name= \"viewport\" content= \"width=device-width, initial-scale=1\" ) This just tells the viewport to conform to your device width (so you don't have landscape mode appear larger than portrait mode, for instance) and scale properly. Some Googlings of this technique also might result in code recommendations to add in a maximum-scale value; this disables pinch-zooming and usually isn't necessary. Put this in your Sass (or SCSS, whatever): body -webkit-text-size-adjust : none You probably know this one - it prevents iOS from sizing up your text for readability. If your site's properly responsive, you're taking care of font size with @media queries. The Viewport Width Mixin Here's our mixin for keeping ourselves sane in the world of media queries. (This one and others require a newer version of Sass that supports passing content blocks to mixins; you should be using that anyway.) = max-width ($ width : 640 ) @media screen and ( max-width : #{ $width } px ) @content Easy as pie. Just set the $width value to whatever you decide is your primary site breakpoint, and add others as necessary. This lets us write rules quickly and read them easily: .foo margin: 60px\n    +max-width\n        margin: 40px\n    +max-width(480)\n        margin: 20px The Retina Mixin Retina backgrounds are their own set of @media -based pain. Here are a pair of mixins that made our lives easier: = retina @media screen and ( - webkit-min-device-pixel-ratio : 2 ) , screen and ( min-device-pixel-ratio : 2 ) @content = retina_bg ($ filename , $ dimensions : false , $ext : \"png\" ) background-image : image-url ( \" #{ $filename } . #{ $ext } \" ) +retina background-image : image-url ( \" #{ $filename } @2x. #{ $ext } \" ) @if $dimensions != false background-size : $dimensions The retina mixin is just generally helpful; the retina_bg mixin is absolutely essential. With retina_bg , we can write our retina background image styles in one line: .bar +retina_bg(\"logo\", 100px) Now non-retina devices use logo.png , while retina uses logo@2x.png , like the iOS filename convention. It's worth noting that background-size isn't always necessary, so it's behind a conditional and just doesn't assign if it's not specified. Take Care Of Yourself Out There I sincerely hope this post is useful to you and isn't just adding to the noise of @media tips that are out there these days. If you start with these pieces, you'll be well on your way to writing responsive styles that don't muck up the rest of your CSS and help you keep all those widths & ratios sorted. Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-04-26"},
{"website": "Hash-Rocket", "title": "Boot + Middleman: The ClojureScript development environment of my dreams", "author": ["\nJoshua Davey\n\n"], "link": "https://hashrocket.com/blog/posts/the-front-end-development-environment-of-my-dreams", "abstract": "Boot + Middleman: The ClojureScript development environment of my dreams by\nJoshua Davey\n\non\nJune  4, 2015 I'm getting closer to the frontend development environment of my dreams. The combination of editor integration, live browser reload, and not having to manually run commands over and over is time-saving and a pleasure to work with. At Hashrocket, designers and developers work very closely\ntogether. Visual design and markup is handled by our\ndesigners, who create \"stubbed out\" templates in the UI\ndirectory .\nIt's a process that works very well for us, and\nallows us to iteratively add features to an application. This process has served us very well in Rails using a UI\ncontroller ,\navailable only in development mode. I've been using ClojureScript a lot lately, particularly with Om, and\nhave missed that directory of collaboration. After all, the designers\nat Hashrocket have a proclivity for HAML and SASS. In the past, I've set up a separate repository using Middleman to\nhandle markup and styles, using middleman build , copying the generated\nCSS files, and eyeballing the generated markup to ensure it matched the\nOm component's markup. Aside from being tedious, it's really easy to\nget out of sync with a manual process like this. The static resource\ngeneration should be a part of our build process. Enter boot . Using Boot for ClojureScript development If you're new to the Clojure world, you may have heard of Leiningen ,\nwhich is the de facto dependency management and build tool for\nClojure/Script. Boot is similar to Leiningen, but adds the ability to\ncompose tasks to create build pipelines. This composability, along with\nsome really smart architectural decisions, is what makes boot a great\nchoice for the problem at hand. Adzerk's example repo is a great way to get started with ClojureScript and boot. Of particular\nnote is the build.boot file. It demonstrates how one can build\nup a dev task that watches a directory for changes, rebuilding\nClojureScript sources, and notifying the browser to reload the code. It\nincludes the setup necessary for source maps, a development server,\nand the browser-connected REPL. But what I want to add to that pot\nof awesome is the ability to compile HAML and SASS as a part of the\npipeline. boot-middleman: Gluing Everything Together I had an epiphany one night after working on this problem for a while: I\ncan just use Middleman. After all, boot and the ClojureScript compiler\nrun on the JVM, and JRuby is easily embeddable. After a short bit, I\ncame up with boot-middleman , the glue I needed to build HAML/SASS as\na part of my build process. It assumes a subdirectory is a Middleman app ( assets by default). This\nworks nicely because my designer pals can collaborate with me without\nhaving to use the JVM at all. They just run middleman in the assets subdirectory and work as normal. See the boot-middleman README for instructions on setting up. I used this workflow to create a minesweeper clone, the\nsource of which is on GitHub.\nJust clone and run boot dev . The Workflow in Action To see the workflow in action, check out the following video. It\ndemonstrates how editing front-end files do not require a manual browser\nrefresh to see the effects. Was this post helpful? Share it with others. Tweet Share Post", "date": "2015-06-04"},
{"website": "Hash-Rocket", "title": "Hashrocket's 12 Gems of Christmas", "author": ["\nJohnny Winn\n\n"], "link": "https://hashrocket.com/blog/posts/holiday_gems", "abstract": "Ruby Hashrocket's 12 Gems of Christmas by\nJohnny Winn\n\non\nDecember 12, 2012 The holiday season is upon us and what better way to celebrate than to share some gems. Inspired by Mike Perham's 12 Gems of Christmas , I thought it would be a great chance to mention some of our gems of choice. These are some of our \"go to\" gems when solving problems or implementing solutions. Each day until Christmas, I'll add a new gem to this post so be sure to check back! On the First Day of Christmas - Draper Every holiday needs some decoration so we will kick off the 12 days of Christmas by looking at Draper. Draper is a gem that applies the decorator pattern to your domain objects in Rails. This allows you to take an object-oriented approach in dealing with helpers and prevents cluttering up your domain objects with presentation logic. Here is a quick example: class PersonDecorator < Draper :: Decorator def full_name model . first_name + \" \" + model . last_name end end In your controller: class PersonController < ApplicationController def show @person = PersonDecorator . new ( Person . find ( params [ :id ])) end end And your view: < %= @person.full_name %> On The Second Day of Christmas - Decent Exposure It's so cold this time of year so be careful how much you expose. Instance variables are meant to be private. Decent Exposure lets you expose methods to your views and preserve encapsulation. Recently this precious gem hit the 2.0 milestone and added an array of new features. Decent Exposure lets you take this: class PersonController < ApplicationController def new @person = Person . new ( params [ :person ]) end def create @person = Person . new ( params [ :person ]) if @person . save redirect_to ( @person ) else render :new end end def edit @person = Person . find ( params [ :id ]) end def update @person = Person . find ( params [ :id ]) if @person . update_attributes ( params [ :person ]) redirect_to ( @person ) else render :edit end end end And transform it into this: class PersonController < ApplicationController expose ( :person ) def create if person . save redirect_to ( person ) else render :new end end def update if person . save redirect_to ( person ) else render :edit end end end On the Third Day of Christmas - Dirty Is your test list getting longer? Want to find out who's are naughty or nice? Dirty is a gem that runs your updated tests based on their git status. This can be a major time saver as your test suite grows because you can run your implementation's tests without invoking the entire suite. Dirty also allows you to specify an argument to scope which type of test you want to execute. $ dirty $ dirty spec $ dirty feature On The Fourth Day of Christmas - Hitch Holiday hacking is better in pairs so make sure everyone gets credit with hitch! Hitch combines the pair into a single author in your commit and creates a unique email address. Author: Johnny Winn and Travis Anderson\n     <dev+nurugger07+syntaxritual@hashrocket.com> This email address can then be used to create a gravatar. Make sure you get hitched this holiday! On The Fifth Day of Christmas - Fabrication The elves are hard at work in the toy factory but you don't have to be. Fabrication lets you quickly generate your objects as needed. Simply define your object schematics and then call them from anywhere in your app or specs. The gem supports most common object types like ActiveRecord, Mongoid Documents, Sequel, and DataMapper but also includes Keymaker Nodes and basic class objects. The simplest way to fabricate an object it to define a Fabricator and pass the name to Fabricate: Fabricator ( :toy ) do name \"Sphero\" description \"Cool Robot\" end Fabricate ( :toy ) # => <Toy name: \"Sphero\", description: \"Cool Robot\"> Attributes can also be overridden at the time of fabrication: Fabricate ( :toy , name: \"Parrot Drone\" ) # => <Toy name: \"Parrot Drone\", description: \"Cool Robot\"> Also, the sequence feature works perfectly for generating multiple unique objects: Fabricator ( :toy ) do name { sequence ( :name ) { | i | \"Sphero version #{ i } .0\" } } end 3 . times do Fabricate ( :toy ) end # => <Toy name: \"Sphero version 1.0\"> # => <Toy name: \"Sphero version 2.0\"> # => <Toy name: \"Sphero version 3.0\"> On The Sixth Day of Christmas - FFaker Trying to express your excitement for the holidays? So many things to say but finding the right words can be hard. Why not fake it? Or at least use FFaker to fake it for you. FFaker is a faster port of Data::Faker from Perl used to generate fake data. It covers everything from names to phone numbers and even IP addresses. This little gem is the gift that keeps on giving, giving fake data that is. Need a fake name? Faker :: Name . name # => \"Nathanael Feil II\" What about a title using the latest buzz words? Faker :: Name . title # => \"Cheif Data Director\" Faker can even be paired with Fabrication to help generate unique objects: Fabricator ( :elf ) do name { Faker :: Name . name } title { Faker :: Name . title } end 3 . times do Fabricate ( :elf ) end # => <Elves @name=\"Sandrine Gorczany\", @title=\"Chief Response Designer\"> # => <Elves @name=\"Jakob Ondricka\", @title=\"National Research Liason\"> # => <Elves @name=\"Kelsi Weimann\", @title=\"Lead Operations Consultant\"> On The Seventh Day of Christmas - mail_view Before you send those letters to Santa, make sure they are perfect with mail_view . Mail_view allows you to preview html and plain text emails in your browser without resending the message. This comes in handy while tweeking your email templates. There are some additional steps when setting up mail_view similar to a mailer unit test. Unlike mailer unit tests you will get the gift of a visual representation. # app/mailers/mail_preview.rb class MailPreview < MailView def welcome user = User . create! mail = Notifier . welcome ( user ) user . destroy mail end end Then add a route for the developement environment. # config/routes.rb if Rails . env . development? mount MailPreview => 'mail_view' end Finally call http://localhost:3000/mail_view to list links to your preview(s). On The Eighth Day of Christmas - stamp Is it December 25, 2012 or Dec 25, 12? Either way you can use stamp to format your date with a human-friendly example. Stamp also handles time formatting. It's easy to set up and you can even configure your Rails applications with self-documenting formats. Once you install the gem, simply pass your human-friendly format: date = Date . new ( 2012 , 12 , 25 ) date . stamp ( \"June 25, 2012\" ) => \"December 25, 2012\" date . stamp ( \"Jun 25, 2012\" ) => \"Dec 25, 2012\" Configuring your Rails date formats in an initializer: Date :: DATE_FORMATS [ :long ] = Proc . new { | date | date . stamp ( \"December 25, 2012\" ) } On The Nineth Day of Christmas - better_errors It's hard to stay off the naughty list if you don't know what you're doing wrong but the Rails default error messages can be hard as coal to break. The better_errors replaces the standard Rails error page with a well formatted and useful page. The gem can also be used with your Sinatra app as Rack middleware. In addition to the stack trace and source code inspection, the page can also include local and instance variable inspection as well as live REPL on every stack frame. The advanced features require the binding_of_caller gem. Setup is simply a matter of adding the gem(s) to a development group in your Gemfile. Here is an example for a Sinatra app with an optional BetterErrors.application_root to abbreviate filenames within an app: require 'better_errors' use BetterErrors :: Middleware BetterErrors . application_root = File . expand_path ( '..' , __FILE__ ) get '/' do raise 'you made the naughty list' end On The Tenth Day of Christmas - database_cleaner Tidying up your database after running tests shouldn't take as long as cleaning up after holiday guests. Cucumber-rails users may be familiar with the little gem database_cleaner which provides strategies for cleaning up databases in Ruby. It was designed to ensure a clean state during tests and it supports several ORMs including ActiveRecord, DataMapper, and Mongoid. Each ORM has a default strategy and some offer alternative options. If you are using database_cleaner in tandem with cucumber-rails then configuring your strategy is handled in features/support/env.rb : # features/support/env.rb begin DatabaseCleaner . strategy = :transaction rescue NameError # raise error to add database_cleaner to Gemfile end What if you're using a combination of ORMs like ActiveRecord and Mongoid? # features/support/env.rb begin DatabaseCleaner [ :active_record ]. strategy = :transaction DatabaseCleaner [ :mongoid ]. strategy = :truncation rescue NameError # raise error to add database_cleaner to Gemfile end With the :truncation strategy you can even limit to specific tables by passing options: DatabaseCleaner . strategy = :truncation , { only: %w[person place other_table] } DatabaseCleaner . strategy = :truncation , { except: %w[thing] } On The Eleventh Day of Christmas - quantum_leap With all the joy of the holidays wouldn't it be great to freeze time or at least stabilize your test suite? The quantum_leap gem lets you travel in time to revisit those past holidays and fix time wrongs in your tests. Quantum . leap ( Time . new ( 1985 , 12 , 25 )) johnny . must_be_kind_of ( Child ) johnny . must_get_nintendo Quantum . leap_back Or with a block: Quantum . leap ( Time . new ( 1989 , 12 , 25 )) do johnny . must_be_kind_of ( Child ) johnny . must_get_sega end On The Twelfth Day of Christmas - redcarpet Forgetting to send out Christmas cards can be a holiday faux pas but forgetting to format them can be down right disatrous. Markdown is a great way to format and the redcarpet gem is perfect for your markdown processing. Redcarpet is an api powered by the  C library sundown . The real beauty is in its simplicity. For basic usage, create a new instance of a Markdown object with a renderer. Then pass the instance the markdown text to render: markdown = Redcarpet :: Markdown . new ( Redcarpet :: Render :: HTML ) markdown . render ( \"## Merry Christmas!\" ) #=> \"<h2>Merry Christmas!</h2>\" As Christmas passes and we look forward to the New Year, we wrap up this present with a thank you! It has been fun sharing these gems with you. Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-12-12"},
{"website": "Hash-Rocket", "title": "16 Tips from the 2014 Winter Miniconf", "author": ["\nJack Christensen\n\n"], "link": "https://hashrocket.com/blog/posts/16-tips-from-the-2014-winter-miniconf", "abstract": "PostgreSQL Ruby 16 Tips from the 2014 Winter Miniconf by\nJack Christensen\n\non\nDecember 18, 2014 Hashrocket hosts an internal conference twice a year called Miniconf where we take a day to share ideas with each other. This last Miniconf I gave a quick 20 minute talk with a grab bag of Linux, shell, PostgreSQL, and Ruby tricks. This article is that talk converted to text format. Autojump Autojump is a quick way to navigate the file system. It keeps a database of where you have been in the file system and uses that database to jump back based on partial paths. For example, Go projects tend to have deep directory structures. With autojump it is easy to go to a favorite directory. jack@hk-4~$ j pgx\n/Users/jack/dev/go/src/github.com/jackc/pgx\njack@hk-4~/dev/go/src/github.com/jackc/pgx$ Autojump is available via apt-get and homebrew. Etckeeper Etckeeper tracks the /etc directory in Git. This eliminates the need to make manual backups of config files before editing. On Debian platforms it hooks into the apt-get process and automatically adds and commits any changes package installation or removal caused. Shell Brace Expansion It is often necessary to run a command with multiple arguments that only differ by a few letters. Brace expansion saves key strokes by eliminating the need to retype the duplicate portions. For example, to change the extension on a file: jack@hk-4/tmp/example$ touch test.foo\njack@hk-4/tmp/example$ mv test.{foo,bar}\njack@hk-4/tmp/example$ ls\ntest.bar Brace expansion is supported in bash and several other popular shells. Shell Command Quick Substitution It is common to need to run a command very similar to the previous command. Sometimes a sequence of commands are similar, and sometimes simple typo needs to be corrected. Quick substitution performs a find and replace on the previous command and reruns it. jack@hk-4/tmp/example$ echo 'Hello, world'\nHello, world\njack@hk-4/tmp/example$ ^Hello^Goodbye\necho 'Goodbye, world'\nGoodbye, world Tail Multiple Files I had used tail for years with the -f option to watch log files, but somehow never realized that it could tail multiple files simultaneously. This is invaluable when you expect something to be logged, but you do not know to what file it will be logged. jack@hk-4/var/log$ tail -f * The -n 0 option is sometimes also useful to avoid the initial wall of input from the last ten lines of every file. Auto-formatting psql Output The default output of psql can be hard to read when the output rows are wider than the terminal. jack=# select * from information_schema.columns;\ntable_catalog |\ntable_schema    |              table_name               |\ncolumn_name             | ordinal_position |              column_default\n| is_nullable |        data_type         | character_maximum_length |\ncharacter_octet_length | numeric_precision | numeric_precision_radix |\nnumeric_scale | datetime_precision | interval_type | interval_precision |\ncharacter_set_catalog | character_set_schema | character_set_name |\ncollation_catalog | collation_schema | collation_name | domain_catalog |\ndomain_schema    |   domain_name   | udt_catalog | udt_schema |   udt_name   |\nscope_catalog | scope_schema | scope_name | maximum_cardinality | dtd_identifier\n| is_self_referencing | is_identity | identity_generation | identity_start |\nidentity_increment | identity_maximum | identity_minimum | identity_cycle |\nis_generated | generation_expression | is_updatable ---------------+------------ --------+---------------------------------------+-------------------------------\n------+------------------+-------------------------------------------+----------\n---+--------------------------+--------------------------+----------------------\n--+-------------------+-------------------------+---------------+---------------\n-----+---------------+--------------------+-----------------------+-------------\n---------+--------------------+-------------------+- <snip> Fortunately, psql can automatically switch between vertical and horizontal layout based on the width of output. jack=# \\x auto\nExpanded display is used automatically.\njack=# select * from information_schema.columns;\n-[ RECORD 1 ]------------+------------------------------------------\ntable_catalog            | jack\ntable_schema             | public\ntable_name               | alarms\ncolumn_name              | id\nordinal_position         | 1\ncolumn_default           | nextval('alarms_id_seq'::regclass)\nis_nullable              | NO\ndata_type                | integer\ncharacter_maximum_length |\n<snip> Timing Query Execution with psql psql makes it easy to measure how long a query takes to run. Simple enable it with \\timing . jack=# \\timing\nTiming is on.\njack=# select count(*) from information_schema.columns;\n count\n-------\n  1566\n(1 row)\n\nTime: 10.577 ms Saving psql Settings It would be nice not to have to manually enable \\timing and \\x auto every time you load psql. The .psqlrc file is the solution. It is executed on startup by psql. jack@hk-4~$ cat .psqlrc\n\\x auto\n\\timing\njack@hk-4~$ psql\nExpanded display is used automatically.\nTiming is on.\npsql (9.3.5)\nType \"help\" for help.\n\njack=# Date and Time Ranges Time periods are frequently stored in databases for resource allocation, reservation, and logging. The typical way of storing this would be to use two columns, one for the start and one for the end of the range. While this works to some extent, the PostgreSQL range types offer a number of advantages. For example, there is nothing to stop an end_date column from being less than a start_date column. But with a PostgreSQL daterange that is impossible. jack=# select daterange('2014-12-12', '2014-12-01');\nERROR:  range lower bound must be less than or equal to range upper bound Ranges also have a number of operators to test inclusion of an element or if another range overlaps. For example, the code below uses the contains element operator ( @> ). jack=# select daterange('2014-12-01', '2014-12-12') @> '2014-12-06'::date;\n ?column?\n----------\n t\n(1 row) Exclusion Constraints Ranges are especially valuable when combined with exclusion constraints . For example, with this combination PostgreSQL can prevent duplicate overlapping room reservations. First, we need to install the btree_gist extension to make an exclusion constraint that includes an integer type. jack=# create extension btree_gist;\nCREATE EXTENSION Now, we will create a table with the exclusion constraint. The constraint will exclude any rows from being inserted where an existing row matches based on room_name equality and date_range overlap. jack=# create table reservations(\njack(#   id serial primary key,\njack(#   room_name text not null,\njack(#   guest_name text not null,\njack(#   date_range daterange not null,\njack(#   exclude using gist (room_name with =, date_range with &&)\njack(# );\nCREATE TABLE We will insert a record. jack=# insert into reservations(room_name, guest_name, date_range)\njack-#   values('101', 'George', daterange('2014-12-01', '2014-12-05'));\nINSERT 0 1 Finally, we will insert a record that overlaps, but PostgreSQL will stop it. jack=# insert into reservations(room_name, guest_name, date_range)\njack-#   values('101', 'John', daterange('2014-12-03', '2014-12-07'));\nERROR:  conflicting key value violates exclusion constraint \"reservations_room_name_date_range_excl\"\nDETAIL:  Key (room_name, date_range)=(101, [2014-12-03,2014-12-07)) conflicts with existing key (room_name, date_range)=(101, [2014-12-01,2014-12-05)). Building JSON in SQL PostgreSQL has many JSON functions and operators for JSON generation. One of the most useful is row_to_json . Continuing with the previous example of reservations, let's turn the row into JSON. jack=# select row_to_json(r) from reservations r;\n-[ RECORD 1 ]----------------------------------------------------------------------------------------\nrow_to_json | {\"id\":1,\"room_name\":\"101\",\"guest_name\":\"George\",\"date_range\":\"[2014-12-01,2014-12-05)\"} Sometimes we want to group many rows into a single JSON array. This is where json_agg comes in handy. Let's insert a few more reservations. jack=# insert into reservations(room_name, guest_name, date_range)\njack-#   values('102', 'John', daterange('2014-12-03', '2014-12-07'));\nINSERT 0 1\njack=# insert into reservations(room_name, guest_name, date_range)\njack-#   values('101', 'Thomas', daterange('2014-12-05', '2014-12-10'));\nINSERT 0 1 Now lets get all the records as a single JSON array. jack=# select json_agg(row_to_json(r)) from reservations r;\n-[ RECORD 1 ]-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\njson_agg | [{\"id\":1,\"room_name\":\"101\",\"guest_name\":\"George\",\"date_range\":\"[2014-12-01,2014-12-05)\"}, {\"id\":3,\"room_name\":\"102\",\"guest_name\":\"John\",\"date_range\":\"[2014-12-03,2014-12-07)\"}, {\"id\":4,\"room_name\":\"101\",\"guest_name\":\"Thomas\",\"date_range\":\"[2014-12-05,2014-12-10)\"}] See Faster JSON Generation with PostgreSQL for more information. Extracting JSON in SQL Sometimes you need to extract data from JSON and operate on it relationally. PostgreSQL has functions and operators for JSON extraction. Let's start with a simple JSON object store. jack=# create table objects(\njack(#   id serial primary key,\njack(#   body json not null\njack(# );\nCREATE TABLE Then add some records. jack=# insert into objects(body) values\njack-#   ('{\"name\": \"John\", \"age\": 30, \"weight\": 180, \"gender\": \"male\"}'),\njack-#   ('{\"name\": \"Emily\", \"gender\": \"female\"}'),\njack-#   ('{\"name\": \"George\", \"occupation\": \"developer\"}');\nINSERT 0 3 json_each is a set-returning function that returns a row for each key/value pair in the JSON object. jack=# select *\njack-# from json_each((select body from objects where id=1));\n  key   | value\n--------+--------\n name   | \"John\"\n age    | 30\n weight | 180\n gender | \"male\"\n(4 rows) Lateral Joins To continue the previous example, what if we wanted to count all the keys in all the rows in the objects table? Lateral joins are the solution. A lateral join lets a table expression refer to columns from previous from items. jack=# select key, count(*)\njack-# from objects\njack-#   cross join lateral json_each(objects.body)\njack-# group by key;\n    key     | count\n------------+-------\n occupation |     1\n gender     |     2\n weight     |     1\n name       |     3\n age        |     1\n(5 rows) Note that in this particular case the lateral key word is optional -- it is included for clarity. Generating Series PostgreSQL makes it easy to generate a series of numbers. jack=# select *\njack-# from generate_series(1, 5);\n generate_series\n-----------------\n               1\n               2\n               3\n               4\n               5\n(5 rows) It also can work with times. jack=# select *\njack-# from generate_series(\njack(#   '2014-01-01'::timestamptz,\njack(#   '2014-01-03'::timestamptz,\njack(#   '8 hours'\njack(# );\n    generate_series\n------------------------\n 2014-01-01 00:00:00-06\n 2014-01-01 08:00:00-06\n 2014-01-01 16:00:00-06\n 2014-01-02 00:00:00-06\n 2014-01-02 08:00:00-06\n 2014-01-02 16:00:00-06\n 2014-01-03 00:00:00-06\n(7 rows) Executing Javascript in PostgreSQL Occasionally, it may be desirable to execute Javascript in the database. With the plv8 extension this is possible. Here is a simple example of a name formatter function in JSON. dev=# create extension plv8;\nCREATE EXTENSION\ndev=#\ndev=# create function format_name(first_name text, middle_name text, last_name text)\ndev-# returns text AS $$\ndev$#\ndev$#   var name = first_name\ndev$#\ndev$#   if(middle_name) {\ndev$#     name += \" \" + middle_name\ndev$#   }\ndev$#\ndev$#   name += \" \" + last_name\ndev$#\ndev$#   return name\ndev$# $$ language plv8 immutable;\nCREATE FUNCTION\ndev=#\ndev=# select format_name('John', NULL, 'Smith');\n format_name\n-------------\n John Smith\n(1 row)\n\ndev=#\ndev=# select format_name('John', 'Mark', 'Smith');\n   format_name\n-----------------\n John Mark Smith\n(1 row) plv8 is installable via apt-get and is part of Postgres.app . Unfortunately, it is not available via homebrew. Extracting Data with the Ruby Flip-flop Sometimes you may need to search a huge file for a starting delimiter and extract data until you get to a ending delimiter. For example, a SQL dump may include millions of lines and you may need to extract the lines for a single table. Ruby has got it covered and it can do it in one line! This will involve a lot of Ruby shortcuts so we will build up one step at a time. To start with, the ruby binary takes a -e option that will execute ruby code. jack@hk-3~$ ruby -e 'puts 42'\n42 Next we will create a test file to work with. Save the following as giant.log. bunch of data we don't care about\nbla\nbla\nbla\n\nBEGIN\nwe care about this\nEND\n\nadfkjl\nadsf The -n option tells Ruby to assume a while gets loop around the code we pass with -e . $_ is a magic variable that stores the last value returned by gets . jack@hk-3~$ ruby -n -e 'puts $_' < giant.log\nbunch of data we don't care about\nbla\nbla\nbla\n\nBEGIN\nwe care about this\nEND\n\nadfkjl\nadsf At this point, all we have done is duplicate cat . But we can do more. The first idea would be to only print if we match a regular expression. If we use a suffix if with a regex then Ruby automatically compares it with $_ . jack@hk-3~$ ruby -n -e 'puts $_ if /BEGIN/' < giant.log\nBEGIN This is getting closer. But we want from BEGIN to END. Enter the flip-flop operator. jack@hk-3~$ ruby -n -e 'puts $_ if /BEGIN/../END/' < giant.log\nBEGIN\nwe care about this\nEND The flip-flop operator looks like a range, but it isn't. Instead it evaluates to true when the first condition is met and stays true until the last condition is met. Summary I use some of these almost every day. Others, many months will go by between uses. But I have found all of them very useful. I hope you do too. Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-12-18"},
{"website": "Hash-Rocket", "title": "Easy Star Ratings in CSS", "author": ["\nCameron Daigle\n\n"], "link": "https://hashrocket.com/blog/posts/easy-star-ratings-in-css", "abstract": "Design Easy Star Ratings in CSS by\nCameron Daigle\n\non\nJanuary  8, 2015 This pattern comes up regularly for us: providing the ability for a user to rate something via stars. Back in the day, we'd use a repeating background image and some JavaScript, but it's 2015 now, and in the world of retina screens and CSS3, we can do better. The Basic Star A star rating is just fancy UI allowing the user to select from a set of values (typically 1-5). Instead of using Javascript to write the user's selection back to a hidden input, we're going to skin a list of radio inputs. Since radios need a common name and unique values & ids, we'll use the arbitrary name rating here. .rating_selection %input ( type= 'radio' name= 'rating' value= \"rating_1\" id= \"rating_1\" ) %label ( for= \"rating_1\" )> %span Rate 1 Star %input ( type= 'radio' name= 'rating' value= \"rating_2\" id= \"rating_2\" ) %label ( for= \"rating_2\" )> %span Rate 2 Stars / ... and so on You'll note that each label has a span inside. This is for two reasons: We're going to be using a :before element on the label to display our stars and make them clickable ( :before elements on inputs behave strangely) We need to hide the text of the label (the label text isn't absolutely necessary, but is useful for accessibility & testing). The star itself is going to be a character in the content attribute of the label's :before element. Normally I'd use an icon font here, but since this is just an example, we'll use the standard ★ character. One other detail: note the > character after each label . This is a special HAML character that removes white space in the HTML output, so our inline-block elements will render directly next to one another. Here's the Sass for our basic star rating: .rating_selection input [ type = 'radio' ], span display : none label cursor : pointer & :before display : inline-block content : \"★\" font-size : 80px letter-spacing : 10px color : #e9cd10 This will give you a row of yellow stars. Hooray! Selecting a Star Rating The next challenge is how to display selected stars. You'll note that our current stars are all yellow: this is because CSS can only look forward, not behind, so we have to use CSS rules to grey out stars AFTER our currently selected star. We have two overall states to account for: when I select a star, I should see that star and all previous stars highlighted. when I hover over a star, regardless of what is currently selected , I should see that star and all previous stars highlighted. The first state is relatively simple, thanks to some sneaky uses of CSS sibling selectors. .rating_selection input :checked + label ~ label :before color : #aaa This amalgamation of selectors works because of our flat DOM structure. It looks forward from our currently checked input PAST the direct sibling (that's the + label ) part, and then selects ALL following label:before elements (that's the magic of the ~ selector: it selects all following siblings, regardless of what other content might also be present). So now our stars will highlight properly when clicked. Adding a Hover State The hover state is a little trickier: we want the user to be able to hover over any star and see the appropriate amount of stars highlighted. To do that, we needs to disregard the currently selected star completely. We'll need to be more specific in our selector chain, in order to override the selection rules. (You could also use the !important crutch here, but we'll rise above that.) We'll add our rules to a :hover state on the containing .rating_selection element, and use an attribute selector on the label to make it more specific than our previous rule. label[for] here is a very safe option, as labels should always have a for attribute. .rating_selection & :hover // make all labels yellow again label [ for ] :before color : #e9cd10 // grey out all labels after hovered label label :hover ~ label :before color : #aaa Accounting for unrated items The last challenge here is to allow for a state where a rating hasn't taken place yet. This is actually pretty straightforward: we'll just add a rating_0 radio, and hide it. .rating_selection %input ( type= 'radio' name= 'rating' value= \"rating_0\" id= \"rating_0\" checked ) %label ( for= \"rating_0\" )> %span Unrated // other rating elements here To hide it, we'll just hide the first label using the :first-of-type selector. That's all we have to add, since radio inputs are already hidden. .rating_selection label :first-of-type display : none And that's it! Scalable, vector, JavaScript-free rating stars in 40 total lines of code. Here's a CodePen with the full example. I hope this will prove useful to you. And Happy New Year! Was this post helpful? Share it with others. Tweet Share Post", "date": "2015-01-08"},
{"website": "Hash-Rocket", "title": "How to get help with Vim", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/how-to-get-help-with-vim", "abstract": "Vim How to get help with Vim by\nJosh Branchaud\n\non\nJanuary 12, 2016 Vim is a behemoth of powerful features, small tricks, copious keybindings, legacy conventions, and dusty corners. It presents rewards and challenges whether you are starting to learn the basics, gaining competency, or even mastering Vim (whatever that looks like). Regardless of the stage in which you find yourself, Vim always offers more to learn and explore. Perhaps you fat-finger the wrong key and something unexpected happens; now you want to know what that key does. Maybe you've started making more use of Vim's unique flavor of regex but you cannot quite get that one regex pattern right. You always use that one feature in normal mode and now you are wondering if there is a visual mode equivalent. Day in and day out as the curious programmer works in Vim, these sorts of questions and wonderings crop up. By following our curiosities, discovering answers to our questions, and even taking brief peeks down the rabbit hole, we sharpen this tool. We become more efficient. We move toward proficiency and even mastery. In order to do all of this, we need to know how to ask Vim for help. Asking For Help The easiest way to ask for help is to start with executing :help during a Vim session. This will drop us into the main help file which has an overview of the basics. To get help with a specific command, we can provide that command as an argument to the :help command. By invoking :help gg , we learn more details about gg including that <C-home> does the same thing and that by providing a [count] , we can use gg to jump anywhere in a file. Contextual Help By default, the command we give to :help is assumed to be a normal mode command. If we want to find commands that are used in other modes, we must use different conventions. For instance, if we run :help u , we will find ourselves in the undo-commands section. In normal mode, u will undo the latest change. But what does u do if we are in visual mode? To find out, we have to format our help command a bit differently. By running :help v_u , we discover that u has drastically different behavior when invoked from visual mode. It transforms all the text in the visual selection to lowercase. To get to the help for any command used in visual mode, we have to prepend v_ to that command. The insert commands can be found by prepending i_ to the command. For instance, :help i_<Esc> will tell us how the escape key will transition us from Insert or Replace mode into Normal mode. The command-line commands can be found by prepending : to the command just as we'd do if we were invoking the command itself. The most obvious example is :help :help . So meta. Regular expressions can be found by prepending / to the character in question. For instance, if we type :help /\\w , we'll discover that \\w when used in a regex pattern represents any word character. From there, we can browse through the other regex patterns. There are a couple additional conventions I didn't mention. See :help help-context for the whole picture. What's That Command Called? Even with :help at our disposal, there are going to be commands that we don't know by keyword and can't seem to find. Not to worry though. If we have an idea of what the command does, we should be able to track it down with :helpgrep . The :helpgrep command is aptly named; it allows us to grep through our Vim help files. When we give :helpgrep a search term, it will look through all help files for occurrences of that term. The end result is a list of locations (filenames and line numbers) where that term appears. Vim will put this list in the Quickfix List for our convenience. Let's look at an example. We want to do a substitution throughout an entire file, but we cannot remember all the details. We try searching :help substitution to no avail. Being unsure of how else to get there, we can lean on :helpgrep . By invoking :helpgrep substitution , we get a big list of places where the word substitution occurs in the help files. If we pop open the Quickfix List with :copen , we see all the places that the word substitution shows up. We can navigate around until we find an entry that looks promising. The entry in change.txt looks good, so we move the cursor over that line and hit enter . After exploring this file a bit, we discover what we were looking for. We also learn that the keyword we should have used with :help is substitute instead of substitution . The Vim help files can be a bit brittle in this way. Good thing we have :helpgrep . Following Tags As we read through the help files, we will often encounter keywords about which we want to know more. If these keywords are highlighted in light blue, they are tags which link to a corresponding definition. If we move the cursor over one of those tags and hit CTRL-] , Vim will jump us to the definition. As we explored the substitute command in the previous section, we noticed pattern mentioned in a number of places. If we want to know more about what pattern means and how we can construct a pattern for the substitute command, we just need to move the cursor over that keyword and hit CTRL-] . Vim will jump us to the definition of pattern where we can read more. It's Just Vim Remember, we are viewing these help files in Vim itself. That means we can take advantage of all our favorite Vim tricks. We can perform a search in the curent buffer with / . We can jump to the top of the file with gg . We can use the jump list to get back to where we've been after we've gone too far off on a tangent. And so on. Plugin Help Most Vim plugin developers are good about including detailed help files with their plugins. We can use all the same techniques listed above to get help with plugin-specific commands. Consider the scenario where we have the vim-abolish plugin included. We know vim-abolish provides the crs command as a way of converting a camel-cased word to snake-case, but what other commands does this plugin provide? We try typing :help crs , but there is no match. We then try :helpgrep abolish which drops us right into the vim-abolish help file. From there, it is trivial to search for snake and find details about all the ways we can coerce the case of a word. Outside Help Efforts to get help and discover answers to questions shouldn't be confined to exploring the Vim help files. Though they are usually the best place to start, there are many external resources of which we can take advantage. When googling a Vim question, I usually find myself in one of a few place: Vim Tips Wiki , #vim stackoverflow , or any number of highly-specific blog posts. At Hashrocket (in Chicago), we host VimChi , a meetup for people to discuss and nerd out on Vim. Even if you aren't in the Chicago-area, we have a VimChi Slack group (let me know if you want an invite). This is a great place for people to ask questions and share the latest Vim tricks they have learned. Final Thoughts Continue to explore and be curious. By adding new commands little by little to your repertoire, you'll march steadily toward mastery of this powerful tool. From time to time, it can be valuable to poke around a little. Think of a random command, open the help file for it, and then read about all the other commands around it. If a command's description mentions a keyword that you don't recognize, use the CTRL-] trick to follow that keyword tag to its definition. You'll discover cool features and dusty corners of Vim that you couldn't have imagined. That being said, there is a lot to know in Vim. Too much really. But as long as you know where to look for help, that's not a problem. Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-01-12"},
{"website": "Hash-Rocket", "title": "What's with the mustaches?", "author": ["\nDaniel Ariza\n\n"], "link": "https://hashrocket.com/blog/posts/what-s-with-the-mustaches", "abstract": "What's with the mustaches? by\nDaniel Ariza\n\non\nMay 18, 2012 If you haven't seen any of the rocketeers lately, then you've been missing out on some of the finest facial hair ever to be grown by humans. Why are we growing such elegant mustaches? Well for charity of course . We're supporting Smile Train by growing mustaches for the month of may. Smile Train is dedicated to helping the millions of children in the world who suffer from cleft lip and palate through free surgery for children, free training for doctors, ancillary treatment and follow-up care. It's a great cause and we're having a blast doing it. You can expect frequent and incredible photos that feature truly breathtaking mustaches. We hope at the very least you'll get a chuckle out of it and in the end you'll help us support a worthy cause. Stacherocket Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-05-18"},
{"website": "Hash-Rocket", "title": "Introducing Komments", "author": ["\nPavel Pravosud\n\n"], "link": "https://hashrocket.com/blog/posts/komments", "abstract": "Community Introducing Komments by\nPavel Pravosud\n\non\nNovember 17, 2014 I'm excited to introduce Komments, a developer-friendly commenting system. What is it? Komments is an embeddable commenting widget that supports markdown, source code highlighting, emojis and mentions. It also provides one-click authentication via social network accounts and clean, unobtrusive UI. Why use it? If you're a developer with a blog, chances are you want your readers to be able to post code code snippets in the comments. With Komments it's easier than ever, since it supports code highlighting for almost every language Github knows about. It also uses markdown that even most non-technical people know how to deal with. How to get it? In order to put Komments on your blog you need to register as a site-owner on http://komments.net and add a new site. It gives you an HTML code snippet that you insert into your website template. The Komments widget will appear in the place where code was inserted. Cool stuff Komments tries to adapt to the color scheme of your site to look natural. You can also customize styles of the widget container to even better match your site. Here are few examples of Komments in action: Vic's Ember Tutorial Vic's Vim Tutorial Finally, Komments are enabled for this post, so you can try it out right now! Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-11-17"},
{"website": "Hash-Rocket", "title": "Introducing Hashshake", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/introducing-hashshake", "abstract": "Introducing Hashshake by\nJake Worth\n\non\nMarch 11, 2021 Remote work can be challenging. It’s easy to overindex on results and forget that there’s a human on the other side of that video call. In an attempt to soften our conversation and humanize us as teammates, I built a program called Hashshake to assign short, randomized, weekly one-to-one coffee dates with members of our Hashrocket team. What is Hashshake? Hashshake (Hashrocket + Handshake) is an application to foster communication and camaraderie between remote teammates. It is inspired by Justin Searls' 2017 RailsConf Keynote . As an aside, I’m not sure if Justin was proposing a serious or hypothetical idea with these virtual coffee dates. I took it seriously! On Monday, Hashshake randomly chooses pairs of coworkers to meet for informal, 20-minute conversations via a Slack bot message. Your coworkers link up, arrange the meeting, and talk about whatever they want. Then next week, we do it again. Initial Observations We’ve run this program once a week for a month. What have we learned? I’ve loved getting to know more about my teammates, even those I’ve worked with for 6+ years. Rocketeers are still doing fascinating things with their work and free time. My coworkers are learning, improving, and doing their best to take care of themselves and others as we negotiate this lockdown world. The first call or two felt rocky at the beginning. It’s strange to talk about normalcy. It takes courage to be vulnerable. But once we’ve gotten through the first few minutes, these calls have been fun. They make me feel less like a cog and more like a valued teammate working with smart people.\n​ Conclusion ​\nThis is the kind of fun MVP you put together in half of a lunchtime; pull requests are welcome on the Hashshake repo .​ How is your team weathering our remote-work world? How do you stay connected beyond a shared IDE session? Let me know at @jwworth on Twitter. Photo by Alesia Kazantceva on Unsplash Was this post helpful? Share it with others. Tweet Share Post", "date": "2021-03-11"},
{"website": "Hash-Rocket", "title": "GitHub Pull Request and Issue Templates", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/github-pull-request-and-issue-templates", "abstract": "GitHub Pull Request and Issue Templates by\nJake Worth\n\non\nJanuary  5, 2021 Communicating via pull request or issue takes considerable skill. The target\naudience can range from coworkers, to open-source maintainers you've never met,\nto yourself in the future. PRs and issues communicate priorities, start\nconversations, and shape the codebase. If conveying the best information in the\nmost compelling way is of interest to you, read on. My coworkers and I have developed some techniques to handle the communication\nopportunity of a PR or issue. Here are the templates\nwe use, and how we use them. Wait, Why Follow a Template? It's a bit more time consuming to ask each customer of your application to fill\nout these forms. Why bother? I believe that in almost every scenario, it's the best option. You'll get\nbetter contributions, because people who don't care enough to fill out a simple\nform will self-select out of the process. That's good! The PR or issue should\nbe important enough that they feel their time is justified. PR Template The first thing you can do is make sure your team has a clear and consistent convention around PR names, whether that be ticket number or special keywords like fix or feature . These details set the stage for your reviewers to have an easy time reading your code. In the body of the PR, I suggest this format: ### What\n\n### Why\n\n### How 'What' What is the reason for this change? I think anybody on the team– dev, PM, QA\ntester, executive– should be able to parse this; plain English is preferred to\njargon. Example: \"Fixes a bug where the count of unread emails was off by one.\" 'Why' Why is this change important? Why is the code worth reviewing? I love filling\nout this step, because when I reframe my code change from a user's perspective,\nI often realize some key detail of the experience I've missed. Example: \"It's important that users know how many emails they need to read.\" 'How' How was this change accomplished? Try to go beyond a play-by-play of the Git diff;\nthat's already available to everyone reading the code. What's unique about this\nimplementation? To what decisions do you want to draw your team members' attention? What are you proud of, or unsure about? Example: \"Migrates the database with an irreversible migration, and removes\nall references to the class.\" Issue Template Peruse the open issues on any popular project and you'll find long, circular\ndialogues between maintainers and issue openers, answering repetitive\ncontextual questions like: how do I reproduce this? What were you expecting,\nand what happened?  What kind of computer setup were you using? The template\nbelow answers these questions preemptively, saving everyone time and\nkeystrokes. # Steps to reproduce\n\n### Expected behavior\n\nTell us what should happen\n\n### Actual behavior\n\nTell us what happens instead\n\n### System configuration\n\n**Operating System**:\n\n**Browser**: The more information you can provide, the better! Have a screenshot? Throw it\nin there! Does this only happen 'some of the time'? Include that! Do you care\nabout this issue– is it preventing you from getting work done? Or do you think\nit's unimportant? Did you try to solve it? What worked and what didn't? I think\nissue openers should feel empowered to share details like this. You might be\nsaving someone a lot of work. Next Level (GitHub) Once your team is feeling good about these templates, commit them to the\nrepository like so: $ mkdir .github\n# create templates...\n$ git add .github/issue_template.md\n$ git add .github/pull_request_template.md\n$ git commit -m \"Add PR and issue templates 🤘\" Now, these templates will be automatically populated in the textareas of your\nGitHub PR's and issues. Next Next Level (GitHub) Thirsty for more? After opening your PR specifically, go to the PR page and\ncomment on your own code! Remember those unique details, decisions you wanted\nto draw attention to, points of pride, and points of worry? Review your own\ncode and give everyone a shortcut to the important information. Conclusion With a PR or issue that follows these templates, and includes as many\nscreenshots, details, and comments as possible, we make code maintenance easier\nfor everyone. Photo by Ricardo Gomez Angel on Unsplash Was this post helpful? Share it with others. Tweet Share Post", "date": "2021-01-05"},
{"website": "Hash-Rocket", "title": "How to Prepare for a Technical Interview", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/how-to-prepare-for-a-technical-interview", "abstract": "Process How to Prepare for a Technical Interview by\nJake Worth\n\non\nAugust  6, 2019 Some folks I know are preparing for their first technical interviews.\nIn this post, I'd like to share what I've learned about technical interviews\nafter several years in this field. The way I approach technical interviews is a bit backward. It begins with a question: what is a\ntechnical interview trying to prove? I've been on both sides of a few, and I'd argue it's the following: Can you solve programming problems? Can you communicate the solution and your opinions? (this is more important\nthan what your solution and opinions are). Do you know and care about what's happening in technology? Today I'll break down these three questions, and how I might suggest\nshowing a decision-maker that the answer to each is 'yes'. Can you solve programming problems? You may be aware of a blog post titled 'Why The New Guy Can't Code'. I didn't write\nthat post, but I think it suggests a truth: lots of people who want to be\nprogrammers struggle with basic programming problems. This is doubly true in high-stakes situations, like an interview. It's\nhard to be clever when sitting next to a person you barely know. I wish this weren't a part of the hiring\nexperience, but it is. Not every company has the ability to assign a take-home project, and that practice has its own biases. So in the meantime, it's worth\ntrying to figure out how to handle solving a live problem. My solution? Exercism.io . Exercism is an open-source project designed to help people get better at\nalgorithms. You sign up for an account, install a CLI, choose a language, pull\nan exercise, solve it, and push it up to the site. Some stop here, but I think that's just when things get interesting. When I\npush a solution to Exercism, I like to browse other people's solutions. I look\nfor solutions that have many comments or likes. Reading two or three remarkable solutions will teach you something. At this point I often refactor my solution, building muscle memory as I write better and better code. To make this practical, heading into a technical interview you should be able to infer which language\na technical challenge you'd face would employ. Let's say it's Python. If so, take a good look at the Exercism Python track . The question is likely to be very similar to one of those exercises, and require the same algorithmic and problem-solving skills. Whatever tool you choose, go get some deliberate practice writing algorithms.\nYou just have to do a lot of them. There are a few dozen programming puzzles\nyou're likely to see during an interview. Someone who has tried many or\nall of them would have an advantage. Can you communicate? Communication is critical at a consultancy like Hashrocket. But it's just as\nimportant for any programmer. I feel that the technical differences between\none junior developer and another tend to iron out after a couple of years; communication skills help you keep advancing after that. The way I stay sharp as a communicator is whiteboarding, and a practice I share\nwith my mentees is simple: go to the whiteboard early and often. Go to the\nwhiteboard before you need to go to the whiteboard. Apart from the truth that a\npicture is worth a thousand words, drawing your programming idea on a wall is a\ngreat way to move forward when you don't know what to do. Bigger picture, knowing how to draw a flow chart, what the symbol for a\ndatabase looks like (stack of pancakes), or how a nested React tree maps to a hasty drawing of a\nwebpage, helps us all share a visual language. Trying to draw a Redux state\nupdate will show you the parts that you don't understand. There are a lot of ways to practice communication, such as pair programming,\ngiving a lightning talk at a Meetup, or just talking. For me,\nwhiteboarding is a fantastic way to strengthen this skill. And often in an\ninterview, you're asked to whiteboard anyway. It's not scary if you do it\noften. Do you know what's happening in technology? Stepping into the programming field, especially if you're a JavaScript\nprogrammer, can feel like trying to jump on a log floating quickly down a river. There's\nso much to know, and it seems like everybody else is so far ahead that\nyou'll never catch up. How can you catch up? I advocate a passive approach: whatever watercooler you\nfrequent (Twitter, Medium, RSS, Reddit, email mailing lists, etc.), find the\naggregators. These are people who read everything--  blog posts, release notes, conference announcements, Twitter fights-- and distill it down into a human-sized product. I subscribe to weekly email newsletters for React,\nJavaScript, Ruby, and Postgres. Each week, I scan the headlines and one or two\narticles. You build up a picture of the scene pretty quickly. I'm trying to prevent a blind spot, like\ngoing into an Elixir interview and not knowing that Elixir 1.8 and 1.9 have\nbeen released. Is that a dealbreaker in a technical interview? Probably not.\nBut why risk it? Companies that are a few years behind on their tech stack\nstill want to get to the latest stack eventually, and they want to believe you\ncan help get them there. Conclusion This isn't a comprehensive list; I'm sure there is a lot more you could do. Having a strong technical blog, a project you\nare proud of, or some unique background, helps. A lot of these traits get\nyou into a technical interview in the first place. But once you're there, I\nwant to see that you can write code, talk to me, and are living somewhat in the\npresent day on the technology stack you want to get paid to use. To get\nthere, practice algorithms and communication, and read technical news. I hope this helps somebody in their first or second round of technical interviews. Please\nlet me know on Twitter if there's something we've missed, or how you approach a\ntechnical interview. Also, if you’re looking to practice coding, chat with other devs and hiring companies, and connect with the pulse of the Ruby and React communities, come join us on October 3-4 at Ancient City Ruby in Jax Beach! I hope to see you there. Photo by Thought Catalog on Unsplash Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-08-06"},
{"website": "Hash-Rocket", "title": "How I Built My Own Heroku for Phoenix Apps: Part 1", "author": ["\nMicah Cooper\n\n"], "link": "https://hashrocket.com/blog/posts/how-i-built-my-own-heroku-for-phoenix-apps-part-1", "abstract": "Elixir Phoenix How I Built My Own Heroku for Phoenix Apps: Part 1 by\nMicah Cooper\n\non\nAugust 23, 2016 Walk through why and how I created Gatling - A deployment tool for Phoenix applications. For the past few months I've been programming in Elixir. From my experience, it offers al the goodies of a functional language with the same amount of joy offered by Ruby's pretty syntax and developer-centric design. I know functional languages have been around for a long time, but the thing that peaked my interest about Elixir was Phoenix. You see, I'm a web developer; I love building web apps. I enjoy the whole process - from the planning and wireframing, to the design, to the HTML, CSS,and Javascript (Holla at your Ember), and finally to the server. As I came across other languages, I typically had two thoughts: My first thought is that they were ugly. I hear once you really start learning and getting comfortable with a language, you don't notice all the {} s and () s but at first, they're ugly. Secondly, when I explored other languages, I asked myself - will this enable me to create web apps in a similar development speed and joy as Rails? I never felt I could answer yes to that question until I discovered Phoenix . After exploring what Phoenix had to offer I decided to give it a fair shake. I started developing my personal sites with a Phoenix back end and Ember front end. So far, it's been a great experience. I enjoy having the power to create flexible web apps that are backed by an API that is small, fast, and maintainable. Plus it's just really fun to write Elixir. As I moved further along in creating a few Phoenix apps there was a thought that kept growing in the back of my mind; How am I going to deploy these things? Deploying my Phoenix Apps The first time I deployed a phoenix app was completely manual. I took the following steps: Setup a server on Digital Ocean Install Nginx, Elixir, and set up a Git repository Push my app up and run mix phoenix.server Configure Nginx to proxy to my app to port 4000 Create an Init.d script  that would run mix phoenix.server if my server restarted. And that was about it. But this felt inconvenient coming from Rails where I could just git push to Heroku and it would do everything for me. I initially considered using Heroku but I had a problem with the way Heroku orchestrates a deploy. Here is an oversimplified process for a Heroku/Rails deploy: Receive a git push of your Rails application Install your gems Precompile your assets Create a slug of your application Launch that slug on a new dyno Proxy web traffic to the new dyno This all seems fine and dandy until those last two steps- Launching your application on different instance, and changing the proxy to direct there doesn't seem to embrace the power of the Erlang VM. Especially when there are tools out there like Exrm that can perform an upgrade of your running application with no downtime (a.k.a hot upgrade). My desire for a Heroku for Phoenix app lead me to create Gatling which works like the following: Create your own Ubuntu 16.04 server on Digital Ocean or Aws Install Gatling via mix archive.install Install Nginx Use Gatling to create an empty Git repository with a post-update git hook Now, you can git push to this new remote repository. After running the initial deploy command on that application, all subsequent git pushes will automatically create an Exrm release of your app, and perform a hot upgrade that feels just like Heroku, but with more of an Elixir philosophy. How It's Made Let's break down what I wanted Gatling to do and how I implemented it in Elixir. Mix Tasks, File Manipulation and System Commands When I began working, I performed a manual deploy of a Phoenix app using Exrm and recorded all my steps: (most, if not all, of these steps can also be found in the Exrm and Phoenix documentation: $ cd path/to/project #project has exrm as a dependency $ MIX_ENV = prod mix deps.get #download dependencies $ MIX_ENV = prod mix compile #compile (in prod you have to manually compile) $ MIX_ENV = prod mix phoenix.digest $ MIX_ENV = prod mix ecto setup $ MIX_ENV = prod mix release $ cp path/to/release path/to/deployment/dir $ tar -xf path/to/deployment/dir # Create an nginx.conf file  in /etc/nginx/sites-available/<project> ln -s etc/nginx/sites-enabled/<project> etc/nginx/sites-available/<project> # Create an init.d file so we can use the generated shell commands from exrm as a service. Put it in /etc/init.d/<project> $ sudo update-rc.d <project> defaults $ sudo service nginx reload $ sudo service <project> start Once I had the gist of what I wanted to happen, I needed to create the following mix tasks. mix gatling.load <project> Create a Git repository with a post-update git hook so every time I do a git push, it will run a mix task to upgrade the app to the newest version. mix gatling.receive <project> The mix task called by the git hook. It looks at the project and sees if it's currently deployed. If it is, run $ mix gatling.upgrade <project> . Otherwise don't do anything. mix gatling.deploy <project> This task is run manually after the first Git push. It performs all the steps outlined above. mix gatling.upgrade <project> Create an \"upgrade\" release of your project and perform a hot upgrade on the currently running application. As you can see in the recored steps, we're really only running a few system commands and moving around a couple files. Much of the heavy lifting is done by mix tasks that already exist within Phoenix and Exrm. What I wanted was a way to execute those mix tasks and stream the output back the to the shell, which would then be streamed to the client that executed the git push. That way the developer can see the progress of the deploy. To accomplish this, I created a small wrapper around Elixir's System.cmd/2 function: defmodule Gatling . Bash do def log ( message ) do Mix . Shell . IO . info ( message ) end def bash ( command , args ) do bash ( command , args , []) end def bash ( command , args , opts ) do options = [ stderr_to_stdout: true , into: IO . stream ( :stdio , :line )] |> Keyword . merge ( opts ) message = if opts [ :cd ] do [ \" $\" , command | args ] ++ [ \" ( #{ opts [ :cd ] } )\" ] else [ \" $\" , command | args ] end log Enum . join ( message , \" \" ) System . cmd ( command , args , options ) end end This works the same as System.cmd , but by default, it streams the output to :stdio and logs the passed in command right before it's run. This provides a little more transparency to Gatling as it performs a deploy. Now any time we call a system command (which is quite often) we'll use this Gatling.Bash.bash/3 instead. mix gating.load <project> This task looks for a project in the place where your apps are deployed. If it's not there, it creates a directory and adds a post-update hook that looks like this: #!/bin/sh unset GIT_DIR exec sudo mix gatling.receive <project> mix gatling.deploy <project> This task is the meat and potatoes of Gatling and it's where most of the work gets done. 1 defmodule Mix . Tasks . Gatling . Deploy do 2 use Mix . Task 3 import Gatling . Bash 4 5 def run ([ project ]), do : deploy ( project ) 6 7 def deploy ( project ) do 8 Gatling . env ( project , port: :find ) 9 |> mix_deps_get 10 |> mix_compile 11 |> mix_digest 12 |> mix_release 13 |> make_deploy_dir 14 |> copy_release_to_deploy 15 |> expand_release 16 |> install_nginx_site 17 |> install_init_script 18 |> mix_ecto_setup 19 |> start_service 20 end 21 end Above, the run function on line:5 is executed when your run $ mix gatling.deploy . The full file can be found here Let's explain what's happening here. Staring with line:8 , Gatling.env(project) returns a map of all the information needed to execute a deploy. This includes, paths, file templates, and an available port on the system. This env map must be returned by every function and, in turn, passed to the next. This may seem similar to how Plug works by passing a Plug.conn through each plug. Now lets look at mix_deps_get/1 def mix_deps_get ( env ) do bash ( \" mix\" , ~w[deps.get] , cd: env . build_dir ) env end We use the parts of env that we need, do our work, and make sure to return it when we're done. Every function in the pipeline behaves this way. Advantages of this approach Using a pipeline allows a lot of flexibility around deploying and testing Gatling. We can: Pass in a Mock env and and customize our test assertions Ensure our test suite doesn't muck up our development system by emulating a production ubuntu server within Gatling's test directory. Allow for a slew of callbacks before and after each function. In future versions of Gatling, the client will be able to write their own custom steps in between each default step. mix gatling.upgrade <project> The upgrade tasks follows the same pattern. And we even import many of the functions from the deploy task: 1 defmodule Mix . Tasks . Gatling . Upgrade do 2 use Mix . Task 3 4 import Gatling . Bash 5 import Mix . Tasks . Gatling . Deploy 6 7 def run ([ project ]) do 8 upgrade ( project ) 9 end 10 11 def upgrade ( project ) do 12 Gatling . env ( project ) 13 |> mix_deps_get () 14 |> mix_compile () 15 |> mix_digest () 16 |> mix_release () 17 |> make_upgrade_dir () 18 |> copy_release_to_upgrade () 19 |> upgrade_service () 20 end 21 end mix gatling.receive <project> The receive task is called in the git post-update hook. This is a very small task with nothing but the run function. def run ([ project ]) do if File . exists? deploy_dir ( project ) do Mix . Tasks . Gatling . Upgrade . upgrade ( project ) end end We are looking to see if the project has already been deployed. If it has, call the upgrade task.\nI've opted not to automatically start the deploy task on the initial git push since you may need to do some additional setup (like adding your secrets file) before the initial deploy. Conclusion So that's all of Gatling as it currently stands. I'm testing it now in production environments to suss out needs that arise from real world deployment situations. I'm excited to see if and how people find this useful for deployment. Future plans for Gatling are to use the EXRM's successor called Distillery for creating the releases and to add callbacks to each step in the deployment process. I think after these two additions, Gatling should be able to handle a vast array of deployment strategies and offer Heroku-like features for an even lower cost. Look for farther posts on this topic as Gatling evolves to handle more use cases. Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-08-23"},
{"website": "Hash-Rocket", "title": "10 Vim Commands for a Better Workflow", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/10-vim-commands-for-a-better-workflow", "abstract": "Vim 10 Vim Commands for a Better Workflow by\nJake Worth\n\non\nMarch  5, 2015 Vim is an essential tool at Hashrocket. It lets us share a common workspace with anybody, onsite or remote. Despite its unassuming veneer, Vim is a very powerful text editor. Small investments in learning slowly compound into a fast and joyful programming experience. Here are ten Vim commands worth exploring. The Setup To follow along, you'll need Vim installed. Head over to the Vim download page for more information. Most Unix-based systems come with Vim; check to see if you have it by running: vim -u NONE This loads Vim without any plugins or custom settings. We don't need them, because we'll be looking at standard Vim commands only. Embrace the minimalism! :file One useful command is :file . This sends the file name, cursor position, and file status to the Vim status bar: \"vim_blog_post.md\" 200 lines --0%-- Aliases include CTRL-G , :f , and :fi . :buffers Mastering buffers is an essential part of a good Vim workflow. The :buffers command is your starting point. As I type this, mine looks like this: :buffers\n  1 %a + \"vim_blog_post.md\"             line 40\n  5 #h   \"show_tell.txt\"                line 5 Aliases include :files and :ls . CTRL-^ One area Vim seems to lag behind more fully-featured editors is file navigation. The command CTRL-^ is one of many tools that can help. CTRL-^ switches to the alternate file in your buffer. This is useful when you want to toggle between two related files, such as a unit test and its related model. :saveas If you want to save your file as a new file in Vim, you might try writing it with :w {filename} . This sort of works, but your buffer is still the original file. You'll need to open the new file to proceed. There is a better way: the :saveas command. :saveas {file} saves the current buffer as {file} and sets the current buffer to {file} . It's the 'save as' you've always wanted. Add a bang ( ! ) to overwrite an existing file. Aliased as :sav . :wall The day is ending and you want to save a work-in-progress commit. Wouldn't it be nice to write every buffer with one command? The :wall ('write all') command writes all changed buffers. The output will only show your current buffer as having saved, but it writes them all. Add a bang ( ! ) to also execute on readonly files. Aliases include :wa and :wa! . :ball The :ball ('buffer all') command opens a window for each buffer. This is helpful if you are switching back and forth between a handful of files and need to see them all at once. If you prefer tabs, try :tab ball . Aliases include :ba , :sba , and :sball . ZZ ZZ and ZQ are the same as :wq and :q! , but require fewer keystrokes. Again, small improvements that compound over time. :.! The :! command is very useful in Vim, allowing you to shell out to programs like psql or rake. A variant of this is :.! , which forwards the output of your external command to the current window and line (hence the . ). Here are a few examples: # Paste my current directory\n:.! pwd\n/Users/jwworth/Documents\n\n# Paste a calendar\n:.! cal \n   February 2015\nSu Mo Tu We Th Fr Sa\n 1  2  3  4  5  6  7\n 8  9 10 11 12 13 14\n15 16 17 18 19 20 21\n22 23 24 25 26 27 28\n\n# Share what else I'm working on\n:.! ls | grep blog\npostgres_blog_post.md\nvim_blog_post.md The practical applications are limited only by your imagination. K Another awesome command is K . Type this while the cursor is over any Unix-based command, and you will be taken straight to that command's manual page! g CTRL-g Once you get past the austerity, Vim is a great platform for writing prose. One tool you'll need when writing prose is a word count. Vim has many possible solutions; perhaps the simplest is g CTRL-g . Here's the output when I ran that command on this blog post: Col 1 of 0; Line 111 of 155; Word 748 of 993; Byte 4218 of 5501 Tons of useful information. Step aside, Microsoft Word! Conclusion As you can see, Vim is powerful without any customization. Explore its depths, and Vim will repay you with a faster and more joyful programming experience. Was this post helpful? Share it with others. Tweet Share Post", "date": "2015-03-05"},
{"website": "Hash-Rocket", "title": "Planning Poker: Speed Mode", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/planning-poker-speed-mode", "abstract": "Process Planning Poker: Speed Mode by\nJake Worth\n\non\nApril  9, 2019 Estimating stories is an important part of development. It helps\ndevelopers imagine the work ahead and stakeholders understand what can\nbe accomplished. On a recent project, my pair and I developed a technique for\nestimating stories that I’d like to share in this post. Problem Estimating is hard. Perhaps some people in the story grooming meeting were\ninvolved in an earlier estimation, and sense a conflict of interest. Others\nhave more context than everybody else, and their estimation is likely accurate.\nSome might be pessimistic about a feature they don't understand, while others\nmaybe too optimistic for the same reason. Add to this the dynamics of a group. New team members hesitate to speak up\nabout a story because they don't want to look, well, new. Seniors hesitate too\nbecause they don't want to influence everybody else. The result? Whoever speaks up wins, and pointing becomes like throwing darts.\nNobody feels like they own the result. Planning Poker: Speed Mode This technique is a variant of planning poker; read\nup on that technique if you're\nnot familiar. To summarize, planning poker is a gamified technique for estimating stories using playing cards, designed to find consensus by reducing the cognitive bias of anchoring . The benefit of planning poker is that it can alleviate the problems described\nabove. But like any ceremony, it must be practiced. For a small team\nmoving quickly, that means keeping things simple. A faster version of planning poker my pair and I used recently is inspired by the children's\ngame 'Rock Paper Scissors'. Here's how it works: For each story, we read through the description, acceptance criteria,\nand comments. When we both feel like we understand the story, we move into the estimation. We count to three and throw our hands out, showing a number between zero and\neight. (Substitute this for the minimum and maximum points values in your\nestimation process). The numbers correspond to the number of points we each think the story is worth. We do it together so we don't influence each other. If we agree, we assign the points and move on. If we disagree, the person with a higher pointing explains their assessment. What complexity do they see that isn't obvious? Are they being\nrealistic, or cautious? When consensus is reached, the final point is recorded. In a group larger than two, you might leave explanation duties to outlier\nratings. Did one person throw one point and most of the group throw four or\nfive? How are they arriving at one? Let's have a debate. Did\none person throw an eight and everyone else threw one or two? What are we\nmissing? Are they perhaps lacking context, or overestimating complexity? \"But I Don't Want to Talk!\" The price of this technique is the discomfort of having to vote on every story,\nand sometimes defend yourself. Why is that worth it? Because the final point value is\nimportant. Everybody's voice matters, and no one person should dominate the discussion.\nIt's in the spirit of the notion that \"there are no dumb questions\"; if you\ndon't understand something you aren't alone. Using a Rock Paper\nScissors-style technique forces marginalized opinions into the spotlight. We can also use this process to inform our assignment of the work. Perhaps a person who ranked a\nstory at low value should be put on the story as an owner; they'll bring\nexpertise or confidence that will make the story move quickly. Perhaps the person who ranked\na story at a high value should also be put on the story. They're seeing edge\ncases others don't see, or they might learn by working in an unfamiliar domain.\nGetting those opinions out there in real-time helps us divvy up the work. Conclusion Estimating stories is an important part of preparing for development. It helps\ndevelopers imagine the work ahead, and helps stakeholders understand what can\nbe accomplished. I hope this technique is useful for other teams trying to estimate their work and move quickly. How do you estimate stories? Have you ever used a modified version of\nplanning poker? Send us a note on Twitter . Photo by Marcus Wallis on Unsplash Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-04-09"},
{"website": "Hash-Rocket", "title": "Introducing Today I Learned", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/introducing-today-i-learned", "abstract": "Hashrocket Projects Introducing Today I Learned by\nJake Worth\n\non\nJune 11, 2015 I'm proud to introduce Today I Learned , a concise mini-blog run by all of us here at Hashrocket. Software is an ever-changing field, and relevancy demands constant learning. We all learn every day, and Today I Learned allows us to continuously document and reflect on that journey as it happens. We use the Hashrocket Blog to share announcements, new products, and software development techniques, but Today I Learned fills a gap for us: many Hashrocket Blog posts are lengthy, deliberate explorations of our field, but sometimes it's best to just post what you've learned the moment it happens. Today I Learned is different because it restricts posts to 200 words. It was designed to be readable and updated daily. Each post is personal– sometimes a tip is bleeding-edge stuff, while other times it's something basic that's just new to that person– but every post is fundamentally functional and directly related to our work. This was my apprentice project, and I paired with Rocketeers from across the company. I learned a lot in that process and am grateful to everybody who lent stories, advice, and encouragement, especially my mentors in the Chicago office. Special thanks go to Cameron Daigle, who quickly styled an intuitive, responsive, and bold user experience. Check it out! Was this post helpful? Share it with others. Tweet Share Post", "date": "2015-06-11"},
{"website": "Hash-Rocket", "title": "Open-Sourcing Today I Learned", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/open-sourcing-today-i-learned", "abstract": "Hashrocket Projects Open-Sourcing Today I Learned by\nJake Worth\n\non\nFebruary 18, 2016 Today I Learned is an open-source project. Before we introduced Today I Learned , it was a small Rails application with a handful of posts. Few people within the company had even seen it. Here is a screenshot of the site, pre-Hashrocket makeover: Today, this application features over 500 posts from Rocketeers across the company. We have written about Vim, design, Rails, Ruby, testing, Bash, Git, HTML/CSS, JavaScript, Clojure, SQL, mobile, DevOps, Go, Elixir, and Ember.js. It's been awesome watching this project grow. Learning and communicating every day is a part of the Hashrocket culture, and this site helps us do that. We've decided to open-source the codebase. In this post, we'll cover the rationale behind this decision, as well as the steps we took to prepare. I hope it helps people who are considering open-sourcing their own work. Rationale Open-sourcing a private project is a decision that deserves some serious thought. We've gotten a lot of requests to publish this code, so it was on our mind from the beginning. The application has even been impressively reverse-engineered in the meantime. The idea was always intriguing because open-source is a core idea at Hashrocket. Our tools are mostly open-source, and we try to give back to that community. A second argument was that some of the features of TIL are pretty nice— it is well-designed, includes Slack and Twitter integration, has an RSS feed, and features a very simple, useful post creation page. We wanted to share the implementation of those features. That somebody else might contribute, including aspiring Rocketeers and junior developers trying to get into open source, won the day. If somebody used this repo as inspiration to build something new, that would be awesome. If that's your intention, fire away! Step 1: Create Pivotal Chore Because we used Pivotal Tracker to build this site, building a Pivotal chore to track the progress toward public release was our first step. Here is our TIL Pivotal project: https://www.pivotaltracker.com/n/projects/1299990 Having something to attach commits to increases transparency. By adding #[<Git SHA>] to each commit message, combined with a Github integration hook, all our progress could be tracked in the Pivotal story. Step 2: Generalize Documentation The TIL README was targeted at Rocketeers; generalizing it included: Adding generalized setup instructions and language Moving as many Hashrocket-specific variables into environmental variables as possible Adding contributing guidelines Adding a license Adding a changelog Tagging v1.0.0 via Semantic Versioning Step 3: Cross Link Synergy Alert! We wanted people to be able to find the repository from the site, so we added a link in the footer. We wanted people to be able to find the site from the repository, so we added a link in the Github repo. Step 4: Scan the Repo As a security precaution, we did a general audit of the site, upgraded to the latest versions of Rails and Ruby, and ran Brakeman against it. We also scanned the code for usernames, passwords, and API keys, removing a few files that were no longer relevant. This is pretty simple if you think about security from the first commit. If you do encounter a file in your repo you shouldn't have checked in, change your passwords and keys immediately, then follow this guide: Github Help - Remove Sensitive Data Step 5: Add Metrics An open-source project should have transparent metrics, so we added Circle CI, SimpleCov, and Code Climate to help evaluate our builds. Step 6: Release You are here. After completing all the other steps, we tagged the last commit and clicked 'Make this repository public' on the Github repo's settings page. Conclusion Today we covered why we open-sourced TIL and the steps we took to get there. Thank you to everybody who helped build this application and its content. Thank you to our readers and retweeters. I hope people find the code interesting, consider building their own TIL or contributing to ours, and continue learning and communicating every day. Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-02-18"},
{"website": "Hash-Rocket", "title": "WebAssembly: The Next Language of the Web", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/webassembly-the-next-language-of-the-web", "abstract": "WebAssembly: The Next Language of the Web by\nJake Worth\n\non\nJanuary 30, 2020 At Hashrocket's Winter Miniconf 2020, I presented a summary of my personal one-month audit of WebAssembly. Here's what I learned. A New Web Consider, if you will, the web. The web is the largest open technology\nplatform in existence. And what is the language of the web? JavaScript. Okay, but what is JavaScript? JavaScript is: A high-level language JIT (just-in-time) compiled Object-oriented, with prototypical inheritance Curly-bracketed, dynamically typed, has first-class functions, etc. Is JavaScript fast? It is today. However, JavaScript wasn't always fast. Is\nJavaScript a good language? That depends on who you ask. Even people who love it (👋) will agree it's far from perfect. Is JavaScript your favorite language? If so, would\nit be your favorite language if it wasn't so foundational to the web? Now picture another web. A web where\nyou can write in your favorite languages, such as Elixir, Ruby, Python, and lower-level languages like C/C++, Rust, Go. You'd write code once in your language of choice, and share it across server and client. You even could even write code in\nthe same language that your browser is written in. The code would run on the client with near-native\nperformance. Some people consider this the Holy Grail of the Web. And if you believe the hype, it is already here, in the form of a language called WebAssembly. What is WebAssembly? WebAssembly is a way of taking code written in programming languages other than\nJavaScript and running that code in the browser. Here's a quote from the\nofficial docs: \"WebAssembly, or wasm, is the most significant new technology to come to the\nweb platform in a decade.\"\n--Mozilla.org The 2019 State of JS Survey polled almost 28,000\nJavaScript developers, and reported that, of that group, only 7.1% have used\nWebAssembly. Most respondents know what it is but haven't used it. This\ntool is new enough that a group of highly engaged front-end devs are just now becoming aware of it. I think WebAssembly is at least two things: A new type of code A standard Let's explore each. A New Type of Code WebAssembly is a low-level Assembly-like language with a compact binary format.\nIt provides other languages with a compilation target so they can run on the web.\nYou may be familiar with Assembly: it's a low-level programming language where the instructions\nclosely match the machine code. Some call Assembly \"symbolic machine code\", so think of this new tool as bringing that machine code into the browser. It's a mashup of technologies. Most people will use WebAssembly as a compiler target, taking their low-level code\nand translating it into WebAssembly using a source-to-source compiler, or\ntranspiler. They'll then load their generated WebAssembly module into a\nJavaScript app, running it side-by-side their JavaScript code at near-native speeds. So where did this idea come from? It has a precursor called asm.js. Asm.js is a strict subset of JavaScript, and it works like WebAssembly; it's also a compiler target that can run in the browser. Asm.js is supported by C/C++, Rust, Lua, Perl, Python, frameworks, and game engines. A Standard WebAssembly is now a W3C recommendation, as of\nDecember 2019. It is the fourth language to run natively in browsers,\nalongside HTML, CSS, and JavaScript. All\nthe major browser teams (Mozilla, Microsoft, Google, and Apple) are\nparticipating in the standardization process. That's why everyone is treating it like such a big deal; this shared\nownership suggests WebAssembly might actually cross the chasm of adoption. Furthermore, being a standard means that, like JS, CSS, or HTML, WebAssembly is a foundation upon which we can build. There isn't a Benevolent-Dictator-for-Life or core team determining its\ndirection. Instead, a committee of practitioners, academics, and\nbusinesspeople have their hands in the construction process. How Might WebAssembly Change the Web? I think this tool is going to affect our workforce as developers, the\nperformance of our apps, and our experience as web consumers. WebAssembly has the potential to expand the web to developers of more\nbackgrounds and interests. There is a dogma in the web that you must learn\nJavaScript, and that may become less true. Soon, low-level languages like Go, Rust, and C/C++ will begin to target\nbrowsers by directly compiling to WebAssembly. At least 40 languages are already doing this right now. That will change who is able to, and choosing to, write code for the browser. When it comes to performance, WebAssembly allows for close to the performance of native\napplications on the web. Even after all of the work people have done to improve JavaScript's performance, WebAssembly has the potential to be orders of magnitude faster than\nJavaScript. Our experience as consumers will also change. Imagine gaming in the browser\nwith the performance of a console or desktop; that is coming. Performance gains\nare likely to translate into more content as online games become bigger, faster, and\nbetter. Yes, WebAssembly lacks some core features such as DOM manipulation and garbage\ncollection that people expect of a modern language. However, most of those missing features can be found in the WebAssembly development roadmap. Conclusion I believe that WebAssembly is going to dramatically change the web. I'm excited by the potential it represents. What could you build with it? Let me know on Twitter . In closing, here's a list of resources I collected while learning about WebAssembly: A cartoon intro to WebAssembly Awesome WebAssembly Languages Developer's Guide Emscripten & asm.js Introduction to WebAssembly Made with WebAssembly Oh the things you'll compile Roadmap Rust: Wasm Wasm by Example WebAssembly Weekly WebAssembly as a Platform for Abstraction WebAssembly becomes a W3C Recommendation WebAssembly format WebAssembly hasn't grabbed JavaScript developers WebAssembly docs WebAssembly: A game changer for the Web | Mozilla What are disadvantages of WebAssembly compared to current HTML/JS? World of WebAssembly! Official site Thanks for reading. Photo by Umberto on Unsplash Was this post helpful? Share it with others. Tweet Share Post", "date": "2020-01-30"},
{"website": "Hash-Rocket", "title": "A Typical Day at Hashrocket", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/a-typical-day-at-hashrocket", "abstract": "A Typical Day at Hashrocket by\nJake Worth\n\non\nMay 18, 2017 What is a typical day like for you at Hashrocket? This is a question people ask. If you've never worked with a\nconsultancy, what happens on the inside can be mysterious. We're\nconsultants... does that mean we write code ? We're developers...\ndoes that mean we only take projects in languages and frameworks we know very\nwell? And perhaps most importantly: do we have a ping-pong table? Today I aim to answer some of these questions the best way I can, by describing my\ntypical day as a developer at Hashrocket in Chicago. I hope this gives insight into consulting,\nand how one person can find their place in the industry. My typical day flows through team standup, client standups, development,\nlunch, development again, and individual contributions, not always in that\norder. Team & Client Standups At Hashrocket, a vital part of our process is standing up together every day,\nsomething I explored in a previous\npost . Our internal standup practice is one of my favorite things about Hashrocket. It\nhelps us understand goals, coordinate effort on problem areas, share problems\nand improvements, and coalesce as a team. We have several remote team members, making this practice even more\nimportant. On our client standups, communication is everything. We work to quickly help our clients develop their ideas,\nthink about priorities, and explore hard technical decisions in the path to a viable product. Development After standups, we get to code. During this part of the day, it's me and often a pair (check out great blog posts on that subject here and here ) working through technical challenges\nto deliver value to our clients. We put on some music (or not), sit or stand at\nour desks, and attack the fun and bedeviling work of programming. In my time at Hashrocket, I've written Ruby, PHP, Go, Elixir, and a lot\nof JavaScript. One of the perks of consulting is that we see tons of languages,\nframeworks, databases, code styles, and design decisions. It's an endless walk\nthrough the many planes of web development. How do I react when thrown into a project with a language or framework I'm\nstill learning? We try to prevent that, something I attempted\nto explain in this Quora\npost .\nBut of course, it happens often. That's one of the joys of\ndeveloping at any job: how can I achieve expertise as fast as\npossible with this tool? What must I know? Are\nthere any rites of passage that I can skip for the sake of expediency? When I'm lacking specific familiarity with a tool, I rely on general expertise\nin web development, problem-solving, and accessible coworkers. I try to stay heads-down for as long as I can, breaking up work by talking to my coworkers about tools, design, client interactions,\nhiring, and current events. Because we follow something resembling an Agile workflow, we have assigned stories\nthat correspond to features or issues with a project. We approach these\none by one, in an order prioritized by the client, requesting clarification,\nexploring edge cases, and finally, writing tests and implementation. Lunch Most of us eat lunch as a team in our office, with groceries provided by\nHashrocket. Conversations continue, mixed with board\ngames, walks around the neighborhood, Halo, and reading. Visiting\nclients and hiring candidates can participate in the experience. At the end of the hour, we transition quickly back to work, since nobody has left the building. Development (Part 2) After lunch, it's back to development. The afternoon block of work is longer\nthan the morning block, and is where I feel the most productive. Because we\nfollow an Agile workflow, we try to keep our commits small and atomic and deliver often to a staging server for client acceptance. The afternoon is\na great time to kick off a round of deployments and feature delivery. These blocks of time are free of mandatory meetings or distractions. Meetings\nand distractions do happen, but they are optional and always take a back seat to work. When 5 PM comes, it's time to stop. This is not a marketing ploy, but an\ningrained part of our culture. We try to keep people rested to prevent burnout and deliver the best possible work. Individual Contributions Hashrocket has a flat hierarchy; we are all peers\nand answer directly to the CEO. This means that if you're interested in taking\non a responsibility to help the business, you just start doing it. There\nare a number of important internal projects that require extra work from\neverybody. Also, a consultancy's product is, in some way, the consultants, and\nwe all try in our own ways to develop our personal and company standing in the community during downtime throughout the day. Here are a few of the ways we make individual contributions: Writing blog posts Responding to job applicants Maintaining open source projects Maintaining our various web properties including https://hashrocket.com Building side projects Learning Organizing Meetups ( Vim Chicago , Chicago Elixir , and React Native\nJax , to name a few) Writing TILs Teaching Helping run the office and communicating with the rest of our team Fridays Fridays at Hashrocket are special, because we spend the afternoon on\n'Open Source Time'. This is a company-sponsored opportunity to dive more deeply\ninto the individual contributions listed above, and show the rest of our team\nthe things we've been working on. It's something we cherish, and I hope to cover it more deeply in a future post. Conclusion That's it! Eight hours talking, listening, thinking, and typing. It can definitely push your brain to the limit, but goes by quickly. I hope this gives some\ninsight into Hashrocket, and what it's like to work with us. Photo Credit: Jonathan Chen, https://unsplash.com/photos/0G8M3LVT5Ds . Accessed 12 May 2017. Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-05-18"},
{"website": "Hash-Rocket", "title": "Today I Learned in Phoenix", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/today-i-learned-in-phoenix", "abstract": "Elixir Phoenix Hashrocket Projects Today I Learned in Phoenix by\nJake Worth\n\non\nSeptember  5, 2017 The Phoenix port of Today I Learned, Tilex , is live. Today I Learned began as a Ruby on Rails application.\nWe open-sourced the code after launch and have maintained it for over two\nyears. Our mission: Today I Learned is an open-source project by Hashrocket that exists to\ncatalogue the sharing & accumulation of knowledge as it happens day-to-day.\nPosts have a 200-word limit, and posting is open to any Rocketeer as well as\nselected friends of Hashrocket. We hope you enjoy learning along with us. Today, we are moving forward with an Elixir/Phoenix port of the project.\nHere's a little more about the project, focusing on the work, technology, some\nobservations, the people behind it, and the future. Work This project began over two years ago as my\nHashrocket apprentice project. Here's the launch\nannouncement from that time. We open-sourced the Ruby on Rails\napplication in February 2016. A handful of forks are in production,\ndocumented in the usage\nguide . Why port this application to Elixir? My curiosity in Elixir\ndeepened after reading Seven More Languages in Seven Weeks by Bruce Tate,\nFred Daoud, Jack Moffitt, and Ian Dees, which features Elixir.\nElixir's unique raison d'être – a Rubyist's quest for a language that sidestepped\nsome of the limitations of the Ruby language– resonated with me.\nMany on our team have grown to love Elixir, and we wanted a project that featured\nit. In October 2016 we made our initial commit to the port. The\nname, Tilex, came from the Elixir convention of adding 'ex' to everything. And\nso, Tilex ('TIL' + 'ex') was born. Technology We created our application using Phoenix 1.2 (since upgraded to 1.3) and set to work. For project management, we chose to use a Github\nproject .\nGithub Projects is a great choice for a simple project. A key\nadvantage over a tool like Trello or Pivotal Tracker is that it's already\nintegrated with your repository. For hosting we chose Heroku. Setup was not as simple as launching a Ruby on\nRails application, but was still pretty straightforward. We've been running our tests with Travis CI, including a Credo integration, and have used AppSignal for error monitoring. Observations Here are few of my personal observations as a member of this team. First, Elixir is a pleasure to write and maintain. The language mixes some of\nthe best ideas from Ruby with an outstanding foundation in Erlang. The combination of\nthe joy of Ruby idioms, functional programming, and features such as pattern matching,\nis truly unique. Also noteworthy: we didn't write as many tests for Tilex. I credit a mix of\nexperience and bravado earned by having written almost this entire application\nbefore. Precompiled languages catch a lot of issues in development,\ngiving us a sense of confidence when deciding whether to test. As noted by many others, Elixir's tooling is not as polished as Ruby's, but\nit's improving all the time. As we built the application, we got to watch it\nget better. This was a learning experience, and here are few posts I wrote along the\nway: Integration Testing Phoenix With\nWallaby Titled URL Slugs in Phoenix The Today I Learned elixir channel is itself loaded\nwith small discoveries we made during the process. People The following people committed to the Tilex release: Brian Dunn Chris Erin Cody Roberts Dorian Karter Jake Worth Josh Branchaud Lexin Gong Taylor Mock Vidal Ekechukwu Thanks as well to Cameron Daigle, who created the design. We also appreciate\nthe Rails app\ncontributors ; many\nof the features we recreated were built by them. Also, my thanks to\nHashrocket for the paid open-source time which made this possible. Most importantly, thank you to my coworkers, who have built this site\ninto something amazing. Today I Learned is nothing without the posts. I learn\nsomething from our feed every day. Future Here's how we plan to celebrate this release. First, more work! As of this writing, we have a handful of open\nissues on Github. Interested in contributing? Check out our contributing page for more information about getting started. I'd also like to do some benchmarking. Is there a performance or resource advantage to this\nplatform? How does our churn compare, and how easy will it be to maintain? I'm curious about these questions. As for the Rails application: can we maintain a codebase we don't use? For now,\nI plan to update the documentation of that project to indicate that it's not\nthe deployed version of Today I Learned, and then work on a plan for the\nfuture. Please enjoy the new Elixir/Phoenix version of Today I Learned, hit us up on Github or Twitter , and keep learning every day. Cover photo by Jonathan Bean on Unsplash.com Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-09-05"},
{"website": "Hash-Rocket", "title": "React Has Been Teaching You Invalid HTML!", "author": ["\nDorian Karter\n\n"], "link": "https://hashrocket.com/blog/posts/react-has-been-teaching-you-invalid-html", "abstract": "React React Has Been Teaching You Invalid HTML! by\nDorian Karter\n\non\nAugust 22, 2019 Modern JavaScript front-end libraries such as React, with their convenient JSX markup, have been teaching us invalid HTML syntax for the past few years. In this post I will explore what are void/non-void HTML elements and the consequences of using a self-closing non-void element in pure HTML. HTML5 has a class of elements called void elements . These include: area, base, br, col, embed, hr, img, input, link, meta, param, source, track, wbr You can self-close these elements (e.g. <hr /> ), meaning - no additional closing tag is necessary (and adding one is generally considered improper). But what about all the other elements? Can you self-close a span ? a div ? It seems logical and necessary in today’s JavaScript and CSS focused web development to self-close div s/ span s. We usually do that when we want a placeholder that would be later hydrated with content from Javascript. Another use case is for styling - a self-closing div can be sized and given a background image in CSS for example. Nevertheless this is not valid HTML and the consequences can be pretty dire! HTML parsers (such as your browser) are trying to be extra helpful in this case and interpret the tag as un-closed and nesting everything below it as children of the tag. So you write this: <!-- ... --> <div class= \"placeholder\" /> <input type= \"text\" /> <!-- ... --> And the browser interprets it like this: <!-- ... --> <div class= \"placeholder\" > <input type= \"text\" /> </div> <!-- ... --> In fact, the W3C’s official HTML validator will show 3 errors if you try to do that: At the very best, this will just produce unintended markup, maybe some improper styling or some time spent debugging, at the very worst it can lead to errors in production (for example when you replace the contents of placeholder using JavaScript, unknowingly removing all elements below the current “self-closed” element). By now, if you’ve done this you probably feel bad… but it’s not your fault. Front End frameworks (like React) have been spoiling us for years, fixing our HTML for us and, in the process, teaching us how to write some terribly invalid HTML. That’s not all, our beloved linter, ESLint, particularly the ESLint-plugin-React , has been auto-“correcting” our code too: This particular rule react/self-closing-comp is enabled by default on CodeSandbox.io and Airbnb's ESLint Config . Both very popular. Conclusion When writing HTML outside of front-end frameworks such as React, you should never self-close non-void HTML tags. Hopefully something changes - either in React and all the other \"convert-to-html\" markup in JavaScript frameworks, or on browsers and HTML parsers. Since I personally find self-closing elements useful as containers, I would much rather see browsers and parsers supporting self closing non-void elements, and see it included as part of the HTML spec, but until then, I'll be writing valid W3C HTML. Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-08-22"},
{"website": "Hash-Rocket", "title": "Interview: Jeremy Huffman, Dialyxir", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/interview-jeremy-huffman-dialyxir", "abstract": "Elixir Interview: Jeremy Huffman, Dialyxir by\nJake Worth\n\non\nApril 11, 2019 Dialyxir is an Elixir library that helps programmers use dialyzer , the Erlang static analysis tool. Dialyxir and dialyzer combine to identify type errors, dead code, and unnecessary tests. I recently emailed some questions to Jeremy Huffman ,\nmaintainer of the Dialyxir project. Here's our correspondence. For the people that don’t know you, could you tell us a bit about yourself? I've been working as a software developer for just over twenty years.\nProfessionally I have experience in popular enterprise industry languages such\nas C#, Java and C++ but I've always programmed for fun with side projects /\nopen source and in those contexts have preferred other languages including\nRuby, Elixir and Haskell. In 2017 I joined a startup using Elixir and Phoenix\nand am still thrilled to work in this language every day. I work in Charlotte,\nNC and live nearby with my wife and kids. How did Dialyxir come into existence? When I was first learning Elixir, the topic of Dialyzer came up on the Elixir\nmailing list and someone pasted a couple of lines of shell input they used to\ninvoke the Erlang dialyzer in their Elixir project. I decided to make a mix\ntask that would do the same thing and manage the configuration (such as apps to\ninclude in the PLT) in the project's mix file. I initially created it in the\nstyle of a glorified shell script that took configuration inputs from the mix\nproject file and then shelled out to the dialyzer CLI. I put it up on Github\nbut didn't really consider it complete enough for general usage and so I didn't\nannounce it, but a week later I got a pull request and the PRs never stopped\ncoming. I eventually did an overhaul / rewrite— back in 2016— but it is still\nthe case that most of the functionality has come from small changes submitted\nin pull requests. What connection do you feel with the Erlang community? What kind of dialogue\nis there between your project and that community? When I first learned Elixir (early 2013) there were not any Elixir books yet\nand while there were some good Elixir resources they were mostly introductory.\nI found it super helpful to read \"Learn You Some Erlang\" and learn Erlang well\nenough to read it comfortably. That book is where I first learned about\nDialyzer. Erlang has fantastic documentation which I've taken advantage of many\ntimes. To be honest though I haven't had much other interaction with the\nErlang community other than through a few bug tickets. In those cases I've\nfound them to be extremely helpful and welcoming. The Erlang core team is\nsuperb and very responsive, as are the community mailing lists. Why would an Elixir team consider adding Dialyxir to their project? At a minimum it is an interest in good hygiene— for similar reasons that you\nmight have a CI suite that runs a linter like Credo, compiles with warnings as\nerrors, or run a mix format --check-formatted , you might want to run dialyzer\non your code with every push. If you are doing TDD or at least writing lots of\ntests you may not see dialyzer discover very many bugs but it will identify\ncases where you have unreachable code such as a pattern that is covered by a\nprevious clause or you may have uncommon branches of code, particularly in\nerror handlers that will fail at runtime and aren't tested. Describe an Elixir project where Dialyxir would be a useful addition. How\ndoes the team get the most out of it? For libraries that you intend other people to use, I think it is important that\nyou use @spec annotations in the public API because not only do they provide\ngreat API documentation but that documentation will be checked statically; if\nyour actual implementation drifts from the @spec dialyzer will catch it. In\nthose cases your tests may all pass (since you've updated them) and the code\nmay work just fine, but dialyzer will identify the incorrect specs so that you\ncan ensure the documentation matches. For application projects it can be useful as well, and while it's good hygiene\nto have it in CI, it can also be a useful part of your personal development\nworkflow. Sometimes it is easier to debug an issue by looking at a dialyzer\nerror rather than a huge stack trace. Are there projects where Dialyxir might be unnecessary? What are the\ntradeoffs? Strictly speaking, it is never necessary— it is always optional. However, many\nmature projects in Elixir include specs for documentation, and when you do that\nthen some means of running dialyzer is a really essential practice. I think one\nof the biggest reasons people may not use it is that the warnings are hard to\nunderstand, even if you have experience with a statically typed language. The\nwork Andrew Summers did last year on pretty printing the terms for the dialyzer\nwarnings (which you can see in the RC) has gone a long way to address this, and\npeople who have only seen dialyzer output in the 0.5 version or below should\ntake another look. How does Dialyxir integrate with your personal coding workflow? I most frequently use test driven development but sometimes I do type-driven\ndevelopment where the types and specs come first, then the implementation and\ntests. Sometimes I will write the specs before the function implementations,\njust to help me think through how the pieces of the module will fit together.\nThen as I'm implementing the functions I can run dialyzer continuously to make\nsure the code I've written fits into the types. Dialyxir has a release candidate in production. What’s the timeline for\n1.0? What changes can we expect? We've had a RC out there for quite awhile so it's a great question. I think\nwe'll see 1.0 before the end of April. We had to run a pretty extended RC\nperiod last year with the new pretty printing changes. Andrew wrote a custom\nparser to take the Erlang-style terms that the dialyzer library returns (as\nstrings), and reformat it in the style that Elixir code is presented. For\nexample, Erlang would report an error related to an Elixir struct as something\nlike #{__struct__ => Elixir.MyApp.User ...} and we want to see that in Elixir\nas %User{...} . Finding all the variations in how that string output could\ncome from dialyzer really required exposure to lots of real code in the\nwild. That work settled in awhile ago but there are a few other changes we\ndecided to make part of the scope of the 1.0 release. Our goal is to take care\nof all known breaking changes in that release. If people want to get involved with Dialyxir, where should they start? What\nkind of help do you need from the open source community? We have some issues tagged \"Help Wanted\" in the issue\ntracker ; certainly those could be\na good place to start, but we're always open to suggestions of new\nfeatures/functionality as well. If you use Dialyxir and would like to see\nsomething a little different than it is today, feel free to open an issue to\ndiscuss it with us! Photo by Louis Reed on Unsplash Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-04-11"},
{"website": "Hash-Rocket", "title": "Silence Logger Messages in ExUnit", "author": ["\nDorian Karter\n\n"], "link": "https://hashrocket.com/blog/posts/silence-logger-messages-in-exunit", "abstract": "Elixir Silence Logger Messages in ExUnit by\nDorian Karter\n\non\nAugust 13, 2019 Logger messages can be a distracting noise when running multiple tests. Fortunately ExUnit provides some great tooling to prevent unwanted logger messages when running your tests. Take this plug route for example: defmodule MyApp . Router do require Logger use Plug . Router use Plug . Debugger , otp_app: :my_app alias MyApp . SlackResponder plug ( Plug . Logger ) plug ( Plug . Parsers , parsers: [ :json , :urlencoded ], json_decoder: { Jason , :decode! , [[ keys: :atoms ]]} ) plug ( :match ) plug ( :dispatch ) post \" /\" do response = SlackResponder . respond ( conn . params ) send_resp ( conn , 200 , response ) end match _ do Logger . debug ( \" Got a request I don't understand #{ inspect ( conn ) } \" ) send_resp ( conn , 404 , \" not_found\" ) end end And the test: test \" returns 404 when url doesn't match\" do conn = conn ( :post , \" /bogus_url\" , %{ \" text\" => \" hi\" }) |> Router . call ( Router . init ([])) assert conn . state == :sent assert conn . status == 404 end Running this test will produce: To mute the logger send the capture_log options to ExUnit.start in test/test_helper.exs : ExUnit . start ( capture_log: true ) Now upon re-running the test the logs will not show up, unless the test failed! What a great feature!!! Passing example: Failure example: You can also control this feature more granularly by tagging individual tests: @tag capture_log: true # Default is `false` test \" returns 404 when url doesn't match\" do # ... end To learn more: https://hexdocs.pm/ex_unit/ExUnit.html#configure/1 Or if you are inside of iex : iex > require ExUnit iex > h ExUnit . configure Photo by Kristina Flour on Unsplash Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-08-13"},
{"website": "Hash-Rocket", "title": "How I Built My Own Heroku for Phoenix Apps: Part 3", "author": ["\nMicah Cooper\n\n"], "link": "https://hashrocket.com/blog/posts/how-i-built-my-own-heroku-for-phoenix-apps-part-3", "abstract": "Elixir Phoenix How I Built My Own Heroku for Phoenix Apps: Part 3 by\nMicah Cooper\n\non\nOctober 20, 2016 Gatling's one major dependency, Exrm has\nbeen succeeded by Distillery . Here,\nwe'll walk through considerations and implementation of replacing Exrm with\nDistillery. Read part 1 here. Read part 2 here For more information on the motivations behind Distillery see\nhere Considerations Initally, there are two major considerations for this upgrade. Backwards combatability First thing to consider is backwards compatibility. Do I still want to support\nDistillery's predecessor; Exrm? Gatling could, in theory, inspect the project\nbeing deployed and break off into two separate paths- One for Exrm and one for\nDistillery. Another option is to release a major version upgrade (at the time\nof this blog post, from 0.1.0 to 1.0.0) and force users to upgrade to\nDistillery. After reading through the Distillery docs and giving it a try, I decided to go\nwith the latter option for a couple reasons:\n1. I'm a big fan of keeping sofware up to date\n2. This major change would require very little work from the client developer.\n   He or she should only have to change the dependency in their mix.exs file. Deployment Flexibility Then next consideration is introduced by Distillery's added features. A user can\ndo much more with a deploy including:\n- Building more customized releases\n- Adding boot hooks to a release\n- Adding custom commands to a release\n- Writing custom overlays for a release See the Distillery docs for more details. With all these new features, the questions is how much of these options should\ncarry over to Gatling? Fortunately, just about everything. The goal of Gatling\nis to provide a convenient deployment strategy but still allow a user to\ncustomize whatever they want. Also, it just so happens most of these features\ndon't really affect the way Gatling deploys a project. How it's made After thinking through the considerations, it's time to take some steps towards\nthe and upgrade. The actualy code required to upgrade is minimal. I'd say the\nmajority of the work comes from testing, and documenting the change for users: The %Gatling.Env{} The %Gatlng.Env{} struct is where we keep all the information (like paths,\ntemplates and modules) needed to perform a deploy. The upgrade required the\naddition of a few new things: release_config_path - The is the location in the deploying project of\nDistillery's new config.exs file. releases - This is a list of all the existing releases inside ./rel/ . More\non this later. Mix Release Exrm uses a mix task called mix release just like Distillery. The only\ndifference here is one added step required by the additional features. Before\ngatling runs mix release , it must run mix release.init to generate a rel/config.exs file. So we simply add this to our pipline in the deploy and upgrade tasks: def deploy ( project ) do Gatling . env ( project , port: :find ) |> call ( :mix_deps_get ) |> call ( :mix_compile ) |> call ( :mix_digest ) |> call ( :mix_release_init ) |> call ( :mix_release ) ... And our mix_release_init/1 function looks like this: def mix_release_init (% Gatling . Env {} = env ) do if File . exists? ( env . release_config_path ) do log ( \" #{ env . release_config_path } found\" ) else bash ( \" mix\" , ~w[release.init --no-doc] , cd: env . build_dir ) end env end Look for rel/config.exs file (the path is provided by our %Gatling.Env{} struct. If it's there, log that it was found, otherwise run mix release.init Mix release --upfrom The next change is in our mix gatlng.upgrade task. With Exrm, when we call mix release an upgrade release would be created automatically from most\nrecent version. Now we do that explicity by finding the most recent release\nfrom our %Gatling.Env{} and calling mix release --upgrade\n--upfrom=<release> .  The introductio of the Gatling.env.releases paves the\nway to much more than just upgrades. Eventaully, we may be able to use this\nupgrade and downgrade to any version. And also list out all the releases of our\nproject. The future These are the major factors of upgrading Gatlig to use Distillery. It must be\nsaid that Distillery is still < 1.0 at the time of this writing. So, it would\nbe fair to expect the changes of this new dependency to also affect Gatling.\nBut that's ok. That's just part of the game. If you notice any bugs or issues\nwith Gatling. Please add an issue in the tracker . Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-10-20"},
{"website": "Hash-Rocket", "title": "Build the Ultimate Elixir CI with Github Actions", "author": ["\nDorian Karter\n\n"], "link": "https://hashrocket.com/blog/posts/build-the-ultimate-elixir-ci-with-github-actions", "abstract": "Elixir Build the Ultimate Elixir CI with Github Actions by\nDorian Karter\n\non\nMay 21, 2020 Learn how to configure the ultimate Continuous Integration (CI) for Elixir using Github Actions. Optimized for speed and consistency using granular caching, formatting, linting and type-checking. Setting up a Continuous Integration (CI) is a part of every mature project (or at least it should be!). CI helps ensure that the code you push doesn't lead you to quip, “works on my machine\", because it will work on all anticipated target machines. Importantly, CI can catch errors before they reach production. It also prevents consistency issues with regards to code formatting and linting, helping the team maintain a singlular approach to writing Elixir code. Lastly, we can use it to run checks that some people either forget to run or avoid running locally when they are time consuming or not part of a developer's tools / habits. When Github introduced it's Github Actions I was happy to see Elixir included as one of the default templates. Now it is a lot easier to set up a CI solution with consolidated billing from your pre-existing Github account. This is a game changer for consultants. It is now much easier to convince clients to allow us to set up a CI, because we do not need to ask them to set up an account for us. Unfortunately, the default Elixir template does not do much other than run your tests. In this post I will show you a configuration that will allow you to: Cache dependencies, which will significantly cut the time needed to run the workflow Lint your code using Credo Check the code's formatting using Mix Format Use and optimize Dialyzer/Dialyxir to check the type safety of your code and avoid potential run-time errors Run ExUnit tests You can find the full configuration in this Github Gist . Creating a Basic Elixir Workflow Using Github's UI, we will start with the default Elixir CI provided by Github. Start by navigating to your repository -> click the Actions tab -> click New Workflow . Github will suggest an Elixir Workflow, click Set up this workflow The default workflow will look something like this: name : Elixir CI on : push : branches : [ master ] pull_request : branches : [ master ] jobs : build : runs-on : ubuntu-latest steps : - uses : actions/checkout@v2 - name : Setup elixir uses : actions/setup-elixir@v1 with : elixir-version : 1.9.4 # Define the elixir version [required] otp-version : 22.2 # Define the OTP version [required] - name : Install Dependencies run : mix deps.get - name : Run Tests run : mix test Defining Environment Variables Github Actions supports setting hard-coded environment variables that will be used throughout every step run by the workflow. For our simple Elixir project we just want to set MIX_ENV to test . Let's add the env section to the top level below the on section, and specify our MIX_ENV : # ... on : push : branches : [ master ] pull_request : branches : [ master ] env : MIX_ENV : test # ... By setting the MIX_ENV globally for the workflow you can avoid some repetitive builds, since Elixir caches build artifacts and dependencies by environment. You can also store secrets in Github and those will be encrypted at rest, and will be decrypted only during the execution of the workflow. For example: env : MIX_ENV : test SUPER_SECRET : ${{ secrets.SuperSecret }} For this to work, you will need to define the value of the secrets in project's settings page. For the purpose of this tutorial we will not use any secrets, but it is good to be aware that those exist. You can read more about creating and storing encrypted secrets here . Using a Build Matrix Build Matrices allow us to test across multiple operating systems, platforms and language versions in the same workflow. This is particularly important for libraries such as Hex Packages . The other benefit of a Build Matrix is granular caching. Since caches use a key for a lookup, we can construct the key from the Build Matrix variables which are hydrated at each run. For our purposes we will only specify the versions of Elixir and Erlang we need, and replace the versions used in the Setup Elixir step with those from the Build Matrix: #... strategy : matrix : elixir : [ 1.10.2 ] otp : [ 22.2.8 ] steps : - uses : actions/checkout@v2 - name : Setup elixir uses : actions/setup-elixir@v1 with : elixir-version : ${{ matrix.elixir }} # Define the elixir version [required] otp-version : ${{ matrix.otp }} # Define the OTP version [required] # ... Setting Up Mix Dependency Cache It is crucial to make the CI run time as short as possible. Every second a developer is waiting on CI to finish running contributes to the high cost of context switching. To speed things up we can cache the Hex dependencies for our project. Let's add a new step after the Setup elixir step: # ... - name : Retrieve Mix Dependencies Cache uses : actions/cache@v1 id : mix-cache # id to use in retrieve action with : path : deps key : ${{ runner.os }}-${{ matrix.otp }}-${{ matrix.elixir }}-mix-${{ hashFiles(format('{0}{1}', github.workspace, '/mix.lock')) }} # ... We will again be using a built-in action provided by Github called cache . We will pass it an ID that we will use later when retrieving the cache as well as a key comprised of the OS, OTP and Elixir matrix combination, as well as a hash of our mix.lock file. This ensures a cache miss when the dependencies have changed in our mix.lock file and segregates the cache based on the Build Matrix to prevent version conflicts. Next up we will install dependencies if the cache was not hit: # ... - name : Install Mix Dependencies if : steps.mix-cache.outputs.cache-hit != 'true' run : | mix local.rebar --force mix local.hex --force mix deps.get # ... The important bit here is the if: steps.mix-cache.outputs.cache-hit != 'true' which will prevent running the commands if there is a matching cache entry. Check Formatting Elixir has a formatter built into the language. To use it in CI simply add another step after Install Mix Dependencies : #... - name : Check Formatting run : mix format --check-formatted #... This command will return an error status code if at least one of the files in the project was not properly formatted and halt the rest of the checks. It is imperative that you sort the steps by their run time. This will allow the CI to fail fast which shortens the iteration cycles. Lint with Credo Credo is my preferred linting tool for Elixir, but because it often takes longer to compile, I chose to place it after the formatting step. After the Check Formatting step add the following: # ... - name : Run Credo run : mix credo --strict # ... I chose to add the --strict since it will also show refactoring opportunities. Running Dialyzer Checks With PLT Cache Dialyzer is an invaluable tool that catches potential run-time errors in development, where they are cheap to correct, before they are pushed to production. However, a common deterrent for using Dialyzer is the amount of time it takes to run the check. We can resolve that by caching the PLT. Dialyzer stores a Persistent Lookup Table (PLT) of your application's dependencies, this takes a long time to build, but makes up for it in time saved on incremental checks. The good news is there is no need to re-run the PLT build unless your dependencies have changed (which shouldn't happen too often). Here's how we can cache the PLT and only rebuild it if our dependencies have changed. In your project's mix.exs , you will need to tell Dialyzer where the PLT is located: # ... def project do [ # ... dialyzer: dialyzer (), # ... ] end defp dialyzer do [ plt_core_path: \" priv/plts\" , plt_file: { :no_warn , \" priv/plts/dialyzer.plt\" } ] end # ... Then in the Github workflow, below the Run Credo step add the following: # ... - name : Retrieve PLT Cache uses : actions/cache@v1 id : plt-cache with : path : priv/plts key : ${{ runner.os }}-${{ matrix.otp }}-${{ matrix.elixir }}-plts-${{ hashFiles(format('{0}{1}', github.workspace, '/mix.lock')) }} - name : Create PLTs if : steps.plt-cache.outputs.cache-hit != 'true' run : | mkdir -p priv/plts mix dialyzer --plt - name : Run dialyzer run : mix dialyzer --no-check --halt-exit-status As in the dependencies cache, we utilize the Build Matrix and the hash of the mix.lock file. If the dependencies have changed, we will run mix dialyzer --plt which will only generate the PLT. Then in the Run Dialyzer we run mix dialyzer with the --no-check flag which skips the check to see if the PLT needs to be updated. Using this technique I was able to cut build times for a small-ish project from ~765 seconds (or 12m 45s) to ~105 seconds (or 1m 45s). That’s ~7x faster! Conclusion In this post we walked through how use Github Actions to build an optimized Elixir CI. We started with the default template provided by Github and were able to cut build times and add additional checks to ensure coding style and formatting guidelines are consistent across our team, which reduces bike-shedding. There's more we can do to optimize the CI, but hopefully this example makes for a better starting template than the one supplied by Github Actions. At Hashrocket, we love Elixir, Phoenix and LiveView and have years of experience delivering robust, well-tested Elixir applications to production. Reach out if you need help with your Elixir project! Photo by Jason Yuen on Unsplash Was this post helpful? Share it with others. Tweet Share Post", "date": "2020-05-21"},
{"website": "Hash-Rocket", "title": "Automate Your Infrastructure with Pulumi", "author": ["\nDorian Karter\n\n"], "link": "https://hashrocket.com/blog/posts/automate-your-infrastructure-with-pulumi", "abstract": "DevOps Automate Your Infrastructure with Pulumi by\nDorian Karter\n\non\nMay  5, 2020 Managing infrastructure can be frustratingly hard. Combing through byzantine interfaces with poor documentation and hidden features, clicking through hundreds of menus - these experiences are all too familiar to DevOps engineers using AWS. A DevOps engineer's worst fear is that the business will ask for a new instance of the application for a client, and they will have to repeat all those manual steps again. (Hopefully in the correct order, and not forgetting anything along the way!) At Hashrocket we are big fans of automation, and part of that means we use the latest technologies for declaring Infrastructure-as-Code (IaC). In this blog post I will introduce you to an up and coming IaC solution called Pulumi and discuss its advantages compared to other tools. You can follow along using the companion repository on Github . Infrastructure Defined Infrastructure usually refers to the provisioning of service on a hosting provider such as AWS, Azure, Google Cloud or my favorite, Digital Ocean. Each provider has its own terminology for the assets you can spin up. On AWS a Virtual Machine is called EC2 (or ECC) , short for Elastic Compute Cloud. Digital Ocean calls those VMs Droplets. When choosing a platform it is important to know all of the specific terminology and naming of their asset types before getting started. One thing to note here is that I am not discussing machine configuration, just the infrastructure pieces such as Firewall, DNS, and VM (e.g. installing packages from apt-get required to compile and run the app). Machine configuration is an entirely different topic for another blog post and requires a different toolset (Configuration Management tools such as Ansible, Puppet or Chef). I am also not discussing delivery of your application to the production machine, there are other tools for that. The goal of this post is to show how to create an idempotent script that you can run to set up your cloud infrastructure automatically and update it as business/technical requirements change. Pulumi There are a few options out there for declaring Infrastructure-as-Code, most notably Terraform by Hashicorp , AWS CloudFormation and Pulumi . After using Terraform a few years back and not really remembering its obtuse configuration language, I bumped into Pulumi, and it looked really promising. Pulumi allows you to specify your infrastructure in a language of your choice (out of JavaScript, TypeScript, Python, Go, or any .NET language, including C#, F#, and VB). This is really appealing to me as a developer since I don't have to learn a new language just for IaC, I can use what I already know. Out of the languages Pulumi supports I prefer TypeScript the most, it is lightweight, statically typed, and tooling such as auto-completion are really well made and work flawlessly in my NeoVim + coc.nvim setup ( checkout my dotfiles here ). Goals By the end of this post I will show you how to setup the following on DigitalOcean: SSH Public Key Droplet Associate the droplet with the SSH Public Key Firewall + Firewall Rules Domain + DNS for bare domain DNS for www domain Project on DigitalOcean (so that our infrastructure assets are grouped in the web interface) You will be able to run the script \"up\" or \"down\" to create or destroy your infrastructure. Prereqs If you want to follow along you will need to do a few things first: install the Pulumi CLI You will need to create a Pulumi account (and log into it via the CLI - this will allow Pulumi to store the current state of your infrastructure every time the script is run, and will allow you to store a private key for your encrypted configuration secrets.) Creating a New Pulumi Project In your project's folder run the pulumi new command, we are going to give it a few parameters to select TypeScript as our language and a directory name ( infra in this case): $ pulumi new --dir infra typescript Next, Pulumi will ask you a few questions about your project, go ahead and fill those in. For the stack name I chose prod . Each project can have many stacks, which are isolated instances of your project, you can have one for dev , staging and production , and each stack can have it's own configuration values. For the purpose of this tutorial we will focus on just one stack which I named prod to follow Elixir conventions. You can read more about stacks here . At this point Pulumi will go ahead and setup the project for you, install the dependencies and at the end you should see Your new project is ready to go!  ✨ . Setting Up DigitalOcean If you don't already have an account on DigitalOcean, go ahead and create one . Log into your account and create an API token . Copy that token. You can let Pulumi read that token in one of two ways: Environment variable: $ export DIGITALOCEAN_TOKEN = 'XXXXXXXXXXXXXXXXXX' Or use one of Pulumi's built in features to store configuration secrets, those are going to be encrypted using a key Pulumi stores on the cloud, so if you commit a secret to git no one will be able to decrypt it unless they are logged in to the same Pulumi account (this comes in handy when collaborating with a team account): $ pulumi config set digitalocean:token XXXXXXXXXXXXXX --secret The --secret is the important bit here! After that bit you can verify your secret was encrypted by opening this file Pulumi.prod.yaml . You'll see something like: config : example:digitalocean:token : secure : AAABAPXu6vBm3uICjVYaEN6MRD8RJ2gCPOsGrmC+xiR6MwDelrl4E67tBw== Setting Up the SSH Key When I start a new project I usually generate a new ssh key just for that particular project. For simplicity sake we will just use your personal ssh key located at ~/.ssh/id_rsa.pub . First, let's add the DigitalOcean plugin to our project: $ npm install @pulumi/digitalocean Then, in the infra directory edit the index.ts in your favorite editor, for best results use one that supports a TypeScript language server such as NeoVim + Coc or VSCode. Import the plugin like so: import * as DigitalOcean from '@pulumi/digitalocean' ; We will also need to read the id_rsa.pub file so let's go ahead and import the fs library that's built into node: import * as fs from 'fs' ; Now we will read the contents of the public key and use the DigitalOcean plugin to upload it: const publicKey = fs . readFileSync ( '~/.ssh/id_rsa.pub' ). toString (); const sshKey = new DigitalOcean . SshKey ( 'example' , { name : 'example' , publicKey , }); Save the file and run pulumi up . If everything worked correctly you should see the plan Pulumi drafted. When confirmed Pulumi will upload your public key to DigitalOcean and you should be able to see it under Settings -> Security -> SSH keys. Now that we have an SSH key all set up and ready to go let's configure the Droplet. Configuring the Droplet Open the index.ts file again and let's add some code to provision the Droplet.  You'll notice one of coolest features of Pulumi when paired with TypeScript, discoverability ! There are many options to choose from and memorizing the internal enums of each cloud provider can be exhausting and error-prone. That's where TypeScript's autocomplete and Pulumi's types come in and help you discover what you can do and what values you can use for properties. Wherever possible Pulumi provides the enums through types, such as the size and location of the Droplet, both are static and don't change much. Start typing DigitalOcean.DropletSlugs. then select the one that fits your requirements.  For this demo I chose the 1VCPU + 1GB RAM $5 machine and placed it in San Francisco 2 region: const projectName = 'example' ; const droplet = new DigitalOcean . Droplet ( ` ${ projectName } -web` , { size : DigitalOcean . DropletSlugs . DropletS1VCPU1GB , region : DigitalOcean . Regions . SFO2 , image : 'ubuntu-18-04-x64' , monitoring : true , ipv6 : true , sshKeys : [ sshKey . fingerprint ], }); For the image it is much harder to keep track since the available images offered by DigitalOcean change often. So you would need to know the slug for the image you want. For most cases you can guess the slug, and you'd be right. If you want to be sure though, check out the DO API which has an endpoint for listing available images . Another very interesting thing to note here is when passing the SSH key we defined earlier to the Droplet creation code, we are using the constant sshKey and calling .fingerprint on it. You might expect the type of sshKey.fingerprint to be a String , which would make sense if Pulumi was entirely synchronous, but it is in fact a Promise . When running pulumi up it will try to parallelize what it can. When it notices you passed a Promise as one of the values to an asset constructor it will wait for that Promise to resolve before creating the asset. I thought that was super cool! It allows some assets to be created in parallel when their dependencies are ready. Setting Up a Domain Name and DNS Records If you have a domain name for your project, you can follow these steps to manage its DNS on DigitalOcean and point it at the Droplet. You'll need to set the nameservers for the domain to point at DigitalOcean's name servers .  This may take a while to propagate. I like to store the domain name in Pulumi's config - this way we can ensure it is different between 'stacks' if needed. To do that run the following in your infra folder: $ pulumi config set domainName example.com Notice that we did not use --secret this time since this is not sensitive information for us. Now we will require it from the config and use it when setting up the domain on DO: const config = new Pulumi . Config (); const domainName = config . require ( 'domainName' ); const domain = new DigitalOcean . Domain ( domainName , { name : domainName , ipAddress : droplet . ipv4Address , }); Again notice that we referred to an attribute on droplet , namely ipv4Address which is a Promise and will be resolved by Pulumi after the Droplet has been created. You can also add an additional record for the www subdomain: new DigitalOcean . DnsRecord ( 'www' , { domain : domain . name , name : 'www' , value : droplet . ipv4Address , type : 'A' , }); I didn't have to read the documentation to discover how to write the above block, I just used TypeScript's autocomplete to guide me through it. I did not have a similar experience when learning to use Terraform. Group Assets Under A Project It's useful to group the assets you create under a project in DO so that they don't just float around. new DigitalOcean . Project ( projectName , { name : domainName , resources : [ droplet . id . apply ( id => `do:droplet: ${ id } ` ), domain . id . apply ( id => `do:domain: ${ id } ` ), ], }); Notice that we did something a little different here. Both droplet and domain have a property called urn which stands for Unified Resource Name , and DigitalOcean.Project.resources expects a list of urns . Unfortunately there's a bug at the time of writing where the urn s were not correct DigitalOcean urn s but rather Pulumi-specific urns. I've found a workaround using apply and it's a good opportunity to learn about some of the tools at our disposal! The apply function on properties of assets accepts a callback that will be run when the promise is resolved, once the asset has been created, and passes it the value of the property. In our case we needed the urns to be in the format of do:type_of_asset:id_of_asset . To get the droplet id injected into a string with that format I've done this: droplet.id.apply(id => do:droplet:${id} ) .  Again, this transformation will only be called when the value of droplet.id becomes available. The apply function will become useful again in moment, when we create our firewall. Creating a Basic Firewall DigitalOcean's web interface offers a \"basic firewall\" configuration that you can add to a Droplet. Default Firewall Configuration (without our modifications): I was able to reverse engineer the configuration for this basic firewall (with some minor additions) using their API and re-create it using Pulumi code: const defaultFirewallAddresses = [ '0.0.0.0/0' , '::/0' ]; new DigitalOcean . Firewall ( ` ${ projectName } -firewall` , { inboundRules : [ { protocol : 'tcp' , portRange : '22' , sourceAddresses : defaultFirewallAddresses , }, { protocol : 'tcp' , portRange : '80' , sourceAddresses : defaultFirewallAddresses , }, { protocol : 'tcp' , portRange : '443' , sourceAddresses : defaultFirewallAddresses , }, ], outboundRules : [ { protocol : 'icmp' , portRange : '1-65535' , destinationAddresses : defaultFirewallAddresses , }, { protocol : 'tcp' , portRange : '1-65535' , destinationAddresses : defaultFirewallAddresses , }, { protocol : 'udp' , portRange : '1-65535' , destinationAddresses : defaultFirewallAddresses , }, ], dropletIds : [ droplet . id . apply ( i => + i )], }); We won't get into the nitty gritty of each of those rules, but they should match what you get when you create a new firewall from the web interface (all outgoing connections allowed, and incoming connection for port 22 allowed - for SSH) with the addition of opening ports 80 and 443 for incoming web connections. Notice that here I also used apply , this time ( again due to a bug ) I am converting the Droplet ID from string to integer. Test the script by running $ pulumi up Exporting Some Properties Finally, you may want to export some properties in the output of pulumi up . In this example I'll be exporting the Droplet IPv4 and IPv6 addresses: export const ip = droplet . ipv4Address ; export const ipv6 = droplet . ipv6Address ; And again test the script $ pulumi up Here's the full script with a few more things pulled out into configuration variables: https://bit.ly/automate-your-infrastructure-with-pulumi-gist Teardown Now that we've created all those assets let's destroy them so that we are not paying for them when we're done experimenting. $ pulumi destroy Conclusion Pulumi provides an easy way for developers and system admins to collaborate on infrastructure in a repeatable, discoverable way. This is just the tip of the iceberg and Pulumi is still young, but it is very promising and a pleasure to use. I hope you enjoyed this post and if you have any questions don't hesitate to reach out to Hashrocket for help with automating your infrastructure. Photo by Saad Salim on Unsplash Was this post helpful? Share it with others. Tweet Share Post", "date": "2020-05-05"},
{"website": "Hash-Rocket", "title": "Automate Your Elixir Deployments - Part 1 - Ansible", "author": ["\nDorian Karter\n\n"], "link": "https://hashrocket.com/blog/posts/automate-your-elixir-deployments-part-1-ansible", "abstract": "Elixir Phoenix DevOps Automate Your Elixir Deployments - Part 1 - Ansible by\nDorian Karter\n\non\nMay 12, 2020 This post will guide you through automating a \"bare-metal\" machine configuration, and getting a server ready for building and deploying Elixir / Phoenix applications (with LiveView support!) There comes a time in every application’s life when, as a developer, you want to share it with the world. There are some easy solutions out there for deploying Elixir, most notably Gigalixir or Heroku . However, when something goes wrong you may want to solve the issue yourself, not get on long support calls or email chains. Knowledge is power, and understanding all the moving pieces of deploying to “bare metal” gives you a powerful skill-set that will allow you to deliver more customized solutions, without the black-box limitations of Platform-as-a-Service software. In a previous post , I demonstrated how we can automate the creation of infrastructure. This guide will build on that idea. You can follow along using the companion repository on Github . Target Audience - Who Is This Post For This post is targeted towards Elixir Developers interested in deploying their application onto a Linux box on a cloud provider of their choice. Automating this process shortens iteration cycles so that the process is easily repeatable. This makes scaling, replicating, and fixing issues a breeze. In the process I hope to demystify \"DevOps\" and empower developers to be more comfortable with Linux, Nginx and automation tools like Ansible. We are going to set up a machine with automatically renewable SSL certificates from Let's Encrypt and unattended security upgrades so that maintenance is kept to a bare minimum. In most cases, there shouldn't be a need to SSH into it once everything is up and running. Prerequisites - Tools We Will Use In this post we will set up a Debian based machine, in this case Ubuntu, to host our Elixir application, so a prerequisite will be creating one on a cloud platform such as Digital Ocean . I've used a $5/month machine with 1 CPU and 1GB RAM, and it is very capable and sufficient for running a Phoenix application. See my previous post if you want to quickly spin one up. I recommend setting up your SSH Config file to point to the IP address or domain of the machine we will be deploying to: Host example\n  User root\n  HostName example.com # or IP address\n  IdentityFile ~/.ssh/your_ssh_key Throughout this post I've used example and example.com as the project name and domain respectively. You will need to make sure to replace those for filenames and other places where the word example is being used. You will need root access to set up many of the pieces in this tutorial. For simplicity purposes I am assuming you already have access to the root user. We will use Ansible , a Configuration Management tool, to set up the machine, install dependencies etc. Then to deliver a release of our application to the server, we will use eDeliver with Distillery . While it is possible to use Ansible to deliver the code, I believe in using the best tool for the job. Ansible is great for configuring a machine, but eDeliver and Distillery are more well suited for Elixir / Erlang's specific build and delivery requirements. With all of that out of the way, let's get started! Setting Up Our Ansible Project First we will need to create a directory for our Ansible project. In my project I placed it under ansible/ . You will need a configuration file for Ansible. For now we will not get too deep into how the Ansible configuration works, you can read more about that here , instead we will use some sensible defaults. Feel free to copy mine and save it into ansible/ansible.cfg : [defaults]\nnocows=1\ninventory = inventories/production\nlog_path = /tmp/ansible.log\nretry_files_enabled = True\nretry_files_save_path = tmp\nroles_path = galaxy_roles:roles\ncallback_whitelist = timer, profile_tasks\nstdout_callback = skippy\ngathering = smart\n\n[ssh_connection]\nssh_args = -o ForwardAgent=yes -o ControlMaster=auto -o ControlPersist=30m\npipelining = True\ncontrol_path = /tmp/ansible-ssh-%%h-%%p-%%r You will need to create a few directories inside the ansible directory to keep things organized: $ mkdir -p inventories/group_vars/ { all,application/secret } playbooks/templates tmp This will create a basic directory structure: ❯ tree\n.\n├── inventories\n│   └── group_vars\n│       ├── all\n│       └── application\n│           └── secret\n├── playbooks\n│   └── templates\n└── tmp NOTE: If you are using git to store your project, I suggest adding ansible/tmp to your .gitignore since Ansible will store some 'retry' files there: $ echo 'tmp/' >> .gitignore Next we need to create a main.yml file, this will be our entry point for the script that will import all of the smaller playbooks and execute them in order. ansible/main.yml : --- - hosts : application remote_user : root We have set the hosts to application - that is our target group - a group of hosts that we will run the tasks on. We will discuss how to set up target groups in the next section. Inventory Ansible needs to know about all the machines it will be targeting.  It keeps track of these machines in an inventory .  For this project, we have one environment, production , and one machine in that environment example . Let's create our production inventory in ansible/inventories/production : [application]\nexample We are using the host example which should match the SSH Config entry we added in a previous step, and putting it under the application group. Configuring Machine Login In this step we will setup a deploy user that will build, deploy and run our application. In addition, we will implement a few hardening steps to ensure your machine's SSH Server is more secure from brute-force attacks. Many worms, scanners, and botnets scan the entire Internet looking for SSH logins, so it's always a good idea to reduce the risk by disabling password authentication over SSH and using a proactive log analyzer such as Fail2Ban . First we need to create some variables to make things easy to refactor and\nmaintain. In your ansible/inventories/group_vars/all directory, create a new file ansible/inventories/group_vars/all/all.yml : --- username : deploy app_name : example domain : example.com These variables will be shared across all your Playbooks (and feel free to change them according to your needs. e.g. There is no requirement for the user to be called deploy ). Next, create a new Playbook and add some tasks in ansible/playbooks/configure-login.yml : --- - hosts : application remote_user : root tasks : - name : Create Deploy User user : name : ' {{ username }}' createhome : yes state : present shell : /bin/bash register : deployuser - name : Disable password for deploy on creation # this will set the password to something untypable and random essentialy # preventing password login for this user shell : /usr/bin/passwd -l '{{ username }}' # this line tells Ansible to only run this task if the deployuser we # defined above has changed when : deployuser.changed - name : Deploy SSH Key authorized_key : user : ' {{ username }}' # you would need to change this line to point to your public key key : \" {{ lookup('file', '~/.ssh/your_ssh_key.pub') }}\" state : present - name : Disable Password Authentication lineinfile : # completely disables password authentication for ssh, so make sure your # root user is set up to connect with a key, not a password! dest : /etc/ssh/sshd_config regexp : ' ^PasswordAuthentication' line : \" PasswordAuthentication no\" state : present backup : yes notify : restart ssh handlers : - name : restart ssh service : name : sshd state : restarted A really cool feature of Ansible is its ability to replace a line in a file (see lineinfile above under \"Disable Password Authentication\" ). It allows you to run a sed -like command to search with regex, and replace a line, but will only run if the line is not already present. You can also pass it backup: yes to create a backup file, just in case the replacement did not go as planned. Also notice the handlers section. You can think of those as functions that are reusable throughout your Playbook. In this case we created one for restarting the sshd service after making changes to its configuration file, and we call it in the notify action of the \"Disable Password Authentication\" task. Notify actions will only be triggered once even if notified by multiple different tasks. Finally, we'll import that playbook in our ansible/main.yml file: --- - hosts : application remote_user : root - name : Configure Machine Login import_playbook : playbooks/configure-login.yml To test everything, run: $ ansible-playbook main.yml If everything worked you should be able to log into the machine using our newly created user like so: $ ssh deploy@example -i ~/.ssh/your_ssh_key Install Packages Our next step is to install some packages from the operating system package manager, in this case apt on Ubuntu. Some of those are optional, so feel free to drop them, and depending on your use case you may want to add more. Ansible comes with built-in support for apt so installing packages is a breeze. We will also utilize Ansible Roles , more specifically Galaxy Roles . You can think of Roles as packages / dependencies; they are groupings of Ansible vars , tasks and handlers . Ansible Galaxy is Ansible's package repository where you can find many different roles for automating common complex tasks. First, let's create our new Playbook in ansible/playbooks/install-packages.yml : --- - hosts : application vars : - packages : # Scans system access logs and bans IPs that show malicious signs - fail2ban # For building with eDeliver - git # For compiling assets using webpack - nodejs - npm # For reverse proxy into our application - nginx remote_user : root tasks : - name : Update APT package cache apt : update_cache : yes cache_valid_time : 3600 - name : Install required packages apt : state : present pkg : \" {{ packages }}\" - name : Check if Erlang is Installed command : dpkg-query -W esl-erlang register : erlang_check_deb failed_when : erlang_check_deb.rc > 1 changed_when : erlang_check_deb.rc == 1 - name : Download erlang.deb get_url : url : \" https://packages.erlang-solutions.com/erlang-solutions_1.0_all.deb\" dest : \" /home/{{ username }}/erlang-solutions_1.0_all.deb\" when : erlang_check_deb.rc == 1 - name : Install erlang dpk src apt : deb : \" /home/{{ username }}/erlang-solutions_1.0_all.deb\" when : erlang_check_deb.rc == 1 - name : Install erlang and elixir apt : update_cache : yes state : present pkg : - esl-erlang - elixir when : erlang_check_deb.rc == 1 - name : Install Hex command : mix local.hex --force changed_when : > \"Will always run, don't show that it changed\" == 1 roles : - role : jnv.unattended-upgrades unattended_origins_patterns : - ' origin=Ubuntu,archive=${distro_codename}-security' unattended_automatic_reboot : true unattended_automatic_reboot_time : ' 09:00' unattended_mail : \" {{ admin_email }}\" Most of the steps above are documented in their name, but generally what this Playbook will do for us is update the apt database, and install some packages, including Erlang and Elixir. Some of those packages are optional, so you should examine the list and modify it according to your needs. To make it work we need to add a variable to our ansible/inventories/group_vars/all/all.yml file that we defined earlier, specifically the admin_email variable: --- username : deploy app_name : example domain : example.com admin_email : admin@example.com If you are going to commit this to a public repository, you may not want to expose your email in clear text. In a future step we will look at how we can utilize Ansible Vault to store variables such as this one in an encrypted file. We will also need to install the Galaxy Role we referenced at the bottom of the file jnv.unattended-upgrades . In your Ansible folder run: $ ansible-galaxy install jnv.unattended-upgrades As always when using a dependency of this nature, skim through the code to give yourself confidence that the code isn't doing something unsafe. This role will install the UnattendedUpgrades package which will keep your server up to date with security updates. Notice that we have enabled unattended_automatic_reboot which will reboot the machine at 9am UTC if any of the security updates installed requires a restart. If automated restarts are not acceptable in your case, you may want to remove the two related configurations, and restart manually. Let's add the Playbook to our ansible/main.yml file: --- - hosts : application remote_user : root - name : Configure Machine Login import_playbook : playbooks/configure-login.yml - name : Install Packages import_playbook : playbooks/install-packages.yml Now re-run Ansible: $ ansible-playbook main.yml You'll notice that the steps we did in the configure-login.yml Playbook have not been re-run again since Ansible is idempotent and is able to determine those changes have already been done. Setting Up Application Deployment Considerations In this section we will prepare the server for deployment using eDeliver and Distillery (which we will cover in the next post). We'll start by creating a new Playbook in ansible/playbooks/application-deployment-setup.yml : --- - hosts : application remote_user : root tasks : - name : Create .env file template : src : \" {{ app_name }}.env\" dest : \" /home/{{ username }}/{{ app_name }}.env\" owner : \" {{ username }}\" group : \" {{ username }}\" - name : Source .env file in user profile lineinfile : dest : ' /home/{{ username }}/.profile' regexp : ' ^\\. \"$HOME/{{ app_name }}.env\"' line : ' . \"$HOME/{{ app_name }}.env\"' state : present backup : yes - name : Ensures shared/config dir exists file : path : \" /home/{{ username }}/app_config\" state : directory owner : \" {{ username }}\" group : \" {{ username }}\" - name : Copy prod.secret.exs with owner and permissions copy : src : ../../config/prod.secret.exs dest : \" /home/{{ username }}/app_config/prod.secret.exs\" owner : \" {{ username }}\" group : \" {{ username }}\" - name : Create Systemd Init Script template : src : \" {{ app_name }}.service\" dest : \" /etc/systemd/system/{{ app_name }}.service\" - name : Enable Systemd service for application systemd : name : \" {{ app_name }}\" enabled : yes We need to create a few templates for this Playbook, first the .env file. This is where you'll store environment variables needed by your application during the build process and during runtime ansible/playbooks/templates/example.env : export SECRET_KEY_BASE = '{{ secret_key_base }}' export ERLANG_COOKIE = '{{ erlang_cookie }}' This file will be copied into your deploy user's home directory, sourced in .profile and in the Systemd service. Next, we'll define the variables used in the template in Ansible Vault: $ ansible-vault create inventories/group_vars/application/secret/phoenix.yml This will ask for a password to use when encrypting your secret variables. Once the password was entered, it will open your text editor defined in $EDITOR and allow you to edit this file. The convention is to use the secret_ prefix before encrypted variables: --- secret_example_secret_key_base : super secret stuff here secret_example_erlang_cookie : it's best to generate these values using mix phx.gen.secret When you save and exit your editor, Ansible Vault will encrypt your secret variables, now for discoverability we will refer to them in a regular unencrypted variable file ansible/inventories/group_vars/application/phoenix.yml : --- secret_key_base : \" {{ secret_secret_key_base }}\" erlang_cookie : \" {{ secret_erlang_cookie }}\" Lastly, we are going to define a template for the Systemd service that will ensure our application re-spawns if the server is restarted ansible/playbooks/templates/example.service : [Unit]\nDescription={{ app_name }}\nAfter=network.target\n\n[Service]\nUser={{ username }}\nRestart=on-failure\n\nType=forking\nEnvironment=MIX_ENV=prod\nEnvironmentFile= \"/home/{{ username }}/{{ app_name }}.env\"\nExecStart= /home/{{ username }}/app_release/{{ app_name }}/bin/{{ app_name }} start\nExecStop= /home/{{ username }}/app_release/{{ app_name }}/bin/{{ app_name }} stop\n\n[Install]\nWantedBy=multi-user.target Add our new Playbook to the ansible/main.yml file: --- - hosts : application remote_user : root - name : Configure Machine Login import_playbook : playbooks/configure-login.yml - name : Install Packages import_playbook : playbooks/install-packages.yml - name : Application Deployment Setup import_playbook : playbooks/application-deployment-setup.yml Now since we have encrypted vault secrets we will need to tell Ansible how to decrypt them. There are a few ways of doing that, you can either tell Ansible to ask you to type the password before running: $ ansible-playbook main.yml --ask-vault-pass Or, my preferred method, store the password in a plain-text file (that should NEVER be committed to git) and tell Ansible where that file is: $ ansible-playbook main.yml --vault-password-file .vault-password Ignore that file in git: echo '.vault-password' >> .gitignore To make your life a bit easier, you may want to create a mix alias in the root of your project under mix.exs : def project do [ aliases: aliases (), # ... ] end defp aliases do [ ansible: & run_ansible / 1 , # ... ] end defp run_ansible ( _ ) do Mix . shell () . cmd ( \" cd ansible/ && ANSIBLE_FORCE_COLOR=True ansible-playbook main.yml --vault-password-file .vault-password\" ) end Now to re-run the Ansible script you can simply run this command from the root of your project: $ mix ansible Setting Up Auto Renewing SSL Certification With Let's Encrypt To set up an SSL certificate we will once again use a Galaxy Role. This will automate the certificate renewal and take care of verification for us. First let's install the role from Ansible Galaxy. From inside our ansible/ directory run: $ ansible-galaxy install geerlingguy.certbot As before, make sure you skim through the code to ensure it is safe to run. Next we will create our Playbook in ansible/playbooks/lets-encrypt.yml : --- - hosts : application vars : - certbot_auto_renew : true - certbot_auto_renew_user : \" root\" - certbot_auto_renew_hour : \" 3\" - certbot_auto_renew_minute : \" 30\" - certbot_auto_renew_options : \" --quiet --no-self-upgrade\" - certbot_create_if_missing : true - certbot_admin_email : \" {{ admin_email }}\" - certbot_create_method : standalone - certbot_create_standalone_stop_services : - nginx - certbot_certs : - domains : - \" {{ domain }}\" - \" www.{{ domain }}\" remote_user : root roles : - geerlingguy.certbot We need to give Certbot an administrator email address, if you remember in a previous step we typed the email in clear text under inventories/group_vars/all.yml . This time we are going to move it into a vault so that it is not exposed. Run this command to create a new vault for all : $ ansible-vault create inventories/group_vars/all/secret/all.yml Enter a password for the vault. In the editor that opens write your email address: --- secret_admin_email : \" admin@example.com\" And replace the previous value in the unencrypted inventories/group_vars/all.yml : admin_email : \" {{ secret_admin_email }}\" Make sure all the settings seem reasonable, and run Ansible: $ mix ansible Now if your domain is configured correctly and pointing at your server's IP address, you should be able to see two files on the remote machine in /etc/letsencrypt/live/example.com/fullchain.pem and /etc/letsencrypt/live/example.com/privkey.pem . In the next section we will configure Nginx to use those files. Setting Up Nginx To serve our application we will use Nginx - this is not a necessary step, but it makes setting up SSL certificates with Let's Encrypt a little easier. Let's start with the Playbook. Create a file at ansible/playbooks/nginx.yml : --- - hosts : application remote_user : root tasks : - name : Remove the default nginx app's config file : path : /etc/nginx/sites-available/default state : absent - name : Remove the default nginx app's symlink if it exists file : path : /etc/nginx/sites-enabled/default state : absent - name : Copy nginx.conf template : src : nginx.conf dest : /etc/nginx/nginx.conf - name : Ensure Nginx Modules dir exists file : path : /etc/nginx/modules state : directory - name : Nginx SSL Shared Settings Module template : src : \" {{ app_name }}_shared_ssl_settings\" dest : /etc/nginx/modules/{{ app_name }}_shared_ssl_settings - name : Configure nginx for the app template : src : \" {{ app_name }}.nginx\" dest : \" /etc/nginx/sites-available/{{ app_name }}\" group : \" {{ username }}\" owner : \" {{ username }}\" force : yes - name : Enable the app file : src : \" /etc/nginx/sites-available/{{ app_name }}\" dest : \" /etc/nginx/sites-enabled/{{ app_name }}\" state : link owner : \" {{ username }}\" group : \" {{ username }}\" - name : Restart nginx service : name : nginx state : restarted changed_when : > \"Will always run, don't show that it changed\" == 1 In this Playbook we are deleting some of the defaults that Nginx came with, and replacing them with another set of defaults. You will need to create the following template ansible/playbooks/templates/nginx.conf : env PATH ; user www-data ; worker_processes 1 ; pid /var/run/nginx.pid ; events { worker_connections 1024 ; multi_accept on ; } http { # Basic Settings sendfile on ; tcp_nopush on ; tcp_nodelay on ; keepalive_timeout 65 ; types_hash_max_size 2048 ; server_tokens off ; include /etc/nginx/mime.types ; default_type application/octet-stream ; # Cache open_file_cache max=1000 inactive=20s ; open_file_cache_valid 30s ; open_file_cache_min_uses 5 ; open_file_cache_errors off ; # Logging Settings access_log /var/log/nginx/access.log ; error_log /var/log/nginx/error.log ; # Gzip Settings gzip on ; gzip_types application/json ; include /etc/nginx/conf.d/*.conf ; include /etc/nginx/sites-enabled/* ; } This is not too interesting so we won't go through it. The only relevant line here is the one that includes all files in /etc/nginx/sites-enabled/ . Our Playbook creates a file in /etc/nginx/sites-available/ and symlinks it to sites-enabled . Let's create the template for this file in ansible/playbooks/templates/example.nginx : upstream { { app_name }} { server 127.0.0.1 : 4000 ; } map $http_upgrade $connection_upgrade { default upgrade ; '' close ; } server { listen 80 default_server ; listen [::]:80 default_server ipv6only=on ; server_name { { domain }} www. { { domain }} ; return 301 https:// { { domain }} $request_uri ; } server { server_name www. { { domain }} ; include modules/ { { app_name }} _shared_ssl_settings ; return 301 https:// { { domain }} $request_uri ; } server { server_name { { domain }} www. { { domain }} ; root /home/ { { username }} /app_release/static ; include modules/ { { app_name }} _shared_ssl_settings ; location / { proxy_pass http:// { { app_name }} ; proxy_set_header Host $host ; proxy_set_header X-Real-IP $remote_addr ; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for ; proxy_buffering off ; } location /live { proxy_pass http:// { { app_name }} $request_uri ; proxy_http_version 1 .1 ; proxy_set_header Upgrade $http_upgrade ; proxy_set_header Connection \"Upgrade\" ; proxy_set_header Host $host ; } } What this configuration does is listen for connections to your bare domain (in our case example.com ) on ports 80 and 443. It will permanently redirect http connections to their more secure counterpart (https). As well as redirect www.example.com to the bare domain. The /live location is set up to support websockets by upgrading the connection, specifically the /live path is used by Phoenix LiveView , which is my favorite new feature in Phoenix 1.5. You might be wondering where the 443 port and SSL cert path are defined. To allow re-use and to DRY up the configuration, we extracted some SSL settings into a shared Nginx module. This allows us to reuse them in both our www. redirect and main server block. So you'll need to create this template next. In ansible/playbooks/templates/example_shared_ssl_settings place the following: listen 443 ssl http2 ; listen [::]:443 ; ssl_certificate /etc/letsencrypt/live/ { { domain }} /fullchain.pem ; ssl_certificate_key /etc/letsencrypt/live/ { { domain }} /privkey.pem ; # TLS ssl on ; ssl_session_cache shared:SSL:20m ; ssl_session_timeout 10m ; ssl_protocols TLSv1.2 TLSv1.3 ; ssl_prefer_server_ciphers on ; ssl_ciphers ECDH+AESGCM:ECDH+AES256:ECDH+AES128:!DH+3DES:!ADH:!AECDH:!MD5 ; # HSTS add_header Strict-Transport-Security \"max-age=31536000 ; includeSubDomains\" always ; # Secure Headers add_header X-Frame-Options DENY ; add_header X-Content-Type-Options nosniff ; add_header X-Permitted-Cross-Domain-Policies none ; Finally, we'll import the Playbook into our ansible/main.yml : --- - hosts : application remote_user : root - name : Configure Machine Login import_playbook : playbooks/configure-login.yml - name : Install Packages import_playbook : playbooks/install-packages.yml - name : Application Deployment Setup import_playbook : playbooks/application-deployment-setup.yml - name : Let's Encrypt SSL Setup import_playbook : playbooks/lets-encrypt.yml - name : Setup Nginx import_playbook : playbooks/nginx.yml Now run the script: $ mix ansible And we're done with Ansible! Your server is now ready to receive build commands and run releases. If we re-run mix ansible , you'll notice there were no changes and the process should take less than a minute to complete. Conclusion In this post we took an initial step towards automating deployment of Elixir applications. Automating this process may take longer than doing it manually, at least initially. However, in the long run it allows for faster iteration on configuration changes, and it allows us to blow away the machine and spin up another one in seconds. In addition, your Ansible files serve as documentation of what it takes to run your application in production. In the next post we will setup releases with Distillery and deploy them using eDeliver. Thanks for reading! At Hashrocket, we love Elixir, Phoenix and LiveView and have years of experience delivering robust, well-tested Elixir applications to production. Reach out if you need help with your Elixir project! Photo by Markus Spiske on Unsplash Was this post helpful? Share it with others. Tweet Share Post", "date": "2020-05-12"},
{"website": "Hash-Rocket", "title": "South Dakota v. Wayfair, Technology, and Your Business ", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/south-dakota-v-wayfair-technology-and-your-business", "abstract": "South Dakota v. Wayfair, Technology, and Your Business by\nJake Worth\n\non\nMay  7, 2020 If you sell goods or services online, you may have heard of the\nSupreme Court case South Dakota v. Wayfair, Inc. In this post, I'll discuss the\nimpact of this case on online goods and service providers from a technological\nperspective, and solutions I have helped implement for Hashrocket's clients. South Dakota v. Wayfair, Inc. was a Supreme Court case decided in June 2018\nthat impacts many businesses. I learned a lot about it recently and it's pretty\nfascinating. This summary on Oyez provides a nice primer; here's the summary: A state may require sellers with no physical presence in the state to collect\nand remit sales tax for goods sold within the state. A practical example: if I sell a React.js course online, and somebody purchases\nit in a state like Hawaii where sales tax applies, I may be required to collect\nHawaii sales tax on that sale. This upends a lot of assumptions about\ne-commerce, and businesses are scrambling to understand their exposure. Here is my description of the problem, followed by a solution I've helped\nimplement. Also, a quick disclaimer: any technical solution should be paired\nwith counsel from a tax professional. The Problem This ruling sounds simple, but the challenges it presents can be surprisingly\ncomplex. Here are a few: Not every state collects sales tax. Of the states that do, some collect\nsales tax for certain classes of goods and services, but not others. Is your\nteam ready to learn the intricacies of these rules for every US state,\nterritory, county, and city? Governments love sales tax, and they are currently racing to pass laws that\ngive them a bigger piece of the pie. Can your team remain conversant on these\never-changing laws? Do you sell enough items in a jurisdiction to qualify for taxation? Many\nstates are creating thresholds to determine who needs to collect tax.\nIf you don't sell enough, you don't have to collect sales tax. Do you have a\nsystem in place to track your sales by jurisdiction, and notify your team\nmembers when you have crossed these moveable thresholds? Do you collect payments through more than one provider? Some providers make\nit easy to attach taxes to the different types of things you sell, others do\nnot. The likelihood of an off-the-shelf integration existing between any\nspecific tax solution and every payment provider you use is low. Are any of your customers tax-exempt? Are they the correct class of\nexemption to be exempt from taxes on the type of thing you sell, in the state\nthey are purchasing from? Religious organizations, charitable organizations,\nand the federal government are just a few classes you'll need to consider. How would you roll out such a feature to existing customers? Do you have\nvalid data to determine how a customer should be taxed? How do you manage that\nrollout from a technical and UX perspective? Whew! These are non-trivial challenges. Their solutions must be tailored to\nyour business and deployed with precision. The cost of inaction, or failure, is\nhigh. Jurisdictions can levy penalties and interest for businesses that aren't\nin compliance. The Solution My team at Hashrocket has solved these problems before, and we can solve them\nfor you. I'd break our solution into a several major steps: choosing a\nprovider, backfill, historical data analysis, applying taxes to future\npurchases, and rollout. One of the most crucial early steps is picking the right tax solutions\nprovider. There are several in the marketplace, and each has its own tradeoffs.\nPart of our consulting practice at Hashrocket is acting as a CTO and helping\nyou evaluate service providers for the right mix of features. Picking the right\ntool matters. Next, we need to figure which taxes to apply to your existing customers. The\ndata we need, and the data you have, are brought closer together with a\ntechnique we call a backfill. Backfilling your customer's billable information\nmay be a big chore. Why? Online goods and service providers don't always think\nabout sales taxes and collect data accordingly when they launch their business.\nWe have a variety of techniques to transparently nudge users to give us their\nbillable addresses. We can't charge the sales tax unless we know where the\ncustomer is purchasing from, and given the proliferation of\nstay-at-home businesses, businesses without a billing address, businesses with\nheadquarters in multiple states, etc., there isn't one simple way to guess this information. Once we have the data, we need to validate it to ensure\nthat we can apply sales taxes to the addresses the customer has provided. What\nconstitutes a good, or good enough, address? Another service we provide is expert-level SQL database analysis to compare\nhistorical sales against state thresholds. You may already be selling enough to\nhave a tax liability! Use this knowledge to empower your team to plan and\ncommunicate effectively with the government. Next, we figure out which taxes might apply to future sales, and apply them. We\ncan build custom solutions in a variety of backend languages (Ruby, Elixir,\netc.) to analyze your sales in real-time and properly determine taxation. We\nthen can take this information and apply it to each of your payment providers\n(Stripe, PayPal, etc.), ensuring that the right taxes apply no matter how your\ncustomers pay. Finally, we are precise about how we roll out a feature like this. Existing\ncustomers who have never been charged sales tax before must be informed that\ntheir bill is going to go up. This isn't news they're going to like, so\ndelivering it effectively and preemptively answering their questions is worth\nthe effort. We might advocate rolling out the feature in stages, testing on\nexisting or new users, logging and triaging issues, and responding to feedback.\nAdditionally, our rigorous TDD\nmethodology catches and permanently fixes many issues before ever affecting a user. Getting this right helps\nus implement tax solutions that work and customers appreciate. Conclusion South Dakota v. Wayfair has shaken up many e-commerce business plans.  Waiting\non an off-the-shelf solution to this problem for your business could be costly.\nThis is when a consultancy like Hashrocket shines: we come in with expertise\nhoned on other projects and apply that expertise immediately to your problem\nwith little overhead. Contact us today and let's talk about South Dakota v.\nWayfair and your business. Photo by Scott Graham on Unsplash Was this post helpful? Share it with others. Tweet Share Post", "date": "2020-05-07"},
{"website": "Hash-Rocket", "title": "Automate Your Elixir Deployments - Part 2 - Distillery & eDeliver", "author": ["\nDorian Karter\n\n"], "link": "https://hashrocket.com/blog/posts/automate-your-elixir-deployments-part-2-distillery-edeliver", "abstract": "Elixir Phoenix Automate Your Elixir Deployments - Part 2 - Distillery & eDeliver by\nDorian Karter\n\non\nMay 14, 2020 In this post we will walk through the steps necessary to build an Elixir release using Distillery and deploy the release to production using eDeliver. This post builds on a previous post in which we set up a server, using Ansible, to accept build and deploy requests from eDeliver. In this post we will configure our release and deploy strategies using Distillery and eDeliver, and deploy our application to production. You can follow along using the companion repository on Github . Assumptions This post assumes you have a basic Phoenix 1.5 application without Ecto (to keep things simple). If you don't have one and want to create a new project you can do so with the following command: $ mix phx.new example --live --no-ecto The --live flag is optional. It will add Phoenix Live View support which is not necessary for this guide, but is supported in the Nginx configuration we added in the previous post. Initial Setup We will start by adding the necessary dependencies. From your project's root directory, open your mix.exs file and add the following dependencies: defp deps do [ { :distillery , \" ~> 2.1.1\" }, { :edeliver , \" ~> 1.8.0\" }, # ... ] end Save the file and run: $ mix deps.get Production Configuration Elixir comes with a config/prod.secret.exs file which we will use to load environment variables: # In this file, we load production configuration and secrets # from environment variables. You can also hardcode secrets, # although such is generally not recommended and you have to # remember to add this file to your .gitignore. use Mix . Config secret_key_base = System . get_env ( \" SECRET_KEY_BASE\" ) || raise \"\"\"\n    environment variable SECRET_KEY_BASE is missing.\n    You can generate one by calling: mix phx.gen.secret\n    \"\"\" config :example , ExampleWeb . Endpoint , http: [ port: String . to_integer ( System . get_env ( \" PORT\" ) || \" 4000\" ), transport_options: [ socket_opts: [ :inet6 ]] ], secret_key_base: secret_key_base # ## Using releases (Elixir v1.9+) # # If you are doing OTP releases, you need to instruct Phoenix # to start each relevant endpoint: # #     config :example, ExampleWeb.Endpoint, server: true # # Then you can assemble a release by calling `mix release`. # See `mix help release` for more information. Make sure to add a few more keys to the endpoint configuration at config/prod.exs : config :example , ExampleWeb . Endpoint , url: [ host: \" example.com\" , port: 80 ], cache_static_manifest: \" priv/static/cache_manifest.json\" , server: true , code_reloader: false , check_origin: [ \" //example.com\" ] Replace example.com with your domain. Configuring Distillery Distillery manages our releases, but before we can start using it we need to initialize a configuration file: $ mix distillery.init Output: An example config file has been placed in rel/config.exs, review it, make edits as needed/desired, and then run mix distillery.release to build the release Let's take a look at rel/config.exs (we only care about the production environment at this time): environment :prod do set include_erts: true set include_src: false set cookie: :\"uH$COG~8[4U9~An<?Ykj0ZqarRnjvxpFelB_1bkOya~mO{lY.sL!fkQS(;kqCCDp\" set vm_args: \" rel/vm.args\" end We are going to change the :prod configuration so that our Erlang Cookie is not committed to git. If you recall, in the previous post we created an environment variable called ERLANG_COOKIE , now we are going to read it from the environment variables. Replace the :prod configuration with: environment :prod do prod_cookie = fn -> cookie = System . get_env ( \" ERLANG_COOKIE\" ) || :crypto . strong_rand_bytes ( 64 ) :sha256 |> :crypto . hash ( cookie ) |> Base . encode16 () |> String . to_atom () end set ( include_erts: true ) set ( include_src: false ) set ( cookie: prod_cookie . ()) set ( vm_args: \" rel/vm.args\" ) end Configuring eDeliver Unlike Distillery, eDeliver does not provide an automatically generated configuration template via a Mix Task, but it does provide a template on the github README . At the the time of writing, it looks like this: # .deliver/config APP = \"myapp\" BUILD_HOST = \"my-build-server.myapp.com\" BUILD_USER = \"builder\" BUILD_AT = \"/tmp/edeliver/myapp/builds\" STAGING_HOSTS = \"stage.myapp.com\" STAGING_USER = \"web\" DELIVER_TO = \"/home/web\" # For *Phoenix* projects, symlink prod.secret.exs to our tmp source pre_erlang_get_and_update_deps () { local _prod_secret_path = \"/home/builder/prod.secret.exs\" if [ \" $TARGET_MIX_ENV \" = \"prod\" ] ; then __sync_remote \"\n      ln -sfn ' $_prod_secret_path ' ' $BUILD_AT /config/prod.secret.exs'\n    \" fi } We will take the default and modify it a bit to suit our needs. In .deliver/config , insert the following (feel free to delete the annotations): APP = \"example\" # We'll be using the production machine for building the app BUILD_HOST = \"example.com\" BUILD_USER = \"deploy\" BUILD_AT = \"/home/deploy/app_build\" # This is where the app will be served from PRODUCTION_HOSTS = \"example.com\" PRODUCTION_USER = \"deploy\" DELIVER_TO = \"/home/deploy/app_release\" # Automatically generate version numbers for builds # (https://github.com/edeliver/edeliver/wiki/Auto-Versioning) AUTO_VERSION = commit-count+git-revision+branch-unless-master # Copy the prod.secret.exs to the build directory pre_erlang_get_and_update_deps () { local _prod_secret_path = \"/home/ $BUILD_USER /app_config/prod.secret.exs\" if [ \" $TARGET_MIX_ENV \" = \"prod\" ] ; then __sync_remote \"\n      ln -sfn ' $_prod_secret_path ' ' $BUILD_AT /config/prod.secret.exs'\n    \" fi } # Source environment variables and build static assets pre_erlang_clean_compile () { status \"Build static assets\" __sync_remote \"\n    set -e\n    . /home/ $PRODUCTION_USER /example.env\n    cd ' $BUILD_AT '\n    mkdir -p priv/static\n    APP=' $APP ' MIX_ENV=' $TARGET_MIX_ENV ' $MIX_CMD phx.digest.clean $SILENCE npm install --prefix assets\n    npm run deploy --prefix assets\n    APP=' $APP ' MIX_ENV=' $TARGET_MIX_ENV ' $MIX_CMD phx.digest $SILENCE \" } # symlink static assets to the release location after deploying a release symlink_static (){ status \"Symlinking statics\" __sync_remote \"\n    set -e\n    cp -r $BUILD_AT /priv/static $DELIVER_TO / $APP /releases/ $VERSION /static\n    ln -sfn $DELIVER_TO / $APP /releases/ $VERSION /static $DELIVER_TO \" } post_extract_release_archive () { symlink_static } post_upgrade_release () { symlink_static } # Temporary workaround from https://github.com/edeliver/edeliver/issues/314#issuecomment-522151151 # start_erl.data is not being upgraded when new release is deployed # should not be necessary once a new distillery version is released (> 2.1.1): # https://github.com/bitwalker/distillery/issues/729 post_extract_release_archive () { status \"Removing start_erl.data\" __remote \"\n    [ -f ~/.profile ] && source ~/.profile\n    set -e\n    mkdir -p $DELIVER_TO / $APP /var $SILENCE cd $DELIVER_TO / $APP /var $SILENCE rm -f start_erl.data $SILENCE \" } The insertions we made above included: Code for compiling the static assets on the server (see steps in the pre_erlang_clean_compile callback). Code for symlinking the compiled assets into the release directory, so that we can rollback releases and each release has a copy of its compiled assets. A workaround to address a known issue in Distillery that is expected to be fixed in the next version (> 2.1.1) - you should be able to remove the post_extract_release_archive callback once the issue is fixed. As always, modify the code to match your project name and domain. Building The Release Now that eDeliver is set up, we can build a new release! In the project's root, run: $ mix edeliver build release We should see an output resembling this: BUILDING RELEASE OF EXAMPLE APP ON BUILD HOST -----> Authorizing hosts\n-----> Ensuring hosts are ready to accept git pushes\n-----> Pushing new commits with git to: deploy@example.com\n-----> Resetting remote hosts to 26b6853c000982d80466818c7a1028e392b60483\n-----> Cleaning generated files from last build\n-----> Updating git submodules\n-----> Fetching / Updating dependencies\n-----> Build static assets\n-----> Compiling sources\n-----> Generating release\n-----> Copying release 0.1.0+1-26b6853 to local release store\n-----> Copying example.tar.gz to release store RELEASE BUILD OF EXAMPLE WAS SUCCESSFUL! We will need to copy the automatically generated version number of the release (in the output above it is 0.1.0+1-26b6853 ). The release that was generated on the server will be copied locally to .deliver/releases , so make sure to add that to .gitignore : $ echo '.deliver/releases' >> .gitignore This is important to do, especially on public repositories, since Elixir compiles secrets into the release. Deploying The Release Using the version number from the build release command, run the deploy to production command: $ mix edeliver deploy release to production --version = 0.1.0+1-26b6853 We should see the following output: DEPLOYING RELEASE OF EXAMPLE APP TO PRODUCTION HOSTS -----> Authorizing hosts\n-----> Uploading archive of release 0.1.0+1-26b6853 from local release store\n-----> Extracting archive example_0.1.0+1-26b6853.tar.gz into /home/deploy/app_release\n-----> Removing start_erl.data\n-----> Starting deployed release DEPLOYED RELEASE TO PRODUCTION! Now to start the deployed release run: $ mix edeliver restart production When you visit your domain you should see your Phoenix application running! The full code for the application and deployment scripts can be found here . Conclusion Deploying Elixir to bare-metal does not have to be scary, and if you automate the process, iteration can be a lot faster and repeatable by your team. At Hashrocket, we love Elixir, Phoenix and LiveView and have years of experience delivering robust, well-tested Elixir applications to production. Reach out if you need help with your Elixir project! Photo by Gareth Davies on Unsplash Was this post helpful? Share it with others. Tweet Share Post", "date": "2020-05-14"},
{"website": "Hash-Rocket", "title": "Managing Tmux Sessions", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/managing-tmux-sessions", "abstract": "Managing Tmux Sessions by\nJake Worth\n\non\nJuly 23, 2019 Tmux is a favorite tool at Hashrocket. In this post, I'll share some commands for managing your Tmux sessions. Hashrocket is known as a Vim shop ,\nand Vim pairs well with Tmux , the\nopen-source terminal multiplexer. Combining these tools creates an environment\nsimilar to an IDE, but in the terminal, which has advantages. Sessions are the workspaces of Tmux. Keeping them alive and separate is like having a woodshop with a\ndifferent bench for each project you're working on, ready to go. Managing these sessions is a meta layer of Tmux, something we think about a lot at\nHashrocket. The tips below are a collection of some of our favorite commands, culled from Today I Learned and conversation. Throughout this list, I'll be using <prefix> + to reference the Tmux leader and whatever comes afterward.  Tmux's default leader is CTRL + b ; at\nHashrocket we've mapped it to CTRL + z . For more information on Hashrocket's\nTmux configuration, check out our tmux.conf dotfile. Create a Named Tmux Session Creating new Tmux session from the command line grants a default name of 0 to the session: $ tmux new We can take this opportunity to pick a good name via the -s flag: $ tmux new -s tiny-rust-app Rename Your Tmux Session Didn't follow the previous tip? That's okay; we can rename our session. Pretty much every Tmux command has a long version and a short version. Like\nmany tools, the long version has a human-readable name like\n'list-sessions', and the short version is just a symbol or number. I think you\ncan coast for a long time just knowing the long version, or guessing at the long version.\nMemorizing the short version makes sense in some cases. That said, here's the short command to rename your session from inside Tmux: <prefix> + $ Replace the existing name with the desired name, tiny-rust-app for example,\nand hit enter. List All Tmux Sessions List your Tmux sessions with: <prefix> + s From here, you can navigate the list with j and k , and switch by hitting enter. Jumping Between Tmux Sessions Was the previous tip too slow for you? I sympathize. Tmux also provides the <prefix> + ) and <prefix> + ( bindings as a way of jumping to the next and\nthe previous session, respectively. Boot Other Users from Inside Tmux When other people attach to your Tmux session, the window shrinks to fit the\nsmallest screen, filling in the gaps for everyone else with dots.\nLess real estate makes reading and writing code more difficult. While your friends are out\npurchasing bigger displays, kick them off your session: <prefix> + D This opens an interactive list of all connections to the current session,\nand their dimensions. You want to\nkick off the users with smaller dimensions. Navigate to your selection and hit enter, or q to back out. Load Tmux and Boot Other Users An alternative to the previous command is to load the session while forcing all\nother sessions to detach, via the -d flag: $ tmux attach-session -d -t tiny-rust-app To quote Josh Branchaud : By detaching all other sessions, you are ensuring that your machine's\ndimensions are the ones that Tmux uses when drawing the window. This is a\ngreat quick fix if you're working on your own, but probably not what you want\nto do if you are in a pair programming situation. Kill Your Current Tmux Session <prefix> + :kill-session Kill All Other Tmux Sessions Okay, you can kill your current Tmux session, or kill it from the command line via $ tmux\nkill-session -t tiny-rust-app . Did you know you can add a flag to kill every\nsession other than your target? Here's that command. $ tmux kill-session -a -t tiny-rust-app\n$ tmux kill-session -at tiny-rust-app # short version All sessions but the target will die. Kill All Tmux Sessions Here's great command when you want to go nuclear. Kill all sessions with: $ tmux kill-server Conclusion Managing sessions can transform your Tmux experience into something sublime. I\nhope you found a new tip or two here, and please let us know your favorite\nTmux session commands on Twitter . We want to know how you Tmux! Special thanks to Dorian Karter, Josh Branchaud, Josh Davey, Chris Erin, and Andrew\nVogel for sharing these insights with the team via Today I Learned. Photo by Jeff Sheldon on Unsplash Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-07-23"},
{"website": "Hash-Rocket", "title": "Adventures in Mentorship", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/adventures-in-mentorship", "abstract": "Adventures in Mentorship by\nJake Worth\n\non\nJanuary  3, 2019 This fall, I participated in the formal mentorship program at the Code Platoon\nbootcamp here in Chicago. In this post, I'd like to talk about my experience as a\nmentor. For the past few years, myself and a few Hashrocket colleagues have taught a\nworkshop at Code Platoon , focusing on test driven development, Ruby, and\nPostgreSQL. As a veteran, it's one of my favorite days of the year, and I'm\ngrateful Hashrocket supports this outreach. When Code Platoon put out a call for mentors this Summer to\nwork with the upcoming cohort, 'Golf Platoon', I decided to give it a try. Why Mentor? My drive to mentor comes from the fact that I have been mentored myself by many\ngreat people. These advisors helped me find opportunities and direct my\nprofessional growth. From the beginning, I hoped someday to be able to provide that service to others. I visualized getting something out of it for myself, too. Being around new programmers\nis refreshing; they ask great questions,\nchallenge conventions, try creative solutions. They often work with the\nlatest technology, or old technology you might have written off. I hoped that\nbeing around a junior developer might expose me to some new things, help me\nrefine the opinions that I take for granted, and expose blind spots\nin my own career. Prep To prepare for this engagement, I took some time to think about what I wanted\nthe experience to be, summarized in this mission statement: As a mentor I will strive to listen, be an advocate, and be an inspiration. Code Platoon sent me a survey where I could indicate the level of\nintensity I wanted out of the relationship; I settled on level 10 because there\nwas no level 11 on the survey. My mentee took the same survey and we were\nmatched up based on compatibility. I attribute much of our chemistry to this\nsurvey. Our Sessions My mentee and I met for an hour, once a week, for about three months. Before long, I\nstarted to truly look forward to our time together. Sometimes we talked about code; sometimes we talked about the industry;\nsometimes we blew off steam talking about movies and travel. The students have\ninstructors and TA's help with the technical work, so I tried to stay focused\non the bigger picture. What's it like to be a programmer? What are some of the\njoys and pitfalls of the field? What sort of obstacles was he facing that I\ncould eliminate, or explain? When we did code, we reviewed his projects, played coding games, and practiced\nmy favorite brainstorming activity, writing on the whiteboard. A cheap thing that I'm proud\nof: I bought my mentee a copy of 'The Pragmatic Programmer', a book that has greatly\nimpacted me. Next Steps Working with my mentee was gratifying: I helped somebody else, and I learned about\nmyself. The next cohort, Hotel Platoon, starts in January, and I've signed up to\nmentor again. If mentorship interests you, do it! It's one of the most powerful\nways to give back to the profession. Like any group not loudly represented\nin tech, veterans benefit from seeing a path to success that somebody else has\nwalked before. Photo by Nik MacMillan on Unsplash Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-01-03"},
{"website": "Hash-Rocket", "title": "Pest Control: How We Manage Bugs", "author": ["\nSuzanne Erin\n\n"], "link": "https://hashrocket.com/blog/posts/how-we-manage-bugs", "abstract": "Process Pest Control: How We Manage Bugs by\nSuzanne Erin\n\non\nJune 18, 2019 There is no such thing as bug-free code. Even code that was functional on delivery can develop issues over time . Part of owning and maintaining software is always being prepared to address bugs. We often work with clients who are new to software development, so how to test for and report bugs is a perennial topic. Here’s how we like to work with stakeholders to catch and resolve issues. The Value of a Detailed Bug Report If you are reporting a bug, the most important thing to be able to do is explain how to reproduce the issue. Often, the majority of the time that a developer spends on resolving a bug is spent just trying to replicate it. Once an issue can be reproduced, then the developer can track down causes and test fixes. Giving the developer as much information as possible up-front in a detailed bug report can save valuable time, and cut down on time lost on back-and-forth questions. How To Report a Bug Give a step-by-step summary of the actions that lead to the issue occurring. If you find it hard to describe, capturing a video or doing a screen-share can be helpful. Confirm the intended behavior. For example, saying “this link should go to the Contact Us page” is more helpful than “this link is broken”. Provide technical information. Include information about the device, operating system, and browsers you being used, along with their respective versions. For design-related bugs, including screen size and resolution may be useful. This information isn’t always top-of-mind or easy-to-find for the person reporting the bug. In that case, https://mybrowser.fyi is an easy link that generates a report of all this information with a single click. If you can, investigate if the issue happens in other contexts. Knowing the situations in which the bug does or does not occur can be valuable information. Other information you can include in your bug report is: Is it intermittent or consistent? Does it happen in other browsers or on other devices? Does it happen if you start in a private browser window? Write your bug report in a clear and easy to understand format. Here is an example of a format we like to use for reporting issues: Steps to reproduce  \n    1.  \n    2.  \n    3.  \n\nExpected Behavior:  \n    Given I'm a [user type]  \n    And I [am in this situation]  \n    When I [take an action]  \n    Then [this desired response happens]  \n  \nActual Behavior:  \n    Given I'm a [user type]  \n    And I [am in this situation]  \n    When I [take an action]  \n    Then [this wrong thing happens]   \n  \nNotes: How To Catch Bugs Early and Often At Hashrocket, we follow processes to reduce the number of issues that make it to a stakeholder or a user in the first place. Here’s how: Pair Programming If we can, we like to pair program, which means that a second set of eyes is “reviewing” the code as it is being written. Two heads are better than one when it comes to implementing code that is more likely to stay stable and maintainable. Test Coverage We use test-driven-development ( TDD ) so that there is a high level of unit and integration test coverage, with continuous integration systems to enforce tests and prevent breaking deployments User-testing on Staging We set up a staging environment where both developers and stakeholders can test features as users. This serves as the acceptance process, to confirm the delivered feature meets the stakeholders’ expectations. It also provides an opportunity to try out other user interactions that might not have otherwise been thought through. For example, what if the user decides to exit partway through a workflow? If we didn’t catch these interactions in storycarding , then a thorough testing process can catch issues related to these interactions in staging before they make it to production. Agile Development Practices We develop software incrementally and iteratively, using an Agile process. Working on features that are broken down into small parts one-at-a-time allows for increased quality control. Delivering updates incrementally decreases the risk that any issue that makes it through will be show-stopping, and issues are easier to trace back to their source. Managing Technical Debt Even after a feature is built, tested and pushed to production, technical debt can lead to problems as the software grows. Adopt development strategies to maintain your application, and reduce the number of issues that arise over time. Image by unsplash-logo Austin Ban Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-06-18"},
{"website": "Hash-Rocket", "title": "Open Source Time at Hashrocket", "author": ["\nChris Erin\n\n"], "link": "https://hashrocket.com/blog/posts/open-source-time-at-hashrocket", "abstract": "Hashrocket Projects Open Source Time at Hashrocket by\nChris Erin\n\non\nJuly  9, 2019 It's Friday afternoon, my brain cells are fried, my stomach is full and it's time for Hashrocket's most cherished employment perk. That time when we can open  whatever editor, on whatever operating system, on whatever computer, with whatever keyboard and work on whatever we want. Open source time. We call it open source time, but don't get it twisted, it's not time to work on open source software, but instead it's the time itself which is open sourced. You can source it any way you want, openly.  And no, I'm not really sure what that means.  All I know is that my mind is free to stretch .  I can crack open the code to Phoenix, I can write a weird blog post and I can scratch that programmer itch that's been itching ever since I found out about useMemo on Monday.  That combined with an itch to make colorful squares move across a web browser at different speeds .  That combined with a wholly necessary need to find out why my vim plugins won't let me navigate around an Elixir project as fast as I want and a realization that I don't have the latest node version. So I crack open github and think about all the better ways that I could find out what the latest version of node is, like, shouldn't there be an alert system for that?  And I read the node changelog, and I look up for the millionth time how to install a plugin using asdf .  Woah, can I hook up asdf plugin updating to xargs with the -P flag so I can update all my dev tools using every core of my processor? Maybe I got something done, maybe I didn't, but I itched and stretched and had fun exploring the technical world.  I got to exercise a different part of my programmer brain instead of trying to jam out one... last... feature for the client during what is typically the least productive and most bug heavy time of the week.  Doing this makes me a better programmer.  This makes me a better programmer for the following Monday and doing this consistently every Friday makes me a better programmer for the following year. Over time, you find that not only is your programmer self rejuvenated but you actually amass a fair number of interesting experiments.  Not all of them are ready to show off, to either the full internet or a limited number of people, but some are.  You can check out a small sample of our open source time projects at our concepts website.  As developers, we're multifaceted and it's great to be reminded that not only can we developer with efficiency and quality for our clients but we can also be creative, whimsical and curious. Whether it's creative fulfillment, tool sharpening, free discussion about hard problems, or whatever else that might happen when you have a computer in front of you and smart people around you, open source time is one of the things that makes me tick as a programmer and a gratifying employment perk that allows me to start the following Monday fresh and ready to once again provide the greatest possible value for my client. Image via unsplash-logo Zachary Peterson Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-07-09"},
{"website": "Hash-Rocket", "title": "Full Stack", "author": ["\nChris Erin\n\n"], "link": "https://hashrocket.com/blog/posts/full-stack", "abstract": "Full Stack by\nChris Erin\n\non\nJuly 16, 2019 From transistors to typography.  From silicon to selectors.  From Layer 1 to Layer 7 of the OSI model . The full stack in Full Stack is big, really big.  You've got to know a lot to create a website from scratch . If you wish to make an apple pie from scratch, you must first invent the universe. — Carl Sagan The individual that can mine the rare earth minerals, design the chip, code an operating system, connect it to an outside network, code some server software and also design human facing interface matching our 2019 standards; that person doesn't exist.  You aren't that person.  No one can do that, just like in the 1950s no one cut down trees, made paper from pulp, created a camera, took great photos and wrote great content for a world-class magazine.  The number of things humans must do and do well to get information to the masses in the 1950s was impossibly deep and since then civilization has become much more economically specialized and diverse.  The true Full Stack is indeed full and growing. You can create an HTML page though.  You can open an editor, put some html and text into it, say something profound, upload it to Go Daddy Dot Com and now something valuable to the world is on the web.  Congrats you're a Full Stack Developer. From expansive to reductive, there is lots of room in Full Stack. It's become a badge of honor, a status signifier, a resume classification, but is it what you want to be? The term Full Stack is a web development industry construction.  Incredibly, the discipline needed to translate information into readable pages via html and css and the discipline of structuring data, routing events, and designing logic have diverged.  Two Ends, Front and Back.  Two Sides, Client and Server.  If you can do a little of both, that's Full Stack.  \"Full Stack Developer\" though, that's a heavy term. If you can do everything necessary to get a modern web app into production, you are Full Stack.  If you can write the html, css and javascript for a feature as well as adding the necessary database columns, you are Full Stack.  In same cases this might be easy, in others hard.  But Full Stack never means that you know everything about every component of your solution.  How can you?  Chips are complex, Operating Systems are complex, server configurations are complex, even things that seem simple, like expanding user-select: none to -moz-user-select: none have very sophisticated solutions .  Full Stack isn't as much a description of what you know as how willing you are to adapt. Full Stack Developer can be a harsh career.  By dismissing specialty you insist on your world being a massively wide field of ever changing elements. Without having the time of day to keep track of all the changes! There is always that illusory point in a developer's career where they have learned everything needed to do get everything done. It feels like conquering the world knowing how all the parts of a particular technology stack fit together and how to assemble them. It's a sham feeling, however, and it's a shock when you stick your head up and read that your language added generics , I got along fine without generics, why do I need this!  Or that CoffeeScript is a dead language, I invested time in a dead language! I was managing my state fine before Redux! CVS is fine !  Why would I need a package manager for JavaScript?  Why would I need a different package manager for JavaScript? So here's my definition of Full Stack developer.  A Full Stack developer is a developer that has experienced the world moving past them enough times to never be shocked and that is always ready to learn something new in any part of the stack that matters. It's a cliche that change is inevitable, pick your own Change is Inevitable Quote .  While you're looking to the West change will come from the East.  While buried on your challening database migration, a new JavaScript paradigm will emerge.  While you are writing an ansible script for deploying to AWS EC2, containerization will rock the devops world. All the best Full Stack Developers I know have built learning into their lifestyle and are looking for the next new thing, whether that's a whole new language or a minor point release.  At Hashrocket, this is something we do by incorporating Open Source Time into our work week.  Some people listen to industry podcasts on their commute, some people read Hacker News to try to keep up.  What is important is that Full Stack Developers find a way to consistently prepare themselves for change. Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-07-16"},
{"website": "Hash-Rocket", "title": "Incident Report: Like Attack", "author": ["\nChris Erin\n\n"], "link": "https://hashrocket.com/blog/posts/like-attack-postmortem", "abstract": "Hashrocket Projects Incident Report: Like Attack by\nChris Erin\n\non\nJune 11, 2019 On May 13th, til.hashrocket.com (TIL) experienced an attack. This attack had the effect of overstating the relative amount of affection that the users of TIL felt towards the posts on the front page of TIL. While most of our developers will bask in the glow of any and all affection, either automated or not, the unanticipated scale of affection combined with the notifications that our slack channel receives on relatively small increments of \"likes\" both chafed at our sense of truth and also served as a Denial of Chat (DOC) attack in our main chat channel. The root cause of this attack was a script making requests from a computer in Dearborn, Michigan written by an author who had discovered that there is no limit to the amount of liking that can be expressed on TIL through clicking and post requests. Timeframe May 13th 5:47 AM America/Chicago First Evidence of Attack - The Hashrocket General channel received a notification that post Values clause in a select statement had 10 likes, subsequent to that notification, the channel received another notification that post Simulate componentDidMount with a useEffect also had 10 likes May 13th 6:02 AM America/Chicago TIL Slackbot Integration Removed - To stop the DOC attack, the only employee working at this time removed the slack integration. May 13th 6:30 PM America/Chicago Like Functionality Taken Down - Without anyone to address this issue until after billing hours, and with no way to block the requests, the decision was made to disable the like functionality and to restore a backup database from Sunday May 12th. May 22nd 9:00 AM America/Chicago Like Functionality Restored - After crowdsourcing a hivemind solution we were able to implement a reasonable limit to our like functionality without having to severely restrain users that are truly thankful for the posts that we write.  Like functionality was down for a total of 207 hours. Root Cause Ever since the inception of TIL we've wanted to allow users to express appreciation without the usual internet barriers.  We don't think a user should have to sign-in to our platform just to \"Like\" a post, nor do we want to build sign-in functionality.  This no-sign-in-for-appreciation functionality is at odds with most of the internet, which views the desire to casually interact with a system as a chance to harvest an email address, the harvesting of such being just one of many ways to monetize your behavior. As such, we placed no restrictions on how much you could like a post on the backend.  In the past, we've had co-workers who demonstrated how this could be taking advantage of by writing a shell script utilizing curl to post to the \"Like\" endpoint until their post was the most liked and at the top of the \"Most Liked Posts\" list on the stats page .  These co-workers always had the good manners to reverse the untruthful and automated likes. On the front-end we restrict \"likes\" by storing the liked posts as cookies.  When the user comes back to the site, we reference the cookie through JavaScript to determine if the user has liked a post before, and render the \"like\" in the \"liked\" state.  When in the \"liked\" state the user is not able to re-\"like\" that post unless they \"unlike\" it first.  When first implemented this feature, we stored each hash of a liked posts in one cookie.  Cookies however have a character limit, and some power users of TIL (it was me, your author, a power user) ran into that limit while expressing enthusiasm for their co-workers' posts.  Subsequently we switched to one cookie per \"liked\" post. We've always known that without a hard server restriction someone could write a script to indiscriminately, callously and maliciously \"like\" posts.  What we did not know until the morning of May 13th, was that someone would actually do this.  The script was actively loading the page and clicking the button.  We know this because our Google Analytics spiked during that time period and in particular showed a spike of traffic from an ip address in Dearborn, Michigan.  Without loading an entire web page, the Google Analytics script would not be requested. We are not ruling out that this could have been the indiscriminate work of a web crawler, but another curious and lazy feature of this attack was to only like posts on the first page.  There are 50 posts on the front page, and only those posts were exposed to the excessive affection.  We think that it is unlikely that a web crawler would not move on to the second page. Resolution and Recovery We take periodic database backups. This is a free feature of our Heroku supported database.  We were able to restore the data to a state that was prior to the attack without losing any posts.  This enabled us to get back to a state where all of our posts had the same number of likes that we had before the attack. We also made the decision that we wouldn't restore the \"Like\" functionality if it would allow for the same disruptive behaviour.  The \"Like\" feature is beloved in our organization. For a little bit, we considered removing it for good, and perhaps relying on Twitter or other social media for our endorphonic feedback. Eventually we determined that we wanted users to express appreciation on their terms and not on the terms of third-party gigantic negativity infested platforms. Our solution was to place a soft rate limit on the \"Like\" functionality.  TIL is an Elixir app which offers some nice in-memory data stores like GenServers and we took advantage of that functionality to create a custom rate limiter . Conclusion In the end, this was a slightly annoying, slightly amusing incident that exposed some purposely-left holes in our application that hadn't been exploited in four years.  We were able to turn it into something positive by using it as an opportunity to write a simple rate limiter, increasing the scope of TIL as an example application for Elixir and Phoenix. Thank you for bearing with us and please remember to like a TIL post if you think it deserves recognition! Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-06-11"},
{"website": "Hash-Rocket", "title": "Hashrocket Guide to Storycarding", "author": ["\nSuzanne Erin\n\n"], "link": "https://hashrocket.com/blog/posts/hashrocket-guide-to-story-carding", "abstract": "Process Hashrocket Guide to Storycarding by\nSuzanne Erin\n\non\nApril 25, 2019 At Hashrocket, we begin new projects with an in-depth storycarding session. It's how we turn client vision into development-ready requirements. It allows us to immerse ourselves in the client’s business domain, define the overall interface and functionality of the application, walk through user work-flows, and discover edge cases. In this blog, I’ll talk about what storycarding is and what it takes to start a session off on the right foot. What is storycarding? What are the requirements for a storycarding session? What are the parts of a storycarding session? 1. Discovery 2. Epics 3. User Stories 4. MVP 5. Prioritization Successful Storycarding What is storycarding? Storycarding is a 2-3 day session where we work together with a client to define in detail the requirements of an application. We sit down together in person and talk through the client vision, capturing features in the form of story cards. If a designer participates in the session, they will produce an analogous set of wireframes. Along the way, we’ll challenge ideas, provide our input, and identify concise and measurable goals. We approach application design from the user’s point of view. Based on conversation, we’ll turn each user-need into a fully fleshed-out feature set. This outside-in technique allows us to build lean and efficient applications by requiring each feature to be justified by its value to a particular user role. The client will leave the session with an extensive set of stories defining the software’s functionality, ready to begin design and development. This is a snapshot of what the needs of the application are at this point in time. Because the stories are detailed and granular, when requirements change, we can easily adjust specific areas as needed later. >> View the Guide to Storycarding visual summary of the requirements and steps described in this blog. What are the requirements for a storycarding session? Time: Depending on the size of the project, we want to dedicate 2 or 3 days to this process. Proximity: The most successful storycarding involves everyone being in the same room together, the developers, project manager, and any designers with key client stakeholders. For us, at this initial stage of the project, it is an essential way to build rapport, context, understanding, and collaboration. Mindset: We’ll come ready to immerse ourselves in the client business domain, put ourselves in the users' shoes, and collaborate on features. We’ll ask the client to be open to scope challenges and alternative approaches, and to think about a “Minimum Viable Product” . Tools: The room will have a big screen on which everyone can see what is being written or drawn on someone’s laptop. The room will also have a whiteboard, so that anyone can get up easily to sketch out what they are thinking. We may have some oversized sticky post-its or easel pads and markers within reach to keep track of key ideas. What are the parts of a storycarding session? 1. Discovery: Business Context and User Needs Before we start writing stories, we will have a discovery conversation to understand the big picture around the app, including business goals and target users. We confirm what the app is supposed to do, who its target is, and what core problem it is aiming to solve for that target. We especially want to make sure that we have a deeper understanding users are and how the app will address their needs. By defining the user needs together with the client, we will form the foundation on which we will build the storycarding session. 2. Epics: Connecting User Needs with Functions Once we understand the goal of the application, then we capture the core functionality needed to achieve that goal in one-line descriptions. The descriptions are written in a way that connects the function with a user need. Imagine a web app that sells textbooks to college students. One such description for a purchase feature might look something like: As a student, I want to be able purchase a textbook, so I that I meet the requirements for my class We’ll put these descriptions up somewhere where we can easily see and reference them later. These are our “Epics” -- descriptions of big function areas of the app -- that are an umbrella over the smaller interactions that make it up. 3. User Stories: Breaking Down Interactions After we’ve identified our epics, we work together with the client break each one down into individual interactions. These are each step that a user would perform to fully engage with a given part of the app. These become our stories. By writing them together on them with the client, they represent an agreement that we are all on the same page about the functionality of the app. This process forms the core of the storycarding session. In our textbook app example, some story titles for the “Purchase” epic might include: Student adds textbook to cart  \nStudent views cart  \nStudent adds payment information  \nStudent submits transaction  \nStudent receives confirmation Each story is given a description that is also framed in use oriented “Given/When/Then” language (read more about story writing here ). The description for the story Student adds textbook to cart might look like: Given I’m a student  \nAnd I’m viewing the details page of a particular textbook  \nWhen I click on the “Add To Cart” button  \nThen I see that the number by the cart icon in the header increments by 1 Using this user-oriented format is important to us for a few reasons: It keeps us in the mindset of the user experience -- what exactly they will see and do -- so that we don’t leave any aspects of that experience unexplored. It makes the requirements testable, so that the developers can ensure high test-coverage through test-driven development. Stakeholders can easily use this story as acceptance criteria. They will be able to walk through the user’s experience of a feature step-by-step to confirm that it is working as envisioned. Beyond the primary user-flow, we’ll also anticipate other user behaviors and edge cases. We will collaborate on how the app should respond in those situations.  An example of a story outside the primary user-flow might be Student exits check-out before completing transaction . 4. MVP: The Simplest Way to Do It Throughout the process of writing stories, we’ll work to foster an “MVP Mindset”. MVP stands for Minimum Viable Product - the simplest version of the app that achieves the core goal of the product while providing business value. One way we’ll do that is by looking out for potentially “risky” app interactions. These are features that look simple on the surface, but can end up being time-sinks that outweigh the value they provide. We’ll think about: What information is required in order for this interaction to work? What is the relative time and effort needed to build the interaction? What is the value that it provides to the business or user? Is there a simpler way to achieve the goal? When we identify a risky interaction, we collaborate with the client to determine the simplest solution that still achieves the goal of the feature. We may write stories both around the simple solution as well as the more complex implementation, tagging the latter story as Phase 2 . This allows us to focus on the items that are core to the MVP, deferring scope enhancements until after launch. There may be other feature ideas that come up that don’t build toward the core goal of the product. We make it clear to clients that we will challenge them on scope by asking: can the app stand on its own and provide value without these features? For our example textbook app, here are some stories that we may push back on: Student adds textbook to favorites list  \nStudent rates textbook  \nStudent leaves a review While these features will be great for expanding and maturing the app, we could still launch a textbook store without them at first. Depending on how important these stories are to the client, we will either defer the writing of these stories until needed, or tag them as Phase 2 . 5. Prioritization: Putting Things in Order Our 2-3 days of storycarding will typically provide about 6-12 weeks of stories, depending on the scope of the MVP, the number of developers, and the complexity of the work. This may not fully describe the entire product vision, but it could represent an initial phase. We limit ourselves to that many stories at a time due to the nature of software: anything that can change usually will. In a few months, there are so many implementation decisions made along the way, that requirements written three months ago will no longer be relevant. In storycarding, we can still capture long-term goals, epics, and maybe a few basic story titles as reminders for the future. After we’ve written the stories, we work with the client to put them in order of importance, which is the order of development. That way, we are always working on the next most important features. Any scope that is beyond the MVP can be quickly identified and prioritized for after the initial release. If there is a strict budget, we are sure to remind the client that lower priority stories may or may not get done depending on how rapidly we progress through the backlog. The client may not have the budget for everything they want, but this empowers them to focus on what is most important. Because the stories are granular, we can easily re-prioritize or edit them if what is important changes. We will push for the app to be released as early as possible. This allows the client to build a user base, start generating value, and begin receiving real user feedback sooner rather than later. While that is happening, we can continue to build upon that foundation in priority order, knowing that we can stop work at any point and they client will still have a working application. The client’s risk is minimized, and they have the opportunity to iteratively build towards a tool that users will truly value. Successful Storycarding At the end of storycarding, the client will walk away with tangible artifacts: an extensive set of stories defining the software’s functionality, ready to begin design and development. Intangibly, we'll know that the session has been successful if we have built a foundation of trust, collaboration, and confidence with our client that we will build with their business goals and user's needs in mind. We'll have established a pattern of successful communication that will cary through to how we'll work together on the project. Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-04-25"},
{"website": "Hash-Rocket", "title": "Pick a Good Name", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/pick-a-good-name", "abstract": "Pick a Good Name by\nJake Worth\n\non\nMarch  5, 2019 I've been fortunate to work on some long-running projects, and they always confirm the importance of choosing good names. In this post, I'd like to talk about my thoughts on what makes a name good. There are only two hard things in Computer Science: cache invalidation and naming things.\n-- Phil Karlton The importance of naming things seems abstract until you've done it badly a\nlot. Imagine a function like this: def a ( b , c ) d = b + c d end This is often called 'golf code' because it requires few keystrokes to\ntype, relative to something more verbose. It's impossible to maintain.\nOnly the person who wrote the function knows what a , b , c , or d represent. Everybody else has to load the domain into their head every time\nthey see it. Bad names make it difficult to search through the code. They make it difficult to talk about the code, because the name confers inaccurate meaning, or no meaning. They make it difficult to refactor or change behavior, because the name becomes\na handle. Like a handle on a basket, when you've mentally reached for it enough\ntimes in a certain way, you become trained to do that forever. In one application I've recently worked on, we had a domain concept, a core type\nof user, that was hard to understand. Why? In our code, it had three\ndifferent names. One of those names represented the concept's internal meaning; the other two described different business terms for the concept that had evolved. This complexity is the problem. Let's focus on solutions: what makes a good name? In my opinion, a good name is\nthe only name that exists in the system, it is meaningful to the system, and\nit is informed by multiple perspectives. The Only Name This is the most important of the three points. A domain concept gets one\nname in your system. This takes work to maintain in a big project with multiple contributors. Name hacking should be discouraged; it's better to have one imperfect name than two. If you find yourself with a different opinion, you might\nhave an overloaded idea that needs to be broken apart. Stick to this rule, and everything else will be simpler. Meaningful to the System What does it mean for a name to be meaningful to the system? It means naming it something that you can understand, and not inheriting a mental model from somewhere else. This can be achieved with aliases or local variables,\nconcepts that are core to almost every language and framework. Take this GraphQL query: allUsers: allCustomersWhoUseOurProductsAPIRefactored {\n  edges {\n    node {\n      streetAddress: main_street_address\n      city: town\n      state: state_or_province\n      zipCode: zip_or_postal_code\n    }\n  }\n} On the right of the colons are the names defined by our third-party GraphQL API. On the left\nare the names that we want to use: cased for JavaScript, and meaningful to\nour system. This is a line in the sand: one side represents concepts from the outside\nworld, on the other are the names we need to get stuff done. Informed by Multiple People Names have significance that is unique to each person.  They're informed by the\nlanguages you speak, where you grew up, and your opinions. When choosing a name, I find pair programming invaluable. A pair can help\nme see that a concept is deeper or shallower than it first appears, and pick a better name. When not pairing, I'll workshop the idea with other developers on\nthe team, or search other codebases for relevant conventions. I want my name to be as good as it can be. This takes more time. But I think in the end, it leads to code that can scale.\nScale beyond yourself, or your ten-person company. Conclusion Take the time to pick out a great name. Spend as much time on that as anything\nelse you do when writing code. Make it the only name, meaningful to your\nsystem, and informed by other people. Future developers will thank you. Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-03-05"},
{"website": "Hash-Rocket", "title": "The Hashrocket Chicago Apprenticeship", "author": ["\nChris Erin\n\n"], "link": "https://hashrocket.com/blog/posts/the-hashrocket-chicago-apprenticeship", "abstract": "The Hashrocket Chicago Apprenticeship by\nChris Erin\n\non\nMay 16, 2017 The Hashrocket apprenticeship is currently open in Chicago.  We hire one apprentice at a\ntime and typically only one per year.  We receive many applications for this\nposition every week, but few are qualified, because few have the professional\nexperience in Ruby on Rails we require.  You may read this and think \"Yes this\nis for me\", if so, please apply . Our Apprenticeship Program is not a First Job It's not really an apprenticeship.  \"Apprentice\" is the easiest term to\ndescribe what we have, which is an opportunity to learn web development and\nconsulting in a great, supportive environment. What distinguishes this position\nfrom other apprenticeships is that we are specifically looking for developers\nwho already have some professional development experience. At Hashrocket we sell senior web development expertise to our clients.  The\ngoal of our apprenticeship is to hire someone who already has some\nexperience with Ruby and Rails and in 3 to 9 months provide the teaching\nand mentorship that enables the apprentice to grow into a senior web developer. Why is having some prior web development experience - Rails experience -\nnecessary? Its because we feel there is too much for the bootcamp graduate,\nself taught dev or computer science student to learn within the apprenticeship\nwindow.  The apprenticeship length is capped at 9 months for two reasons.\nFirst, because we do not want to put an apprentice under pressure for longer\nthan 9 months.  Second, we have to balance the cost of training the apprentice\nwith the return of investment they would create as a senior web developer.  9 months is the longest time period that achieves that balance. Communication The focus of each apprentice is, of course, exploring the deep technology stack\nthat powers modern websites. As web developers we spend most of our day making\nuse of the knowledge we have in those areas. However, we place a lot of value\nin communication. Communication with our co-workers, communication with our\nclients, communication with future developers through code and documentation\nand communication with the outside world through writing and speaking. For apprentices, we place a lot of value in communication, but that doesn't\nmean we wouldn't hire someone for whom that is a weakness.  We always try to\nlook at the whole picture of who someone is and, when hired, help them grow with\nthe communication style that fits them. The Stack JavaScript, Ruby-on-Rails, Postgres. This is the stack that we focus on with\napprentices.  Nobody knows the whole stack from end to end and we don't expect\napprentices to come through the apprentice program and know the whole stack.\nAll three technical areas are impossibly deep and changing all the time. We probably place a heavier emphasis on the database then most other\nconsultancies in the same space. Postgres is powerful and can solve some\nproblems much more performantly than Ruby as well as providing the guarantees\nthat business critical data needs. Ruby on Rails has become pervasive and our clients bring us many varieties of\nlegacy code that we figure out, refactor and augment. We try to teach\napprentices to be unafraid of tangled, messy legacy code, to understand it, and\nto bring the most out of it. The JavaScript world changes to often for us to even throw our weight behind\none framework or one way to write it. The language itself has so many\nvarieties that its challenging to learn the JavaScript of one project and then\nmove to another project with a completely different set of JavaScript syntax\nand patterns.  As much as we can we try to teach the fundamentals of the\nlanguage itself and familiarize ourselves with frameworks project by project. This is the stack. These are the technologies that we expect our senior\ndevelopers to know, but the world of technology is vast and ever changing and\ntechnical interest will always be the greatest guide at Hashrocket. Ensuring success Its important for us to be successful with an apprentice, we are a small\ncompany and the apprentice - along with every new hire - has a big impact on\nour culture.  Given that, we put a lot of effort into every apprentice. Each\napprentice is given a program that is specific to them and their experience,\nstrengths and weaknesses.  To ensure each apprentice stays on track and gets\ntimely feedback we have an apprentice meeting each Friday with the apprentice\nprogram leaders and any relevant co-workers the apprentice may have worked with\nthat week.  Critically, we only ever have one apprentice at a time and each\napprentice has access to all developers in the office to ask questions and discuss\ntechnical problems with. Conclusion If you have a bit of professional programming experience, if you want to have a\nsupportive environment focused on helping you take the next step in your\ncareer, then please apply ! Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-05-16"},
{"website": "Hash-Rocket", "title": "Extracting programmer employment data from BLS", "author": ["\nChris Erin\n\n"], "link": "https://hashrocket.com/blog/posts/extracting-programmer-employment-data-from-bls", "abstract": "Extracting programmer employment data from BLS by\nChris Erin\n\non\nMay 14, 2019 This is the description of a technical journey to better understand the labor market for computer programmers by using numbers from the Bureau of Labor Statistics.  As a government agency, I expected the BLS to provide a readily available, significant amount of raw historical data, and while true, this data is provided in a number of ways each of which is unsatisfying in its own way.  The BLS has an api, a large cache of public flat files, and a set of spreadsheets for each year dating back to 1997.  In this blog post I explore each of these methods of extracting data from BLS. The growth of the labor market for Computer Programmers As a consultant that gets to experience a number of different project environments either through my own experiences or the experiences of my co-workers, I'm interested in how the rate of growth in the industry affects the ability to produce quality software.  But how fast is the industry growing?  The United States Bureau of Labor Statistics (BLS) produces employment numbers per employment type and industry in every field on a yearly basis.  I should be able to access those numbers to estimate the generic levels of experience across the computer programming industry. The BLS Text Files https://download.bls.gov/pub/ The above address points to a file server where a number of text files are available for download.  The particular set of files that I care about are the occupational employment (oe) files, located at https://download.bls.gov/pub/time.series/oe/ . To gain understanding, you must learn file structure and cross reference them as described in the oe.txt file.  Here, you'll learn about the series_id an id you can use to both query the text files and later the api.  It looks like this: OEUN000000000000015113201 The above code can be looked up in the oe.series file: > cat oe.series | grep \"OEUN000000000000015113201\"\n... Employment for Software Developers, Applications in All Industries in the United States ... The important part of the series_id to me is the occupation_code , in this case: 151132. The occupation codes for Computer occupations look like this (check them out here ): 15-1100     Computer Occupations\n15-1111     Computer and Information Research Scientists\n15-1120     Computer and Information Analysts\n15-1121     Computer Systems Analysts\n15-1122     Information Security Analysts\n15-1130     Software Developers and Programmers\n15-1131     Computer Programmers\n15-1132     Software Developers, Applications\n15-1133     Software Developers, Systems Software\n15-1134     Web Developers\n15-1140     Database and Systems Administrators and Network Architects\n15-1141     Database Administrators\n15-1142     Network and Computer Systems Administrators\n15-1143     Computer Network Architects\n15-1150     Computer Support Specialists\n15-1151     Computer User Support Specialists\n15-1152     Computer Network Support Specialists\n15-1199     Computer Occupations, All Other There are some codes here that don't relate closely to what I do, the ones that do are all under 15-1130 : 15-1130     Software Developers and Programmers\n15-1131     Computer Programmers\n15-1132     Software Developers, Applications\n15-1133     Software Developers, Systems Software\n15-1134     Web Developers Using the occupation_code 15-1130 and the series_id OEUN000000000000015113001 we can grep through the oe.data.1.AllData text file to find the employment numbers. > cat oe.data.1.AllData | grep OEUN000000000000015113001\nOEUN000000000000015113001       2018    A01     1666270 And as of May 2018, the 15-1130 code and its subcategories accounted for the employment of 1,666,270 people! This is just 2018.  In fact if you grep through the entire oe.data.1.AllData file for something that isn't 2018, you won't find anything.  For my goal of finding employment data for the last 20 years this isn't good enough. The BLS API The documentation for the api is here .  The first example is missing a couple of things.  First, while it details an Example Payload, it doesn't detail how to communicate that payload in a request.  Second, the example says nothing about api keys or registration. Without registering you will hit the daily rate limit in 10 requests .  You can register for an api key here . Here is an example curl request using the Computer Programmers series_id and a registration key: curl https://api.bls.gov/publicAPI/v2/timeseries/data/OEUN000000000000015113001?registrationkey=<registration-key> | jq\n{\n  \"status\": \"REQUEST_SUCCEEDED\",\n  \"responseTime\": 146,\n  \"message\": [\n    \"No Data Available for Series OEUN000000000000015113001 Year: 2016\",\n    \"No Data Available for Series OEUN000000000000015113001 Year: 2017\"\n  ],\n  \"Results\": {\n    \"series\": [\n      {\n        \"seriesID\": \"OEUN000000000000015113001\",\n        \"data\": [\n          {\n            \"year\": \"2018\",\n            \"period\": \"A01\",\n            \"periodName\": \"Annual\",\n            \"latest\": \"true\",\n            \"value\": \"1666270\",\n            \"footnotes\": [\n              {}\n            ]\n          }\n        ]\n      }\n    ]\n  }\n} Again, we discover a value of 1,666,270 for this series, but also again, we don't get any data from before the year 2018. The XLS files There is one page on the BLS Occupational Employment Statistics site that clearly provides historical data. https://www.bls.gov/oes/tables.htm This page does have the data I'm looking for, in a series of inconsistently named zip files containing with inconsistent directory structures supporting both xls and xlsx files.  As a programmer, I can either write a scraper with a lot of special cases, or I can just start downloading and unzipping. There are two naming conventions for the national files, oes\\<year\\>nat.zip or oesm\\<year\\>nat.zip .  The m in the second file convention was introduced in 2003 to accommodate producing the file in both May(m) and November(n).  That lasted 2 years, but the m persisted. Here is a script to get all files (requires wget / brew install wget ) for x ( '97' '98' '99' '00' '01' '02' 'm03' , 'm04' 'm05' 'm06' 'm07' 'm08' 'm09' 'm10' 'm11' 'm12' 'm13' 'm14' 'm15' 'm16' 'm17' ) ; do │       wget https://www.bls.gov/oes/special.requests/oes ${ x } nat.zip ; │ yes | unzip oes ${ x } nat.zip ; done This next command will convert all of the files into csv files and place them into a directory called csvfiles (requires csvkit / pip install csvkit ): mkdir csvfiles && find . -name \"*xls*\" | grep -v field_descriptions |  xargs -I % sh -c \"echo % && basename %\" | xargs -L2 bash -c 'in2csv $0 > csvfiles/$1.csv' Now, you can grep through all the csv files to find the employment numbers for the occupational code you are interested in (requires csvkit / pip install csvkit ): cat csvfiles/ * .csv | grep '15-1130' | csvcut -c 1,4 Which produces: 15-1130,1397780.0\n15-1130,1442500.0\n15-1130,1492040\n15-1130,1554960\n15-1130,1604570\n15-1130,1617400\n15-1130,1666270 Fortunately, all the files sort nicely which leads to sorted by year output, and at the bottom you'll again see the number \"1,666,270\" but before that you'll see 6 numbers that represent the 6 previous years.  We had 20 csv files, why did we only get 6 years? Well, the codes keep changing: 1999 - 2009: 15-1021 Computer Programmers\n15-1031 Computer Software Engineers, Applications\n15-1032 Computer Software Engineers, Systems Software 2010 - 2011: 15-1131 Computer Programmers\n15-1132 Software Developers, Applications\n15-1133 Software Developers, Systems Software 2012 - 2018: 15-1130     Software Developers and Programmers\n15-1131     Computer Programmers\n15-1132     Software Developers, Applications\n15-1133     Software Developers, Systems Software\n15-1134     Web Developers Conclusion Now, understanding how to get the data and understanding that I have to follow codes through time, I have a better chance of creating a table of the changing levels of experience in the industry over time.  This data was not straightforward to get and understand so I hope you can use some of this in your own BLS data journeys. Photo by: unsplash-logo Armand Khoury Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-05-14"},
{"website": "Hash-Rocket", "title": "Evaluating JavaScript Dependencies with CodeSandbox", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/evaluating-javascript-dependencies-with-codesandbox", "abstract": "Javascript Evaluating JavaScript Dependencies with CodeSandbox by\nJosh Branchaud\n\non\nMarch 26, 2019 While building a large React SPA, we reached a point where we needed to cut\ndown the bundle size a bit. We identified some candidate dependencies --\npackages that were being used sparingly but had a large footprint on the\nbundle. We then found some alternatives to those packages with a fraction of\nthe bundle size. We just needed to be sure that these packages could do what\nwe needed them to do. This is where CodeSandbox comes in. We decided that we wanted to try to replace moment.js with day.js . Before making the switch, we wanted\nto be sure that day.js could do the things we needed it to do. So, we spun up a fresh, vanilla Javascript\nCodeSandbox . Under the Dependencies tab in the File Editor section is a big, blue\nbutton -- Add Dependency . That button pops open a search modal that allows\nyou to find packages from the npm registry . Start\ntyping day.js and it will quickly show up as one of the results. Click on\nit and it will be added as a dependency of the project. We can now give day.js a test run in this isolated environment. We can\ncompare what we are seeing in the docs with what we are able to get working\nin this sandbox environment. We were primarily using moment.js for\nformatting dates, so we went to work reproducing our formatting utilities\nwith day.js . This not only gave us confidence that day.js could do the things we needed\nit to do. It also gave us a tangible artifact that we could pass around to\nothers on the team to get buy-in on the change. Once you save a CodeSandbox\ninstance, you get a unique URL that you can pass around to anyone. Now that we were satisfied with day.js and had buy-in from the rest of the\nteam, we could set out to create a pull request that would make the switch. CodeSandbox has become invaluable to many parts of my workflow. But I've\nespecially come to depend on it for evaluating my dependencies. Cover photo: unsplash-logo Wolfgang Hasselmann Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-03-26"},
{"website": "Hash-Rocket", "title": "Big Reasons to Write Small User Stories", "author": ["\nSuzanne Erin\n\n"], "link": "https://hashrocket.com/blog/posts/big-reasons-to-write-small-user-stories", "abstract": "Process Big Reasons to Write Small User Stories by\nSuzanne Erin\n\non\nMay 30, 2019 In software development, before any code can be written, you begin by gathering requirements. The clearer the requirements, the better the code reflecting those requirements will be. Writing quality user stories is an important first step towards setting development up for success. What is one way to make sure that the user stories are good? Make them small. Why should you write small user stories? 1. They are easier to discuss and understand Small stories are easier to talk about and get your head around. A large story has room for vagueness. You either risk making wrong assumptions or you spend time coming back for clarification again and again. A small story lets you have focused discussion around the specifics of a single behavior. 2. They are easier to test and accept It is easier to write a test for a small story than a big story. Testing a single behavior of the system will be more straightforward than testing a complex set of interactions. Small stories facilitate high test coverage with meaningful tests. Similarly, the requirements for acceptance will be clearer in a small story. 3. They are easier to troubleshoot Big stories lead to big commits. Big commits are harder to review. If a bug pops up after a small commit, you’ll know where to look. If a bug arises after a big commit, have fun figuring out where the issue was. 4. They provide transparency Imagine you’re a developer working on a big complicated feature. Every day at stand-up, you report “I’m still working on the big feature”. The stakeholders begin to get antsy. They’re not sure if you’re making progress. You’re not sure if you’re making progress. Every time you think you’ll be ready to deliver soon, you discover an aspect of the feature no one thought about yet. You’ve “gone dark.” Breaking big features down into small stories will allow you to chart a path forward and mark your progress, so no one is lost in the dark. 5. They illuminate scope If you have a big feature, how do you know how big it really is? Break it down into small stories to capture all of its dimensions. Having more pieces allows prioritization and enables discussion about if and how to simplify. It makes it easier to identify pieces that could be saved for later. 6. They let you be agile Big stories are lumbering; small stories are spry. Small stories are at the heart of what it means to be agile. Agility is about responding nimbly, being ready to switch gears when the situation changes or when things simply don’t come out right. Small stories put you in a position to pivot. They allow you to deliver and deploy more frequently, which means that you can start generating value and feedback sooner. Small stories make agility possible. How do you write a small user story? Imagine that you are building an app, and the stakeholder wants to add some \"social\" features. The stakeholder might say: \"I want the user to be able to like posts\" So, they might add to the backlog: User likes a post This isn't a small story. It's bigger than it looks. What does it mean to like a post? A developer or product manager just working from those few words would either have to make a few assumptions to answer that question, or spend time gathering more details about the requirements. A user story is small if it is focused on a specific interaction between the user and the system. This reduces ambiguity. You can make sure user stories are small by walking through each part of the user experience, step by step. What does the user do next? How does the system respond? Each user story consists of a user action, and a system reaction. A story can be as small as a single such interaction. The above feature could be rewritten as: Given I’m a user And I’m viewing a post When I select the heart icon Then I see the number by the heart increment by 1 And I see the heart turn red The story is written so that it can serve as testing instructions and acceptance criteria. It is written from the user’s perspective, describing the user’s actions and how the app behaves in response. It uses the stakeholder's domain language to make sure everyone understands all the details. By continuing to walk though the subsequent steps of the user experience with the stakeholder, you can uncover different paths and edge cases. In the “liking posts” example, you might follow-up by asking: Can the user like the post more than one time? What if they leave and come back later? Do other users see the number of likes incrementing? Can users see who liked what when? Does a user need to be logged-in to like a post? These questions might generate more small stories. Then, everyone will have a better understanding of the scope of the ask, and the stakeholder can make decisions about which aspects are important. A small story doesn’t necessarily mean that it is a simple story or a short story. It just means that it is focused on a single interaction, in a particular set of circumstances. A single-interaction user story might still be pretty lengthy. You may need to list a long set of preconditions that describe the scenario. The reaction it elicits from the system might be quite complex. In order to fully capture a feature, you may need to use multiple stories. By focusing on one interaction step at a time, you are in a better position to suss out the details. At Hashrocket we write an initial set of user stories in a storycarding session. We start the discussion by talking at a high level about the functionality of an app. Then we walk through the user experience step-by-step, being sure to explore happy paths, sad paths, and edge cases. This ensures that the intended behavior is accurately captured at a granular level, and helps uncover otherwise unanticipated scope. Successfully creating and enjoying the benefits of small stories demands frequent and quality communication. Both developers and product owners or stakeholders should be involved in the discussion at creation. The stories represent an agreement about the intended functionality of the app. The benefits of transparency, scope illumination, and agility will only come about if stories are part of a communicative and iterative process that connects developers to stakeholders regularly. At Hashrocket, we take advantage of the benefits of small stories by having daily stand-ups, frequent delivery, and a healthy feedback loop. Image by: unsplash-logo Iker Urteaga Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-05-30"},
{"website": "Hash-Rocket", "title": "6 Tips for Better Communication with a Client", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/6-tips-for-better-communication-with-a-client", "abstract": "Process 6 Tips for Better Communication with a Client by\nJosh Branchaud\n\non\nApril  4, 2019 Every project is different and every client is different. That means communication and process can vary quite a bit depending on what you are working on and who you are working with. In over 4 years of working at Hashrocket, I've encountered some communication best practices that tend to work across this spectrum. In this post, I'll walk through the why and how of 6 of these best practices. 1. Cut the back and forth Communicating is hard. Asynchronous communication is even harder. Keep an eye out for email threads and Slack conversations that start to get out of control. Remember one of those conversations where you are trying to settle on the details of a complex feature or misunderstood bug? Once you've gone back and forth a couple times, you may notice people talk past one another or new scope getting exposed. You've passed a threshold. It is time to cut the back and forth and have a voice/video call to get things straightened out. During this call you'll likely answer some lingering questions and uncover valuable information. Don't keep it to yourself. Make sure it gets captured in a document or your project management software so that everyone can learn from that conversation. 2. Daily standup There is no better way to get your day off to a productive start than to make sure you are working on the right things. Let your client know what you were able to accomplish and what your game plan for the day is. If your plan is out of line with your client's priorities, this is your chance to course correct. If you and your client are on the same page, then you can have confidence in moving forward and your client can be assured that this process is moving them toward their goals. Stick to these daily standups and keep them brief. Once you start skipping days, it is hard to recover the routine and that shared sense of alignment will take a hit. If these meetings aren't kept brief (10-15 minutes), then the purpose becomes lost and they can become burdensome or frustrating to attend (\"how long am I going to be stuck in this?\"). Standups running long is also a sign that a topic or question is in need of more extensive conversation — take it offline or schedule a different meeting. 3. Take it offline Whether or not your meetings have a fixed agenda, you know when a particular point of conversation has started to go down a rabbit hole. If you have a lot of people in a meeting, these tangents can have a measurable, and sometimes significant, cost. These tangents can also have an indirect cost. If there is pertinent information that needs to be disseminated amongst a group of people, or if there is time-sensitive feedback that you need from everyone, then these tangents can prevent that communication. As soon as you find that a particular conversation is headed into the weeds, stop for a moment. Find out if this is a conversation that needs to happen now, with all of these people. If it isn't, then suggest that it is taken offline. In fact, you can even take this as an opportunity to identify who needs to be in that offline conversation. 4. Lean on user stories When you first start discussing a feature or bug, it is easy to talk at a high-level. In fact, it is often necessary in order to provide the context that brings everyone in. You eventually need to move past the hand waving and imprecision. The software you write needs to be precise and so do the specifications. User stories can help capture the necessary precision. Find what works for your team, but I highly recommend using the 'Given/When/Then' style . This prescribes that you define preconditions for the feature ('Given'), interactions the user will have with the app ('When'), and postconditions that can be verified as a result of the user's actions ('Then'). Stories written in this way remove ambiguity about what needs to be done. This makes your job as the developer easier. It also provides clear acceptance criteria for the client to determine if the implementation hits the mark or not. Plus, all of this is focussed on the user who will be using this software at the end of the day. 5. Screenshots are great Screenshots are a visual tool that can enhance asynchronous communication. If you encountered a bug in the web interface, take a screenshot and upload it with the bug report. If you have an idea about how to reposition an element in that form, do a quick mock up, take a screenshot, and then include it in your proposal. A good screenshot can preempt a variety of questions and ultimately cuts down on some of the back and forth. 6. Gifs are better If a still-image screenshot is good, then a moving-picture gif is better. You can convey a lot of information by quickly putting together a gif. In fact, the interactive nature of modern UIs often requires a gif. You aren't just building screens, you are building interactions. First impressions matter, so if you really want to sell your client on a new interaction, give them the full effect with a gif that shows off what you are proposing. If you've encountered a tricky bug, you can show reproduction steps with a gif. This is a two way street. Encourage (and teach) your client to do the same — this will make bug hunting go way faster. There are many options for capturing gifs, Giphy Capture for Mac is my favorite. Conclusion These six practices have served me well across many client engagements. I encourage you to give them a try. I think you'll find that they help you build trust, rapport, and effective communication patterns with each of your clients. Cover photo: unsplash-logo rawpixel Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-04-04"},
{"website": "Hash-Rocket", "title": "Technical Debt Part 2: Management Strategies", "author": ["\nSuzanne Erin\n\n"], "link": "https://hashrocket.com/blog/posts/technical-debt-part-2-management-strategies", "abstract": "Process Technical Debt Part 2: Management Strategies by\nSuzanne Erin\n\non\nMay 23, 2019 Just like good financial planning involves a strategy for paying off credit card debt or budgeting for home maintenance, good project planning involves setting a strategy to avoid and reduce technical debt. This post discusses methods to manage technical debt. This is the second part of a two-part series. Visit the first part for an introduction to technical debt, its similarity to personal finance, and how it slows down development. Why you should manage your technical debt load Your bank isn’t going to give you a contract for a credit card or a home repair loan without some strict rules about how they will be paid back. To keep technical debt from piling up, slowing you down, and costing you precious time, you need to develop your own system of accountability to manage debt. First, avoid taking on debt that you don’t need. Using the credit card analogy, what are the terms for paying off this card? What if there is an emergency when you are already over-extended? What will this do to your credit score? Likewise, consider the risks of taking shortcuts in software development: does rushing through something now provide enough business value to outweigh the costs of losing velocity down the line? Have you thought about the other risks? Code written in a hurry is harder to understand and navigate. It makes it harder to onboard new developers. It makes it harder to maintain a great user experience. How would problems impact your brand reputation? Does your business deal with money? Information that needs to stay secure?  Is your site live in production? What are the potential consequences of bugs or breakage? Choosing to take on technical debt will increase risk for your product beyond just the impact on future velocity. On the other hand, it is important to recognize that there is no truly debt-free software development. This is true even if you minimize risky practices.  As you move forward, you’ll always discover ways that you can improve the underlying code. There will always be maintenance and upgrades that need to be performed. The trick is addressing them on an ongoing basis, so that their costs don’t compound. There are a few methods to address technical debt. Consider using one or a few combined to manage your debt load. How to manage technical debt: Leave code better than you find it. Prioritize paying off debt that slows you down the most. Budget time to pay off debt. Program in pairs. Plan ahead when taking on new debt. Leave code better than you find it. This is known as the Boy Scout Rule. Boy Scouts leave a campsite cleaner than when they found it, picking up any litter they find. When a developer following this rule is working in an area, they take the time to improve the existing code at least a little bit. This way the code is always improving at the same time as value is being generated for clients. Prioritize paying off debt that slows you down the most. The rate at which technical debt slows new development down is like a compound interest rate. A financial planner will advise you to start paying down the debt with the highest interest rate first. Likewise, identify the issues that are slowing down new development the most. Prioritize these as chores to complete in order of severity. Budget time to pay off debt. The biggest barrier to addressing technical debt is the pressure to keep finishing new features as quickly as possible. The business concerns that drive this pressure aren’t going to go away on their own. Neither is technical debt. Just like maintenance is a fact of home ownership, technical debt is a fact of software development. But you can plan for it. Budget regular buffers into your development cycle to account for unforeseen trouble. One way is to set aside time. One client set aside the last three days of a three week iteration for developers to focus on refactoring. Rather than time, this could be in whatever metric you use to track effort, whether that be points, stories, or issues. Set a target amount to address per iteration. As needs are raised regarding refactoring, be sure to capture them in a way that can be tracked. Program in pairs. One of the benefits of pair programming is that two sets of eyes are continuously writing, inspecting, and changing the code all the time. It is code review by the parties who have the most investment and context for that part of the code. Two heads are better than one when it comes to coming up with the best approach to solving a problem or implementing a structure. More issues will be caught as they are being written, reducing the amount that become technical debt. Plan ahead when taking on new debt. When you take on debt, you always agree to terms to pay it back. Likewise, If you agree to take a shortcut, ensure that there is a plan for going back to readdress it. For example, you go into crunch mode for an iteration to squeeze out features, then you know you’ll need to factor in more time to address road-bumps later. If you skip a step, like forgoing the implementation of error logging, make sure it has a place in the backlog.  If you see that a new version of the language you are using has come out, then set a timeline to upgrade, even if it is a long term goal. The more you can commit to measurable milestones around these items, the better you’ll be able to avoid the traps of waving them off into the future indefinitely. Conclusion Carrying a lot of technical debt is risky, both in terms of lost velocity as well as other concerns like stability and user experience. Technical debt can be minimized through best practices, but it is inevitable that improvements will be needed on any project as time goes by. Therefore, it is important to have a strategy in place to manage technical debt as it comes up. Consider integrating strategies to address technical debt into your development process to minimize ongoing costs and risks. Image by: unsplash-logo Saad Chaudhry Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-05-23"},
{"website": "Hash-Rocket", "title": "Cast/Share your iPhone/iPad screen to Mac", "author": ["\nDorian Karter\n\n"], "link": "https://hashrocket.com/blog/posts/cast-iphone-screen-to-mac", "abstract": "Cast/Share your iPhone/iPad screen to Mac by\nDorian Karter\n\non\nMarch 12, 2019 If you are pairing remotely on a mobile web project or doing a presentation that includes a mobile demo it is often helpful to share your iOS screen on your Mac. Recently, we have been working on a project that uses ApplePay on the web which proved difficult to test on a desktop computer, and we wanted to show how it worked on an iPhone. There are quite a few software products that can help you achieve this goal, but many of them cost money or are unreliable. In this post I will share with you some solutions I have used successfully to share my mobile device's screen. I’m not going to cover all the solutions that exist out there, and ultimately there are no right or wrong answers - it is whatever works for your use case. Reflector - Cost: $14.99 Reflector is a software solution by Air Squirrels that turns your Mac into an AirPlay/ChromeCast device that you can then mirror your phone into. It supports multiple device screen mirroring and can even record your screen on multiple devices at once. Pairing is super easy and uses common protocols for sharing a screen such as AirPlay, which is built into all iOS devices. You can even use Android devices using ChromeCast. Out of all the streaming solutions I used, this one is the best at what it does. Cons: Although this solution works great most of the time, I did have errors connecting a few times in the past. But, those are relatively rare and sometimes depend on your network setup (you have to be on the same WiFi your Mac is on with your iPhone). Updates are not free, and there appears to be a new version every year or so with no earth shattering changes in functionality. Your old version will probably work until macOS gets updated or something breaks and then you have to spend another $15. Quick Time - Cost: FREE QuickTime comes built-in with macOS and gives you this functionality for free. To get started, you will need to connect your iOS device to your computer via Lightning to USB cable and follow these steps: Open QuickTime Click File -> New Movie Recording In the window that opens, click the tiny down facing arrow next to the record button and select your iOS device under the Camera section (you can also select your device as the Microphone to record the sounds from the device): That’s it. You can now share your screen via VNC or any other screen sharing software you use, and your screen will include your phone. Or, you can record a video of your screen to share with clients. The QuickTime solution is usually my preferred option since it is installed on all Macs, and it’s pretty straight forward. The quality also seems much better than the streaming solution. Cons: You must have a Lightning to USB cable lying around to use this solution If you have a newer MacBooks you would need a Lightning to USB-C or an adapter from USB to USB-C You are tethered to your computer while casting Zoom - FREE 40-minute sessions, Subscription based Zoom is becoming my favorite video conferencing solution, despite being proprietary and subscription based. The video and audio quality compared to any other solution I’ve used is just phenomenal. The free version will get you pretty far; it should suffice for most business related calls or debugging sessions. To start, open a video call and press the Share Screen button: Then in the menu select iPhone//iPad via AirPlay or iPhone//iPad via Cable: That’s it. This solution was best when we needed our client to show us what she was experiencing when testing our solution on her mobile phone. Cons: Privacy. I am usually reasonably suspicious of anything that can record my screen and transmit it to others. Since this is a third party solution they could be recording the video and storing it on their servers which could include information such as your phone password/credit card information or basically anything you type on your phone. Their servers could be compromised in a data breach, or they could be sharing your data with third parties without your knowledge. It’s not completely free, and the 40-minute limit means it would not work for all-day remote pairing sessions. Conclusion Casting your iPhone screen to a Mac can be done in a variety of different ways and it is fairly easy to do. It can allow you to pair on mobile projects or help debug an issue with a client on call. Depending on your needs and your budget, you should do your research and make the best decision, but be sure to try the free solutions first. Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-03-12"},
{"website": "Hash-Rocket", "title": "The nature of software and fixed bid contracts", "author": ["\nChris Erin\n\n"], "link": "https://hashrocket.com/blog/posts/the-nature-of-software-and-fixed-bid-contracts", "abstract": "Process The nature of software and fixed bid contracts by\nChris Erin\n\non\nMarch 19, 2019 Software is abstract.  It is soft, where is it? I can not hold it in my hands or feel it's weight. Is it substantial, insubstantial? Are you impressed by the effort I put into it?  Are you disappointed in my apparent slowness while making this software? Before I write software for you, I am confident that it will be wholly unique.  Never before will it have been written, or else why would you want it?  If it's not unique, can you instead purchase it elsewhere?  Even if it exists, you can't purchase it, and you'd like me to recreate it, I cannot duplicate someone else's software bit for bit.  That's like trying to act Patrick Stewart's MacBeth in a way that's indistinguishable from Patrick Stewart.  With my first breath I will have failed.  I have not lived Patrick Stewart's life, my voice box and diaphragm are different dimensions, and my soul has not acquired the gravity of an actor with his experience. With my first keystroke, the software I write becomes unique. Everytime I start a fresh project, I am standing on the shoulders of giant bugs whose exoskeleton accretion accelerates constantly. The compiler has been updated, my computer's operating system has been updated, the terminal I use has been updated, the editor, the shell, the browser, the massive ecosystem for my language of choice has been updated.  Millions of lines of software have changed since I last started anew.  I could not have the written the same \"Hello World\" today that I did six months ago. Maybe that is opaque to you.  You can't see that the whole world is shifting under our feet with every step.  Software is abstract, you can't observe the intangible structures beneath the interface.  But trust me when I say that each piece of software is unique. From your vantage point, the shape of the software you'd like me to create looks like a square to you, but to me it's an infinitely expanding fractal that changes with each magnification. And with each zig or zag I have a decision to make:  Can I do this with an if statement? Maybe pattern matching fits? A class might be more communicative. Communicative to who? to me? the next developer? Would you like the software I write to be written for an additional audience, which is every developer that will come after me if this software is successful enough to see a stream of developers, junior, senior, Java devs, C++ devs, devs that have been coding since their family purchased a used Commodore 64, devs that went to a code boot camp after a 10 year career in sales?  How could those developers even read a line of code in the same way? No developer given the same task will make these same microdecisions the same way. Writing software maps our minds as nicely as writing prose or poetry. It can be written to adhere to strict ideals, or it can be written to flaunt those same strict ideals. Do they even matter? Are your opinions about responsibility in software design the same as mine? Do those ideals matter after 5pm when I'm tired, you expect something by tomorrow morning, but the internet has been sketchy and I can't get to the documentation I need? I make thousands of microdecisions every day on top of a handful of major decisions and while I am a confident programmer I'm not sure that every decision I make is as finely balanced as I would like.  Defects are endemic in software and every developer creates defects.  Sometimes the defects are due to wrong decisions but sometimes they are due to ignoring entire sets of circumstances.  Every defect has its own depth and bredth and story and many are insidious. Forgive me for saying this, but I am not confident in every decision the person defining what the software should do is making.  That person could be you, and again, I am sorry.  I have never met a person who could conceive of every detail in full and I have never met a person capable of communicating that vision to a developer.  This is why we talk about iterative development .  I build something, I show it to you, I ask you if this is what you want, I ask you to verify that you looked at it, and I ask you to tell me how it should change.  We iterate on every single feature like this.  You detail the course, I travel in a straight line for a day, and you look at the stars and correct the course.  Every day. Given this description of software, how can I tell you the exact time it will fully realize your vision?  There are so many variables, variables in me, variables in you, variables in the industry, variables in the software we depend on, and maybe it's not fair to say the course you have charted is the exact course that will accomplish the goals.  I can give you an estimate; you deserve an estimate; and you deserve clear daily communication about all the choices we're making but if the course we chart takes detours I cannot work for free.  This is why fixed bid contracts are bad. Image by unsplash-logo Kyle  Wong Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-03-19"},
{"website": "Hash-Rocket", "title": "A Friendly Introduction to Convolutional Neural Networks", "author": ["\nIfu Aniemeka\n\n"], "link": "https://hashrocket.com/blog/posts/a-friendly-introduction-to-convolutional-neural-networks", "abstract": "A Friendly Introduction to Convolutional Neural Networks by\nIfu Aniemeka\n\non\nAugust 22, 2017 Introduction Convolutional neural networks (or convnets for short) are used in situations where data can be expressed as a \"map\" wherein the proximity between two data points indicates how related they are. An image is such a map, which is why you so often hear of convnets in the context of image analysis. If you take an image and randomly rearrange all of its pixels, it is no longer recognizable. The relative position of the pixels to one another, that is, the order, is significant. Convnets are commonly used to categorize things in images, so that's the context in which we'll discuss them. A convnet takes an image expressed as an array of numbers, applies a series of operations to that array and, at the end, returns the probability that an object in the image belongs to a particular class of objects. For instance, a convnet can let you know the probability that a photo you took contains a building or a horse or what have you. It might be used to distinguish between very similar instances of something. For example, you could use a convnet to go through a collection of images of skin lesions and classify the lesions as benign or malignant. Convnets contain one or more of each of the following layers: convolution layer ReLU (rectified linear units) layer pooling layer fully connected layer loss layer (during the training process) We could also consider the initial inputs i.e. the pixel values of the image, to be a layer. However, since no operation occurs at this point, I've excluded it from the list. Convolution Layer The architecture of a convnet is modeled after the mammalian visual cortex, the part of the brain where visual input is processed. Within the visual cortex, specific neurons fire only when particular phenomena are in the field of vision. One neuron might fire only when you are looking at a left-sloping diagonal line and another only when a horizontal line is in view. Our brains process images in layers of increasing complexity. The first layer distinguishes basic attributes like lines and curves. At higher levels, the brain recognizes that a configuration of edges and colors is, for instance, a house or a bird. In a similar fashion, a convnet processes an image using a matrix of weights called filters (or features) that detect specific attributes such as diagonal edges, vertical edges, etc. Moreover, as the image progresses through each layer, the filters are able to recognize more complex attributes. To a computer, an image is just an array of numbers. An image using the full spectrum of colors is represented using a 3-dimensional matrix of numbers. The depth of the matrix corresponds to RGB values. For the sake of simplicity, we'll initially consider only grayscale images. Each pixel in a grayscale image can be represented using a single value that indicates the intensity of the pixel. These values lie between 0 and 255, where 0 is black and 255 is white. The convolution layer is always the first step in a convnet. Let's say we have a 10 x 10 pixel image, here represented by a 10 x 10 x 1 matrix of numbers: I've stuck to zeroes and ones to make the math simpler. You'll see what I'm talking about in a bit. The output of a convolution layer is something called a feature map (or activation map). In order to generate a feature map, we take an array of weights (which is just an array of numbers) and slide it over the image, taking the dot product of the smaller array and the pixel values of the image as we go. This operation is called convolution. The array of weights is referred to as a filter or a feature. Below, we have a 3 x 3 filter (as with the image, we've used 1s and 0s for simplicity). We then use the filter to generate a feature map The image shows the calculation for the first dot product taken. Imagine the filter overlaid on the upper left-hand corner of the image. Each weight in the filter is multiplied by the number beneath it. We then sum those products and put them in the upper left hand corner of what will be the feature map. We slide the filter to the right by 1 pixel and repeat the operation, placing the sum of the products in the next slot in the feature map. When we reach the end of the row, we shift the filter down by 1 pixel and repeat. Filters are used to find where in an image details such as horizontal lines, curves, colors, etc. occur. The filter above finds right-sloping diagonal lines. Note that higher values in the feature map are in roughly the same location as the diagonal lines in the image. Regardless of where a feature appears in an image, the convolution layer will detect it. If you know that your convnet can identify the letter 'A' when it is in the center of an image, then you know that it can also find it when it is moved to the right-hand side of the image. Any shift in the 'A' is reflected in the feature maps. This property of the convolution layer is called translation equivariance. Thus far, we've been discussing the convolutional layer in the context of a grayscale image with a 2-dimensional filter. It's also possible that you would be dealing with a color image, the representation of which is a 3-dimensional matrix, as well as multiple filters per convolutional layer, each of which is also 3-dimensional. This does require a little more calculation, but the math is still basically the same. You'll see that we're still taking the dot product, it's just that this time around we need to add the products along the depth dimension as well. In the above example, we convolved two 3 x 3 x 3 filters with a 6 x 6 x 3 image and the result was a 3 x 3 x 2 feature map. If we had used three filters, the feature map would have been 3 x 3 x 3. If there were 4, then the size would have been 3 x 3 x 4, and so on. To get the dimensions of the feature map is fairly straightforward. Our input image has a width of 6, a height of 6, and a depth of 3, i.e. wi= 6, hi = 6, di = 3. Each of our filters has a width of 3, a height of 3 and a depth of 3 i.e. wf = 3, hf = 3, df = 3. The stride is the number of pixels you move the filter between each dot product operation. In our example, we would take the dot product, move the filter over by one pixel, and repeat. When we get to the edge of the image, we move the filter down by one pixel. Hence, the stride in our example convolution layer is 1. The width and height of the feature map are calculated like so: w m = (w i - w f )/s + 1 h m = (h i - h f )/s + 1 where s is the stride. Let’s break that down a bit. If you have a filter that’s 2 x 2 and your image is 10 x 10, then when you overlay your filter on your image, you’ve already taken up 2 spaces across, i.e. along the width. That counts for 1 spot in the feature map (that’s the ‘+1’ part). To get the number of spots left, you subtract the filter’s width from the total width. In this case, the result is 8. If your stride is 1, you have 8 spots left, i.e. 8 more dot products to take until you get to the edge of the image. If your stride is 2, then you have 4 more dot products to take. The depth of the feature map is always equal to the number of filters used; in our case, 2. ReLU Layer The ReLU (short for rectified linear units) layer commonly follows the convolution layer. The addition of the ReLU layer allows the neural network to account for non-linear relationships, i.e. the ReLU layer allows the convnet to account for situations in which the relationship between the pixel value inputs and the convnet output is not linear. Note that the convolution operation is a linear one. The output in the feature map is just the result of multiplying the weights of a given filter by the pixel values of the input and adding them up: y = w 1 x 1 +w 2 x 2 + w 3 x 3 + ... where w is a weight value and x is a pixel value. The ReLU function takes a value x and returns 0 if x is negative and x if x is positive. f(x) = max(0,x) As you can see from the graph, the ReLU function is nonlinear.\nIn this layer, the ReLU function is applied to each point in the feature map. The result is a feature map without negative values. Other functions such as tanh or the sigmoid function can be used to add non-linearity to the network, but ReLU generally works better in practice. Pooling Layer The pooling layer also contributes towards the ability of the convnet to locate features regardless of where they are in the image. In particular, the pooling layer makes the convnet less sensitive to small changes in the location of a feature, i.e. it gives the convnet the property of translational invariance in that the output of the pooling layer remains the same even when a feature is moved a little. Pooling also reduces the size of the feature map, thus simplifying computation in later layers. There are a number of ways to implement pooling, but the most effective in practice is max pooling. To perform max pooling, imagine a window sliding across the feature map. As the window moves across the map, we grab the largest value in the window and discard the rest. As mentioned earlier, the output indicates the general region where a feature is present, as opposed to the precise location, which isn't really important. In the above diagram, the result of the pooling operation indicates that the feature can be found in the upper left-hand corner of the feature map, and, thus, in the upper left hand corner of the image. We don’t need to know that the feature is exactly, say 100 pixels down and 50 pixels to the right relative to the top-left corner. As an example, if we're trying to discern if an image contains a dog, we don't care if one of the dog's ears is flopped slightly to the right. The most common implementation of max pooling, and the one used in the example image, uses a 2 x 2 pixel window and a stride of 2, i.e. we take the largest value in the window, move the window over by 2 pixels, and repeat. The operation is basically the same for 3D feature maps as well. The dimensions of a 3D feature map are only reduced along the x and y axes. The depth of the pooling layer output is equal to the depth of the feature map. The Fully-Connected and Loss Layers The fully-connected layer is where the final \"decision\" is made. At this layer, the convnet returns the probability that an object in a photo is of a certain type. The convolutional neural networks we've been discussing implement something called supervised learning. In supervised learning, a neural network is provided with labeled training data from which to learn. Let's say you want your convnet to tell you if an image is of a cat or of a dog. You would provide your network with a large set of pictures of cats and dogs, where pictures of cats are labeled 'cat' and pictures of dogs are labeled 'dog'. This is called the training set. Then, based on the difference between its guesses and the actual values, the network adjusts itself such that it becomes more accurate each time you run a test image through it. You confirm that your network is in fact able to properly classify photos of cats and dogs in general (as opposed to just being able to classify photos in the training set you provided) by running it against an unlabeled collection of images. This collection is called the test set. In this example, the fully-connected layer might return an output like \"0.92 dog, 0.08 cat\" for a specific image, indicating that the image likely contains a dog. The fully-connected layer has at least 3 parts - an input layer, a hidden layer, and an output layer. The input layer is the output of the preceding layer, which is just an array of values. You'll note in the image, there are lines extending from the inputs (x a to x e ) to nodes (y a to y d ) that represent the hidden layer (so called because they’re sandwiched between the input and output layers and, thus, “invisible”).  The input values are assigned different weights w xy per connection to a node in the hidden layer (in the image, only the weights for the value xa are labeled). Each of the circles in the hidden layer is an instance of computation. Such instances are often called neurons. Each neuron applies a function to the sum of the product of a weight and its associated input value. The neurons in the output layer correspond to each of the possible classes the convnet is looking for. Similar to the interaction between the input and hidden layer, the output layer takes in values (and their corresponding weights) from the hidden layer, applies a function and puts out the result. In the example above, there are two classes under consideration - cats and dogs. Following the fully-connected layer is the loss layer, which manages the adjustments of weights across the network. Before the training of the network begins, the weights in the convolution and fully-connected layers are given random values. Then during training, the loss layer continually checks the fully-connected layer's guesses against the actual values with the goal of minimizing the difference between the guess and the real value as much as possible. The loss layer does this by adjusting the weights in both the convolution and fully-connected layers. Hyperparameters It should be said at this point that each of these layers (with the exception of the loss layer) can be multiply stacked on one another. You may very well have a network that looks like this: convolutional layer >> reLU >> pooling >> convolutional layer >> pooling >> fully-connected layer >> convolutional layer >> fully-connected layer >> loss layer There are other parameters in addition to the order and number of the layers of a convnet that an engineer can modify. The parameters that are adjusted by a human agent are referred to as hyperparameters. This is where an engineer gets to be creative. Other hyperparameters include the size and number of filters in the convolution layer and the size of the window used in the max pooling layer. Overview So, let's run through an entire convolutional neural network from start to finish just to clarify things. \nWe start with an untrained convnet in which we have determined all of the hyperparameters. We initialize the weights in the convolution and fully-connected layers with random inputs. We then feed it images from our training set. Let’s say we have one instance of each layer in an example network. Each image is processed first in the convolution layer, then in the ReLu layer, then in the pooling layer. The fully-connected layer receives inputs from the pooling layer and uses these inputs to return a guess as to the contents of the image. The loss layer compares the guess to the actual value and figures out by how much to adjust the weights to bring the guess closer to the actual value. For instance, if the convnet returns that there is an 87% chance that an image contains a dog, and the image does indeed contain a dog, the guess is off by 13% and the weights are adjusted to bring that guess closer to 100%. Other Resources Understanding Neural Networks Through Deep Visualization by Jason Yosinksi, Jeff Clune, Ang Nguyen, et al. How Do Convolutional Neural Networks Work? by Brandon Rohrer A Quick Introduction to Neural Networks by Ujjwal Karn Convolutional Neural Networks for Visual Recognition by Andrej Karpathy Photo by NASA on Unsplash Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-08-22"},
{"website": "Hash-Rocket", "title": "Use Yarn Like a Pro", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/use-yarn-like-a-pro", "abstract": "Javascript Use Yarn Like a Pro by\nJake Worth\n\non\nFebruary 19, 2019 Yarn is a fast, reliable, and secure alternative to NPM. In this post, we'll look at techniques to use Yarn to its fullest. The expectations of life depend upon diligence; the mechanic that would\nperfect his work must first sharpen his tools. —Confucius Yarn is a great tool. I'm won't wade into the Yarn vs. NPM debate,\nbecause the tradeoffs are always in flux. I just say that I've been\nusing it for two years on a variety of projects, and have nothing but praise. Part of that journey has been learning Yarn's nuances, and today I'm happy to be\nsharing them with you. Some of these epiphanies came to me through my own work,\nand some via our company knowledge base, Today I\nLearned . I'm going to skip the setup and some common Yarn commands; head over to Yarn's\nofficial site to boot up. All of these commands were\nrun against Yarn 1.7.0. Alias Yarn Let's start out with a quick win. Alias yarn to y and enjoy those saved keystrokes. # .bashrc (or equivalent) alias y = 'yarn' To minimize confusion, be using the full yarn command throughout the rest of this post. Initialize a Project I love initializers. Any heavy-handedness is outweighed by\nhaving the right files in place from the start. Like\nNPM, Yarn has a command for initializing a new JavaScript project. $ yarn init Run this from your project directory; it creates a package.json file with all the basic details. Bootstrap an App Yarn includes a create command for generating bootstrapped apps, following the create-<name>-app convention. Here's how you'd use it with create-react-app : $ yarn create react-app my-app Type Less with the Default Command Like Bundler for Ruby, yarn with no other qualifier runs yarn install ,\npassing through any provided flags. If updating JavaScript packages is your\nintention, run yarn and enjoy those saved keystrokes. Update Packages Automatically This isn't a Yarn-specific tip, but for me it's essential when using this tool.\nWhen I pull down changes to a JavaScript project, those changes can add,\nupdate, or remove dependencies. Unlike a language like Elixir, in JavaScript it's possible to start your server, run your tests,\nor do a number of activities without syncing your packages. When your\nlocal code chokes, which it will, the results can range from annoying to\ndisastrous. Here's my solution, which I borrowed from the internet a while\nago. This is the executable file .git/hooks/post-merge and it lives in the\nroot directory of all my collaborative JavaScript projects: #/usr/bin/env bash changed_files = \" $( git diff-tree -r --name-only --no-commit-id ORIG_HEAD HEAD ) \" check_run () { echo \" $changed_files \" | grep --quiet \" $1 \" && eval \" $2 \" } check_run yarn.lock \"yarn\" When my Yarn lockfile changes in version control, this script runs yarn (AKA yarn install , as explained above), keeping me current. This has saved me many times, and it's just less mental overhead. Autoclean To quote Dorian Karter: \"The node_modules directory is often a resource-consuming hog, both in space and number of files. It is full of junk such as\ntest files, build scripts, and example directories.\" We want a repo full of quality code, not cruft. yarn autoclean to the rescue. Autoclean cleans and removes unnecessary files\nfrom package dependencies. To enable it, you first must generate the .yarnclean config file: $ yarn autoclean --init .yarnclean lists about forty files you should remove from a typical\nJavaScript project. Once enabled, autoclean will remove them when you install or add\ndependencies, or force it to run. Here's what happened when I set it up on a long-running React\nproject: $ yarn autoclean --force yarn autoclean v1.13.0 [ 1/1] Cleaning modules...\ninfo Removed 8588 files\ninfo Saved 53.06 MB. Enjoy your pruned program. Add an Alias Naming collision ahead? Alias a dependency with yarn add : $ yarn add react-select@v1.2.0 $ yarn add react-select-next@npm:react-select@next Now you can import your aliased dependency like so: import Select from 'react-select-next' Test Your Build create-react-app is an example\nof a project that requires all dependencies precompiled to ES5. If they aren't,\nthe build will fail. A bad time to discover this would be when your tooling is trying to build\nthe app on a staging or production server. Surface this issue in development like so: $ yarn <your-build-step> Just run your build command anytime you add a new dependency. This could be even be added as a post-merge step. Conclusion We've barely scratched the surface of what Yarn can do. If you write a lot of JavaScript, I recommend exploring this tool! Thank you to Andrew Vogel, Brian Dunn, Chris Erin, Dorian Karter, Gabe Reis, and Josh Branchaud for writing about Yarn on Today I\nLearned . We've all been learning together as this\nlibrary matures. What are your favorite Yarn hacks? Let us know on Twitter . Photo by Steve Johnson on Unsplash Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-02-19"},
{"website": "Hash-Rocket", "title": "Write A Reduce Function From Scratch", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/write-a-reduce-function-from-scratch", "abstract": "Javascript Write A Reduce Function From Scratch by\nJosh Branchaud\n\non\nFebruary 26, 2019 JavaScript's Array object has a built-in reduce method . This is a handy,\nyet underutilized method. Let's take a closer look at what reduce does and\nsee if we can implement it from scratch. Reduce? Here is what MDN has to say about reduce : The reduce() method executes a reducer function (that you provide) on each\nmember of the array resulting in a single output value. So, we can take an array of things and turn it into something else using a\nfunction that we get to define. const sum = list => list . reduce (( total , val ) => total + val , 0 ); sum ([ 1 , 2 , 3 , 4 ]) // => 10 The reduce method can seem pretty limiting at first when we read that it\nproduces \"a single output value.\" That makes it sound like all it is good\nfor are things like a sum function. The reduce method can also produce\nan array as its single output value . Objects too! const countWords = wordList => { return wordList . reduce (( acc , word ) => { acc [ word ] = ( acc [ word ] || 0 ) + 1 ; return acc ; }, {}); }; countWords ([ \"hello\" , \"world\" , \"hello\" , \"dogs\" , \"hello\" , \"cats\" ]); // => {hello: 3, world: 1, dogs: 1, cats: 1} This method can do just about anything. It's very versatile. I'll say more\nabout that at the end. Implement Our Own Let's push our understanding of reduce a bit further by implementing our\nown. What we ought to have noticed in the examples above is that there are three\nstandard parts to a reduce . First, there is the list to be reduced. Then\nthere is always some initial value . For sum it was 0 and for countWords it was {} . Third, and perhaps most importantly, is our reducer function . We can stub out a naive version with just this knowledge: const myReduce = ( list , initialValue , reducer ) => { return initialValue ; } This naive implementation will even work in cases where list is empty. myReduce([], 0, (total, val) => total + val); // => 0 That there is an important observation. Once we have an empty list, we\nshould return whatever our initialValue is. This is what is known as our base case or terminating case . The full implementation will involve\nrecursive calls of itself, so it is important to understand when and how we terminate the process. const myReduce = ( list , initialValue , reducer ) => { if ( list . length === 0 ) { return initialValue ; } } We are still missing the meat of the function. We need to decide what\nhappens when there are items left in the list. This is where our reducer function comes into the picture. The Reducer The general shape of the reducer function is important. It takes two\nvalues as arguments: the accumulator , what everything is being reduced\ninto, and the current value we are reducing. It then performs some reduction\nlogic and returns a new version of the accumulator . Let's look back at our reducer function for sum : ( total , val ) => total + val The accumulator is the total of our summing so far and the current value is val . We had those two values together, returning them as the new version\nof the accumulator . The reducer function for countWords is a bit more complicated: ( acc , word ) => { acc [ word ] = ( acc [ word ] || 0 ) + 1 ; return acc ; } The accumulator is some object full of words and respective counts. We\nhave to first update the accumulator with the next word accounting for\nwhether or not it is the first time we've seen this word. Then the updated accumulator is returned. Now Reduce With that in mind, let's handle the case where we get to use the given reducer function. const myReduce = ( list , initialValue , reducer ) => { if ( list . length === 0 ) { return initialValue ; } else { const [ first , ... rest ] = list ; const updatedAcc = reducer ( initialValue , first ); return myReduce ( rest , updatedAcc , reducer ); } } We need to access the next value to be reduced, hence the list\ndestructuring. We then apply the reducer function with the initialValue and that next list value. Because this is a recursive function, we can think\nof the accumulator value being the initialValue of that given step of\nthe reduce. With our new accumulator value ( updatedAcc ) in hand, we can continue to\nreduce with a recursive call on the rest of the list. And that's it. In just a couple lines of code we've implemented reduce with JavaScript (ES6) primitives. Let's update countWords to use our version of reduce : const countWords = wordList => { return myReduce ( wordList , {}, ( acc , word ) => { acc [ word ] = ( acc [ word ] || 0 ) + 1 ; return acc ; }); }; Conclusion The reduce method is a bit unapproachable. Yet it is a powerful and\nversatile method that can take the place of methods like map , find , and filter . We broke reduce down into its constituent parts and then\nimplemented our own from scratch. Go forth and reduce! Cover photo: unsplash-logo Reuben Teo Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-02-26"},
{"website": "Hash-Rocket", "title": "Vim Hashrocket", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/exploring-hashrockets-vimbundle-vim-hashrocket", "abstract": "Vim Hashrocket Projects Vim Hashrocket by\nJake Worth\n\non\nApril 20, 2017 Hashrocket committed to Vim early in our history, and it's been a crucial decision. When we set up a new development machine, we\ninstall the Dotmatrix , a library of\nconfigurations that includes our favorite, must-have Vim plugins. This blog focuses on Vim Hashrocket . Vim Hashrocket Vim Hashrocket is a collection\nof settings our team shares. Spend enough time\nwriting code with Vim on a shared machine, and you'll find that the Vim configuration\nfile in the root directory ( .vimrc ) swells to hundreds of lines,\nas each developer adds their own favorite settings. Vim Hashrocket codifies that into a plugin. It's a neatly organized\njunk drawer that reflects many of our evolving goals as a development team. Here are a few noteworthy settings in the 227-LOC hashrocket.vim plugin. NotRocket :NotRocket (line 36) is one of those incantations that makes Vim users feel like\nmagicians. NotRocket substitutes pre-Ruby 1.9 hashrockets ( => ) for Ruby\n1.9-and-after colons ( : ).  Both are supported in modern versions of\nRuby, but many people prefer the colon, and whichever you choose, consistency\nis a worthy goal. :NotRocket helps us standardize legacy code. Rails and Gem Projections Lines 62-144 define a series of projections for use with Projectionist.vim . What are\nprojections? These are command-mode commands like :Econtroller users , which\nhelps you navigate to a UsersController or provides a boilerplate file if it does not exist. We start by extending the existing projections provided in other libraries, adding\na presenter projection to support the Presenter pattern. Next, we define a series\nof rails_gem_projections – projections that leverage Bundler.vim to\nadd projections based on the contents of your Rails Gemfile . Using the gem Cucumber?\nYou'll get an :Efeature command. Factory Girl? :Efactory . These commonsense\nmappings save time. Mappings hashrocket.vim continues with a series of great copy-and-paste mappings, including mappings to copy to\nend of line, copy to system clipboard, and copy a whole file to system clipboard. Lines 162 and 163 add two insert-mode mappings I love: bpry and ipry , which extend to require 'pry'; binding.pry; and require IEx; IEx.pry; respectively. We use these to require and bind leading debuggers for Ruby and Elixir. Lines 190 to 207 add the command :UnusedSteps , which identifies unused Cucumber step definitions in your integration test\nsuite. This command is useful for cleanup on a large test-driven application. We also turn spell checking on as a default in Git commit messages, with: autocmd FileType gitcommit setlocal spell One mapping I like a lot was contributed by Josh Branchaud, on line 223: autocmd FileType help nnoremap q : q < cr > This lets us quit Vim help pages with q in normal mode (no semicolon), the same way you\ncan quit other terminal programs. Here are two more excellent mappings from lines 224 and 225: autocmd FileType ruby nmap < buffer > < leader > bt < Plug > BlockToggle\nautocmd BufRead *_spec . rb map < buffer > < leader > l < Plug > ExtractRspecLet The first mapping gives us easy access to the features of vim-blockle , which swaps Ruby blocks\nwritten in the curly braces syntax ( {} ) to the do-end syntax ( do end ), and\nvice versa. When to use one or the other is mostly subjective, and with this\nmapping, we can rapidly compare each choice. The second mapping calls the :ExtractRspecLet from the vim-weefactor plugin, swapping\nlocal variables defined in an RSpec examples for let blocks: # spec/application_spec.rb user = FactoryGirl . create ( :user ) # => :ExtractRspecLet let ( :user ) { FactoryGirl . create ( :user ) Again, this choice can be subjective, and with this mapping and the underlying\nlibrary, we get to compare each option with one command. The last mapping in the file is a blockbuster. If you're editing an SQL file in\na Tmux session, run <leader>t from normal mode, and your file will be read\ninto the session and window of your choice: autocmd FileType sql nmap < buffer > < leader > t :< C - U > w \\ | call Send_to_Tmux ( \"\\\\i \" . expand ( \"%\" ). \"\\n\" )< CR > Start a psql session, and you've got a fast way to iterate on an SQL statement from Vim. The command even writes the file for you before sending it to the window. Conclusion How does this apply to you? Consider writing your own Vim plugin that captures\nyour favorite settings, install Vim Hashrocket to write code the way we do, or\ninstall the Dotmatrix for this and many other settings. Not every person on our\nour team has all of these plugins on their personal machines, but the shared\ntooling is vital when we start collaborating. No exploration of a Hashrocket Vim plugin would be complete without thanking Tim Pope , a Rocketeer who built many of these libraries, as well as all the contributors who have\nmade them better over the years. If you're in Chicago, stop by Vim\nChicago to talk plugins and more. Likewise, the 'Today I Learned' #vim channel contains\nhundreds of our favorite tips and tricks to leverage this classic editor. Stay tuned as I continue to explore the Hashrocket Vimbundle in future posts. Photo Credit: Barn Images, https://unsplash.com/photos/t5YUoHW6zRo . Accessed 15 April 2017. Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-04-20"},
{"website": "Hash-Rocket", "title": "Introducing PG Casts", "author": ["\nJosh Branchaud\nand\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/introducing-pgcasts", "abstract": "Hashrocket Projects Introducing PG Casts by\nJosh Branchaud\nand\nJake Worth\n\non\nJune 16, 2016 We are excited to share our latest project, PG Casts , with the world. At Hashrocket, we love PostgreSQL. Postgres is the relational database of choice for Hashrocket projects. We like it because it's free, well supported, and very friendly to developers. Postgres offers advanced features, outstanding data integrity guarantees, and competitive performance. Hashrocket has invested in this tool and all of our developers seek mastery of it. We love nothing more than helping our clients take full advantage of Postgres. In order to share our Postgres expertise with everyone else, we are introducing PG Casts . PG Casts are short, free weekly screencasts covering beginner to advanced level tips and tricks for getting the most out of a Postgres database. This project is a collaboration between Rocketeers across the company. Look for new screencast announcements on Twitter ( @postgresqlcasts ). May your data be valid, and your queries performant. Stay Up-to-date with PG Casts News! Email Address Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-06-16"},
{"website": "Hash-Rocket", "title": "Today I Learned 2018: Year In Review", "author": ["\nSuzanne Erin\n\n"], "link": "https://hashrocket.com/blog/posts/today-i-learned-2018-year-in-review", "abstract": "Hashrocket Projects Today I Learned 2018: Year In Review by\nSuzanne Erin\n\non\nJanuary 15, 2019 Today I Learned is an open-source Elixir Phoenix project by Hashrocket. One of the fun things about January is being inspired to look back at the past year. This blog revisits some of the top trends and posts in Today I Learned of 2018. In 2018, we at Hashrocket posted 379 TILs. On average, we collectively were learning something new every day! Our top-two most popular posts of the year were both #elixir posts: Parameter Filtering in Elixir Phoenix Logs Your slowest Elixir Tests The three topics that we posted TILs about most often were: #javascript ( 56 posts) #react ( 52 posts) #reasonml ( 44 posts) A huge portion of our work this year involved React and JavaScript so it is no surprise that we were learning a lot in those areas! Here are the top posts from each of those categories: Share SCSS Variables with Javascript create-react-app Has A Default Test Setup File Format The Current ReasonML File Within Vim Our top two most loved channels were the same as above, but #command-line squeezed onto the podium as the number three most loved topic. Everyone loves a good command line trick! This end-of-year post was the most-loved command line TIL of 2018: Easily delete a long word in terminal In order to come up with the 2018 stats, I had to learn how to write some Postgres queries. Of course, it is only appropriate that I posted about what I learned in a TIL. You can read it here (and find out who had the on-average most-loved TILs of 2018): First Postgres Queries: a Join and an Alias As interesting as it is to look back on all the things we learned in 2018, it is even more exciting to consider the year ahead. Will we see a uptick in Elixir posts? Will we discover even more Vim tricks? Our goal is to keep learning new things every day, and to open up new areas of exploration. Here's to TIL in 2019! Photo by unsplash-logo Ahmad Ossayli from Unsplash Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-01-15"},
{"website": "Hash-Rocket", "title": "How To Quit Vim", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/how-to-quit-vim", "abstract": "Vim How To Quit Vim by\nJosh Branchaud\n\non\nFebruary 12, 2019 A lot of people would have you believe that Vim is near impossible to quit.\nI'm here to dispel the rumors and show that there are quite a few ways to\nquit Vim. The Basic Quit First thing if you start up a new Vim session with just vim , you'll see a\nsort of splash screen. Below the title and copyright information is this\nline: type  :q<Enter>               to exit This is a good place to start because it is perhaps the most straightforward\nway to quit a Vim session -- the :q or :quit command. It does come with\nsome minor caveats. Here is the first caveat. If the current file (or some other file open in\nanother buffer) has been edited, but not yet saved, then Vim will stop you\nfrom quitting. It wants to make sure you don't accidentally lose your\nchanges. If you're sure you want to quit and discard any unsaved changes, then you\ncan use a more forceful version of the quit command -- :q! . This will quit\nwithout writing changes to the modified file. The other caveat to the :quit command is that what it is really doing is\nquitting the current window . If you have split windows or multiple tabs\nopen, then :q will only quit the one that is in focus. Save And Quit More often than not -- whether we use Vim as our primary editor or just need\nit occasionally to edit files on a remote server -- we want to save the\nchanges we make. We can write the changes to the current file to disc and\nquit in one command with :wq . Something to note about the :wq command is that it will always do a disc\nwrite, even if the file hasn't actually been edited. That means the modified date on the file will be updated even if no actual changes were\nmade to the file. If you'd like only do a disc write when the file has\nactually changed, then you should use the :x command. It is short for :xit . It will write only if changes have been made and then quit. Normal Mode Quits There are two keybindings that allow you to quit from Normal mode, rather\nthan Command mode. The ZZ binding can be used to write and quit. It works in the exact same\nway as :x . You may find it easier to hold shift and hit Z twice than to\nfumble for the : key. Do what works best for you. There is an accompanying ZQ binding which quits without checking for\nchanges. It works in the same way as :q! . Rage Quit Since the :q command only quits the current window or tab, we have the :qa (or :qall ) command which we can use to quit all of it at once. Vim\nwill only quit files that are unmodified. If you'd like to quit everything\nand ignore unwritten changes, use the more forceful version, :qa! Another command that behaves similarly is :cq . Vim describes this command\nlike so: Quit always, without writing, and return an error code. It works nearly the same as :qa , but returns a non-zero exit code. One\nplace in particular that I have found this useful is quitting out of a Git\ncommit message screen, especially during an amend, when I want to back out\nof the commit. Conclusion Vim gets a tough rap for being hard to use and hard to quit. This post\nshows that there are quite a few different ways to quit a Vim session. Each\nof these ways can be utilized in different situations depending on the\neffect you want quitting to have. Cover photo: unsplash-logo Alexander Andrews Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-02-12"},
{"website": "Hash-Rocket", "title": "Generate Images for Instagram", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/generate-images-for-instagram", "abstract": "Generate Images for Instagram by\nJake Worth\n\non\nFebruary  5, 2019 Followers of Hashrocket on social media will note our renewed presence\non Instagram, punctuated by a collection of custom images. In this post, I\nwill try to reverse-engineer these images using ImageMagick. Hashrocket project manager Suzanne Erin recently started creating a series of\nengaging images for our Instagram account to pair with blog posts. An example: Soon, I started to wonder if I could reverse-engineer these\nimages with code. Here’s what I came up with: In this post, I'll explain how I created this image. Concept I wanted a CLI accepting two arguments: a background image and a blog post\ntitle. With no other interaction, it should return a finished image. Part of what powers this idea are the folks at Unsplash , who provide wonderful free photographs. I'm a huge\nfan of their service, and always try to give the appropriate credit. These images would work with stock photography, but it would cost money and be a little less fun. Here's what I wanted the program to do: Resize the image Crop the image into the ideal dimensions for Instagram Darken the image so text can be readable on top Add our company logo Add a blog header Add a custom blog title Implementation I decided to use ImageMagick for the image manipulation. ImageMagick is a FOSS suite for\ndisplaying, converting, and editing images. The first and hardest task was to build the image editing script. I haven't\ndone much image processing, so I knew this would be an iterative process. To minimize the thrashing, I decided to break the manipulation into steps, each with its own output. That way, I could inspect each version of the file on its own and diagnose issues. I started by resizing the image to a workable size, noting that the resize function forces the image into a box matching the dimensions you specify. I\nchose to make the resized image bigger (2000 x 2000) than my crop so\nthat I would always have enough material to work with. # Resize magick \"input.jpg\" -resize 2000x2000 output/resized.jpg Next, I cropped the image to 1080 x 1080, which seemed like a good shape for Instagram. # Crop (1080 x 1080) magick output/resized.jpg -crop 1080x1080+0+460 output/cropped.jpg The numbers 0+460 are X/Y coordinates telling the crop function where to start cropping. They also seem to force the\nprogram to make one crop and stop. These numbers put the crop at the\nleft-middle of the source image, which looks okay most of the time. It could always be made customizable later. Next, I darkened the image, so any text on top of would be readable. # Darken image magick convert output/cropped.jpg -fill black -colorize 40% output/darkened.jpg Then, I superimposed the Hashrocket logo, 100 pixels down\nand to the right on the upper-left-hand corner. Resizing this PNG outside of the program helped me cut down on the repetitive work the program had to do. # Superimpose logo magick composite -compose atop -geometry +100+100 assets/logo.png output/darkened.jpg output/with-logo.jpg Next, I added the header. This required downloading the Hashrocket logo font,\nCircular Standard. Saving the font file in the root directory and referencing it in a relative\npath makes the program more portable. # Add header magick convert output/with-logo.jpg -fill white -font fonts/CircularStd-Book.otf \\ -pointsize 100 -annotate +100+500 'Blog | Hashrocket' output/with-title.jpg Finally, I added the blog post title, using a bold version of the same font family. # Add blog title magick convert output/with-title.jpg -fill white -font fonts/CircularStd-Bold.otf \\ -pointsize 100 -annotate +100+630 \"My great blog post\" output/ready-for-instagram.jpg On top of this I added a Thor CLI. Here's the final command that generated my image. $ ruby ./cli generate --source source.jpg --title \"Generate Images\\nfor Instagram\" And here's the script after some continued work. We add a setting so the script exits if any command doesn't succeed, we reorder the conversions, we ditch magick so it works on more versions of ImageMagick, and we pipe the output so there are no physical files written. #!/bin/bash set -eu -o pipefail # Resize, crop, darken convert - \\ -resize 2000x2000 \\ -crop 1080x1080+0+460 \\ -fill black -colorize 40% \\ jpg:- | # Superimpose logo composite \\ -compose atop -geometry +100+100 \\ assets/logo.png \\ jpg:- jpg:- | # Add header and title convert \\ -fill white -font fonts/CircularStd-Book.otf \\ -pointsize 100 -annotate +100+500 'Blog | Hashrocket' \\ -fill white -font fonts/CircularStd-Bold.otf \\ -pointsize 100 -annotate +100+630 \" $1 \" \\ jpg:- jpg:- Follow Hashrocket on Instagram to see the final version of this image. Conclusion Fun project! I recently added a web layer to this service, so my teammates can generate images without any setup. Thanks to Suzanne for inspiring this project, and to Thomas Allen for helping me clean up the script. Photo by Casey Horner on Unsplash Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-02-05"},
{"website": "Hash-Rocket", "title": "How I Built My Own Heroku for Phoenix Apps: Part 2", "author": ["\nMicah Cooper\n\n"], "link": "https://hashrocket.com/blog/posts/how-i-built-my-own-heroku-for-phoenix-apps-part-2", "abstract": "Elixir Phoenix How I Built My Own Heroku for Phoenix Apps: Part 2 by\nMicah Cooper\n\non\nSeptember 13, 2016 Building upon the the first version of Gatling. We're adding functionality to handle event hooks for each action in the deployment process Click here to read part 1 first. Gatling is a deployment tool in for Phoenix apps. In this post, we'll walk through how I updated gatling to add a lot more flexibility to the deployment process. First, let's walk through all the steps gatling takes to execute the mix gatling.upgrade task and the elixir functions that execute them. Remember, this is triggered by a a git post-update hook on the server. $ mix deps.get                   -> mix_deps_get\n$ mix compile                    -> mix_compile\n$ mix digest                     -> mix_digest\n$ mix release                    -> mix_release\n$ mkdir /path/to/deploy          -> make_upgrade_dir\n$ cp -r release /path/to/deploy  -> copy_release_to_upgrade\n$ sudo service <project> upgrade -> upgrade_service\n# configure nginx                -> configure_nginx All these functions are already defined and execute on the server. But I wanted the ability to execute some elixir code before and/or after each of these steps and I wanted to configure this in each app I deployed as each app my require different steps. I wanted to put a file called ./upgrade.ex in my project. And when I deploy with a git push, that file would be picked up by Gatling to execute any hooks I defined. Here is an example of one of these hooks: #./deploy.ex defmodule MyProject . UpgradeHooks do def before_mix_deps_get ( _env ) do #do some work #log some things #track some things end end With this above example, every time we do a git push, Gatling would find the ./upgrade.ex and call before_mix_deps_get right before the default mix_deps_get function. How it's made Now we know the desired functionality, lets walk throught the implementation. Here is the (abbreviated) original verion of the Mix.Tasks.Gatling.Upgrade module: defmodule Mix . Tasks . Gatling . Upgrade do def upgrade ( project ) do Gatling . env ( project ) |> mix_deps_get |> mix_compile |> mix_digest |> mix_release |> make_upgrade_dir |> copy_release_to_upgrade |> upgrade_service |> configure_nginx end end We want to have a wrapper function around each of these that will hook in a before and after function. Let's just use call . So our upgrade function will now look lik this: defmodule Mix . Tasks . Gatling . Upgrade do def upgrade ( project ) do Gatling . env ( project ) |> call ( :mix_deps_get ) |> call ( :mix_compile ) |> call ( :mix_digest ) |> call ( :mix_release ) |> call ( :make_upgrade_dir ) |> call ( :copy_release_to_upgrade ) |> call ( :upgrade_service ) |> call ( :configure_nginx ) end end call/1 Let's see how call works: 1 def callback ( env , action , type ) do 2 module = env . upgrade_callback_module 3 callback_action = [ type , action ] 4 |> Enum . map ( & to_string / 1 ) 5 |> Enum . join ( \" _\" ) 6 |> String . to_atom () 7 8 if function_exported? ( module , callback_action , 1 ) do 9 apply ( module , callback_action , [ env ]) 10 end 11 12 nil 13 end 14 15 def call ( env , action ) do 16 callback ( env , action , :before ) - 17 apply ( __MODULE__ , action , [ env ]) 18 callback ( env , action , :after ) 19 env 20 end Starting with callback/3 on line:1 we take in the following parameters: env => A struct with all the information we need for a deploy action => An atom of the function we want to call e.g. :mix_deps_get type => An atom of the callback type. Either :before or :after line:2 Assign our callback module which was previously loaded from the file ./upgrade.ex line:3 - 6 Assign our callback function. :before_mix_deps_get line:8-10 If the function exists in the module we defined in .upgrade.ex , then execute it now. line:12 Return nil. We don't want this callback function to be able to change the current env in any way. So we return nil to express that. In our call/2 function we take in the following parameters env => A struct with all the information we need for a deploy action => An atom of the function we want to call e.g. :mix_deps_get You'll notice these arguments are the same as callback/3 just without a type. line:16 Execute the before action callback e.g before_mix_compile line:17 Execute the actual action e.g. mix_compile line:18 Execute the after action callback e.g. after_mix_compile line:20 Return the env struct so it can be used by the next function in the pipeline. And that's it! It's actually quite simple. There is still one part missing though. How did that env.upgrade_callback_module get in there. How is Gatling loading our ./upgrade.ex file and using it in its own project? Loading code from another project Before we start loading code, lets look at hour our env is created. First we have our %Gatling.Env{} struct. For brevity's sake, we're only seeing the relavent attributes here: defmodule Gatling . Env do defstruct ~w[\n    #...\n    upgrade_callback_module\n    #...\n  ]s end Our Gatling module is what actually populates this struct: 1 defmodule Gatling do 2 def env ( project ) do 3 % Gatling . Env { 4 #... 5 :upgrade_callback_module => callback_module ( project , task: \" upgrade\" ), 6 #... 7 } 8 end 9 10 def callback_module ( project , [ task: task_name ]) do 11 callback_path = Path . join ( path / to / project , \" #{ task_name } .ex\" ) 12 if File . exists? callback_path do 13 Code . load_file ( callback_path ) |> List . first () |> elem ( 0 ) 14 else 15 nil 16 end 17 end 18 end When we call Gatling.env we populate upgrade_callback_module with callback_module/2 : line:11 build the path that points to the deployed project and assign it line:12-16 If the file exists, use the Code module to load the elixir file into the Gatling runtime When we call Gatling.env.upgrade_callback_module we'll have assess to the module we defined in ~/upgrade.ex which (again) looks like this: defmodule MyProject . UpgradeHooks do def before_mix_deps_get ( _env ) do #do some work #log some things #track some things end end And that's it! That's all the moving parts and in my opinion, it's quite a minimal effort to gain the flexablity I desired. This concludes part 2 of this blog serires. Next, we'll look into migrating Gatling's underlying dependency exrm with it's successor - Distillery Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-09-13"},
{"website": "Hash-Rocket", "title": "Keep Anaconda from Constricting Your Homebrew Installs", "author": ["\nIfu Aniemeka\n\n"], "link": "https://hashrocket.com/blog/posts/keep-anaconda-from-constricting-your-homebrew-installs", "abstract": "Keep Anaconda from Constricting Your Homebrew Installs by\nIfu Aniemeka\n\non\nAugust 29, 2017 Fixing conflicts between Anaconda and Homebrew I recently installed Anaconda to my local machine and noticed that Anaconda modified my PATH variable. For Mac installations, if you check your .bash_profile, you should see something like the following: # Added by Anaconda3 4.4.0 installer export PATH = \"/Applications/anaconda/bin: $PATH \" Now if you run brew doctor , you'll get the following gnarly message: Warning: Anaconda is known to frequently break Homebrew builds, including Vim and\nMacVim, due to bundling many duplicates of system and Homebrew-available\ntools.\n\nIf you encounter a build failure please temporarily remove Anaconda\nfrom your $PATH and attempt the build again prior to reporting the\nfailure to us. Thanks!\n\nWarning: python is symlinked to python3\nThis will confuse build scripts and in general lead to subtle breakage.\n\nWarning: \"config\" scripts exist outside your system or Homebrew directories. ` ./configure ` scripts often look for * -config scripts to determine if software packages are installed, and what additional flags to use when\ncompiling and linking.\n\nHaving additional scripts in your path can confuse software installed via\nHomebrew if the config script overrides a system or Homebrew provided\nscript of the same name. We found the following \"config\" scripts:\n  /Applications/anaconda/bin/curl-config\n  /Applications/anaconda/bin/freetype-config\n  /Applications/anaconda/bin/icu-config\n  /Applications/anaconda/bin/libpng-config\n  /Applications/anaconda/bin/libpng16-config\n  /Applications/anaconda/bin/python3-config\n  /Applications/anaconda/bin/python3.6-config\n  /Applications/anaconda/bin/python3.6m-config\n  /Applications/anaconda/bin/xml2-config\n  /Applications/anaconda/bin/xslt-config Basically, the universe will collapse upon itself because you thought it might be cool to try a Kaggle competition. Damn. Fortunately, the message includes a fix - remove Anaconda from $PATH, run brew , then add Anaconda back to $PATH. Having said that, doing that every time I use Homebrew sounds like a pain in the backside. So here's a bash script to do it for you so you can keep on being lazy. export SANS_ANACONDA = \"/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin\" # added by Anaconda3 4.4.0 installer export PATH = \"/Applications/anaconda/bin: $SANS_ANACONDA \" alias perseus = \"export PATH=\" \\$ SANS_ANACONDA \" && echo Medusa decapitated.\" alias medusa = \"export PATH=\" /Applications/anaconda/bin: \\$ SANS_ANACONDA \" && echo Perseus defeated.\" brew () { perseus command brew \" $@ \" medusa } There are aliases for the addition and removal of Anaconda from $PATH to make doing it manually even easier. Happy (data science) coding! Photo by Samuel Zeller on Unsplash Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-08-29"},
{"website": "Hash-Rocket", "title": "Simple PostgreSQL Upgrades in Development", "author": ["\nJack Christensen\n\n"], "link": "https://hashrocket.com/blog/posts/simple-postgresql-upgrades-in-development", "abstract": "PostgreSQL Simple PostgreSQL Upgrades in Development by\nJack Christensen\n\non\nAugust  8, 2017 Upgrading a production PostgreSQL server can be a complex and delicate process. Downtime must be minimized. Writes must be prevented while the upgrade is in progress to avoid data loss. For a large database, excessive storage usage during the upgrade may be a problem. Many other guides describe how to handle these and related issues. But in a development environment downtime is not an issue, writes can be avoided simply by not using the database, and database size is typically fairly small. For these cases performing the same upgrade process as production can be overkill. There is a simpler way. This guide is not platform specific, but assumes that PostgreSQL was installed with a package manager or you otherwise know how to install and remove it. First, use pg_dumpall to backup the entire database cluster. pg_dumpall will write all the SQL necessary to recreate all users and databases in the cluster. ` $ pg_dumpall > pg95.sql ` Next, remove your existing version of PostgreSQL. This could be done with apt remove for a Debian/Ubuntu or brew uninstall of MacOS. Strictly speaking, it's not necessary to remove the old version first. brew will try to do an upgrade and apt will install multiple versions of PostgreSQL side-by-side. But additional edge cases and complications can arise in these cases. Once the old PostgreSQL server is uninstalled, install the new version. In Debian/Ubuntu this would be something like apt install postgresql-9.6 (prefer installing a specific version to the postgresql metapackage) or brew install postgresql in MacOS. Lastly, we need to run psql -f pg95.sql . However, the newly installed version of PostgreSQL will only have the postgres user. This means we need to connect to PostgreSQL server as the postgres user. On Debian/Ubuntu the user must be authenticated via the OS user. So we need to sudo to the postgres user. $ sudo -u postgres psql -f pg95.sql On MacOS with homebrew the user can be specificied via argument to the psql command without any authentication. $ psql -U postgres -f pg95.sql You can read more details in the documentation . Dawid Zawiła Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-08-08"},
{"website": "Hash-Rocket", "title": "Where Am I: URL Assertions with Cypress", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/where-am-i-url-assertions-with-cypress", "abstract": "Javascript React Where Am I: URL Assertions with Cypress by\nJosh Branchaud\n\non\nAugust 23, 2018 The URL that appears in the browser's URL bar is one of the first and\nprimary ways that users interact with our web apps. The URL tells the app\nand the user of their location within the route hierarchy of the app. It\nsometimes even contains state. The URL consequently serves as an excellent\nfirst assertion when writing integration tests. When writing Cypress integration tests, there are\nseveral ways for us to assert about the current URL. The cy.url() and cy.location() functions are the primary ways for us to gain access to information about\nthe current state of the URL. From there, we can perform a variety of\nassertions. We can assert about the entire URL: // URL: http://localhost:8000/pokemon cy . url (). should ( 'eq' , 'http://localhost:8000/pokemon' ); // ✅ passes This is certain the most precise assertion. The tradeoff is that it requires\nus to know something about the environment configuration (namely, baseUrl ). This can get verbose and make our tests brittle. Alternatively, we can make a partial assertion about the URL: // URL: http://localhost:8000/pokemon cy . url (). should ( 'contain' , '/pokemon' ); // ✅ passes This frees us from having to specify the full path with the base URL. This,\ntoo, has a potential drawback. This way of asserting about the URL can be\noverly permissive and may result in a false-positive. Imagine we are writing a test for our pokemon show page. As part of that\ntest we want to be sure that the back button works. We are on the show\npage for a specific pokemon ( /pokemon/2 ) and our back button is not\ncorrectly wired up yet. Here is our assertion: // Click: 'Back' -- no change in URL // URL: http://localhost:8000/pokemon/2 cy . url (). should ( 'contain' , '/pokemon' ); // ✅ passes (false positive) What we'd like is a stricter assertion without the need to specify the base\nURL. This is where cy.location() can help. // Click: 'Back' -- no change in URL // URL: http://localhost:8000/pokemon/2 cy . location ( 'pathname' ). should ( 'eq' , '/pokemon' ); // ❌ fails This time our assertion correctly fails because /pokemon does not match\nthe current pathname of /pokemon/2 . Once we update our application code\nwith a working back button we should see this test pass. Going forward we\nwill have the confidence that if the back button breaks again, so will the\ntest. We looked at a couple ways to assert about the current state of your app's\nURL. Though cy.url() can certainly be used to this end, I recommend\nutilizing cy.location() . There is a lot more to cy.location() than we\ncovered in this post. Check out the examples in\nthe Cypress docs for all the other ways this function can be used to assert\nabout the URL. Want to see more of the code? Check out the supplementary repository . Cover photo credit: unsplash-logo Jean-Frederic Fortier Was this post helpful? Share it with others. Tweet Share Post", "date": "2018-08-23"},
{"website": "Hash-Rocket", "title": "The Adventures of Generating Random Numbers in Erlang and Elixir", "author": ["\nDorian Karter\n\n"], "link": "https://hashrocket.com/blog/posts/the-adventures-of-generating-random-numbers-in-erlang-and-elixir", "abstract": "Elixir The Adventures of Generating Random Numbers in Erlang and Elixir by\nDorian Karter\n\non\nSeptember 20, 2016 While working in Elixir I needed to generate a random number and to my surprise could not find an easy way to do so without using Erlang (at first - see below). That lead me to learning a few interesting things about random seeds, how to create secure ones and how Erlang handles random. Erlang comes out of the box with two modules in charge of generating pseudo-random values: random and rand . The Erlang documentation for random warns that one should use \"the improved\" rand instead of random for generating random numbers.  Another (less official) site mentioned that random is scheduled to be removed in Erlang/OTP 20. I wondered what the difference between rand and random was so I fired the Erlang repl ( erl ) and typed: 1 > random : uniform (). 0 . 4435846174457203 Then I closed and re-opened erl and ran that same command again: 1 > random : uniform (). 0 . 4435846174457203 Surprisingly we get the same answer for random. That's not very random. Turns out random uses the same seed by default for every VM instance. That's not great, to say the least. By contrast, the new and improved rand will use a different seed each time: Eshell V8 . 0 . 2 ( abort with ^ G ) 1 > rand : uniform (). 0 . 6015998630734499 After restarting erl : Eshell V8 . 0 . 2 ( abort with ^ G ) 1 > rand : uniform (). 0 . 05250536227383032 Much better. So that's it right?? Not so fast... The documentation for rand states that the default seed is not cryptographically strong, and suggests that we use \"one of the functions in crypto \" to generate a cryptographically strong seed. There are however no examples whatsoever as to how one would do that and I had to dig deep to discover what follows. What does it mean for a seed to be safe? (a bit of a tangent) I found an excellent explanation online by Thomas Pornin that I think explains it better than I could: When you generate a private key, you do so with a source of randomness. If that source of randomness can output N different streams of bits, then, at most, you may get N different private key. This is where we like to talk of entropy , which is a measure of how big that N is. When the source of randomness is said to offer \"100 bits of entropy\", then it means that (roughly) N = 2100. The attacker will want to obtain your private key. If he knows that you use a weak source with a low N (say, only 40 bits of entropy), then he can, on his own machine, enumerate all possible outputs from the random source and work out the corresponding private key. For instance, suppose that you used a PRNG seeded with the current time, expressed in microseconds. This is the time as known by your machine. The attacker assumes that your machine is reasonably well set with the current time, say within 10 seconds. So, from the point of view of the attacker, the seed for your PRNG is known within a 10 seconds range; since the PRNG use the time in microseconds, that leaves him with N = 10000000 possible seeds. The attacker then says to himself: \"IF that guy used as seed value x, THEN his code produced private key value Kx; let's see if that matches his public key... nope. So he did not use x. Let's try again with x+1 (and so on).\" So a weak PRNG is deadly in such situations. How to generate cryptographically strong seeds with the new rand library If you are still stuck in an old version of Erlang you can seed random like this: erlang\nseed = crypto:bytes_to_integer(crypto:strong_rand_bytes(12)).\nrandom:seed(seed). To generate a cryptographically strong seed for the rand library we can use the following code: << I1 : 32 / unsigned - integer , I2 : 32 / unsigned - integer , I3 : 32 / unsigned - integer >> = crypto : strong_rand_bytes ( 12 ). rand : seed ( exsplus , { I1 , I2 , I3 }). % then we can use rand:uniform as usual and all of our generated numbers will be based of the secure random seed rand : uniform ( 1000 ). Remember that there is a performance penalty for generating cryptographically secure seeds so you would want to be strategic about seeding your random number generator. In the meantime a quickrand library has appeared which enforces proper random number seeding: https://github.com/okeuday/quickrand How to generate a secure random string To generate a secure random string for an authentication token you can use the following: base64 : encode ( crypto : strong_rand_bytes ( N )). Where N is the number of bytes to generate. Here is an example of generating a 20 byte string by encoding strong_rand_bytes with base64: 1 > base64 : encode ( crypto : strong_rand_bytes ( 20 )). << \"XKRL5NDnnuIWo0KD9Taz4A6JSZw=\" >> Using rand in Elixir Using rand in Elixir is as easy as calling it as an atom and replacing the : operator with a . operator: :rand . uniform () In Elixir there is no need to terminate the statement with a . . To generate a cryptographically strong random number in Elixir by way of interop to Erlang: << i1 :: unsigned - integer - 32 , i2 :: unsigned - integer - 32 , i3 :: unsigned - integer - 32 >> = :crypto . strong_rand_bytes ( 12 ) :rand . seed ( :exsplus , { i1 , i2 , i3 }) :rand . uniform ( 1000 ) # generate random number from 0 - 1000 This conversion from Erlang to Elixir was not as trivial to me. Particularly the binary pattern matching to 32-bit integers. Fortunately users Ben Wilson ( benwilson512 ) and zackehh on the Elixir Slack channel were able to help. Enum.random After writing this blog post I discovered a more idiomatic way of generating random numbers in Elixir using the Enum.random function. The function is described as such def random(enumerable)\n\nReturns a random element of an enumerable.\n\nRaises Enum.EmptyError if enumerable is empty.\n\nThis function uses Erlang's :rand module to calculate the random value. Check\nits documentation for setting a different random algorithm or a different seed.\n\nThe implementation is based on the reservoir sampling\n(https://en.wikipedia.org/wiki/Reservoir_sampling#Relation_to_Fisher-Yates_shuffle)\nalgorithm. It assumes that the sample being returned can fit into memory; the\ninput enumerable doesn't have to, as it is traversed just once.\n\nExamples\n\n┃ # Although not necessary, let's seed the random algorithm\n┃ iex> :rand.seed(:exsplus, {1, 2, 3})\n┃ iex> Enum.random([1, 2, 3])\n┃ 2\n┃ iex> Enum.random([1, 2, 3])\n┃ 1 What is not mentioned in this documentation is that this method can also take ranges. So say we want to generate a random number from 1 to 1,000 we could call it like so: Enum . random ( 1 .. 1_000 ) Like the documentation suggests, Enum.random uses Erlang's :rand module behind the scenes, but does not seed it securely by default. If you are going to use Enum.random for cryptographic purposes and need it to be secure you should still seed it as in my examples above. I hope you found this blog post educational and that it shed some light on random seeding concepts in Erlang and Elixir. Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-09-20"},
{"website": "Hash-Rocket", "title": "SQL: Inner Join vs. Outer Join", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/sql-inner-join-vs-outer-join", "abstract": "PostgreSQL SQL: Inner Join vs. Outer Join by\nJosh Branchaud\n\non\nJanuary 29, 2019 A standard join in SQL is implicitly an inner join . This means that only\nrecords that can be matched on both sides of the join will be included in\nthe result set. Here is an example of that: > select * from table_a join table_b on table_a . letter = table_b . letter ; name | letter | name | letter -------+--------+--------+-------- Brian | A | Dunn | A Derek | C | Parker | C That same query could be written with inner join . We can alternatively write an outer join which will ensure that the subject of the join has all of it's rows included even if there isn't a\nmatching row from the other table. So, which table is the subject of the join? If it is a left outer join ,\nthen it is the table on the left of the join. If it is a right outer\njoin , then it is the table on the right of the join. Here is a left outer join : select * from table_a left outer join table_b on table_a . letter = table_b . letter ; name | letter | name | letter --------+--------+--------+-------- Brian | A | Dunn | A Thomas | B | Ø | Ø Derek | C | Parker | C And here is a right outer join : select * from table_a right outer join table_b on table_a . letter = table_b . letter ; name | letter | name | letter -------+--------+--------+-------- Brian | A | Dunn | A Derek | C | Parker | C Ø | Ø | Worth | D Notice the difference in where full result sets come from and where null\nvalues have to be filled in depending on the left and right keywords. For reference, here are the constituent tables: > select * from table_a ; name | letter --------+-------- Brian | A Thomas | B Derek | C > select * from table_b ; name | letter --------+-------- Dunn | A Parker | C Worth | D Photo Credit: unsplash-logo Wynand Uys Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-01-29"},
{"website": "Hash-Rocket", "title": "Fun with Views and CTEs", "author": ["\nIfu Aniemeka\n\n"], "link": "https://hashrocket.com/blog/posts/fun-with-views-and-ctes", "abstract": "PostgreSQL Fun with Views and CTEs by\nIfu Aniemeka\n\non\nFebruary  1, 2018 A view is a stored query the results of which can be treated like a table. Note that it is the query that is saved and not the results of the query. Each time you use a view, its associated query is executed. A related concept is that of the common table expression or CTE. A CTE can be thought of as a short-lived view; you can only use it within the query in which it appears (you can refer to it multiple times, however). Let's say that you're the SQL-savvy owner of a store that sells kits for building robots. On your site, customers are guided through the process of selecting all of the components needed to build a quadrupedal robot, i.e. getting the right motors, microcontrollers, sensors, batteries, grippers, etc. Robots have lots of component parts and you have to make sure that the correct components for each order arrive or your customers will leave mean comments on social media. Right now you have the following tables: And those tables contain the following data: You want to see a list of all the purchases made and the components in those purchases. You might write a query like the following: select p.id as purchase_id,\narray_agg(concat('[ ', cp.component_id::text, ', ', c.name, ', ', cp.num_components::text, ' ]')) as component_info,\nsum(cp.num_components * c.price) as purchase_total_price,\ncustomer_id,\ncreated_at\nfrom\npurchases as p\ninner join component_purchases as cp on p.id = cp.purchase_id\ninner join components as c on c.id = cp.component_id\ngroup by p.id\norder by p.created_at Just a quick explanation in case anything in the query is unclear. We're joining the tables purchases , component_purchases , and components and then grouping the rows by purchase_id . SQLFiddle here . For each of the components in a purchase, the sum function adds the multiple of the number of units of a component ordered and the price of a component. The array_agg function composes an array of the details of the purchase. Notice that the sum and array_agg functions act across rows, but the concat function acts across columns. The result of our query is thus: Check out the SQLFiddle here . It's not an exceptionally long query, but you suspect that you'll be using it quite a lot and you'd prefer not to have to write the whole thing out all of the time. That's where views come in: create view purchase_information as\nselect p.id as purchase_id,\narray_agg(concat('[ ', cp.component_id::text, ', ', c.name, ', ', cp.num_components::text, ' ]')) as component_info,\nsum(cp.num_components * c.price) as purchase_total_price,\ncustomer_id,\ncreated_at\nfrom\npurchases as p\ninner join component_purchases as cp on p.id = cp.purchase_id\ninner join components as c on c.id = cp.component_id\ngroup by p.id\norder by p.created_at; Now you have a simple way to reuse this query. Let's say that you want to know all of the purchases made within the last week. Now you can just write the following: select * from\npurchase_information\nwhere\ncreated_at > now() - interval '7 months'; SQLFiddle here From within the PostgreSQL interactive terminal, you can use the command \\dv to see a list of all the views in the database. Dropping a view is as simple as dropping a table: drop view purchase_information; Note that if you drop a table using cascade , any views dependent on that table will be dropped as well. It's also possible to perform operations on the data in the underlying tables of some views. Updatable views are views where data can be edited, inserted, or deleted. To be updatable, a view must be really basic. Think something along the lines of: create view customer_info as\nselect first_name, last_name, email\nfrom\ncustomers\nwhere last_name > 'F'; Updatable views can only select rows from a single table (or updatable view). They also cannot contain 'group_by', 'limit', or 'with' clauses. You can see all of the relevant restrictions in the Postgres docs here . If you do have an updatable view, you can do something like insert into customer_info values\n('Anne', 'Droid', 'anne.droid@example.com'); A new row for 'Anne Droid' has been inserted into the customer table.\nNote that the customer_info view only includes customers whose names begin with letters after 'E'. If you were to run the following query: select * from customer_info; Anne's name would not appear in the results because her last name begins with a 'D'. A new row has been inserted into the customers table using the customer_info view, but that row is not accessible via the view. SQLFiddle here Note that you cannot insert values for columns that are present in the underlying table, but not in the view. The following query would result in an error: insert into customer_info (first_name, last_name, email, phone) values\n('Simon', 'Borg', 'simon.borg@example.com', '555-777-9999'); So, that's views in a nutshell. Let's move on to CTEs. To start with, CTE stands for 'common table expression'. They're quite similar to views except that once the query is executed, a CTE ceases to be. In the view purchase_information , we were able to get all of the components in a purchase, as well as the price of the purchase. Let's say I wanted a query that would provide me with purchase information, but also the customers to whom the purchases belonged. This can be achieved pretty easily using a common table expression.\nYou start a CTE using the keyword 'with' followed by the name of the CTE and the query itself. with\npurchases as\n(select p.id as purchase_id,\narray_agg(concat('[ ', cp.component_id::text, ', ', c.name, ', ', cp.num_components::text, ' ]')) as component_info,\nsum(cp.num_components * c.price) as purchase_total_price,\ncustomer_id\nfrom\npurchases as p\ninner join component_purchases as cp on p.id = cp.purchase_id\ninner join components as c on c.id = cp.component_id\ngroup by p.id\norder by p.created_at)\nselect\npurchases.purchase_id,\npurchases.purchase_total_price,\nconcat(cu.last_name, ', ', cu.first_name) as customer_name,\nconcat(cu.email, ', ', cu.phone) as customer_contact_info,\nconcat(sa.street1, ', ', sa.street2, ', ', sa.postal_code, ', ', sa.city, ', ', sa.region, ', ', sa.country) as customer_address,\ncomponent_info\nfrom purchases\ninner join customers as cu on purchases.customer_id = cu.id\ninner join shipping_addresses as sa on sa.customer_id = cu.id; SQLFiddle here The purchases CTE can be referred to like a table. Of course, once the query is executed, purchases doesn't exist. The results of this query are: Phew!\nNow you can send the correct robot parts to your customers. But wait! There's some complexity to the orders that you've forgotten. Some components need to be combined with other components in order to work. For the sake of simplicity, let's say that any given component requires one or fewer other components to function.\nDrawing out the relationship between our components, we see that we get a tree graph. To describe this in the database, we'll need to add another column to the component table. The subcomponent_id field will point to the component upon which the current component depends. Let's say that a customer orders the component with id 4. In order to get all of the parts the customer needs, we'll need traverse the tree all the way down. Luckily, Postgres has just the solution for this - recursive common table expressions. Recursive CTEs provide a way to allow queries to perform some action with a table, and then to perform the same action on the results of the previous action. That's probably kind of confusing. Below is the structure of a recursive common table expression: with recursive cte_name as (\n  query to produce initial working table\n\n  union/union all\n\n  recursive query\n)\n\ndo something with cte_name The recursive CTE begins much like a non-recursive CTE. You'll notice that the modifier recursive is required after with . Immediately after recursive is the name of the CTE. Above the union operator is a query that initializes the working table. The working table is the result set upon which the recursive term is currently acting. For instance, you might put select id, name from robots where name='Cindi Mayweather' . Since there is one and only one of the illustrious Cindi Mayweather, this query will return one row. Note that this query is not executed again. We'll come back to what purpose the union operator serves in a bit. For now, let's talk about the recursive query. The recursive query will make a reference to the CTE name. Wherever you see the CTE name in the query, that is a reference to the working table. Given a single component that needs to be shipped, we need to fetch the ids of all of its subcomponents, and their subcomponents, etc. To get all of the subcomponents for the component with an id of 4, we use the following query: with recursive purchase_subcomponents as (\n  select id, subcomponent_id from components\n  where id = 4\n\n  union\n\n  select components.id, components.subcomponent_id from\n  components\n  inner join purchase_subcomponents\n  on purchase_subcomponents.subcomponent_id = components.id\n)\nselect id from purchase_subcomponents; That's pretty gross-looking, so let's go through what's happening step by step. Execution of the cte starts with the non-recursive query. Running that query returns the following: This is the first working table. We see that an inner join of the working table and the components table produces the following: From this, we get the next working table. The working table is substituted for purchase_components in the recursive query, with the following result: And so on... The recursion ends when the working table has no rows, i.e. when the values for the rows are all null. [equivalent???] Now we come to what union is doing. The union operator acts on all of the working tables that have been generated, combining them all into one table. To ensure that there are no duplicate rows, you use union all . Hence the result of running the query is SQLFiddle here Boom! Nailed it. Kinda. If you recall, up top we have a list of purchases with the associated components and customer information. Well, that component list isn't complete; we have to include subcomponents as well. with recursive\npurchases_made as\n(\nselect p.id as purchase_id,\nc.id as component_id,\nc.name as component_name,\nsubcomponent_id,\nnum_components\nfrom\npurchases as p\ninner join component_purchases as cp on p.id = cp.purchase_id\ninner join components as c on c.id = cp.component_id\norder by p.created_at\n),\npurchase_components as (\n  select * from purchases_made\n\n  union\n\n  select  \n  purchase_id,\n  components.id,\n  components.name,\n  components.subcomponent_id,\n  num_components\n  from components\n  inner join purchase_components\n  on purchase_components.subcomponent_id = components.id\n)\nselect\npurchase_id,\narray_agg(concat('[', num_components::text, ' x cid:', component_id, ', ', component_name, ']')) as purchase_items,\nsum(num_components * components.price) as purchase_total_price,\nconcat(cu.last_name, ', ', cu.first_name) as customer_name,\nconcat(cu.email, ', ', cu.phone) as customer_contact_info,\nconcat(sa.street1, ', ', sa.street2, ', ', sa.postal_code, ', ', sa.city, ', ', sa.region, ', ', sa.country) as customer_address\nfrom purchase_components\ninner join components on purchase_components.component_id = components.id\ninner join purchases on purchases.id = purchase_components.purchase_id\ninner join customers as cu on purchases.customer_id = cu.id\ninner join shipping_addresses as sa on sa.customer_id = cu.id\ngroup by purchase_id,\ncu.last_name,\ncu.first_name,\ncu.email,\ncu.phone,\nsa.street1,\nsa.street2,\nsa.postal_code,\nsa.city,\nsa.region,\nsa.country; SQLFiddle here In this query, we have two CTEs. When you want to use multiple CTEs in a query, you place with at the start of the group and separate each CTE with a comma. Notice that, even though only one of the CTEs is recursive, the recursive modifier is placed directly after with , as opposed to directly before the recursive CTE. Each CTE is executed normally. Another nifty thing about using recursive is that its presence means your CTEs can be in any order, i.e. one CTE can make reference to a CTE that is defined later. If you're wondering how the component_id alias showed up in the final results, remember that aliases are preserved in the results of a union. To understand how this query works, we first need to understand what the results of the purchases_made query are. SQLFiddle here This is our first working table. Following a process similar to the one in the earlier recursive query, the first inner join results in: The results of the purchase_components CTE can be found in this SQLFiddle. I would recommend drawing out tables in a notebook to really hammer home some of the ideas presented in this blog post. And feel free to play around in the SQLFiddle examples provided. It's a pretty great tool. Related articles: Tree Traversal Algorithms Graph Algorithms in a Database: Recursive CTEs and Topological Sort in Postgres Understanding Common Table Expressions with FizzBuzz Photo by Eduard Militaru on Unsplash Was this post helpful? Share it with others. Tweet Share Post", "date": "2018-02-01"},
{"website": "Hash-Rocket", "title": "Development of a Simple Command Line Websocket Client", "author": ["\nJack Christensen\n\n"], "link": "https://hashrocket.com/blog/posts/development-of-a-simple-command-line-websocket-client", "abstract": "Community Development of a Simple Command Line Websocket Client by\nJack Christensen\n\non\nSeptember  8, 2016 While working on the Websocket Shootout , I frequently needed to connect to a websocket server and interact directly. None of the existing tools I was aware of had exactly the features I was wanted. That led to the creation of ws . The goals for ws were as follows: Telnet style interaction with remote websocket servers Persistent, readline style history Cross-platform No or minimal dependencies Go was an obvious choice for meeting the cross-platform and dependency requirements. Thanks to excellent library support the rest of the features were easy to build. The github.com/spf13/cobra is used for command line argument parsing. The Go standard library flag package would have been sufficient, but I prefer GNU style flags. cobra makes it easy to define arguments. rootCmd := &cobra.Command{\n  Use:   \"ws URL\",\n  Short: \"websocket tool\",\n  Run:   root,\n}\nrootCmd.Flags().StringVarP(&options.origin, \"origin\", \"o\", \"\", \"websocket origin\")\nrootCmd.Flags().BoolVarP(&options.printVersion, \"version\", \"v\", false, \"print version\") The websocket protocol is handled by the github.com/gorilla/websocket package. With one Dial function call, we get a connection that let's us work at the websocket message level. headers := make ( http . Header ) headers . Add ( \"Origin\" , origin ) ws , _ , err := websocket . DefaultDialer . Dial ( url , headers ) if err != nil { return err } github.com/chzyer/readline provided GNU readline style functionality in pure Go. One command initializes the readline system. rlConf := & readline . Config { Prompt : \"> \" , HistoryFile : historyFile , } // ... rl , err := readline . NewEx ( rlConf ) if err != nil { return err } Reading a line is similarly easy. line , err := s . rl . Readline () if err != nil { s . errChan <- err return } I encountered one bug/issue , but the author pushed a fix in less than a day of my post. The github.com/fatih/color library made it easy to output color text in a cross platform way. It lets you get a Sprintf style function that wraps the text in the appropriate escape characters for a given color. rxSprintf := color . New ( color . FgGreen ) . SprintfFunc () Goroutines made handling keyboard input and websocket IO simple by treating them independently. // ... go sess . readConsole () go sess . readWebsocket () // ... func ( s * session ) readConsole () { for { line , err := s . rl . Readline () if err != nil { s . errChan <- err return } _ , err = io . WriteString ( s . ws , line ) if err != nil { s . errChan <- err return } } } func ( s * session ) readWebsocket () { rxSprintf := color . New ( color . FgGreen ) . SprintfFunc () for { _ , buf , err := s . ws . ReadMessage () if err != nil { s . errChan <- err return } fmt . Fprint ( s . rl . Stdout (), rxSprintf ( \"< %s \\n \" , string ( buf ))) } } The entire program is just over 130 lines. You can check it out at github.com/hashrocket/ws . If you have a working Go installation, you can build ws with a single command: go get -u github.com/hashrocket/ws Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-09-08"},
{"website": "Hash-Rocket", "title": "Friday Lunch at Hashrocket Chicago", "author": ["\nSuzanne Erin\n\n"], "link": "https://hashrocket.com/blog/posts/friday-lunch-at-hashrocket-chicago", "abstract": "Friday Lunch at Hashrocket Chicago by\nSuzanne Erin\n\non\nJanuary 24, 2019 Friday is a special day at Hashrocket. Friday is our open source day. After spending the first half of the day working on regular client projects, Rocketeers spend the afternoon honing their skills by contributing to open source, working on side projects, brushing up on new languages, or working on company initiatives. The other very special part about Friday is lunch. We kick-off the open source afternoon by dining out together as a team. This is an important tradition, because we are divided across many separate project teams and there aren’t always as many opportunities for everyone to have a conversation together as we would like. This helps us stay connected to each other. In Chicago our office is nearby the West Loop, known for its restaurants and cafes. This means that we have many delicious dining options. How to choose? If you’ve ever tried to get 9 people to agree on anything, then you know it is no easy task. Some people want to go to the same place every time. Other people value the team lunch as an opportunity to try something new. The way we have resolved this decision-making challenge is through the institution of the Lunch Dictatorship. We have established an order. Each week, the next person on the list is declared Lunch Dictator. They have full power to declare where lunch will be that week. They can even delegate the responsibility. What the dictator says, goes, and all must obey. Not a fan of a particular place? Not to worry, it will be your turn to dictate soon. What makes a good lunch spot? First of all, it has to be able to comfortably accommodate all nine of us. There are some excellent eateries that just aren’t easy to dine at as a group. We tend not to return to those. Furthermore, we want something a little special. It shouldn’t be one of the same places we grab a cheap fast lunch from on the other days of the week. Since we all look forward to doing together, it should live up to that anticipation. How does the dictatorship end up working out in practice? We go to a pretty good mix of regular favorites that provide us a consistent experience, with periodic injections of variety. But, I know you’re just waiting for me to get to the point. Where do we go? What do we like the best? Here’s the establishments we have frequented multiple times over the last year: Kuma’s Corner - By far the most popular choice among Rocketeers, they’ve always welcomed our large party, provided delicious huge portions of food, and have a great beer list. Ramen San - A relatively new addition to our rotation, they have big cafeteria sized tables and big bowls of delicious ramen. Chicken and Farm Shop - The menu is pretty simple, chicken, but they do it well. Even better if you get it on a waffle. The only downside is that it doesn’t have a ton of options for the vegetarians among us. 5411 Empanadas - There are some days when it is just hard to get out of the office. On those days, we order in. We have created a special spreadsheet to get our orders just right, and ensure everyone gets just want they want. If you find yourself visiting Hashrocket Chicago and you’re looking for a good place to eat nearby, you won’t be disappointed. This might seem like an awful lot of thought just about lunch! But, it is important that we sometimes step away from our desks and screens and spend some time together without thinking about code or clients. It’s a chance to have an interesting conversation with coworkers who otherwise sit in the distant lands on the far side of the office. One of the special things about Hashrocket is being a part of a team of smart, talented people who make each other better. Friday lunch is when we get to appreciate that. Photo by Edward Franklin on Unsplash Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-01-24"},
{"website": "Hash-Rocket", "title": "Setting up an Ember App with a Rails Backend", "author": ["\nVic Ram"], "link": "https://hashrocket.com/blog/posts/setting-up-an-ember-app-with-a-rails-backend", "abstract": "Javascript Ember Ruby Setting up an Ember App with a Rails Backend by\nVic Ramon\n\non\nSeptember 15, 2013 Update: June 26, 2014 This tutorial is now out of date, and you should instead look at Vic Ramon's Ember Tutorial for a tutorial introducing Ember with Rails. Update: November 8th, 2013 Some of the instructions below are out of date, particularly the code in the store.js file. Please look at the Ember Data Rails Example that I have posted to Github for correct, updated code. This app can be viewed live on Heroku . Today I'm going to show you how to setup an Ember app with a Rails backend. The process is relatively straightforward, but . there are some gotchas that I'd like to help you avoid. The app will use Haml, CoffeeScript, Emblem , and Ember Data. In this tutorial I'll be starting a sample app called Launch Bay. Launch Bay would essentially be an ultra-lightweight version of Pivotal Tracker. In this first post I'm just going to get my app setup to receive data. Basic App Setup Let's hit the ground running with a new app rails new launchbay -d postgresql Setup your database and rvm gemset, and make sure to include these in your Gemfile: gem 'coffee-rails' , '~> 4.0.0' gem 'ember-rails' gem 'ember-source' gem 'emblem-rails' gem 'haml-rails' Open your configs and add the following: # environments/production.rb config . ember . variant = :production # environments/development.rb config . ember . variant = :development # environments/test.rb config . ember . variant = :development Backend Setup Ok, now we've got the basics we need to get our Rails backend up and running. Our app is just going to show a list of stories. For the sake of simplicity these won't be scoped to a particular user or project. Let's create a stories table. rails g migration create_stories name :string body :text rake db :migrate We'll also need a story model, controller, and serializer. # app/models/story.rb class Story < ActiveRecord :: Base end # app/serializers/story_serializer.rb class StorySerializer < ActiveModel :: Serializer attributes :name , :body end I'm going to create a versioned api for the controller just in case I want to change things later: # config/routes.rb namespace :api do namespace :v1 do resources :stories , only: :index end end This controller is going to accept json so that we can interact with our Ember app. # app/controllers/api/v1/stories_controller.rb class Api :: V1 :: StoriesController < ApplicationController respond_to :json def index respond_with Story . all end private def story_params params . require ( :story ). permit ( :name , :body ) end end To see if this is all setup properly spin up your server and open a rails console. First let's seed the database: $ rails console $ Story.create ( name: 'User views a list of stories' , body: 'Given I am a user <br /> When I visit the stories index <br /> Then I should see a list of stories' ) Now hit the api controller and you should get your one story back as json: http://localhost:3000/api/v1/stories.json Providing an Outlet for Ember We need to have a place for our Ember app to actually show up, so let's do that now. I am going to create a generic home controller for now. # config/routes.rb root to: 'home#index' # app/controllers/home_controller.rb class HomeController < ApplicationController end Now give Ember an outlet in the view: # app/views/home/index.html.haml %script { type: 'text/x-handlebars' } {{ outlet }} We also need a Rails layout: # app/views/application.html.haml !!! % html ( lang = \"en-US\" ) % head % title Launch Bay = stylesheet_link_tag \"application\" = javascript_include_tag \"application\" = csrf_meta_tags % body = yield Setting up Ember Internals First, run the generator provided by ember-rails: rails g ember:bootstrap -g --javascript-engine coffee -n App Now restart your Rails server and hit localhost:3000 . You should see a blank page. Open your developer console and you should see output like this: DEBUG: ------------------------------- \nDEBUG: Ember.VERSION : 1.0.0\nDEBUG: Handlebars.VERSION : 1.0.0 \nDEBUG: jQuery.VERSION : 1.10.2 \nDEBUG: ------------------------------- For more advanced debugging, check out the Ember Inspector for Google Chrome Setting Up Ember Data We'll be using Ember Data to handle client-server communication. The Ember-rails generator created a store.js file. Open it and remove everything, and just add the following: # app / javascripts / store . js DS . RESTAdapter . reopen namespace : 'api/v1' Ember Models Now to create the actual story model: # app/assets/javascripts/models/story.js.coffee App . Story = DS . Model . extend name : DS . attr ( 'string' ) body : DS . attr ( 'string' ) Ember Routes app/assets/javascripts/router.js contains our top-level routes. Open that file, change it to Coffeescript, then add this: # app/assets/javascripts/router.js.coffee App . Router . map () -> @ resource 'stories' Note that resource is singular, in contrast to Rails. Ember Template Create the following file: # app/assets/javascripts/templates/stories.js.emblem\n\n| Hello world. Save that, now head over to http://localhost:3000/#/stories . You should see your message on the page. There a few things to note. Use a pipe to tell emblem that \"Hello\" is text and not an html tag. We created a file named stories in the templates folder. This works right now because the stories route has no subroutes. If you added a subroute to the stories resource, then this template would need to be moved to templates/stories/index.emblem.js. We didn't need to create a stories controller or stories route to make this work. Ember will create those in memory for us if we don't explicitly create them. Pulling in real data Ok, we've got text showing up on the page, but we need to actually get data out of our database.To do that we need to bind data to the route, and we do that by creating a stories route. # app/assets/javascripts/routes/stories.js.coffee App . StoriesRoute = Ember . Route . extend model : -> @ get ( 'store' ). findAll ( 'story' ) Open the template back up and add the following: # app/assets/javascripts/templates/stories.js.emblem\n\nh1 Story Listing\n\n= each story in controller\n  h2= story.name\n  | {{{story.body}}} Open up http://localhost:3000/#/stories and you should see the story you put in the database. I'm doing {{{story.body}}} to prevent the html line breaks from being escaped. Notice that I am referring to the array of stories in the template as controller . That's because the data is bound do the Stories controller (which is still in memory). Wrap Up That's it for app setup with Rails, Ember, and Ember Data. If you were to continue with this project the next step would be to get project models setup, then nest stories under projects in the Ember router. I hope this was helpful. Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-09-15"},
{"website": "Hash-Rocket", "title": "Customize ActiveAdmin Index Filters", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/customize-activeadmin-index-filters", "abstract": "Ruby Customize ActiveAdmin Index Filters by\nJosh Branchaud\n\non\nAugust 31, 2017 ActiveAdmin allows us to quickly bootstrap an admin interface to our application. One of the features we get out of the box is a robust, comprehensive filtering sidebar. When viewing a resource's index page with hundreds of records, filtering allows us to narrow down to a digestible subset. Unfortunately, the set of filters generated by ActiveAdmin for a particular model can sometimes be a bit hairy. If there are lots of fields and associations, you can end up with way more filters than you need, detracting from the user experience. One solution is to remove the filters altogether. ActiveAdmin . register Post do config . filters = false end This is a bit drastic though. Perhaps we can find some middle ground by customizing the filters a bit. In the rest of this post, I'll layout some approaches we can take to make our filters cleaner and more directed. Table of Contents Remove Specific Filters Declare Specific Filters Change A Filter's Label Scope A Filter's Collection Change The String Representation Coerce The Input Type Specify HTML Attributes Go Beyond Remove Specific Filters Often times the majority of the filters ActiveAdmin produces are fine. There are only one or two that need to be removed. You can provide one or more attributes or associations to be excluded from the set of filters with the remove_filter directive. ActiveAdmin . register Post do remove_filter :developer , :max_likes end Declare Specific Filters Perhaps there are only a couple filters you'd like to include. The rest of them ought to be excluded. Instead of declaring a large list of filters to remove -- a list which may need to be updated as the model changes -- you can explicitly declare the filters you'd like. This works because, when adding a filter, all the default filters are removed. ActiveAdmin . register Post do filter :developer # association from `belongs_to` end If you aren't wanting to remove all the default filters when modifying a filter as we did in the above example, include the preserve_default_filters! directive before your filter changes. ActiveAdmin . register Post do preserve_default_filters! filter :developer filter :max_likes , label: \"Maximum Likes\" end Change A Filter's Label You can customize the label used by a particular filter -- just include the label option with the preferred text. ActiveAdmin . register Post do filter :max_likes , label: \"Maximum Likes\" end If you are using i18n for your model's name elsewhere, you'll need to be mindful of that here as well. Scope A Filter's Collection The collection of things that are included in a drop down filter defaults to all of those things. For example, if we were to fully spell out the filter's default collection for our developer association, it would look something like this. ActiveAdmin . register Post do preserve_default_filters! filter :developer , collection: -> { Developer . all } end We can take advantage of that syntax to scope the collection in whatever way we would like. For example, to only include developers with at least one post, we can scope our developer collection like so. ActiveAdmin . register Post do filter :developer , collection: -> { Developer . preload ( :posts ). select { | dev | dev . posts . present? } } end Change The String Representation There are two scenarios in which you'll want to adjust the string representation of items in a drop down. The more common is when ActiveAdmin doesn't know how to represent an object, so it uses the fallback object representation (e.g. #<Klass:0x401b3998 ...> ). The other is when the existing string representation just isn't what you want. In either case, the simplest solution is to add or modify to_s for the particular model. If we want to restrict our changes to the app/admin directory, we can again adjust the collection. ActiveAdmin . register Post do filter :developer , collection: -> { Developer . all . map { | dev | [ dev . email , dev . id ] } } end In this case, I chose to display a developer's email instead of the default of their username. We have to provide a tuple with the string representation and the id . We have to provide the id for ActiveAdmin to be able to look up and filter by a particular developer. Coerce The Input Type ActiveAdmin does its best to infer the type of input that should be used for a particular attribute, but it doesn't always nail it on the head. We can use the :as option to coerce the input type to something else that makes more sense. For example, a drop down with yes and no options would be better off as a couple check boxes. A field that is an integer would be easy to interact with using a number input. ActiveAdmin . register Post do filter :tweeted , as: :check_boxes filter :max_likes , as: :number end Specify HTML Attributes We can even go a step further by specifying some attributes of an input field using the input_html option. For example, we can set the step , min , and max of our number input like so. ActiveAdmin . register Post do filter :max_likes , as: :number , input_html: { step: 10 , min: 0 , max: 100 } end Go Beyond All of the examples I have laid out in this post can, generally speaking, be mixed and matched. If you want to do something that I haven't explicitely laid out, just give it a try. The syntax for filters is pretty flexible, so try bending it in various ways to meet your needs. The filter sidebar is powered by Ransack and Formtastic . Feel free to dig into those repositories for even more insight. To create filters that use completely custom search/filtering, I recommend reading about Using Ransackers and checking out this blog post . Have fun and build the filters of your dreams! Cover image by Guillaume Lebelt on\nUnsplash.com Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-08-31"},
{"website": "Hash-Rocket", "title": "A Simple List of Ruby Conferences", "author": ["\nJon Allured\n\n"], "link": "https://hashrocket.com/blog/posts/a-simple-list-of-ruby-conferences", "abstract": "Community A Simple List of Ruby Conferences by\nJon Allured\n\non\nJanuary 20, 2014 I looked for a simple list of upcoming Ruby conferences and didn't find what I was looking for, so I made one. Ruby Conferences is a simple list of Ruby-specific\nevents. You'll find event dates, location, CFP and registration information.\nWith a kickass design by fellow Rocketeer Cameron Daigle, this project\nstarted as just a list, but ended up with news published to a Twitter\naccount and a news feed . To come up with the initial list of events and their details, I did a bunch of\nGoogle searching and Twitter stalking. I used Middleman to create the site\nand have a GitHub repo that I hope people might contribute to with updates\nand event information. So, this is part scratching my own itch and part experiment in collaborative\npublishing, mixed with a (hopefully) useful service for the Ruby community. I'd\nlove any feedback on the site you might have - feel free to email me at rubyconfs@gmail.com , hit me up at @rubyconferences or open an issue on\nthe repo . Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-01-20"},
{"website": "Hash-Rocket", "title": "Mentoring at Jacksonville Startup Weekend", "author": ["\nCameron Daigle\n\n"], "link": "https://hashrocket.com/blog/posts/mentoring-at-jacksonville-startup-weekend", "abstract": "Community Mentoring at Jacksonville Startup Weekend by\nCameron Daigle\n\non\nJanuary 26, 2012 Paul and I recently had the privilege to mentor at Jacksonville Startup Weekend. Paul gave a preparatory talk to the 150-odd participants about how to deliver a MVP (minimum viable product), and we both provided mentorship to teams throughout the event. You can read more about the event in the official recap , or if you're a Jacksonville Business Journal subscriber, there's a good writeup here . We saw a lot of potential in the ideas that were worked on over the weekend, and wish the best to all of the teams involved. Startup Weekend is just another way that we here at Hashrocket are looking to give back to the Jacksonville community, and we're excited for future opportunities to do more! While we don't have footage from the event, here's the video that Kaz Sheekey put together to give Startup Weekend folks an idea of what Hashrocket is all about. Enjoy! Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-01-26"},
{"website": "Hash-Rocket", "title": "Technical Debt Part 1: Understanding Debt", "author": ["\nSuzanne Erin\n\n"], "link": "https://hashrocket.com/blog/posts/technical-debt-part-1-understanding-debt", "abstract": "Process Technical Debt Part 1: Understanding Debt by\nSuzanne Erin\n\non\nMay 16, 2019 One of the biggest challenges around technical debt in software development is how to communicate its true costs to stakeholders and decision-makers. The first part of this two-part series on technical debt explores some ways it can be helpful to think of technical debt like financial debt. What is Technical Debt? Technical debt is a term to describe the shortcomings of existing code that could be changed to perform better, and the ongoing consequences for failing to address it. There are two ways to generate technical debt: actively and passively. Technical debt is incurred actively when development has taken a shortcut. Maybe it was a hack solution or skipped a step to build a feature faster, but now the code is hard to extend or unsuitable for long-term performance. Sometimes, a shortcut is a conscious choice to get something done sooner. Other times it’s an unanticipated consequence of not having enough time or resources to think through more elegant long-term solutions. Either way, eventually it becomes a noticeable source of friction. Technical debt also grows passively with the passage of time. You may have written perfect code when you started -- but the ecosystem that the software lives in is changing. The version of the language you are using may become out of date. Dependencies, chunks of open-source code that are incorporated into your codebase, may become unsupported or develop vulnerabilities. Browsers develop new requirements. You need to maintain this code in order for it to keep working as intended. How Technical Debt is Like Financial Debt: The thing that I love about the term technical debt is that its name gets right to the heart of why it is important. Technical debt operates similar to financial debt. The two definitional aspects of debt are salient here: You have to pay it someday. It accrues interest. The longer you wait to pay it, the more it will cost to get rid of it. These two points are true of technical debt too. In software, development time is literally money. The parallel is less an analogy and more a literal truth: You’ll have to spend time fixing it some day. Time makes it harder to address. The longer you wait to fix it, the more it will cost you. Making the choice to get into technical debt in order to get a feature out faster is like paying with a credit card. It is a more convenient method to pay for something right away. It might be a valid decision in the moment. But, you will pay interest on that purchase over the long term. It isn’t wise to make a habit of putting everything on your credit card without knowing how or when you’ll be able to pay it off. On the other hand, technical debt that accrues naturally over time is a little like owning a home. You have to decide whether to keep up with maintenance or defer it. If there’s a small intermittent leak under the sink, you might be able to get away with sticking a bucket under it. But, if left unaddressed, small leaks can turn into big expensive problems. Your home’s value depends on how well you keep it up. How Technical Debt Accrues Interest: Technical debt in your software has real costs in terms of time. Like compounding interest, they aren’t one-time costs, but continue to build on each other. It continues to slow you down. There are three main ways that technical debt accrues this interest: It can generate more issues It slows down new feature development It gets harder to fix It can generate more issues Technical debt will generate issues if your code is no longer working well for you. If you put off improving underlying code, users will be more likely to report problems that you will need to spend time fixing.. Eventually, the time you’ve spent putting band-aids on issues will outweigh the amount of time it would have taken to fix the underlying problem. It slows down new feature development Technical debt slows new feature development. It is code that is hard to work with. It might have patterns that are tedious to replicate, or it might require long work-arounds every time you want to add something new. It might require creating or looking up documentation that becomes increasingly harder to find. It will continue to slow development down more and more. It gets harder to fix Technical debt gets harder to fix the longer you ignore it. As you grow the software, the troublesome pattern is being extended and integrated into a larger and more complex codebase. There will be more of it and it will be touching more things. The longer you wait, the harder it will be to change. Conclusion In each of the above cases, we can see technical debt has real compounding costs in terms of lost time. Time is money, so ignoring these issues for too long is like failing to pay a credit card, or failing to budget for maintenance on your home. Dealing with debt involves making a budget. The second part of this series explores strategies to help manage your technical debt load. Image by: unsplash-logo jonathan Ford Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-05-16"},
{"website": "Hash-Rocket", "title": "Best of TIL Year One: SQL", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/best-of-til-year-one-sql", "abstract": "Hashrocket Projects Best of TIL Year One: SQL by\nJake Worth\n\non\nJune 21, 2016 Here are some of the top SQL posts from Today I Learned . My goal with this series is to pause and highlight some of the top posts from\nthe first year of Today I Learned . Today we'll look at SQL, our second-most active channel. Databases matter to Hashrocket. 'Your database will be\nmulti-tenant' is a common axiom. We feel that a\nrobust database, with precise constraints, capable of standing alone \nfrom any framework, is crucial to the long-term success of a project.\nSQL mastery is a badge of honor here. Here are the top five most liked SQL posts, in order, from the first year of Today I Learned. Enjoy! Watch for database changes on #PostgreSQL (Dorian Karter) If you are trying to debug multi-threaded environments or a really fast job queue you might need to \"watch\" a table for changes. In psql use the \\watch command like so: \\ watch 1 \"select * from job_queue\" ; This will run the query every 1 second (you can change the second argument if you need it slower) and display the result as well as past results. Quickly see the contents of a table in #PostgreSQL (Dorian Karter) Next time you want to see the contents of a table in Postgres' CLI don't type the whole: select * from name_of_table ; Just use: table name_of_table ; h/t Josh Branchaud Be aware! Postgres rounds. (Chris Erin) Yesterday, my pair and I created a test that calculated a value and compared that to the value of a calculation in the code we were testing.  This worked out great except for one hitch, we were asserting about the derived value after it had been inserted into the database.  What we didn't count on is that Postgres rounds.  Check this out: create table money ( amount numeric ( 4 , 2 )); insert into money ( amount ) values ( 10 . 342 ) returning amount ; amount -------- 10 . 34 insert into money ( amount ) values ( 10 . 347 ) returning amount ; amount -------- 10 . 35 Postgres rounds! Using Expressions In Indexes With PostgreSQL (Josh Branchaud) Though we usually see column names by themselves when defining an index, it\nis also possible to create an index with an expression. Let's say I have a users table with an email column. Then I may end up\ncreating an index like this create index email_idx on users ( email ); If I always perform queries on the email column with the lower() function, like this select * from users where lower ( email ) = lower ( 'some@email.com' ); then I will want to also create an index with that full expression -- lower(email) I can do this with a statement like the following create index lower_email_idx on users ( lower ( email )); Without an index that uses the full lower(email) expression, select statements like the one above will be forced to do full sequential scans\ninstead of indexed scans. Types and type casting in #PostgreSQL (Dorian Karter) To see the type of column or any entity in PostgreSQL use pg_typeof . Here's an example: select pg_typeof ( array [ 'thing' ]); -- OUTPUT: -- pg_typeof -- --------- -- text[] To cast to another type use the :: operator: select pg_typeof ( array [ 'thing' ]:: varchar []); -- OUTPUT: --      pg_typeof -- ------------------- -- character varying[] h/t Josh Branchaud Conclusion Thanks to Dorian, Chris, and Josh for those posts. Today I Learned had a spike in traffic near the beginning of the year, and these posts are mostly from that time. But there's a lot of great SQL tips from earlier. See them all here: https://til.hashrocket.com/sql Want more SQL tips from Hashrocket? Check out our newest production, PG Casts . Keep committing that data, and learning every day. This blog post is part two of a series; here's part one . Next, we will look at the top Ruby posts from year one. Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-06-21"},
{"website": "Hash-Rocket", "title": "Session Zero for Software Teams", "author": ["\nSuzanne Erin\n\n"], "link": "https://hashrocket.com/blog/posts/session-zero-for-software-teams", "abstract": "Process Session Zero for Software Teams by\nSuzanne Erin\n\non\nJanuary 31, 2019 I recently spoke at our Hashrocket Miniconf about a concept I learned from the tabletop role-playing game community: Session Zero. Before I get into Session Zero, let’s set the stage. What is tabletop role-playing? Think Dungeons and Dragons. I know we’re all some flavor of nerd here on the internet, but if you’re not familiar with these types of games, here’s a simplified description: Generally a tabletop role-playing game (RPG) consists of participants who play characters with different attributes and abilities. Players describe their characters actions and roll dice to see whether they succeed. A campaign in an RPG is a series of challenges or situations that are presented to the players as they progress through a fictional setting. A Game Master (GM) sets the scenes, presents the challenges, and adjudicates the rules for the players. A campaign is a longer term game played out over a series of sessions. One of the most frustrating situations that can develop is if participants aren’t on the same page about what they’re playing. What if one person is trying to play a lighthearted fantastical game consisting of unicorns and banana puns, while another person is in the mindset of a grim and realistic setting full of vampires and betrayal?  Imagine a situation where one character is trying to diplomatically negotiate their way through a conflict while another jumps straight at the enemy with swords drawn. Or worse, what if the Game Master, who is organizing and refereeing the whole thing, has a different idea of what game they want to play than the players do? Odds are, if these underlying assumptions go unacknowledged, all participants will end up having a bad time and each will feel like the other is playing the game wrong . Have you ever been a part of a pair or a project team that felt like that? Like there was a fundamental expectations mismatch about what you were even trying to do? Where each person thought the other person was going about it wrong? That’s where Session Zero comes in. In the world of tabletop role-playing games, Session Zero  is dedicated to working together to lay the groundwork for upcoming game. Importantly, It is the session before game-play begins. Instead of starting with unstated assumptions about how people will play together, you have a dedicated hang-out to talk it through before you even roll a die. So what do you talk about at Session Zero? As experienced players have discovered, a few things are going to need to happen in order to have an enjoyable campaign. You need to be aligned on things like: Game Rules - There’s more than one game you can play. You may have your own house rules. Agree on the source of truth up front, and how disagreements will be adjudicated. Campaign Expectations - What kind of atmosphere do players want to be a part of? Is it a scary or funny? Is it a classic dungeon crawl or an epic open world? Player Goals - Some players are going to be motivated by killing monsters and getting loot. Others like playing out social interaction and developing backstory. What aspects of game-play will keep players engaged? Character Creation / Party Composition -  Different characters will bring different strengths to the party. The party might not be very successful if everyone chooses to be a fighter and no one wants to be a healer or a wizard. A balanced party will use a mix of strengths to achieve the goals that the participants set out. Table Etiquette - How you behave with your childhood friends might be different than at a public meet-up. What do you want to do about phone use at the table, or people interrupting each other? Now’s a chance to generate some ground rules together. Snack/Beverage Situation - Yes, more than one gaming group has fallen apart over arguments about around arranging and paying for necessities like pizza, beer, and snacks. All of these decisions are likely to go over better than rules that are unspoken or are handed down from on high, because you have generated them together. The conversation is productive because there’s no pressure to try and squeeze in a couple hours of the game itself; you’ve given the discussion its own dedicated space to ensure all participants are heard. I’m hoping you are beginning to see some parallels with your own project experiences. The above items could be pretty easily translated into a meeting agenda: What process do we agree to use? What goal are we trying to achieve? How will we determine progress/success? What are our roles? How will our roles relate? What are the participants work-styles? What do they find motivating? What strengths do the team members bring? How will we plan to communicate effectively? How do we agree to interact? What kind of culture are we promoting? Where is the coffee? When do we get to drink the coffee? Where could this be useful in your process? You could some of these ideas when you first start pairing with someone new. You can talk about what you’re hoping to get out of pairing together and how you can do it in a way that gets you there. Maybe it’s a good way to think about running a project kick-off, by asking the group to articulate the goals, processes, and rules. How to make this successful? Have this conversation before you are worried about ticket completion or velocity. Remember that working these things out up front is going to be better than trying to lay the track at the same time you drive the train. Make sure that you’re listening. The point is for you to understand what everyone needs in order to be successful. The concept of Session Zero provides a useful framework to think about how you start something new. In an RPG, there isn't one right or wrong way to play; every game is unique depending on the players. Similarly, there's a number of different approaches you can take to working with a pair or a team. What is important is getting on the same page about how you want to go about it. Take the time to figure out that context and agree on how you all want to work together. The result? You know where you’re headed, you head there together, and you have a good time doing it. Photo by Alex Chambers on Unsplash Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-01-31"},
{"website": "Hash-Rocket", "title": "5 JavaScript Object Destructuring Tricks", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/5-javascript-object-destructuring-tricks", "abstract": "Javascript 5 JavaScript Object Destructuring Tricks by\nJosh Branchaud\n\non\nJanuary 22, 2019 The ES6 destructuring syntax is quite flexible. It allows us to do a lot of\nnice things with objects and arrays that in the past would have taken many\ndistracting lines of code. Let's look at five handy destructuring tricks for objects. Trick #1 Remove an item from an object // const data = { a: 1, b: 2, thingToRemove: \"bye\" }; const { thingToRemove , ... rest } = data ; console . log ( rest ); // { a: 1, b: 2 } This is a concise way of getting a new copy of an object with a specific\nkey/value pair removed. Note that it is immutable and you are free to\nspecify as many keys as you'd like separated from the ...rest object. Trick #2 Rename things in a destructuring // const data = { coordinates: { x: 5, y: 6 }}; const { x : xCoord , y : yCoord } = data . coordinates ; console . log ( xCoord , yCoord ); // 5, 6 It is not always desirable or possible to adjust naming within an object\nupstream. If we need different naming locally, we can do this inline with\nour destructuring. The colon syntax allows us to do this with specific keys. Trick #3 Nested Destructuring // const data = { coordinates: { x: 5, y: 6 }}; const { coordinates : { x , y }} = data ; console . log ( x , y ); // 5, 6 We can get at key/value pairs in a single destructuring regardless of how\nthey are nested. This again allows us to concisely access the data we want\nwithout having to make any upstream adjustments. This can of course be combined with Trick #2: // const data = { coordinates: { x: 5, y: 6 } }; const { coordinates : { x : xCoord , y : yCoord } } = data ; console . log ( xCoord , yCoord ); // 5, 6 Fancy! Trick #4 Destructuring In Function Arguments All the things we did above -- we can do those in the argument part of a\nfunction declaration. const myFunction = ({ coordinates : { x : xCoord , y : yCoord } }) => { console . log ( 'Coords:' , xCoord , yCoord ); }; We've accessed and renamed nested values all in the function declaration.\nThe body of the function can be focused strictly on the logic. Trick #5 Non-default Imports As Named Object Consider a growing list of imports that starts to look like this: import { rootPath , blogPath , aboutUsPath , teamPath , pricingPath , contactPath , signInPath , signOutPath , } from '../routes' ; It's already unwieldy and its bound to get worse. This import destructuring\nsyntax allows us to tame those imports. import { * as routes } from '../routes' ; console . log ( routes . rootPath ); // '/' console . log ( routes . blogPath ); // '/blog' This can be a nice way to not have to explicitly import dozens of things The\ntradeoff is that your compiler will no longer be able inform you if you are\nreferencing an undefined import. Conclusion These five object destructuring tricks allow us to write more concise,\ndirect code. I can attest to the usefulness of these tricks -- quickly\nbrowsing through a few files of a React app I've been maintaining for over a\nyear reveals each of these employed in a variety of contexts. Give them a\ntry, I think you'll find that it makes JavaScript even more fun to write. Cover Photo Credit: unsplash-logo Franck V. Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-01-22"},
{"website": "Hash-Rocket", "title": "Where In the World are the Rocketeers?", "author": ["\nJohnny Winn\n\n"], "link": "https://hashrocket.com/blog/posts/summer_conference_2013", "abstract": "Where In the World are the Rocketeers? by\nJohnny Winn\n\non\nMarch 25, 2013 This year's conference schedule is in full swing and there are many opportunities to get engaged in the Ruby community both in the States as well as abroad. Whether you are looking to attend a regional conference, or want to venture a little further, you will most likely run into a Rocketeer or two. So get out, get engaged, we'd love to meet you! Ancient City Ruby April 4-5 | St. Augustine, FL Hashrocket is organizing its first Ruby conference in sunny Florida, and we invite everyone to attend. The historic city of St. Augustine provides a unique setting to discover knowledge in the Ancient City. Attendees will be treated to a variety of perspectives on topics ranging from the philosophy behind software to Test Driven Development. The lineup includes notable speakers including Jim Weirich, Sandi Metz, Avdi Grimm and Russ Olsen. You don't want to miss this one so make sure to register before it's too late! RailsConf April 29 - May 2 | Portland, OR In 2013, the largest gathering of Ruby on Rails developers in the world returns to Portland. RailsConf promises to be one of the highlights of the year as it spotlights the latest in innovations within the Rails community. The conference draws developers and companies from around the world to connect and learn about new products as well as the tools of the trade. Rocketeers Attending Travis Anderson Andy Borsz Taylor Mock Thais Camilo Gabriel Reis Paul Elliott Scottish Ruby Conf May 12-13 | Perthshire, Scotland Scottish Ruby is a premier Ruby conference in the UK and provides a unique perspective on the techniques of the software craftsman. The venue has changed this year but the conference is guaranteed to deliver. Rocketeers Attending Cameron Daigle (speaking) MagmaConf June 5-7 | Manzanillo, Mexico MagmaRails is now MagmaConf ! MagmaConf is a web development conference that covers cutting-edge topics in Ruby, Ruby on Rails, frontend, and other web technologies. This is one of the most important web development conferences in Central America and it's a chance for engineers from around the world to meet and experience Mexico. Rocketeers Attending Johnny Winn (speaking) Vic Ramon Madison Ruby August 23-24 | Madison, WI Want to discover a hidden gem in the Ruby community? Madison Ruby is a showcase to the locals and a chance for visitors to experience a true gem of the community. On the 22nd attendees have a chance to partake in pre-conference workshops. Rocketeers Attending Josh Davey Jon Allured Brian Dunn Check back later this summer to see where you'll find the Rocketeers in Fall 2013! Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-03-25"},
{"website": "Hash-Rocket", "title": "Hashrocket Loves: Code and Coffee", "author": ["\nDave Ly"], "link": "https://hashrocket.com/blog/posts/hashrocket-loves-code-and-coffee", "abstract": "Community Hashrocket Loves: Code and Coffee by\nDave Lyon\n\non\nJanuary  8, 2012 Code and Coffee is a weekly event that several of us at Hashrocket Chicago have been attending regularly since it started this past summer, and are now starting up in Jacksonville Beach. It takes place every week on Tuesday morning at 7am. For Jacksonville, see: http://www.meetup.com/jax-code-and-coffee For Chicago, follow Dave Lyon or Mike Busch on Twitter, or just stop by the Starbucks at Chicago and Franklin 7am on Tuesday. Originating in Columbus, OH, Code and Coffee was brought to Chicago by Mike Busch , and to Jacksonville Beach by Rocketeer Paul Elliott What do you do? Code and Coffee, as the name implies, is mostly about coding and drinking coffee. An alternative to after work meetups, Code and Coffee works out well for people with evening commitments who still want to get together with fellow coders and discuss their craft. The activities on a given day are as varied as the people in attendance. Sometimes there are discussions of new programming techniques and ideas. If you need help, there's always someone willing to pair with you or teach you something. In general, it's a meetup to practice, learn and grow as a developer. Who goes to this thing? Anyone and everyone. There are no requirements to attend, and no specific language or skill set targeted. Here are some quotes from Rocketeers that have attended: \"[I love that I] can always go there for some quick discussion about code or to see what people are working on at the time.\" -- Aaron Kalin \"As a dad, I can't make it to many after work community events. Having an early morning hack time set aside is great for this family man.\" -- Brian Dunn \"I made the mistake of coming to code and coffee the first time without my laptop. Luckily it's as much about people as it is code!\" -- Stephen Caudill \"I like both code and coffee\" -- Josh Davey Not in Chicago or Jacksonville? Not to worry! There might be a Code and Coffee near you. Check out http://codeandcoffee.info to see if there is one near you! Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-01-08"},
{"website": "Hash-Rocket", "title": "Websocket Shootout: Clojure, C++, Elixir, Go, NodeJS, and Ruby", "author": ["\nJack Christensen\n\n"], "link": "https://hashrocket.com/blog/posts/websocket-shootout", "abstract": "Community Ruby Elixir Phoenix Websocket Shootout: Clojure, C++, Elixir, Go, NodeJS, and Ruby by\nJack Christensen\n\non\nSeptember  1, 2016 When a web application has a requirement for real time bidirectional communication, websockets are the natural choice. But what tools should be used to build a websocket server? Performance is important, but so is the development process. A performance benchmark alone is not enough. We also need to consider the ease of development. This shootout compares a simple websocket server implemented idiomatically in Clojure, C++, Elixir, Go, NodeJS, and Ruby. The Test The servers all implement an extremely simple protocol with only two messages: echo and broadcast . An echo is returned to the sending client. A broadcast is sent to all connected clients. When the broadcast is completed a broadcast result message is returned to the sender. Messages are encoded in JSON. Both messages take a payload value that should be delivered to the appropriate destination. Example broadcast message: { \"type\" : \"broadcast\" , \"payload\" :{ \"foo\" : \"bar\" }} For the platforms with low level websocket implementations the above message would work directly. For platforms with higher level abstractions such as Phoenix and Rails the message must be encoded to be compatible with their message standards. Meet the Contestants Let's start by examining the interesting parts of each contestant. Clojure The Clojure server is built on the HTTP Kit web server. The interesting parts are in server.clj . When a client connects they are stored in an atom of channels. This gives us concurrency safety, which is important since Clojure will handle requests in parallel. ( defonce channels ( atom # {})) ( defn connect! [ channel ] ( log/info \"channel open\" ) ( swap! channels conj channel )) ( defn disconnect! [ channel status ] ( log/info \"channel closed:\" status ) ( swap! channels disj channel )) Broadcasting is quite simple. The message is sent to all channels, then the result is sent to the sender. ( defn broadcast [ ch payload ] ( doseq [ channel @ channels ] ( send! channel ( json/encode { :type \"broadcast\" :payload payload }))) ( send! ch ( json/encode { :type \"broadcastResult\" :payload payload }))) Including some uninteresting setup and HTTP routing (but excluding the CLI runner), the core Clojure server weighs in at under 50 LOC. C++ The C++ server uses websocketpp which in turn relies on boost for its websocket server infrastructure. It is easily the most verbose and complicated code base. The server class is in server.cpp and server.h . Multithreading is explicit. Multiple threads are started that all execute the websocketpp server run method. void server :: run ( int threadCount ) { boost :: thread_group tg ; for ( int i = 0 ; i < threadCount ; i ++ ) { tg . add_thread ( new boost :: thread ( & websocketpp :: server < websocketpp :: config :: asio >:: run , & wspp_server )); } tg . join_all (); } Similar to the Clojure implementation, the live connections are stored in a collection. void server :: on_open ( websocketpp :: connection_hdl hdl ) { boost :: lock_guard < boost :: shared_mutex > lock ( conns_mutex ); conns . insert ( hdl ); } void server :: on_close ( websocketpp :: connection_hdl hdl ) { boost :: lock_guard < boost :: shared_mutex > lock ( conns_mutex ); conns . erase ( hdl ); } To be thread-safe a lock must be acquired before adding or removing a connection. Creating the boost::lock_guard object locks conns_mutex . The destructor unlocks it automatically when the function returns. Given this is C++, the syntax for defining the set of connections is interesting . class server { // ... std :: set < websocketpp :: connection_hdl , std :: owner_less < websocketpp :: connection_hdl >> conns ; }; This is a std::set templatized for websocketpp::connection_hdl . Since websocketpp::connection_hdl is a std::shared_ptr it also needs to be templatized on the comparison predicate for the set. The core broadcast function is relatively straightforward, though verbose. void server :: broadcast ( websocketpp :: connection_hdl src_hdl , const Json :: Value & src_msg ) { Json :: Value dst_msg ; dst_msg [ \"type\" ] = \"broadcast\" ; dst_msg [ \"payload\" ] = src_msg [ \"payload\" ]; auto dst_msg_str = json_to_string ( dst_msg ); boost :: shared_lock_guard < boost :: shared_mutex > lock ( conns_mutex ); for ( auto hdl : conns ) { wspp_server . send ( hdl , dst_msg_str , websocketpp :: frame :: opcode :: text ); } Json :: Value result_msg ; result_msg [ \"type\" ] = \"broadcastResult\" ; result_msg [ \"payload\" ] = src_msg [ \"payload\" ]; result_msg [ \"listenCount\" ] = int ( conns . size ()); wspp_server . send ( src_hdl , json_to_string ( result_msg ), websocketpp :: frame :: opcode :: text ); } First, it builds the JSON data to send to all clients. Then it acquires a shared lock on conns and sends the message to all of them. Finally, it builds and sends the broadcast result message. The core websocket server class is around 140 LOC. Elixir The Elixir server uses the Phoenix framework . The relevant code is in room_channel.ex . Phoenix has a channel abstraction built-in so there is no need to manually manage the collection of connected clients. It automatically uses JSON as the transport which further simplifies the code. defmodule PhoenixSocket . RoomChannel do use Phoenix . Channel def join ( \" room:lobby\" , _message , socket ) do { :ok , socket } end def handle_in ( \" echo\" , message , socket ) do resp = %{ body: message [ \" body\" ], type: \" echo\" } { :reply , { :ok , resp }, socket } end def handle_in ( \" broadcast\" , message , socket ) do bcast = %{ body: message [ \" body\" ], type: \" broadcast\" } broadcast! socket , \" broadcast\" , bcast resp = %{ body: message [ \" body\" ], type: \" broadcastResult\" } { :reply , { :ok , resp }, socket } end end Pattern matching makes handling the client requests elegant. The entire relevant channel code in only about 20 LOC. Go The Go server directly uses the net/http and golang.org/x/net/websocket libraries for websockets. The websocket handler is defined in handler.go . There is no higher level abstraction other than the connection, so similar to C++ a map is used to store all connected clients. As with C++, a mutex is used for concurrency safety. func ( h * benchHandler ) Accept ( ws * websocket . Conn ) { // ... h . mutex . Lock () h . conns [ ws ] = struct {}{} h . mutex . Unlock () // ... } The broadcast method is fairly simple. Take a read lock on the connection mutex. Then send the message to every connection. Finally, unlock the mutex and send the broadcast result to the sender. func ( h * benchHandler ) broadcast ( ws * websocket . Conn , payload interface {}) error { result := BroadcastResult { Type : \"broadcastResult\" , Payload : payload } h . mutex . RLock () for c , _ := range h . conns { if err := websocket . JSON . Send ( c , & WsMsg { Type : \"broadcast\" , Payload : payload }); err == nil { result . ListenerCount += 1 } } h . mutex . RUnlock () return websocket . JSON . Send ( ws , & result ) } Boilerplate such as explicit package importing, explicit error handling, and using strong types instead of untyped maps combine to push the Go websocket code to nearly 100 LOC. Javascript / NodeJS The Javascript implementation uses NodeJS and websockets/ws . The entire server is contained in index.js . The websockets/ws server keeps track of connected clients automatically. The code is so simple it needs little explanation. var WebSocketServer = require ( 'ws' ). Server ; var wss = new WebSocketServer ({ port : 3334 }); function echo ( ws , payload ) { ws . send ( JSON . stringify ({ type : \"echo\" , payload : payload })); } function broadcast ( ws , payload ) { var msg = JSON . stringify ({ type : \"broadcast\" , payload : payload }); wss . clients . forEach ( function each ( client ) { client . send ( msg ); }); ws . send ( JSON . stringify ({ type : \"broadcastResult\" , payload : payload })); } wss . on ( 'connection' , function connection ( ws ) { ws . on ( 'message' , function incoming ( message ) { var msg = JSON . parse ( message ); switch ( msg . type ) { case \"echo\" : echo ( ws , msg . payload ); break ; case \"broadcast\" : broadcast ( ws , msg . payload ); break ; default : console . log ( \"unknown message type: %s\" , message ); } }); }); The overall application is easily the shortest: only 31 LOC in one file. Ruby / Rails Rails 5 introduced ActionCable, a websocket abstraction very similar to the one in Phoenix. Like Phoenix it automatically manages the collection of connected clients and handles JSON parsing and serialization. The code is similar in structure and size. class BenchmarkChannel < ApplicationCable :: Channel def subscribed Rails . logger . info \"a client subscribed: #{ id } \" stream_from id stream_from \"all\" end def echo ( data ) ActionCable . server . broadcast id , data end def broadcast ( data ) ActionCable . server . broadcast \"all\" , data data [ \"action\" ] = \"broadcastResult\" ActionCable . server . broadcast id , data end end At about 20 LOC for the websocket handling, Rails is in the same class as Phoenix and NodeJS. Benchmarks The goal of these tests is to see how a server performs under heavy load, not merely testing if it can handle a large number of mostly idle connections. As part of this comparison a benchmark tool websocket-bench was built to test the performance of these websocket servers. websocket-bench is designed to find how many connections a server can handle while providing an acceptable level of performance. For example, given the requirement that with 10 simultaneous broadcasts in progress at least 95% of broadcasts must be completed within 250ms, how many connections can the server handle? Here is an example benchmark run: $ bin/websocket-bench broadcast ws://earth.local:3334/ws --concurrent 10 --sample-size 100 --step-size 1000 --limit-percentile 95 --limit-rtt 250ms\nclients:  1000    95per-rtt:  47ms    min-rtt:   9ms    median-rtt:  20ms    max-rtt:  66ms\nclients:  2000    95per-rtt:  87ms    min-rtt:   9ms    median-rtt:  43ms    max-rtt: 105ms\nclients:  3000    95per-rtt: 121ms    min-rtt:  21ms    median-rtt:  58ms    max-rtt: 201ms\nclients:  4000    95per-rtt: 163ms    min-rtt:  30ms    median-rtt:  76ms    max-rtt: 325ms\nclients:  5000    95per-rtt: 184ms    min-rtt:  37ms    median-rtt:  95ms    max-rtt: 298ms The above benchmark starts by connecting 1000 websocket clients to ws://earth.local:3334/ws. Then it sends 100 broadcast requests with a concurrency of 10. It increases by 1000 clients at a time until the 95th percentile round-trip time exceeds 250ms. In this case the server can meet its performance requirements with up to 5000 clients. These results are from running the server on one machine and the benchmark tool as another. Both machines are bare metal 4ghz i7 4790Ks with 16GB of RAM running Ubuntu 16.04 connected via GB ethernet. The tests were run with a concurrency of 4 and a 95th percentile requirement of 500ms round-trip time. Tests were run multiple times and the best results were recorded. Memory usage can also be a factor when running in constrained environments. Unsurprisingly, C++ is at the top of the performance chart by a substantial margin. It also is most efficient in memory usage for the number of connections. However, the C++ server is also the most verbose and the most complex implementation. The language is an enormous multi-paradigm conglomeration that includes everything from the low-level memory management, raw pointers, and inline assembly to classes with multiple inheritance, templates, lambdas, and exceptions. A developer also must delve into compile flags, makefiles, long compile times and parsing arcane error messages. On the plus side, deployment can be simple as you can compile a project down to a single binary. If you absolutely need the most performance this is where it's at, but be prepared to pay for it in development time and difficulty finding skilled developers. At 82% the performance of the leader, Clojure proves that very high level languages can be very fast. It is a JVM hosted language, so it can benefit from the rich assortment of Java libraries. Clojure is in the Lisp family, so it may be a challege to developers who have never worked in that style of language. But once that hurdle is overcome, Clojure can be quick to develop and the code is very concise. Deployment depends on the JVM, but beyond that an entire project can be contained in a jar file so deployment is relatively easy. If you have developers with Clojure skills, Clojure is a great choice. Elixir is in third place at 73% of the speed of C++. Elixir attaches a friendly Ruby-style syntax to the functional and highly concurrent Erlang VM. The code is clear, concise, and easily read even by developers who do not know Elixir. Phoenix adds a channel abstraction that makes websockets very easy to work with. Best practices for deployment are still being determined, see Gatling for Hashrocket's take on Elixir deployment. On the downside, Elixir developers are still rare and the language is new and still changing (for example, date and time types were just standardized in Elixir 1.3 which was released in June 2016). Elixir is a solid option for websocket servers. Go tied Elixir for third place for performance. It uses about half the memory as Clojure and Elixir. Go has a different type of simplicity than the other languages on this list. It eschews any magic or hidden behavior. This has lead to a very simple and clear but verbose language. Go's implementation of CSP makes it far easier to reason about concurrent systems than event-driven systems. Go compiles to a static binary which makes deployment simple. As with the last couple options, Go is a good choice for websocket servers. Trailing further behind is NodeJS at 39%. NodeJS performance is hampered by its single-threaded architecture, but given that fact it performs very well. The NodeJS server was the smallest overall in lines of code and extremely quick and easy to write. Javascript is the lingua franca for web development, so NodeJS is probably the easiest platform for which to hire or train developers. At less than 2% the performance of C++, Rails running on Ruby MRI simply cannot compete on web socket performance. It's role is supplementing traditional Rails applications that only need websockets for a small number of concurrent clients. In that particular case, avoiding another technology stack just for websockets can be a win. The winner for Ruby performance testing was JRuby. Though still far behind the other contestants, JRuby more than doubled the performance of MRI. JRuby should definitely be considered for any Rails deployment. Conclusions When it comes to websockets, Clojure, Elixir, and Go are all choices that balance great performance with ease of development. NodeJS is in a lower performance tier, but is still good enough for many uses. Rails can only be recommended when part of an existing Rails application with few concurrent clients. C++ offers the best performance, but it is hard to recommend due to the complexity and difficulty of development. Code and complete benchmark results for this shootout are at Github . Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-09-01"},
{"website": "Hash-Rocket", "title": "Best of TIL Year One: Command Line", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/best-of-til-year-one-command-line", "abstract": "Hashrocket Projects Best of TIL Year One: Command Line by\nJake Worth\n\non\nJuly  5, 2016 Here are some of the top command line posts from Today I Learned . My goal with this series is to pause and highlight some of the top posts from\nthe first year of Today I Learned . Today we'll\nlook at Command Line, our fourth-most active channel. At Hashrocket, our developers pair every day using Vim and Tmux, sometimes remotely, so command-line proficiency is\na must. The Dotmatrix is one of\nseveral projects we maintain to maximize our terminal environment. Here are the top five most liked command line posts, in order, from the first year of\nToday I Learned. Enjoy! Homebrew is eating up your harddrive (Dorian Karter) If you've been using Homebrew on your Mac (if you have a Mac you really should) you may not be aware that every time you upgrade your brew formulas using brew update && brew upgrade Homebrew is leaving behind a copy of the old versions. With package updates becoming available daily you may end up with gigabytes of space being wasted away on old package versions. To see how much space the old versions are using run brew cleanup -n . When I ran that command I got: ==> This operation would free approximately 9.2G of disk space. Holy cannoli! If you feel like reclaiming that diskspace just run the command again without the -n : brew cleanup . Enjoy space! 🚀 Last Argument Of The Last Command (Josh Branchaud) You can use !$ as a way to reference the last argument in the last\ncommand. This makes for an easy shortcut when you want to switch out\ncommands for the same long file name. For instance, if you just ran cat on\na file to see its contents $ cat /Users/jbranchaud/.ssh/config and now you want to edit that file. You can just pass !$ to the vim command: $ vim ! $ Hit enter or tab to get the full command: $ vim /Users/jbranchaud/.ssh/config h/t Dorian Karter Rename The Current tmux Session (Josh Branchaud) If you've created an unnamed tmux session or you no longer like the original\nname, you can open a prompt to change it by hitting <prefix>$ Replace the existing name with the desired name and hit enter. h/t Dorian Karter Never leave the home row in bash (Dorian Karter) To edit a command in bash you often need to jump around in the line and revert to using the arrow keys. As a vim/emacs user this becomes a bit of a clumsy non-ergonomic movement, here are some shortcuts to help you keep your fingers on the home row. If you are an emacs user these are going to look familiar. ctrl-p - previous command entered ctrl-n - next command entered ctrl-a - jump to BOL ctrl-e - jump to EOL alt-b - jump word forward alt-f - jump word backwards alt-b - jump character forward alt-f - jump character backwards If these shortcuts don't work try running set -o emacs Alternatively you can turn on vi mode in your shell by calling set -o vi , which you can add to your .zshrc or add set editing-mode vi to your .inputrc . After setting this you can enter normal mode by hitting escape and use vi motions to edit/move. Your shell just got upgraded. 💪💻 Check if a #ruby gem is installed from bash script (Dorian Karter) If you are adding customizations to your zshrc, such as adding auto completion for a certain gem, you want to make sure that the gem is installed before performing any action. The gem list [gemname] -i command returns a boolean representing whether a gem is installed or not. if ` gem list lunchy -i ` ; then echo \"Lunchy gem is installed!\" ; # do some configuration here fi If you wish to be more granular you can check whether a specific version is installed by adding the --version [version] flag. if ` gem list lunchy -i --version 0.10.4 ` ; then echo \"Lunchy v0.10.4 is installed!\" ; # do some configuration here fi Conclusion Thanks to Dorian and Josh for these posts. Today I Learned had a spike in traffic near the beginning of the year, and\nthese posts are mostly from that time. But there's a lot of great Command Line tips\nfrom earlier. See them all here: https://til.hashrocket.com/command-line Keep refining those commands, and learning every day. This blog post is part four of a series; here's part one , two , and three . Next, we\nwill look at the top Rails posts from year one. Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-07-05"},
{"website": "Hash-Rocket", "title": "Dabbling with Backbone.js", "author": ["\nKevin Wang\n\n"], "link": "https://hashrocket.com/blog/posts/dabbling-with-backbone-js", "abstract": "Javascript Dabbling with Backbone.js by\nKevin Wang\n\non\nMay 21, 2012 Here at Hashrocket , every Friday afternoon is reserved for contributing to open source or learning new skills. Last Friday I decided to use this time to teach myself Backbone.js , with the goal of building a toy app with it in a few hours. The idea is a simple \"leaderboard\" to rank Rocketeers by the number of \"hugs\" they received on Twitter - to \"hug\" a rocketeer, you  just add the hashtag \"#hugarocketeer\" on a tweet mentioning a rocketeer's Twitter handle. (You can see the implemented app here .) The main reason for this idea is that I could lean on Twitter APIs to provide data so I can focus on just the front end with Backbone. I have never done a Backbone app before, and I didn't feel like starting from scratch. I grabbed the Backbone implementation of ToDo.js to serve as a starting point as well as to learn from the code base. First I mocked up the UI with plain HTML and CSS, just to have an idea what the end product will be. The ToDo.js serves as a great base here with all the supporting files and boilerplates set up, so I only had to change the \"body\" part of the index.html, and modified the css to make it look right. <body> <div id= \"hug_a_rocketeer\" > <ul id= \"rocketeers\" > <li> <div id= 'pic_and_name' > <img src= 'http://dummyimage.com/50x50/574b57/bcbfeb.png' /> <span> Marian Phalen </span> </div> <ul id= \"props\" > <li> 3 Hugs </li> </ul> </li> <li> <div id= 'pic_and_name' > <img src= 'http://dummyimage.com/50x50/574b57/bcbfeb.png' /> <span> Micah Cooper </span> </div> <ul id= \"props\" > <li> 4 Hugs </li> </ul> </li> </ul> </div> </body> Extracting Underscore template from the markup: <body> <div id= \"hug_a_rocketeer\" > <h1> Hug a Rocketeer! </h1> <span> Add #hugarocketeer to your tweet mentiong your favorite rocketeer, and watch them climb up on this board. :) <ul id= \"rocketeers\" > </ul> </div> <script type= \"text/template\" id= \"person-template\" > < div id = 'pic_and_name' > < img src = \"<%= img_url %>\" /> < span > <%= name %> < /span > < /div > < ul id = \"hugs\" > < li > <%= hugs %> hugs < /li > < /ul > </script> </body> On to the javascript file: ToDo.js has a nice and small Backbone implementation and with helpful comments - I learned the fundamentals of Backbone just by reading the code. For our purpose, I kept the code structure and removed all its logic, and put in our own Backbone model, collection and view. $ ( function (){ var Person = Backbone . Model . extend ({ defaults : { hugs : 0 } }); function retrieveTwitterInfo ( element ) { { img_url : 'https://api.twitter.com/1/users/profile_image?screen_name=' + element + '&size=bigger' } }; var rocketeerTwitterHandles = [ 'marianphalen' , 'mrmicahcooper' , 'knwang' ]; var rocketeers = rocketeerTwitterHandles . map ( retrieveTwitterInfo ); var People = Backbone . Collection . extend ({ model : Person }); var PersonView = Backbone . View . extend ({ tagName : \"li\" , template : _ . template ( $ ( '#person-template' ). html ()), initialize : function () { _ . bindAll ( this , 'render' ); this . model . bind ( 'change' , this . render ); }, render : function () { $ ( this . el ). html ( this . template ( this . model . toJSON ())); return this ; } }); var AppView = Backbone . View . extend ({ el : $ ( \"hug_a_rocketeer\" ), initialize : function () { }, render : function (){ var self = this ; _ ( this . collection . models ). each ( function ( item ){ self . appendItem ( item ); }, this ); }, appendItem : function ( item ) { var itemView = new PersonView ({ model : item }); $ ( \"ul#rocketeers\" ). append ( itemView . render (). el ); } }); var App = new AppView ; }); There's no magic at this point, still just to render out a static page, but we are now running this through the Backbone framework. The AppView render() function loops through all the people and append each person's rendered html to \"ul#rocketeers\". We are, however, pulling off people's profile photo from directly from Twitter with their Twitter handle. The next iteration took me a while. I wanted to actually hit Twitter's search API for each person to fetch mentions with the hashtag of \"#hugarocketeer\", and refresh the board on the callbacks. There's a bit of \"gotcha\" here: though Backbone supports automatically sorting of models in the collections, it happens at the time models are inserted into the collection. In our case, the retrieval of data from Twitter is asynchronous and the \"hugs\" attribute on Person won't get set until after they are inserted into the collection. So we cannot just rely on Backbone's collection sorting (by defining \"comparator\" on the collection.)  The solution I came up with was to clear the board and sort and re-render on every callback. The result is that you'll see the list flickering a few times until finally settling on the final rank. $ ( function (){ var Person = Backbone . Model . extend ({ defaults : { img_url : '' , name : '' , hugs : 0 , }, refresh_hugs : function () { var self = this ; $ . getJSON ( this . get ( \"search_url\" ), function ( data ) { var hugs = data . results . length ; self . set ({ hugs : hugs }); }) } }); var PersonView = Backbone . View . extend ({ tagName : \"li\" , template : _ . template ( $ ( '#person-template' ). html ()), initialize : function () { _ . bindAll ( this , 'render' ); this . model . bind ( 'change' , this . render ); }, render : function () { $ ( this . el ). html ( this . template ( this . model . toJSON ())); return this ; } }); var rocketeers = [ 'marianphelan' , 'mrmicahcooper' , 'p_elliott' , 'knwang' , 'martinisoft' , 'adam_lowe' , 'bthesorceror' , 'higgaion' , 'camerondaigle' , 'ChrisCardello' , 'biggunsdesign' , 'daveisonthego' , 'jonallured' , 'joshuadavey' , 'videokaz' , 'mattonrails' , 'mattpolito' , 'therubymug' , 'shaneriley' , 'shayarnett' , 'voxdolo' , 'syntaxritual' , 'johnny_rugger' ]. map ( function ( twitter_handle ){ return new Person ({ img_url : \"https://api.twitter.com/1/users/profile_image?screen_name=\" + twitter_handle + \"&size=normal\" , name : '@' + twitter_handle , search_url : 'http://search.twitter.com/search.json?q=' + '@' + twitter_handle + '%20' + \"%23hugarocketeer\" + \"&rpp=100&callback=?\" }); }); var People = Backbone . Collection . extend ({ model : Person , comparator : function ( person ) { return - person . get ( 'hugs' ); } }); var AppView = Backbone . View . extend ({ el : $ ( \"#hug_a_rocketeer\" ), returned_results : 0 , initialize : function () { var self = this ; this . collection = new People (); _ . each ( rocketeers , function ( person ) { $ . getJSON ( person . get ( \"search_url\" ), function ( data ) { var hugs = data . results . length ; person . set ({ hugs : hugs }); self . collection . add ( person ); self . returned_results += 1 ; if ( self . returned_results == rocketeers . length ) { self . render (); } }) }); _ . bindAll ( this , 'render' ); }, render : function (){ var self = this ; _ ( this . collection . models ). each ( function ( item ){ self . appendItem ( item ); }, this ); }, appendItem : function ( item ) { var itemView = new PersonView ({ model : item }); $ ( \"ul#rocketeers\" ). append ( itemView . render (). el ); }, }); var App = new AppView ; }); This is actually when I stopped last week after about 5 hours of hacking... not bad to pick up a new technology and make a fun app to entertain my coworkers. Just as I was finishing this blog post, I asked @shaneriley , our Javascript expert-in-residence, for the asynchronous callback ordering problem, and he suggested a better solution - I could count the number of returned callbacks from Twitter queries and render just once once I know all have come back. So here it is for that: .... returned_results : 0 , initialize : function () { var self = this ; this . collection = new People (); _ . each ( rocketeers , function ( person ) { $ . getJSON ( person . get ( \"search_url\" ), function ( data ) { var hugs = data . results . length ; person . set ({ hugs : hugs }); self . collection . add ( person ); self . returned_results += 1 ; if ( self . returned_results == rocketeers . length ) { self . render (); } }) }); _ . bindAll ( this , 'render' ); }, .... Check it out live and start hugging rocketeers! Code is here on github Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-05-21"},
{"website": "Hash-Rocket", "title": "Best of TIL Year One: Vim", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/best-of-til-year-one-vim", "abstract": "Hashrocket Projects Best of TIL Year One: Vim by\nJake Worth\n\non\nJune 14, 2016 Here are some of the top Vim posts from Today I Learned . Today I Learned is now over a year old. The participation from my coworkers has been inspiring, rocketing our posts\ncount to over 750. The enthusiasm from fans of the\nsite has been encouraging, on Twitter, Hacker News, and elsewhere. We open-sourced the codebase earlier this\nyear and have gotten issues and pull requests, as well as several clones in\nproduction. It's been awesome. TIL content is like a river; if you haven't posted today, your content gets lost\nin the stream. That's why I'd like to pause and highlight some of the\ntop posts from the first year of Today I Learned. Beyond a chance to reflect, I hope we can all benefit from the hard-won epiphanies of Rocketeers\nat work. Leverage the head-banging of others! We'll start with Vim, our first and most active channel. Below are the top five\nmost liked Vim posts, in order, from the first year of Today I Learned. Enjoy! Highlight #Markdown Fenced Code Syntax in #Vim (Dorian Karter) Up until now I was editing all my blog posts in Atom for lack of markdown fenced code block highlighting in Vim. Atom supported it out of the box. As it turns out Vim can do that too, out of the box! If you are running a recent version of Vim (mine at time of writing 7.4.9910) you can setup highlighting for fenced code blocks in markdown. Open your .vimrc and add the following line: let g:markdown_fenced_languages = [ 'html' , 'vim' , 'ruby' , 'python' , 'bash=sh' ] You can customize it to your liking and add more languages. > Note: Not all languages are supported but you can try and see if they work. To see a list of languages you have installed on Mac (with Vim installed by Homebrew) run ls -l /usr/local/Cellar/vim/7.4.1190/share/vim/vim74/syntax This is a game changer. Vim Change Case of Words (Vinicius Negrisolo) If you want to switch the case in Vim: gu => lowercase gU => uppercase g~ => toogle case You can also combine with Vim motions like: g~ip => toogle case for the current paragraph. Browse and repeat Vim commands (Dorian Karter) If you typed a command in Vim's command line and want to repeat it you can open the command-line window and browse through past commands. There are two ways two do that: From Normal mode type q: From the command line after typing a colon press CTRL-f In the command window you can browse up and down with HJKL and hit return to re-run the command. Delete to the End of the Line (Josh Branchaud) There are a number of ways to delete from the cursor position to the end of\nthe line. Generally when I am doing this, I want delete to the end of the\nline and then start typing something different. Perhaps the best way to do\nthis is by hitting C . It deletes to the end of the line and then leaves\nyou in insert mode. This also makes for easier repetition with the dot\ncommand. This is synonymous with hitting c$ . See :h C for more details. h/t Dorian Karter VSwitch Direction of Selection in Vim (Dorian Karter) Switch the free end of the cursor in Vim's VISUAL MODE. Simply press o while selecting and the free end will go to the beginning of your selection. Conclusion Thanks to Josh, Dorian, and Vinicius for those posts. Today I Learned had a spike in traffic near the beginning of the year, and these posts are mostly from that time. But there's a lot of great Vim tips from earlier. See them all here: https://til.hashrocket.com/vim Keep composing those commands, and learning every day. This post is part of a series. Stay tuned for my next post, when we will look at the top SQL posts from year one. Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-06-14"},
{"website": "Hash-Rocket", "title": "Modeling Polymorphic Associations in a Relational Database", "author": ["\nJack Christensen\n\n"], "link": "https://hashrocket.com/blog/posts/modeling-polymorphic-associations-in-a-relational-database", "abstract": "PostgreSQL Modeling Polymorphic Associations in a Relational Database by\nJack Christensen\n\non\nJuly  7, 2016 Polymorphic associations can be difficult to correctly represent in a relational database. In this post I will compare and contrast four different ways to model these challenging relationships. Imagine you are tasked with designing a fine-grained access control system for a large system. There are many resources across multiple types with permissions such as read, write, and admin. Permissions can be granted to users or user groups. A good first step is to introduce an access control list entity. This ACL entity will be responsible for the entire process or determining whether a user has a permission on a resource. Every access controlled resource should have its own ACL. The ACL concept itself would be rather complicated, probably composed of several tables. But for this example we won't concern ourselves with the implementation of the ACL, instead we will use a stub acl table and focus on the challenge of how to relate an acl to arbitrary resource types. For this example, the resource types we want to access control will be document , image , file , and report . Polymorphic Joins A simple approach to connect an acl to a resource is to use two columns on the acl table: resource_type and resource_id . This approach was popularized by Ruby on Rails. The acl table could be defined as follows: create table acl ( id serial primary key , resource_type varchar not null , resource_id integer not null , -- other fields omitted unique ( resource_id , resource_type ) ); The query for retrieving an acl for document id:42 would be like the following: select * from acl where resource_type = 'document' and resource_id = 42 ; A serious problem with this approach is the database is very limited in the data integrity it can enforce due to the lack of foreign key constraints. It can ensure a resource has no more that one acl , but that is all. A resource can be missing an acl and an acl can point to a missing resource. Join Table Per Relationship Type Another approach is to use a join table for each relationship type. create table acl_document ( acl_id integer not null unique references acl , document_id integer not unique null references document ); create table acl_image ( acl_id integer not null unique references acl , image_id integer not null unique references image ); -- and so on for each resource type The query for retrieving an acl for document id:42 would be like the following: select acl . * from acl join acl_document on acl_document . acl_id = acl . id where document_id = 42 ; This approach does use foreign key constraints, so the database can ensure that any connections between an acl and a resource are valid. However, it has no way to require that a resource has an acl . And while the uniqueness constraints ensure that an acl cannot be connected to multiple records of the same type (e.g. two document rows) it has no way to prevent an acl from being connected to resources of different types (e.g. one acl could be incorrectly connected to both a document and an image ). Reverse Belongs-To Even though the acl logically belongs to the resource, this relationship can be reversed by including an acl_id on the resource. This is similar in implementation to class table inheritance , but the relationship is more of inclusion rather than is-a . create table document ( id serial primary key , acl_id integer not null unique references acl , --- other fields omitted ); With this design, all resources are guaranteed to have valid references to an acl . However, there is no way to prevent orphan acl records. As with the previous technique there is no way to prevent different types of resources from pointing to the same acl . Exclusive Belongs To (AKA Exclusive Arc) In this model, the acl has foreign keys to all tables to which it can belong. create table acl ( id serial primary key , document_id integer references document , image_id integer references image , file_id integer references file , report_id integer references report , -- other fields omitted check ( ( ( document_id is not null ):: integer + ( image_id is not null ):: integer + ( file_id is not null ):: integer + ( report_id is not null ):: integer ) = 1 ) ); create unique index on acl ( document_id ) where document_id is not null ; create unique index on acl ( image_id ) where image_id is not null ; create unique index on acl ( file_id ) where file_id is not null ; create unique index on acl ( report_id ) where report_id is not null ; Take note of the check constraint. This ensures that an acl belongs to exactly one resource of any type. With this design an acl cannot be orphaned, but there is no way to enforce that a resource has an acl . Also important are the partial unique indexes. Limiting the unique indexes to only not null values dramatically saves space as well as reducing write operations on insert. How to Choose I usually dismiss a polymorphic join immediately due to its lack of data integrity guarantees. The only advantage it has is an ORM such as Rails' ActiveRecord may make it very easy to use. Join table per relationship type is an improvement over polymorphic joins , but at the cost of an extra table per relationship. In addition, it doesn't do anything that either of the belongs-to models can't do better. Both belongs-to models are nearly equivalent in their data integrity and performance characteristics. The major difference is whether the acl or the resource has to exist first to satisfy the foreign key constraint. As a resource is naturally the master record it is usually more convenient and intuitive to insert the resource first. This leads me to slightly slightly prefer exclusive belongs-to . But there are a few concerns with exclusive belongs-to . First, multiple null fields offends traditional relational design sensibilities. However, data integrity is maintained via the check constraint. Second, wide tables may cause performance to suffer. But in the case of PostgreSQL, null values are almost free. A nullable field has a 1-bit per row overhead rounded up to the nearest byte (e.g. 30 nullable fields have 4 bytes of overhead). Combine that with partial indexes ignoring null values and the performance is not an issue. Third, adding a new table requires adding a column to the exclusive belongs-to table. If this was a large, heavily used table there might be an issue with how long the table would be locked. With PostgreSQL, this is not a problem. Nullable fields can be added quickly regardless of table size. The updated check constraint can also be added without blocking concurrent usage. In conclusion, I suggest using an exclusive belongs-to model to represent a polymorphic association. If situation or developer sensibilities preclude this, then use a reverse belongs-to model. It has nearly the same characteristics. Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-07-07"},
{"website": "Hash-Rocket", "title": "Managing React Router Pathnames", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/managing-react-router-pathnames", "abstract": "React Javascript Managing React Router Pathnames by\nJosh Branchaud\n\non\nJanuary 10, 2019 So, you've introduced react-router into your\nsingle page React app as a way of managing routing. The app has started to\ngrow. There are many top-level routes for rendering different \"pages\". Some\nof those \"pages\" now have sub-routing within them for managing wizard\nworkflows and sections with multiple \"sub-pages\". This is great. The dream\nof routing within your React SPA has been realized. Fast-forward a few months, and your elegant routing solution has become a brittle, error-prone maintenance nightmare. What happened? A Simplified Example Early into the development of a large React SPA, I noticed the above\nscenario beginning to emerge. Some of what I noticed can be summarized in\nthe following series of code snippets. Note: these code snippets are not\nfull, working examples. Only the instructive parts have been included. /* App.js */ return ( < Switch > < Route path= \"/\" component= { Home } /> < Route path= \"/blog\" component= { Blog } /> < Route path= \"/team\" component= { Team } /> < Route path= \"/about-us\" component= { AboutUs } /> </ Switch > ); This is the top-level routing in our main app file. /* NavLinks.js */ < nav > < Link to= \"/\" > Home </ Link > < Link to= \"/blog\" > Blog </ Link > < Link to= \"/team\" > Team </ Link > < Link to= \"/about-us\" > About Us </ Link > < / nav > These are some navigational links that get used in a shared header. /* Footer.js */ < div className = \"footer\" > < Link to= \"/about-us\" > About Us </ Link > < Link to= \"/team\" > Team </ Link > < Link to= \"/blog\" > Blog </ Link > < / div > Here is a shared Footer component that provides links in a different layout\nthan the NavLinks component. /* Blog.js */ < div className = \"blog\" > < h1 > Our Blog </ h1 > < Switch > < Route path= \"/blog\" component= { BlogIndex } /> < Route path= \"/blog/:id\" component= { BlogShow } /> </ Switch > < / div > This is the blog part of the site that gets routed to when one of the Blog links is clicked. It has a nested Switch which handles both the index path\n( /blog ) and a path to a specific blog post ( /blog/123 ). /* BlogShow */ < div className = \"blog-show\" > < Link to= { `/blog/${props.blogId}` } >< h2 > { props . title } </ h2 ></ Link > { /* blog content ... */ } < Link to= { `/blog/${props.prevBlogId}` } > Previous Post </ Link > < Link to= { `/blog/${props.nextBlogId}` } > Next Post </ Link > < / div > This is the component that shows the content of a specific blog post. It has\nlinks throughout for itself as well as the previous and next blog posts. The Problem There are two separate, but related issues emerging in the above code\nsnippets. The first is the duplication of strings like '/blog' over and over across\nmultiple files in the codebase. Even in this simplified example you can\nimagine how this could quickly get out of hand. When a string has to be\ntyped the same way over and over, there is bound to be a time when it is\nmistyped. Babel cannot help with mistyped string constants. Without a standardized way of managing pathnames, it is likely that a\nvariety of divergent approaches to defining and referencing pathnames will\nresult. This will become hard to maintain over time. The second issue is that of terse, error-prone string concatenation.\nEverywhere you are building a parameterized pathname requires that you get\nall the details right. This is why many frameworks, like Rails, have URL\nbuilding APIs. The Solution The fix to all of this is to create a single source of truth for all of your\npathname needs. First, create a routes file -- routes.js . Then move all of the string\npathnames into that file, each one is its own exported constant. export const blogPath = \"/blog\" ; Additionally, find everywhere that parameterized pathnames are being manually\nconcatenated together. Each of these can instead be a standard pathname\nbuilding function in the routes file. export const buildBlogShowPath = blogId => `/blog/ ${ blogId } ` ; These constants and parameterized path building functions can be imported\nwherever they are needed. If changes of any kind need to be applied, you are\nin a much more secure position to be making them. Mistyped string constants\nnow become mistyped variable names which will be flagged by Babel. Here is what some of the above simplified example could instead look like. /* routes.js */ export const rootPath = \"/\" ; export const teamPath = \"/team\" ; export const aboutPath = \"/about-us\" ; export const blogPath = \"/blog\" ; export const blogShowPath = \"/blog/:id\" ; export const buildBlogShowPath = blogId => `/blog/ ${ blogId } ` ; /* App.js */ import { rootPath , blogPath , teamPath , aboutPath } from \"./routes\" ; < Switch > < Route path= { rootPath } component= { Home } /> < Route path= { blogPath } component= { Blog } /> < Route path= { teamPath } component= { Team } /> < Route path= { aboutPath } component= { AboutUs } /> </ Switch > /* NavLinks.js */ import { rootPath , blogPath , teamPath , aboutPath } from \"./routes\" ; < nav > < Link to= { rootPath } > Home </ Link > < Link to= { blogPath } > Blog </ Link > < Link to= { teamPath } > Team </ Link > < Link to= { aboutPath } > About Us </ Link > </ nav > /* Blog.js */ import { blogPath , blogShowPath } from \"./routes\" ; < div className= \"blog\" > < h1 > Our Blog </ h1 > < Switch > < Route path= { blogPath } component= { BlogIndex } /> < Route path= { blogShowPath } component= { BlogShow } /> </ Switch > </ div > /* BlogShow */ import { buildBlogShowPath } from \"./routes\" ; < div className= \"blog-show\" > < Link to= { buildBlogShowPath ( props . blogId ) } >< h2 > { props . title } </ h2 ></ Link > { /* blog content ... */ } < Link to= { buildBlogShowPath ( props . prevBlogId ) } > Previous Post </ Link > < Link to= { buildBlogShowPath ( props . nextBlogId ) } > Next Post </ Link > </ div > Conclusion There are two general principles at play in this solution. The first is the\nidea of reducing the number of magic strings in a codebase. The second is\nnormalizing data so that there is a single source of truth. I've found this\nthinking to be well-suited to the large react-router -powered SPA on which\nI've been working. These, however, are broader ideas. I generally recommend\nlooking out for opportunities to apply them. They make code more robust and\nresilient to change. Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-01-10"},
{"website": "Hash-Rocket", "title": "Mocking Requests in Elixir Tests", "author": ["\nJason Cummings\n\n"], "link": "https://hashrocket.com/blog/posts/mocking-requests-in-elixir-tests", "abstract": "Elixir Mocking Requests in Elixir Tests by\nJason Cummings\n\non\nJuly 26, 2016 When writing code that relies heavily on the results of external API calls, we need to be thinking about how we're going to test how our program reacts to different responses.  Making an actual request would make our test suite extremely slow, so this is a good time to mock our responses. Here's where we're at: we're working though an OAuth 2.0 Authorization flow.\nWithout going into too much detail, the first step is to request an\nauthorization code from the API by sending over encoded credentials.\nThe next step, and the one we're using for this example, is the user\nauthentication.  After successful authorization the external API will hit our\nauthentication endpoint with the authorization code in the params.  That's all\nwe really need to know here. defmodule Authentication do def authenticate ( conn , %{ \" code\" => code }) do if get_refresh_cookie ( conn ) do AuthenticationClient . post ( conn , refresh_body_params ( conn )) else AuthenticationClient . post ( conn , body_params ( code )) end end # ... functions and stuff end defmodule AuthenticationClient do @url \" http://www.an-awesome-api.com/authenticate\" def post ( conn , params ) do case HTTPoison . post ( @url , params , AnAwesomeApi . headers ) do { :ok , % HTTPoison . Response { status_code: 200 , body: body }} -> { :ok , response } = Poison . decode ( body ) cookies = get_cookies_from_response ( response ) conn = set_cookies ( conn , cookies ) { :ok , conn } { :error , % HTTPoison . Error { reason: reason }} -> { :error , reason , conn } end end end defmodule OauthAuthorizationFlowTest do use ExUnit . Case use Plug . Test describe \" authentication\" do test \" a successful attempt sets the cookies\" do conn = conn ( :post , \" /authenticate\" , %{ \" code\" => \" valid\" }) |> Plug . Conn . fetch_cookies assert { :ok , new_conn } = Authentication . authenticate ( conn , conn . params ) assert new_conn . cookies [ \" spotify_access_token\" ] == \" access_token\" assert new_conn . cookies [ \" spotify_refresh_token\" ] == \" refresh_token\" end end end Our Authentication module uses the AuthenticationClient to make a POST request for authentication. In the above test, we will end up hitting our client\nand making an actual http call. This is not what we want. In order to mock this\nrequest, we need to pull out our http call into its own function. defmodule AuthenticationClient do def post ( conn , params ) do case AuthRequest . post ( params ) do { :ok , % HTTPoison . Response { status_code: 200 , body: body }} -> { :ok , response } = Poison . decode ( body ) cookies = get_cookies_from_response ( response ) conn = set_cookies ( conn , cookies ) { :ok , conn } { :error , % HTTPoison . Error { reason: reason }} -> { :error , reason , conn } end end end defmodule AuthRequest do @url \" https://www.an-awesome-api.com/api/token\" def post ( params ) do HTTPoison . post ( @url , params , AnAwesomeApi . headers ) end end Not only did we pull it into it's own function, we pulled it into it's own module. Why? We will be using Mock , a wrapper for the Erlang\nmocking library Meck.  When Mock stubs a function, it creates it's own version\nof the module containing that function with the stubbed function available, and\nnothing else. If we moved the call to HTTPoison to another function within the\nsame module, post/2 would be undefined. Let's tell the test to use a mock api client:\nThe with_mock macro is documented in the library, linked above. defmodule OauthAuthorizationFlow do use ExUnit . Case use Plug . Test import Mock describe \" authentication\" do test \" a successful attemp sets the cookies\" do with_mock AuthRequest , [ post: fn ( params ) -> AuthenticationClientMock . post ( params ) end ] do conn = conn ( :post , \" /authenticate\" , %{ \" code\" => \" valid\" }) |> Plug . Conn . fetch_cookies assert { :ok , new_conn } = Authentication . authenticate ( conn , conn . params ) assert new_conn . cookies [ \" access_token\" ] == \" a_valid_access_token\" assert new_conn . cookies [ \" refresh_token\" ] == \" a_valid_refresh_token\" end end end end Now, our call to AuthRequest.post/1 will be sent to AuthenticationClientMock.post/1 , which doesn't exist yet.\nWe can throw a debugger into our actual call using a catch all at the top of our\ncase statement to get a copy/pastable response to use in our mock. (If we did\nthings in this exact order we'd also need to comment out the with mock line\nso we can hit the actual request.) defmodule AuthenticationClient do def post ( conn , params ) do case AuthRequest . post ( params ) do { :ok , response } -> require IEx ; IEx . pry { :ok , % HTTPoison . Response { status_code: 200 , body: body }} -> { :ok , response } = Poison . decode ( body ) cookies = get_cookies_from_response ( response ) conn = set_cookies ( conn , cookies ) { :ok , conn } { :error , % HTTPoison . Error { reason: reason }} -> { :error , reason , conn } end end end response will be the struct returned by HTTPoison . We'll paste it into our\nmock module (and replace some lengthly values with \"foo\") in successful_response/0 , then remove our catchall clause and debugger. #test/mocks/authentication_client_mock.exs defmodule HTTPoison . Response do defstruct body: nil , headers: nil , status_code: nil end defmodule AuthenticationClientMock do def post ( params ) do { :ok , successful_response } end defp successful_response do % HTTPoison . Response { body: \" {\\\" access_token \\ \" :\\\" a_valid_access_token \\ \" ,\\\" token_type \\ \" :\\\" Bearer \\ \" ,\n        \\\" expires_in \\ \" :3600,\\\" a_valid_refresh_token \\ \" :\\\" refresh_token \\ \" }\" , headers: [{ \" Server\" , \" nginx\" }, { \" Date\" , \" Thu, 21 Jul 2016 16:52:38 GMT\" }, { \" Content-Type\" , \" application/json\" }, { \" Content-Length\" , \" 397\" }, { \" Connection\" , \" keep-alive\" }, { \" Keep-Alive\" , \" timeout=10\" }, { \" Vary\" , \" Accept-Encoding\" }, { \" Vary\" , \" Accept-Encoding\" }, { \" X-UA-Compatible\" , \" IE=edge\" }, { \" X-Frame-Options\" , \" deny\" }, { \" Content-Security-Policy\" , \" default-src 'self'; script-src 'self' foo\" }, { \" X-Content-Security-Policy\" , \" default-src 'self'; script-src 'self' foo\" }, { \" Cache-Control\" , \" no-cache, no-store, must-revalidate\" }, { \" Pragma\" , \" no-cache\" }, { \" X-Content-Type-Options\" , \" nosniff\" }, { \" Strict-Transport-Security\" , \" max-age=31536000;\" }], status_code: 200 } end end With our mock in place it doesn't matter if we use a real response or a mock\nresponse, our client post function will still pattern match on the same struct\nfields and behave the same. There would be more work if we wanted to mock out\nfailed responses and other outcomes, but you get the idea. Let's take another look at the line that sets up our mock: with_mock AuthRequest , [ post: fn ( params ) -> AuthenticationClientMock . post ( params ) end ] do There's a case for cleaning up this line of code.  Since we've kept the\ninvolved modules small there likely won't be any different mock setups. That and\nthe fact that we're going to need to use the same setup for more test cases,\nthis is most likely a good candidate for being wrapped up in a macro. I say probably because it's very easy to find yourself writing macros in any\nlanguage that allows you to do so; however if anything can be done with simple\nfunctions, we should refrain from writing a macro. This is a good candidate for\na macro in my opinion, so we'll go with it. Now we can clean up our mocks like so: defmodule OauthAuthorizationFlow do use ExUnit . Case use Plug . Test import Mock defmacro with_auth_mock ( block ) do quote do with_mock AuthRequest , [ post: fn ( params ) -> AuthenticationClientMock . post ( params ) end ] do unquote ( block ) end end end describe \" authentication\" do test \" a successful attempt sets the cookies\" do with_auth_mock do conn = conn ( :post , \" /authenticate\" , %{ \" code\" => \" valid\" }) |> Plug . Conn . fetch_cookies assert { :ok , new_conn } = Authentication . authenticate ( conn , conn . params ) assert new_conn . cookies [ \" aaa_access_token\" ] == \" access_token\" assert new_conn . cookies [ \" aaa_refresh_token\" ] == \" refresh_token\" end end end end If you want to see real world application of this, I took this from concept from\na Spotify wrapper I'm working on. If you're interested in your next project being written in Elixir, we're going\nnuts over Elixir and Phoenix at Hashrocket and we want to work with you. Get in touch today! Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-07-26"},
{"website": "Hash-Rocket", "title": "Integration Testing Phoenix With Wallaby", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/integration-testing-phoenix-with-wallaby", "abstract": "Elixir Phoenix Integration Testing Phoenix With Wallaby by\nJake Worth\n\non\nJanuary  5, 2017 Let's write an integration test for Phoenix using Wallaby. Integration tests are used for behavior description and feature delivery. From\nthe test-writer's perspective, they are often seen as a capstone, or, to the\noutside-in school, an initial 10,000-foot description of the landscape. At Hashrocket\nwe've been perusing the integration testing landscape for Elixir, and\nrecently wrote a suite using Wallaby . In this post, I'll walk through a test we wrote for a recent Phoenix project. Overview Two emerging tools in this space are Hound and Wallaby ; they differ in many ways, enumerated in this Elixir\nChat thread . The Wallaby team describes the project as follows: Wallaby helps you test your web applications by simulating user interactions.\nBy default it runs each TestCase concurrently and manages browsers for you. We chose Wallaby for our project– an ongoing Rails-to-Phoenix port of Today I Learned ( available\nhere )– because we liked the API. It's\nsimilar to Ruby's Capybara. Setup Here are the basic steps we took to create our first Wallaby test. Wallaby concurrently powers multiple PhantomJS headless browsers. To leverage that feature we'll need PhantomJS: $ npm install -g phantomjs Next, add Wallaby to your Phoenix dependencies: # mix.exs def deps do [{ :wallaby , \" ~> 0.14.0\" }] end As always, install the dependencies with mix deps.get . Ensure that Wallaby is properly started, using pattern matching, in your test_helper.exs : # test/test_helper.exs { :ok , _ } = Application . ensure_all_started ( :wallaby ) If you're using Ecto, enable concurrent testing by adding the Phoenix.Ecto.SQL.Sandbox plug to your endpoint (this requires Ecto v2.0.0-rc.0 or\nnewer). Put this is at the top of endpoint.ex , before any other plugs. # lib/tilex/endpoint.ex if Application . get_env ( :your_app , :sql_sandbox ) do plug Phoenix . Ecto . SQL . Sandbox end # config/test.exs # Make sure Phoenix is setup to serve endpoints config :tilex , Tilex . Endpoint , server: true config :tilex , :sql_sandbox , true Use test_helper.exs for any further configuration, like so: # test/test_helper.exs Application . put_env ( :wallaby , :base_url , \" http://localhost:4001\" ) We also enabled a feature which saves a screenshot\non every failure. This is crucial when testing with a headless browser: # config/test.exs config :wallaby , screenshot_on_failure: true That's the basic setup. If you get stuck, here's the pull\nrequest where we made all these changes. Testing Anytime we're writing a test, we want to extract shared logic into\na central place. Phoenix accomplishes this with the IntegrationCase concept. This module does\na lot of things, including importing other modules, defining aliases, and assigning variables. I don't want to dig too deeply into this file, but will include it here in its entirety so all our setup is clear. A few\nnoteworthy points are import Tilex.TestHelpers , which will let us build custom helper functions, and the setup tags block, which\nI copied directly from the Wallaby docs. # test/support/integration_case.ex defmodule Tilex . IntegrationCase do use ExUnit . CaseTemplate using do quote do use Wallaby . DSL alias Tilex . Repo import Ecto import Ecto . Changeset import Ecto . Query import Tilex . Router . Helpers import Tilex . TestHelpers end end setup tags do :ok = Ecto . Adapters . SQL . Sandbox . checkout ( Tilex . Repo ) unless tags [ :async ] do Ecto . Adapters . SQL . Sandbox . mode ( Tilex . Repo , { :shared , self ()}) end metadata = Phoenix . Ecto . SQL . Sandbox . metadata_for ( Tilex . Repo , self ()) { :ok , session } = Wallaby . start_session ( metadata: metadata ) { :ok , session: session } end end Let's test! We're going to assert about a user's (or a developer's, in the language of this app) ability to create a new post through a form. Forms are fantastic\nsubjects for integration tests because there are many edge cases. First, create a test file. Here's the structure, which\ndefines our test module and includes our IntegrationCase module. We use async: true to configure this test case to run in parallel with other test cases. # test/features/developer_create_post_test.exs defmodule DeveloperCreatesPostTest do use Tilex . IntegrationCase , async: true end I start by creating an empty test to confirm my setup. # test/features/developer_create_post_test.exs defmodule DeveloperCreatesPostTest do use Tilex . IntegrationCase , async: true test \" fills out form and submits\" , %{ session: session } do end end Run it with mix test : $ mix test\n\nwarning: variable session is unused\n  test/features/developer_creates_post_test.exs:6\n\n.\n\nFinished in 1.3 seconds\n1 test, 0 failures Okay, our test runs. Also, notice that Elixir compile-time warning, variable session is unused ? That's the kind of perk I love about writing\nin this language. We'll keep the variable, because it will prove useful. What now? Let's fill out the form, which looks like this: < % # web/templates/post/form.html.eex %> <%= form_for @changeset , post_path ( @conn , :create ), fn f -> % > < dl > < dt > <%= label f , :title % > </ dt > < dd > <%= text_input f , :title % > </ dd > </ dl > < dl > < dt > <%= label f , :body % > </ dt > < dd > <%= textarea f , :body % > </ dd > < dt > <%= label f , :channel_id , \" Channel\" % > </ dt > < dd > <%= select f , :channel_id , @channels , prompt: \" \" % > </ dd > </ dl > <%= submit \" Submit\" % > < % end % > Our test will use Ecto Factory to generate a channel struct, which we'll need to select the channel on the form. All the\ntest code that follows is implied to be inside our test block. EctoFactory . insert ( :channel , name: \" phoenix\" ) visit ( session , \" /posts/new\" ) h1_heading = get_text ( session , \" main header h1\" ) assert h1_heading == \" Create Post\" So we create a channel, use the visit/2 function to visit our new post path, and then\nget the text from the header and make an assertion about it. get_text/2 is a custom helper function, available in a test_helpers.exs file that we\nalready imported via our IntegrationCase module. It extracts a common task: getting\nthe inner text from an HTML selector. Here's the definition: # test/support/test_helpers.ex defmodule Tilex . TestHelpers do use Wallaby . DSL def get_text ( session , selector ) do session |> find ( selector ) |> text end end Extract as many helpers as your tolerance for abstraction allows. Okay, time to fill in the form. Here, our session variable and Elixir's pipe\noperator ( |> ) shine. session |> fill_in ( \" Title\" , with: \" Example Title\" ) |> fill_in ( \" Body\" , with: \" Example Body\" ) |> Actions . select ( \" Channel\" , option: \" phoenix\" ) |> click_on ( 'Submit' ) We'll alias Wallaby.DSL.Actions to Actions for convenience. Why be so verbose at all? Because select/3 is ambiguous\nwith Ecto.Query . Let's assert about our submission. If it worked, we should be on the post index page, which looks like this: < % # web/templates/post/index.html.eex %> < section id = \" home\" > <%= for post <- @posts do % > <%= render Tilex . SharedView , \" post.html\" , conn: @conn , post: post % > < % end % > </ section > And here's the post partial it references, which we'll assert about: < % # web/templates/shared/post.html.eex %> < article class = \" post\" > < section > < div class = \" post__content copy\" > < h1 > <%= link ( @post . title , to: post_path ( @conn , :show , @post )) % > </ h1 > <%= raw Tilex . Markdown . to_html ( @post . body ) % > < footer > < p > < br /> <%= link ( display_date ( @post ), to: post_path ( @conn , :show , @post ), class: \" post__permalink\" ) % > </ p > </ footer > </ div > < aside > < ul > < li > <%= link ( \" # #{ @post . channel . name } \" , to: channel_path ( @conn , :show , @post . channel . name ), class: \" post__tag-link\" ) % > </ li > </ ul > </ aside > </ section > </ article > Okay, on to the assertions: index_h1_heading = get_text ( session , \" header.site_head div h1\" ) post_title = get_text ( session , \" .post h1\" ) post_body = get_text ( session , \" .post .copy\" ) post_footer = get_text ( session , \" .post aside\" ) assert index_h1_heading =~ ~r/Today I Learned/i assert post_title =~ ~r/Example Title/ assert post_body =~ ~r/Example Body/ assert post_footer =~ ~r/#phoenix/i Once again we use our helper function get_text/2 to capture the text on the page. Then,\nwe use ExUnit to assert about the copy we found. Here it is all together: # test/features/developer_create_post_test.exs defmodule DeveloperCreatesPostTest do use Tilex . IntegrationCase , async: true alias Wallaby . DSL . Actions test \" fills out form and submits\" , %{ session: session } do EctoFactory . insert ( :channel , name: \" phoenix\" ) visit ( session , \" /posts/new\" ) h1_heading = get_text ( session , \" main header h1\" ) assert h1_heading == \" Create Post\" session |> fill_in ( \" Title\" , with: \" Example Title\" ) |> fill_in ( \" Body\" , with: \" Example Body\" ) |> Actions . select ( \" Channel\" , option: \" phoenix\" ) |> click_on ( 'Submit' ) index_h1_heading = get_text ( session , \" header.site_head div h1\" ) post_title = get_text ( session , \" .post h1\" ) post_body = get_text ( session , \" .post .copy\" ) post_footer = get_text ( session , \" .post aside\" ) assert index_h1_heading =~ ~r/Today I Learned/i assert post_title =~ ~r/Example Title/ assert post_body =~ ~r/Example Body/ assert post_footer =~ ~r/#phoenix/i end end And the final test run: $ mix test\n\n.\n\nFinished in 4.5 seconds\n1 test, 0 failures\n\nRandomized with seed 433617 Don't be alarmed by the 4.5 second run time. At the time of this post's\npublication, Tilex has twenty-two unit and integration tests, and the entire suite often runs about that fast on a normal laptop. Setup and teardown is the big cost. Conclusion If you haven't read Plataformatec's post about integration testing with\nHound, check it out . Both Hound and Wallaby seem great. Integration testing will be a big part of developing in Phoenix, as developers build more and more complex applications. I hope\nthis post gave you a new tool for developing your Phoenix apps from the\nsupportive harness of a test suite. Please let me know if you've used Wallaby,\nand how it worked for you. Photo Credit: NASA/Landsat8, Acadia National Park , flickr.com, https://www.flickr.com/photos/gsfc/29143744581 . Accessed 2 January 2017. Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-01-05"},
{"website": "Hash-Rocket", "title": "Elm by Example: Soup to Nuts", "author": ["\nDorian Karter\n\n"], "link": "https://hashrocket.com/blog/posts/elm-by-example-soup-to-nuts-part-1", "abstract": "Javascript Elm by Example: Soup to Nuts by\nDorian Karter\n\non\nJanuary 18, 2016 I've been experimenting with Elm for the past few months and have come to really appreciate its style of programming. It is very similar to React in the sense that you can render modular components based on DOM events, but the functional style and syntactic sugar are a pleasure to work with. In this blog post I will guide you in building your first Slack inspired component. By now you have probably heard about Elm, the statically typed, immutable, Haskell inspired, polite and helpful, functional reactive language for the web. It's extremely FAST too. It consistently performs better than React, Ember, Angular and others in the TODO MVC performance tests. I've been experimenting with Elm for the past few months and have come to really appreciate its style of programming. It is very similar to React in the sense that you can render modular components based on DOM events, but the functional style and syntactic sugar are a pleasure to work with. My favorite thing about Elm is that it is statically typed, yet type inferred. What that means is you can prototype quickly, and don't have to use type annotations, but the compiler will infer the types for you by flowing through your code and failing to compile when you did something wrong. This gives rise to Elm's best feature: NO RUNTIME EXCEPTIONS! This is a really big deal! After having written a fair amount of Elm it almost feels irresponsible writing JavaScript without this feature. Elm accomplishes this by forcing you to handle values that can be null before allowing you to compile your project. It also makes sure that you handle all potential values when using conditionals/pattern matching. As you work with Elm its awesomeness unfolds before you, and you will learn interesting Computer Science concepts, particularly if you have never worked with Haskell or other functional languages. Although Haskell can be hard to learn, Elm is very pragmatic and approachable and can be used to replace both standalone JavaScript libraries and rich UI components. Elm also lends itself really well for game programming due to its rich HTML5 Canvas abstraction and input interaction using signals. Motivation My reason for writing this blog post is that I was struggling with some of the more advanced concepts of Elm, namely Signals, Mailboxes, and Ports. I started writing a post about how to roll out your own Model View Update pattern in Elm without the StartApp but it was hard to start without an initial example, so I decided to write this post first to lead into the next one. In this two-part blog post I will take you through building your first Elm component - a Slack inspired quick channel switcher (Cmd+k). I chose this component because it was small, practical, and combined multiple Signals, namely HTML Signals and Keyboard Signals, making it an ideal candidate for introducing Signals and Mailboxes. Prerequisites I'm assuming basic familiarity with the Elm syntax. If you are not familiar with the Elm syntax see the official syntax documentation . Consider the above to be Part 0. Getting Elm To get started you will need to install Elm on your machine. npm update && npm install -g elm This article is written for Elm v0.16 You can also download the .pkg installer from the elm-lang.org website. You will also need a syntax highlighter for your editor. Here's the one I use for Vim: https://github.com/ElmCast/elm-vim Installing required packages Elm comes with an especially \"polite\" and quite \"intelligent\" package manager. It takes a Github relative url as an argument. Create a project directory and cd into it. Install the following packages: elm package install evancz/elm-html\nelm package install evancz/start-app\nelm package install circuithub/elm-html-extra Bootstrapping the component In your favorite code editor, create a ChannelSwitcher.elm file. First we need to declare the component, this is done with one line in elm which should be at the top of your file. module ChannelSwitcher where Now, below that, we need to import all the necessary modules: -- IMPORTS import Html exposing ( .. ) import Html . Attributes exposing ( .. ) import Html . Events exposing ( .. ) import Html . Events . Extra exposing ( .. ) import String import StartApp . Simple as StartApp Note that I'm using exposing (..) on some of the imports, the .. exposes all public functions from that module into the current scope so you can call them without prefixing with the module name. It is usually a best practice to avoid this type of exposure as much as possible to prevent naming collisions and ambiguity. However in this case it provides convenience when writing HTML. MODEL VIEW UPDATE Elm uses a Model View Update architecture which dictates the way data flows through an Elm application. You can think of an Elm application as a stream of events which are converted into actions, which then calculate the new state and render HTML. I like annotating my code with sections so that it is organized and I know where to look for things so I label it like so: module ChannelSwitcher where -- IMPORTS import Html exposing ( .. ) import Html . Attributes exposing ( .. ) import Html . Events exposing ( .. ) import Signal import StartApp . Simple as StartApp -- MODEL -- UPDATE -- VIEW Writing the HTML in Elm Now that we imported all of the HTML attributes it's time to write some Elm-flavored HTML. Under the view section declare a view function, follow that function with a main function, the entry point for any component. For now we will just use it to call view so we can see the HTML we generated. -- VIEW view = div [ class \" container\" ] [ input [ class \" search-box\" ] [ ] , ul [ class \" collection\" ] [ li [ class \" collection-item active\" ] [ text \" #Elm\" ] , li [ class \" collection-item\" ] [ text \" #react.js\" ] , li [ class \" collection-item\" ] [ text \" #ember\" ] ] ] main = view If you are familiar with React the code above should seem familiar. Think of it as the render function in react. The code above should be pretty self explanatory, the first square bracket of each element is its attributes, and the second is the tag content. The text function and all other HTML elements (div, input, ul, li) are exposed and available to us by exposing the Html module.   The class function used in the square brackets is imported from Html.Attributes . It is important at this point to think of the markup representing html elements as functions, because they are. When you call Html.div [] [] in the repl you would get an Elm record representing the DOM element as data. It will look something like this: { type = \" node\" , tag = \" div\" , facts = {} , children = {} , namespace = < internal structure >, descendantsCount = 0 } This is important because you cannot have two adjacent elements (e.g. <h1></h1><h2></h2> ) without a top level wrapping element (e.g. <div><h1></h1><h2></h2></div> ) and thinking of h1 and h2 as functions you quickly realize there is no nice way to return them as a function result in an immutable programming language without wrapping them with a third function. Compiling and Running in your browser Now we are ready to compile and run our application. In the terminal run elm make ChannelSwitcher.elm. This will generate an index.html for you, go ahead and open that in your browser. You should be able to see the following interface: Defining the Model The data that we need to flow through our component for it to generate the correct output should represent a list of channels. We start by defining a record, and we will also define an initial model so we have something to work with. -- MODEL type alias Model = { channels : List String , selectedChannel : Int , query : String } initialModel : Model initialModel = { channels = [ \" Elm\" , \" React.js\" , \" Ember\" , \" Angular 2\" , \" Om\" , \" OffTopic\" ] , selectedChannel = - 1 , query = \" \" } You don't have to call your type Model it can be anything. Defining Actions and the Update function As the user is interacting with the component, a new \"state\" of the model will be calculated. For example, selectedChannel will start at -1 and as we press the arrow keys up and down it will change to 0, 1, 2 etc.. Elm creates that new model using the update function. Our update function will take an action and return a new version of the model with a small modification. This makes it very convenient since you can look at the action type definition (see below) and immediately know what kind of transformations can happen to the model in this component. -- UPDATE type Action = NoOp | Filter String | Select Int update action model = case action of NoOp -> model Filter query -> { model | query = query } Select index -> { model | selectedChannel = index } The Action type we defined is a Union Type which allows us to perform Pattern Matching in the update function with the case statement. The Filter String part is basically a Tagged Union where Filter is the action tag with a String argument. This helps us differentiate it from other actions that may have a one string argument. Putting it all together with StartApp It's time to put it all together using StartApp.\nStartApp lets us declare which methods correspond with our model, update and view parts of our component. Replace the main function with the following: main : Signal Html main = StartApp . start { model = initialModel , update = update , view = view } If you try and compile the code so far you will get the following message: ==================================== ERRORS ====================================\n\n-- TYPE MISMATCH ------------------------------------------- ChannelSwitcher.elm\n\nThe argument to function `start` is causing a mismatch.\n\n51│   StartApp.start\n52│>    { model = initialModel\n53│>    , update = update\n54│>    , view = view\n55│>    }\n\nFunction `start` is expecting the argument to be:\n\n    { ..., view : Signal.Address Action -> Model -> Html }\n\nBut it is:\n\n    { ..., view : VirtualDom.Node }\n\nDetected errors in 1 module. That's because StartApp is passing a Mailbox address and the currently computed model to the view. Don't worry about understanding Mailboxes just yet, we will cover those in the second part of the tutorial. For now, to fix this error let's refactor our view function signature to the following, and add a type annotation while we are at it: view : Signal . Address Action -> Model -> Html view address model = Now you should be able to compile but as you notice when you open index.html the component still does not filter the list. Next we will render the model and implement the search/filter functionality. Rendering the model Let's render the model now instead of static data. Under the VIEW section we will add a new method that renders the li elements using the channels list on the model. -- VIEW renderChannel : String -> Html renderChannel name = li [ class \" collection-item\" ] [ text <| \" #\" ++ name ] renderChannels : List String -> Html renderChannels channels = let channelItems = List . map renderChannel channels in ul [ class \" collection\" ] channelItems view : Signal . Address Action -> Model -> Html view address model = div [ class \" card-panel\" ] [ input [ ] [] , renderChannels model . channels ] We created two new methods renderChannel , which renders an individual li representing a channel with the hash symbol (#), and renderChannels which uses a List.map to return a list of li elements. We then pass that list as the second argument of ul . Lastly, we call renderChannels from the view function, passing in the model.channels . Note: If you are wondering about the <| operator: it is a reverse pipe and it means the result of everything on the right of that operator (until the closure) will be piped into the function to the left of the operator. It's just a way to avoid parens. Filtering the list We are displaying the list rendered directly from the model, now it's time to filter the view according to the user input in the search box. First we need to store the filter the user types in on the model. For that we will add an onInput event on the input box, so we can filter the list as the user is typing. view : Signal . Address Action -> Model -> Html view address model = div [ class \" card-panel\" ] [ input [ onInput address Filter ] [] , renderChannels model . channels ] The Filter action tag provides us with a free \"constructor\" that takes in a string, that string will be passed in to onInput from the browser as event.target.value or in elm-html targetValue . Note: onInput is not yet part of the elm-html package, which is why we imported the Html.Events.Extra package which comes from circuithub/elm-html-extra Then we will use the List.filter method to only display channels starting with the input text. filterChannels : List String -> String -> List String filterChannels channels query = List . filter ( String . contains query ) channels Then use this function in the view function: view : Signal . Address Action -> Model -> Html view address model = div [ class \" card-panel\" ] [ input [ onInput address Filter ] [] , renderChannels ( filterChannels model . channels model . query ) ] To make sure that the filter is case insensitive we will need to refactor the filterChannels and pass both the query and list item to String.toLower . filterChannels : List String -> String -> List String filterChannels channels query = let containsCaseInsensitive str1 str2 = String . contains ( String . toLower str1 ) ( String . toLower str2 ) in List . filter ( containsCaseInsensitive query ) channels Adding style For styling the component we will create a new HTML file and include the Materialize CSS library. This is what your HTML should look like: <!DOCTYPE html> <html> <head> <link rel= \"stylesheet\" href= \"https://cdnjs.cloudflare.com/ajax/libs/materialize/0.97.5/css/materialize.min.css\" > <script src= \"channel_switcher.js\" ></script> </head> <body> <div id= \"elm-goes-here\" class= \"container\" ></div> <script> Elm . embed ( Elm . ChannelSwitcher , document . getElementById ( 'elm-goes-here' ) ); </script> </body> </html> To compile the Elm file into channel_switcher.js use the --output flag: elm make ChannelSwitcher.elm --output channel_switcher.js When you open the HTML file you should see something like this: And here is our Elm code so far: module ChannelSwitcher where -- IMPORTS import Html exposing ( .. ) import Html . Attributes exposing ( .. ) import Html . Events exposing ( .. ) import Html . Events . Extra exposing ( .. ) import String import StartApp . Simple as StartApp -- MODEL type alias Model = { channels : List String , selectedChannel : Int , query : String } initialModel : Model initialModel = { channels = [ \" Elm\" , \" React.js\" , \" Ember\" , \" Angular 2\" , \" Om\" , \" OffTopic\" ] , selectedChannel = - 1 , query = \" \" } -- UPDATE type Action = NoOp | Filter String | Select Int update action model = case action of NoOp -> model Filter query -> { model | query = query } Select index -> { model | selectedChannel = index } -- VIEW filterChannels : List String -> String -> List String filterChannels channels query = let containsCaseInsensitive str1 str2 = String . contains ( String . toLower str1 ) ( String . toLower str2 ) in List . filter ( containsCaseInsensitive query ) channels renderChannel : String -> Html renderChannel name = li [ class \" collection-item\" ] [ text <| \" #\" ++ name ] renderChannels : List String -> Html renderChannels channels = let channelItems = List . map renderChannel channels in ul [ class \" collection\" ] channelItems view : Signal . Address Action -> Model -> Html view address model = div [ class \" card-panel\" ] [ input [ onInput address Filter ] [] , renderChannels ( filterChannels model . channels model . query ) ] main : Signal Html main = StartApp . start { model = initialModel , update = update , view = view } What's Next? In the next post I will build a version of this component utilizing Messages, Effects and Ports. This will allow us to add keyboard interaction and JavaScript interop. Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-01-18"},
{"website": "Hash-Rocket", "title": "Elixir With Love", "author": ["\nMicah Woods\n\n"], "link": "https://hashrocket.com/blog/posts/elixir-with-love", "abstract": "Elixir Elixir With Love by\nMicah Woods\n\non\nMay  6, 2016 Elixir Alchemists love the pipe ( |> ) operator, and with good reason: it enables transformation of data in ways that are very expressive. These expressive declarations of code – or \"pipelines\" – can seem like a magic bullet, but let’s look at an example where the pipeline becomes cumbersome and unwieldy. Then we’ll refactor to use the new with macro introduced in Elixir 1.2, and fall in love with pipelines again. Take a look at the following code. If you've been using Elixir, you've written code like this (or been tempted to). defmodule Example . Pipeline do def transform_data do get_data_from_internet |> put_data_in_storage |> make_second_api_request |> put_data_in_storage |> transform_result end def get_data_from_internet do { :ok , :got_it } end def put_data_in_storage ({ :ok , thing_to_store }) do { :ok , thing_to_store } end def make_second_api_request ({ :ok , _value }) do { :ok , :second_request } end def transform_result ({ :ok , value }) do value end end Start a mix REPL ( iex -S mix ) and try it out. This code is great: the transform_data/0 function expresses exactly what the intention of the code is. But this pattern isn't without problems: imagine if any or all of the functions in the pipeline were to produce an error. Let’s refactor the code to produce an error in any of the functions twenty percent of the time: defmodule Example . Pipeline do def transform_data do get_data_from_internet |> put_data_in_storage |> make_second_api_request |> put_data_in_storage |> transform_result end def get_data_from_internet do random_failure ({ :ok , :got_it }, { :error , :getting_data }) end def put_data_in_storage ({ :ok , thing_to_store }) do random_failure ({ :ok , thing_to_store }, { :error , :data }) end def make_second_api_request ({ :ok , _value }) do random_failure ({ :ok , :second_request }, { :error , :second_data }) end def transform_result ({ :ok , value }) do value end defp random_failure ( pass , fail ) do if Enum . random ( 1 .. 10 ) > 2 do pass else fail end end end If you try to run this code in the REPL, you will quickly get a function clause error, explaining that no functions match the error states. In order to fix this we need to add a function head for every error state. For this simple example, this isn’t so hard, but it can become very complex if you have multiple types of errors that can occur. Here are the function heads for this simple example: defmodule Example . Pipeline do def transform_data do get_data_from_internet |> put_data_in_storage |> make_second_api_request |> put_data_in_storage |> transform_result end def get_data_from_internet do random_failure ({ :ok , :got_it }, { :error , :getting_data }) end def put_data_in_storage ({ :ok , thing_to_store }) do random_failure ({ :ok , thing_to_store }, { :error , :data }) end def put_data_in_storage ({ :error , _ } = error ) do error end def make_second_api_request ({ :ok , _value }) do random_failure ({ :ok , :second_request }, { :error , :second_data }) end def make_second_api_request ({ :error , _ } = error ) do error end def transform_result ({ :ok , value }) do value end def transform_result ({ :error , _ } = error ) do error end defp random_failure ( pass , fail ) do if Enum . random ( 1 .. 10 ) > 2 do pass else fail end end end Now if you run the code again, you get the result or the error. This code works, but once again, what if we add another type of error that can occur? Now we have to write another function head all the way down the pipeline. Thankfully Elixir has given us the with macro. Here is our refactored code using with . defmodule Example . Pipeline do def transform_data do with { :ok , value } <- get_data_from_internet , { :ok , value } <- put_data_in_storage ( value ), { :ok , value } <- make_second_api_request ( value ), { :ok , value } <- put_data_in_storage ( value ), do : transform_result ( value ) end def get_data_from_internet do random_failure ({ :ok , :got_it }, { :error , :getting_data }) end def put_data_in_storage ( thing_to_store ) do random_failure ({ :ok , thing_to_store }, { :error , :data }) end def make_second_api_request ( _value ) do random_failure ({ :ok , :second_request }, { :error , :second_data }) end def transform_result ( value ) do value end defp random_failure ( pass , fail ) do if Enum . random ( 1 .. 10 ) > 2 do pass else fail end end end This works because with tries to match the thing on the left of the <- and then continues the pipeline. When it doesn’t match the left, it stops the pipeline and returns the thing on the right. For example, if get_data_from_internet/0 returned {:error, :dont_match} , it would not match {:ok, value} . In this case the pipeline would stop and {:error, :dont_match} would immediately return. Woohoo, pipelines are back! Even if we had multiple types of errors, this code would just continue to work. Alchemists can once again write elegant, declarative code. Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-05-06"},
{"website": "Hash-Rocket", "title": "Mocking API’s with Elixir", "author": ["\nMicah Woods\n\n"], "link": "https://hashrocket.com/blog/posts/mocking-api-s-with-elixir", "abstract": "Elixir Mocking API’s with Elixir by\nMicah Woods\n\non\nMay  4, 2016 Elixir allows me to create fast, scalable applications and be productive like never before. Almost every time I think I need a tool, I discover a simple and easy way to do the same thing with just vanilla Elixir. For example, coming from the Ruby world, I instinctively want to reach for a gem to get things done. When testing I almost always reach for mock5 , which allows me to easily mock APIs with Sinatra’s elegant DSL. In the Elixir world, such tooling doesn’t exist yet – but I don’t think it’s even necessary. Let’s imagine we are writing a tool that reaches out to Github’s api and gives us information about users. Here is what the request and response might look like: $ curl -X \"GET\" \"https://api.github.com/users/mwoods79\" \\ -H \"Accept: application/vnd.github.v3+json\" { \"login\" : \"mwoods79\" , \"avatar_url\" : \"https://avatars.githubusercontent.com/u/129749?v=3\" , # Lots of other stuff } Now that we know what the response looks like, we can write a test. Our test is going to assert on a function get/1 . (This test is fictitious and doesn’t really do anything; it's just an exercise to show how easy it is to mock an API.) defmodule Example . UserTest do use ExUnit . Case alias Example . User test \" retrieving user\" do assert { :ok , response } = User . get ( \" mwoods79\" ) assert %{ \" login\" => \" mwoods79\" } = response . body assert %{ \" avatar_url\" => _ } = response . body end end Now that we have a failing test, I am going to make an abstraction around the Github API. This is going to be a base that can be used with other modules. You will see shortly that this will be the piece we replace during tests. defmodule Example . Github do use HTTPoison . Base def headers do %{ \" Content-type\" => \" application/json\" , \" Accept\" => \" application/vnd.github.v3+json\" } end def make_request ( :get , url ) do get ( url , headers ) end # Example of a POST # def make_request(:post, url, body) do #   post!(url, body, headers) # end # HTTPosion Hooks def process_url ( \" /\" <> path ), do : process_url ( path ) def process_url ( path ), do : \" https://api.github.com/\" <> path def process_response_body ( body ) do body |> Poison . decode! end end defmodule Example . User do import Example . Github , only: [ make_request: 2 ] def get ( username ) when is_binary ( username ) do make_request ( :get , \" /users/ #{ username } \" ) end end Now if we run the tests, they pass. Which is great, except that we're actually hitting the live Github API. This will slow our tests down, and we're dealing with production data. Luckily, Elixir and mix have our back. Uncomment the following from your config/config.exs : import_config \" #{ Mix . env } .exs\" Add this to a new file named config/test.exs : use Mix . Config config :example , :github_api , Example . GithubMock Create config/dev.exs and config/prod.exs and add the following: use Mix . Config config :example , :github_api , Example . Github Now modify our Example.User module to use our changes: defmodule Example . User do @github_api Application . get_env ( :example , :github_api ) def get ( username ) when is_binary ( username ) do @github_api . make_request ( :get , \" /users/ #{ username } \" ) end end That’s it! We're ready to create a mock: defmodule Example . GithubMock do def make_request ( :get , \" /users/mwoods79\" ) do { :ok , %{ body: %{ \" login\" => \" mwoods79\" , \" avatar_url\" => \" https://avatars.githubusercontent.com/u/129749?v=3\" , } } } end end Run the tests again, and you see they still pass. And if you open up a mix REPL, you can hit real data. Go ahead try it. That’s it! There's no new DSL to learn and no extra dependency in your app – just Elixir and productivity. Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-05-04"},
{"website": "Hash-Rocket", "title": "Ignore specific files only on current machine in Git", "author": ["\nDorian Karter\n\n"], "link": "https://hashrocket.com/blog/posts/ignore-specific-file-changes-only-on-current-machine-in-git", "abstract": "Ignore specific files only on current machine in Git by\nDorian Karter\n\non\nJune  9, 2016 If you ever edit versioned files such as configuration files that are usually shared but require a slight modification for your dev machine, you should consider excluding those files locally. If you edit .gitignore the files in question will be ignored on every dev machine. You probably do not want that. Instead consider the following: First add the files in question to the file at .git/info/exclude . This file is formatted just like .gitignore . Example: # git ls-files --others --exclude-from=.git/info/exclude\n# Lines that start with '#' are comments.\n# For a project mostly in C, the following would be a good set of\n# exclude patterns (uncomment them if you want to use them):\n# *.[oa]\n# *~\n\nconfig/database.yml If your file already contains unstaged changes you may need to run: git update-index --skip-worktree [ <file>...] Some online sources will tell you to use this command: git update-index --assume-unchanged [<file>...] Instead I recommend using --skip-worktree . The difference between the two is well summarized here : --assume-unchanged assumes that a developer should not change a file. This flag is meant for improving git's performance for non-changing folders like SDKs. This allows git to skip these folders when checking which files changed on the local machine. --skip-worktree is useful when you instruct git not to touch a specific file ever because developers are likely to change if. For example, if the main repository upstream hosts some production-ready configuration files and you don’t want to accidentally commit changes to those files, --skip-worktree is exactly what you want. Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-06-09"},
{"website": "Hash-Rocket", "title": "Debugging Action Callbacks (aka Filters) in Rails", "author": ["\nDorian Karter\n\n"], "link": "https://hashrocket.com/blog/posts/debugging-action-callbacks-aka-filters-in-rails", "abstract": "Ruby Debugging Action Callbacks (aka Filters) in Rails by\nDorian Karter\n\non\nJuly 28, 2016 Rails provides before and after actions in controllers as an easy way to call methods before or after executing controller actions as response to route requests.\nAction Callbacks can be particularly helpful when implementing authentication/authorization for example, and are heavily used by gems such as Devise . Unfortunately this feature can be awfully painful to debug. With many callbacks affecting your controllers this can be a rather arduous process to track down a nasty bug. In this post I will focus on before_action for the sake of convenience, but the same technique can be applied to debug after_action and around_action . Setting Up a Starting Point Start by creating an anonymous before_action and inserting a pry statement into it. class SomeController < ApplicationController before_action :authenticate_user! end Turns into: class SomeController < ApplicationController before_action do require 'pry' ; binding . pry authenticate_user! end end This is a good starting point for getting you exploring variables in the pry debugger. In fact, if you had defined authenticate_user! yourself you can just drop a binding.pry or puts directly in there without messing with the before actions. But reality is far from ideal, and with all the layers of abstraction in Rails, various gems, inheritance and mixins, we often have to deal with a list of before actions happening without our control or knowledge. We are forced unwillingly to deep-dive into heavily abstracted, meta-programmed gems to debug a problem. We often encounter this type of situation in apps we get from clients that use Devise. This is usually the fault of developers getting creative by overriding or worse so, monkey-patching a piece of Devise which causes a bug that is tightly related to Devise's code. If you are dealing with such a problem (callbacks are not home brewed) and not sure where to start try using prepend_before_action and putting it at the bottom of your controller. This will ensure this callback will run first. prepend_before_action do require 'pry' ; binding . pry true end Listing All Active Action Callbacks So we hit the controller action using the request to the route pointing at it and now we are in a pry session. The first thing you need to know about action callbacks is that they are stored behind the scenes in in a data structure called _process_action_callbacks . If you just write _process_action_callbacks in pry you will get a whole lot of information about all the action callbacks registered for the current request context: [ 1 ] pry ( #<SalesDashboardController>)> _process_action_callbacks => [ #<ActiveSupport::Callbacks::Callback:0x007f95671589a8 @callback_id = 36 , @chain = [ #<ActiveSupport::Callbacks::Callback:0x007f95671589a8 ...>, #<ActiveSupport::Callbacks::Callback:0x007f9567153160 @callback_id = 34 , @chain = [ ... ], @compiled_options = [], @filter = :prepare_session , @kind = :before , @klass = ApplicationController , @options = { :prepend => true , :if => [], :unless => []}, @per_key = { :if => [], :unless => []}, @raw_filter = :pr! []( https :/ / i . imgur . com / 3 hpeXMz . jpg ) epare_session > , #<ActiveSupport::Callbacks::Callback:0x007f956db44240 @callback_id = 32 , @chain = [ ... ], @compiled_options = [], @filter = :clear_search_path , @kind = :before , @klass = ApplicationController , @options = { :prepend => true , :if => [], :unless => []}, @per_key = { :if => [], :unless => []}, @raw_filter = :clear_search_path > , #<ActiveSupport::Callbacks::Callback:0x007f956de429d8 @callback_id = 30 , @chain = [ ... ], @compiled_options = [], @filter = :authenticate_user! , @kind = :before , @klass = ApplicationController , @options = { :prepend => true , :if => [], :unless => []}, @per_key = { :if => [], :unless => []}, @raw_filter = :authenticate_user! > , #<ActiveSupport::Callbacks::Callback:0x007f9564ef6428 @callback_id = 28 , @chain = [ ... ], @compiled_options = [], @filter = :set_search_path , @kind = :before , @klass = ApplicationController , @options = { :prepend => true , :if => [], :unless => []}, @per_key = { :if => [], :unless => []}, @raw_filter = :set_search_path > , # .... That's a lot of information that we can use some ruby functions to filter (no pun intended) to just what we need. For example if we want to see the names of all the :before filters we can use the following query: _process_action_callbacks . map { | c | c . filter if c . kind == :before } Which yields something like this: [ 13 ] pry ( #<SalesDashboardController>)> _process_action_callbacks.map {|c| c.filter if c.kind == :before } => [ \"_callback_before_35\" , :prepare_session , :clear_search_path , :authenticate_user! , :set_search_path , :set_session_settings , :verify_authenticity_token , :handle_password_change , :set_paper_trail_whodunnit , :set_paper_trail_controller_info , :set_paper_trail_enabled_for_controller , \"_callback_before_37\" , \"_callback_before_39\" , \"_callback_before_41\" , nil ] This command makes the callback hash more manageable and understandable, however it hides important details. If there are conditions attached to the filter such as only , except , if or unless . Also don't forget about prepend which affects the order of execution. These details are stored in options . A small tweak can be added to our query to improve the results: _process_action_callbacks . map { | c | { c . filter => c . options } if c . kind == :before } Now we get something that looks like this: [ 14 ] pry ( #<SalesDashboardController>)> _process_action_callbacks.map {|c| {c.filter => c.options} if c.kind == :before } => [{ \"_callback_before_35\" => { :prepend => true , :if => [], :unless => []}}, { :prepare_session => { :prepend => true , :if => [], :unless => []}}, { :clear_search_path => { :prepend => true , :if => [], :unless => []}}, { :authenticate_user! => { :prepend => true , :if => [], :unless => []}}, { :set_search_path => { :prepend => true , :if => [], :unless => []}}, { :set_session_settings => { :prepend => true , :if => [], :unless => []}}, { :verify_authenticity_token => { :prepend => true , :if => [], :unless => []}}, { :handle_password_change => { :if => [], :unless => []}}, { :set_paper_trail_whodunnit => { :if => [], :unless => []}}, { :set_paper_trail_controller_info => { :if => [], :unless => []}}, { :set_paper_trail_enabled_for_controller => { :if => [], :unless => []}}, { \"_callback_before_37\" => { :if => [], :unless => [], :only => [ :destroy ]}}, { \"_callback_before_39\" => { :if => [], :unless => []}}, { \"_callback_before_41\" => { :if => [], :unless => []}}, nil ] This improved output tells us that anonymous callback _callback_before_37 will only execute on the :destroy action. Good to know it's not relevant. Debugging All Action Callbacks As you may have noticed a lot of callbacks are coming in from gems which we have included in our project. Say one of them is causing a silent error with no stack trace printed anywhere, we could use :Btabedit in Vim which is available to us from the excellent vim-bundler Vim plugin by Tim Pope. If we go with that approach it may prove cumbersome and tedious to go through each gem. Inspecting the Source of Action Callbacks We can easily inspect the source of potentially suspect callbacks by using the show-source method in pry. [ 18 ] pry ( #<SalesDashboardController>)> show-source authenticate_user! From : /Users/ dev / . rvm / gems / ruby - 1.9 . 3 - p448 / gems / devise - 1.5 . 3 / lib / devise / controllers / helpers . rb @ line 46 : Owner : Devise :: Controllers :: Helpers Visibility : public Number of lines: 4 def authenticate_ #{mapping}!(opts={}) opts [ :scope ] = : #{mapping} warden . authenticate! ( opts ) if ! devise_controller? || opts . delete ( :force ) end Inserting a Pry Debugger Into Action Callbacks Similar to show-source , Pry also offers the edit command which can be used for opening the file where the method is defined in our default editor: [ 18 ] pry ( #<SalesDashboardController>)> edit authenticate_user! In my case this opens up Vim and puts me right where the method is defined, even if it was defined using meta-programming with class_eval (granted FILE and LINE were used). e.g.: 42 def self . define_helpers ( mapping ) #:nodoc: 43 mapping = mapping . name 44 45 class_eval <<- METHODS , __FILE__ , __LINE__ + 1 # CURSOR WILL BE ON LINE BELOW\n 46           def authenticate_ #{ mapping } !(opts={})\n 47             opts[:scope] = : #{ mapping } 48             warden.authenticate!(opts) if !devise_controller? || opts.delete(:force)\n 49           end\n 50\n 51           def #{ mapping } _signed_in?\n 52             !!current_ #{ mapping } 53           end This allows us to quickly edit and save the gem file and insert a binding.pry so that we can debug that specific method. Another option we have is monkey-patching the callback name being called in memory: [ 21 ] pry ( #<SalesDashboardController>)> authenticate_user! => #<User id: 332, email: \"dev@hashrocket.com\", encrypted_password: \"xxx\", reset_password_token: nil, reset_password_sent_at: nil, sign_in_count: 185, current_sign_in_at: \"2016-07-15 15:32:22\", last_sign_in_at: \"2016-07-13 21:12:26\", current_sign_in_ip: \"127.0.0.1\", last_sign_in_ip: \"127.0.0.1\", created_at: \"2015-10-01 19:39:13\", updated_at: \"2016-07-15 15:32:22\", first_name: \"Hashrocket\", last_name: \"Hashrocket\", job_title: \"\", role: nil, location: \"\", group: nil, role_id: 9, company_code: \"test\", phone: \"\", password_changed_at: \"2016-07-13 21:12:13\", remember_token: nil, remember_created_at: nil, branch_id: nil, deactivated: false> [ 22 ] pry ( #<SalesDashboardController>)> def authenticate_user! [ 22 ] pry ( #<SalesDashboardController>)*   puts \":::::::::::::::::DEBUG => AUTHENTICATING USER\" [ 22 ] pry ( #<SalesDashboardController>)*   super [ 22 ] pry ( #<SalesDashboardController>)* end => nil [ 23 ] pry ( #<SalesDashboardController>)> authenticate_user! :::::::::::::::: :DEBUG => AUTHENTICATING USER => #<User id: 332, email: \"dev@hashrocket.com\", encrypted_password: \"xxx\", reset_password_token: nil, reset_password_sent_at: nil, sign_in_count: 185, current_sign_in_at: \"2016-07-15 15:32:22\", last_sign_in_at: \"2016-07-13 21:12:26\", current_sign_in_ip: \"127.0.0.1\", last_sign_in_ip: \"127.0.0.1\", created_at: \"2015-10-01 19:39:13\", updated_at: \"2016-07-15 15:32:22\", first_name: \"Hashrocket\", last_name: \"Hashrocket\", job_title: \"Rocketeer\", role: nil, location: \"\", group: nil, role_id: 9, company_code: \"test\", phone: \"\", password_changed_at: \"2016-07-13 21:12:13\", remember_token: nil, remember_created_at: nil, branch_id: nil, deactivated: false> [ 24 ] pry ( #<SalesDashboardController>)> You can also define those monkey-patched methods in your controller if you think you will have multiple debug cycles and insert prys in all of them. If you would like to do that for all action callbacks you can use the list you got in a previous step with some meta-programming wizardry: [ \"_callback_before_35\" , :prepare_session , :clear_search_path , :authenticate_user! , :set_search_path , :set_session_settings , :verify_authenticity_token , :handle_password_change , :set_paper_trail_whodunnit , :set_paper_trail_controller_info , :set_paper_trail_enabled_for_controller , \"_callback_before_37\" , \"_callback_before_39\" , \"_callback_before_41\" , ]. each do | cb | define_method ( cb ) { require 'pry' ; binding . pry ; super } end Finally, if you want to automate the process you can combine some pieces from the code above into this: _process_action_callbacks . each do | callback | define_method ( callback . filter ) do puts \"Running before_action: #{ c . filter } \" require 'pry' ; binding . pry ; super end if callback . kind == :before end Now you can use something like pry-byebug to step through your code and figure out the issue. I hope this post was helpful in solving your action callback bugs and added some new tricks to your arsenal. Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-07-28"},
{"website": "Hash-Rocket", "title": "Living it up in the mountains", "author": ["\nBrandon Farmer\n\n"], "link": "https://hashrocket.com/blog/posts/living-it-up-in-the-mountains", "abstract": "Community Living it up in the mountains by\nBrandon Farmer\n\non\nOctober  1, 2012 Laying in the middle of Colorado is a small town with a large Ruby community and an amazing conference. The town is Boulder and the conference is Rocky Mountain Ruby. I was fortunate enough to be able to attend this year with my fellow rocketeer Paul Elliott, and was also part of the crew of rocketeers giving a workshop over the Hashrocket way during the pre-conference activities. This was my first Ruby conference, and I was in for a treat. There was an awesome group of presenters this year, starting with Sandi Metz giving her \"Go ahead, Make a mess\" talk over the importance of well-timed design, and ending with Russ Olsen 's \"Eloquent Explainations\" covering better methods for training the next generation of programmers entering the ruby community. One of my favorite talks was Ben Orenstein’s live coding session where he paired with the entire audience to refactor ruby code. I thought it was an incredible way to involve everyone and at one point, I even learned an interesting fact about ruby arrays from a fellow attendee. Rocky Mountain Ruby doesn't just have great speakers--the two-day conference was also full of events after hours. We started with a great speakers' dinner Tuesday night at Pizzeria Locale. This was a great networking experience where I got to mingle with all the speakers as well as some recent graduates from DaVinci Coders , including Elaine from the \"Growing Developers\" panel. The next evening started with a reception at the Oak where all attendees met for drinks. During the reception, I did sneak off to take a look at the Arduino hackfest going on at QuickLeft, and was throughly impressed by the number of teams participating and the amount of hardware being supplied by Sparkfun. The night ended with an excellent dinner with Sandi Metz and Russell Olsen at the Rio, and tequila tasting at the Oak. The second night of the conference ended with an after party at Pivotal Labs just off Pearl Street. While there, I got a very interesting tour of downtown Boulder aboard the Handlebar--a 16-person bicycle/bar. We pedaled our way down the streets and stopped at several bars along the way. After biking around town, we took a relaxing tour of the Avery Brewery on the outskirts of Boulder aboard the Banjo tour bus. Overall, the party was one of the best social events of the conference. I think that the end of the conference may have been my favorite part because we ended the day with an 8-mile hike on the Sanitas Trail. I have to say that the view from the top was well worth the hike. After the hike and a small break, we made our way back to Quickleft for one final gathering with all of the speakers and attendees to talk about everything that happened, and what we were inspired to do in our own work. I think that I returned from this conference with a lot of information about a wide variety of topics, but most importantly, I have a better grasp of what it means to be a part of the Ruby community. Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-10-01"},
{"website": "Hash-Rocket", "title": "Elixir Streams for Lazy Evaluation", "author": ["\nJohnny Winn\n\n"], "link": "https://hashrocket.com/blog/posts/elixir-streams-for-lazy-evaluation", "abstract": "Elixir Elixir Streams for Lazy Evaluation by\nJohnny Winn\n\non\nMarch 13, 2014 Lazy evaluation is a great way to delay the execution of larger datasets. In a typical enumeration each item is evaluated one-by-one. This isn't a problem with smaller sets but as those sets get larger the amount time to process grows exponentially. Every function has to evaluate the entire set before the next function can execute. Elixir's Stream module allows us to compose our enumerations prior to execution. Let's look at some examples. First will take a look at using the Enum module to filter a range of numbers and take from the list. iex ( 1 ) > Enum . filter ( 1 .. 10000000000000000 , & ( rem ( &1 , 3 ) == 0 || rem ( &1 , 5 ) == 0 )) |> Enum . take 5 Wait for it... No really because it's going to take awhile. See, the Enum.filter function has to complete its iteration over the entire set before it can pipe the filtered list to the take function. However, if we use the Stream module we can compose the computation prior to executing. iex ( 1 ) > Stream . filter ( 1 .. 10000000000000000 , & ( rem ( &1 , 3 ) == 0 || rem ( &1 , 5 ) == 0 )) |> Enum . take 5 [ 3 , 5 , 6 , 9 , 10 ] Chaining Streams When you compose a chain of streams it's easy to see how the execution differs from the typical chain of Enum functions. We can borrow the next example from the Stream module docs. First we look at an example with piping the Enum functions together. You will note that each map is applied in order so first all the numbers are printed then they are doubled and printed. 1 .. 3 |> Enum . map ( & IO . inspect ( &1 )) |> Enum . map ( & ( &1 * 2 )) |> Enum . map ( & IO . inspect ( &1 )) 1 2 3 2 4 6 #=> [2,4,6] Now we can look at the same example composed with Streams. stream = 1 .. 3 |> Stream . map ( & IO . inspect ( &1 )) |> Stream . map ( & ( &1 * 2 )) |> Stream . map ( & IO . inspect ( &1 )) Enum . to_list ( stream ) 1 2 2 4 3 6 #=> [2,4,6] Now we see that the each number is completely evaluated before moving to the next number in the enumeration. That's just a quick look at the power of streams in Elixir for lazy evaluation. Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-03-13"},
{"website": "Hash-Rocket", "title": "Commit Lint for Danger", "author": ["\nJon Allured\n\n"], "link": "https://hashrocket.com/blog/posts/commit-lint-for-danger", "abstract": "Ruby Commit Lint for Danger by\nJon Allured\n\non\nSeptember  6, 2016 I love using Danger to automate routine Pull Request feedback and so I\nmade a plugin that lints commit messages . It was an interesting\nprocess and I wanted to share some thoughts about it. Commit Guidelines I tend to follow Tim Pope's ideas about commit messages. To boil down a\ngreat post into something Danger could automate, I started with these three\nrules: Message subject should be no longer than 50 characters Message subject should not end in a period Message subject and body should be separated by an empty line Using the plugin After having setup Danger , simply add this line to your Dangerfile: commit_lint . check Additionally, you may want to configure the plugin . Maybe you'd rather\nwarn instead of fail the PR, you can do that like this: commit_lint . check warn: :all Or maybe you don't care about subjects ending in a period, you can disable a\nparticular check like this: commit_lint . check disable: [ :subject_period ] Initial Code This plugin started life as just a few lines in the Dangerfile of\nRubyConferences.org : # not included here is the error_messages hash # but this is the interesting part ;) for commit in git . commits ( subject , empty_line , * body ) = commit . message . split ( \" \\n \" ) fail error_messages [ 6 ] if subject . length > 50 fail error_messages [ 7 ] if subject . split ( '' ). last == '.' fail error_messages [ 8 ] if empty_line && empty_line . length > 0 end It was a great start and actually caught a couple mistakes! Plugin Extraction As I worked on extracting the plugin, I realized that there were things I'd want\nto do, like the configuration I mentioned above. I also was able to get tests\naround the plugin's behavior, which (surprise!) found bugs and helped improve\nthe code quite a bit. A Danger plugin is simply a Ruby class that inherits from Danger::Plugin and\nexposes some public methods. In my case, I wrote a class called Danger::DangerCommitLint and exposed a check method: module Danger class DangerCommitLint < Plugin NOOP_MESSAGE = 'All checks were disabled, nothing to do.' . freeze def check ( config = {}) @config = config if all_checks_disabled? warn NOOP_MESSAGE else check_messages end end end end At a high-level that's about it - we take in some config, ensure there is at\nleast one check to perform and then perform those checks. Pretty small public\ninterface to test, right?? Testing Danger Plugins The plugin template includes a spec helper that provides you a Dangerfile\ncontext and you simply grab a reference to your plugin and then call methods on\nit: commit_lint = testing_dangerfile . commit_lint commit_lint . check Probably the easiest way to test your plugins is by asserting about their status_report , a hash of :errors , :warnings , :messages and :markdowns .\nI wrote a little helper to assert about the counts: def report_counts ( status_report ) status_report . values . flatten . count end I've got a constant with various test messages and then I wrote a bunch of\nintegration-style tests like this: describe 'check without configuration' do context 'with all errors' do it 'fails every check' do commit_lint = testing_dangerfile . commit_lint commit = double ( :commit , message: TEST_MESSAGES [ :all_errors ]) allow ( commit_lint . git ). to receive ( :commits ). and_return ([ commit ]) commit_lint . check status_report = commit_lint . status_report expect ( report_counts ( status_report )). to eq 3 expect ( status_report [ :errors ]). to eq [ SubjectLengthCheck :: MESSAGE , SubjectPeriodCheck :: MESSAGE , EmptyLineCheck :: MESSAGE ] end end end We use the :all_errors test message to create a double and stub out the git\ncommits with it. Then, we run our checks and ensure that both the error count\nand particular error messages match our expectations. Easy! Check Classes I like classes, so it wasn't long before I was extracting those simple lines in\nthe initial implementation into classes that I could use to check the commits.\nHere's the superclass I came up with and then the checker for subject length: module Danger class DangerCommitLint < Plugin class CommitCheck # :nodoc: def self . fail? ( message ) new ( message ). fail? end def initialize ( message ); end def fail? raise 'implement in subclass' end end end end module Danger class DangerCommitLint < Plugin class SubjectLengthCheck < CommitCheck # :nodoc: MESSAGE = 'Please limit commit subject line to 50 characters.' . freeze def self . type :subject_length end def initialize ( message ) @subject = message [ :subject ] end def fail? @subject . length > 50 end end end end All the superclass really does is provide a class method that instantiates and\nthen calls that fail? method. I really like this pattern and use it all the\ntime on simple classes like this. More on this in a bit. The main job of the SubjectLengthCheck class is to implement that fail? method and provide both a MESSAGE and type . The former gets sent to the user\nwhen this check fails and the latter is used to map the config symbols to\nchecker classes. The Private Parts I sorta avoided the details when showing the DangerCommitLint class above, but\nI wanted to lay some groundwork first. Let's look at the private parts of that\nfile: module Danger class DangerCommitLint < Plugin # public stuff ... private def check_messages for message in messages for klass in warning_checkers messaging . warn klass :: MESSAGE if klass . fail? message end for klass in failing_checkers messaging . fail klass :: MESSAGE if klass . fail? message end end end def checkers [ SubjectLengthCheck , SubjectPeriodCheck , EmptyLineCheck ] end def checks checkers . map ( & :type ) end def enabled_checkers checkers . reject { | klass | disabled_checks . include? klass . type } end def warning_checkers enabled_checkers . select { | klass | warning_checks . include? klass . type } end def failing_checkers enabled_checkers - warning_checkers end def all_checks_disabled? @config [ :disable ] == :all || disabled_checks . count == checkers . count end def disabled_checks @config [ :disable ] || [] end def warning_checks return checks if @config [ :warn ] == :all @config [ :warn ] || [] end def messages git . commits . map do | commit | ( subject , empty_line ) = commit . message . split ( \" \\n \" ) { subject: subject , empty_line: empty_line } end end end end An Array of classes?? Sure, this is Ruby, we can do whatever we want! Our public interface simply ensures all_checks_disabled? return false and then\ncalls check_messages . That method then iterates over the commits in the PR and\nruns whatever checks are supposed to be run. To determine which checks should be run we get to think about classes and their\ntypes (symbols). We've abstracted this code to the point where adding a new\ncheck is as easy as adding an item to that checkers array - nice! Remember that superclass and the class-level fail? method? That's what we're\nusing so that we can write this super cute line: messaging . warn klass :: MESSAGE if klass . fail? message Rubocop I had never used Rubocop before, but the plugin template sets it up\nfor you, so I thought I'd give it a shot. I was really enjoying SwiftLint in my current work project, so I figured it would be easy\nto get setup and configured the way I like things . Boy was my initial code bad! I got warnings about things like cyclomatic complexity , perceived\ncomplexity and ABC Metric . Huh? What am I, a computer\nscientist?? I had tons of work to do to make Rubocop happy, but it was worth it. I feel like\nI ended up with a very readable codebase and some of that is because of\nRubocop's nudges. I initially had some pain because the default template uses fail and that was\ncausing a Rubocop warning. Once I learned that you can use messaging.fail instead, I was able to remove the comment that disabled the warning. Conclusion What started as just a few lines of code ended up cloc ing in at around\n450 lines, so that's pretty good, right?? But seriously, I have to give a big thanks to Orta and Felix for creating Danger, I think it's a really great tool and I hope to use it on\nmany future projects! Also: check out this VISION.md file - sick,\nright?? I had a lot of fun extracting this plugin and working on improving the code\nuntil not only the tests passed, but the Rubocop and documentation checks also\npassed. And I even got it on the official plugin list !! Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-09-06"},
{"website": "Hash-Rocket", "title": "Working with Email Addresses in PostgreSQL", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/working-with-email-addresses-in-postgresql", "abstract": "PostgreSQL Working with Email Addresses in PostgreSQL by\nJosh Branchaud\n\non\nMay  2, 2016 Most of the data we work with day to day is not case-insensitive. For the\ndata that is though, we need to check our assumptions. Is our database\nenforcing the constraints we think? Do we understand where our indexes are\nworking for us and, more importantly, where they are not? The more verbose title for this post would read, \"Working With Email\nAddresses And Other Case-Insensitive Data In PostgreSQL\". This is to say\nthat while the rest of this post will focus on the email address example,\nthe concepts will generally apply to anything you consider to be\ncase-insensitive. When working with email addresses from a database perspective, there are a few\nthings I'd like to keep in mind. First, an email address is generally used to identify a user. Because of this, I'd\nlike the database to ensure the uniqueness of stored email addresses. Second, email addresses should be treated as case-insensitive. JACK@nbc.com and jack@nbc.com should be handled as the same email address. This is important for\nscenarios like a user logging in where I cannot assume how they will\ncapitalize their email address. Third, the email column in my database will be accessed frequently. For\ninstance, every time someone signs in, I find the user record by email\naddress and then verify the given password. The database should be able to\ndo lookups on the email column efficiently. In brief, I'd like the database to provide efficient lookups of a unique email column. Setup With that in mind, let's do some initial setup. create table users ( id serial primary key , email text not null unique , password_digest text not null ); insert into users ( email , password_digest ) select 'person' || num || '@example.com' , md5 ( 'password' || num ) from generate_series ( 1 , 10000 ) as num ; This abbreviated table is representative of what I usually see for users tables storing email addresses 1 . Here is a look at the table's description: \\ d users Table \"public.users\" Column | Type | Modifiers -----------------+---------+---------------------------------------------------- id | integer | not null default nextval ( 'users_id_seq' :: regclass ) email | text | not null password_digest | text | not null Indexes : \"users_pkey\" PRIMARY KEY , btree ( id ) \"users_email_key\" UNIQUE CONSTRAINT , btree ( email ) The email column has a unique constraint, which should cover our unique\nemail addresses requirement, with a btree index, that will help with efficient\nlookups. That should cover it. The Catch This post isn't quite over though because, unfortunately, it isn't that\neasy. First, we aren't really preventing duplicates. The first record in the table\nis person1@example.com . I can easily insert another record with a\nduplicate email address: insert into users ( email , password_digest ) values ( 'Person1@example.com' , md5 ( 'password' )); And inserted. All I had to do was change the casing. The unique constraint is only constraining the email column to a\ncase-sensitive uniqueness. For email addresses, person1@example.com and Person1@example.com should be treated as identical. These kinds of issues are especially likely if we are working\nwith an ORM , such\nas the one provided by ActiveRecord .\nThough ORMs provide lots of convenience, they can hide the important details\nof the database and ultimately lead us to believe our database schema is\ntighter than it actually is 2 . We'll have to update our constraint, but first let's explore the lookup\nefficiency. We don't care if the user enters person1@example.com or PERSON1@EXAMPLE.COM when signing in, we want our application to handle it\nthe same. This is key to providing consistent lookup of user records. We\nneed a way to look at email addresses in a case-insensitive way. Let's use PostgreSQL's lower() function for that. select * from users where lower ( email ) = lower ( 'PERSON5000@EXAMPLE.COM' ); --   id  |         email          |         password_digest -- ------+------------------------+---------------------------------- --  5000 | person5000@example.com | 81aa45be581a3b21e6ff4da69b8c5a15 Despite the casing not matching, we are able to find the record. If we look\nat the explain analyze output, though, we'll see a problem. explain analyze select * from users where lower ( email ) = lower ( 'PERSON5000@EXAMPLE.COM' ); --                                              QUERY PLAN -- ---------------------------------------------------------------------------------------------------- --  Seq Scan on users  (cost=0.00..264.00 rows=50 width=59) (actual time=5.784..11.176 rows=1 loops=1) --    Filter: (lower(email) = 'person5000@example.com'::text) --    Rows Removed by Filter: 10000 --  Planning time: 0.108 ms --  Execution time: 11.243 ms Postgres ends up doing a sequential scan of the users table. In other\nwords, it is not using the users_email_key index. In order to provide a\nconsistent lookup via the lower() function, we have lost the speed\nbenefits of our index. We've gone from feeling pretty good about our index to realizing that it\nboth allows duplicates and cannot provide both consistent and efficient\nlookups on the email column. We can remedy this, though. A Better Index We plan to almost exclusively do user email address lookups in conjunction\nwith the lower() function. So, what we need is a functional index; one\nthat indexes the email column with lower() . If we also make this a\nunique index, then we will have constrained the email column to email\naddresses that are unique after the lower() function has been applied to\nthem 3 . This will solve both of our problems. Let's add it. We still have that duplicate record ( Person1@example.com ), so the first\nthing we want to do is clean that up (and anything else that will violate\nthe upcoming index). delete from users where email = 'Person1@example.com' ; We can then use the create index command to add this better index: create unique index users_unique_lower_email_idx on users ( lower ( email )); Because the index uses the lower() function, we call it a functional\nindex. We can see the new index by taking another look at the users table's\ndescription: \\ d users Table \"public.users\" Column | Type | Modifiers -----------------+---------+---------------------------------------------------- id | integer | not null default nextval ( 'users_id_seq' :: regclass ) email | text | not null password_digest | text | not null Indexes : \"users_pkey\" PRIMARY KEY , btree ( id ) \"users_email_key\" UNIQUE CONSTRAINT , btree ( email ) \"users_unique_lower_email_idx\" UNIQUE , btree ( lower ( email )) If we try to insert a duplicate record like we did earlier, Postgres will\nthrow up a red flag. insert into users ( email , password_digest ) values ( 'Person1@example.com' , md5 ( 'password' )); -- ERROR:  duplicate key value violates unique constraint \"users_unique_lower_email_idx\" -- DETAIL:  Key (lower(email))=(person1@example.com) already exists. Fantastic. We can also use explain analyze again to get some insight into the\nperformance with our new index: explain analyze select * from users where lower ( email ) = lower ( 'PERSON5000@example.com' ); --                                                              QUERY PLAN -- ------------------------------------------------------------------------------------------------------------------------------------- --  Index Scan using users_unique_lower_email_idx on users  (cost=0.29..8.30 rows=1 width=59) (actual time=0.051..0.052 rows=1 loops=1) --    Index Cond: (lower(email) = 'person5000@example.com'::text) --  Planning time: 0.134 ms --  Execution time: 0.082 ms The part to focus on is the first line under QUERY PLAN . Earlier this same\nquery necessitated a full sequential scan. Now, Postgres is able to do an Index Scan using users_unique_lower_email_idx . This gives us performant,\nconsistent lookups 4 . Another Approach The citext module gives us\nanother approach to this issue of handling case-insensitive data. The citext module provides a case-insensitive character string type,\ncitext. Essentially, it internally calls lower when comparing values.\nOtherwise, it behaves almost exactly like text. By declaring our email column with the citext type instead of text or varchar , we get the same benefits as the previous section without the\nadditional index. Taking the citext approach, we would create a table like the following: create extension citext ; create table users ( id serial primary key , email citext not null unique , password_digest varchar not null ); \\ d users Table \"public.users\" Column | Type | Modifiers -----------------+-------------------+---------------------------------------------------- id | integer | not null default nextval ( 'users_id_seq' :: regclass ) email | citext | not null password_digest | character varying | not null Indexes : \"users_pkey\" PRIMARY KEY , btree ( id ) \"users_email_key\" UNIQUE CONSTRAINT , btree ( email ) There are a few benefits to using citext . We have less hoops to jump\nthrough because we no longer need to setup an additional index and we don't\nhave to include lower() on both sides of every comparison. We also get some performance gain on writes by reducing the number of indexes we have\nto update by one. The main benefit for me is being able to write a case-insensitive lookup\nlike we would any other statement: select * from users where email = 'PERSON5000@example.com' ; id | email | password_digest ------+------------------------+---------------------------------- 5000 | person5000 @ example . com | 81 aa45be581a3b21e6ff4da69b8c5a15 And if you are to check the explain analyze , you'll see that it is able to\nperform an index scan 5 . In Summary When it comes to case-insensitive data like email addresses, you now\nunderstand how to enforce uniqueness and get efficient lookups. Perhaps you\nare even considering moving from text to citext . I hope you are also a\nbit more wary of what your application framework's ORM is doing for you as\nwell as what database-level details it is hiding from you. I assume some level of ubiquitousness because this is the column\ndefinition and index provided by the Devise gem. ↩ You may have application-level uniqueness validations (a la Devise),\nbut I'd argue that's not good enough. Your database should have the\nfinal say when it comes to preventing invalid and inconsistent data.\nThis is an especially important point in a tech landscape that is often\nembracing microservices and multiple application clients (i.e. web, iOS,\nandroid). ↩ Why don't we just lowercase all email addresses before inserting them\nor querying for them? Well, that is yet another thing we have to remember\nto do at every access point to our database. It's easier to let an index\ndo it.  Less cognitive overhead as well. ↩ I am going to leave our old index ( users_email_key ) on the table.\nThough I expect the lower(email) -style query to be most common, I still\nassume there will be the occasional query on email by itself. I'd still\nlike the benefits of the query. I can tolerate the very minor overhead of\nmaintaining two indexes on a read-heavy table. ↩ There are some limitations to the citext module that you may never\nrun up against, but are worth knowing about. They are detailed in the Limitations\nsection of the\ndocs for citext . ↩ Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-05-02"},
{"website": "Hash-Rocket", "title": "SQL Window Functions and You", "author": ["\nJoshua Davey\n\n"], "link": "https://hashrocket.com/blog/posts/sql-window-functions", "abstract": "Ruby PostgreSQL SQL Window Functions and You by\nJoshua Davey\n\non\nMay 22, 2012 Suppose you have a storefront application that sells pictures of cats.\nThese cat pictures are categorized in meaningful ways. For example,\nthere are LOLcats pictures and \"Classic\" cat pictures. Now, on the\nlanding page of the store, you'd like to feature one picture from each\ncategory. It can't be a random picture from each. You need to feature\nthe cheapest picture from each category, displaying its name and price. Also, it turns out that some \"low\" prices are very common. For example,\n$9.99 is a common sale price for LOLcats pictures. However, we should\nonly ever feature one picture per category. When there are multiple\npictures with the same low price, we fallback to the name, and show the\nfirst one alphabetically. How can we solve this problem, while also\nremaining performant? As an aside, adding a cat to a Rennaisance painting amplifies its appeal\nninefold. Let's look at some of the ways that we can approach this problem,\ndisplaying a list of cat pictures that are the cheapest for their\nrespective category. Approach 1: Ruby Implementing the solution in Ruby is fairly straightforward. ActiveSupport Enumerable provides the group_by and sort_by methods on\ncollections, and we can use those to help us cut down on some typing. class CatPicture < ActiveRecord :: Base attr_accessible :category_id , :description , :name , :price belongs_to :category def self . cheapest_per_category all . group_by ( & :category_id ). map do | category_id , subset | subset . sort_by { | pic | [ pic . price , pic . name ] }. first end end end First, we group all of the cat pictures by their category. Then, for\neach set of pictures, we sort them by their price and name, and take\nonly the first one. Perhaps you are wondering if inverting the responsibility would improve\nthe implementation, putting the mapping and reduction impetus in the\nCategory model instead. Although it would be possible to go through\nthe Category model to find its cheapest picture, that would lead to an\n\"n+1\", as each category would subsequently need fetch its cat pictures.\nAlternatively, eager-loading all categories with their cat pictures\nwould be expensive, and would essentially duplicate what we've done\nabove with the group_by . Either way, as you can probably imagine, the above method would become\nmore expensive as the data set continued to grow. Additionally, we lose\nthe ability to continue to chain ActiveRecord scopes to filter the set\nfurther: as soon as we fetch the collection from the database, all\nfiltering has to be done in Ruby. Pros: Easy to grok All domain logic stays in application Cons: Expensive (all objects loaded into memory) No scope chaining Once you go Ruby, you don't go back Approach 2: SQL subselects We can improve performance by doing the filtering at the database level,\nrather than loading all cat pictures into memory each time. class CatPicture < ActiveRecord :: Base attr_accessible :category_id , :description , :name , :price belongs_to :category def self . cheapest_per_category find_by_sql <<- SQL SELECT DISTINCT ON(category_id) cat_pictures.*\n      FROM cat_pictures\n      WHERE ((category_id, price) IN (\n        SELECT category_id, min(price)\n        FROM cat_pictures\n        GROUP BY category_id\n      ))\n      ORDER BY category_id ASC, cat_pictures.name ASC SQL end end Here, we use a subselect to filter the initial set down to only those\nthat have the cheapest price per category. In this inner query, each row\nwill contain a category_id and its lowest price . In the outer query,\nwe choose all cat pictures whose price and category_id match a row\nfrom this inner query, using the IN syntax. We would be done here, except that there still exists the possibility\nthat there could be more than one that have that low price for a given\ncategory. So, depending on the database vendor, we can here find\n\"distinct\" rows, according the columns of interest. In Postgresql,\nthe syntax for this is DISTINCT ON([column,...]) , which will omit\nduplicates of the listed columns. For our purposes, we don't want more\nthan one per category, so we distinct on category_id . It is worth noting that without an ORDER BY clause, DISTINCT ON is\nnondeterministic: we are not guaranteed to get the same result each\ntime. Thus, we order by category_id and name , so that only the first\ncat picture alphabetically will show up. We can improve the implementation above by making it a true chainable\nscope. Whereas find_by_sql returns an array of objects, we can\nrefactor this to return an ActiveRelation instead. class CatPicture < ActiveRecord :: Base attr_accessible :category_id , :description , :name , :price belongs_to :category def self . cheapest_per_category where ( \"(category_id, price) IN ( #{ category_id_and_lowest_price_sql } )\" ). select ( \"DISTINCT ON(category_id) #{ table_name } .*\" ). order ( \"category_id ASC, #{ table_name } .name ASC\" ) end private def self . category_id_and_lowest_price_sql scoped . select ( \"category_id, min(price)\" ). group ( :category_id ). to_sql end end Functionally, this generates the exact same query as before, but allows\nfurther chaining. Using ActiveRelation's to_sql method, we're able\nto build up our inner query without actually executing it. We then\ninterpolate that query into what was the outer query, which we've\nreduced to calls to where , select and order . Pros: More performant than Ruby method Scope chaining still possible Cons: Nested subselects Very difficult to read in application code The use of DISTINCT ON - only some RDBMS' have such functionality Approach 3: Window functions But there is still another option. The SQL standard defines a concept\ncalled window functions, which act a lot like aggregates, but don't\nchange the result set. From the Postgresql documentation's excellent\nintroduction to window functions : A window function performs a calculation across a set of table rows that\nare somehow related to the current row. This is comparable to the type\nof calculation that can be done with an aggregate function. But unlike\nregular aggregate functions, use of a window function does not cause\nrows to become grouped into a single output row - the rows retain their\nseparate identities. Let's see how this would work with our dataset. First of all, let's assume the following cat pictures: # SELECT id, name, category_id, price FROM cat_pictures ORDER BY category_id, price;\n\n id |         name         | category_id | price \n----+----------------------+-------------+-------\n  7 | Triple LOL           |           1 |  9.99\n  5 | Hugs not Drugs       |           1 |  9.99\n  2 | Puss in Boots        |           1 | 14.99\n  3 | Cats Gone By         |           1 | 19.99\n  6 | Cats in it for me    |           1 | 22.99\n  4 | Turkleton's Folly    |           2 | 11.99\n  1 | Meowna Lisa          |           2 | 19.99\n  8 | Lady Caterly's Lover |           2 | 22.99 Given this data, our goal is to select \"Hugs not Drugs\" and \"Turkleton's\nFolly\", which are the cheapest pictures from their categories. Whereas a normal aggregate function with GROUP BY would collapse the\nresults, a window function retains the original row. Let's consider how\nthis would affect the inner query from the subselect approach above: # SELECT category_id, min(price) FROM cat_pictures GROUP BY category_id;\n\n category_id |  min  \n-------------+-------\n           1 |  9.99\n           2 | 11.99 # SELECT category_id, min(price) OVER (PARTITION BY category_id) FROM cat_pictures;\n\n category_id |  min  \n-------------+-------\n           1 |  9.99\n           1 |  9.99\n           1 |  9.99\n           1 |  9.99\n           1 |  9.99\n           2 | 11.99\n           2 | 11.99\n           2 | 11.99 Above, we've replaced the GROUP BY clause with an OVER clause. We\nhave the original rows with an additional column for this aggregate\ndata. This is useful in its own right, but the real power of window\nfunctions comes from this concept of window framing. The use of PARTITION BY creates a frame for each group. In our case, we have\ntwo frames, one for each category_id . Then, all aggregate and window\nfunctions before the OVER clause operate against this frame. Each\nwindow frame effectively has its own result set, according to the\ndefined partition. When a window frame is ordered, using an ORDER BY clause, even more\noptions are possible. For example, consider the following: # SELECT id, name, category_id, price, rank() OVER (PARTITION BY category_id ORDER BY price) FROM cat_pictures;\n\n id |         name         | category_id | price | rank \n----+----------------------+-------------+-------+------\n  7 | Triple LOL           |           1 |  9.99 |    1\n  5 | Hugs not Drugs       |           1 |  9.99 |    1\n  2 | Puss in Boots        |           1 | 14.99 |    3\n  3 | Cats Gone By         |           1 | 19.99 |    4\n  6 | Cats in it for me    |           1 | 22.99 |    5\n  4 | Turkleton's Folly    |           2 | 11.99 |    1\n  1 | Meowna Lisa          |           2 | 19.99 |    2\n  8 | Lady Caterly's Lover |           2 | 22.99 |    3 Look familiar? This is essentially the original , except we've added a\nnew column: its price rank within a window partitioned by category_id .\nIt's a mouthful to describe, but we're very close to our original goal\nof finding the cheapest cat picture per category. All we need to do now\nis select rows that have a rank of 1. Not so fast. Can you spot the issue with the above? The rank() window\nfunction assigns the same rank to ties, but we need the first one\nalphabetically in the case of \"ties\". We can remedy that by using a\ndifferent window function, row_number() , which guarantees different\nnumbers. # SELECT id, name, category_id, price, row_number() OVER (PARTITION BY category_id ORDER BY price, name) FROM cat_pictures;\n\n id |         name         | category_id | price | row_number \n----+----------------------+-------------+-------+------------\n  5 | Hugs not Drugs       |           1 |  9.99 |          1\n  7 | Triple LOL           |           1 |  9.99 |          2\n  2 | Puss in Boots        |           1 | 14.99 |          3\n  3 | Cats Gone By         |           1 | 19.99 |          4\n  6 | Cats in it for me    |           1 | 22.99 |          5\n  4 | Turkleton's Folly    |           2 | 11.99 |          1\n  1 | Meowna Lisa          |           2 | 19.99 |          2\n  8 | Lady Caterly's Lover |           2 | 22.99 |          3 Perfect! Looking at the rows with \"1\" as their \"row_number\", we see\nwhat we expect, \"Hugs not Drugs\" and \"Turkleton's Folly\", which are the\ncheapest pictures from their categories. We can use an IN clause for\nfiltering, similar to the previous approach: SELECT id , category_id , name , price FROM cat_pictures WHERE ( id , 1 ) IN ( SELECT id , row_number () OVER ( PARTITION BY category_id ORDER BY price , name ) FROM cat_pictures ); id | category_id |         name         | price \n----+-------------+----------------------+-------\n  5 |           1 | Hugs not Drugs       |  9.99\n  4 |           2 | Turkleton's Folly    | 11.99 The where clause above filters records that both have an id that appears\nin the subquery next to a rank of 1. Now that we have the SQL down,\nlet's convert our Ruby model to take advantage of this window function\ntechnique: class CatPicture < ActiveRecord :: Base attr_accessible :category_id , :description , :name , :price belongs_to :category def self . cheapest_per_category where ( \"( #{ table_name } .id, 1) IN ( #{ price_rank_sql } )\" ) end private def self . price_rank_sql scoped . select ( \"id, row_number() OVER (PARTITION BY category_id ORDER BY price ASC, name ASC)\" ). to_sql end end Groovy. Just like before, we can use to the power of ActiveRelation\nto build up our subselect, which then gets interpolated into the where clause. I've also prepended id in the where clause with table_name , to avoid potential ambiguous column problems. There is one potential issue with using window functions: limited vendor\nsupport. While most of the big boys implement window functions (Oracle,\nPostgresql, and SQLServer, to name a few), MySQL and SQLite users are\nout of luck. Pros: Very performant (consistently twice as fast as Approach 2 on my laptop) Much less noise than SQL subselect stuff Easy to understand, assuming a basic knowledge of SQL window functions Cons: Not portable (window functions are not available in MySQL or SQLite) Conclusion While they may not be appropriate for every situation, window functions\nare a great tool for your toolbelt. They excel at filtering down rows\nbased on aggregate data, or adding aggregate data to the rows you'd\nalready like to select. For more information about window functions, the Postgres documentation\nis an excellent resource, both for its introduction ,\nand its list of window functions . Example app While writing this post, I created a sample Rails app to iterate\nquickly. I used TDD to write the pure-ruby approach, and reused the\nspecs while I \"refactored\" the implementation to the subsequent\napproaches. Of particular note is the history of the CatPicture\nmodel , which mirrors the code above. Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-05-22"},
{"website": "Hash-Rocket", "title": "Faster JSON Generation with PostgreSQL", "author": ["\nJack Christensen\n\n"], "link": "https://hashrocket.com/blog/posts/faster-json-generation-with-postgresql", "abstract": "PostgreSQL Faster JSON Generation with PostgreSQL by\nJack Christensen\n\non\nJanuary 29, 2013 A new feature in PostgreSQL 9.2 is JSON support. It includes a JSON data type and two JSON functions. These allow us to return JSON directly from the database server. This article covers how it is done and includes a benchmark comparing it with traditional Rails JSON generation techniques. How To The simplest way to return JSON is with row_to_json() function. It accepts a row value and returns a JSON value. select row_to_json ( words ) from words ; This will return a single column per row in the words table. { \"id\" : 6013 , \"text\" : \"advancement\" , \"pronunciation\" : \"advancement\" ,...} However, sometimes we only want to include some columns in the JSON instead of the entire row. In theory we could use the row constructor method. select row_to_json ( row ( id , text )) from words ; While this does return only the id and text columns, unfortunately it loses the field names and replaces them with f1, f2, f3, etc. { \"f1\" : 6013 , \"f2\" : \"advancement\" } To work around this we must either create a row type and cast the row to that type or use a subquery. A subquery will typically be easier. select row_to_json ( t ) from ( select id , text from words ) t This results in the JSON output for which we would hope: { \"id\" : 6013 , \"text\" : \"advancement\" } The other commonly used technique is array_agg and array_to_json . array_agg is a aggregate function like sum or count . It aggregates its argument into a PostgreSQL array. array_to_json takes a PostgreSQL array and flattens it into a single JSON value. select array_to_json ( array_agg ( row_to_json ( t ))) from ( select id , text from words ) t This will result in a JSON array of objects: [{ \"id\" : 6001 , \"text\" : \"abaissed\" },{ \"id\" : 6002 , \"text\" : \"abbatial\" },{ \"id\" : 6003 , \"text\" : \"abelia\" },...] In exchange for a substantial jump in complexity, we can also use subqueries to return an entire object graph: select row_to_json ( t ) from ( select text , pronunciation , ( select array_to_json ( array_agg ( row_to_json ( d ))) from ( select part_of_speech , body from definitions where word_id = words . id order by position asc ) d ) as definitions from words where text = 'autumn' ) t This could return a result like the following: { \"text\" : \"autumn\" , \"pronunciation\" : \"autumn\" , \"definitions\" : [ { \"part_of_speech\" : \"noun\" , \"body\" : \"skilder wearifully uninfolded...\" }, { \"part_of_speech\" : \"verb\" , \"body\" : \"intrafissural fernbird kittly...\" }, { \"part_of_speech\" : \"adverb\" , \"body\" : \"infrugal lansquenet impolarizable...\" } ] } Obviously, the SQL to generate this JSON response is far more verbose than generating it in Ruby. Let's see what we get in exchange. Benchmarks I created a sample benchmark application to test multiple JSON generation approaches. The sample domain is a dictionary. The source is at https://github.com/JackC/json_api_bench . The first test is of an extremely light weight auto-complete search. The result set is simply an array of strings. I tested three approaches: loading the entire ActiveRecord domain model, using pluck, and using PostgreSQL ( view source ). +-----------------------------+----------+\n| Name                        | Reqs/Sec |\n+-----------------------------+----------+\n| Quick Search Domain         | 467.84   |\n| Quick Search Pluck          | 496.89   |\n| Quick Search PostgreSQL     | 540.54   |\n+-----------------------------+----------+ Pluck should probably be the preferred approach in this case. While PostgreSQL is about 8% faster, the code is less clear. The next test is of a slightly richer word search. It returns an array of objects that each include text, pronunciation, part of speech, and definition ( view source ). +-----------------------------+----------+\n| Name                        | Reqs/Sec |\n+-----------------------------+----------+\n| Rich Search Domain          | 322.58   |\n| Rich Search Select All      | 418.85   |\n| Rich Search PostgreSQL      | 500.00   |\n+-----------------------------+----------+ In this case, select_all should still usually be preferred over PostgreSQL. The loss of clarity is not worth the 19% performance increase. Now we get to a test of an entire object graph for a word. This returns an object with text, pronunciation, an array of definitions, an array of quotes, an array of synonyms, and an array of antonyms ( view source ) +-----------------------------+----------+\n| Name                        | Reqs/Sec |\n+-----------------------------+----------+\n| Definition Domain           | 130.72   |\n| Definition PostgreSQL       | 457.14   |\n+-----------------------------+----------+ Now things start to get more favorable for PostgreSQL. In exchange for a substantial block of SQL we get a 3.51x throughput increase. Finally, we run a test of returning multiple definitions per call. This is a more synthetic benchmark to exercise heavy weight API responses ( view source ). +-----------------------------+----------+\n| Name                        | Reqs/Sec |\n+-----------------------------+----------+\n| Many Definitions Domain     | 25.82    |\n| Many Definitions PostgreSQL | 330.58   |\n+-----------------------------+----------+ For a large JSON object graph PostgreSQL JSON generation can offer well over 12x the throughput. Conclusions PostgreSQL is always faster than traditional Rails JSON generation, but the code is always more verbose. For simple responses that do not involve nested objects, the performance gain is insufficient to warrant the loss in code clarity. As the object graph increases in size and complexity, the performance gains become more and more attractive. Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-01-29"},
{"website": "Hash-Rocket", "title": "Materialized View Strategies Using PostgreSQL", "author": ["\nJack Christensen\n\n"], "link": "https://hashrocket.com/blog/posts/materialized-view-strategies-using-postgresql", "abstract": "PostgreSQL Materialized View Strategies Using PostgreSQL by\nJack Christensen\n\non\nApril 23, 2015 Queries returning aggregate, summary, and computed data are frequently used in application development. Sometimes these queries are not fast enough. Caching query results using Memcached or Redis is a common approach for resolving these performance issues. However, these bring their own challenges. Before reaching for an external tool it is worth examining what techniques PostgreSQL offers for caching query results. Example Domain We will examine different approaches using the sample domain of a simplified account system. Accounts can have many transactions. Transactions can be recorded ahead of time and only take effect at post time. e.g. A debit that is effective on March 9 can be entered on March 1. The summary data we need is account balance. create table accounts ( name varchar primary key ); create table transactions ( id serial primary key , name varchar not null references accounts on update cascade on delete cascade , amount numeric ( 9 , 2 ) not null , post_time timestamptz not null ); create index on transactions ( name ); create index on transactions ( post_time ); Sample Data and Queries For this example, we will create 30,000 accounts with an average of 50 transactions each. All the sample code and data is available on Github . Our query that we will optimize for is finding the balance of accounts. To start we will create a view that finds balances for all accounts. A PostgreSQL view is a saved query. Once created, selecting from a view is exactly the same as selecting from the original query, i.e. it reruns the query each time. create view account_balances as select name , coalesce ( sum ( amount ) filter ( where post_time <= current_timestamp ), 0 ) as balance from accounts left join transactions using ( name ) group by name ; Note that this uses an aggregate filter clause, an awesome feature introduced in PostgreSQL 9.4 . Now we simply select all rows with negative balances. select * from account_balances where balance < 0 ; After several runs to warm OS and PostgreSQL caches, this query takes approximately 3850ms. We are going to examine multiple solutions. To keep them namespaced we will create separate schemas for each approach. create schema matview ; create schema eager ; create schema lazy ; PostgreSQL Materialized Views The simplest way to improve performance is to use a materialized view . A materialized view is a snapshot of a query saved into a table. create materialized view matview . account_balances as select name , coalesce ( sum ( amount ) filter ( where post_time <= current_timestamp ), 0 ) as balance from accounts left join transactions using ( name ) group by name ; Because a materialized view actually is a table, we can create indexes. create index on matview . account_balances ( name ); create index on matview . account_balances ( balance ); To retrieve the balance from each row we simple select from the materialized view. select * from matview . account_balances where balance < 0 ; The performance impact is impressive. It now only takes 13ms to retrieve all the accounts with negative balances -- 453x faster! Unfortunately, these materialized views have two substantial limitations. First, they are only updated on demand. Second, the whole materialized view must be updated; there is no way to only update a single stale row. -- refresh all rows refresh materialized view matview . account_balances ; In the case where possibly stale data is acceptable, they are an excellent solution. But if data must always be fresh they are not a solution. Eager Materialized View Our next approach is to materialize the query into a table that is eagerly updated whenever a change occurs that would invalidate a row. We can do that with triggers . A trigger is a bit of code that runs when some event such as an insert or update happens. First, we create the table to store the materialized rows. create table eager . account_balances ( name varchar primary key references accounts on update cascade on delete cascade , balance numeric ( 9 , 2 ) not null default 0 ); create index on eager . account_balances ( balance ); Now we need to think of every way that account_balances could become stale. An account is inserted On account insertion we need to create a account_balances record with a zero balance for the new account. create function eager . account_insert () returns trigger security definer language plpgsql as $$ begin insert into eager . account_balances ( name ) values ( new . name ); return new ; end ; $$ ; create trigger account_insert after insert on accounts for each row execute procedure eager . account_insert (); The syntax for create function and create trigger is quite extensive. Refer to the documentation for details. But the summary explanation is this: We create the function eager.account_insert as a trigger function that will run with the permissions of the user who created it ( security definer ). Inside a insert trigger function, new is a variable that holds the new record. An account is updated or deleted Account update and deletion will be handled automatically because the foreign key to account is declared as on update cascade on delete cascade . A transaction is inserted, updated, or deleted Transaction insert, update, and delete all have one thing in common: they invalidate the account balance. So the first step is to define a refresh account balance function. create function eager . refresh_account_balance ( _name varchar ) returns void security definer language sql as $$ update eager . account_balances set balance = ( select sum ( amount ) from transactions where account_balances . name = transactions . name and post_time <= current_timestamp ) where name = _name ; $$ ; Next we can create trigger function that calls refresh_account_balance whenever a transaction is inserted. create function eager . transaction_insert () returns trigger security definer language plpgsql as $$ begin perform eager . refresh_account_balance ( new . name ); return new ; end ; $$ ; create trigger eager_transaction_insert after insert on transactions for each row execute procedure eager . transaction_insert (); Perform is how you execute a query where you do not care about the result in PL/pgSQL. For the delete of a transaction we only get the variable old instead of new row. old stores the previous value of the row. create function eager . transaction_delete () returns trigger security definer language plpgsql as $$ begin perform eager . refresh_account_balance ( old . name ); return old ; end ; $$ ; create trigger eager_transaction_delete after delete on transactions for each row execute procedure eager . transaction_delete (); For the update of a transaction, we have to account for the possibility that the account the transaction belongs to was changed. We use the old and new values of the row to determine which account balances are invalidated and need to be refreshed. create function eager . transaction_update () returns trigger security definer language plpgsql as $$ begin if old . name != new . name then perform eager . refresh_account_balance ( old . name ); end if ; perform eager . refresh_account_balance ( new . name ); return new ; end ; $$ ; create trigger eager_transaction_update after update on transactions for each row execute procedure eager . transaction_update (); Finally, with all this set up we need to initialize the account_balances table. -- Create the balance rows insert into eager . account_balances ( name ) select name from accounts ; -- Refresh the balance rows select eager . refresh_account_balance ( name ) from accounts ; To query the negative account balances we simply select from the acount_balances table. select * from eager . account_balances where balance < 0 ; This is really fast (13ms / 453x faster) just like the materialized view. But it has the advantage of it stays fresh even when transactions change. Unfortunately, this strategy doesn't account for one key requirement -- row invalidation by the passage of time. Lazy Materialized View The previous solution was not bad. It was just incomplete. The full solution lazily refreshes the materialized rows when they are stale. As with the eager materialization strategy, our first step is to create a table to store the materialized rows. The difference is we add an expiration time column. create table lazy . account_balances_mat ( name varchar primary key references accounts on update cascade on delete cascade , balance numeric ( 9 , 2 ) not null default 0 , expiration_time timestamptz not null ); create index on lazy . account_balances_mat ( balance ); create index on lazy . account_balances_mat ( expiration_time ); We will create the initial rows for lazy.account_balances_mat with expiration_time as -Infinity to mark them as dirty. insert into lazy . account_balances_mat ( name , expiration_time ) select name , '-Infinity' from accounts ; The same data changes that could invalidate materialized rows in the eager strategy must be handled with the lazy strategy. The difference is that the triggers will only update expiration_time -- they will not actually recalculate the data. An account is inserted As with the eager strategy, on account insertion we need to create a account_balances_mat record with a zero balance for the new account. But we also need to provide an expiration_time . The balance for an account with no transactions will be valid forever, so we provide the special PostgreSQL value Infinity as the expiration_time . Infinity is defined as greater than any other value. create function lazy . account_insert () returns trigger security definer language plpgsql as $$ begin insert into lazy . account_balances_mat ( name , expiration_time ) values ( new . name , 'Infinity' ); return new ; end ; $$ ; create trigger lazy_account_insert after insert on accounts for each row execute procedure lazy . account_insert (); An account is updated or deleted As before, account update and deletion will be handled by the the foreign key cascades. A transaction is inserted For the insert of a transaction, we update the expiration_time if the post_time of the transaction is less than the current expiration_time . This means the update only happens when absolutely necessary. If the account will already be considered stale at the post_time of the new record we avoid the IO cost of the write. create function lazy . transaction_insert () returns trigger security definer language plpgsql as $$ begin update lazy . account_balances_mat set expiration_time = new . post_time where name = new . name and new . post_time < expiration_time ; return new ; end ; $$ ; create trigger lazy_transaction_insert after insert on transactions for each row execute procedure lazy . transaction_insert (); A transaction is updated Unlike when a transaction is inserted, when a transaction is updated, it is not possible to compute the new account expiration_time without reading the account's transactions. This makes it cheaper to simply invalidate the account balance. We will simply set expiration_time to -Infinity , a special value defined as being less than all other values. This ensures that the row will be considered stale. create function lazy . transaction_update () returns trigger security definer language plpgsql as $$ begin update accounts set expiration_time = '-Infinity' where name in ( old . name , new . name ) and expiration_time <> '-Infinity' ; return new ; end ; $$ ; create trigger lazy_transaction_update after update on transactions for each row execute procedure lazy . transaction_update (); A transaction is deleted For transaction deletion, we invalidate the row if the post_time is less than or equal to the current expiration_time . But if at is after the current expiration_time we do not have to do anything. create function lazy . transaction_delete () returns trigger security definer language plpgsql as $$ begin update lazy . account_balances_mat set expiration_time = '-Infinity' where name = old . name and old . post_time <= expiration_time ; return old ; end ; $$ ; create trigger lazy_transaction_delete after delete on transactions for each row execute procedure lazy . transaction_delete (); Final Steps The penultimate step is to define a function to refresh a materialized row. create function lazy . refresh_account_balance ( _name varchar ) returns lazy . account_balances_mat security definer language sql as $$ with t as ( select coalesce ( sum ( amount ) filter ( where post_time <= current_timestamp ), 0 ) as balance , coalesce ( min ( post_time ) filter ( where current_timestamp < post_time ), 'Infinity' ) as expiration_time from transactions where name = _name ) update lazy . account_balances_mat set balance = t . balance , expiration_time = t . expiration_time from t where name = _name returning account_balances_mat . * ; $$ ; This function uses a common table expression and aggregate filters to find balance and expiration_time in a single select. Then results are then used to update acount_balances_mat . Finally, we define the account_balances view. The top part of the query reads fresh rows from account_balances_mat . The bottom part reads and refreshes rows that are stale. create view lazy . account_balances as select name , balance from lazy . account_balances_mat where current_timestamp < expiration_time union all select r . name , r . balance from lazy . account_balances_mat abm cross join lazy . refresh_account_balance ( abm . name ) r where abm . expiration_time <= current_timestamp ; To retrieve the all accounts with negative balances balances we simply select from the account_balances view. select * from lazy . account_balances where balance < 0 ; The first time the query is run it takes about 5900ms because it is caching the balance for all accounts. Subsequent runs only take about 16ms (368x faster). In general, the query run time should not be nearly so variable because only a small fraction of the rows will be refreshed in any one query. Comparison of Techniques PostgreSQL's built-in materialized views offer the best performance improvement for the least work, but only if stale data is acceptable. Eager materialized views offer the absolute best read performance, but can only guarantee freshness if rows do not go stale due to the passage of time. Lazy materialized views offer almost as good read performance as eager materialized views, but they can guarantee freshness under all circumstances. One additional consideration is read-heavy vs. write-heavy workloads. Most systems are read-heavy. But for a write-heavy load you should give consider leaning toward lazy and away from eager materialized views. The reason is that eager materialized views do the refresh calculation on every write whereas lazy materialized views only pay that cost on read. Final Thoughts PostgreSQL materialization strategies can improve performance by a factor of hundreds or more. In contrast to caching in Memcachd or Redis, PostgreSQL materialization provides ACID guarantees. This eliminates an entire category of consistency issues that must be handled at the application layer. In addition, the infrastructure for a system as a whole is simpler with one less part. The increased performance and system simplicity is well worth the cost of more advanced SQL. References Example code for this post Chapter 12 of Enterprise Rails describes materialized views Materialized view talk from 2008 PGCon Jonathan Gardner materialized view notes (header image via Flickr user t_buchtele ) Was this post helpful? Share it with others. Tweet Share Post", "date": "2015-04-23"},
{"website": "Hash-Rocket", "title": "Exploring the Postgres Gin index", "author": ["\nDerek Parker\n\n"], "link": "https://hashrocket.com/blog/posts/exploring-postgres-gin-index", "abstract": "Ruby PostgreSQL Exploring the Postgres Gin index by\nDerek Parker\n\non\nOctober  9, 2014 Postgres has 4 different types of indexes, each better suited for a particular task. In this post, I will explore the Postgres Gin index and how to leverage it to quickly search text columns. Leveraging PostgreSQL Gin index What problem will we be solving? Suppose we wanted to implement simple search functionality for a web app. Say, for example, we wanted to search through all users in the system. Also imagine that we have ~ 1 million users currently stored in the system. The requirements for this search implementation state that we should be able to search via partial matches, and search via multiple columns (e.g. first_name, last_name). More concretely, if we have the following users: \"Hank Lillard\" and \"Lilly Adams\", an input query of \"Lil\" should return both users. We can solve this problem using a Gin index with a special option in order to achieve extremely performant searches. Let's first dive into a little bit of explanation about Gin indexes. What is the Gin index? From the docs : \"GIN stands for Generalized Inverted Index. GIN is designed for handling cases where the items to be indexed are composite values, and the queries to be handled by the index need to search for element values that appear within the composite items. For example, the items could be documents, and the queries could be searches for  documents containing specific words.\" We will be using a Gin index accross multiple columns in our table. Along with our index, we will be passing a special option called gin_trgm_ops . We will explain more about this option and how it benefits us shortly. Creating test data Let's create an example table and fill it with random strings so that you can follow along at home. CREATE TABLE users ( first_name text , last_name text ) Next, let's fill that table up with random data: SELECT md5 ( random ():: text ), md5 ( random ():: text ) FROM ( SELECT * FROM generate_series ( 1 , 1000000 ) AS id ) AS x ; This will give us a million rows of random data to search through. Now, notice we have not created our index yet. Let's try searching through this data without, and see what kind of results we get back. Make sure to type in \\timing in order to get time data back from these queries. SELECT count ( * ) FROM users where first_name ilike '%aeb%' ; Running this query takes about 942.97 ms on my system. Let's see what happens when we search using both first_name and last_name. SELECT count ( * ) FROM users where first_name ilike '%aeb%' or last_name ilike '%aeb%' ; This query takes 1641.049 ms to run on my system. Obviously, this is completely unacceptable. We can do so much better by leveraging the amazing power of the indexes Postgres provides. Introducing our Gin index Let's create our Gin index, using the gin_trgm_ops option I mentioned earlier. NOTE: If you're on Ubuntu you must ensure you have the contrib packages installed. On 14.04 simply run sudo apt-get install postgresql-contrib-9.3 before running the following queries. CREATE EXTENSION IF NOT EXISTS pg_trgm ; CREATE INDEX users_search_idx ON users USING gin ( first_name gin_trgm_ops , last_name gin_trgm_ops ); Creating this index may take a decent amount of time. Once it finishes, let's try running those same queries again and see what we get. First query: 33.355 ms Second query:  72.728 ms Obviously this is a major improvement. We could actually see performing this query in request now without any issues. What is gin_trgm_ops? This option tells Postgres to index using trigrams over our selected columns. A trigram is a data structure that hold 3 letters of a word. Essentially, Postgres will break down each text column down into trigrams and use that in the index when we search against it. Caveats The only downside of this approach is that the input query must be at least 3 letters, as Postgres will need to be able to extract at least one trigram from the input query in order to use our trigram index. Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-10-09"},
{"website": "Hash-Rocket", "title": "Advisory Locks in PostgreSQL", "author": ["\nDerek Parker\n\n"], "link": "https://hashrocket.com/blog/posts/advisory-locks-in-postgres", "abstract": "PostgreSQL Advisory Locks in PostgreSQL by\nDerek Parker\n\non\nNovember  7, 2013 PostgreSQL provides various lock modes to control concurrent access to data in tables. Advisory locks provide a convenient way to obtain a lock from PostgreSQL that is completely application enforced, and will not block writes to the table. Imagine you have a scheduled task that runs in the background, mutates the database and sends information off to another 3rd party service. We can use PostgreSQL Advisory Locks to guarantee that the program cannot cause any unexpected behavior if ran multiple times concurrently. Concurrency is fun, but surprise concurrency can be brutal! In our case it's beneficial to have a guarantee that the program in question can never run concurrently. Application Enforced Locks Throughout this post we will explore Postgres advisory locks , which are application enforced database locks.  Advisory locks can be acquired at the session level and at the transaction level and release as expected when a session ends or a transaction completes. So what does it mean to have an application enforced database lock? Essentially, when your process starts up, you can acquire a lock through Postgres and then release it when the program exits. In this way, we have a guarantee that the program cannot be running concurrently, as it will not be able to acquire the lock at startup, in which case you can just exit. The benefit of this is that the tables are never actually locked for writing, so the main application can behave as normal and users will never notice anything is happening in the background. Here is the syntax for obtaining the lock: SELECT pg_try_advisory_lock ( 1 ); Now, there's a few things happening in that statement so lets examine it. pg_try_advisory_lock(key); is the Postgres function that will try to obtain the lock. If the lock is successfully obtained, Postgres will return 't', otherwise 'f' if you failed you obtain the lock. pg_try_advisory_lock(key) takes an argument which will become the identifier for the lock. So, for example, if you were to pass in 1, and another session tried to call SELECT pg_try_advisory_lock(1) it would return 'f', however the other session could obtain SELECT pg_try_advisory_lock(2) . pg_try_advisory_lock(key) will not wait until it can obtain the lock, it will return immediately with 't' or 'f'. Implementation So how would we go about using this in Ruby? def obtained_lock? connection . select_value ( 'select pg_try_advisory_lock(1);' ) == 't' end We can grab our ActiveRecord connection and call #select_value in order get back a 't' or 'f' value. A simple equality check let's us know whether or not we have obtained the lock, and if we haven't we can choose to bail and exit the program. class LockObtainer def lock_it_up exclusive do # do important stuff here end end private def exclusive if obtained_lock? begin yield ensure release_lock end end end def obtained_lock? connection . select_value ( 'select pg_try_advisory_lock(1);' ) == 't' end def release_lock connection . execute 'select pg_advisory_unlock(1);' end def connection ActiveRecord :: Base . connection end end Additional Information There are a few interesting things about Advisory Locks: They are reference counted, so you can obtain a lock N times, but must release it N times for another process to acquire it. Advisory Locks can be acquired at the session level or the transaction level, meaning if acquired within a transaction, an Advisory Lock will be automatically released for you. Outside of a transaction, you must manually release the lock, or end your session with PostgreSQL. Calling SELECT pg_advisory_unlock_all() will unlock all advisory locks currently held by you in your session. Calling SELECT pg_advisory_unlock(n) will release one lock reference for the id n. Lastly, as a way to quickly obtain a lock multiple times, you can run SELECT pg_try_advisory_lock(1) FROM foo; where foo is a table in your database. That will obtain a lock for every row in that table. You may also provide a limit in the select like so: SELECT pg_try_advisory_lock ( 1 ) FROM ( SELECT id FROM foo LIMIT 10 ); Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-11-07"},
{"website": "Hash-Rocket", "title": "Writable Common Table Expressions", "author": ["\nJack Christensen\n\n"], "link": "https://hashrocket.com/blog/posts/writable-common-table-expressions", "abstract": "Ruby PostgreSQL Writable Common Table Expressions by\nJack Christensen\n\non\nAugust 18, 2013 Writable common table expressions are a powerful feature of PostgreSQL that is rarely used when building applications with an ORM like ActiveRecord. It allows inserts, updates, and deletes to be performed in the WITH clause of an SQL statement. Here are a few techniques that I have found useful. Writable common table expressions documentation Write Master-Detail Records in a Single Statement Creating 1 master record and 3 detail records in ActiveRecord makes 6 round trips to the database server -- 4 inserts, 1 begin, and 1 commit. question = Question . new text: \"What is your favorite text editor?\" question . answers . new text: 'vim' question . answers . new text: 'emacs' question . answers . new text: 'Sublime Text' question . save! # this does 6 SQL statements This can be reduced to a single complex statement that returns the question id with a writable CTE: with q as ( insert into questions ( text ) values ( 'What is your favorite text editor?' ) returning * ), a as ( insert into answers ( question_id , text ) select q . id , t from q cross join ( values ( 'vim' ), ( 'emacs' ), ( 'Sublime Text' )) t ) select id from q ; Let's take a closer look at that SQL. On line 4, returning * makes the insert return a result set just like a select does. Therefore q becomes a result set that can be referenced later so the answers know the question_id foreign key. The rows to insert into answers are producted by cross joining the inserted question with a values list of answer texts. After the question and answers are inserted, we select question_id as the result of the entire statement. Unfortunately, we need to build this SQL by hand. Let's look at how we do that: class Question < ActiveRecord :: Base has_many :answers def self . insert_via_cte ( attributes ) sql = <<- SQL with q as (\n        insert into questions(text) values(?) returning *\n      ), a as (\n        insert into answers(question_id, text)\n        select q.id, t.text\n        from q cross join (values SQL args = [ attributes [ :question ]] sql << attributes [ :answers ]. map do | a | args << a \"(?)\" end . join ( \", \" ) sql << \") t(text)) select id from q\" connection . select_value sanitize_sql ([ sql , * args ]) end end Building the SQL string and the arguments array by hand is much more complex than the original ActiveRecord version. In particular, whenever building SQL strings care should be taken to avoid SQL injection vulnerabilities. The sanitize_sql method can help. A simple benchmark against a local PostgreSQL server shows the CTE version is 8x faster. With a remote server the round trip times would be much higher, meaning that the gains could be even more dramatic. If you need every last bit of write performance this can make a substantial difference and the increased complexity may be worth it. Merge Sometimes you want to update a record if it exists and create it if it doesn't. In traditional ActiveRecord this could be done like this: begin product = Product . find_or_initialize_by ( id: pid ) product . counter += 1 product . save! rescue ActiveRecord :: RecordNotUnique , ActiveRecord :: StaleObjectError retry end Provided Product has a lock_version column this will be correct. However, depending on the how bad the concurrency collisions are it could be fairly slow. In PostgreSQL we can do this: with upd as ( update products set counter = counter + 1 , lock_version = lock_version + 1 -- play nice with ActiveRecord where id = 42 returning * ) insert into products ( id , counter ) select 42 , 1 where not exists ( select * from upd ); This does have a small race condition that will cause uniqueness errors on the insert. Provided the above SQL was wrapped in a Product.merge_increment class method this could be resolved by retrying on ActiveRecord::RecordNotUnique . begin Product . merge_increment ( pid ) rescue ActiveRecord :: RecordNotUnique retry end In an admittedly contrived benchmark of very heavy concurrent updates, the PostgreSQL version was over 3x faster. Delete and Archive This tip is more for database maintenance than for application code. Sometimes you need to delete some records, but you would like to keep them archived somewhere else. Writable CTEs make this easy. with deleted as ( delete from products where id < 10 returning * ) insert into deleted_products select * from deleted ; Source Code Source code for samples is on Github . Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-08-18"},
{"website": "Hash-Rocket", "title": "Understanding Common Table Expressions with FizzBuzz", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/understanding-common-table-expressions-with-fizzbuzz", "abstract": "PostgreSQL Understanding Common Table Expressions with FizzBuzz by\nJosh Branchaud\n\non\nOctober 12, 2015 For a long time, I avoided trying to understand what common table expressions were or how they worked. The name alone was intimidating. I figured I could just get by with the database features supported by my ORM. Better that way; I won't have to get my hands dirty with all that raw SQL. It turns out that I was missing out on some solid database features by avoiding all that raw SQL goodness. Common table expressions (CTEs) are a potent feature of PostgreSQL and most other modern relational databases. By avoiding features like CTEs, I was treating Postgres like a dumb data store instead of the powerful computation engine that it is. CTEs are one Postgres feature that epitomize this power. The PostgreSQL documentation gives the following description of common table\nexpressions: [CTEs] can be thought of as defining temporary tables that exist just for\none query. That sounds nice, but it undersells what we can do with CTEs once we understand how they work. Let's interact with some concrete examples. You'll want to pop open a psql session for this. Use \\e in psql to edit the SQL in your default editor. For a first example, how about something straightforward like a CTE that counts to 100: with numbers as ( select generate_series ( 1 , 100 ) ) select * from numbers ; This results in: generate_series ----------------- 1 2 3 ... all the way up to 100 . Sure, that example isn't all that exciting. We could easily achieve the same with a simpler looking query that involves a sub-select. Nevertheless, we can see how the temporary table constructed by the CTE is visually set apart from the rest of the query and its namespace ( numbers ) made explicit. This temporary table numbers now has the set of integers from 1 to 100 from which we make our selection -- all of them. We depart from what can be achieved with mere sub-selects when we employ the recursive option. By making a CTE recursive, it essentially becomes a dynamically-built temporary table. With a recursive CTE, we can achieve the same as above, but doing so on the fly and without the generate_series function: with recursive numbers ( x ) as ( select 1 union select x + 1 from numbers where x < 100 ) select * from numbers ; That is a funky looking query, so let's break it down. I've added in the recursive keyword to declare that this will be a\nrecursive CTE. I've also named the single column of the CTE as x . If there\nwere more columns, I could include them with separating commas. This makes\nit so that I can reference the individual columns within the CTE. The end of the query looks the same, so it's just the internals of the CTE\nthat we have left to dissect. So let's take a look at that starting from the top. The select 1 means we start with a single column, single row table that\ncontains 1 . This provides a base on which the recursive part of the\nCTE can build. This base table is then unioned with a dynamically-built table where\neach row is recursively computed as 1 added to the row before it ( select x + 1 ). This\nprocess happens until some terminating condition is met. In this case, that\nterminating condition is when the value of the latest x reaches 100 ( where x < 100 ). The end result is a temporary table, numbers , with the values from 1 to 100 . We can select from this dynamically generated temporary table like we do any other table. If we only want the rows where x is greater than 50 , we just query this table like we do any other table -- select * from numbers where x > 50 . In our case, we want to grab everything -- all 100 rows -- which we can do with select * from numbers . We wanted a selection that included the values from 1 to 100 and now we've got it. Let that soak in for a moment. We just dynamically built a temporary table of values where each value was an individual computation based on previously computed values. And we can do a lot more than just add 1 to x . We can fill the CTE with all kinds of formulas\nand logic in order to build a temporary table full of interesting and useful\ndata. That's powerful. This means that with a little SQL we\ncan do a lot of fancy stuff right in the database. Better yet, we can avoid expensive and complicated trips between the database and some\nscript or process that we would otherwise rely on to build that\nintermediate set of data. Let's see what it means to add some logic and formulas to our CTE by taking our existing query a step further. How about solving the FizzBuzz programming challenge? We've already got the set of numbers from 1 to 100 ; they just need to be transformed into Fizz , Buzz , and FizzBuzz . So let's start with Fizz . The rule is that any value that is evenly divisible by 3 should be represented by Fizz (we'll ignore the FizzBuzz rule for the moment). We can know that the given value is divisible by 3 if we take the modulo of 3 and the result is 0 . A list of values and whether or not they are divisible by 3 is a good starting point. with recursive fizzbuzz ( num , val ) as ( select 0 , false union select ( num + 1 ), ( num + 1 ) % 3 = 0 from fizzbuzz where num < 100 ) select num , val from fizzbuzz where num > 0 ; The result with just the first 10 rows: num | val\n-----+-----\n   1 | f\n   2 | f\n   3 | t\n   4 | f\n   5 | f\n   6 | t\n   7 | f\n   8 | f\n   9 | t\n  10 | f\n... Great. Now we want to display Fizz whenever there is a true , otherwise leave the value as is. We can use a case ( switch ) statement to populate the second row accordingly. with recursive fizzbuzz ( num , val ) as ( select 0 , '0' union select ( num + 1 ), case when ( num + 1 ) % 3 = 0 then 'Fizz' else ( num + 1 ):: text end from fizzbuzz where num < 100 ) select num , val from fizzbuzz where num > 0 ; The result with just the first 10 rows: num | val\n-----+------\n   1 | 1\n   2 | 2\n   3 | Fizz\n   4 | 4\n   5 | 5\n   6 | Fizz\n   7 | 7\n   8 | 8\n   9 | Fizz\n  10 | 10\n... This is really close to what we want. By incorprating the rules for Buzz and FizzBuzz , we should have what we are looking for. That can be achieved by extending the case statement a bit further for values of 5 and 15 . with recursive fizzbuzz ( num , val ) as ( select 0 , '0' union select ( num + 1 ), case when ( num + 1 ) % 15 = 0 then 'FizzBuzz' when ( num + 1 ) % 5 = 0 then 'Buzz' when ( num + 1 ) % 3 = 0 then 'Fizz' else ( num + 1 ):: text end from fizzbuzz where num < 100 ) select val from fizzbuzz where num > 0 ; The result out to 20 rows: num |   val\n-----+----------\n   1 | 1\n   2 | 2\n   3 | Fizz\n   4 | 4\n   5 | Buzz\n   6 | Fizz\n   7 | 7\n   8 | 8\n   9 | Fizz\n  10 | Buzz\n  11 | 11\n  12 | Fizz\n  13 | 13\n  14 | 14\n  15 | FizzBuzz\n  16 | 16\n  17 | 17\n  18 | Fizz\n  19 | 19\n  20 | Buzz\n... And there you have it, FizzBuzz solved with PostgreSQL. Try that out at your next programming job interview. CTEs are great for a lot more than solving FizzBuzz. Non-recursive CTEs alone can be used to simplify and dry up complicated queries that would otherwise involve many nested sub-selects 1 . Recursive CTEs allow you to do things in-database that simply aren't possible with other SQL constructs (e.g. sub-selects). The possibilities include but are not limited to generating reports from data aggregated from many tables and transforming data when migrating from one schema to another. This makes them a valuable addition to your SQL repertoire. CTEs are even more powerful than I've illustrated. To see some other cool capabilities of CTEs, check out this post on Writeable Common Table Expressions . It is my understanding that the Postgres query planning engine optimizes sub-selects differently than with queries, which may have performance implications. That is, sub-selects may be more aggressively optimized. That being said, CTEs generally reduce round-trips to the database and simplify SQL code, so unless you have hard performance numbers saying otherwise, CTEs are likely still a win. ↩ Was this post helpful? Share it with others. Tweet Share Post", "date": "2015-10-12"},
{"website": "Hash-Rocket", "title": "Upsert Records with PostgreSQL 9.5", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/upsert-records-with-postgresql-9-5", "abstract": "PostgreSQL Upsert Records with PostgreSQL 9.5 by\nJosh Branchaud\n\non\nJanuary 14, 2016 With the release of PostgreSQL 9.5, we now have a better way to upsert data. No more making multiple trips to the database. No more shoehorning writeable common table expressions. No more defining custom merge functions. We finally have the upsert feature we've been waiting for. For those not familiar with the term upsert , it is also sometimes referred to as a merge . The idea is this. Some data needs to be put in the database. If the data has an existing associated record, that record should be updated, otherwise the data should be freshly inserted as a new record. Depending on the state of the database, a record is either updated or inserted , hence the name upsert . This type of functionality has been available in other relational databases (e.g. MySQL) for a while, so it's exciting to see it makes its way into PostgreSQL. Traditionally, to achieve this kind of update or insert as necessary functionality, our application code would have to make two trips to the database. The first is to check if there is already an existing record that can be updated. The second is either an update or an insert depending on the result of the first query. There are also some more clever approaches to upserting that only take a single trip to the database. Since the release of PostgreSQL 9.1, we can take advantage of Writeable Common Table Expressions to upsert records . It's also possible to use PL/pgSQL to create a custom upsert function . And now, we can do an explicit upsert using the on conflict clause of the insert statement. Before we go any further, let's give ourselves an example database for illustrative purposes. Pinned tweets Let's use Twitter as an example and model the three tables we need to represent pinned tweets. Each Twitter user can have many tweets. Of their tweets, a user may choose to have one of these tweets designated as a pinned tweet . They can change their pinned tweet at any time and can even go back to having no pinned tweet at all. For this, we, of course, need users and tweets . We'll also have a pinned_tweets table that tracks the pinned tweet for each user along with the time at which they pinned that tweet. This may be a bit contrived, but bear with me. Also, feel free to follow along in your own psql session. All the code you need is posted as we go. So, the tables... create table users ( handle varchar primary key , name varchar not null , bio varchar not null default '' , created_at timestamptz not null default now () ); create table tweets ( id serial primary key , content varchar not null , user_handle varchar references users ( handle ), created_at timestamptz not null default now (), unique ( id , user_handle ) ); create table pinned_tweets ( tweet_id integer not null , user_handle varchar not null , pinned_at timestamptz not null default now (), primary key ( user_handle ), foreign key ( tweet_id , user_handle ) references tweets ( id , user_handle ) ); We'll now need some users. insert into users ( handle , name ) values ( 'rey' 'Rey' ); insert into users ( handle , name ) values ( 'kyloren' , 'Kylo Ren' ); insert into users ( handle , name ) values ( 'hansolo' , 'Han Solo' ); select handle , name from users ; --    handle  |     name --  ----------+-------------- --   rey      | Rey --   kyloren  | Kylo Ren --   hansolo  | Han Solo We will start our users out with some tweets as well (don't worry, no spoilers). insert into tweets ( user_handle , content ) values ( 'rey' , '@kyloren you’re afraid that you will never be as strong as Darth Vadar.' ); insert into tweets ( user_handle , content ) values ( 'kyloren' , 'I will fulfill our destiny.' ); insert into tweets ( user_handle , content ) values ( 'hansolo' , 'Well, you tell them that Han Solo just stole back the Millennium Falcon, for good!' ); insert into tweets ( user_handle , content ) values ( 'hansolo' , 'I’ve got a bad feeling about this.' ); insert into tweets ( user_handle , content ) values ( 'rey' , 'I didn’t think there was this much green in the whole galaxy...' ); select user_handle , count ( * ) from tweets group by user_handle ; --   user_handle | count --  -------------+------- --   rey         |     2 --   kyloren     |     1 --   hansolo     |     2 Upserting Pinned Tweets Before we look at the new upsert functionality, let's upsert with a writeable CTE (Common Table Expression) so that we have something for comparison. We can pin Han Solo's first tweet for him. select * from pinned_tweets where user_handle = 'hansolo' ; --  tweet_id | user_handle | pinned_at -- ----------+-------------+----------- -- (0 rows) with upsert as ( update pinned_tweets set ( tweet_id , pinned_at ) = ( 3 , clock_timestamp ()) where user_handle = 'hansolo' returning * ) insert into pinned_tweets ( user_handle , tweet_id , pinned_at ) select 'hansolo' , 3 , clock_timestamp () where not exists ( select 1 from upsert where upsert . user_handle = 'hansolo' ); -- INSERT 0 1 select * from pinned_tweets where user_handle = 'hansolo' ; --  tweet_id | user_handle |           pinned_at -- ----------+-------------+------------------------------- --         3 | hansolo     | 2016-01-11 16:59:13.410657-06 -- (1 row) Let's now try the same sort of thing, but with the on conflict clause that we get with the 9.5 release. We'll pin a tweet for Rey. select * from pinned_tweets where user_handle = 'rey' ; --  tweet_id | user_handle | pinned_at -- ----------+-------------+----------- -- (0 rows) insert into pinned_tweets ( user_handle , tweet_id , pinned_at ) values ( 'rey' , 5 , clock_timestamp () ) on conflict ( user_handle ) do update set ( tweet_id , pinned_at ) = ( 5 , clock_timestamp ()) where pinned_tweets . user_handle = 'rey' ; -- INSERT 0 1 select * from pinned_tweets where user_handle = 'rey' ; --  tweet_id | user_handle |          pinned_at -- ----------+-------------+------------------------------ --         5 | rey         | 2016-01-11 17:08:52.41554-06 -- (1 row) The addition of the upsert feature means we don't have to tangle with writeable CTEs here. Instead, we get to write a standard insert statement. All we have to add is the on conflict clause. The on conflict clause is a way to tell the insert statement what to do if it is colliding with an existing record. In our case, if the user_handle conflicts with an existing record in the pinned_tweets table, then we react by doing an update instead. That's an upsert. Now that Rey has a pinned tweet, we can run the same statement as above to update to a different tweet. insert into pinned_tweets ( user_handle , tweet_id , pinned_at ) values ( 'rey' , 1 , clock_timestamp () ) on conflict ( user_handle ) do update set ( tweet_id , pinned_at ) = ( 1 , clock_timestamp ()) where pinned_tweets . user_handle = 'rey' ; -- INSERT 0 1 select * from pinned_tweets where user_handle = 'rey' ; --  tweet_id | user_handle |          pinned_at -- ----------+-------------+------------------------------ --         1 | rey         | 2016-01-11 17:09:55.41554-06 -- (1 row) This time, the on conflict clause is triggered which executes the update part of the clause. It inserts if it can. It updates if there is a conflict. I suggest choosing the new upsert feature over the writeable CTE approach for a couple reasons. First, it is conceptually simpler. Writing an insert statement with an on conflict clause is more straightforward than a writeable CTE. The writeable CTE approach feels clumsy, whereas the upsert syntax was made for this. Second, readability. The difference may seem small in the contrived example above, but when faced with a wall of SQL, the new upsert syntax is going to win in readability. Lastly, it is more performant. I performed a basic benchmark to compare the relative performance of the two approaches. Though the difference wasn't significant, the new upsert feature has an edge on the writeable CTE approach. Update (1/15/2016): a redditor ( pilif ) pointed out that the previous approaches can all suffer from race conditions. The new upsert feature is free of race conditions. We can now upsert in a way that is race-proof and performant. They linked to Why Is Upsert So Complicated? which goes into more detail. Counting You now have a basic idea of how to use insert ... on conflict ... to perform an upsert. Let's look at another example of how we might use upsert. This time we'll use it to update a count. Without fully specifying another example database, let's consider another scenario. Imagine we are trying to model an Inventory for characters in an RPG-style video game. Whenever a character picks up an item, the item is added to their inventory. Internally we will represent that with a counting join table between Characters and Items . If they are picking up a new item, we add a record with a count of 1. If they already have identical items in their Inventory , then we just increment the count. An upsert is just what we need. insert into characters_items as ci ( character_id , item_id , quantity ) values ( 1 , 23 , 1 ) on conflict ( character_id , item_id ) do update set quantity = ci . quantity + 1 where ci . character_id = 1 and ci . item_id = 23 ; -- INSERT 0 1 There are a couple of details worth discussing in this example. First, the on conflict (character_id, item_id) has two arguments. A composite key. The on conflict clause needs a unique key with which to work. A composite primary key works well in this example. This, of course, means that the characters_items table was defined with just such a composite primary key. Second, just as we saw in the previous example, we can (and should) use a where clause to specify exactly what record is to be updated. Lastly, there is the potential for some ambiguity. As such, we need to tell the update part of the statement from what table the referenced columns are coming. Declaring an alias for the table in the insert part of the statement keeps things concise. That's It? That's really all there is to the basics of upserting in PostgreSQL 9.5. There is a lot more that we can do with the on conflict clause though. We can target constraints. Instead of specifying indexed columns, we can have the on conflict specify a particular constraint as the target of a conflict. We can do nothing. If our use case calls for it, we can also opt to do nothing when there is a conflict. In this case, we don't have to specify any indexed rows or constraints. We can get fancy. In the update part of the query, we can use things like DEFAULT , expressions, and even sub-selects. Check out the latest INSERT documentation to see the full specification and to read up on all the nitty-gritty details. If you've already started using the on conflict clause in your applications, whether for upserting or otherwise, let me know how you're using it. May The Upsert Be With You! Thanks to Andrew Dennis and Craig Kerstiens for providing feedback on earlier drafts of this post. Cover image credit: Internet Archive Book Image's on Flickr Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-01-14"},
{"website": "Hash-Rocket", "title": "Custom Aggregates in PostgreSQL", "author": ["\nJack Christensen\n\n"], "link": "https://hashrocket.com/blog/posts/custom-aggregates-in-postgresql", "abstract": "PostgreSQL Custom Aggregates in PostgreSQL by\nJack Christensen\n\non\nFebruary 15, 2016 Given a bank account of debit and credit transactions, what is the greatest balance the account ever had? Given a hotel with arrivals and departures, what is the greatest number of guests the hotel ever had? Both of these are examples of finding the greatest running total. Finding the greatest running total is a great exercise with which to explore some of the lesser known features of PostgreSQL such as window functions, custom aggregates, and C functions. The Setup All code can be found on Github . For this example, we will use a simple data schema that only contains  an amount column and id column to provide ordering. create table entries ( id serial primary key , amount float8 not null ); We will use random() and generate_series() to insert 1,000,000 rows of test data. By calling setseed() before calling random() we can ensure that this code always produces the same data. select setseed ( 0 ); insert into entries ( amount ) select ( 2000 * random ()) - 1000 from generate_series ( 1 , 1000000 ); Running Total To find the greatest running total, we first have to find the running total for every row. This can be easily done with a window function. select id , amount , sum ( amount ) over ( order by id asc ) as running_total from entries order by id asc ; id    |        amount         |   running_total\n---------+-----------------------+--------------------\n       1 |     -462.016298435628 |  -462.016298435628\n       2 |      162.440904416144 |  -299.575394019485\n       3 |     -820.292402990162 |  -1119.86779700965\n       4 |     -866.230697371066 |  -1986.09849438071\n       5 |      -495.30001822859 |   -2481.3985126093\n       6 |      772.393747232854 |  -1709.00476537645\n       7 |     -323.866365477443 |  -2032.87113085389\n       8 |     -856.917716562748 |  -2889.78884741664\n       9 |      285.323366522789 |  -2604.46548089385\n      10 |     -867.916810326278 |  -3472.38229122013\n-- snip -- The expression sum(amount) over (order by id asc) can be read as sum amount for all rows ordered by id ascending from the first row to the current row. See the window function tutorial in the PostgreSQL documentation if you need a primer on window functions. Greatest Running Total Now that we have the running total for every row it should be simple to use the max aggregate function to find the greatest running total. select max ( sum ( amount ) over ( order by id asc )) from entries ; Unfortunately, we get an error: ERROR:  aggregate function calls cannot contain window function calls\nLINE 1: select max(sum(amount) over (order by id asc)) Instead we have use a subquery. select max ( running_total ) from ( select sum ( amount ) over ( order by id asc ) as running_total from entries ) t ; Here is the result: max\n------------------\n 396271.274807863\n(1 row)\n\nTime: 643.848 ms Not too bad, but there are two potential areas for improvement: query simplicity and speed. What we really want to do is this: select greatest_running_total ( amount order by id asc ) from entries ; Note the order by id asc in the aggregate. Because a greatest_running_total function would require its inputs to be ordered to be correct, it is vital that we include this clause. Custom Aggregates The greatest_running_total function doesn't exist, but PostgreSQL gives us the functionality to create our own aggregate functions . In this case, our greatest_running_total aggregate should accept float8 values and return a float8 . To create an aggregate function we first need a state transition function. This function will be called for each input row with the aggregate internal state and the current row value. The internal state needs to contain the current running total as well as the greatest running total. So we need a structure of two float8 values. Fortunately, PostgreSQL has the point type which is exactly what we need (a float8 array would also work, but point is simpler use and potentially faster). The following state transition function is implemented in PL/pgSQL . create function grt_sfunc ( agg_state point , el float8 ) returns point immutable language plpgsql as $$ declare greatest_sum float8 ; current_sum float8 ; begin current_sum : = agg_state [ 0 ] + el ; if agg_state [ 1 ] < current_sum then greatest_sum : = current_sum ; else greatest_sum : = agg_state [ 1 ]; end if ; return point ( current_sum , greatest_sum ); end ; $$ ; The point agg_state is used as a 2-element, zero-based array. agg_state[0] is the current sum; agg_state[1] is the greatest sum the aggregate has seen. We simply add agg_state[0] and the current row value el to get the new current sum. The new greatest sum is the greater of the old greatest sum ( agg_state[1] ) and the new current sum. Finally, we return a new point value with the new current and greatest sums. Because our aggregate's internal state is of type point and the output of our aggregate is float8 , we need an aggregate final function that takes the final value of the aggregate's internal state and converts it to a float8 . create function grt_finalfunc ( agg_state point ) returns float8 immutable strict language plpgsql as $$ begin return agg_state [ 1 ]; end ; $$ ; Lastly, we have to create the aggregate by providing the state transition function,  internal aggregate state type, and the final function. create aggregate greatest_running_total ( float8 ) ( sfunc = grt_sfunc , stype = point , finalfunc = grt_finalfunc ); Let's try our new function. select greatest_running_total ( amount order by id asc ) from entries ; greatest_running_total\n------------------------\n       396271.274807863\n(1 row)\n\nTime: 3386.443 ms Success! The new function returns the same result and the query is much simpler. Unfortunately, performance took a huge hit. It is now over 5x slower than before. Clearly, this is not acceptable. Custom Aggregates in C The majority of the computation is in the state transition function. What if we implement that in C? The code below is the same logic implemented in C. For more details on C extensions see the documentation . #include \"postgres.h\"\n#include \"fmgr.h\"\n#include \"utils/geo_decls.h\" \"postgres.h\" and \"fmgr.h\" are needed by all custom C functions. \"utils/geo_decls.h\" is needed to import the Point struct. #ifdef PG_MODULE_MAGIC PG_MODULE_MAGIC ; #endif The PG_MODULE_MAGIC macro that ensures extension won't load against incompatible version of PostgreSQL. PG_FUNCTION_INFO_V1 ( grt_sfunc ); Datum grt_sfunc ( PG_FUNCTION_ARGS ) { PG_FUNCTION_INFO_V1 is a macro that specifies that a function will use the version 1 calling convention. Version 1 functions always have the same signature. Point * new_agg_state = ( Point * ) palloc ( sizeof ( Point )); A Point is a C struct behind the SQL point type. It has two fields x and y . These directly correspond to point[0] and point[1] in SQL. palloc is a PostgreSQL provided function to allocate memory. PostgreSQL will ensure all memory allocated with palloc is released at an appropriate time. double el = PG_GETARG_FLOAT8 ( 1 ); Here we use the PostgreSQL provided macro PG_GETARG_FLOAT8 to extract a float8 from the second argument to the function (in C arrays are zero based so argument 1 is the second argument). bool isnull = PG_ARGISNULL ( 0 ); if ( isnull ) { new_agg_state -> x = el ; new_agg_state -> y = el ; PG_RETURN_POINT_P ( new_agg_state ); } If argument 0 ( agg_state ) is null this is the first value provided to the aggregate. Return a new state with the current sum ( x ) and greatest sum ( y ) equal to that value. PG_ARGISNULL is a macro that evaluates to true if the argument is null. PG_RETURN_POINT_P is a macro the returns a point as the result of the function. Point * agg_state = PG_GETARG_POINT_P ( 0 ); new_agg_state -> x = agg_state -> x + el ; if ( new_agg_state -> x > agg_state -> y ) { new_agg_state -> y = new_agg_state -> x ; } else { new_agg_state -> y = agg_state -> y ; } PG_RETURN_POINT_P ( new_agg_state ); } With the null state for the first row handled, it is just a simple translation of logic from PL/pgSQL: Compute the new running total ( x ), and update the greatest running total ( y ) if the new running total is larger than the old. Installing the new function takes several steps. First, the shared libary must be built and installed. The following Makefile will accomplish that task. Just run make install (PostgreSQL installed with homebrew on OSX should have all PostgreSQL dependencies installed, on Debian or Ubuntu install postgresql-server-dev-9.5 ). MODULES = grt\nPGXS := $(shell pg_config --pgxs)\ninclude $(PGXS) Next, we must create the function in SQL. create function grt_sfunc ( point , float8 ) returns point as 'grt.so' , 'grt_sfunc' language c immutable ; The final function and aggregate creation are unchanged. create function grt_finalfunc ( agg_state point ) returns float8 immutable language plpgsql as $$ begin return agg_state [ 1 ]; end ; $$ ; create aggregate greatest_running_total ( float8 ) ( sfunc = grt_sfunc , stype = point , finalfunc = grt_finalfunc ); Let's see what happens: select greatest_running_total ( amount order by id asc ) from entries ; greatest_running_total\n------------------------\n       396271.274807863\n(1 row)\n\nTime: 825.365 ms 4x faster than the PL/pgSQL version, but still a bit slower than the subquery version. Summary PostgreSQL gives us many ways to tackle problems. In the greatest running total example, the initial solution with a subquery and window function is best. However, sometimes a calculation may be exceedingly difficult without a custom aggregate. In these cases, a PL/pgSQL implementation may be ideal. However, PL/pgSQL performance can be lacking. If a custom aggregate is necessary and performance of PL/pgSQL is insufficient, a C function may be a solution. But this step should not be undertaken lightly. A bug in a C extension can crash PostgreSQL and even corrupt data. The subtleties of C and the deployment and portability difficulties of custom C are costs that should only be paid when there is no reasonable alternative. Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-02-15"},
{"website": "Hash-Rocket", "title": "Jacob (JR) Richardson is a Rocketeer", "author": ["\nStephen Caudill\n\n"], "link": "https://hashrocket.com/blog/posts/jr-rocketeer", "abstract": "Jacob (JR) Richardson is a Rocketeer by\nStephen Caudill\n\non\nOctober  3, 2013 Please help me welcome JR Richardson to Hashrocket Boulder as our newest Rocketeer. As our first hire in Boulder, JR is a home run. JR has been programming for as long as he can remember and started his professional programming career whilst still in high school. After a successful apprenticeship at Groupon he moved on to join their internal tools team, where he spent time building the tooling that ran the company, eventually open-sourcing some of that work . JR loves Ruby and having quite the academic bent is ever-passionate about Lisps (with clojure being chief among them) and is very excited about go . When he's not busy building automated code analysis tools and analyzing programming language features with a fine-toothed comb, he relaxes by playing disc-golf and brewing craft beers. You can find him on github and twitter . Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-10-03"},
{"website": "Hash-Rocket", "title": "Journey to ElixirDaze 2018", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/journey-to-elixirdaze-2018", "abstract": "Elixir Journey to ElixirDaze 2018 by\nJake Worth\n\non\nMarch 20, 2018 Last October I discovered an interesting Github issue, Elixir-Lang #6643. It was opened José Valim, the creator of Elixir. Here's part of the introduction to this issue: Elixir master now includes a code formatter that automatically formats your code according to a consistent style. We want to convert Elixir's codebase to use the formatter on all files. We understand this means a lot of code churn now but, on the positive side, it means we can automatically enforce and/or format future contributions, making it easier for everyone to contribute to Elixir. 1 This issue set off a massive refactoring project to get Elixir conforming with its new formatter. Today, new pull requests to Elixir must pass through the formatter without requiring changes. I think this will have a ripple effect on every Elixir project in production. While closing this issue, José provided the following statistics on the refactor: 425 files were changed 214 pull requests were opened 84 committers contributed 368 commits were added Most interestingly to me, the entire project took just four days to complete. I think this shows the power of the community and the effectiveness of the tool. This is a big change that affects everybody who writes Elixir. As an open-source Elixir project maintainer, it led me to some questions, such as: what is autoformatting? Why should I care? What is it going to do to my code? After some research, my conclusion is we should all format our Elixir code now. It's the future of this language. I started thinking about this tool and its importance a lot this year, leading to this blog post . Continued investigation led to a talk that I presented at ElixirDaze 2018 in Denver, CO. I focused on the history of autoformatting, arguments for and against the practice, and practical applications for a production Elixir application. Want to learn more? Check out the recording of my talk at ElixirDaze, and follow along with the slides . Elixir mixologists of every experience level will leave this talk with a better understanding of this important tool, and a deeper grasp of the persistent debate around code style. Thanks to ElixirDaze, the great attendees I met, my friends at the ChicagoElixir Meetup for vetting my talk, and Hashrocket for continuing to sponsor my public speaking adventures. Photo credit: unsplash-logo Samuel Zeller José Valim, Github Elixir-Lang Issue #6643, https://github.com/elixir-lang/elixir/issues/6643 . ↩ Was this post helpful? Share it with others. Tweet Share Post", "date": "2018-03-20"},
{"website": "Hash-Rocket", "title": "Put your programming tools in a Toolbox", "author": ["\nMicah Cooper\n\n"], "link": "https://hashrocket.com/blog/posts/put-your-programming-tools-in-a-toolbox", "abstract": "Ruby Put your programming tools in a Toolbox by\nMicah Cooper\n\non\nNovember 14, 2013 There are certain files, scripts, and snippets (gists) that we ofter use on many projects we create- especially with rails projects. I find using gists for these types of files cumbersome. I propose another approach: The birth of my Toolbox I was teaching a collection of people how to make a rails application at a RubyJax meetup the other day and I found my self in an interesting situation- There was a bit of code that I needed to move forward through the app but I didn't want to spend the time actually writing it. So I decided to put this code online so the \"students\" could simply download it (they were following along on their own machines) when we got that that point. I started by putting a bunch of gists on github. Next I needed to have a list for everyone to reference where to get these files. Then I decided to write a README file for each gist so people could go back later and understand how to use this code. Now I have a bunch of gists, all with different, unintelligible urls, and a bunch of of READMEs and no where to put them. To solve this, I created a repo called toolbox, and filled it with the READMEs. In each README I had the link to the gist. To make it a little easier I even created a 'curl' command with the file so they didn't have to think about the less important parts of the learning session. This was great- until I had to update all these gists and found meself with 8 different repos to deal with. So naturally I stuck them all into one repo with along with the READMEs and I had myself a toolbox. I see my toolbox as a place to maintain and share all my gists. Also, I think it could be a source of pride for people liking to show of their tips and tricks! One repository is a better facility for that than a list of gists in my opinion. Using my tools This toolbox worked out pretty well for the Rubyjax session. But I wanted a way to to grab my tools from the command line that was more convenient then curling them. So I did what every overengineering Rubyist does- I created a gem cleverly named Rench to grab the tools from my toolbox. Now, whenever I want to grab a tool from my toolbox, whether it's for a new project or teaching the a group of people, all I need to remember is a github username: gem install rench\n\nrench mrmicahcooper And Rench will show me all the tools in that user's toolbox: => Choose a file: [ 0] active_record_spec_helper.rb [ 1] formbuilder.rb [ 2] html5.js [ 3] mixins.sass [ 4] step_definitions.rb [ 5] support_paths.rb [ 6] ui_controller.rb [ 7] ui_index.html.haml [ 8] whitespace-reset.scss So I can download them to wherever- easily.\nI think this approach is hugely convenient. Give it a shot!  Let me know what you think. http://github.com/mrmicahcooper/rench Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-11-14"},
{"website": "Hash-Rocket", "title": "Caching ActiveModel::Serializers on Heroku", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/caching-activemodel-serializers-on-heroku", "abstract": "Ruby Caching ActiveModel::Serializers on Heroku by\nPaul Elliott\n\non\nAugust  5, 2013 If you've been using ActiveModel::Serializer, then you're in for a treat. Rails' caching is now a first class citizen of the library, and enabling it couldn't be easier. I like to define global serializer settings in an ApplicationSerializer and then extend it in all my other serializers. That way, everything is automatically cached – and since we're delegating the cache_key to the underlying object, the serializer will use whatever the object determines that to be. It can go in app/serializers and would look like this: class ApplicationSerializer < ActiveModel::Serializer\n  cached\n  delegate :cache_key, to: :object\nend If your model is ActiveRecord based, the cache key will look like this by default: table_name/id-updated_at_timestamp Since the updated_at timestamp is in there, it should automatically bust the cache if the object is changed. It gets a little more complicated if you have nested models, though: changing a child record won't bust the cache of the parent unless you touch it whenever you make a change, like so: belongs_to :parent, touch: true The Cache Store In development and test, you can enable Rails' in-memory cache store. It comes with 32MB by default, which should be plenty for local development. Add the following line to your config/development.rb and config/test.rb and you're done: config.cache_store = :memory_store That isn't going to cut it for your staging and production servers, though. The easiest thing to do is add memcached to those environments. If you're running on AWS, you can get a developer MemCachier instance for free. That should be fine until you have serious traffic. On Heroku, it's as simple as this: heroku addons:add memcachier:dev Now you'll need to set up your application to actually use memcached. Just as we should expect, this is super easy. Just add this to your Gemfile and bundle ... gem \"dalli\"\ngem \"memcachier\" ... and add this to your environments/production.rb and staging.rb. config.cache_store = :dalli_store That's it! Enjoy a practically free speed boost for your API! Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-08-05"},
{"website": "Hash-Rocket", "title": "Congrats to our Client Satellite on Adobe Acquisition", "author": ["\nChris Cardello\n\n"], "link": "https://hashrocket.com/blog/posts/congrats-to-our-client-satellite-on-adobe-acquisition", "abstract": "Congrats to our Client Satellite on Adobe Acquisition by\nChris Cardello\n\non\nJuly 31, 2013 A great client with an even more amazing product have been acquired by Adobe. It was two years ago when I first met Lee Blankenship and Evan LaPointe from Search Discovery. They were coming in to story card a new project which would eventually become the product 'Satellite' . We're proud to say that Satellite was recently acquired by Adobe. As a member of the project team who worked on the Satellite project, I had the pleasure of working with them and the great team at Search Discovery since that first colorful story carding session. We held a discovery call the week prior to story carding, as we would typically do for any project. However, it wasn't until we actually got them in our conference room that we really were able to get a good grasp on what it was they were doing, and it was revolutionary. They had developed the Satellite engine prototype for analytics tracking. This engine allowed them to track page events with more granularity than had ever been offered, while removing the need for marketing tags in markup. At the time, they were configuring the engine manually, and they wanted us to build out the web frontend, which would allow users to configure the rules the engine would follow. In our standard fashion, we quickly moved through development of their MVP and they went to market. With the power of their engine and intuitive UI, it wasn’t long before they had a burgeoning userbase clamoring for more features and tools to assist in their tag management needs. We worked with Evan and Lee to identify, interview, and train their own development staff to take over day-to-day application development. For around 6 months, the Satellite developers would come to our Jacksonville office off and on to pair with us and immerse themselves in the domain and codebase. This personal training served to expedite ramp-up time on such a complex application, and it wasn’t long before we were only being utilized on as needed basis, proud of the assistance we were able to provide Search Discovery in helping the Satellite team succeed. This week's announcement that Adobe acquired Satellite and its resulting media coverage has come as no surprise to us. The folks at Search Discovery developed software which will truly revolutionize tag management and the way companies go about obtaining their analytics data. The words “revolutionary” and “disruptive” get thrown around in today’s tech community so much that they hold little meaning. However, when we look back at what Satellite has been able to accomplish, those words seem perfectly appropriate. Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-07-31"},
{"website": "Hash-Rocket", "title": "Taking Advantage of the Polyglot Lifestyle", "author": ["\nJohnny Winn\n\n"], "link": "https://hashrocket.com/blog/posts/taking-advantage-of-the-polyglot-lifestyle", "abstract": "Ruby Taking Advantage of the Polyglot Lifestyle by\nJohnny Winn\n\non\nAugust 25, 2013 If you haven't heard yet or you've simply been under a rock for the past several months, there is a new functional language getting quite a bit of attention. Elixir is the brainchild of José Valim and it's built to run on the Erlang Virtual Machine (EVM). It's a highly meta-programmable, dynamic language with an elegant and flexible syntax. Since Elixir compiles to the same bytecode as Erlang, you can make calls directly to Erlang from Elixir. It also leverages the power of Erlang to build concurrent, distributed, and fault-tolerant applications. At best it could drive the next big functional revolution and at worst it is a fun language to work with. Now that you have had the Elixir elevator pitch, it's on to the business at hand... Learning a variety of languages is a great way to enhance your skill set and bring experiences back to your primary language. Not only that, but learning different programming paradigms can allow you to look at problems from new angles. To quote Abraham Maslow, \"if you only have a hammer, you tend to see every problem as a nail\". However, learning a \"new\" language can be cumbersome or intimidating when comfortable features aren't available. Never fear because that gives you the chance to take advantage of the Polyglot Lifestyle. What's that? You haven't heard about the Polyglot Lifestyle? A polyglot is simply someone that knows and can use several languages. To make it a lifestyle you need to exercise those languages and what better way then to use one language to learn another? The dynamics of Ruby provide a great springboard for working with other languages. Since there are many pieces to the puzzle already build you can focus on learning without distractions. That's not to say that a little challenge isn't expected or even necessary, but if you can focus on what the language offers without concerning yourself with what it doesn't, you are free to explore without the distraction of incompleteness. Keep in mind, learning a new language doesn't mean that it will be replacing your current language but you may find new was to utilize the tools that it offers. I tend to follow a progression when learning a new language. Starting out with the basics and moving into more complicated applications. Inevitably, I create a blog web application. This isn't a unique pattern but that's because blogs offer a chance to explore several aspects of how the language behaves within the confines of the web. Also, for the most part, I'm a web developer so the most common way in which I would use a language would be building web applications. My experience learning Elixir was no different. Elixir is still pre 1.0 so this post isn't going to teach you Elixir. I would suggest you check out the Programming Elixir book by Dave Thomas . It's also not an authoritative tutorial for Dynamo , Elixir's web framework, or Ecto , its data query library for postgres. That would also be a bit premature. However, this will show you how to use Ruby to supplement missing features while learning Elixir. First Things First, but Not in that Order The first thing would be to setup your Elixir environment with the correct versions of Erlang, Elixir and Ruby. This post assumes the following versions: Erlang R16B01, Elixir 0.10.1-dev and Ruby 2.0. Dynamo and Etco both require the latest versions of Elixir so staying current isn't only a nicety, it is also a requirement. To find the details on installation and project setup review the latest documentation. Ruby is added to handle ancillary support for data migrations and behavior-driven development. Data migrations are great not only for keeping the database up-to-date but we can use them to populate and clean our test data. Behavior-driven development is just something we do. And Now We Begin Although setting up a Dynamo project is straight forward we need to add the Ecto dependency to query the database. Start by opening the mix.exs file and adding the following: defp deps do [ { :cowboy , github: \" extend/cowboy\" }, { :dynamo , github: \" elixir-lang/dynamo\" }, { :ecto , github: \" elixir-lang/ecto\" }, { :pgsql , github: \" semiocast/pgsql\" } ] end The pgsql dependency is an Erlang library that is required by Ecto. This also illustrates the seamless integration between Elixir and Erlang. Calls to the Erlang library can be made directly from your Elixir code. After adding the dependencies to the mix file, we need to run mix deps.get from our shell to pull the dependencies and compile. Additional information on setting up the Ecto.Repo and connection is beyond the scope of this post but can be found in the Ecto README on github. Finally, to test that our server is running, we can run mix server and navigate to localhost:4000 to see the default Dynamo page. Now with a Dynamo project up and running with database support, we can begin incorporating Ruby into our project. Because we are simply adding Ruby we will create a .ruby-version file in the project root and specify the version. Next we add a Gemfile along with our first gems. ruby '2.0.0' group :test do gem 'cucumber' , require: false gem 'selenium-driver' gem 'capybara' gem 'rspec' end Configuring Capybara for Cucumber Our cucumber steps will use capybara to simulate the user interactions but we need to give it some direction. Create a support directory in the test\\features folder and add a cucumber.rb file. Open the file and add the following requirements: require 'capybara' require 'capybara/cucumber' require 'capybara/rspec' This allows us to call capybara methods from our step definitions. However, this only supplies part of the solution because capybara will be looking for a server that doesn't exist. We can prevent capybara's default behavior by changing the default settings. require 'capybara' require 'capybara/cucumber' require 'capybara/rspec' Capybara . run_server = false Capybara . app_host = 'http://localhost:8888' Capybara . default_driver = :selenium You'll notice that we set the app_host port to 8888. When Mix runs in the test MIX_ENV , the server is actually spun up on port 8888 so we want capybara to be looking at the correct environment. Just directing capybara to the correct server isn't enough to start the test environment. We will need to add a Mix test to initiate cucumber within the context of mix. Add a file to the test directory named cucumber_test.exs and the following code: Code . require_file \" ../test_helper.exs\" , __FILE__ defmodule CucumberTests do use Blog . TestCase setup do Blog . Dynamo . run :ok end test :run_cucumber_features do Mix . shell . cmd ( \" cucumber test/features --format progress\" ) end end This is a fairly basic ExUnit test but lets take a second to review what's actually happening. First, the setup macro will start our server with Blog.Dynamo.run which will spin up to listen on port 8888. Then we define our cucumber call out in the test macro. In this scenario, I prefer to format my cucumber output as progress to reduce the noise when running the test suite. Note here that the test macro will take either a string or an atom when defining a test. We could just as easily said: test \" Run Cucumber Features\" do ... end Moving on to Migrations With our test suite set to run, the next hurdle is handling the migrations. Assuming that you have created two databases, one for development and one for testing, we can add a Rakefile to the root and define our migration tasks. Since our migration solution is temporary, having the tasks in a single file will suffice. However, we do want to consider where we are storing the migration files. Create the directories db/migrations in the project root and add the following to the Rakefile to require any migrations within the path: #!/usr/bin/env rake require 'active_record' Dir [ 'db/migrate/*' ]. each do | file | require File . expand_path ( file ) end The rake tasks will be defined within the db namespace but we have multiple tasks to run based on the environment. For example, whether we are in development or test, we may need to migrate or rollback and we will need to establish a connection. Both establishing the connection and running the migrations can be DRY'd up into two methods: namespace :db do def connection ( config ) ActiveRecord :: Base . establish_connection ( config ) ActiveRecord :: Migration . verbose = false end def run_migrations ( direction , config ) connection ( config ) ActiveRecord :: Migration . descendants . each { | m | m . send ( direction ) } end end The connection method takes a configuration hash and establishes the connection through ActiveRecord. The run_migrations method opens a connection and then iterates through the descendants of ActiveRecord::Migration to call a directional method, i.e. up or down . Here is an example of the configuration and both a migrate and rollback rake tasks: namespace :db do # code removed PG_CONFIG = { :adapter => 'postgresql' , :host => 'localhost' , :database => 'blog_dev' , :username => 'postgres' , } task :migrate do run_migrations ( :up , PG_CONFIG ) end task :rollback do run_migrations ( :down , PG_CONFIG ) end end When we run rake db:migrate or rake db:rollback it will perform the proper actions defined within our migration methods. Now we can scope the migrations to the test database by simply merging the database name into the configuration hash in the test namespace. namespace :db do # code removed namespace :test do def config PG_CONFIG . merge ({ :database => 'blog_test' }) end task :prepare do run_migrations ( :up , config ) end task :rollback do run_migrations ( :down , config ) end end end Putting the Pieces Together The Rakefile is great for keeping the databases current but testing presents a different set of hurdles. How can we generate data to test against? How do we clean up between tests? These are actually easily solved problems. Within our features/support directory we can add the active_record_connection.rb file with the following: require 'active_record' PG_CONFIG = { :adapter => 'postgresql' , :host => 'localhost' , :database => 'blog_test' , :username => 'postgres' , } ActiveRecord :: Base . establish_connection ( PG_CONFIG ) ActiveRecord :: Migration . verbose = false When cucumber runs it will load the file and open a connection to the test database. As we create entities to test we can also add those files to this directory. Here is an example of a migration to insert and delete posts: class Posts < ActiveRecord :: Migration def self . up execute %q{\n      INSERT INTO posts (headline, content)\n      VALUES ('A Fishful of Dollars', 'Mo money, mo fish');\n      } end def self . down execute %q{DELETE FROM posts;} end end In our step definitions we can call Post.up to insert a record and to clean up after our cucumber scenarios we can add the following to our cucumber support file: After do Posts . down end This can be made more verbose but it's a start that can help you on your way to living the Polyglot Lifestyle. Experimenting with new languages can present a unique set of challenges but you can use the knowledge you have to ease the transition. The important takeaway is to keep learning and testing your skills. That is the best way to stay sharp. Be sure to take a look at the full Elixir/Ruby mashup on github . Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-08-25"},
{"website": "Hash-Rocket", "title": "Oh, give me a home...", "author": ["\nStephen Caudill\n\n"], "link": "https://hashrocket.com/blog/posts/oh-give-me-a-home", "abstract": "Oh, give me a home... by\nStephen Caudill\n\non\nSeptember 26, 2013 At 300 days of sun a year, Colorado is most definitely a place where the skies are not cloudy all day .  A state boasting one of the best places to live in America , wonderful school systems and is also a great place for startups. According to a recent entrepeneur.com article , Colorado takes 4 of the top 25 spots for best cities for tech startups (including first and second place), making it safe to say that it's a great place for tech. So, Colorado is great and all, but why are we belaboring the point? Because it\nserves as good background information about the following announcement: Hashrocket is expanding to Boulder, Colorado I'm super pleased to announce that we've found a new home in Boulder. And when\nI say \"home\", I mean it. Hashrocket is a lifestyle company. We've grown\nsustainably for years now and we're not looking to get acquired by a product\nshop. Hashrocket is here to help businesses build awesome stuff and to help\nits employees hone their respective crafts. Expanding to Boulder and bringing our office count up to 3 gives Rocketeers\nmore options of how and where to live within the company they love. We're firm\nbelievers that excellence in our work life is fostered by quality of personal\nlife. This is one more way we're trying to make that achievable. Home on the range We're releasing this announcement on the same day as the start of Rocky\nMountain Ruby Conf and it's no coincidence, as it's part of what helped\nus fall in love with Colorado in 2012. We're in attendance and we'd love to\nchat with you about anything and everything. We couldn't be more pleased to\nbecome a part of this great community and can't wait to meet you all. If you can't make it by Rocky Mountain Ruby Conf, feel free to stop in at our\ntemporary office space in the Scrib coworking facility. Speaking of which, if you\nknow about some great space in or around the Pearl Street Mall, let us know ;) Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-09-26"},
{"website": "Hash-Rocket", "title": "Anatomy of the new Hashrocket: Frontend Design", "author": ["\nCameron Daigle\n\n"], "link": "https://hashrocket.com/blog/posts/anatomy-of-the-new-hashrocket-frontend-design", "abstract": "Javascript Design Anatomy of the new Hashrocket: Frontend Design by\nCameron Daigle\n\non\nAugust 16, 2013 As you may have noticed, we launched a new version of the Hashrocket site recently. This new design is a ground-up redesign & rewrite of the frontend – much of the backend was rewritten as well, but I'm a design guy, so if you want a backend breakdown, you'll have to buy Polito an ice cream cake or something. Here's some cool stuff about the frontend. Liquidity The Hashrocket site is designed to look good on a continuous spectrum from 1200px wide down to 320px. We don't target specific devices or widths – responsive sites are about making sure a site looks good at any size. Base styles are designed for the widest viewport, and additional styles override those as the site gets narrower. We've found it's much more straightforward to design first for more complex wide layouts and then remove attributes (columns, for example) as the page becomes narrower than vice versa. You can learn more about our patterns for responsive design in this previous post of mine , but the tl;dr of it all is that you should design wide, override narrow. Retina Images Only certain images on the new Hashrocket site are doubled up for retina: the header logo, for one, because I wanted to make sure it was sharp and not being resized by the browser. So the logo maintains its size no matter the width of the device, and the spacing around it tightens up as the window gets narrower. The open source icons on the Community page and contact forms are doubled for the same reason – and the social icons in our footer & people page are simply SymbolSet characters. All other images on the site are not doubled up for retina. For example, each illustration on the Process page is in a column set to a width of 25% (actually 23.5%, thanks to additional margin) – so even though the images themselves are 360x190, even at that page's widest point (1200px), they're only rendered at 282x149. We found that was enough breathing room for the images to look nice in retina as well as all browser sizes. The same sort of thing applies to the slideshows on the client detail pages and homepage. The homepage features a slideshow where the images are actually 1200x600 despite being rendered at 862x431 at their largest non-retina size – a compromise to allow for decent file sizes without depending on a second set of slideshow images loaded through a Javascript call or media query. I think this will happen more and more as retina becomes the standard: it will be more important for images to have exact proportions rather than exact pixel sizes, because pixel accuracy won't be as much of a concern. Fun with background-size The header backgrounds on our site (for example, the astronauts on the team page ) are all being displayed smaller than native, and thanks to background-size , they always fit the content area properly. This took a few tries to get right. The height of that header area is dependent upon the content, and the width is obviously variable all the way down to mobile, so multiple size-specific background images would be a pain (and result in jarring transitions as you resize your browser window) – and fixed-pixel background sizes would be inconvenient to maintain. The solution was actually far simpler than I initially realized. The header section just has a background image with a background-size of auto 90% – that is, auto width, 90% of height. So as the page gets narrower, the background image gets smaller, and if the text gets longer, the background image grows to fit. Piece of cake, actually – it just took some getting used to to think of a background image as such a fluid element. The Cycle2 Plugin Our slideshows are courtesy of the indomitable Cycle2 plugin, combined with the Carousel transition plugin. The original Cycle plugin (contributed to by our own Shane Riley) was solid and used often on our projects, but Cycle2 adds the ability to handle varying sizes, and does so very easily. Here's how that works on our homepage. First, we have a couple of divs enclosing a set of images: .slides .pano - ( 1 .. 12 ). to_a . shuffle . each do | i | = image_tag \"img_home_ #{ i } .jpg\" We call the cycle plugin. There's a little more going on here (events, a little cuteness to center the first slide), but this is the main initialization: this . $el . cycle ({ fx : \"carousel\" , paused : true , carouselVisible : 3 , carouselFluid : true , swipe : true }); Here's the trick. The carousel plugin has a carouselVisible parameter that controls how many slides are shown at once. This is how it calculates the height of the carousel – we don't set that ourselves. However, we don't want 3 whole slides visible – we just want the edges of the left & right slides to be visible. So, we do this: .slides overflow : hidden .pano margin : 0 -40% The negative margin on the .pano accomplishes what we want. The Cycle2 plugin is still sizing for 3 slides, but we're just letting those bleed off of the edges. Additionally, at a certain width, we don't want to show the edges of the left & right slides anymore. So, using the media query mixin detailed in my aforementioned post about that : +max-width .slides .pano margin : 0 -100% Voila! Clean and nice. Etc. So there you have it. It's really exciting to have launched something that I consider to be such a huge step forward from our old site. There are other interesting bits & pieces out there – I'll leave the implementation of the faux-chart on our homepage as an exercise for the reader – but those were some of the main tricks & techniques we used. Thanks for reading. Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-08-16"},
{"website": "Hash-Rocket", "title": "Book Club with Sandi Metz", "author": ["\nAndy Borsz\n\n"], "link": "https://hashrocket.com/blog/posts/book-club-with-sandi-metz", "abstract": "Ruby Book Club with Sandi Metz by\nAndy Borsz\n\non\nMay  3, 2013 May is here and it's time for book club! Read Practical Object-Oriented Design in Ruby this month with the Rocketeers and submit your questions for a community-driven interview with author and Ruby Hero Sandi Metz ! We're very excited to spend the month with Sandi's buzzworthy book and hope you are too. What's this book all about? From the POODR website: Practical Object-Oriented Design in Ruby is about how to write object-oriented code. It was written by an everyday programmer and it explains object-oriented design (OOD) using realistic Ruby examples. OOD is not a mysterious black art that’s impossible to understand, it’s just stuff you don’t yet know.   POODR is a practical, readable, and understandable introduction to how OOD can lower your costs and improve your applications. If your code is killing you and the joy is gone, POODR has the cure. How do I submit my questions for the interview? Write your questions in a Github gist . Tweet the gist URL to @hashrocket with the hash tag #hrbookclub . We'll publish the interview at the beginning of next month. Submit your questions now through Friday, May 24th 2013 for inclusion in the interview We look forward to your questions and happy reading! Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-05-03"},
{"website": "Hash-Rocket", "title": "Stop, Collaborate and Listen", "author": ["\nBrandon Farmer\n\n"], "link": "https://hashrocket.com/blog/posts/stop-collaborate-and-listen", "abstract": "Stop, Collaborate and Listen by\nBrandon Farmer\n\non\nJuly  5, 2012 When pairing, better communication equals better code. Communication is easy in theory, but difficult in practice. I am an introvert by nature, but even though I know listening is an important communication tool, I need to learn to take an active role. One of the important new developments in my life is working for Hashrocket. I now work in a pair constantly, which is drastically different from programming alone. Pair programming is about constant communication to create better software. Over the last few months, I have learned a lot about what communication is and what it is not. I’ve learned that you must be willing to admit mistakes, and understand that disagreements are not personal attacks—they are part of the communication process. I’ve learned that communication is really about understanding. Understanding is hard because it takes patience and willingness to devote attention to others. As a programmer, this is challenging because I am so focused on solving a technical problem, I sometimes fail to communicate. I have now discovered that there are many times when a deep breath and calmed down description of the problem can help everyone to rethink the problem and to ensure that you are really working on the same problem. Another part of communication is understanding people’s quirks. I have been fortunate enough to work with a great set of programmers who, for the most part, share my sense of humor. However, I realize that you have to be aware how your pair reacts to jokes and respect their boundaries. This comes back to being aware of your pair, and knowing that everyone may not like the same things, or see the world the same way you do. My simplified list of communication rules is that you pay attention, try to have patience, and know that your pair is working toward a common goal. I think that if you use these rules (and a little bit of compromise) you really can communicate and in my case, make better software. Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-07-05"},
{"website": "Hash-Rocket", "title": "Your Code Should Tell a Story", "author": ["\nJohnny Winn\n\n"], "link": "https://hashrocket.com/blog/posts/sandi_metz", "abstract": "Your Code Should Tell a Story by\nJohnny Winn\n\non\nAugust 22, 2012 Last week Sandi Metz paid a visit to Hashrocket to share her knowledge and perspective on a variety of topics around life, learning and a little design too. As a proponent of Object Oriented Design, Sandi has led the movement to embrace sound practices in \"good\" code design within the Ruby community. In dynamic languages such as Ruby it is easy to get lost in a sea of code, especially for novice developers. However, when a developer is armed with an understanding of OOD they are free to explore the applications that they create. Objects become just that, objects. They are decoupled, behavior is abstracted and the messages emerge as the relevant focal point of testing. You are free to make a “mess”! Regardless of your level of expertise, Sandi does a phenomenal job of explaining OOD and summarizes code expectations with a simple statement: \"Your code should tell a story\". Everyone at Hashrocket thoroughly enjoyed Sandi Metz's visit and look forward to many more. Be sure to check out her new book Practical Object Oriented Design in Ruby due out in September and keep a lookout for the Sandi Metz lunch & learn series coming soon! Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-08-22"},
{"website": "Hash-Rocket", "title": "Announcing \"In The Spotlight\"", "author": ["\nKaz Sheekey\n\n"], "link": "https://hashrocket.com/blog/posts/announcing-in-the-spotlight", "abstract": "Announcing \"In The Spotlight\" by\nKaz Sheekey\n\non\nJune  5, 2011 We're happy to announce our new video series: 'In the Spotlight'. In this series, we interviewed a variety of talented folks at RailsConf to find out what they've been up to and what interesting products are springing up in the rails community. Here's the first of many videos: We'll be posting a new video each day to the In The Spotlight channel on the Hashrocket Vimeo . Hope you enjoy! Was this post helpful? Share it with others. Tweet Share Post", "date": "2011-06-05"},
{"website": "Hash-Rocket", "title": "Hashrocket University 2011 Recap", "author": ["\nAdam Lowe\n\n"], "link": "https://hashrocket.com/blog/posts/hashrocket-university-2011-recap", "abstract": "Hashrocket University 2011 Recap by\nAdam Lowe\n\non\nJune 14, 2011 This year, we were fortunate enough to host our second Hashrocket University course on Rails development. Pair programming is a core piece of our learning culture and sharing of knowledge. While there are a few opportunities through Craftsmanship shops and apprentice programs to engage in this immersive style of learning, the majority of short-term training available to the Ruby community is classroom-driven. We sought to turn that model on its head by offering a small class size, top-end workstations, and a 1:1 Rocketeer-to-student ratio. This allows us to provide attendees with a private tutor for two full days of pair programming focused on the areas where they personally want to improve and dig deep. We interviewed the attendees to get their feedback, and this is what they had to say: Was this post helpful? Share it with others. Tweet Share Post", "date": "2011-06-14"},
{"website": "Hash-Rocket", "title": "The Synchronized Project", "author": ["\nAdam Lowe\n\n"], "link": "https://hashrocket.com/blog/posts/the-synchronized-project", "abstract": "Design Process The Synchronized Project by\nAdam Lowe\n\non\nJuly  2, 2012 Design, story carding and development should be transparent and closely integrated parts of a project. Unfortunately this is not always the case. At Hashrocket we have put a lot of effort into a set up and a process that allows us to keep these efforts in sync for a project. Give this post a read for some insight into how we accomplish this. Background Our process at Hashrocket has evolved greatly over the years and one of the areas that I think has been key in our success is how closely we have integrated design, story carding and development. In many environments a list of requirements will be put together for a business case, taken to someone for design, then technical requirements or story cards are distilled from the design and finally it is all handed to developers to build. It may not be that exact order but unfortunately the process is frequently that disconnected. The Problems designers build something in photoshop that is painful or not feasible from a development perspective development is done without consistent design input leading to a poor user experience design and development do not communicate efficiently, become combative or simply waste tons of time due to disjointed communication The Solution At Hashrocket at the very start of a project we bring the stakeholder in to work with a team comprised of a designer, a project manageer and a pair of developers. As the stakeholder describes the application's features we wireframe their ideas and write the story cards that describe those interactions. This is a transparent process where everyone can see what is being worked on in real time. This goes a long way to prevent confusion and miscommunication. This allows for tighter iterations and course corrections with design and development in the same loop. Design is able to help guide feature development towards the best user experiences. Development is able to provide feedback towards the best use of technology to accomplish those interactions. This is accomplished within the shared context of ensuring everyone is working towards tangible value for the stakehodler. The Set-Up Everyone on the team together Large marker board 50\" television with a mac mini attached Mac mini screen sharing with designer's laptop to display wireframes on the television as they work Project manager connected to mac mini via tmux to display story cards as they are written Repeat as necessary with daily stand ups involving everyone. The Result A synchronized effort that sets a project up for success. Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-07-02"},
{"website": "Hash-Rocket", "title": "Best of TIL Year One: Ruby", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/best-of-til-year-one-ruby", "abstract": "Hashrocket Projects Ruby Best of TIL Year One: Ruby by\nJake Worth\n\non\nJune 28, 2016 Here are some of the top Ruby posts from Today I Learned . My goal with this series is to pause and highlight some of the top posts from\nthe first year of Today I Learned . Today we'll look at Ruby, our third-most active channel. Hashrocket invested early in Ruby, and it remains the lingua franca of our company.\nThough many of us enjoy hacking on JavaScript, Elixir, Go, Clojure, and other languages, Ruby is still\na well-worn and trusty tool. We travel to Rails Conf and Ruby Conf\neach year, and host our own annual Ruby conference in St. Augustine. There's a cache at Hashrocket to an elegant line of Ruby, because it's a language we all know well. Here are the top five most liked Ruby posts, in order, from the first year of Today I Learned. Enjoy! Ruby String Mutability (Vinicius Negrisolo) Until Ruby 3 we need to explicitly call the method freeze on literal strings, so they become immutable. And, if you have a lot of literal strings in a file, this will be very repetitive and verbose. In order to let our code cleaner there is a magic comment that can be added in the top of each file. the magic: # frozen_string_literal: true And it is done, all literal string are frozen now :) example: class Unfrozen def foo 'bar' end end class StringFrozen def foo 'bar' . freeze end end # frozen_string_literal: true class ClassFrozen def foo 'bar' end end To test that: require 'spec_helper' describe 'Ruby String Mutability' do it 'validates string mutability' do expect ( Unfrozen . new . foo . frozen? ). to be false expect ( StringFrozen . new . foo . frozen? ). to be true expect ( ClassFrozen . new . foo . frozen? ). to be true end end Randomized with seed 51265 . Finished in 0.00179 seconds ( files took 0.45396 seconds to load ) 1 example, 0 failures \\o/ Ruby Retry- Where you been? (Micah Cooper) For some reason, I never knew about ruby's retry keyword. The more you know... def api_request TwitterWrapper . make_request # Throws a ServiceUnavailabe(506)- Server overloaded rescue ServiceUnavailable => error retries = retries . to_i + 1 # Increment a retry counter retries < 5 ? retry : raise ( error ) # run the method again until \"retries is exceeded\" # notice the local variable \"retries\" is persisted through retries end You could put a sleep in there if you wanted to wait a certain amount of time before retrying. h/t Vinicius Negrisolo Percent Notation (Josh Branchaud) Ruby has many uses for the % character. One of the more obscure uses is as\na notion for custom delimited strings. Use the percent notation with a\nnon-alphanumeric character to surround a string. & gt ; %=Jurassic Park= =& gt ; \"Jurassic Park\" & gt ; % Ghostbusters =& gt ; \"Ghostbusters\" It even works with balanced characters & gt ; %(The Goonies) =& gt ; \"The Goonies\" This is useful for defining a string that has both types of quotes & gt ; %[That'll be the \"day\"] =& gt ; \"That'll be the \\\" day \\\" \" It's also useful for creating horribly obfuscated code & gt ; %=what= == %?what? =& gt ; true h/t Josh Davey Ruby array shortcuts - \"&:\" and \"&method\" (Vinicius Negrisolo) Call a method on every items with &: So this: [ :foo , :bar ]. each do | item | item . to_s end Can be reduced to: [ :foo , :bar ]. each ( & :to_s ) But, what if you want to call a method for each item in an array, and this item should be a parameter for this method? Call a method with every items as a parameter with &method So this: [ :foo , :bar ]. each do | item | puts ( item ) end Can be reduced to: [ :foo , :bar ]. each ( & method ( :puts )) A high level view of RSpec tests (Chris Erin) Test files in ruby/rspec can grow to gigantic soul crushing sizes, which makes it hard to really get a sense of what tests are in the file and where.  This is troublesome when trying to determine a sensible place to add a new test to the already gigantic file. To get a better sense of the structure of the file you can combine the dry-run and format options for readable, hierarchical documentation in a small amount of time. $ rspec -fdoc --dry-run specs/my_massive_test_file_spec.rb Conclusion Thanks to Vinicius, Micah, Josh, and Chris for these posts. Today I Learned had a spike in traffic near the beginning of the year, and these posts are mostly from that time. But there's a lot of great Ruby tips from earlier. See them all here: https://til.hashrocket.com/ruby Keep instantiating those objects, and learning every day. This blog post is part three of a series; here's part one and two . Next, we will look at the top command line posts from year one. Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-06-28"},
{"website": "Hash-Rocket", "title": "Titled URL Slugs in Phoenix", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/titled-url-slugs-in-phoenix", "abstract": "Elixir Phoenix Titled URL Slugs in Phoenix by\nJake Worth\n\non\nMarch 21, 2017 Today I Learned has a feature I love that\nwe call 'titled slugs'. This week I reimplemented it in Phoenix. Here's an overview of the feature. Visit any post on 'Today I Learned', and\nthe URL looks like this: https://til.hashrocket.com/posts/61e2f0db67-logrotation-for-a-rails-app 61e2f0db67 is the url_slug for the post, a unique string that does not\nchange. But what's with the rest of the URL? -logrotation-for-a-rails-app is\na slugified version of the title of the post. It makes the URL easier to read,\nand might improve our SEO in some magical way. Bur what happens if an author changes the title? The slugified version of the\ntitle should change, altering the URL and breaking any old links to the post. To test this scenario, visit this link, and look at the URL in your browser: https://til.hashrocket.com/posts/61e2f0db67-actually-irrelevant Surprised? The feature I want to discuss today allows you to remove or replace\nthe title part of the slug, and still load the post. It works because the\ntitled slug is the implied parameter for the post, but we look up\nthe post by the slug itself , without the title. The titled\nslug is used just for display; while the slug alone makes the post unique. This is a nicety that I wanted to preserve in our Phoenix\nport, Tilex , of 'Today I Learned'. Here's the\ncode I wrote to make that happen (Phoenix 1.2.1). Change the Parameter In Ruby on Rails, models have an implied parameter of their ID, which you can\nverify by calling to_param on an instance. Here's the Phoenix equivalent: # web/models/post.ex defimpl Phoenix . Param , for: Tilex . Post do def to_param (%{ slug: slug , title: title }) do \" #{ slug } - #{ Tilex . Post . slugified_title ( title ) } \" end end We define the implementation for the given protocol Phoenix.Param.to_param ,\npattern matching the slug and title and returning our 'titled slug'. Slugify the Title Next, we need to be able to convert a title into a valid URL slug. To do this, we'll implement the slugified_title/1 function seen above. Here's the unit test: # test/models/post_test.ex test \" can slugify its own title\" do title = \" Hacking Your Shower!!!\" result = \" hacking-your-shower\" assert Post . slugified_title ( title ) == result end And the implementation: # web/models/post.ex def slugified_title ( title ) do title |> String . downcase |> String . replace ( ~r/[^a-z0-9\\s-]/ , \" \" ) |> String . replace ( ~r/(\\s|-)+/ , \" -\" ) end We take the title, downcase it, remove anything not a space, dash, or alphanumeric character,\nand replace whitespaces with a dash. Handle the New Parameter This application is expecting posts to identify with their ID, so we'll need to change how our\ncontrollers and links behave. First, let's name our new parameter 'titled_slug', to reflect what it is: # web/router.ex defmodule Tilex . Router do resources \" /posts\" , PostController , param: \" titled_slug\" end When the controller receives 'titled_slug', we must add some logic: # web/controllers/post_controller.ex def show ( conn , %{ \" titled_slug\" => titled_slug }) do [ slug | _ ] = titled_slug |> String . split ( \" -\" ) post = Repo . get_by! ( Post , slug: slug ) |> Repo . preload ([ :channel ]) render ( conn , \" show.html\" , post: post ) end Here, a new parameter ( titled_slug ) is pattern-matched, chopped up (again,\nwith pattern matching), and used to find a post. Make sure all your links use the model, not an attribute like .slug or .title , and that's it. Integration Test Here's our new, passing integration test (Wallaby 0.16.1): # test/features/visitor_views_post_test.exs test \" and sees a titled URL slug\" , %{ session: session } do post = Factory . insert! ( :post , title: \" Super Sluggable Title\" ) url = visit ( session , post_path ( Endpoint , :show , post )) |> current_url assert url =~ \" #{ post . slug } -super-sluggable-title\" changeset = Post . changeset ( post , %{ title: \" Alternate Also Cool Title\" }) Repo . update! ( changeset ) post = Repo . get ( Post , post . id ) url = visit ( session , post_path ( Endpoint , :show , post )) |> current_url assert url =~ \" #{ post . slug } -alternate-also-cool-title\" end A new title changes how the URL is presented, but not how it works. Conclusion Here's the pull request where all these changes were made: https://github.com/hashrocket/tilex/pull/25 Tilex has been a fun project, forcing us\nto figure out how features we built in Ruby on Rails might be\nrecreated in Elixir. If you build this feature in a Phoenix application of your\nown, I'd love to hear your experience and any improvements you might have made. Photo Credit: Untitled, Nick Tiemeyer, unsplash.com, https://unsplash.com/photos/tNGcZlycLtQ . Accessed 18 March 2017. Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-03-21"},
{"website": "Hash-Rocket", "title": "Ruby5 Roundup - Episode 397", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/ruby5-roundup-episode-397", "abstract": "Ruby Ruby5 Roundup - Episode 397 by\nPaul Elliott\n\non\nAugust 22, 2013 Fellow Rocketeer Taylor Mock and I teamed up for another episode of the Ruby5 podcast. Here is a quick roundup of what's new this week. http://ruby5.envylabs.com/episodes/433-episode-397-august-23rd-2013 ruby_identicon https://github.com/chrisbranson/ruby_identicon GitHub's identicons have quickly become an easy and popular way to provide a default profile pic. With the ruby_identicon gem from Chris Branson, it is easy to create identicons in your Ruby app. It provides some customization so you can easily tailor them to your needs. Design Patterns: The Observer Pattern http://reefpoints.dockyard.com/2013/08/20/design-patterns-observer-pattern.html Ever wondered about the thought process behind the Observer pattern? This blog post from DockYard takes a deep dive into building it out from scratch. Understanding the reasoning behind the patterns we use every day is critical to building complex system, so put your computer science hat on and give this a read! lita http://www.jimmycuadra.com/posts/getting-started-with-lita Like Hubot but don't want to write coffeescript? Lita is a new chat bot written in Ruby and has a very easy to implement plugin architecture. It is easy to set up on Heroku and dead simple to customize. It is a new project so there aren't a lot of plugins for it yet, but I'm sure that will change over the coming weeks. Give it a try today and contribute a plugin to this new ecosystem! figaro https://github.com/laserlemon/figaro If you're still putting keys in your codebase, shame on you! Storing them in your server's environment configuration makes for a more secure and easier to manage deployment environment, but presents a new challenge in development. Luckily it is a solvable problem thanks to gems like figaro. You can store your development and test configurations it in an environment specific yaml file and put those security concerns behind you. Writable Common Table Expressions http://hashrocket.com/blog/posts/writable-common-table-expressions Learning some of the more advanced features of postgres can help you streamline your code and solve performance problems in ways you probably wouldn't have dreamed of before. This new blog post from fellow Rocketeer Jack Christensen walks you through postgres' writable common table expressions, which you can leverage to wrap up what would have been multiple round-trips to the database into a single request. Top 10 Sites Built with Ruby on Rails http://blog.netguru.co/post/58995145341/top-10-sites-built-with-ruby-on-rails Wondering who is processing lots of traffic on Rails apps these days? Netguru compiled a list of the top 10 Rails apps based on data from Alexa. It is a pretty cool read and these companies are definitely worth checking out! So that's it for this episode of Ruby5. If you haven't already, subscribe to the podcast and keep yourself up to date. Thanks for listening! Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-08-22"},
{"website": "Hash-Rocket", "title": "In Conversation: Russ Olsen", "author": ["\nAndy Borsz\n\n"], "link": "https://hashrocket.com/blog/posts/in-conversation-russ-olsen", "abstract": "Ruby In Conversation: Russ Olsen by\nAndy Borsz\n\non\nAugust 12, 2013 For the past two years I've noticed a trend when I've gone out to Ruby user\ngroups in various parts of the country: beginning Rubyists who appear the most\nequipped and well on their way to Ruby proficiency all have seemed to have a\ncopy of the same book with them: Eloquent Ruby. Further back in time I recall Design Patterns in Ruby occupying a similar place in the Ruby infosphere.\nLast month I got the chance to talk with a person who has literally been\nshaping the minds of learning Rubyists for the past several years, Russ Olsen. AB: Russ, what have you been up to lately? RO: Oh, I've been on the road quite a bit – it's been kind of a blur. I\nwas in North Carolina twice and in Boston once in the last month, so I'm now\nreacquainting myself with my family and house, that kind of thing.\nI did a conference in Boston last week, and then there was some\njob related travel, and RubyNation was tucked in there – that's the local Ruby\nconference that I help organize. AB: You've been involved with RubyNation since the first one, right? RO: Yeah I actually helped organize it from the very beginning. We were all\ndoing the NOVARUG – the Ruby user group. RubyNation started out as a way\nto finance our pizzas. AB: [laughing] I expect that there's a lot of work that goes in to setting up a\nnew conference just for pizza financing. RO: Our original idea was since there was no conference in Northern Virginia,\nwe'd just use our company's big conference room – we could fit seventy or eighty people,\nand it would just be very low key and very simple. About two weeks in, it just mushroomed into\nsomething else. I did set a record this year – I'm now the only person who has\nspoken at every RubyNation. I finally outlasted the competition. AB: Does setting up a conference give you some insight when you're speaking at\nanother conference? Does seeing the inside-track and doing that work give some\ndifferent perspective as a speaker? RO: Oh, God yes! I never complain at conferences. It doesn't matter what goes\nwrong – I just think \"Oh, those poor people. Those poor, poor people.\" AB: Do you have advice for speakers – besides not complaining? RO: Show up on time and then get off the stage on time. The funny thing is that\na lot people will finish on time or even finish with no time for questions, but\nthen they'll just plant themselves there, and people naturally\nwant to come up and say hello, and they're so happy to be talking\nto people, and next thing you know you need a big old vaudeville hook to pull them\noff. AB: So it's really about logistics then? Show up, do your thing. RO: Yes, but mostly the speakers are just great – they come,\nspend their own time, they spend their own money, and it takes a long time to put\nthese talks together, so who can complain? So there you go – I said I don't\ncomplain, but I just complained! AB: Well I goaded you, really. RO: Yes! AB: You're one of the early adopters of Ruby and started working with Ruby\nseveral years before Rails. RO: I don't know how serious I was, but I actually picked it up sometime in late\n2000 or early 2001. I've used versions of Ruby that probably no longer exist or\nthat exist only for archeological purposes now. AB: What was it like coming through that and seeing the the Ruby ecosystem\nchange with Rails? RO: Well, if there was any place that there was a Ruby\ncommunity outside of maybe Tokyo it was Northern Virginia where I was living at the time.\nBut I picked up this obscure language and I just\ndidn't think I'd there was ever going to be anyone else using it who I'd\nactually meet in the flesh. I didn't even go out and look for other\npeople or anything because I figured, \"you know, there's probably this guy in\nJapan and this guy's in Europe, so what's the point?\" So I was pretty surprised\nwhen 2004 rolled around and inside of a week I had\na couple of people send me emails, and my coworkers were pestering me – they \nwere all saying \"hey, you know something about this Ruby\nthing, don't you?\" AB: Curiosity ramped up so quickly that you can think about a\nparticular week? RO: Yeah I honestly don't remember when that was specifically, but the curve really\nramped up in that week. There were people chasing me around saying \"oh, Ruby,\nRuby, Ruby\", and it was only then that I looked around and found the screencast\nshowing you how to make a blog in 10 minutes or whatever. Everybody knew about that before me\nbecause I wasn't looking, you know? Just was not looking. AB: So was Java your primary language before that? RO: Yeah, I started doing Java in maybe '97 or so, so I had done a\nlot of Python and Java for a while. Eventually everything was being done\nin Java, and I was doing a lot of Java, and then doing web applications in Java,\nand like everyone else I was just so frustrated with the state of web applications\nin Java – so certainly by 2004 it was just absolutely maddening. Then Rails\ncame along, and it took me a couple of years to talk whoever I was\nworking for at the time into trying it. AB: That's funny that you say \"like everyone else\" – that's the first time I've ever\nheard of a general discontent with Java web applications. I mean, people are\nstill using it. RO: Java has gotten a lot better, and at least in my opinion, Rails the reason it's\ngotten a lot better. Rails is a wonderful thing in Ruby, but I think it's been a\nwonderful thing for a lot of other language communities too, because people\nlooked at Rails and said \"oh, it's possible. It's possible to do better than\nwe're doing.\" For instance, there's a web framework in Java called Play,\nwhich is actually pretty good. I don't want to say anything to be\nderisive – like \"how can it possibly be good\" – but I think from a Rails programmer's\npoint of view, yes, it's Java – but it's really well done in that Rails-style 'trying to be as convenient as\nhumanly possible' kind of way. And that's a direct influence of Rails. So it's\nfunny, Rails is a wonderful thing in Ruby but I think it's spilled over\ninto a lot of other languages. Just Google for 'the Rails framework in X'\nwhere X is another language and I think for any X you'll get a lot of hits. AB: But you've been programming long before you were doing Java – that's not your\nfirst language. RO: Yeah, I realized something the other day – if programming started roughly in\nthe 1960's, I've been doing it about half the time that it's been around. AB: [both laughing] That's awesome. RO: Yes! But I never really wanted to do anything else. AB: But I though you started out as a mechanical engineer, isn't that right? RO: I was, I was. I have a degree in mechanical engineering technology, which was\nitself sort of a frustration with mechanical engineering as it had been taught.\nIt was getting too theoretical. Mechanical engineering technology was supposed\nto be a throwback to sort of more hands on kind of stuff, and boy was I really\nbadly suited to that. AB: Really? RO: It always impresses people to say I used to do mechanical engineering, but I\nwas terrible at it. Just awful. AB: [both laughing] That's really surprising to hear. RO: Well you know you've got a talent for some things and not so much for other\nthings. AB: Well, how did that transition occur, what was that like? RO: My specialty was heating, ventilating, and air conditioning systems.\nI was fascinated by heat flow and that kind of stuff. I graduated at\nthe time when it was just about possible to – at great expense – use mainframe\ncomputers to analyze the heat flow in buildings. Engineers would run an hour by hour\nsimulation of heating and air conditioning systems in large buildings – this was\nduring the 1980s energy crises, so people were really interested in making buildings energy efficient. So I graduated, and I had always been fairly computer\nliterate and I liked to program, but anybody more than about three of four years\nolder than me didn't know anything about computers. There were all these sort of\ntraditional \"guesstimation\" techniques for designing air conditioning and heating\nventilation systems, but we started doing these simulations. Specifically, we were\ngoing into existing buildings someone else had\ndesigned and saying \"what can you change to make this building more energy\nefficient?\" I can remember coming to a building that had a boiler in it that was two thousand\ntimes bigger than it needed to be. Two thousand times bigger. AB: Whoa! Is that from people being over cautious or not having accurate data? RO: Well, there were standard tables, literally a\nfive volume handbook, and it was just full of all these interesting facts,\nlike how much heat does a chicken give off and that kind of stuff – AB: [laughing] A chicken? RO: I'm not kidding! In fact, if I can remember correctly, I think a person gives\noff something like 250 BTUs of heat per minute or something. Or maybe it's per\nhour. AB: Wow. RO: It's funny the things that stick in your mind, right? AB: [laughing] Yeah. RO: But there were all these tables, so you could take a standard 20,000 square foot\noffice building and estimate how big a boiler it needs. Except\nthat this particular office building wasn't an office building at all – it was a\ntelephone company central switching center, so essentially it was a self-heating\nbuilding. It was a building that was end to end filled with electrical equipment that was never\never turned off, right? So you didn't really need much of a heater. You need\na lot of air conditioning in the summer, but not much. So I was doing\nthese really complicated simulations in FORTRAN, and I ended\nup being the expert on fixing simulation bugs. From there I just kind of\ndecided that I wasn't really crazy about mechanical engineering, and I got into\nCAD – computer aided design – where there were more big FORTRAN programs. AB: Automated graphics? RO: CAD systems are built around a graphics engine, so\nI was doing a lot of that, and from there I spent a long time with mapping\nsystems, GIS (geographic information) systems, and office\nautomation. AB: Do you mean like Microsoft Office? RO: No, I mean like before Microsoft Office. [both laughing] This was a mini\ncomputer system with dumb terminals – I'm telling you\nI've been doing this a long time. It's fun to think about. You know, if you look at\nwhere I started and where we are now, the really good news, is that we're getting\nbetter at this. It's not just that the computers getting faster and the languages are getting more\nsophisticated – we are getting better at this. When I started working on large systems – systems where ten people would work on\nit for a year – if you and I were going to work on somewhat related systems, we'd\nsit in a room for half a day, you and I, and we'd scratch out some interface ideas\non how our pieces would work together, take a couple of pages of notes, maybe do\nsome whiteboarding, and then we'd go down the hall to our separate offices\nand six months later we'd emerge and expect these two huge pieces of\nsoftware to just mesh together. And if you couldn't do that, you weren't any good\nat it or you weren't really trying. And the funny thing was, I was never any\ngood at it. I never knew anybody who could make that work. And now it's funny\nbecause we know it won't work and never could work. AB: It seems like a reasonable idea on paper. RO: Yeah, but that's the kind of thing you have to learn – that we as a profession\nhad to learn. I can remember people talking about \"source control Nazis\"\nwho were making us check in our work once a week. AB: Oh my goodness... RO: I'm not kidding, you know. \"You want me to check it in? Well what if it's\nnot right? I only had a week to work on it.\" AB: [both laughing] That's amazing. RO: But the good news is we're getting better at it. AB: How and when did you make the decision to start writing technical books? RO: I was working at a company that had one of lunchtime speaking events the way\nyou guys do, and I had actually given a talk about the Apollo moon landings. At\nthe time, I was terrified of public\nspeaking, but people really liked that talk and it got me to thinking that maybe\nI could turn it into writing – so I developed this two or three year\nplan where I would write a blog for about a year, and then I'd graduate from that\nand start submitting articles to websites or maybe actual\npaper magazines, and then after a year of that I'd put together a book. It was a\nvery long term thing. Well, I started blogging, and a couple of months into\nblogging I wrote this article that got sixty or seventy thousand hits in a day.\nAnd not too long after that, I wrote about three hundred words on\ndesign patterns in Ruby – the article is actually on the design patterns\nin Ruby site somewhere. I'd written that article on a Monday, and on Friday\nafternoon I was just coming back from the company happy hour feeling pretty good\nand my phone rang – it was this guy from Addison Wesley, and basically he said,\n\"I see you've written three hundred words on this; can you write a hundred\nthousand?\" AB: That's great! It must feel awesome to get approached cold like that. RO: Yeah, I think people who are interested in writing but haven't been published\nhave this feeling that there's fierce competition – you're holding your\nmanuscript up above the crowd trying to get it in publishers' hands, but in fact,\nthey're just looking for content. I think they're desperate for content. AB: So I know that you're doing some programming with Clojure now. I doubt this\nis your first experience with a functional programming language, so what initially\ndrew you to Lisp languages? RO: I'll put it to you this way: an old Internet joke is that every large program has\na half-implemented, buggy version of Common Lisp embedded in it somewhere.\nBack when I was working on CAD systems I actually wrote a buggy,\nhalf-implemented version of Common Lisp and embedded it in the program I was\nworking on. I actually wrote it in FORTRAN, God help me, because that's what\nthat system was written in. I like the simplicity of Lisp. I'm not going to go all mystical and tell you the\nuniverse is constructed in Lisp – in fact according to the XKCD guy it was hacked\ntogether in Perl, right? – but it was never really practical for my career, never\nran in the right environment, was never fast enough ... and then Clojure came along,\nruns in the JVM, is reasonably fast and it's got this laser\nfocus on being practical, so there you go. It's also got some cool things that a lot of other Lisps don't have. I think\npeople have this idea of Lisps as being functional; I think Scheme is reasonably\nfunctional but with Javascript or with Scheme or Common Lisp you can be down in\nthe bowels of some pure function twenty-nine levels deep and simply plug\nsomething into a data structure, change something, mutate something and then\nsuddenly it's not functional at all. So in most of the functional languages the\nfunctionality is voluntary, but in Clojure most of the native structures are\nimmutable – you just can't change anything. The Lisp is simply immutable, there's no\nchanging it. You can make a new one that looks a lot like the old one, but you can't just go in and change\nthe fourth element to twenty-four in place; it's just not possible. It's\na purity thing with some really deep implications if you're\ntrying to write functional programs. AB: What kinds of things do you think that programmers using object-oriented\nlanguages can learn from languages like Clojure or Scala or from functional\nprogramming in general? RO: In object-oriented programming the objects tend to be accretions of data.\nYou've got the first name, last name, employee number, salary, all of that stuff\nin your employee object, and it's all typically mutable. You know the\nproblem in testing where you have to somehow build a shell around the object\nyou're testing to give it an environment context? A functional programmer or\nsomeone who's used to functional programming languages looks at that and it just\nseems wrong from the beginning. So even when I'm writing in an\nobject-oriented language I have a respect for isolating objects so that they're\nnot tangled to other things, because then you can make it sort of functional and\nyou can get some of those functional advantages. My default position now if I'm\njust writing some object, some class, is that I tend to start with a class\nthat's immutable. I make an instance of this thing and I cannot change it. And\nthen grudgingly I will let you mutate things if I absolutely, positively have\nto. And it's funny, people tend to think \"Oh, you're going to write some\nfunctional style and you're going to have these long chains of method calls\" and\nthat kind of stuff – and there's some of that too – but it's mostly focused around\nhow much is changing, who has control of it, and when does it change. Take the classic problem where you've got two threads and the one thread is changing\nsome object – maybe it's the a bank account and you're running a money transfer,\nor there's an employee and you change their title\nbut then you change their salary later and somebody looks at the object and\nthey've got the new title and the old salary, that kind of stuff -- let me just plug\nClosure and say that just can't happen with it. You've got the old version and you've\ngot the new version, and that inconsistency just cannot happen,\nwhich is pretty cool. It's generally something you worry about in other\nlanguages, through – you start that second thread and suddenly you're looking over\nyour shoulder. AB: It seems you've spent a lot of time embedded in companies on software teams\ndoing internal projects. How has the change to consulting been for you? What do\nyou enjoy or find frustrating about the differences? RO: It can be a challenge – I'll put it to you that way. When you work for a product\ncompany, you can build tools that stand on top of the tools that\nyou just built. There's a lot of time to work up a set of things that make your\nlife easier, and we do a lot of that at Relevance too, but you just can't build like\nthat on the same scale for clients because things are changing so fast. I'm on this\nproject, I'm on that project, that's the part – if any – that can be kind of\nfrustrating. The fun part of it is things change so fast! It's hard to get\nbored. In the last six months or so I have worked on everything from very, very\nhigh level system designs to situations where we have this little Rails application and we need\nthree new features and we need them right away. My title at Relevance is\n\"architect\", and I guess I have as much right to that title as anybody else, but\nit's a title I've always avoided because I always have this vision of an\narchitect as the guy down the\nhall in the corner office who never really does anything, but is always wanting\nto take credit for anything good that happens. I never want to be that\nguy. So the range of things I do at Relevance means every so often I'm down\nthere adding the next feature to some little Rails application. That keeps you\ngrounded, you know? It keeps you from saying [in a stuffy voice] \"Well, in my\nextensive experience...\" AB: I saw someone online kind of somewhat humorously applying some pressure on\nyou to write a Clojure book, or implied that you had already had written a Clojure book. RO: I remember, yeah – that was much to my surprise. AB: Is there any plan for that? RO: I am writing about Clojure; it's not in book form, and it will see the light\nof day soon, but I can't actually talk about how it's going to come out. I'm\ntrying to do the Eloquent Ruby thing to Clojure, and we'll see how successful I\nam – I'm trying to at least get close. It's hard for you and me to see the world from the point of view of a\nbeginning Ruby programmer, right? So for all these assumptions that we have and\nthings that we know, there's all these moments of learning that we've forgotten, and\nwe've lost touch with the fact that it took some time to know what we know now. So\nI've been really wanting to do this Clojure thing because I'm still new enough\nat it that I still have that feeling of what it's like to be a beginner. AB: What's next in the works besides this Clojure writing? RO: I actually spent the last year going around and giving this talk about\nexplaining technical things, I called the talk Eloquent Explanations. AB: Yes! I love that one. RO: Thank you. That was actually based on lots and lots of notes\nthat I have for a book about explaining things. I think it's one of the things\nthat we as a community can improve. It's gotten better in the last few years – there are\nsome spectacular examples of good explainers, our friend Sandi Metz, for example –\nbut in the mean, in the average, we are the people in the world that have the most things to explain and\nare the worst at it. Part of the problem is that we tend to be introverts, so public\nspeaking doesn't exactly come naturally, but I also think we've actually been\ntrained to do it badly. As a writer of technical books, I've read a lot of them\nand some of them have wonderful things to say and they don't say them very well.\nBut if you think about conferences like Ancient City Ruby a few months ago,\nyou go out in the hallway and you'll just see people literally in each others'\nfaces telling them what they're doing and what they're thinking about, and you\njust stand there in the hallway and listen – and they're explaining themselves so much more clearly\nthan if you put them up on a stage or asked them to write an article. And so it's there. We can do it – I just think a lot of natural communication has been\ntrained out of us. I also think that programmers overgeneralize – there are lessons\nwe've learned about programming that we overgeneralize into trying to\nexplain things. Like DRY – Don't Repeat Yourself is a marvelous principle of\nprogramming, but is the worst possible thing to do in an explanation. So that's one of the books that's kind of been on my\nback burner for a long time. It's something close to my heart, because I think\nthat if we can get better at explaining things, we can move the ball forward\nrapidly. If everyone gets 5% better at explaining things, that\nmeans everyone gets better in something that they're not very good at, and that's pretty cool. \nIf I could make something like that happen, I will really have done something. - Russ Olsen is a programmer, writer and occasional teacher. The author of two\nbooks on Ruby, Design Patterns in Ruby and Eloquent Ruby , Russ lives outside\nof Washington DC with his wife Karen, his son Jackson and one very moody\nturtle. Twitter: @russolsen Website: http://russolsen.com Books: Eloquent Ruby (2011) Design Patterns in Ruby (2007) Talks: Insight, Intuition and Programming (Ancient City Ruby 2013) Eloquent Explanations (Rocky Mountain Ruby 2012) Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-08-12"},
{"website": "Hash-Rocket", "title": "Start contributing to open source", "author": ["\nTaylor Mock\n\n"], "link": "https://hashrocket.com/blog/posts/start-contributing-to-open-source", "abstract": "Community Start contributing to open source by\nTaylor Mock\n\non\nMay  6, 2013 Open source software can be an intimidating and scary world to the up and coming developer. You pop open a gem you’ve been using and try to follow along with the code but to no avail. It’s far too advanced (or poorly written) for your newly developed skill set. No worries, we were all there once and we still need your help. Before you dive into your first open source project you should be familiar with Git. You probably had a brief introduction to Git while working through the Ruby on Rails Tutorial . As a refresher, CodeSchool has some excellent git intro courses . You should  also checkout Hashrocket's video ‘Feature Development with Git Branching’ . Once you have the basics down you're ready to jump in. So where do you start? Browse Github. Look for something that you’re interested in. A gem that you used? A project you heard of? Something you’ve been wanting to try? Once on a projects Github page, click the ‘issues’ tab. Do you see any problems that you would like to tackle? A couple great ways you can contribute are through documentation and tests. Documentation Did you run into any problems using a gem in your passion project?  Was it because you didn’t understand how to configure something? Maybe the outdated documentation needs an update. Documentation is something that is always needed, appreciated and It’s a great foot in the door to the world of open source. Open the README (or similar file) and start adding the steps you had to complete to get the project working correctly. Tests Open source projects often start as toy’s with broken or insufficient testing. As the project’s grow in size from the addition of other user’s requirements, the need for better tests becomes more and more apparent. Testing other’s work is also a great way to understand bizarre looking code. The spec your writing will help you develop your programming skills and add value to the project. Now that you understand what the code is doing and have tests around it, are there places you could improve the code? A few general things to remember as you are writing your code: Make small, explicit git commits. Some projects will have contribution guidelines.Please read them and stick to them. If the project doesn’t have any contribution guidelines, look at older commits and try to follow the general pattern. If you are writing code, please test it. No one likes to accept an untested pull request. It’s scary. When you have completed your contributions on an appropriately named feature branch, push them up to Github and put in a pull request. Be ready to answer questions about your code from the original repositories maintainer. Explain the issue you ran into and how you went about fixing them. Don’t worry if your pull request isn’t accepted. Ask the maintainer if they have an idea for a better solution to fix the problem or what they didn’t find acceptable about the code you wrote. This is a learning process and because you have been using version control, going back and refactoring is a stress free process. If you're looking for some projects to get started on then Hashrocket has numerous open source projects for you to look into. Decent Exposure , Fabrication , Authem and Paratrooper are a few of the currently maintained open source projects. If you are unable to find anything there then checkout RailsApps by Daniel Kehoe. He's put together quite a few example rails apps for people to use as they see fit. The repositories there are wonderfully well documented and most of the apps could be built upon or better tested. It’s the first place I got my hands dirty writing open source. Last but certainly not least is Code Triage . Code Triage has collected a large amount of open source projects looking for contributions. Would you like some help on your own open source project? Please leave your Github url in a comment below. Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-05-06"},
{"website": "Hash-Rocket", "title": "Front loaded estimation is selling your stakeholder a lie", "author": ["\nChris Cardello\n\n"], "link": "https://hashrocket.com/blog/posts/front-loaded-estimation-is-selling-your-stakeholder-a-lie", "abstract": "Process Front loaded estimation is selling your stakeholder a lie by\nChris Cardello\n\non\nMay 12, 2012 Being a consultancy, we at Hashrocket are frequently presented with the challenge of accurately setting budgetary expectations for our clients.  Traditionally this has been handled by capturing requirements via story-cards and then estimating the complexity and risk of those stories using a point-based scale. We would then sum the total points for the project and divide that by our historic average velocity to project the amount of time we think it might take to complete the project.  Over time we have learned how absolutely flawed this system is. The main reasons we have found this to be an issue are: • We have no idea how complex something which is going to be implemented in X months time is actually going to be. In fact, the more time that passes after a story is captured, the less an estimate is a reflection of the actual risk and complexity associated with a given story. • As we (project and client teams) are always learning more about the domain and application, features are subject to substantial change between the time they are captured and the time they are implemented. • The stories we write in the latter part of a story-carding session have a high probability of not being in the final scope of the project due to changes in desired functionality They’re all 2’s Before you can even start development on a new project the client is going to need at minimum a ballpark of time to market and cost.  At Hashrocket there has been an ongoing joke that estimation is pointless because “Every story is 2 points”.  In reality, a closer look at past projects shows that in most cases they ARE all 2’s.  Reviewing the past four launched projects I personally have worked, the average points per story delivered are as follows: project a: 1.69 project b: 2.14 project c: 2.09 project d: 1.96 Historically the stories were assigned point values to help communicate complexity and ensure that the features being described were broken down to the smallest deliverable piece of software.  More and more we found that stakeholders were making a direct correlation between the point values associated with the story and either time or money.  We knew it was our fault, without knowing it we had essentially taught our clients that the way to measure the health and success of a project was to look at the points being delivered.  The real measure of success and health should be whether we are delivering features the client cares about.  When estimating scope for a client its just as easy to look at the story count as a reflection of the scope.. just assume they are all 2‘s.  After all it’s only an estimate and no matter what, any time or budget projections we provide are intended to be interpreted as educated guesses. Where the magic happens We have found that it’s much better to do just in time estimations during Iteration Planning Meetings to make client’s aware of complexity and risk associated with a given story.  During story-carding the idea is to try to get a minimal slice through the entire application.  Iteration Planning Meetings are where the magic happens.  This is where we fill any gaps in stories, talk through complexity, ensure acceptance criteria is still accurate, uncover hidden scope, and also challenge the relevance of a given story’s place in the backlog.  If we estimate during our Iteration Planning Meeting the client is immediately aware of the riskiness of a given feature. They also have context that better equips them to make the decision as to whether a story is valuable enough to be in scope at that time.  They know how much they have spent thus far, how much they have left to spend, and how much scope remains.  Also, they have been working with us for however many weeks or months at that point and are more familiar with what something means to be risky.  Up front estimation and scope discussion directly after story-carding are difficult for a client because at that point it is hard for them to detach themselves from the big picture of what they think their application is and root themselves in the reality of what is actually capable of being done within their budget. The V Word Velocity.  It’s not as dirty a word as some might have you believe.  Hate it or love it it’s the best option for reflecting what has been delivered.  Sure it’s easy to look at a number and conclude that things are going poorly or well.  I like to remind my stakeholders that when looking at velocity you have to remember that it’s always a representation of what HAS been done and is not necessarily going to accurately project how much WILL get done in the coming weeks.  Also, it’s our job as the project team to communicate with the client all of the different factors which can impact velocity from week to week, and stress that to really measure the health of the application.. log in, test it, and exercise the features. Be truthful When kicking a project off don’t pretend to know how complex or risky every story in the backlog is going to be.  As the majority of stakeholders are not going to be technical, take the time to educate them.  Be informative as to the factors that can impact delivery over the life of a project.  Explain why estimating into the distant future is not reliable.  Provide examples on previous projects where something which was initially thought to be simple ended up being a 3 week endeavor, and vice versa.  This isn’t to say you shouldn’t provide ballpark estimates or points of comparison when attempting to communicate complexity, as it is vitally important to communicate openly with your stakeholder.  It is to say that setting expectations at a point where you are not informed enough to know the full scope of what you are estimating is essentially lying to your stakeholder. Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-05-12"},
{"website": "Hash-Rocket", "title": "Writing the story of an API", "author": ["\nAdam Lowe\n\n"], "link": "https://hashrocket.com/blog/posts/writing-the-story-of-an-api", "abstract": "Process Writing the story of an API by\nAdam Lowe\n\non\nMay  5, 2012 A question we often get at Hashrocket is, \"story carding create, read, update and delete pages is easy but how do you story card an API without talking about implementation?\" and the answer is that you can story card an API as interactions. You might end up describing more implementation or technical details and that is ok. When I story card an API I give each request response pair a story and I treat the XML fields or JSON like I would a view in the application only instead of approved designs you have an approved API doc. API Consumer creates a movie Given an API consumer When I submit a post to http://yourapp.com/movies/ And I have filled in - title (required, limit to 50 characters) - description (limit to 100 characters) - url to movie file Then an asset is created And the system returns an http status code 201 created And the system returns the URI for the created movie API Consumer sends a create request for a movie with no title Given an API consumer When I submit a post to http://yourapp.com/movies/ And I have not filled in title Then an asset is not created And the system returns - an http status code 400 bad request - an error message that the title cannot be blank - the body of the request Things to keep in mind are that you may have already put in place limits on character length if you built the GUI for these resources first and it would be redundant when you are writing the API stories. In that case they could be omitted from these stories. In the second story for the bad request I can get away with just saying that I did not fill in title because the other attributes are not required otherwise I would specify that I provided valid values for those fields as well. Next I want to have a stories that cover a title too long error message and a description too long message since we validate those as well. We want to cover the golden path first but in order build an API that consumers will enjoy using we want to provide meaningful error messages for all of the things we validate against and could subsequently reject their request based on. When you have a specific story for each you will allow the users of your API to quickly identify issues with their requests and enjoy using your API. In addition to having a well structured RESTful API covering these cases will contribute an API with a good user experience. Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-05-05"},
{"website": "Hash-Rocket", "title": "Keeping the Magic Alive", "author": ["\nDaniel Ariza\n\n"], "link": "https://hashrocket.com/blog/posts/keeping-the-magic-alive", "abstract": "Design Keeping the Magic Alive by\nDaniel Ariza\n\non\nJuly  9, 2012 Something I've been thinking about a lot lately and discussing with clients, is keeping the magic in their applications. I don't mean card tricks and disappearing rabbits… but also I kind of do. Let's say you were to watch a magic show. Brandon Copperfield is performing. He's David's younger less flashy brother who's only just switched to magic after being a VCR repairman for many years. He pulls out all the standard tricks and occasionally you see a wire, mirror or secret compartment during the performance. All to the tune of the Final Countdown. These glimpses into inner workings of the tricks really ruin the show. It's not entertaining, you have no respect for the illusionist and you sure as heck aren't going to return for encore performance. I believe a bulky, overly complex interfaces can be a lot like this terrible magic experience. When we try to make our apps too flexible or offer too much choice for users, as opposed to letting the app do the heavy lifting, we're destroying the magic. Here's a small example of what might look like in practice. How often have you seen a text input field that requires some sort of formatting? Take a phone number for instance. We could ask the user for any number of different formats. With or without parens or dashes. Our input field should accept them all. It will require a regex but it's one less thing for the user to have to think about. In the end it will remove visual clutter and streamline the process. Another example might be that your app requires an email address and a unique username for creating separate subdomains. When the user goes to sign in it asks for their email address. Guess what? A lot of your users are going to try and sign in with their username. How about it just takes both. You give us one of those two things and valid password and we'll just sort it out for you. It's magic. Those are 2 very simple examples but over the life of an app will add up to a lot. There are of course larger examples. Maybe you have a powerful search function. You could either force the user to sort through a series of selects to narrow the search or put extra work into the logic of the search under the covers. Depending on how critical this feature is to your business the magical user experience and extra work might be worth it. My hope is that this forces us down the path of more focused and intuitive applications. Less choice and decision making during a workflow will result in a much more magical experience. And if it helps, you can design to the Final Countdown. Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-07-09"},
{"website": "Hash-Rocket", "title": "Using a Matrix to Generate Sprite Map SASS", "author": ["\nShane Riley\n\n"], "link": "https://hashrocket.com/blog/posts/using-a-matrix-to-generate-sprite-map-sass", "abstract": "Design Using a Matrix to Generate Sprite Map SASS by\nShane Riley\n\non\nAugust  1, 2012 I'd like to show you a SASS technique that I've been waiting to implement myself first in a client project, but so far have been unable to. I cannot wait any longer; I must share this awesomeness with all of you! Let's say you have an image sprite of country flags that will be used to display which locale your internationalized site is currently in. Your designer hands you a sprite map all made up, but this time it's different. Instead of each flag being laid out in one row or column for you to easily iterate over like we learned in a previous blog post , we have rows of 5 flags each. You may be shouting \" FFFFUUUU \" at this point while thinking how much trouble it's going to be to recut this sprite map as a single row or column in order to use your awesome SASS trick. There is a way to work with it as is, however. For this solution to work, we need to employ a new control directive and built-in SASS function. We're going to use the @for loop rather than an each loop to run from 0 to the length of each direction in our flag matrix. We'll then use a SASS function called nth that will let you pull the nth element in an array in SASS. To start, let's create our styles for an individual flag. We're using a subset of the Famfamfam flag set as a sprite sheet, so we're going to style our anchors based on their dimensions. ul .flags overflow : hidden width : 90px li float : left padding : 0 2px 2px 0 a display : block width : 16px height : 11px text-indent : -8685px overflow : hidden background : transparent url(\"/images/famfamfam_flag_icons/example.png\") 0 0 no-repeat Nothing too fancy. We're using an image replacement technique to give dimensions to the anchor and hide the text within it in favor of the icon background. In the HAML I'm creating these links with a title attribute that tells me which country each flag is from. - @flags = { ca: \"Canada\" , cn: \"China\" , de: \"Germany\" , uk: \"England\" , es: \"Spain\" , fr: \"France\" , it: \"Italy\" , nl: \"Netherlands\" , se: \"Sweden\" , us: \"United States of America\" } %h1 Country Flag Sprite Matrix %ul .flags - @flags . map do | cc , name | %li = link_to name , \"#\" , class: cc , title: name Next, we're going to define our rows and columns based on the class names we've given each flag. In SASS we would define our rows in variables, then define our matrix using those rows. SASS will allow you to skip the comma separators in these declarations, but they read and scan much easier with them. $row_1 : ca , cn , de , uk , es $row_2 : fr , it , nl , se , us $flags : $row_1 , $row_2 We'll then iterate over the flags matrix with two for loops. There are two keywords, to and through, that you use when defining a for loop in SASS. through is inclusive, meaning it will go up to and including the last value in the range. We're going to use to for the purpose of setting the x and y positions of our background. a background : transparent url(\"/images/famfamfam_flag_icons/example.png\") 0 0 no-repeat @for $y from 0 to length ( $flags ) @for $x from 0 to length ( nth ( $flags , $y + 1 )) & . #{ nth ( nth ( $flags , $y + 1 ) , $x + 1 ) } background-position : ( - $x * 16px ) ( - $y * 11px ) In the first for loop, we'll run from 0 to the number of rows read from calling the length function and passing the $flags matrix. We then start a for loop to iterate over that row's elements up to its length. A little bit of calculation based on the width and height of the icon, and we've got a properly positioned background image for each country code. Check out the example page . When time permits, I'll create a version of this demo with the entire flag set loaded in for anyone that needs it. Follow me to find out where you can get it at when I'm done. In the meantime, get going on those sprite matrices! Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-08-01"},
{"website": "Hash-Rocket", "title": "SASS Iteration using @each", "author": ["\nShane Riley\n\n"], "link": "https://hashrocket.com/blog/posts/sass-iteration-using-each", "abstract": "Design SASS Iteration using @each by\nShane Riley\n\non\nApril 23, 2012 If you're writing SASS, you should be taking advantage of the @each iterator to handle minor changes in styles between elements that share common styles. The syntax is very easy to pick up, with the only stumbling point (for me, anyways) being that the collection to be iterated over is not contained in an array-like object (like []) and the elements of the collection are not encapsulated in quotes. Let's jump right in to an example. Say you have a list of social media links you need to style. You have equal size icons for Twitter, Facebook, GitHub, etc. You've cut them all into a sprite map. Now you need to write out the styles for each class to change the background position. What you'd do to output these classes and associated positions is something like this: ul .social_media li padding : 5px 10px a display : block padding : 0 0 0 21px background : transparent url(\"icon_social-media.png\") 5px 0 no-repeat // Set your initial y-position $i : 0 @each $icon in twitter , facebook , github & . #{ $icon } a background-position : 5px $i $i : $i - 16px This makes adding more icons much easier. All you need to do is add the icon to the bottom of the sprite sheet and add the class name to the end of our collection being iterated over. SASS also has for and while loops that you can take advantage of. Check out the SASS reference for more info on this and the other control directives. Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-04-23"},
{"website": "Hash-Rocket", "title": "Dotmatrix: The Hashrocket Dot File Repo", "author": ["\nJon Allured\n\n"], "link": "https://hashrocket.com/blog/posts/dotmatrix-the-hashrocket-dot-file-repo", "abstract": "Ruby Dotmatrix: The Hashrocket Dot File Repo by\nJon Allured\n\non\nMay 20, 2013 At Hashrocket we love our tools and pride ourselves on being good at using them. One way we go about getting good with these tools is by customizing them with some great configuration. Dot files are used by many programs to configure behavior and sharing these dot files on GitHub is a nice way for developers to have a consistent development environment. We share our dot files in a project called Dotmatrix . Its been around for quite a while and been contributed to by many current and former Rocketeers . It's the distillation of the taste of quite a few very picky programmers. It's easy to install, upgrade and use all or just parts of it and there's lots of cool stuff in there even if you just want to read through it a little. What is Dotmatrix? Dotmatrix is really two things: A vehicle for keeping a set of dot files in sync Version history of the contents of these dot files It's the thing you use to install and upgrade a set of dot files and it's also a git repo of those actual dot files. Some of it concerns the installation and upgrading of these files and the rest of it is those actual dot files. Local Versions of A Dot File Every program is different, but for the most part, they have a load order for their configuration. Most support both a config file and a local version. Take your Z shell config--its specified in this file: ~/.zshrc , but if you have a file called ~/.zshrc.local , that local file will get a chance to override the normal version. The bottom line is that you can inherit configuration in the ~/.zshrc and then have your local modifications in ~/.zshrc.local . Before You Install Dotmatrix It's likely that you already have some configuration in your dot files, so before you install Dotmatrix, pull that configuration into a local version. Better yet, create a repo of those local dot files! For example, if you already have some configuration in your ~/.zshrc file, but want to use Dotmatrix, then move your file to a local version: $ mv ~/.zshrc ~/.zshrc.local Take a look at the list of files in Dotmatrix and then look at your own configuration to see which files you'd like to maintain a local version of. Installation Installing Dotmatrix on a brand new machine is easy. Start by cloning the repo down, maybe in a ~/Project folder or something like that: $ git clone git://github.com/hashrocket/dotmatrix.git Then just go into that directory and run bin/install . This installer script will respect local versions of dot files and then create symlinks to its set of dot files. If a given dotfile already exists, then that particular file will NOT be over-ridden. On the one hand, this is nice because you can't accidentally lose something, but it also means that you wont get some of the goodies in dotmatrix if you unwittingly leave a dot file around, so pay attention to the output to ensure it's doing what you want. Upgrading To upgrade your dotmatrix install go back to the folder where you cloned it and run bin/upgrade . It will fetch the latest and greatest from the GitHub repo and then run the installer script all over again. For Vim Users Another big part of Dotmatrix is the Vim setup that's been built-in. To get this part, after an install or upgrade run this: $ bin/vimbundles.sh We use a LOT of Vim plugins and this will grab those and get you all set. Partial Install Maybe you'd like to just grab some of the files in Dotmatrix, maybe just our Git shortcuts, for example--no prob, just do a partial install ! This is great for those that are new to Dotmatrix and want to just check out some part of it, here are some partial installs that might work for you: Just Git Config Create a file called FILES in the Dotmatrix folder with just this line: .gitconfig This will get you our Git aliases and some other cool configuration. Just Vim Config To grab just the Vim parts of Dotmatrix, create that same FILES file in the Dotmatrix folder and have this in it: .vim\n.vimrc And then be sure to run the bin/vimbundles.sh command. Just ZSH Config Maybe you've heard great things about Z shell and want to try it out--no prob, grab our ZSH config and you'll be glad you did. Put this in the FILES file: .hashrc\n.zsh\n.zshrc Don't forget to run chsh -s /path/to/zsh too--that's what switches your shell. Under Active Development So, that's Dotmatrix--give it a spin and I think you'll find some great stuff. One last thing to note is that this project is fairly active, so be sure to upgrade often for the latest goodies! Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-05-20"},
{"website": "Hash-Rocket", "title": "Silencing Your Staging Environment", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/silencing-your-staging-environment", "abstract": "Ruby Silencing Your Staging Environment by\nPaul Elliott\n\non\nAugust  8, 2012 One of the things we like to do when an application is in production is clone the production databases to the staging environment. This makes for some solid real world testing of the app and any migrations, etc. What's better than testing with real live production data, amiright? Well testing with real live production data does come with at least one catch. All those real live users in your production environment have real live email addresses that receive real live emails. Your staging environment sends real live emails to real live users when things happen. Those real live emails contain links to, oops, your staging environment instead of production. Now you have real live users in a system that looks exactly like production but isn't. We can't just completely shut off email delivery in staging, either. We need to receive them when we smoke test to verify system components and that email content is correct. Hooking into our mailers is not a great approach. We could filter the emails when we pass them in, but then we have to touch the code in a bunch of places. I don't want to have to remember to do this everytime I use or make a new mailer, either. We are bound to wind up missing a few. With a little bit of monkey patching we can ensure that emails are delivered to our employees but not the rest of the world though. Whenever you invoke a mailer anywhere it returns a Mail::Message object. This is what you are actually calling deliver on. We can use ruby's alias_method_chain to put a filter on that method and remove emails that aren't ours. Without further ado, here are the goods. Enjoy! class Mail :: Message def deliver_with_filtering unless Rails . env . production? self . to = scrub_emails ( to ) self . cc = scrub_emails ( cc ) self . bcc = scrub_emails ( bcc ) end deliver_without_filtering if deliverable? end alias_method_chain :deliver , :filtering private def deliverable? to . present? || cc . present? || bcc . present? end def scrub_emails ( emails ) Array . wrap ( emails ). select { | email | email [ /hashrocket.com$/ ] } end end NOTE: We have to make sure the email is still deliverable because our SMTP relay (SendGrid) blows up if you try to send an email with no recipients. You may or may not care if there are recipients depending on your choice of relay. Development and test don't require any. Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-08-08"},
{"website": "Hash-Rocket", "title": "Writing the story of an application", "author": ["\nAdam Lowe\n\n"], "link": "https://hashrocket.com/blog/posts/writing-the-story-of-an-application", "abstract": "Process Writing the story of an application by\nAdam Lowe\n\non\nApril 29, 2012 I work on story cards for a lot of projects each year at Hashrocket and also see a lot of backlogs maintained by clients and open source projects. A notable difference that I find between healthy projects and unhealthy projects is the way in which stories are written. I am purposefully leaving out any discussion about managing and grouping stories at a feature set level but will cover that in a future post. Below is a break down of stories for an example book store that sells pdf's the way I would story card buying a book and the two alternative ways that I see this get broken out the wrong way. Keep in mind that for different projects and project teams story composition will vary to a degree and this is not meant to be a set in stone prescription. The Good (Interaction Driven Stories) Shopper views book list Given a shopper When I click library Then I am taken to the books list page And I see a list of books And for each book I see - title - author - isbn number - price Shopper views book show page Given a shopper on the book list page When I click a book Then I am taken to the book show page And I see - book cover image (104 x 138 pixels) - author name - isbn number - price - description Shopper purchases a book Given a shopper on the book show page When I click buy book Then I am taken to the check out page When I fill in my credit card number And I fill in my security code number And I click check out Then my payment is processed And I am taken to the payment confirmation page Shopper views payment confirmation Given a shopper that has purchased a book And is on the payment confirmation page Then I see a message that I successfully purchased the book And I see the book - title - book cover image (74 x 98 pixels) - price Shopper downloads a purchased book Given a shopper on the payment confirmation page When I click on the book title Then a pdf of the book is downloaded to my computer The Bad Users can view a list of books - title - author - isbn User can view the show page for a book - cover image (104 x 138 pixels) - title - author - isbn - description Users can buy a book - credit card number - security code User views receipt page - book information, title, price - book cover image (74 x 98 pixels) Users can buy a book they downloaded - pdf The Ugly Users can buy books - list page - show page (generate cover images) - users check out (set up payment processing) -- card and security code - users can down load a book (set up pdf generation) In developing software, especially software for someone else stories are more than just a placeholder for conversation. At the point in time when you build the thing they describe they become a social contract to deliver some piece of value. With that in mind it behooves you to be specific in what exactly you are going to deliver. Provide your client with a clear and meaningful set of steps that they can walk through and accept that you provided that value once it is built. This will go a long way towards preventing communication issues and keep you moving forward delivering value to your client rather than resolving communication and perception issues. Some Basic Guidelines Each story should describe a single clear interaction and its outcome. Conditional \"if\" statements or an over abundance of \"and\" or \"when/then\" combinations is a smell. Use your client's business language to describe the actors in your stories and avoid ambiguous terms like \"users\" and \"admins\" when possible. Your story should provide clear steps that your client can walk through to verify you built the feature they asked for. Title and compose your stories with clear, affirmative statements. \"Can\" is noise in stories. Your story should describe where you are and where you end up. This will go a long way to preventing bugs related to user's paths through the application. Implementation doesn't belong in your stories. The exception is sometimes stories around things like API's but I will tackle that in another post. There is an implicit acceptance item in every story that the feature delivered matches the design your client previously approved. This also helps you not muddy up your stories trying to describe design elements. Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-04-29"},
{"website": "Hash-Rocket", "title": "Rubyists - An Encounter", "author": ["\nMicah Cooper\n\n"], "link": "https://hashrocket.com/blog/posts/rubyists-an-encounter", "abstract": "Ruby Community Rubyists - An Encounter by\nMicah Cooper\n\non\nApril 25, 2012 I'm a newer Ruby developer, and RailsConf 2012 was my first chance to observe Rubyists in the wild. Most of my experiences come from Hashrocket – and since we're a consultancy, we run into a vast range of problems and puzzles – but somehow, that's a little different than having thousands of people, each with their own variance of issues, together in one place to share their problemsolving tactics. The Intros My first glimpse into this community was Monday's keynote. David Heinemeier Hansson gave what I found to be an interesting and sensible speech as he warned us of letting our curiosity decay into suspicion. This was not what I expected. I figured something more along the lines of \"We are great. Rails is great. Let's celebrate our awesomeness.\" But the introspection and self criticism was refreshing, inspiring, and reassuring that I'm in the right place in my career as a software craftsman. It led me to believe the leaders of the community are aware of the dangers of complacency and know what it takes to make it last a long time. The following keynotes took similar approaches by reminding us of aspects that are easy to forget or take for granted, and proposing challenging (and somewhat controversial) ideas for the community to chew on. If a young ambitious student were ever in need of a lesson in humility and how it benefits the growth of one's self and community, I would grab his or her head and slowly turn it to the Ruby community as a prime example. The Meat The talks of RailsConf 2012 had an interest range of topics, all with different scopes. Some talks were very narrow and solved a specific problem that many could benefit from. Others seemed to simply cover new trends in the community. There was some questioning as to whether or not certain topics were worth an entire 45 minute block. I obviously didn't go to every talk (haven't figured out how to be in multiple place at one time yet) but I'd say they were worth it. They all seemed relevant to what's going on in the community. I'm sure there were others (besides myself) that left each block with at least one thing to take home and add to their toolset. I also think it's important to keep in mind that an idea I don't immediately see the value in could be exactly what another has been searching for. The Wild I've been wondering for a while what my fellow developers are like. And I'm talkin' developers in the wild, not the ones in my own back yard- there is a good group of us here at Hashrocket. But we're family and I'm around them all the time so it's a little different. I wanted to see other people's problems, how they approached them, and how their ideas and programming skills compare to my own. Maybe I'm a more competitive person than most (which I doubt), but I find validation of my own abilities to be encouraging. So what did I get from meeting other developers at RailsConf? I found that most Rubyists are enthusiastic, dedicated, intelligent, and very nice - Willing to share their knowledge and ideas with anyone who asked, open to criticism and welcoming to ideas that challenge their own. I can't remember having a sour interaction with anyone. And how do my skills stack up against the rest of the community? Ehh, it doesn't matter. I don't care if someone can talk circles around me in some topic they have been studying for the past four months. It seems like real gauge in this arena is how much you contribute, give back and consider the community as a whole. I've decided this will be my new approach to measuring my skills as a developer. Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-04-25"},
{"website": "Hash-Rocket", "title": "Time-Sensitive Cucumber Testing", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/time-sensitive-cucumber-testing", "abstract": "Ruby Time-Sensitive Cucumber Testing by\nPaul Elliott\n\non\nJuly 10, 2012 If you've written any cucumber features that rely on dates, you know what a pain it can be. What's worse, using Timecop or the like on the server has no impact on the browser and can cause inconsistencies in your client/server state. Luckily fabrication gives you some tools to alleviate that pain. Fabrication ships with step definitions that let you create test objects with cucumber tables. A common practice is to use something like Timecop to travel to a point in time and run the tests as of that date. That would produce a scenario like this: Given today is '2012-06-01' And the following campaigns : | name | starts at | ends at | | Current One | 2012-05-25 | 2012-06-03 | | Old One | 2012-05-01 | 2012-05-10 | | Future One | 2012-06-10 | 2012-06-15 | When I am on the campaigns page Then the \"Current One\" campaign should be active And the \"Old One\" campaign should not be active And the \"Future One\" campaign should not be active Although this is a little tough to read, it will work fine as long as you don't have any javascript that compares the dates to the current time. If we leverage a Fabrication transform, however, then we can eliminate the time travel and make the test a little more readable. Fabrication transforms act on any object fabricated through the official Fabrication steps. You can specify them on specific attributes of specific models and more than one can apply to a model, unlike the built-in cucumber table transforms. In this case, let's parse the starts_at and ends_at dates with Chronic. We can do so by adding this to a steps file. Fabrication :: Transform . only_for ( :post , :starts_at , lambda { | date | Chronic . parse ( date ) } Fabrication :: Transform . only_for ( :post , :ends_at , lambda { | date | Chronic . parse ( date ) } Now we can be a little less technical in our cucumber steps and use relative dates instead of these hard coded date strings. Given the following campaigns : | name | starts at | ends at | | Current One | 3 days ago | 3 days from now | | Old One | 2 months ago | 1 month ago | | Future One | 2 weeks from now | 1 month from now | When I am on the campaigns page Then the \"Current One\" campaign should be active And the \"Old One\" campaign should not be active And the \"Future One\" campaign should not be active Now the browser and server can work as one without the time paradox! Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-07-10"},
{"website": "Hash-Rocket", "title": "Feature development with git branching", "author": ["\nKevin Wang\n\n"], "link": "https://hashrocket.com/blog/posts/feature-development-with-git-branching", "abstract": "Feature development with git branching by\nKevin Wang\n\non\nJuly  9, 2012 Git branching can be an effective tool to compartmentalize feature development and maintain multiple work streams while evolving a project. Here is a screencast that I put together to demo the way we introduce new features into projects here at Hashrocket. it is simplified somewhat, but the core principle remains: The master branch is always clean and deployable Develop features on feature branches Bring in changes from other streams and run integration tests on feature branches to guard against regression Merge code back to master and deploy Feature development with git branching from Hashrocket on Vimeo . Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-07-09"},
{"website": "Hash-Rocket", "title": "Test with a Sign In Backdoor", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/test-with-a-sign-in-backdoor", "abstract": "Ruby Test with a Sign In Backdoor by\nPaul Elliott\n\non\nJune 15, 2012 Did you know that almost every scenario in your acceptance test suite has to sign the user in? Did you know that you are crazily overtesting your sign in page as a result? It doesn't have to be this way. Typically a login step looks something like this: Given /^I am signed in$/ do visit '/sign_in' fill_in 'Email' , with: user . email fill_in 'Password' , with: 'password' click_button 'Sign In' end It looks reasonable but every single scenario is requesting and rendering the same page, filling out the same fields, and submitting them. Signing in through the browser is necessary to establish the session, but it doesn't need to be this painful.\nYou can implement a backdoor to take this down to a single page request with no rendering. Even better, you can include it in features/support so you don't open yourself up to any security vulnerabilities. features/support/sign_in_backdoor.rb class UserSessionsController def backdoor sign_in ( User . find_by_email ( params [ :email ])) redirect_to :root end end MyRailsApp :: Application . routes . tap do | routes | routes . disable_clear_and_finalize = true routes . draw do match 'backdoor' , to: 'user_sessions#backdoor' end end Then you can update your step to look like this: module SessionStepMethods def sign_in ( user ) visit \"/backdoor?email= #{ user . email } \" end end World ( SessionStepMethods ) Given /^I am signed in$/ do sign_in ( @user ) end If you have 500 scenarios in your suite, you just saved 500 full page requests. Great work! Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-06-15"},
{"website": "Hash-Rocket", "title": "Bridging ActiveRecord and Mongoid", "author": ["\nKevin Wang\n\n"], "link": "https://hashrocket.com/blog/posts/bridging-activerecord-and-mongoid", "abstract": "Ruby Bridging ActiveRecord and Mongoid by\nKevin Wang\n\non\nAugust 12, 2012 When we pick technologies for clients, we use the right tool for the job and quite often roll custom solutions when necessary. In a recent project, we used both SQL ( ActiveRecord over Postgres) and NoSQL (Mongoid over MongoDB) solutions for the data persistence layer, modeled after the client's data characteristics, and it worked out quite well. While working on this project, we found ourselves needing to traverse between the ActiveRecord side and the Mongoid side quite often as \"1..many\" relationships, so we wrote this piece of glue code to make that easier. class ActiveRecord :: Base def self . has_many_documents ( association_name ) class_eval %<\n      def #{association_name}\n        #{association_name.to_s.singularize.classify}.where(#{name.underscore}_id: id)\n      end\n    > end end module Mongoid::ActiveRecordBridge extend ActiveSupport :: Concern included do def self . belongs_to_record ( association_name , options = {}) association_class = options [ :class_name ] || association_name . to_s . singularize . classify class_eval %<\n        field :#{association_name}_id, type: Integer\n        index(#{association_name}_id: 1)\n\n        def #{association_name}\n          @#{association_name} ||= #{association_class}.where(id: #{association_name}_id).first if #{association_name}_id\n        end\n\n        def #{association_name}=(object)\n          @#{association_name} = object\n          self.#{association_name}_id = object.try :id\n        end\n      > end end Put this in an initializer, then in your ActiveRecord model, you can use \"has_many_documents\", and in your Mongoid document, you can use \"belongs_to_record\" and it just works! You could easily expand this to include \"1..1\" and \"many..many\" relationships. class Property < ActiveRecord :: Base ... has_many_documents :default_rules ... end class Rule include Mongoid :: Document include Mongoid :: ActiveRecordBridge ... belongs_to_record :property belongs_to_record :updated_by , class_name: 'User' belongs_to_record :approved_by , class_name: 'User' ... end Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-08-12"},
{"website": "Hash-Rocket", "title": "Fabrication 2.0 Upgrade Guide", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/fabrication-2-0-upgrade-guide", "abstract": "Ruby Fabrication 2.0 Upgrade Guide by\nPaul Elliott\n\non\nJune  7, 2012 Fabrication 2.0 came out a few days ago and to bring some of the cool new enhancements we had to break some of the api for existing users. Here is a short list of the changes you'll need to make in your test suite to upgrade. Remove all the \"!\" suffixes Lazy generation is no longer a thing in fabrication so the \"!\" is no longer necessary. If you leave them in you will see a deprecation warning but things will work Update usage of attribute block variables In fabrication 1.x, a block was passed the object being generated. Now is just collects them into an attributes hash and provides that to the block. The old way: Fabricator ( :person ) do first_name { Faker :: Name . first_name } email { | person | \" #{ person . first_name . downcase } @example.com\" } end The new way: Fabricator ( :person ) do first_name { Faker :: Name . first_name } email { | attrs | \" #{ attrs [ :first_name ]. downcase } @example.com\" } end As you can see, the change is very simple to make. You will just need to go through and update your Fabricators. Regenerate the fabrication cucumber steps Some bug fixes have been made in the steps and a new feature was introduced, so you should regenerate the steps with rails g fabrication:cucumber_steps if you are using them in your project. Getting Help If you run into any issues, be sure to check the documentation ( http://fabricationgem.org ). If that doesn't help you can open an issue on github ( https://github.com/paulelliott/fabrication/issues ). Or if you prefer, you can send a message to the mailing list and I will reply to you ( https://groups.google.com/group/fabricationgem ). Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-06-07"},
{"website": "Hash-Rocket", "title": "Fabrication gem walk through with Paul Elliott", "author": ["\nKevin Wang\n\n"], "link": "https://hashrocket.com/blog/posts/fabrication-gem-walk-through-with-paul-elliott", "abstract": "Ruby Fabrication gem walk through with Paul Elliott by\nKevin Wang\n\non\nAugust 13, 2012 I invited Paul Elliott to talk about his Fabrication gem. Paul explains what Fabrication is for and gives code walkthrough on the way he prefers using Fabrication. The video is about 26 minutes long - enjoy! Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-08-13"},
{"website": "Hash-Rocket", "title": "Better Date and Time Formatting for Ruby", "author": ["\nDave Ly"], "link": "https://hashrocket.com/blog/posts/better-date-and-time-formatting-for-ruby", "abstract": "Ruby Better Date and Time Formatting for Ruby by\nDave Lyon\n\non\nApril 15, 2012 While working with dates and times in a Rails app recently, my pair and I found myself having to deal with some really annoying padding issues. We wanted to output times like \"8:00pm\" using strftime and found that with single digit hours, what we got was \" 8:00pm.\" Initially, we thought the simplest thing to do was to 'strip' the string, but after looking at the documentation for Time#strftime we noticed some extra flags that could be passed to control the padding of interpreted formats. -  don't pad a numerical output.\n_  use spaces for padding.\n0  use zeros for padding. Super useful! Here's some examples: date = Date . new ( 2012 , 1 , 1 ) date . strftime ( \"%m\" ) # => \"01\" date . strftime ( \"%-m\" ) #=> \"1\" date . strftime ( \"%_m\" ) #=> \" 1\" Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-04-15"},
{"website": "Hash-Rocket", "title": "Using Recursive SQL with ActiveRecord trees", "author": ["\nJoshua Davey\n\n"], "link": "https://hashrocket.com/blog/posts/recursive-sql-in-activerecord", "abstract": "Ruby PostgreSQL Using Recursive SQL with ActiveRecord trees by\nJoshua Davey\n\non\nSeptember  4, 2012 tl;dr When you have an ActiveRecord tree structure, using the WITH syntax for recursive SQL can provide large performance boons, especially when a tree get several levels deep. In a previous post , I outlined a Cat Picture store application. As our store grows, more and more categories have to be created, and we end up with a tree of categories.\nHow can we create a homepage that includes all cat pictures for a given category and all of its subcategories? Pictorally, the category tree might look like this: Cat Pictures\n|-- Funny\n|   |-- LOLCats\n|   `-- Animated\n`-- Classic\n    `-- Renaissance On the landing page for the Cat Pictures category, we want to display\nall cat pictures for any category below Cat Pictures . Navigating to\nthe Funny category would display all of its pictures, as well as the\npictures for LOLCats and Animated . This is the kind of interaction\nseen on Amazon, for example. The store's categories become like an\nad-hoc filtering system. Here's what the Category class looks like: class Category < ActiveRecord :: Base attr_accessible :name , :parent has_many :cat_pictures belongs_to :parent , :class_name => \"Category\" has_many :children , :class_name => \"Category\" , :foreign_key => 'parent_id' scope :top_level , where ( :parent_id => nil ) def descendents # implement me! end end Each category has a parent_id column that points at its parent\ncategory. In database speak, modeling a tree like this is known as\nan Adjacency List ; each node of the tree can only see a children\nimmediately adjacent to it. For this reason, crawling an Adjacency List\nrequires recursion. This is actually the database setup common for use\nwith the acts_as_tree plugin. Let's see how we can implement the descendents method to get all descendent categories. A Simple Ruby Approach As you've probably already guessed, we need to recursively collect\nchildren for each of our category's children. class Category < ActiveRecord :: Base # ... def descendents children . map do | child | [ child ] + child . descendents end . flatten end end This does the job quite nicely. However, our requirements above state\nthat we want all cat pictures for each descendent category, and our\ncategories . Right now, we've omitted the root category, self. Let's add\na new method to include it into the equation: class Category < ActiveRecord :: Base # ... def descendents children . map do | child | [ child ] + child . descendents end . flatten end def self_and_descendents [ self ] + descendents end end Good deal. Now gathering all cat pictures is just a matter of collecting\nthem for each category: class Category < ActiveRecord :: Base # ... def descendent_pictures self_and_descendents . map ( & :cat_pictures ). flatten end end For a tree like we have above, this is probably good enough. Our tree is\nonly 3 levels deep. We've introduced plenty of N+1 queries, but given\nour small dataset, that shouldn't be a huge concern. That said, as our store grows, and the tree gets deeper and more\ndetailed, this kind of implementation could become a bottleneck. Also,\nbecause we're doing Array operations on the children collection,\nwe lose the ability to take advantage of ActiveRelation outside of the descendents method itself. Among other things, this means that we\ncan't eager-load cat pictures unless we always eager-load them within\nthe descendents method. Surely we can do better. SQL WITH queries Since we're using PostgreSQL, we can take advantage of its special\nfeatures. In this case, we can use a WITH query. From the PostgreSQL\ndocumentation : WITH provides a way to write auxiliary statements for use in a larger\nquery. These statements, which are often referred to as Common Table\nExpressions or CTEs, can be thought of as defining temporary tables that\nexist just for one query. On its own, this might not seem like a big deal, but when combined with\nthe optional RECURSIVE modifier, WITH queries can become quite powerful: The optional RECURSIVE modifier changes WITH from a mere syntactic\nconvenience into a feature that accomplishes things not otherwise\npossible in standard SQL. Using RECURSIVE, a WITH query can refer to\nits own output. A very simple example is this query to sum the integers\nfrom 1 through 100: WITH RECURSIVE t(n) AS (\n     VALUES (1)\n   UNION ALL\n     SELECT n+1 FROM t WHERE n < 100\n )\n SELECT sum(n) FROM t; The general form of a recursive WITH query is always a non-recursive\nterm, then UNION (or UNION ALL), then a recursive term, where only the\nrecursive term can contain a reference to the query's own output. In other words, the expression contained in the AS statement has two\nparts. The first part is executed just once. The second part, after the\nUNION ALL, is executed until it returns an empty result set. Taking advantage of WITH RECURSIVE, we can reduce our tree crawling\ntechnique from n queries to just 1! Let's how we can use this to crawl\nour category tree. As a reminder, here's what our categories table looks like: # SELECT id, name, parent_id FROM categories;\n\n id |     name     | parent_id \n----+--------------+-----------\n  1 | Cat Pictures |          \n  2 | Funny        |         1\n  3 | LOLCats      |         2\n  4 | Animated     |         2\n  5 | Classic      |         1\n  6 | Renaissance  |         5 And this is the query: WITH RECURSIVE category_tree ( id , name , path ) AS ( SELECT id , name , ARRAY [ id ] FROM categories WHERE parent_id IS NULL UNION ALL SELECT categories . id , categories . name , path || categories . id FROM category_tree JOIN categories ON categories . parent_id = category_tree . id WHERE NOT categories . id = ANY ( path ) ) SELECT * FROM category_tree ORDER BY path ; Running the query above returns the following: id |     name     |  path   \n----+--------------+---------\n  1 | Cat Pictures | {1}\n  2 | Funny        | {1,2}\n  3 | LOLCats      | {1,2,3}\n  4 | Animated     | {1,2,4}\n  5 | Classic      | {1,5}\n  6 | Renaissance  | {1,5,6} Whoa! That's a lot of SQL. Let's break it down a bit. Declare the Table Expression First, we declare our \"temporary table\" using the WITH syntax. We're\ngoing to call it category_tree . This \"table\" has 3 \"columns\": id , name , and path . The id and name columns are fairly obvious; they\nrefer to corresponding columns on the categories table. The path is an\narray of ids that each row will have. More on this in a bit. Define the Non-recursive Term The non-recursive term is next: SELECT id , name , ARRAY [ id ] FROM categories WHERE parent_id IS NULL It grabs the id and name for each top-level category, that is, each\ncategory that has no parent. It also initializes an array containing just\nits id . On its own, this isn't very interesting, but this array will\nbecome helpful during the recursive step of the query. Define the Recursive Term The recursive term is the juiciest bit of the query: SELECT categories . id , categories . name , path || categories . id FROM category_tree JOIN categories ON categories . parent_id = category_tree . id WHERE NOT categories . id = ANY ( path ) Notice that we're selecting from category_tree . By doing this, we're\nable to use each result set in the subsequent iteration. The first time\nwe recurse, the result set will be what we selected in the non-recursive\nterm above. Given that we have a root result set, we join with categories to find\nits children. From our new result set, we select id and name , as\nbefore. But this time, we concatenate the child id onto the path array\nusing SQL's || operator. Having this materialized path allows us to\nguard against infinite loops; the WHERE clause makes sure that the row\nwe're selecting has not appeared in the path before. This infinite loop check is important. If two categories pointed at each\nother as parents, the query would never return. Including this check\nprevents such a mistake from killing our server. Query the Common Table Expression Finally, a WITH query is only useful if you select from it outside of\nits declaration, so we'll do just that: SELECT * FROM category_tree ORDER BY path ; In addition to the infinite loop guard, the path column answers the\nquestion \"How did I get here?\" Like a directory structure on a file\nsystem, the path demonstrates the ids necessary to get from grandparent\nto parent to child, etc. You may have noticed that we're also ordering by the path column.\nWe do this because the default sort from a recursive query is\nnondeterministic. Normal array sorting works well for us here, and\ngroups the categories just like we'd expect, with parents listed before\ntheir children. Using WITH queries in ActiveRecord class Category < ActiveRecord :: Base # ... def descendents self_and_descendents - [ self ] end def self_and_descendents self . class . tree_for ( self ) end def descendent_pictures subtree = self . class . tree_sql_for ( self ) CatPicture . where ( \"category_id IN ( #{ subtree } )\" ) end def self . tree_for ( instance ) where ( \" #{ table_name } .id IN ( #{ tree_sql_for ( instance ) } )\" ). order ( \" #{ table_name } .id\" ) end def self . tree_sql_for ( instance ) tree_sql = <<- SQL WITH RECURSIVE search_tree(id, path) AS (\n          SELECT id, ARRAY[id]\n          FROM #{ table_name } WHERE id = #{ instance . id } UNION ALL\n          SELECT #{ table_name } .id, path || #{ table_name } .id\n          FROM search_tree\n          JOIN #{ table_name } ON #{ table_name } .parent_id = search_tree.id\n          WHERE NOT #{ table_name } .id = ANY(path)\n      )\n      SELECT id FROM search_tree ORDER BY path SQL end end You should notice right away where our recursive query is. The tree_sql_for class method returns a SQL string that can be used with\nother queries. Compared to the WITH query we looked at before, there a\nfew differences worth mentioning. First, and probably most importantly for our original problem, we've\nchanged our starting place. The non-recursive term is our \"start here\"\nresult set. Rather than starting with all top-level categories, we're\nusing the id of whichever instance is passed in to scope our tree. Another change we've made is to remove the name column from the query.\nIt isn't necessary for what we're doing, but made the example easier to\ndemonstrate. We're also interpolating the table name. This makes the\nmethod much more reusable. In fact, we could extract the method to a RecursiveTree module to tidy up our class. One big advantage of the SQL approach here is that we can create scopes\nto further filter our results within just one database round-trip.\nFor example, the tree_for class method is really just a named scope\nthat takes a category instance as a parameter. Likewise, the the descendent_pictures method returns a CatPicture\nrelation that includes all pictures from this category and all\nsubcategories. In other words, what used to take 2 database round trips\nfor each category in the tree (one to grab children, one to get its\npictures) will now only take 1 for the entire set. Conclusion Taking advantage of PostgreSQL's advanced features can provide large\nperformance boons, especially when a tree get several levels deep. Although using database recursion is an efficient way of improving\nperformance with our existing schema, other methods of handling tree\nstructures in SQL exist. The SQL Antipatterns book has a great\nbreakdown of other tree solutions that would require schema changes. Example app As before, while writing this post, I created a sample Rails app to\niterate quickly. I used TDD to write the pure-ruby approach, and reused\nthe specs while I \"refactored\" the implementation to the subsequent\napproaches. Of particular note is the history of the Category\nmodel , which mirrors the code above. Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-09-04"},
{"website": "Hash-Rocket", "title": "Heroku Deploy Scripts", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/heroku-deploy-scripts", "abstract": "Heroku Deploy Scripts by\nPaul Elliott\n\non\nMarch  8, 2012 Deploying to Heroku is so easy, right? It's just a git push away! And that's true unless you need to run any rake tasks, like a db:migrate. Well the good news is it is really easy to write a quick script to execute all your deployment tasks. Here is an example task that is similar to what we would use to deploy one of our applications. We would typically have separate tasks for each environment, but they essentially look like this. namespace :deploy do desc 'Deploy the app' task :production do app = \"my-amazing-heroku-app-name\" remote = \"git@heroku.com: #{ app } .git\" system \"heroku maintenance:on --app #{ app } \" system \"git push #{ remote } master\" system \"heroku run rake db:migrate --app #{ app } \" system \"heroku maintenance:off --app #{ app } \" end end As you can see, it is pretty straightforward stuff. It just wraps up all the commands you need to run. One important thing to note is that each of these system commands is a separate operation for Heroku. That means that in one call, your code is deployed and the app is restarted. Once that is done, it runs rake db:migrate . Heroku won't actually load the classes in your application until the first request is made. That is important to understand because if you add a table or column in a migration and someone hits the application between the push and the migration, the models will be loaded without that information. 500s will be generated if those resources are accessed. You can easily solve this problem by turning on the maintenance pages while deploying or restarting after the migrate. Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-03-08"},
{"website": "Hash-Rocket", "title": "Ruby5 Roundup - Episode 376", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/ruby5-roundup-episode-376", "abstract": "Ruby Ruby5 Roundup - Episode 376 by\nPaul Elliott\n\non\nJune  6, 2013 This week I had the privilege to host the Ruby5 podcast with fellow Rocketeer, Taylor Mock. If you haven't listened to Ruby5 before, you really should start. It is a five minute podcast twice a week that highlights new gems, new blog posts, and current events in our community. Here is a quick roundup of this episode. http://ruby5.envylabs.com/episodes/412-episode-376-june-7th-2013 Octokit https://github.com/blog/1517-introducing-octokit GitHub's API has become an important part of our ecosystem as of late. Many apps are using it for sign in and others are pulling information from it for a variety of reasons. They recently released client libraries to facilitate interacting with those APIs both for Ruby and Objective-C. The Ruby version has actually been around for a while. It was a community maintained effort but is now officially part of GitHub's offering. If you connected in the past, you likely have already used this codebase. The Objective-C version is another story though. Their Mac team extracted it from the GitHub for Mac app and open sourced it. You can now easily integrate it into your iOS or OSX apps. They have a really nice doc site for it that is worth taking a look at whether you are using it now or not. Developer-Friendly European Payment Gateways http://cookieshq.co.uk/posts/developer-friendly-or-not-european-payment-gateways The folks over at Cookies HQ wrote a quick roundup of the various payment gateway options available in Europe. This is definitely something to consider if your app is going to take payments there. Some of the US processors also have a presence there and some are exclusively in that region. It is high level but gives you a good place to start looking if you need to choose a provider. DbSync https://github.com/scottschulthess/db_sync This is a new gem that allows you to import and export database tables. It only does data but is a handy utility to have around. If you are running your own servers, you can easily export the data on the server, scp it to your local machine, then load it into your dev database. You could also use it to share data between developers or even just make a backup of your existing dev database. Lots of potential use cases and it looks very easy to work with. ActiveRecord::Calculations.pluck http://blog.hashrocket.com/posts/rails-quick-tips-activerecord-calculations-pluck This is a blog post by fellow Rocketeer, Matt Polito, from a few days ago. He discusses how to pull a single column from a database query with Rails using the pluck method. It is a really handy method in the console, where you'll often need to get at a column while investigating issues. Using ember-auth with Rails 3 and Authlogic http://blog.centresource.com/2013/06/04/using-ember-auth-with-rails-3-and-authlogic/ This article was very interesting to me. It fuses new technology, Ember.js, with a real blast from my past, Authlogic. If you haven't been doing Rails for that long you may not have heard of Authlogic. It was the go-to for many of us a few years back, long before Devise and OmniAuth came on the scene. There is still a community of people using Authlogic and this blog post goes into detail on how to integrate it with ember-auth. Lots of code examples and explanation of how to do it. Definitely worth checking out. The Protector! http://staal.io/blog/2013/06/04/the-protector/ Last but not least we have protector, a gem that allows you to control resource authorization in your models. It provides a nice DSL for specifying who can read and write to resources, even down to the field level. You can define default scopes based on roles and conditionally provide read-only access to certain users. You define a block in your model that receives the current user instance and configure away from there. It looks like a really interesting alternative to cancan if you need that level of authorization. So that's it for this episode of Ruby5. If you haven't already, subscribe to the podcast and keep yourself up to date. Thanks for listening! Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-06-06"},
{"website": "Hash-Rocket", "title": "jQuery Tips: Adding Filter Expressions", "author": ["\nShane Riley\n\n"], "link": "https://hashrocket.com/blog/posts/jquery-tips-adding-filter-expressions", "abstract": "Javascript jQuery Tips: Adding Filter Expressions by\nShane Riley\n\non\nApril 16, 2012 Are you writing complex filter methods like this to get at the elements you need? $ ( \"form\" ). find ( \":input\" ). filter ( function () { return this . value === \"active\" ; }); Then it might be time to start writing your own jQuery expressions. They're really simple to make and are, in most cases, easier for others to read because it hides the DOM element inspection behind the expression filter. Here's a quick example: just like the example code above, you want to find only inputs with a given value. To convert this to an expression, you'd write something like this: $ . expr [ \":\" ]. value = function ( el , idx , selector ) { return el . value === selector [ selector . length - 1 ]; }; Then you can find an input with a value of active using a selector like this: $ ( \":input\" ). filter ( \":value(active)\" ); In the method you create for any jQuery expression, you are given three arguments. The first is the DOM element for that particular iteration. Second is the index of the DOM element relative to the original collection the expression was called on. Last is an array similar to what you'd get from a RegExp exec method call. The full expression is the first array position, second is the text from the selector between the colon and opening paren, and the last is the string that occurs within the parens. Let's take a look at another example to illustrate how this selector array is structured. Here's an expression that will return you any elements with a particular data attribute that equals a specific value. $ . expr [ \":\" ]. data = function ( el , idx , selector ) { var attr = selector [ 3 ]. split ( \", \" ); return el . dataset [ attr [ 0 ]] === attr [ 1 ]; }; If you had a div with a data attribute of data-default=\"Hello world\" and you wanted to get at it in a single selector, you could write: $ ( \"div:data(default, Hello world)\" ) If you want to create multiple expression filters, it's best to use jQuery's extend method to combine a new object with your filter methods and the $.expr[\":\"] object. $ . extend ( $ . expr [ \":\" ], { data : function ( el , idx , selector ) { var attr = selector [ 3 ]. split ( \", \" ); return el . dataset [ attr [ 0 ]] === attr [ 1 ]; }, value : function ( el , idx , selector ) { return el . value === selector [ selector . length - 1 ]; } }); Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-04-16"},
{"website": "Hash-Rocket", "title": "Book Club recap: Javascript Patterns", "author": ["\nCameron Daigle\n\n"], "link": "https://hashrocket.com/blog/posts/book-club-recap-javascript-patterns", "abstract": "Javascript Book Club recap: Javascript Patterns by\nCameron Daigle\n\non\nMarch  9, 2011 Hashrocket Book Club is a chance for Rocketeers and viewers like you to meet up at 12:12 each week and walk through a book on any number of topics. In the past we've done Ruby books, David Allen's Getting Things Done , usability books, you name it ... but we just wrapped up a multi-week session on Stoyan Stefanov's Javascript Patterns . It was an interesting couple of months, to be sure. While we found some common ground in many of his patterns (and a few were downright enlightening), there were definitely some sections that could have used a couple of more revisions, especially towards the end of the book. But that's exactly why Book Club is great! With enough heads in the same space, those iffy parts tend to be caught more often than not, and make for great discussion material. Here's episode 1 in all of its Javascripty glory. You can watch the rest of the episodes here on our Vimeo . Enjoy! Was this post helpful? Share it with others. Tweet Share Post", "date": "2011-03-09"},
{"website": "Hash-Rocket", "title": "Go Performance Observations", "author": ["\nJack Christensen\n\n"], "link": "https://hashrocket.com/blog/posts/go-performance-observations", "abstract": "PostgreSQL Go Performance Observations by\nJack Christensen\n\non\nAugust  7, 2014 In the course of optimizing the pgx PostgreSQL driver I observed a number of performance characteristics of Go that I hope you will find useful. Measure First \"Premature optimization is the root of all evil\" -- Donald Knuth Go has two tools that are invaluable in performance tuning: a profiler and a benchmarking tool. The profiler helps find the trouble spots and benchmarks show the results of an optimization. See How to write benchmarks in Go by Dave Cheney and Profiling Go Programs by Russ Cox for introductions to these tools. Below are several specific techniques I found with benchmarks and the profiler. Source code for the benchmarks is on Github . Reuse Memory Every allocation of memory has several potential costs. The Go runtime must ensure that the memory is initialized to the zero value. The garbage collector must track the references to value and eventually clean it up. Additional memory usage also makes it less likely to get a CPU cache hit. This simple example fills a slice of up to 1024 bytes with 1s. func BenchmarkNewBuffers ( b * testing . B ) { for i := 0 ; i < b . N ; i ++ { n := rand . Intn ( 1024 ) buf := make ([] byte , n ) // Do something with buffer for j := 0 ; j < n ; j ++ { buf [ j ] = 1 } } } func BenchmarkReuseBuffers ( b * testing . B ) { sharedBuf := make ([] byte , 1024 ) for i := 0 ; i < b . N ; i ++ { n := rand . Intn ( 1024 ) buf := sharedBuf [ 0 : n ] // Do something with buffer for j := 0 ; j < n ; j ++ { buf [ j ] = 1 } } } Note the -test.benchmem flag for measuring memory allocations. jack@hk-47~/dev/go/src/github.com/jackc/go_pgx_perf_observations$ go test -test.bench=Buffers -test.benchmem\ntesting: warning: no tests to run\nPASS\nBenchmarkNewBuffers  2000000        1033 ns/op       540 B/op        0 allocs/op\nBenchmarkReuseBuffers  5000000         436 ns/op         0 B/op        0 allocs/op\nok    github.com/jackc/go_pgx_perf_observations 5.704s Allocating a new buffer each iteration is substantially slower. Obviously, the more work done on the buffer the less relative impact eliminating the allocation would have. Surprisingly, both versions show 0 allocs/op. How can that be? Let's rerun the test with the -gcflags=-m option to ask Go to tell us the details. jack@hk-47~/dev/go/src/github.com/jackc/go_pgx_perf_observations$ go test -gcflags=-m -test.bench=Buffers -test.benchmem\n# github.com/jackc/go_pgx_perf_observations_test\n<snip/>\n./bench_test.go:15: BenchmarkNewBuffers b does not escape\n./bench_test.go:18: BenchmarkNewBuffers make([]byte, n) does not escape\n./bench_test.go:27: BenchmarkReuseBuffers b does not escape\n./bench_test.go:28: BenchmarkReuseBuffers make([]byte, 1024) does not escape\n<snip/> The Go compiler performs escape analysis . If an allocation does not escape the function it can be stored on the stack and avoid the garbage collector entirely. So in a real world system where many allocations do escape to the heap reducing the allocations can have an even bigger impact. Buffered IO Go does not buffer IO by default. The bufio package provides buffered IO. This can make a massive difference in performance. func BenchmarkUnbufferedFileWrite ( b * testing . B ) { file , err := os . Create ( \"unbuffered.test\" ) if err != nil { b . Fatalf ( \"Unable to create file: %v\" , err ) } defer func () { file . Close () os . Remove ( file . Name ()) }() for i := 0 ; i < b . N ; i ++ { fmt . Fprintln ( file , \"Hello world\" ) } } func BenchmarkBufferedFileWrite ( b * testing . B ) { file , err := os . Create ( \"buffered.test\" ) if err != nil { b . Fatalf ( \"Unable to create file: %v\" , err ) } defer func () { file . Close () os . Remove ( file . Name ()) }() writer := bufio . NewWriter ( file ) defer writer . Flush () for i := 0 ; i < b . N ; i ++ { fmt . Fprintln ( writer , \"Hello world\" ) } } jack@hk-47~/dev/go/src/github.com/jackc/go_pgx_perf_observations$ go test -test.bench=Write\ntesting: warning: no tests to run\nPASS\nBenchmarkUnbufferedFileWrite   1000000        2588 ns/op\nBenchmarkBufferedFileWrite  10000000         271 ns/op\nok    github.com/jackc/go_pgx_perf_observations 5.626s A simple test of writing \"Hello, world\" repeatedly to a file shows a greater than 9x improvement in performance by using a buffered writer. Binary vs. Text Formats PostgreSQL allows the transmission of data in binary or text format. The performance of the binary format is far faster than the text format. This is because the only processing typically needed is converting from network byte order. The binary format should also be more efficient for the PostgreSQL server and it may be a more compact transmission format. However, we will isolate our benchmarks to the parsing of int32 and time.Time values. func BenchmarkParseInt32Text ( b * testing . B ) { s := \"12345678\" expected := int32 ( 12345678 ) for i := 0 ; i < b . N ; i ++ { n , err := strconv . ParseInt ( s , 10 , 32 ) if err != nil { b . Fatalf ( \"strconv.ParseInt failed: %v\" , err ) } if int32 ( n ) != expected { b . Fatalf ( \"strconv.ParseInt decoded %v instead of %v\" , n , expected ) } } } func BenchmarkParseInt32Binary ( b * testing . B ) { buf := make ([] byte , 4 ) binary . BigEndian . PutUint32 ( buf , 12345678 ) expected := int32 ( 12345678 ) for i := 0 ; i < b . N ; i ++ { n := int32 ( binary . BigEndian . Uint32 ( buf )) if n != expected { b . Fatalf ( \"Got %v instead of %v\" , n , expected ) } } } func BenchmarkParseTimeText ( b * testing . B ) { s := \"2011-10-25 09:12:34.345921-05\" expected , _ := time . Parse ( \"2006-01-02 15:04:05.999999-07\" , s ) for i := 0 ; i < b . N ; i ++ { t , err := time . Parse ( \"2006-01-02 15:04:05.999999-07\" , s ) if err != nil { b . Fatalf ( \"time.Parse failed: %v\" , err ) } if t != expected { b . Fatalf ( \"time.Parse decoded %v instead of %v\" , t , expected ) } } } // PostgreSQL binary format is an int64 of the number of microseconds since Y2K func BenchmarkParseTimeBinary ( b * testing . B ) { microsecFromUnixEpochToY2K := int64 ( 946684800 * 1000000 ) s := \"2011-10-25 09:12:34.345921-05\" expected , _ := time . Parse ( \"2006-01-02 15:04:05.999999-07\" , s ) microsecSinceUnixEpoch := expected . Unix () * 1000000 + int64 ( expected . Nanosecond ()) / 1000 microsecSinceY2K := microsecSinceUnixEpoch - microsecFromUnixEpochToY2K buf := make ([] byte , 8 ) binary . BigEndian . PutUint64 ( buf , uint64 ( microsecSinceY2K )) for i := 0 ; i < b . N ; i ++ { microsecSinceY2K := int64 ( binary . BigEndian . Uint64 ( buf )) microsecSinceUnixEpoch := microsecFromUnixEpochToY2K + microsecSinceY2K t := time . Unix ( microsecSinceUnixEpoch / 1000000 , ( microsecSinceUnixEpoch % 1000000 ) * 1000 ) if t != expected { b . Fatalf ( \"Got %v instead of %v\" , t , expected ) } } } jack@hk-47~/dev/go/src/github.com/jackc/go_pgx_perf_observations$ go test -test.bench=Parse\ntesting: warning: no tests to run\nPASS\nBenchmarkParseInt32Text 50000000          62.8 ns/op\nBenchmarkParseInt32Binary 500000000          3.40 ns/op\nBenchmarkParseTimeText   2000000         775 ns/op\nBenchmarkParseTimeBinary  100000000         15.4 ns/op\nok    github.com/jackc/go_pgx_perf_observations 9.159s Parsing an int32 takes over 18x longer than to parse from text than simply to read in binary. Parsing a time takes over 84x longer. The absolute numbers are small, but they add up. In general, binary protocols are vastly faster than text protocols. More Binary Tricks When reading or writing a binary stream using binary.Read with an io.Reader or binary.Write with an io.Writer is very convenient. But working directly with a []byte and binary.BigEndian.Get* or binary.BigEndian.Put* is more efficient. func BenchmarkBinaryWrite ( b * testing . B ) { buf := & bytes . Buffer {} for i := 0 ; i < b . N ; i ++ { buf . Reset () for j := 0 ; j < 10 ; j ++ { binary . Write ( buf , binary . BigEndian , int32 ( j )) } } } func BenchmarkBinaryPut ( b * testing . B ) { var writebuf [ 1024 ] byte for i := 0 ; i < b . N ; i ++ { buf := writebuf [ 0 : 0 ] for j := 0 ; j < 10 ; j ++ { b := make ([] byte , 4 ) binary . BigEndian . PutUint32 ( b , uint32 ( j )) buf = append ( buf , b ... ) } } } jack@hk-47~/dev/go/src/github.com/jackc/go_pgx_perf_observations$ go test -test.bench=BenchmarkBinary -test.benchmem\ntesting: warning: no tests to run\nPASS\nBenchmarkBinaryWrite   1000000        1075 ns/op        80 B/op        5 allocs/op\nBenchmarkBinaryPut  20000000         113 ns/op         0 B/op        0 allocs/op\nok    github.com/jackc/go_pgx_perf_observations 3.485s Not only is binary.Write much slower, it also incurs additional memory allocations. Just this change made a substantial improvement to pgx performance. Measure Last Let me close with another warning to measure before committing optimizations. One use case I wanted to optimize was that of a web API that served JSON produced directly in PostgreSQL. The normal way to do this is to read the JSON into a string then write that string to the HTTP io.Writer. But wouldn't it be so much faster to copy directly from the PostgreSQL io.Reader to the HTTP io.Writer? It's obvious it should be faster, but unfortunately it is incorrect. Benchmarks revealed it was actually slower in the vast majority of cases, and only marginally faster in the best cases. So once again: measure first and measure last. Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-08-07"},
{"website": "Hash-Rocket", "title": "Ruby5 Roundup - Episode 491", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/ruby5-roundup-episode-491", "abstract": "Ruby5 Roundup - Episode 491 by\nPaul Elliott\n\non\nAugust 22, 2014 It's that time again! Lynn and I are back again to bring you the latest news in the Ruby and Rails communities. Here is a quick roundup of what's new in this episode of the Ruby5 podcast. http://ruby5.envylabs.com/episodes/528-episode-491-august-22nd-2014 Semaphore Free for OSS https://semaphoreapp.com/blog/2014/08/14/semaphore-gets-free.html Tired of Travis-CI or just want to try something different? Semaphore just launch a free plan for private and open source projects! PostRank URI https://github.com/postrank-labs/postrank-uri Google's PostRank service released a new Ruby gem that can find URLs in random text and clean all the tracking tokens from popular websites. Renewed Life for STI https://netguru.co/blog/posts/renewed-life-for-sti-with-postgresql-json-type The folks at Netguru just published a blog post about leveraging Postgres' JSON data type for better STI in Rails. Email Validation http://2n.pl/blog/validating_email_address This blog post will give you some example code for validating email addresses using the MX records from the email domain's DNS configuration. Pretty cool stuff! transit-rails https://github.com/jgdavey/transit-rails Rocketeer Josh Davey published a gem that makes it easy to respond with transit formatted data from your Rails app. Pixel Perfect Precision Handbook http://ustwo.com/ppp/ The Pixel Perfect Precision Handbook from UsTwo will give you an amazing crash course in good design. Everyone should read this! So that's it for this episode of Ruby5. If you haven't already, subscribe to the podcast and keep yourself up to date. Thanks for listening! Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-08-22"},
{"website": "Hash-Rocket", "title": "The UI Controller, part 3: the UI Helper", "author": ["\nCameron Daigle\n\n"], "link": "https://hashrocket.com/blog/posts/the-ui-controller-part-3-the-ui-helper", "abstract": "Ruby Design The UI Controller, part 3: the UI Helper by\nCameron Daigle\n\non\nJanuary 27, 2013 Here's the thing: I've settled on a group of simple, practical helper methods that I've ended up using time and time again to reliably generate useful dummy data. It's made my life easier, and I bet it could do the same for you. My use of Faker alongside our UI Controller process (detailed here: part 1 , part 2 ) has developed to the point that I have deemed it worthy of one more post on the topic. So, here's a recent use case. I needed to output a category management interface. This would basically be a list of category checkboxes with subcategory checkboxes, e.g. %ul %li %label %input ( type= \"checkbox\" )\n            Category Label %ul %li %label %input ( type= \"checkbox\" )\n                    Subcategory Label Pretty straightforward, right? Well, the issue we were having is that we were unsure how the interface we were designing would handle long category names or large numbers of subcategories (there's a design post in there somewhere about designing interfaces that elegantly handle odd data, but this post is about the coding side of the process). So here's where Faker helps a ton. I have a couple of cute methods that do the randomization work for us. Here's one that gives us any number in a range (e.g. around(1..5) ): def around ( range ) range . to_a . sample end And here's one that gives us either a number or range of filler words: def lorem_words ( num ) num = around ( num ) if num . is_a? ( Range ) Faker :: Lorem . words ( num ). join ( ' ' ). html_safe end Lastly, this particular interface would hide & show category sections based on whether the checkbox was checked, so I needed to output either true or false. I could just say around([true, false]) but that's a common enough need and these helpers are for readability & ease of typing, so I'll just have a method for it. def coinflip around ([ true , false ]) end So now, with very little extra effort, we can test our UI against a wide range of possible values: %ul - around ( 4 .. 20 ). times do %li %label %input ( type= \"checkbox\" checked= coinflip ) = lorem_words ( 1 .. 3 ) %ul - around ( 3 .. 8 ). times do %li %label %input ( type= \"checkbox\" checked= coinflip ) = lorem_words ( 1 .. 3 ) Hooray! This is some entry-level Ruby, I know, but it's infinitely more valuable than static dummy content, and using techniques like this has allowed me to catch potential layout issues long before they're uncovered by real data after implementation. Here's a gist of the UI Helper as it currently stands – and many thanks to a number of fellow Rocketeers who know far more about Ruby than I do and helped make the whole thing much more elegant. I hope it's as useful for you as it has been for me. Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-01-27"},
{"website": "Hash-Rocket", "title": "Ecto Migrations: Simple to Complex", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/ecto-migrations-simple-to-complex", "abstract": "PostgreSQL Elixir Ecto Migrations: Simple to Complex by\nJosh Branchaud\n\non\nFebruary 23, 2017 Over the lifetime of an application, the application's database undergoes\nmany transformations. New tables are added, new relationships are formed,\ncolumns are added and subtracted, data is massaged, and so on. We know this well. This constant change is what underlies our need for\ndatabase migrations. These migrations span a wide range. On the one end we have the textbook\nmodifications to our schema. We can pull up the Getting Started\nguide for Ecto to see how we\nmight add a table or column. On the other end are more sophisticated\nmigrations. These may involve multiple steps or even migrating and massaging\ndata. In this post, we'll explore this range. Adding A Table Let's add a posts table for our blog app. def change do create table ( :posts ) do add :title , :varchar add :content , :text timestamps () end end This gives us a primary key column id by default. This is a unique\ninteger column starting at 1 and counting upwards using a sequence managed\nby the database. Every post ought to have a title and some content, so we've\nadded columns for those. Ecto gives us the flexibility to use data types\nnative to our database, so we've done just that with some of PostgreSQL's\nnative data types ( varchar and text ).\nLastly, the timestamps() function will produce inserted_at and updated_at timestamp columns. Running mix ecto.migrate will add the posts table and posts_id_seq sequence to our database schema. Altering A Table We said that each post ought to have a title and some content , but we\naren't actually enforcing that. Our database can enforce the presence of\nthose columns when inserting and updating records. We just have to tell it\nto do so with the not null modifier. Let's generate a new migration. def up do alter table ( :posts ) do modify :title , :varchar , null: false modify :content , :text , null: false end end We use the alter function to target our posts table. The modify function declares how the named column is to be modified. We have to specify\nthe data type of the column. If it is the same, nothing changes. This is,\nhowever, an opportunity to change a column's type. For our purposes though,\nwe are just declaring that these columns cannot be null, hence the null:\nfalse . Every up migration requires a down migration. Because we are modifying\nexisting columns, the down migration cannot be inferred, so we have to\nspell it out explicitly. def down do alter table ( :posts ) do modify :title , :varchar , null: true modify :content , :text , null: true end end The down migration explicitly says that these columns can be null. This\ntakes us back to where we were if need to rollback. Adding A Column With A Default Posts take time to write and sometimes go through a number of iterations.\nWe'd like to know whether a particular post is in a draft or published\nstate. We can achieve this by adding a published flag in a new migration. def change do alter table ( :posts ) do add :published , :boolean , null: false , default: false end end A post is either published or it isn't, so a boolean published column will\ndo the trick. This is a binary status, so null doesn't mean anything to\nus, so we make the column not nullable. Lastly, we want to be explicit about\npublishing a post, so we default the published status of a post to false . Referencing Another Table Our posts don't appear out of nowhere. People write them and those people\nwould probably like attribution. We should add a table of people and then\nreference those people in the posts 1 . def change do create table ( :people ) do add :name , :varchar end alter table ( :posts ) do add :people_id , references ( :people ), null: false end end This first creates the people table, again with an implicit primary key id column. The second portion of the migration alters the posts table to\nadd a people_id column. The references function adds a foreign key constraint to ensure the\nintegrity of the relationship between people and their posts. In other\nwords, the database makes sure that post records never get orphaned. With a\nforeign key constraint in place, we can be sure that a post pointing to a\nperson with an id of 6 will definitely be there. If we try to delete the\nperson with an id of 6 , our database will stop us reminding us that we\nneed to deal with the posts that depend on person 6 first. Perhaps the\nright thing to do is delete those posts as well. It should be noted that by default the references function assumes the key it is referencing has a name of id . So, in our\ncase references(:people) means there will be a foreign key from people_id of posts to id of people . If you want to reference a\ndifferently-named column, you can include the :column option as part of\nthe second argument to references specifying the intended name of the\nreferenced column. Transitioning A Column Let's imagine some time has passed and our app, now in production, has some\npeople who are creating and publishing posts. We'd like to be able to display in the UI the time at which a post was\npublished. We consider utilizing the updated_at column when a post in the published state, but a post can be updated after it is published, so this\nis not an accurate indicator. Our next thought is to add a published_at timestamp column. If the post is\ntransitioned to the published state, then we set published_at to the\ncurrent time and then display that time as long as the post remains in the published state. If published is changed from true to false for a\npost, then we can null out the published_at value. That'll work, but\nwe've run into a data design smell. With this second approach, we've introduced a denormalization of our data.\nThe published and published_at fields have partially overlapping\nconcerns and it is possible for them to get out of sync. We can achieve the\nsame ends with a third, simplified option that only involves one column. We can replace the published flag with the published_at timestamp. We\nmaintain the binary indicator because published_at being null tells us it\nis in a draft state whereas published_at being some timestamp tells us\nboth that it is published and the time at which it was published. Let's add this migration. def change do alter table ( :posts ) do add :published_at , :timestamp remove :published end end That will do the trick. But hold up a second. If we run this migration\nagainst our production database, we will be doing something very sad. We'll\nbe throwing away a bunch of data and messing up the publish status of all\nour existing posts. We need to transition some data before we can remove the published column.\nWe need to set all existing unpublished posts to have null for published_at . All the existing published posts, however, need to be\nassigned a timestamp. We already know that our existing data model was not\nsufficient for representing when a post was published. We are going to have\nto do some approximation, and that's okay. Let's say that, in terms of this data migration, setting the published_at value for all published posts to the same time that they were all last\nupdated is sufficient. So, for each post with published set to true , we\nneed to duplicate that post's updated_at value into the newly-created published_at column. That will require an update_all statement 2 . Let's modify our migration from above. import Ecto . Query def up do alter table ( :posts ) do add :published_at , :timestamp end from ( p in \" posts\" , update: [ set: [ published_at: p . updated_at ]], where: p . published ) |> MyApp . Repo . update_all ([]) alter table ( :posts ) do remove :published end end This migration looks like exactly what we want, but when we run mix\necto.migrate , we are going to see an odd error. ** ( Postgrex . Error ) ERROR ( undefined_column ): column p0 . published_at does not exist How can it not exist for our update_all function when we clearly added\nthat column just a few lines above? The issue is that the Ecto migrator is queueing up everything in our\nmigration to be executed against the database. When it reaches the update_all statement it tries to create a prepared statement that involves\na column that has not yet been created. To ensure that the first portion of our migration is executed before the update_all is reached, we can utilize the flush() function.\nPlacing flush() after the first portion of the migration will force\neverything queued up so far to be executed against the database. This clears\nthe way for the rest of our migration to work. def up do alter table ( :posts ) do add :published_at , :timestamp end flush () from ( p in \" posts\" , update: [ set: [ published_at: p . updated_at ]], where: p . published ) |> MyApp . Repo . update_all ([]) alter table ( :posts ) do remove :published end end And like before, with any up migration, we need a down . def down do alter table ( :posts ) do add :published , :boolean , null: false , default: false end flush () from ( p in \" posts\" , update: [ set: [ published: true ]], where: not is_nil ( p . published_at )) |> MyApp . Repo . update_all ([]) alter table ( :posts ) do remove :published_at end end This requires some of the same tricks as the up migration because we want\nto be able to put the data back in place as best as we can in the event of\na rollback. Conclusion In this post, we've explored a range of simple to complex Ecto migrations\ninvolving both schema changes and data transformation. We looked at how you\ncan create new tables as well as alter existing ones. We even explored in\ndetail how you would go about massaging and migrating data in the event that\nyou need to replace one column with another. Ecto is an abstraction on top\nof SQL and we may need to drop down to raw SQL migrations in some cases,\nbut from what we've seen Ecto can get us pretty far. Photo credit for cover image: Paul Morris, unsplash.com The following migration assumes an empty database, as if we are in the\nmidst of creating an initial MVP of our application. If the database\nalready contained posts, then adding a non-nullable column would cause an\nerror due to a violation of the not null constraint. ↩ When using the update_all function, there are two ways of specifying the terms of the update. As we\ndid above, the :update keyword can be used within our Query struct to\nspecify how to update the relevant records. Because we went this route, we\nleave the required updates argument to update_all as an empty list. If\nwe don't specify the update as part of the query, then we need to include\nthose details as part of the updates argument to update_all . ↩ Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-02-23"},
{"website": "Hash-Rocket", "title": "Hashrocket University 2011 is coming up!", "author": ["\nAdam Lowe\n\n"], "link": "https://hashrocket.com/blog/posts/hashrocket-university-2011-is-coming-up", "abstract": "Hashrocket University 2011 is coming up! by\nAdam Lowe\n\non\nMarch 11, 2011 We launched Hashrocket University in 2010 at Railsconf and had a tremendous first class. The feedback we received from the attendees afterward was that they got a lot out of Hashrocket University but the would have benefitted from three things, a venue that was more development friendly, a higher instructor to attendee ratio and a longer format that allowed for more time pairing. This year we are excited to be hosting the Spring 2011 Hashrocket University at our headquarters in Jacksonville Beach, FL. This will allow us to host the class on our home turf with first class development equipment and surroundings. This also allowed us to raise the instructor to student ratio to 2 to 1. Each pair will have their own Rocketeer tailoring the instruction to the areas they want to get better at. We have expanded the format from one day to two days and modified the course content to focus primarily on pairing with attendees. The class will be limited to 18 attendees. It includes two nights of beach front hotel accommodations, a welcome reception Friday night, breakfast and lunch both days, and two days of pairing working with veteran Rocketeers. Plus you'll get a sweet t-shirt and other cool swag. If you have any questions drop us an email at university@hashrocket.com and you can book tickets at the official Hashrocket University site . Was this post helpful? Share it with others. Tweet Share Post", "date": "2011-03-11"},
{"website": "Hash-Rocket", "title": "Functional Eye for the Ruby Guy", "author": ["\nJohnny Winn\n\n"], "link": "https://hashrocket.com/blog/posts/functional_eye_ruby_guy", "abstract": "Ruby Functional Eye for the Ruby Guy by\nJohnny Winn\n\non\nJanuary 23, 2013 The new year is in full swing with February approaching fast and bringing with it the much anticipated release of Ruby 2.0 . So it seems like the perfect time to explore a couple of its new features. Two that I found interesting were the Enumerator::Lazy and Refinements so today we'll look at implementing a solution to a Project Euler challenge. This implementation will allow us to utilize these two features without a contrived example. After all, the Ruby 2.0 features are the focus. Gaining Focus For those new to Project Euler, it's a site that offers a series of mathematical, or computer programming, challenges to sharpen your problem solving skills. The challenges range in difficulty but since our focus is the language features the problem isn't as important as the solution. Our challenge is to find the sum of all multiples of 3 and 5 below 1000. Since we are looking for an answer before we begin our implementation, it's safe to spike using existing Ruby functionality. ( 1 .. 1000 ). select { | i | i % 3 == 0 || i % 5 == 0 }. reduce (: + ) => 234168 After verifying that the answer is correct on the Project Euler site, we can write a test and start exploring the new features. Our test will provide a fall back as we make our changes. describe \"Ruby 2.0 features\" do context \"Multiples of 3 and 5\" do it \"Sum of all the multiples of 3 or 5 below 1000\" do FunctionalRuby . sum_multiples_of_3_and_5_for_range (( 1 .. 1000 )). should eq ( 234168 ) end end end This is arguably a bit verbose and of course the test fails, however, it's a good starting point so we can begin. A Lazy Sequence to Start The first Ruby 2.0 feature we will implement is Enumerator::Lazy but before we do, let's look back at our original spike. ( 1 .. 1000 ). select { | i | i % 3 == 0 || i % 5 == 0 }. reduce (: + ) => 234168 At first glance this code seems harmless enough and many of us have probably implemented a similar solution. However, this code can be misleading because Ruby creates a complete array prior to executing the reduce(:+) function. This proves particularly sticky when subsequent calls pull only a subset of data from the original array. The entire array must be built before retrieving the subset. Take a look at this example. ( 1 .. Float :: INFINITY ). select { | x | x % 3 == 0 || x % 5 == 0 }. take ( 5 ). reduce (: + ) Because the array must be completely built prior to calling take(5) and it's an infinite set, the additional methods are never called. A common idiom in functional programming is lazy evaluation. Lazy evaluation delays the functions execution until its value is needed. Ruby 2.0 introduces lazy evaluation through Enumerator::Lazy. This addition can solve our infinity problem by delaying the execution of the iteration. ( 1 .. Float :: INFINITY ). lazy . select { | x | x % 3 == 0 || x % 5 == 0 }. take ( 5 ). reduce (: + ) => 33 So our next step is to implement a solution for our challenge using Enumerator::Lazy feature. class FunctionalRuby def self . sum_multiples_of_3_and_5_for_range ( range ) range . lazy . select { | i | i % 3 == 0 || i % 5 == 0 }. reduce (: + ) end end When we run our tests, everything passes and all is well, kinda. Refining with Refinements Although we are moving in the right direction, our method doesn't really reflect a functional programming style. First, we have a single function with multiple responsibilities. Second, if we could curry the method calls it would provide a solution that is easier to read. Prior to Ruby 2.0 one might suggest monkey patching the Enumerator::Lazy module to add methods in order to chain them to lazy . But we don't want our methods to pollute the Ruby module in every instance because these methods are specific to our solution. Ruby 2.0 offers a solution in the Refinements feature. The Refinement feature is dependent on two methods: Module#refine and using . We can start our refactoring by adding a custom module and refining the Enumerator::Lazy module. Because it's unique to our Project Euler challenge we will name it accordingly. module ProjectEuler refine Enumerator :: Lazy do def multiple_of_3 ( number ) number % 3 == 0 end def multiple_of_5 ( number ) number % 5 == 0 end def select_multiples_of_3_and_5 select { | x | multiple_of_3 ( x ) || multiple_of_5 ( x ) } end def then_sum reduce (: + ) end end end class FunctionalRuby using ProjectEuler def self . sum_multiples_of_3_and_5_for_range ( range ) range . lazy . select_multiples_of_3_and_5 . then_sum end end A quick run of the test to verify we are still passing and we can start to look at what's happening in the solution. The ProjectEuler module has defined a refinement for the Enumerator::Lazy module and added the additional methods. Any class that applies the ProjectEuler module through a using method can take advantage of the monkey patch. This also includes overriding existing methods. We can DRY this code up by overriding the method_missing call and consolidating the multiple_of_ methods. module ProjectEuler refine Enumerator :: Lazy do def method_missing ( method , * args , & block ) if method . to_s =~ /multiple_of_(.*)$/ multiple_of ( $1 . to_i , args [ 0 ]. to_i ) end end def multiple_of ( multiple , number ) number % multiple == 0 end def select_multiples_of_3_and_5 select { | x | multiple_of_3 ( x ) || multiple_of_5 ( x ) } end def then_sum reduce (: + ) end end end Now this may be taking things to the extreme for this scenario but the purpose is to demonstrate that the method_missing call can be overridden in this refinement without affecting the original module. We can test this by calling our custom method without applying the using statement to a class class WithoutEuler def self . call_multiple_of_3_on_lazy ( 1 .. 5 ). lazy . select_multiples_of_3_and_5 end end WithoutEuler . call_multiple_of_3_on_lazy => NoMethodError : undefined method `select_multiples_of_3_and_5' for #<Enumerator::Lazy: 1..1000> Wrapping Up It's clear that these examples are more academic then real-world but they provide an opportunity to review specific language features rather than focusing on the solution details. Ruby is a procedural language, but it can be fun to apply functional idioms to explore its nuances. These two features seemed like an excellent starting point for this type of experimentation. The Ruby 2.0 release has several more interesting features and we will be exploring many of them through similar experiments and reporting back our findings. Until then, happy hacking! Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-01-23"},
{"website": "Hash-Rocket", "title": "Selleck: a jQuery Handlebars-like Templating System", "author": ["\nShane Riley\n\n"], "link": "https://hashrocket.com/blog/posts/selleck-a-jquery-handlebars-like-templating-system", "abstract": "Javascript Selleck: a jQuery Handlebars-like Templating System by\nShane Riley\n\non\nApril 16, 2012 Recently, I was working on a project that refreshed search results and their corresponding Google Map markers via Ajax and wanted to make it as easy to write the JSON data to HTML as possible. I initially thought to use Handlebars and write my HAML partials as part of the view, but thought that might be more than what I needed for our use, and since we're loading jQuery, jQuery UI, Google Maps and our search script, I wanted to try to keep the total amount of code added to the front end as small as possible. 10 lines of code later, I had my first version of Selleck parsing my JSON data and writing my HTML search results for me. As a disclaimer, let me point out that Selleck isn't nearly as robust as Handlebars or Mustache (or underscore templates, etc.), but with all of the Hashrocket Javascript plugins, they are just enough to get the job done and give us the flexibility we want to make them useful in all of our projects. On to the flashy bits! Here's how you'd use Selleck to transform a JSON object and HTML partial into a fleshed-out view. First, you write you HAML/HTML within a container that will show up on the page (or load it via Ajax!). Something like this will do the trick: %script #tweet ( type= \"text/selleck_tmpl\" ) %article ( id= \"{{id}}\" ) %blockquote ( cite= \"{{author}}\" ) %p {{tweet}} %time ( datetime= \"{{post_date}}\" ) {{localized_post_date}} Then you can take a tweet object like this: var tweet = { id : \"08486\" , author : \"@shaneriley\" , post_date : \"2012-03-30\" , localized_post_date : \"Mar 30, 2012\" , tweet : \"Waterfall Selleck sandwich!  http://vurl.me/QDSJ\" }; and write it to your page with jQuery like this: $twitter_stream . html ( selleck ( $ ( \"script#tweet\" ). html (), tweet )); That's it. You've got HTML that has been filled in with the JSON data that was a much lighter request than your standard $.load or other typical get-HTML-via-Ajax request and much cleaner than what you'd have to write to fill a series of HTML elements with object properties. Selleck will also process an array of JSON-like objects and create a completed HTML partial for each automatically. If our previous example were to look like this: var tweets = [{ id : \"08486\" , author : \"@shaneriley\" , post_date : \"2012-03-30\" , localized_post_date : \"Mar 30, 2012\" , tweet : \"Waterfall Selleck sandwich!  http://vurl.me/QDSJ\" }, { id : \"08487\" , author : \"@shaneriley\" , post_date : \"2012-03-30\" , localized_post_date : \"Mar 30, 2012\" , tweet : \"It's too bad Mustache May is no more.\" }]; Selleck would iterate over each tweet and return one combined string of all the filled-in partials. What about loops within a partial? Let's convert our tweet example to blog posts and add tagging. We may have 0 to n tags for each post, and rather than create another template for the tags and run Selleck on that collection afterwards, why not spit them out on first pass? Selleck's got you covered. var posts = [{ id : \"08486\" , author : \"@shaneriley\" , post_date : \"2012-03-30\" , localized_post_date : \"Mar 30, 2012\" , content : \"Witty blog post goes here.\" }, { id : \"08487\" , author : \"@shaneriley\" , post_date : \"2012-03-30\" , localized_post_date : \"Mar 30, 2012\" , content : \"Kickball at 2 this Saturday!\" , tags : [ \"kickball\" , \"weekend\" ] }]; %script #post ( type= \"text/selleck_tmpl\" ) %article ( id= \"{{id}}\" ) %p {{content}} %footer %p by {{author}} on %time ( datetime= \"{{post_date}}\" ) {{localized_post_date}} %ul {{- tags.each do |tag|}} %li {{tag}}\n                {{- end}} Oh noes! Since there's no tags in the first post, we get an li with the text {{tag}} in it! That's embarrassing. Let's remove any empty elements while we're processing the template. $blog . html ( selleck ( $ ( \"script#post\" ). html (), posts , { remove_empty_els : true })); Yay! Now we're rocking nested loops and removing any elements whose content is nothing more than an empty template tag. Note that this option doesn't remove an element if it has a combination of plain and template text. In the near future, I hope to add if blocks that will allow you to conditionally remove or provide alternate content for missing properties. For now, remove_empty_els is the only option you can pass to Selleck. And that's it! Now you no longer have to write those nasty Ajax callbacks that generate gobs of HTML in a place where it doesn't belong. Want to give it a try? Grab the code from GitHub and get to it! Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-04-16"},
{"website": "Hash-Rocket", "title": "We're hosting a hackfest!", "author": ["\nAaron Kalin\n\n"], "link": "https://hashrocket.com/blog/posts/we-re-hosting-a-hackfest", "abstract": "We're hosting a hackfest! by\nAaron Kalin\n\non\nJuly 18, 2011 Have you ever wanted to contribute back to Rails? Hashrocket Chicago will be hosting a 2-day hackfest to help test the upcoming 3.1 release.  The purpose is to help iron out any outstanding bugs or issues before they officially release.  You can upgrade an existing Rails project, make a new one or check open bugs and reproduce some of the issues. If you're interested, pelase visit the signup page at gathers.us Looking forward to seeing you there! Was this post helpful? Share it with others. Tweet Share Post", "date": "2011-07-18"},
{"website": "Hash-Rocket", "title": "Ruby Developer Meetups: Chicago", "author": ["\nDave Ly"], "link": "https://hashrocket.com/blog/posts/ruby-developer-meetups-chicago", "abstract": "Community Ruby Developer Meetups: Chicago by\nDave Lyon\n\non\nSeptember 21, 2011 In anticipation of a large group of new Rubyists coming to Chicago in October via Code Academy , I thought it would be helpful to put together a resource for various meetups around Chicago. First, you might consider signing up for Geek Out Chicago! -- a monthly newsletter of events around Chicago. Here's a list of meetups that occur at least monthly: Code and Coffee : Meets every Tuesday at the Starbucks by the brown line stop at Chicago and Franklin. 7AM - 9AM Chicago Ruby : Meets several times a month in various locations. A great place to meet your fellow Rubyists. Chicago Ruby on Meetup Downtown Chicago: On the first Tuesday of each month. 6PM-6:30PM pizza & networking. 6:30PM - 8PM meeting. Elmhurst: On the third Saturday of each month at 10:30AM. Hack Nights: As scheduled. VimChi : Hosted by Hashrocket Chicago, this meetup happens once a month and covers various plugins and techniques for using Vim. Vim Chicago on Meetup Geekfest : Groupon has continued Obtiva's tradition of weekly Geekfests.  Covers a wide range of topics around Ruby, Agile, Startups, Databases... etc. Geekfest on GathersUs Geek Breakfast : Once a month, Chicago's geek community meets for breakfast at Wow Bao at State and Lake.  Events are posted on GathersUs Chicago Javascript : Talks about anything and everything regarding Javascript. Chicago Javascript on Meetup Chicago NodeJS : Hosted by Hashrocket Chicago, this meetup is all about NodeJS. Chicago NodeJS on Meetup Did I miss one? Tweet me @daveisonthego and I'll add it to the list! Was this post helpful? Share it with others. Tweet Share Post", "date": "2011-09-21"},
{"website": "Hash-Rocket", "title": "Format Your Elixir Code Now", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/format-your-elixir-code-now", "abstract": "Elixir Format Your Elixir Code Now by\nJake Worth\n\non\nOctober 17, 2017 Elixir 1.6 will be shipping with an important new feature: autoformatting. Let's explore it today. Autoformatting is a common practice in languages communities like Go.\nNow, this practice is coming to Elixir as part of the language. Borrowing the\narguments in favor of autoformatting\nfrom the Go community, autoformatted code ought to be: Easier to write: never worry about minor formatting concerns while hacking away. Easier to read: when all code looks the same you need not mentally convert others' formatting style into something you can understand. Easier to maintain: mechanical changes to the source don't cause unrelated changes to the file's formatting; diffs show only the real changes. Uncontroversial: never have a debate about spacing or brace position ever again! Whether you agree with all these points or not, the Elixir language (which is\nlargely written in Elixir) just underwent a massive community-powered\nrefactor to conform with its new built-in formatter. New pull requests to Elixir must satisfy the\nformatter to be considered for acceptance. The Elixir community is behind the\ntool, and running it on your Elixir project is going to become the norm. But we don't have to wait for Elixir 1.6. Let's try it now! Install Elixir Master There are a few ways to get the latest Elixir. If you use Homebrew , install Elixir from master: $ brew install elixir --HEAD $ elixir -v Erlang/OTP 20 [ erts-9.1.2] [ source ] [ 64-bit] [ smp:8:8] [ ds:8:8:10] [ async-threads:10] [ hipe] [ kernel-poll:false] [ dtrace]\n\nElixir 1.6.0-dev ( 425cebf ) ( compiled with OTP 20 ) Alternately, build from source: $ git clone git@github.com:elixir-lang/elixir.git $ cd elixir $ make clean compile $ bin/elixir -v Erlang/OTP 20 [ erts-9.1.2] [ source ] [ 64-bit] [ smp:8:8] [ ds:8:8:10] [ async-threads:10] [ hipe] [ kernel-poll:false] [ dtrace]\n\nElixir 1.6.0-dev ( 425cebf ) ( compiled with OTP 20 ) If mix help format returns a man page, we're in business. Format Your Code Now that we have the formatter, let's use it: $ mix format lib/path/to/file.ex Or, run it against your entire program! To do this, we'll need a .formatter.exs config file that\ntells the formatter what to include. An example: [ inputs: [ \" mix.exs\" , \" {config,lib,test}/**/*.{ex,exs}\" ] ] This will capture the Mixfile and Elixir files in config , lib , and test . With this file in the root of your project, format all the files: $ mix format Conclusion Jose Valim's Elixir issue #6643 inspired this post. Check it out, and learn how to use the formatter in a variety of edge cases. We're running the formatter right now against all the files in the Today I\nLearned codebase, starting with this pull\nrequest . Part of open-sourcing\nToday I Learned (first as a Rails app, now as a Phoenix app )\nmeans writing code that's as readable as possible. Complying with the Elixir formatter now helps us achieve that. Try the Elixir formatter out today, and consider opening a pull request against an Elixir project you care about.\nYour proposed changes should spark an interesting debate about Elixir style and autoformatting as a practice. Sources: https://blog.golang.org/go-fmt-your-code https://github.com/elixir-lang/elixir/issues/6643 https://unsplash.com/photos/6GjHwABuci4 Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-10-17"},
{"website": "Hash-Rocket", "title": "Generate Dates in PostgreSQL", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/generate-dates-in-postgresql", "abstract": "PostgreSQL Generate Dates in PostgreSQL by\nJosh Branchaud\n\non\nOctober  3, 2017 PostgreSQL has shipped with the generate_series() function for quite some\ntime. This function is often used to generate series of numeric data. For\ninstance, in my post, Understanding Common Table Expressions with\nFizzBuzz ,\nI used it to generate a series of integers from 1 to 100. As of PostgreSQL 8.4, support for generating series of timestamp data was\nadded. I don't see this used often, so let's take a look at it. We could take a peak at the online documentation to see the function\nsignature, or we could pull it up in psql using \\df . > \\ df generate_series () List of functions Schema | Name | Result data type | Argument data types | Type ------------+-----------------+-----------------------------------+--------------------------------------------------------------------+-------- pg_catalog | generate_series | SETOF bigint | bigint , bigint | normal pg_catalog | generate_series | SETOF bigint | bigint , bigint , bigint | normal pg_catalog | generate_series | SETOF integer | integer , integer | normal pg_catalog | generate_series | SETOF integer | integer , integer , integer | normal pg_catalog | generate_series | SETOF numeric | numeric , numeric | normal pg_catalog | generate_series | SETOF numeric | numeric , numeric , numeric | normal pg_catalog | generate_series | SETOF timestamp with time zone | timestamp with time zone , timestamp with time zone , interval | normal pg_catalog | generate_series | SETOF timestamp without time zone | timestamp without time zone , timestamp without time zone , interval | normal The last two records are what we are looking for -- support for timestamps\nwith and without time zones. Notice that each requires three arguments. The first two are the lower and\nupper bound timestamps of the series to be generated. The interval specifies\nwhat amount of spacing to put between each timestamp when generating the\nseries. Let's try it out by generating the series of days in 2017. > select generate_series ( ( date '2017-01-01' ):: timestamp , ( date '2017-12-31' ):: timestamp , interval '1 day' ); generate_series --------------------- 2017 - 01 - 01 00 : 00 : 00 2017 - 01 - 02 00 : 00 : 00 2017 - 01 - 03 00 : 00 : 00 2017 - 01 - 04 00 : 00 : 00 2017 - 01 - 05 00 : 00 : 00 2017 - 01 - 06 00 : 00 : 00 2017 - 01 - 07 00 : 00 : 00 2017 - 01 - 08 00 : 00 : 00 2017 - 01 - 09 00 : 00 : 00 2017 - 01 - 10 00 : 00 : 00 2017 - 01 - 11 00 : 00 : 00 2017 - 01 - 12 00 : 00 : 00 ... Take note that we have to satisfy the function signature, so we create a\ndate using date and then coerce it to a timestamp . There are 365\nresults, so I've truncate them a bit. PostgreSQL understands the calendar, so you can even count on proper\nhandling of concepts like a leap year. > select * from ( select generate_series ( ( date '2020-01-01' ):: timestamp , ( date '2020-12-31' ):: timestamp , interval '1 day' ) ) as twenty_twenty ( d ) where date_part ( 'month' , twenty_twenty . d ) = 2 ; d --------------------- 2020 - 02 - 01 00 : 00 : 00 2020 - 02 - 02 00 : 00 : 00 2020 - 02 - 03 00 : 00 : 00 2020 - 02 - 04 00 : 00 : 00 2020 - 02 - 05 00 : 00 : 00 2020 - 02 - 06 00 : 00 : 00 2020 - 02 - 07 00 : 00 : 00 2020 - 02 - 08 00 : 00 : 00 2020 - 02 - 09 00 : 00 : 00 2020 - 02 - 10 00 : 00 : 00 2020 - 02 - 11 00 : 00 : 00 2020 - 02 - 12 00 : 00 : 00 2020 - 02 - 13 00 : 00 : 00 2020 - 02 - 14 00 : 00 : 00 2020 - 02 - 15 00 : 00 : 00 2020 - 02 - 16 00 : 00 : 00 2020 - 02 - 17 00 : 00 : 00 2020 - 02 - 18 00 : 00 : 00 2020 - 02 - 19 00 : 00 : 00 2020 - 02 - 20 00 : 00 : 00 2020 - 02 - 21 00 : 00 : 00 2020 - 02 - 22 00 : 00 : 00 2020 - 02 - 23 00 : 00 : 00 2020 - 02 - 24 00 : 00 : 00 2020 - 02 - 25 00 : 00 : 00 2020 - 02 - 26 00 : 00 : 00 2020 - 02 - 27 00 : 00 : 00 2020 - 02 - 28 00 : 00 : 00 2020 - 02 - 29 00 : 00 : 00 We repurposed our previous query for the year 2020. After wrapping it in a\nsubquery, we are able to filter what we are looking at, in this case, just\nthe month of February (i.e. when the month is 2 ). Notice it has 29 days! We've only look at intervals of '1 day' so far, but we can do whatever\ninterval we want. How about every 3 days? > select generate_series ( ( date '2017-01-01' ):: timestamp , ( date '2017-12-31' ):: timestamp , interval '3 days' ); generate_series --------------------- 2017 - 01 - 01 00 : 00 : 00 2017 - 01 - 04 00 : 00 : 00 2017 - 01 - 07 00 : 00 : 00 2017 - 01 - 10 00 : 00 : 00 2017 - 01 - 13 00 : 00 : 00 2017 - 01 - 16 00 : 00 : 00 2017 - 01 - 19 00 : 00 : 00 2017 - 01 - 22 00 : 00 : 00 ... Or every month? > select generate_series ( ( date '2017-01-01' ):: timestamp , ( date '2017-12-31' ):: timestamp , interval '1 month' ); generate_series --------------------- 2017 - 01 - 01 00 : 00 : 00 2017 - 02 - 01 00 : 00 : 00 2017 - 03 - 01 00 : 00 : 00 2017 - 04 - 01 00 : 00 : 00 2017 - 05 - 01 00 : 00 : 00 2017 - 06 - 01 00 : 00 : 00 2017 - 07 - 01 00 : 00 : 00 2017 - 08 - 01 00 : 00 : 00 2017 - 09 - 01 00 : 00 : 00 2017 - 10 - 01 00 : 00 : 00 2017 - 11 - 01 00 : 00 : 00 2017 - 12 - 01 00 : 00 : 00 We can even use a really odd interval of time. > select generate_series ( ( date '2017-01-01' ):: timestamp , ( date '2017-12-31' ):: timestamp , interval '1 month 1 day 1 hour' ); generate_series --------------------- 2017 - 01 - 01 00 : 00 : 00 2017 - 02 - 02 01 : 00 : 00 2017 - 03 - 03 02 : 00 : 00 2017 - 04 - 04 03 : 00 : 00 2017 - 05 - 05 04 : 00 : 00 2017 - 06 - 06 05 : 00 : 00 2017 - 07 - 07 06 : 00 : 00 2017 - 08 - 08 07 : 00 : 00 2017 - 09 - 09 08 : 00 : 00 2017 - 10 - 10 09 : 00 : 00 2017 - 11 - 11 10 : 00 : 00 2017 - 12 - 12 11 : 00 : 00 PostgreSQL's date and timestamp capabilities are quite powerful. What we've\nlooked at above can be put to great use in report generation. For instance,\nif you need aggregate data for certain intervals of time, you can join your\ntimestamped data against any generate_series table. Instead of pulling\nlarge amounts of data into application land for processing, it can be done\nin the database. Your CPU and your users will thank you. Cover image by bady qb on Unsplash.com Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-10-03"},
{"website": "Hash-Rocket", "title": "Programming Robot Sub Systems with Elixir", "author": ["\nChris Erin\n\n"], "link": "https://hashrocket.com/blog/posts/programming-robot-sub-systems-with-elixir", "abstract": "Elixir Programming Robot Sub Systems with Elixir by\nChris Erin\n\non\nJuly 25, 2019 I like to dream about robots.  That's a pretty broad statement so let me narrow that down.  I like to dream about semi-automonous 4 wheeled rovers, moving around the surface of the moon controlled by teams of scientists, students, and hobbyists via a command line interface.  OK, that's a bit more specific. Alas, it's just a dream, but it gets me imagining different ways that Elixir and Processes might work to facilitate the robot rover's operation. This post is about an imagined light sensing sub system.  Light is sensed with a sensor and when that sensor changes it sends a message to a broker which then forwards the message to it's subscribers.  I want to test this system with ExUnit, both to get a feel for how the different parts fit together and to get a feel for how to test GenServers.  For my purposes this system translates into 3 GenServers: LightSensor - a GenServer that interfaces with a hardware sensor and sends messages to a broker. MessageBroker - a GenServer that manages subscribed processes and sends messages to those processes LightServer - a GenServer that is subscribed to light messages, receives light messages and makes decisions based on those messages In this post I will go through the code of each GenServer to show how each is implemented, how they work together and how they can be tested. LightSensor This GenServer would provide the interface for the hardware.  I'm not really a hardware person (despite having attended the Nerves Workshop!). While I can imagine a GenServer being the interface to a sensor like this , I don't particularly care about how that's implemented. What I do care about is that the GenServer produces a steady stream of messages that might look like what a light sensor might produce.  Because I plan to test drive this with ExUnit, instead of actual hardware, I want a MockLightServer that will produce data that looks and behaves like real data. That will look something like this: defmodule LightSensor . Data do def data do 1 .. 100 |> Enum . map ( fn n -> { Enum . random ( 0 .. n ), Enum . random ( 0 .. 100 )} end ) end end defmodule MockLightSensor do use GenServer @name __MODULE__ def start_link ( message_broker_pid , start_end_notifier_pid ) do result = { :ok , _pid } = GenServer . start_link ( __MODULE__ , %{ message_broker: message_broker_pid , start_end_notifier: start_end_notifier_pid , data: LightSensor . Data . data () }, name: @name ) result end def start_messages () do GenServer . cast ( @name , :next ) end @impl true def init ( args ) do { :ok , args } end @impl true def handle_info ( :next , state ) do GenServer . cast ( @name , :next ) { :noreply , state } end @impl true def handle_cast ( :next , %{ message_broker: broker , start_end_notifier: start_end_notifier_pid , data: [{ next_light_message , _ } | []] }) do GenServer . cast ( broker , { :light_message , next_light_message }) # signal to the test process that the data sending has ended send ( start_end_notifier_pid , :ended ) { :noreply , %{}} end @impl true def handle_cast ( :next , %{ message_broker: broker , start_end_notifier: start_end_notifier_pid , data: [{ interval , next_light_message } | rest ] }) do Process . send_after ( self (), :next , interval ) GenServer . cast ( broker , { :light_message , next_light_message }) { :noreply , %{ message_broker: broker , start_end_notifier: start_end_notifier_pid , data: rest }} end end This process has three keys in it's state map, message_broker , start_end_notifier and data . message_broker - the pid that it will send light messages to start_end_notifier - the pid that will be notified when there is no more data data - the mock data in the format of {interval, next_light_message} This GenServer is sending messages to three different places.  The first is imitating how the light sensor might behave by sending light messages to the message broker.  The second and third messages are for testing purposes.  The test needs to know when it's done, so the GenServer will send a message to the test process when it's out of data.  The GenServer needs to know when to send another message, so it will take the interval message from the data tuple and send a message to itself after the interval has passed. MessageBroker This GenServer follows a pub/sub pattern.  A pub/sub GenServer here will allow many different business logic processes to subscribe to light messages without the LightSensor server needing to know or manage processes that might subscribe temporarily. It looks like this: defmodule MessageBroker do use GenServer @name __MODULE__ def start_link () do GenServer . start_link ( @name , %{ subscriptions: []}) end @impl true def init ( args ) do { :ok , args } end @impl true def handle_cast ({ :subscribe , pid }, %{ subscriptions: subs }) do { :noreply , %{ subscriptions: [ pid | subs ]}} end @impl true def handle_cast ({ :light_message , message }, %{ subscriptions: subs }) do Enum . each ( subs , fn sub_pid -> GenServer . cast ( sub_pid , { :light , message }) end ) { :noreply , %{ subscriptions: subs }} end end This version only handles two messages subscribe and light_message . subscribe messages come from interested processes and light_message messages come from the LightSensor itself. When receiving a LightSensor message it will loop through all the subscribed processes and send a light message to each. LightServer The LightServer will attach meaning and behavior to the messages it receives.  This is where the business logic would live.  Conceivably, the LightServer would make decisions about where to move, how much energy to use, whom to notify, or maybe even when to fold or unfold the solar panels.  In this example, it's pretty dumb, just recording the light messages that are above a certain value. defmodule LightServer do use GenServer @name __MODULE__ @impl true def init ( args ) do { :ok , args } end def start_link ( broker ) do { :ok , pid } = GenServer . start_link ( __MODULE__ , %{ very_bright_measurements: []}, name: @name ) GenServer . cast ( broker , { :subscribe , pid }) { :ok , pid } end def get_very_hot_temperatures do GenServer . call ( @name , :get_very_hot_temperatures ) end @impl true def handle_cast ({ :light , light_amount }, %{ very_bright_measurements: amounts }) when light_amount > 90 do { :noreply , %{ very_bright_measurements: [ light_amount | amounts ]}} end @impl true def handle_cast ({ :light , _light_amount }, state ) do # do nothing { :noreply , state } end @impl true def handle_call ( :get_very_bright_measurements , _from , %{ very_bright_measurements: amounts } = state ) do { :reply , amounts , state } end end Testing the Sub System What makes programming this subsystem fun is being able to test it. I want to test that the LightServer has data when all the processes have been sent. The ExUnit test will look like this: defmodule LightSensorTest do use ExUnit . Case setup do { :ok , message_broker_pid } = MessageBroker . start_link () { :ok , _pid } = MockLightSensor . start_link ( message_broker_pid , self ()) { :ok , _pid } = LightServer . start_link ( message_broker_pid ) end test \" Does Light Server receive messages?\" do MockLightSensor . start_messages () assert_receive ( :ended , 10000 ) assert very_bright_measurements = LightServer . get_very_bright_measurements () assert ( length ( very_bright_measurements ) > 0 ) end end It's a simple test, just assert that the LightServer has received the expected data.  This assertion will fail, however, without allowing all the messages to fly around and be processed.  Because the messages are almost all cast (read: asynchronous) the test would reach it's end before anything had happened without the assert_received call.  This function waits for a specific message to be sent to the test process in a certain amount of time.  In this case, we wait 10 seconds for all the messages to be processed and for the MockLightSensor to send the :ended message to the test process. Conclusion This imagined subsystem isn't imagined exhaustively.  It's just a sketch of something that might exist in a future rover robot.  It's an exercise that I can use to get better at understanding Erlang/Elixir process patterns and to get better at testing processes in general.  In the future it might be fun to imagine other rover robot subsystems. Image via NASA Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-07-25"},
{"website": "Hash-Rocket", "title": "Ruby5 Roundup - Episode 485", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/ruby5-roundup-episode-485", "abstract": "Ruby5 Roundup - Episode 485 by\nPaul Elliott\n\non\nAugust  1, 2014 The soothing yet informative voices of Lynn and I are back again to bring you the latest news in the Ruby and Rails communities. Here is a quick roundup of what's new in this episode of the Ruby5 podcast. http://ruby5.envylabs.com/episodes/522-episode-485-august-1st-2014 Capistrano Tutorial http://www.gotealeaf.com/blog/deploy-rails-apps-with-capistrano Kevin Wang from Tealeaf Academy sets the bar high yet again with another feature-filled blog post on capistrano. If you are new to using it for deployments or are wondering how any why it does some of the things it does, this step-by-step tutorial will certainly give you a good education. Memoization Patterns http://www.justinweiss.com/blog/2014/07/28/4-simple-memoization-patterns-in-ruby-and-one-gem This blog post from Justin Weiss talks about the different ways of memoizing method return values. If you're making the same call twice, take a look at this article and learn how to cache those responses! Mocaroni http://mocaroni.com/documentation Rocketeer Micah Cooper just released a new service that lets you collaborate with your team on API contracts. You can work together on the URLs and request/response bodies to build the most beautiful services imaginable. Check it out! middleman-presentation https://github.com/maxmeyer/middleman-presentation A new layer on top of the always fantastic middleman gem lets you easily build HTML-based presentations using your favorite templating language. RubyConf 2014 http://rubyconf.org/registration Registration is open for RailsConf 2014! This November we can all descend upon San Diego to talk about ruby and ruby-related things! So that's it for this episode of Ruby5. If you haven't already, subscribe to the podcast and keep yourself up to date. Thanks for listening! Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-08-01"},
{"website": "Hash-Rocket", "title": "Pair programming questions", "author": ["\nChris Erin\n\n"], "link": "https://hashrocket.com/blog/posts/pair-programming-questions", "abstract": "Community Pair programming questions by\nChris Erin\n\non\nJanuary 24, 2017 I received an email recently from a programmer who had just read Pairing in practice and had some questions.  I try to answer them as best I can below. The questions have been edited for clarity Do you at Hashrocket pair as a strict practice, or is it just for training new developers? Developers at Hashrocket pair on a regular basis. As a consequence, whenever we have an apprentice, we pair with them too. Actually, pairing an experienced developer with an apprentice slows down the pace. Apprentices should ask questions and senior developers should take the time to explain all that they can, within reason and the process of explaining for the purpose of teaching certainly takes time. Two senior developers will share a lot of knowledge which makes many joint decisions easier and increases the pace of development. Do your developers pair program for all projects? How do experienced developers cope up with this? Our default is to pair all the time, and our experienced developers (each developer at Hashrocket has a fair amount of experience) prefer this over working alone. Its enjoyable to share your craft with someone who also appreciates the finer points of programming. Your pair will force you to write better code than you would otherwise write by yourself. The joy of coding with an equal and the knowledge that the code will be better overall makes pairing a welcome practice each and every day. I came to know the benefits of this practice from your article \"Pairing in Practice\" what are some of the downfalls of this practice? Pairing tends to take away both the highs and lows of individual efforts. There is no chance in pairing to achieve the kind of singular development flow that many developers have experienced in their best moments. When conditions are right, an individual developer can create amazing things in a short period of time. Look at the two-week creation of Javascript by Brendan Eich. There is no way this could have happened if Brendan was pairing with another developer, because that effort took extraordinary vision and was very opinionated. If Brendan was pairing he would have had to justify each of the many decisions that he made and that discussion may have resulted in a watered down vision that took longer to implement. There are other circumstances where a developer might feel like they could have done a better job while soloing. Writing algorithms seems to elicit a desire to write it yourself rather than being constrained by a pair. It's the type of programming that as high-level web programmers, we don't get to do very often and enjoy doing when we get the chance. You and your pair may have different approaches to solving an algorithmic problem, but can only try one approach at a time. Ultimately, if the tests pass (and hopefully there are enough tests) then you've done a good job, whatever the approach, because the tests will provide a platform for refactoring, should that prove necessary. Another struggle we face when pairing is the conflict between \"thinking\" about a problem and \"poking\" at a problem. \"Poking\" at a problem means writing a test and trying to solve that test without really having any good idea what the solution will be. This mode of work can be unnerving to some people: why would you code something you don't really understand?  The point to \"poking\" is not to solve the problem but to learn about the contours of a problem through programming. The same thing can be done by \"thinking\" about a problem, and done more abstractly which can yield a better solution. The conflict with pairing, however, is that the poker would rather not watch another person think. The thinker will not want to start coding before having thought. This happens sometimes, but not often enough to detract from our desire to pair. Is productivity affected by pair programming, if followed as a daily practice? Two developers pairing on a problem will solve problems at a faster rate with better quality than one developer working alone, but not twice as fast. Two developers working on their own on different features will be faster than two developers working together, but not twice as fast and with lower overall code quality. The one aspect of pairing to consider, vs. two lone programmers, is communication overhead. Some tasks that two developers work on alone don't need communication overhead, perhaps there are two separate applications in two different languages. Some tasks require developers to communicate. The tasks may need to touch the same lines of code, in which case communication will be necessary to resolve conflicts. Both programmers probably want the application to have internal consistency, to use the same patterns to solve similar problems across the application, but without communication this can be difficult. If these types of things tend to be accomplished during code reviews, where iterative development slows down, then perhaps two developers working on one problem will be twice as fast. Pair programming tends to produce code that would pass initial code reviews 80% of the time. Not all deadlines are created equal. Some deadlines are permissive and developers can put forth an effort to write the best code possible in the first iteration with minimal technical debt. Some deadlines are constrained and require developers to knowingly take on technical debt. When pairing on a problem where technical debt must be created to achieve a constrained deadline, pairing can be very helpful in order to acquire that debt in ways that can be unwound gracefully in the future. What are some prerequisites to consider if a company decides to follow pair programming as a daily practice? It's important to never force pairing on a developer. If a developer is not open to the idea then it will fail. At Hashrocket, we've been fortunate to have had the opportunity to pair program as part of our hiring process and this has led to a collection of developers that believe in and enjoy pairing. Many developers are wary of new workplace techniques, which is understandable. We all learn to program alone, late at night in the computer lab, in a basement on the family computer, or with our laptop outside in the park. In order to be a good pair programmer, a developer must appreciate that there is no such thing as perfect code and that any dedicated review of any given code will produce justified criticisms, both large and small. It's very beneficial to have a workstation setup that conforms to pair programming. One large monitor, two keyboards and two mice per workstation are what we use at Hashrocket. Conclusion Thanks for reading.  If you have questions about pairing let me know at chris.erin@hashrocket.com . Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-01-24"},
{"website": "Hash-Rocket", "title": "Using Github Repos in your Gemfile", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/using-github-repos-in-your-gemfile", "abstract": "Ruby Using Github Repos in your Gemfile by\nPaul Elliott\n\non\nJune  6, 2012 There is always that one gem that you need to pull from a github repo. The author has slacked off on cutting a release for that bugfix that had you blocked and now you've got to not only type out the whole repo url but you have this monstrosity mixed into your beautiful Gemfile . gem 'fabrication' gem 'decent_exposure' gem 'none_such' , git: 'git://github.com/avdi/none_such.git' gem 'quantum_leap' Unfortunately you can't git rid of that ugliness completely, but you can make it a little more pleasant and easier to type by using the github option instead of git . gem 'fabrication' gem 'decent_exposure' gem 'none_such' , github: 'avdi/none_such' gem 'quantum_leap' The branch option still works like you would expect, so you can safely use the new syntax without losing functionality. You must be on bundler 1.1 or newer to use this feature. Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-06-06"},
{"website": "Hash-Rocket", "title": "LittleDecorator Gem: Rails Model Decoration in 42 Lines", "author": ["\nVic Ram"], "link": "https://hashrocket.com/blog/posts/little-decorator-gem-rails-model-decoration-in-42-lines", "abstract": "Community Ruby LittleDecorator Gem: Rails Model Decoration in 42 Lines by\nVic Ramon\n\non\nJuly 15, 2014 We usually hand-roll our own decorators here at Hashrocket. After doing this a few times I've found a pattern that I really like, and now I've turned it into a gem called LittleDecorator . This gem delivers on its name: there are only 42 lines of code in lib . This blog post explains how to use it and how it works. Create a Decorator First create a decorator that subclasses LittleDecorator: # app/decorators/post_decorator.rb class PostDecorator < LittleDecorator end You can call model methods directly: def full_title \" #{ title } : #{ subtitle } \" end You can override model methods, but make sure to prefix calls to the original model with record (or its alias model ) to avoid an infinite loop: def updated_at record . updated_at . strftime ( \"%A, %B %e, %Y\" ) end Helper methods are available to you: def price number_to_currency price end As well as route helpers: def post_link link_to title , record end Decorate Models LittleDecorator gives you a decorate helper method that's available in both controllers and views. Just pass it a model to get the decorated version. Calling decorate on a collection will return an array of decorated objects: decorated_post = decorate(post)\ndecorated_posts = decorate(posts) How It Works LittleDecorator uses method_missing to send any unimplemented methods to the model. If the method is not found in the model, then it checks to see if the view context of the object responds to the method -- that's how you get free access to view helpers and route helpers. If the method is still not found then you'll get a NoMethodError on the decorator. The decorate helper method works by looking for a decorator with the same class name as the model but with a suffix of \"Decorator\": decorator_class = \" #{ model . class . name } Decorator\" . constantize Extras Decorated records also behave well when used in a link_to : <%= link_to decorated_post . title , decorated_post %> That covers most everything you might want to know about this gem. Maybe it can save you some hassle the next time you need a decorator. Check it out on GitHub . Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-07-15"},
{"website": "Hash-Rocket", "title": "Create Quick JSON Data Dumps From PostgreSQL", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/create-quick-json-data-dumps-from-postgresql", "abstract": "PostgreSQL Create Quick JSON Data Dumps From PostgreSQL by\nJosh Branchaud\n\non\nOctober 10, 2017 There are lots of ways that we consume the data stored in our PostgreSQL\ndatabases.  We build apps that query against our database. We explore the data\nmanually from a PSQL session. Perhaps we even occasionally create a dump of the\ndatabases' schema and data using pg_dump . What if we want to export some subset of our data in a general purpose format\nthat we can easily pass along to others? A JSON dump would fit that bill. In\nthis post we'll explore how to quickly create a JSON dump of some of our data. Why a JSON dump? A customized JSON dump of our data is appealing for two reasons. First, we can craft the shape of the data being dumped. We can be sure to\ninclude specific, relevant fields. This includes computed values. Likewise, we\ncan exclude sensitive and irrelevant data. For instance, we may not want to\nexpose serial ids that make up the primary key of our data. Second, JSON is a clean and friendly export format. By dumping our data as JSON\nwe are not only ensuring that it is relatively human-readable, we also know\nthat any developer regardless of their development stack will have access to a\nJSON parser with which they can read that data into a program meant to consume\nor otherwise use the data dump. What We Need To dump a subset of our data from Postgres as JSON, we'll need to do a\ncouple things. We will need to come up with the shape and content of the\ndata as discussed above, turn the result table into JSON using json_agg ,\nadjust some PSQL settings related to the output formatting, and, lastly,\nsave the resulting JSON dump to a file. Shape and Content We are creating a JSON dump of our data because someone needs access to our\ndata. They have specific needs though. You'll have to identify what they\nneed and consider how cleanly that maps to your data model. Some things will\nmap directly where as others will require computing a value based on a\ncombination of rows or some known constant. To make this a bit more concrete, let's consider how we would create a data\ndump of the Today I Learned posts from our tilex app. The posts table can be described as follows: \\ d posts Table \"public.posts\" Column | Type | Modifiers --------------+--------------------------+---------------------------------------------------- id | integer | not null default nextval ( 'posts_id_seq' :: regclass ) title | character varying | not null body | text | not null inserted_at | timestamp with time zone | not null default now () updated_at | timestamp with time zone | not null default now () channel_id | integer | not null slug | character varying ( 255 ) | not null likes | integer | not null default 1 max_likes | integer | not null default 1 published_at | timestamp with time zone | not null default now () developer_id | integer | tweeted_at | timestamp with time zone | There are a lot of columns in this table. To start we only want to dump the title , body , published_at , and a permalink for each post. The first\nthree -- title , body , and published_at -- map directly to columns on\nthe table. There is no permalink field, but we can easily construct one from\nthe slug . select title , body , published_at , 'https://til.hashrocket.com/posts/' || slug as permalink from posts ; - [ RECORD 1 ] + --------------------------------------------------- title | Start rails server in production mode body | `rails s -e production` ... published_at | 2015 - 04 - 16 23 : 04 : 53 . 142287 - 05 permalink | https : // til . hashrocket . com / posts / 97 e26f5f68 - [ RECORD 2 ] + --------------------------------------------------- title | Variable Hoisting in JavaScript body | I while back I wrote about [ variable ho ... published_at | 2016 - 04 - 03 13 : 47 : 25 . 763974 - 05 permalink | https : // til . hashrocket . com / posts / eeedb8dda0 - [ RECORD 3 ] + --------------------------------------------------- title | Three ways to compile Elixir ... Great! We took a big step in the right direction by pulling out values we want\nincluding a computed value (the permalink). Columns like id and inserted_at have been ignored. Let's push this example a bit further by joining in the\nchannel names for each post as well as attributing each post to the developer\nwho wrote it. select title , body , published_at , 'https://til.hashrocket.com/posts/' || slug as permalink , channels . name , developers . username from posts join channels on channels . id = posts . channel_id join developers on developers . id = posts . developer_id ; With the shape and content of our dump settled, let's move on to the JSON step. Transitioning To JSON Data in a Postgres database is always in a tabular form. It is stored that way,\nthe results come back that way, even something as simple as select 1; is\npresented as a table. We want to transition from a tabular result to a JSON one. Postgres has a json_agg function which will do most of the heavy lifting here.\nTo prepare our previous statement for json_agg we need to wrap it in either a\nsubquery or a CTE (common table expression). I'll show both and then proceed to\nuse the CTE version throughout the rest of the post. json_agg with subquery select json_agg ( t ) from ( select title , body , published_at , 'https://til.hashrocket.com/posts/' || slug as permalink , channels . name , developers . username from posts join channels on channels . id = posts . channel_id join developers on developers . id = posts . developer_id ) t ; json_agg with CTE with t as ( select title , body , published_at , 'https://til.hashrocket.com/posts/' || slug as permalink , channels . name , developers . username from posts join channels on channels . id = posts . channel_id join developers on developers . id = posts . developer_id ) select json_agg ( t ) from t ; Either way, we get a JSON result that looks something like the following: -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- -------------------------------------------------------------------------------- ---------------------------------- [ { \"title\" : \"Start rails server in production mode\" , \"body\" : \"`rails s -e production`\" , \"published_at\" : \"2015-04-16T23:04:53.142287-05:00\" , \"?column?\" : \"https://til.hashrocket.com/posts/97e26f5f68\" , \"name\" : \"rails\" , \"username\" : \"jakeworth\" } , + { \"title\" : \"Variable Hoisting in JavaScript\" , \"body\" : \"I while back I wrote about[variable ho\" , \"published_at\" : \"2016-04-03T13:47:25.763974-05:00\" , \"?column?\" : \"https://til.hashrocket.com/posts/eeedb8dda0\" , \"name\" : \"javascript\" , \"username\" : \"jakeworth\" } , + There are two things to note regarding this output. First, we have that odd\ndashed line header at the top and weird spacing between results marked by the + signs. We don't want any of that included in our final output and will deal\nwith it in the next section. Second, the names of some of the fields ( keys in JSON speak) are not exactly\nwhat we'd like. We can and should adjust these. For the subquery approach, we\ncan follow the pattern of the permalink field by tacking on as <preferred\nname> after each field as needed. Here is what our CTE approach will look like: with t ( title , body , published_at , permalink , channel , author ) as ( select title , body , published_at , 'https://til.hashrocket.com/posts/' || slug as permalink , channels . name , developers . username from posts join channels on channels . id = posts . channel_id join developers on developers . id = posts . developer_id ) select json_agg ( t ) from t ; We name each field in the description of the CTE. This makes things rather\nexplicit. We know exactly what json_agg will do as a result. Now, to deal with\nthe formatting. Output Formatting Though json_agg is able to produce valid JSON, the PSQL output formatting\nquickly invalidates it. Both the table header and the result spacing delimited\nby + signs are not valid JSON. With the help of a post on TIL itself, Export text exactly from\npsql ,\nwe can clear up these issues. The table headers can be removed by turning Tuples Only on. > \\ t on The spacing between the results can be removed by switch PSQL's formatting to unaligned . > \\ pset format unaligned These settings will only persist for the duration of the PSQL session. The\nnext time you connect they will be back to their defaults. Export The Results The last step is to take the fruits of our labor and persist them to some file.\nWe can do this with ease right from Postgres using the \\g meta-command in\nplace of the semicolon. On its own, it will execute the query just as ; would.\nHowever, if followed by a filename, the result of the query will be written to\nthat file. with t ( title , body , published_at , permalink , channel , author ) as ( select title , body , published_at , 'https://til.hashrocket.com/posts/' || slug as permalink , channels . name , developers . username from posts join channels on channels . id = posts . channel_id join developers on developers . id = posts . developer_id ) select json_agg ( t ) from t \\ g my_data_dump . json The output will now be in the specified file. In what directory is that file\nsaved? Presumably the directory from which the PSQL session was initiated. To be\nsure, you can shell out to the pwd command. > \\ ! pwd Conclusion You now have some data to share as JSON in the exact shape you want. Hopefully\nyou've learned a little about CTEs, json_agg , PSQL's formatting, and the \\g meta-command along the way. Happy Data Dumping! Cover image by Taylor Bryant on Unsplash.com Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-10-10"},
{"website": "Hash-Rocket", "title": "A Module By Any Other Name: Aliases in Elixir", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/modules-and-aliases-in-elixir", "abstract": "Elixir A Module By Any Other Name: Aliases in Elixir by\nJosh Branchaud\n\non\nApril 18, 2017 Modules provide a way for us to organize our code and compose complex\nsystems. \"A module is a collection of functions, something like a namespace. Every\nElixir function must be defined inside a module. To call a function of a\nmodule, you use the syntax ModuleName.function_name(args) .\" -- Elixir in Action, pg. 22, Saša Jurić Any module, whether part of the standard library or defined by us,\nis available if it has been loaded into memory or is on the BEAM's code\npath. It is easy to see this in practice within an IEx session. For\ninstance, the String and Enum modules are readily accessible. > \" 1 2 3\" |> String . split () |> Enum . map ( fn ( x ) -> String . to_integer ( x ) * 10 end ) [ 10 , 20 , 30 ] Enum and String are aliases. They are a convenient syntax for referring\nto the various Elixir modules our code needs. If they are aliases, then for what are they an alias? > to_string ( Enum ) \" Elixir.Enum\" When we refer to the Enum module, it turns out this is an alias for :\"Elixir.Enum\" . We can see that is true with a comparison. > Enum == :\"Elixir.Enum\" true I'd personally prefer to only type Enum each time I want to use the Enum module, but we are free to ignore these aliases. > \" 1 2 3\" |> :\"Elixir.String\" . split () |> :\"Elixir.Enum\" . map ( fn ( x ) -> :\"Elixir.String\" . to_integer ( x ) * 10 end ) [ 10 , 20 , 30 ] I cannot think of a good reason to ignore these aliases. Instead, we should\ntake advantage of aliases, even creating some of our own using alias/2 . We\ncan keep our code concise and readable by defining aliases for lengthy\nmodule names. > alias MyApp . User . Account MyApp . User . Account > MyApp . Repo . get! ( Account , 1 ) Account % Account { ... } We can even make short module names even shorter. > alias String , as: Str String > Str . split ( \" 1 2 3\" ) [ \" 1\" , \" 2\" , \" 3\" ] How does our code know that Str and String and :\"Elixir.String\" all\nrefer to the same thing? The compiler makes it so. Elixir creates the String alias and we created the Str alias. When the compiler is\nprocessing our source files, it transforms each occurrence of String and Str into :\"Elixir.String\" . All of this is important because it has everything to do with how our Elixir\nmodules are compiled into beam files and with how module resolution\nhappens within the runtime. This can be further demonstrated by creating a file example.ex with the\nfollowing two modules: defmodule MyApp . User . Account do end defmodule Pokemon do end Let's compile the file with elixirc and then see what the compiler gives\nus. $ elixirc example.ex $ ls Elixir.MyApp.User.Account.beam\nElixir.Pokemon.beam\nexample.ex Elixir and Erlang don't care much for the name of our files beyond ensuring\nthey are loaded. When it comes to compiling the code, each module is\ncompiled into its own file based on the fully expanded module name -- this\nmeans for our Elixir files Elixir will be tacked onto the beginning. One last thing of note. We define modules with atoms. Generally, we use the\ncamel case style syntax for this -- e.g. MyApp . If modules are defined\nwith this specific kind of atom, what is stopping us from using the other\nstyle of atom syntax? In short, nothing. Let's do it. defmodule :my_app do end Compiling a file with this module definition will produce a BEAM file with\nthe name my_app.beam . Notice that the Elixir bit is not there. That's\nbecause :my_app is the fully expanded form of that atom. This is what\nErlang modules look like when compiled. The Elixir bit gives us a\nlanguage-specific namespace for the code we write. Though there is nothing,\nbesides naming collisions, stopping us from subverting this. Elixir can sometimes feel like its own language, but it is important to\nremember that it is deeply embedded in Erlang and the BEAM VM. Module\nnaming, compilation, and module resolution are all part of this story. Sources: Elixir in Action Elixir: Getting Started - Understanding Aliases Photo Credit: Dmitri Popov, https://unsplash.com/@dmpop?photo=mnFGmGiuupw Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-04-18"},
{"website": "Hash-Rocket", "title": "Let me finish", "author": ["\nAdam Lowe\n\n"], "link": "https://hashrocket.com/blog/posts/let-me-finish", "abstract": "Process Let me finish by\nAdam Lowe\n\non\nJanuary  2, 2013 Often, the more experienced we become at consulting with clients and writing software we develop a bad habit that can undermine our credibility. We start presenting our solution before we take the time to walk through our client's problem with them. Background Patterns are part of building software. We are wired to recognize them. They help us break down our clients' problems into manageable chunks and help us build solutions to those problems. The Problem There comes a point when the ability to recognize patterns can start to work against us. It plays out when clients begin to describe a problem or an idea to us and a few words in we know where they are going. The problem isn't that we are wrong. The problem is that we are right and we don't let the client finish. In doing so the client doesn't think to them self, \"Wow, that consultant is amazing. They knew what I was about to say, cut me off mid-sentence and gave me the answer.\" What they do think is something along the lines of, \"Wow that consultant doesn't respect me enough to let me finish a thought and thinks I'm an idiot to boot.\" The Solution Realize that there is value in listening. In every client interaction we as consultants have the opportunity to build credibility or lose it. One of the best ways to build credibility with clients is to hear them out. Patterns are a good thing but they are not prescriptive. They can help us identify problems and solutions but at times they can also work against us. Listen to your client, take detailed notes and then provide them with a solution that solves their specific problem. Whether their problem is completely new or one you have solved a thousand times this is something we owe our clients. We as human beings value being listened to, really heard out. Being a developer is not just about writing software. It is about taking the time to truly understand our clients, their business domain and their specific solution needs. To not give them this courtesy is not only disrespectful but it also sets us up for failure. Taking the time to truly understand a client and their needs builds the credibility and relationship necessary to provide them difficult feedback that is needed at some point in every project. When you take the time to listen to your client you communicate that you value them, that their problem is real and that you build confidence with them that you as a technologist will be able to provide a solution to their problem. While patterns do help us identify the problem and craft a solution it is rare that a client's problem fits perfectly with the pattern. Listening and understanding are what bridge this gap, allow us to provide them with the solution and give them the ear to hear that solution from us as a consultant. Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-01-02"},
{"website": "Hash-Rocket", "title": "In Conversation: Noel Rappin", "author": ["\nAndy Borsz\n\n"], "link": "https://hashrocket.com/blog/posts/in-conversation-noel-rappin", "abstract": "Ruby Ember In Conversation: Noel Rappin by\nAndy Borsz\n\non\nNovember  4, 2013 This summer I had the chance to sit down and talk with Noel Rappin about Ruby on Rails, Ember.js, self-publishing, and the benefits of public speaking. Noel's latest offerings of self-published technical books has brought us Mastering Space and Time With JavaScript and the upcoming Trust-Driven Development . AB : How long have you been working with Rails, and what got you into it? NR : Well, I'd been a web developer on and off\nsince about 1998, starting with ColdFusion as my first tool. I got out of web\nconsulting for a while and started hearing about Rails around 2007.  I was\nmanaging a remote team split between Chicago and Poland, and I started building\nup a couple of tools that we could use to collaborate. The first Rails program I\never built was a little task tracker that you could think of as a very, very,\nstupid version of Pivotal Tracker. AB : There are a lot of versions of Pivotal to be built out there. NR : Yes! Yes, it was something I needed that could be built quickly, and\nRails turned out to be really good at it. I was already a little bit familiar\nwith Ruby, so it was nice to be able to write Ruby in a real context. It wasn't\nlong after that when I moved back into web consulting and started working with\nRails professionally – and started writing about it too. AB : You bring up writing about Rails – one could say that you wrote\nthe book on testing Rails apps. NR : [laughing] You're just saying that because it's called that. AB : Therefore it is! Right? NR : Okay. Yeah. AB : So how has the way that we test Rails apps changed since when you\nstarted working with Rails? NR : Well, Rails has always had a very strong culture of testing, and one of the\nthings that attracted me was that the core team put in a lot of great testing\ntools. So it had already started from a good foundation, and\nthere's always been a sense in the community that testing is the right thing to\ndo.  What's changed over the last few years is that the tools have all\ngotten so much better – and there are more of them. We talk more specifically\nabout unit tests versus integration tests in a more separated way than we did\nwhen I started working with Rails. I came to Test::Unit first, and then later to\nRSpec – after being kind of skeptical about it – and have started using\nother tools that didn't exist when I started, like Cucumber and Capybara. AB : Is the Rails testing culture stronger now? NR : I don't think that the community's attitude has changed about it, but I\ndo think that more people in the community are really signing on to what what we\nlike to believe that we all do. AB : I remember as a beginner feeling aware of a strong test culture, but I\nthink it's a common tendency to take for granted the things that you already\nknow. You can grow comfortable with the TDD process and forget that some of this\ncan be really tough for beginners – I remember that phase as a beginner. I go\nto meetups and there are a lot of questions about testing. Are we taking it for\ngranted that we've gotten used to testing? Is it hard to test a Rails app? NR : Well, it is hard to test certain things. I think one of the things I\ncan't say that we've done is gotten a lot better at explaining how to do this to\nnewcomers in the community, and I'm probably as much to blame for that as anybody – AB : Ha! NR : – but I think that there are two aspects that are difficult. One is\nthat certain parts of a real application are just challenging to test, and they\noften tend to be important parts, like security and third-party tool integration.\nAlso, for some things there are tricks for how to test them that you\njust need to learn, and it's not the natural red/green/refactor process; that can only\ntake you so far. Figuring out how to write a good red test\nfor dealing with a billing system is not trivial, and I think that that's one of\nthe problems. I also think that the message of why you do this kind of thing\nsometimes gets a little muddied – this has always been true about TDD\nsince the term was invented. There's always been this ongoing conflict between\ntesting as a tool for driving design and testing as a tool for validating your\napplication, and I think people who are new to testing can get confused because they're\nactually doing one of those things and they think they're doing the other.\nThey're writing tests as though they're driving design and they're surprised\nit's not validating their app and catching all their errors, or they're writing\ntests because they're validating their app, and the design part of it feels\nreally tedious. But those are two separate activities. AB : Absolutely. I think that that's a very good reflection on the kinds of\nconfusion I've seen with newcomers, where they're writing a test but they're not\nsure exactly why. NR : One thing that I think has changed in the community – and I think this is\nan outgrowth of now having longer lived applications – is that there's a lot\nmore attention to dealing with legacy test suites and a lot more attention to\nobject-oriented structure. In the last year and a half or so, a lot of discussions\nin the Rails comment have  come down to the fact that we have to maintain\nolder applications – codebases that we may wish we architected\ndifferently when they started. AB : In mentioning Rails Test Prescriptions compared to what you're doing\nnow, you have experience on both sides of the publishing spectrum. You've done\nthe traditional publishing company route and also the DIY self-publishing\napproach. NR : Yes. I've also gone in both directions in that I had a DIY book that\nbecame a traditionally published book and a traditionally published book that\nbecame a DIY book. [both laughing] Yeah, it's a little weird. AB : So how and why did you get started self-publishing? NR : I actually started with self-publishing. Rails Test Prescriptions was\noriginally self-published for a specific reason:\nI wanted to write about testing because I wanted to write about\nsomething I felt I really knew and didn't need to research. At the same time,\nI knew the publisher I wanted to work with, Pragmatic, was working on an\nRSpec book – so I just assumed they wouldn't be interested in another testing\nbook. So I decided to self-publish, which I did for about six months,\neven though the tools for self-publishing and distributing books when I started\ndoing this in 2009 weren't as good as they are now. It was very, very hard at the time to come up with a place that would easily\ndistribute digital media and allow you to send updates.\nIn fact, I actually wound up having to hack my own because there wasn't a service\nthat provided that at the time. A few months in, Brian Hogan actually\nsuggested that I submit the book to Pragmatic, which I did, and they bought it – so\nthen it became a traditionally published book. There's an immediacy to self-publshing that you don't get from a traditional publisher –\neven a really agile company like Pragmatic. I see every time an order comes in. I see\nthe name of the person – [laughing] – which also tells you that I'm not getting\nso many orders that it's an avalanche distracting me from everything else I'm\ndoing – but there's a connection there. Self-publishing is enough of a novelty\nthat a lot of the people that buy\nthe book become very invested in its success, and that doesn't really\nhappen with traditional publishing.\nPeople have gone well beyond\nsending me corrections and actually do copy edits. It's really gratifying to\nhave people donate some time to help in that way. You gain a lot of control over\nyour distribution, your layout, things like that – and you give up some\nresources in terms of having access to professional designers and editors and marketing\n– so everything is a tradeoff, but I have a lot of fun doing it myself. AB : Do you have any advice for prospective authors whether they're pursuing\nself-publishing or another medium to get their work out? NR : You don't need to be an expert in a topic to start something. It helps, but\nit actually helps less than you might think. If you come to something\nalready as an expert, a lot of times you've kind of lost touch with the process\nof learning something new, and that can make it harder to get in the mindset of\nexplaining something to someone coming in cold.\nNow, the topic does need to be something\nthat you become an expert in over the course of working on a book.\nWriting a book is a long process, and if it's not something you're really\nexcited about learning more of then the process will be really, really long. I recommend if you've never really done anything like that to start\nwith something shorter. Blogs are great for this kind of thing – if you set\nyourself the goal of doing regular blogging, like \"every Wednesday I'll write\nabout a gem that I use\" or \"every Thursday I'll write something new I learned\nwhile working on my Ember app.\" That's a good way to start easing into the\nrhythm of working on something regularly, and it's also a really good way to\nbuild traffic. One of the best ways to build traffic is to just be there\nconsistently and get people in the habit of finding content in a particular\nplace. AB : I really enjoyed your talk at RailsConf this year where you were going\nover some of your experiences with Javascript frameworks. When did you start\ngiving talks and throwing your hat into the ring at conferences – was it before\nor after your writing career? NR : I had done a couple of talks in grad school at SIGCHI (the computer human\ninteractions conference), so speaking came\nreally naturally for me when I joined the Ruby community. My first talk submission,\nI think, was contemporaneous with the first\nthing I published in the Rails community. That's something that I really like.\nYou know there really aren't very many technical communities that have the kind\nof consistent regional get-togethers that Ruby communities have, and to be\nable to go and meet people doing the same things and facing the same problems is\nreally great. When they occasionally let me get in front of a bunch of them and\ntry to explain something, it's even better. AB : I think, for most people, 'natural' and 'public speaking' are not words\nthat they put together in the same part of their vocabulary. You should probably\nconsider yourself lucky in that regard. NR : You know, it's a skill like anything else. And\nif you've done it a few times, you get better at it and are less nervous.\nMerlin Mann, the podcaster and writer, talks\nabout really thinking about the worst things that's going to happen – nobody's\ngoing to eat you. AB : [laughing] NR : I also was in a lot of theater and that kind of stuff when I was in high\nschool, and that was deliberately because it was something that made me\nnervous. I wanted it to not make me nervous, because it's a really useful\nskill to be comfortable with that kind of thing, in part because so many people\naren't. Just showing up is a big part of success, and if you're the person who's\ncomfortable getting in front of a group of people, you become associated with\nbeing knowledgeable because you're communicating that. NR : With Mastering Space and Time you've been writing not just about\nJavascript but about a lot of frameworks as well. Are you using some\nframeworks that you haven't written about? Is your writing about a framework\nkind of a 'Noel Rappin Endorsement' of that framework? NR : [laughing] It's an endorsement in that it's something I found\ninteresting enough at the time to write about it. I was actually using Ember as\nI was writing about it. I was not writing a large-scale Backbone application at\nthe time, although I was doing some smaller projects with\nit. \nAt the time that I started writing about Ember there\nwas a huge gap in documentation and that's much, much less true now eight months\nlater. There's Ember content all over the place now. I've been asked a couple of\ntimes if I'm going to add something on Angular, and I'm resistant in\npart because I don't really know much about it yet, and I'm not sure that that's\nwhere I want to spend time next – but there does seem to be some interest,\nso that might be something that\nI look at. If it's an endorsement, then it will usually pretty explicitly say\nthat in what I write. It will come through pretty clearly what parts I'm\nendorsing and what parts I'm not. I don't think I'm at all shy\nwhen I talk about parts of Javascript that that absolutely make\nno sense, and that carries through to frameworks too. AB : I've been getting really into Ember and finding that I like it a lot;\nyou stay pretty partial in the books in talking about Backbone and Ember –\nbut would it be fair to ask for your Ember pitch? What\nexcites you about Ember? NR : I'm actually really excited about Ember. I think that it's very powerful,\nand Yehuda has gone on record talking about how their goal\nwas to try and pull the best practices from a lot of different frameworks in\nand opinionated kind of way, and I think that they've been pretty successful\nwith that. One of the things that really encourages me about it is that\nthey've been very, very good at listening to the community that's forming\naround them. Their original router syntax was really, really hard to\nwork with, and they got a lot of feedback on it and came up with another one that\nwas better – and got a lot of feedback on that and came out with another one that\nwas actually even better.\nThey've made a lot of updates to the framework that have been\nmotivated by people's experience using the it.\nThey started from a place of trying to do something really powerful and ambitious, but\nthey've admitted in cases where they were wrong, and they've taken things that were\nmaybe more difficult than they needed to be and put a lot of\neffort into improving them. You see that happening right now with Ember Data,\nwhich has some very well-documented limitations and problems, and they're really\nmaking a concerted effort to polish that up right now. AB : I was not one of the people who had tried Ember or SproutCore early on,\nbut in combination with Yehuda's talk and your talk at RailsConf,\nI got the sense that this is definitely a time to try Ember if\nyou're remotely interested in Javascript frameworks. NR : Yeah, it seems like it needs about a year of thousands of developers\nbuilding apps on it to really get to where it needs to be. Rails was the same\nway. Rails in the 1.0 and pre-1.0 times was obviously trying to do a lot of\nthings that frameworks hadn't done very well in the past. It succeeded at some\nof them and didn't succeed at others – and they were also open (for the most part) to\nimprovements based on the actual community's\nexperiences using it. There's a lot of that process going on with\nEmber right now. AB : I don't want to lead you into any topic that you don't wish to discuss,\nso let me know if it's okay – but for a while in the Rails community, there was an\nundeniably massive topic: there were these two very large companies,\nthese deal sites – NR : [laughing] AB : – huge Rails companies that were hiring, from the outside perspective,\npractically all Rails developers. This has died down somewhat as an issue, but a\ncouple of years ago this was the hot topic. NR : Now it's just GitHub hiring them all, right? AB : [laughing] Yes. So for people who don't have that experience, what is\nit like to work in a massive Rails company that was basically competing with\nanother massive Rails company? NR : I actually feel like I am pretty comfortable talking about this, because\nI can be completely up front and say that my experience at Groupon was really\nidiosyncratic. I can speak with perfect authority on something that has nothing\nto do with anybody else's experience, and with intermittent authority on stuff\nthat has nothing to do with me at all. AB : [laughing] NR : I joined Groupon as part of a group of developers that came in\nwhen Groupon purchased the consulting company Obtiva, and I went straight into a\nrole at Groupon that largely involved internal developer training, so I\nnever had the experience of being a normal part of a regular development team at\nGroupon. I think that one of the things about the way that Groupon was organized – at\nleast the Rails side of it, and again I'm speaking\nfrom a somewhat outsider perspective – is that they tended towards smaller\nautonomous teams.\nTeams tended to have their own structures and way of doing things, so\nexperiences there tended to vary quite widely. Some teams were really\ncommitted to testing and agile practices like pairing, and some\nother teams just weren't. So there was a very, very wide range in\nGroupon. The company – I think for reasons that made sense – was not really\ninterested in mandating stuff from the top, and for whatever\nreason there was never (at least in the time that I was there) a really strong\npush for a unification of that kind of stuff. There\nwere some common tools being used – there was a continuous integration set of\ntools built on a Jenkins core that they'd built up that was well suited to\nhaving 250 Rails developers hitting the same codebase, which Jenkins just is not\nsuited for out of the box. That aspect of it was interesting, but in practice it\ntended to be a lot of small teams sort of pointed in the same direction more\nthan it felt like one really big team. AB : That's probably the healthiest way for something like that to work. I\ncouldn't imagine feeling like there were 250 Rails developers on my team . NR : Yeah, and that's unsustainable. But what's tricky about that is that a\nlot of those teams really were sharing a codebase, and probably sharing a\ncodebase more efficiently than they were sharing knowledge about that codebase.\nAgain, I can speak with no authority whatsoever on what actually happened on the\ninternals of these teams. But it was very interesting to see what Rails\ndevelopment looks like with that many people throwing code in the same codebase,\nand throwing tests in the same test suite, and how you start to control the sizes\nof those, and how you deal with situations that the framework really wasn't designed to\nhandle. AB : You have a new book in the works, right? NR : Yes. Trust Driven Development It's currently available at an\nintroductory price, which means that you get the introduction and nothing else\nyet. [laughing] The idea to write about software projects is something that I've\nwanted to do for a long time. We talk a lot about craft as it applies to\nhow we put our code together, and how we put our applications together – but\nthere's another side to that craft, in terms of how we put projects together, and\nhow we relate to the people that we're building software for,\nwhether they're an internal client or an external client or customer.\nThere are right and wrong things to do, and there's a professionalism inherent\nthere too – and I have\nopinions about that. It's something that I've wanted to write down for\na while. Right now my biggest worry is that it's going to come out as series of\nhuge angry rants – AB : [laughing] NR : I'm taking care to mold it into something that is more appropriately\nstructured as a book and less like a cranky guy telling everyone to get off his\nlawn. A lot of it is about the\nrelationship between development teams and either the management or the\ncustomers, and how you can build trust. It's\ntricky to write about that and not make it sound like one side or the other is\nalways right, because of course it's complicated – and as much as developers have\nissues making themselves understood to the teams that are depending on them,\nthose teams also have difficulty making themselves understood to developers. It  can become a frustrating exercise on both sides, so a lot of this is about\ntechniques and processes to try and improve the way these teams work together. A\nlot of is Agile based. I think that among the reasons Agile processes\nwork is that they give you a lot of small opportunities to build trust. The Agile\ncommunity describes it as feedback, and what that turns out to be is a lot\nof situations where you have a chance to say \"I told you I was going to do this, I did it\",\n\"I told you this was going to be difficult, it was difficult\", \"I told you that\nthis would be easier if you did something a certain way, and it turns out that\nthat's true.\" At some point in the project, you as the developer are going to\nhave to go to somebody and say \"something went wrong,\" \"this is a lot more\ncomplicated than we thought\", or \"we just made a mistake.\" I talk in the\nintroduction of the book – which is also available online for free right now – about\nthis being the moment when you're at the auto mechanic and the mechanic comes to\nyou and says \"you need new brakes, it'll cost $2000\". Is that a\ngood price? Do I need new brakes? I have to trust that this person understands\nwhat's happening and is communicating that to me effectively, because I don't\nunderstand it myself. It's incumbent on the mechanic to be right, and also to\nexplain the problem to me in a way that leads me to that same understanding. As developers, when we're working with people that are outside the\ndevelopment team, we have to have that same ability. We need to be able to\nexplain what we're doing, and when things go wrong we need to be able to say so –\nand what happens when you say something has gone wrong\ndepends on how much trust you've built up to that point. If you've built\nup a lot of credibility by doing things well and meeting commitments, then\nideally you have a certain amount of leeway when you need to say \"oh, this\nis actually more complicated that we thought and it's going to take some extra\ntime.\" The boy who cried wolf becomes a real issue here. If you always say that\nthen the one time you really need it you have less credibility when you ask. AB : Absolutely. I think those are very good topics for consultants as well as\nin-house developers – all of that applies regardless of your customer. NR : Yeah, I hope so. It's a mix of theoretical stuff about\nprocess and practical stuff – like how to have an IPM that actually is\neffective.  I'm enjoying writing it, and I'm hoping to structure it into\nsomething that other people will find useful. -- Noel Rappin is a developer at Table XI . He is the author of Rails Test\nPrescriptions , Trust-Driven Development , and Mastering Space and Time With\nJavaScript . Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-11-04"},
{"website": "Hash-Rocket", "title": "Look Good Naked", "author": ["\nDaniel Ariza\n\n"], "link": "https://hashrocket.com/blog/posts/look-good-naked", "abstract": "Process Look Good Naked by\nDaniel Ariza\n\non\nAugust 20, 2012 At Hashrocket we have an opportunity to launch a lot of applications. It's a really awesome experience and every leap to production is unique to the product. Across all these launches there is one feeling that remains the same. It''s the \"Oh crap! People are actually going see this thing\" feeling. It's like standing there naked and exposed to the world. It's a difficult feeling as a designer and can be even more overwhelming for projects stakeholders. ** Warning this post contains at least two Home Alone quotes. ** You've been discussing it, crafting it and pouring yourself into it every day. You know it's flaws, blemishes and dirty secrets. What can you do but hold on to it longer. Maybe more polish will make it presentable. Maybe another week… a month… a year. It's got to be perfect. I can't put it out there flapping in the breeze. What if they laugh. What if they find it inadequate. This could be the internal dialog of a stakeholder or it's project team. At Hashrocket we see a lot of the same patterns in stakeholders who are reluctant to launch. I think we can take a queue about launching from the wise Kevin McCallister when he said \"This is it. Don't get scared now.\" Below are some of the things we encounter most often. They're fairly common and hopefully being aware, what thoughts might sneak in, will help you to be more successful. I don't want to launch until everything is there.\nThe truth is no one knows all the features that aren't there yet. They have no idea about all the cool stuff you're going to drop on them and when it arrives they're going to be so stoked. Not to mention you'll be better informed about your user's needs. If anything is completely true, from app to app, it's that things will change. The only way to minimize change is to be better informed. Until people are using it, it's just your best guess. My competitors already have X and Y. I need those too.\nYour competitors have so many features. All of the features in fact. They also might have a years head start. The goal shouldn't be to match them feature for feature. If it is then you've already lost. You probably started down this road because you either had a compelling idea or fresh take on an existing one. Let it shine. Do it smarter. Don't just try to do more. So and so said we need to change everything.\nFeedback is important for validating ideas. Heck, most of this post was about putting it out there to get real user feedback. Who are real users though? Is it your nephew? Maybe or maybe not depending on your apps purpose. If it's an app for tracking investments and savings for retirement, then maybe your 11 year old nephew's feedback shouldn't carry much weight. We want to listen and adapt but before we do any of that we need to filter it. How relevant is this feedback? Is it shared by others? Keep in mind the people you present the app to, before launch, have no investment in it. They are not going to care as much as you do. It's your dream, not their's, and sometimes they don't know what better is until you give it to them. I need all the things and I need them now.\nWant to make these problems exponentially worse? Couple any of the feelings above with a deadline. Don't get me wrong. Deadlines are not inherently bad on their own. They only become a problem when you're not willing to cut scope. A tight deadline may just mean you have to let go of some things. This can actually be a great exercise because it forces you to choose what things are most important. You should be doing this anyway, even without the pressure of a deadline. Some things just take longer than others. One MVP might be smaller than another. Just keep in mind the sooner you can get it out there, with real users banging on it, the better. Afraid of failure? We all are. That feeling doesn't become dangerous until it stops you from following through. Put it out there. I think Kevin McCallister said it best. \"At least you'll know. Then you could stop worrying about it. You won't have to be scared anymore.\" Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-08-20"},
{"website": "Hash-Rocket", "title": "How We Pair", "author": ["\nTaylor Mock\n\n"], "link": "https://hashrocket.com/blog/posts/how-we-pair", "abstract": "How We Pair by\nTaylor Mock\n\non\nMay 30, 2013 After hearing Avdi Grimm give an excellent talk on why pair programming is important to the Ruby community at this years Ancient City Ruby , I realized people may want to hear a little about our pairing process here at Hashrocket. At Hashrocket we pair all the time. Whether delivering important features to clients or working on open source work during Open Source Friday’s, we always program using the buddy system. Why pair programming? Simply put, two heads are better than one! But here’s a few example scenarios: I can’t tell you how many times as a single developer I’ve gone down the wrong path while programming. My pair will catch me and help me climb out by offering alternative solutions. Maybe I got stuck on a particularly difficult problem when the solution was right there in front of me the entire time. My pair is there to point that solution out to me. We’ve all had those moments where we bang our heads on the wall and it was something so simple as a mistyped variable. Luckily my pair was paying attention and knows how to spell! Having an extra set of eyes will save you time and the clients money. Working alone, programmers tend to become too comfortable using the same toolset. We become stuck in our comfortable ways. Pairs mix up the repetitiveness by introducing new ideas, tools and styles helping us continually grow as software developers. Programming is hard. Having a partner to talk to helps develop innovative, well planned solutions to difficult problems. Our tools. Vim - Here at hashrocket we all use Vim. Some use MacVim and others use terminal, they differ only slightly so switching pairs around is a seamless process. iMac - We have large 27” iMac’s here at hashrocket. Each one has two keyboards, two mice and two awesome Rocketeers. Tmux ? - Around half of the Rocketeers use Tmux, the other half just use tabs. It’s really up to the pair’s preferences as we are all proficient in Tmux and can switch back and forth. Dotmatrix - All our dotfiles are in an open repository on Github. This keeps our iMac’s in sync and enables us to move around the room without ever leaving the comfort of our development environment. Our style. We switch fairly often between the driver and navigator. Having separate keyboards in front of each of us encourages people to take charge and start typing. Communication is key when pairing. The pair are constantly talking to each other about what direction the code is heading in. If you are the more experienced pair, let the other person drive every now and then. It gives you more freedom to think of solutions and forces you to explain the reasoning behind your thought process. This enhances your speaking abilities which is a necessity for us as  consultants. On the opposite side, it helps the less experienced developer learn faster than they would by just watching you bang away at the keyboard. So thats our process here at Hashrocket. Do you have any tips or systems that work well for you? Leave a comment below. Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-05-30"},
{"website": "Hash-Rocket", "title": "8 Great Vim Mappings", "author": ["\nVic Ram"], "link": "https://hashrocket.com/blog/posts/8-great-vim-mappings", "abstract": "Vim Community 8 Great Vim Mappings by\nVic Ramon\n\non\nJuly 10, 2014 I frequently create mappings or functions whenever I find myself repeating the same keystrokes. By now I've accumulated a decent list of custom mappings. Here are a few of my favorites: Number 8: Clone Paragraph with cp noremap cp yap<S-}>p This will copy the paragraph your cursor is on then paste a copy of it just below. This is great when you're about to create a block of code that will be similar, but not different enough to abstract (e.g. it blocks in rspec). Number 7: Align Current Paragraph with Leader + a noremap <leader>a =ip Quickly align your current paragraph with this command. Sometimes this can help you see where you're missing an end or a bracket. Number 6: Toggle Paste Mode set pastetoggle=<leader>z Avoid typing :set paste and :set nopaste by setting a paste toggle command. Number 5: Apply Macros with Q nnoremap Q @q\nvnoremap Q :norm @q<cr> This mapping makes macros even easier to remember: hit qq to record, q to stop recording, and Q to apply. This mapping also allows you to play macros across a visual selection  with Q . Number 4: Shift + Direction to Change Tabs noremap <S-l> gt\nnoremap <S-h> gT Who has time to type gt and gT ? Not me. Using shift and a direction to change tabs is a great alternative. Number 3: Control + Direction to Change Panes noremap <C-l> <C-w>l\nnoremap <C-h> <C-w>h\nnoremap <C-j> <C-w>j\nnoremap <C-k> <C-w>k Same thing goes for changing panes, but these use control . Skip that pesky w and get straight to the good stuff. Number 2: Quit Files with Leader + q noremap <leader>q :q<cr> Quickly close a file with <leader>q . Number 1: Save File with Leader + s nnoremap <leader>s :w<cr>\ninoremap <leader>s <C-c>:w<cr> If you're like me and you like to constantly save files, this one is nice. Just hit <leader>s . The second line makes this work in insert mode too, saving you even more effort. That's it. Got your own favorite mappings? Share them in the comments! Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-07-10"},
{"website": "Hash-Rocket", "title": "Vimscript And You!", "author": ["\nJonathan Jacks"], "link": "https://hashrocket.com/blog/posts/vimscript-and-you", "abstract": "Vim Vimscript And You! by\nJonathan Jackson\n\non\nJuly  8, 2013 At Hashrocket we love configuring our development environments to be highly portable and customizeable.  Occassionally, that necessitates rolling up our sleeves, busting out our editor, and cranking out some Vimscript .  If you've ever implemented a plugin in Vimscript then you have undoubtedly ran into some pain trying to excercise the code. Pathogen gets you a long way by reloading your plugins conveniently, but what if there were a faster way?  And wouldn't it be great to be able to test drive it?  Well, as luck would have it, we can! In this post we are going to cover: Setting up Vimrunner Writing a test using Rspec to exercise Vim Writing a plugin to satisfy the test By the end you should have a solid foothold to start testing your own Vim plugins. Setting up Vimrunner Vimrunner is a project written by Andrew Radev that let's you leverage the scriptability of Vim.  It's documentation is pretty great so if you have questions be sure to refer to the Vimrunner README . First start by making a directory structure that looks something like this (simplified for post): plugin\n└── example_plugin.vim\nspec\n  ├── plugin\n  │   └── example_plugin_spec.rb\n  └── spec_helper.rb Inside the spec/spec_helper.rb let's place the Vimrunner config block to get access to its built in vim helper and support modules. require 'vimrunner' require 'vimrunner/rspec' Vimrunner :: RSpec . configure do | config | config . reuse_server = false config . start_vim do vim = Vimrunner . start vim end end Above we set reuse_server to false to ensure it uses a new instance for each test.  This is slower, but makes everything easier.  The start_vim block gives us access to the vim runner inside our specs. Now that we have Vimrunner set up for the most part we can open up spec/plugin/example_plugin_spec.rb and write a test. Writing a test This plugin is simply going to strip whitespace for ruby files.  So let's start by setting up a file that has trailing whitespace so we can test our expectations. require 'rspec' require_relative '../spec_helper' describe \"Trim\" do before do vim . add_plugin ( File . expand_path ( '../../../' , __FILE__ ), 'plugin/example_plugin.vim' ) end it \"strips whitespace\" do sample = \"class TestingWhitespaceStripping   \" write_file ( \"test.rb\" , sample ) vim . edit \"test.rb\" vim . write expect ( File . read ( filename )). to eql ( sample . strip + \" \\n \" ) end end Alright, let's run that and make sure it fails how we expect it to fail. Making it pass Now that we are nice and red it is time to go green.  Let's actually write the plugin.  At this point, you may want to refer to Learn Vimscript the Hard Way by Steve Losh. It's a great way to get up and running in Vimscripting. Our example is naively simple, but it would be trivial to expand it into something more robust. autocmd BufWritePre * . rb call s:Trim () function ! s:Trim () %s /\\s\\+$/ / e endfunction So there you have it, our plugin in all its glory. We are telling it to trim ruby files on BufWritePre , which means it will execute s:Trim() right before saving the file.  Vioala, no more pesky trailing whitespace.  Let's head over to the command line and test it out.  Run the following commands cd path/to/plugin/spec/dir\nrspec --color example_plugin_spec.rb And we've officially arrived at green. Concerns Vimrunner requires whichever vim you are using as the server to be compiled with +clientserver and +xterm-clipboard if you plan on using GUI vim.  Once you have that you're ready to begin testing all your vim plugins.  If you would like to see a more robust example refer to Vim-Spacejam , the plugin that inspired this post. I hope this has inspired you to dive into Vimscript.  Thanks for reading. References Image by Zimpenfish Vimrunner Vim-Spacejam example plugin Learn Vimscript the Hard Way Damien Conway's 5 Part Vimscript series /ty @telemachus Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-07-08"},
{"website": "Hash-Rocket", "title": "Ruby5 Roundup - Episode 474", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/ruby5-roundup-episode-474", "abstract": "Ruby5 Roundup - Episode 474 by\nPaul Elliott\n\non\nJune 20, 2014 The news in the Ruby and Rails communities isn't going to deliver itself, but Lynn and I have your back. Here is a quick roundup of what's new in this episode of the Ruby5 podcast. http://ruby5.envylabs.com/episodes/511-episode-474-june-20th-2014 Vic Ramon's Ember Tutorial http://ember.vicramon.com Trying to use Ember with Rails but struggling with the concepts? Fellow Rocketeer Vic Ramon wrote a very comprehensive guide to using them together. gemoji https://github.com/github/gemoji Want to grab emoji by the colons? Check out the gemoji gem on GitHub! It gives you a rake task and helpers that make it easy to integrate into your app. stack_rescue https://github.com/excid3/stack_rescue Learning Rails and inundated with exceptions in your server log? Save valuable time with the stack_rescue gem! It automatically searches Stack Overflow and prints the top five results in your server log for every exception raised. rack-dev-mark https://github.com/dtaniwaki/rack-dev-mark The rack-dev-mark gem gives you a middleware that will inject a banner into your non-production environments so your testers and product folks will never be confused again. Grand Central Dispatch http://blog.motioninmotion.tv/grand-central-dispatch-isn-t-just-for-background-queues This blog post from MotionInMotion walks you through some interesting uses of Dispatch.once . It is a short read and I think the ideas are applicable to all Rubyists, whether you do RubyMotion or not. laptop http://robots.thoughtbot.com/laptop-setup-for-an-awesome-development-environment The fine folks at thoughtbot have made it even easier for you to have an awesome development setup. The laptop project is an open source set of scripts for building out and customizing your local development environment. So that's it for this episode of Ruby5. If you haven't already, subscribe to the podcast and keep yourself up to date. Thanks for listening! Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-06-20"},
{"website": "Hash-Rocket", "title": "Ember Routing: The When and Why of Nesting", "author": ["\nAndy Borsz\n\n"], "link": "https://hashrocket.com/blog/posts/ember-routing-the-when-and-why-of-nesting", "abstract": "Javascript Ember Routing: The When and Why of Nesting by\nAndy Borsz\n\non\nJuly 29, 2013 Understanding the proper use of the Router in an Ember.js app is a fundamental core concept of the framework. The docs tell us that the router translates a URL into a series of nested templates and almost all available tutorials show how to nest routes as part of an explanation of how the Router works. This is great when you want a UI where a list of items are present at the same time a single item is shown. This leaves many beginners struggling, however, as they try and replace the entire contents of a page with another route's template. Today we'll explore two ways to tackle this problem. Getting started First, let's start off with a perfunctory application template. <script type= \"text/x-handlebars\" > < h1 > My Ember App < /h1 > </script> var App = Ember . Application . create (); JS Bin example: http://jsbin.com/alewof/2/edit Our application is instantiated with one line to Ember.Application.create(); When Ember encounters a Handlebars template without a data-template-name attribute, it will use it by default as the template for the index route and consider it's data-template-name as 'application' if an 'application' template is otherwise not found. Thus Ember will create an app and render this template with little code. It can be helpful to consider this template the application layout. Adding a Products Route, Model and Template <script type= \"text/x-handlebars\" > < h1 > My Ember App < /h1 > {{ outlet }} </script> <script type= \"text/x-handlebars\" data-template-name= \"products\" > < h2 > Products < /h2 > {{ # each controller }} {{ title }} {{ /each} } </script> var App = Ember . Application . create (); App . Router . map ( function () { this . resource ( 'products' ); }); App . Store = DS . Store . extend ({ revision : 13 , adapter : 'DS.FixtureAdapter' }); App . Product = DS . Model . extend ({ title : DS . attr ( 'string' ) }); App . Product . FIXTURES = [ { id : 1 , title : 'Rube Goldberg Breakfast-o-Matic' } ]; App . ProductsRoute = Ember . Route . extend ({ model : function () { return App . Product . find (); } }); This is quite a bit of code at once but if you're acquainted with Ember basics it should look familiar. We've defined an {{ outlet }} in our application template where new templates will render. We've added a 'products' template that iterates the products available in the controller and lists their titles. We define 'products' as a route within App.Router and in our ProductsRoute we set the model property on the route to return all products. The rest of this code is our Product model, fixtures, and establishing our App.Store with ember-data. Additionally, we can automatically redirect to the 'products' route from the root by defining an IndexRoute. App . IndexRoute = Ember . Route . extend ({ redirect : function () { this . transitionTo ( 'products' ); } }); Now when we load our app the products route is activated and we should see the products template rendered. JS Bin example: http://jsbin.com/alewof/3/edit Nesting Routes As most tutorials available show, we can nest our routes as follows: App . Router . map ( function () { this . resource ( 'products' , function () { this . resource ( 'product' , { 'path' : '/:product_id' }); }); }); This will look very familiar to Rails developers, and in my opinion this is part of the rub. It seems very natural to nest this resource because the URLs implied match the notion of REST that most Rails developers recognize. From here we can add links to individual products in our products template: <script type= \"text/x-handlebars\" data-template-name= \"products\" > < h2 > Products < /h2 > {{ # each controller }} {{ # linkTo 'product' this }}{{ title }}{{ /linkTo} } {{ /each} } </script> And correspondingly we can add a minimal product template like so: <script type= \"text/x-handlebars\" data-template-name= \"product\" > < h2 > Product : {{ title }} < /h2 > </script> For the sake of demonstration, if we want our templates to nest just as our routes do we can add an {{ outlet }} to the bottom of our products template. Then when we click the link in the products template the product template will render inside of it. <script type= \"text/x-handlebars\" data-template-name= \"products\" > < h2 > Products < /h2 > {{ # each controller }} {{ # linkTo 'product' this }}{{ title }}{{ /linkTo} } {{ /each} } {{ outlet }} </script> JS Bin example: http://jsbin.com/alewof/4/edit We don't want to do this today, though. Let's look at some other Ember methods available to render templates. Replacing page content with renderTemplate We can define a renderTemplate method inside of an Ember Route object to handle \nthe specifics of which template the route will render and where: App . ProductRoute = Ember . Route . extend ({ renderTemplate : function () { this . render ( 'product' , { into : 'application' }); } }); JS Bin example: http://jsbin.com/alewof/5/edit Our new function is calling out to render the product template inside of 'application'. Remember that our Handlebars template without the data-type-name attribute is by default treated as 'application'. Now our product template is replacing the products template. Note that this happens even if you still have an added {{ outlet }} to the bottom of the products template. This seems like an innocuous and easy change to get the behavior we want, but we very subtly departed from the 'Ember Way' without realizing it. First, however, we've introduced a bug. Fixing the empty products template bug If you are on the products page and click the product link all works as expected. However, if you press the back button on your browser, (you can replicate this in the JS Bin example by pressing backspace or delete) you end up with an empty area where you expect the products template to render. Before the explanation, let's look at the code to fix this: App . ProductsIndexRoute = App . ProductsRoute = Ember . Route . extend ({ renderTemplate : function () { this . render ( 'products' , { into : 'application' }); }, model : function () { return App . Product . find (); } }); JS Bin example: http://jsbin.com/alewof/6/edit This should hopefully trigger some alarms that you've strayed from the path. Now in our ProductsRoute we need to add almost the same code to ensure the template is rendered where we want. Additionally and even worse is that we need to define a ProductsIndexRoute which we previously didn't even care about. Why is this? The reason for this behavior illuminates a fundamental difference in the way a Rails developer approaching Ember must look at nested routes. In Ember routes are very much tied to how templates will render on the page and not just constrained to URL construction and interpretation. In Ember when you designate that a route is nested, you are essentially confirming that the child route and the parent route will render at the same time, on the same page. This is a default behavior of nested routes. Remember that we're talking about routes and not necessarily URLs. In the case above we have defined our routes in a nested fashion, and then strong-armed Ember away from its defaults by forcing the product template to render over its parent. When we click the back button on our browser, Ember expects that we are simply returning to a route which it believes has already rendered. Thus as far as your Ember app is concerned, the content you expect to see should already be there. To make matters even more complex, when you supply a function as an argument to a resource in your route (nesting them), an index route is created in memory and its template will be rendered after the products route. Therefore when we go back from the product route Ember considered the products index the former route, and then the products route. For this reason if they are all to render into the application template they must all implement the same renderTemplate code. Whether this seems complex or not, rendering over the parent with renderTemplate is going against the grain. Let's look at something a little more idiomatic. The Ember Way To correct this error, we need to go back to where it began, in the Router. App . Router . map ( function () { this . resource ( 'products' ); this . resource ( 'product' , { 'path' : 'products/:product_id' }); }); This may seem weird at first because we expect product to be nested under products from a URL routing standpoint, but here they are declared flatly beside each other. We can maintain the URL structure we expect by passing the appropriate value in for our 'path'. We can also now remove the renderTemplate method from our ProductsRoute: App . ProductsRoute = Ember . Route . extend ({ model : function () { return App . Product . find (); } }); We've also removed the declaration of the ProductsIndexRoute from this code as well. Another object that's ripe for the chopping block is the ProductRoute. Once you remove the renderTemplate code from the ProductRoute you aren't left with anything expect for declaring the ProductRoute's existence which Ember will take care of for you by default. This is significantly more straightforward, and feels more in line with Ember idioms. JS Bin example: http://jsbin.com/alewof/7/edit Routing revisited Ember's Router largely represents the state of your application in a way that is both more firmly established and enforced than any other Javascript MVC framework that I have used. This is a good thing, but it requires a little adjustment in your thinking if you're coming from other frameworks. Plan your routes according to your UI. If a route replaces another, it should should be represented at the same level in your router. If a route is to render on the same page as another route within the {{ outlet }} in its template, then you should nest that route in your Router. Ember is doing a lot for you. If something feels tricky, take a step back and make sure you're respecting the defaults. Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-07-29"},
{"website": "Hash-Rocket", "title": "Vim can have better tab characters for Golang", "author": ["\nChris Erin\n\n"], "link": "https://hashrocket.com/blog/posts/cool-looking-tabs-in-vim", "abstract": "Vim Vim can have better tab characters for Golang by\nChris Erin\n\non\nFebruary 21, 2017 A short tutorial showing how to display beautiful tabs for a beautiful language: Golang. Tabs characters can be distracting in vim I have a problem.  That problem is: Whenever I open a file in vim for a language that uses tabs rather than spaces to indent I see the > char followed by 8 spaces.  And the > is blue.  To me, this is ugly. I see this in both Php and Go.  For PHP I don't care.  For Golang however, I do.  Go is a beautiful language and it deserves to look good in my editor.  I want something clean looking that doesn't detract from the code but still conveys the sense of whitespace. The characters The first step is replacing the > character with something more pleasing to my eye, like a pipe ( | ). listchars is the option that governs what characters you see and my default setting (which you can check with :set listchars ) is: listchars = tab :> , trail :-, extends :>, precedes :<, nbsp :+ The value we want for tab is instead \\|\\ .  We need to escape the pipe\nbecause that is a special command mode operator in vim.  When editing listchars\nyou can type :set listchars= and hit the tab key to place the current value\ninto the command and then edit that current value.  The final command I want\nto run is: :set listchars=tab:\\|\\  ,trail:-,extends:>,precedes:<,nbsp:+ There are better characters, but they are non-ascii The pipe doesn't fill the entire line height as I was hoping.  There are longer pipes in unicode, but they are a bit trick to access. Vim has a concept of digraphs to make it easier to remember special characters.  A digraph is short two ascii character keys that map to a unicode character.  Digraphs are easier to remember than unicode digital values.  To see all the digraphs mapped in vim type: :digraphs or just :dig . I've spotted a longer pipe ( │ ) that has the digraph of vv and also a centered pipe that has the digraph of VV .  To use a digraph type CTRL-K and the two character key ( vv or VV ) in this case.  With that knowledge I can now edit the listchars with this command: :set listchars=tab:\\│\\ ,trail:-,extends:>,precedes:<,nbsp:+ Note the longer pipe.  A bit better, yes? Tab width As a matter of personal preference I prefer shorter tabs.  In this case I'm going to shorten the tab width with :set tabstop=4 or set ts=4 . Colors The character is still light blue.  I would like a neutral gray\nand a dark grey background to differentiate the whitespace determined by the tab character from the space\ncharacter. Vim's highlighting system is fairly sophisticated, in this case I can change the colors of this particular syntactic element with: :highlight SpecialKey ctermfg=grey ctermbg=black Which I like, but I want something more precise.  We're using the cterm{fg,bg} but values those values can represent 256 colors.  Too limited. There is a way to use rgb hex colors instead of the 256 colors, but I need macvim in iTerm2 rather than vim .  I use to have vim symlink to macvim but when I upgraded to vim 8.0 I accidentally overwrote that symlink. I am using macOS and brew so the commands I used to fix this were: brew upgrade macvim --with-override-system-vim\n#and\nbrew link --overwrite macvim Now I'm using macvim but how do I access a larger pallete of colors?  By\nusing the gui{fg,bg} values rather than the cterm{fg,bg} values.  Let me try to\nset some ridiculous colors just to see if that works. :highlight SpecialKey guifg=red guibg=purple No, still grey.  What I am missing is an option introduced in 7.4 that allows a vim instance running in a terminal to have access to the enabled gui{fg,bg} values.  This option is: :set termguicolors I now see the ridiculous colors shining through.  Let me try the more neutral greys that I would rather have. :highlight SpecialKey guifg=#333333 guibg=#111111 Go now has the tab character display that I like.  I hope you can use these tips to customize tab characters to your own preference. Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-02-21"},
{"website": "Hash-Rocket", "title": "Ember JS Tutorial", "author": ["\nVic Ram"], "link": "https://hashrocket.com/blog/posts/ember-js-on-rails-tutorial", "abstract": "Ember Ember JS Tutorial by\nVic Ramon\n\non\nJune 12, 2014 I’ve created a thorough Ember Tutorial that shows you how to build a complex Ember app with a Rails backend. If you’re new to Ember it should be a good introduction to the framework. There are both CoffeeScript and Javascript versions. My previous blog post on Ember was one of our most popular blog posts of the year -- there are clearly many people interested in learning Ember. Many of them are coming from a Rails background. I think coming from Rails helps, but there's still a lot to learn. Trek , an Ember core member, has said that “Ember applications start out with a complexity rating of 4/10 but never get much higher than 6/10, regardless of how sophisticated your application becomes.” I agree with that. Some things in Ember are easy, like understanding the way Ember Objects work. Other things are more complex, like routing and communication between the various objects in the framework. But once you get it, you get it, and you realize how simple the whole thing actually is. So my advice to aspiring Emberistas is to do my tutorial, build some stuff, and play with the framework. See what things do and what they don’t. I’m happy to help if you have questions along the way. Good Ember-ing! Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-06-12"},
{"website": "Hash-Rocket", "title": "Hashrocket Code Audits", "author": ["\nMicah Woods\n\n"], "link": "https://hashrocket.com/blog/posts/hashrocket-code-audits", "abstract": "Process Hashrocket Code Audits by\nMicah Woods\n\non\nJuly  3, 2014 Hashrocket has a thorough and well-structured code audit process. We evaluate code history, bootstrapping, architecture, value of tests, code clarity, security risks, and any area of concern a developer might encounter.  Depending on project size, it could last from a couple of days to a week – and the client will come away with invaluable insights into their product. Code History Architecture Decisions We gather information about historical development of the application by the structure of classes and modules. From this we can get an idea of the developer's intent. Commit History We comb the project's Git history (if available) and check for: Concise and descriptive commit messages Large unrelated commits Commented-out code Unnecessary blocks of comments Bootstrapping During the lifetime of an application, new developers are brought onboard and they will need to set up the project.  The first thing we try to do is start the project, even before we try to run the tests.  If it's difficult for our team of experienced Rocketeers, it could seem impossible for a green developer. Large projects have many moving parts that need to be configured for a development environment. Here are some questions we ask: Are there instructions in the README? This is basic, but you'd be surprised how often the answer is 'no'. Do any of the following need to be configured to run the application, and how difficult was it to set up the following? Database connection Worker processes Full-text search server Redis (computed property cache & counters) Creating a development user account API keys Deployment keys & configuration Ideally, there should be an example database configuration and instructions in the README about how to create the database, run migrations, and populate seed data. Additionally, we look for configured Foreman or Rake tasks to start external processes and include instructions. Really awesome projects have a bootstrapping script to quickly get up and running. When all of this is accounted for, we ask ourselves: how confident we are that the development environment is set up successfully? Smoke tests are a great way to know that all dependencies are working and that the application is running correctly. However, they are rarely seen in even very mature products. Architecture Code Climate offers excellent information about the quality of your code. Sometimes it's hard to know what to do with those stats. Hashrocket does more than just code metrics – we suggest solutions on how bad code may be refactored, and how those solutions effect the overall architecture of your application. Security You'd be hard-pressed to find an area so poorly defined as security. Furthermore, you'd find it equally difficult to find an area so important. There is no portion of the code audit process that has implications so directly tied to risk and loss. Problems here carry weight that directly effects your wallet – and more importantly, your users' wallets. Web applications by nature touch myriad technologies that have exposures at different rates. The best thing we can advocate is that you keep your dependencies up to date as much as possible. We also look into the following troublesome areas: Cross Site Scripting (XSS) Cross Site Request Forgery (CSRF) Unsanitized user inputs (database injection, evals, etc) Plain text passwords Tracked production configuration (Environment variables) Insecure API actions Tracked private keys The above areas are typically more the result of programmer oversight than the result of the framework being out of date. (You can view a more detailed list of potential security vectors on the rails security page .) Testing Do the tests actually test the behavior of the system? Tests (integration and unit) should test the behavior of the system without testing implementation.  In many projects, it's easy to find a unit test that tests a private method that is never used.  Test like these are useless.  Also, tests that check an implementation of a method are also useless.  Good tests treat the system like a black box: for input A, expect result B.  This allows the system to be changed easily without changing tests. Proceeding with development Given the information we obtain during a code audit, we can advise on the development process, testing and code quality – and we can also give time estimates for rewriting vs. refactoring problem areas. Clients come away with a comprehensive document that shows them where their project currently stands, and helps them find a way forward. photo credit: ondral on Flickr Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-07-03"},
{"website": "Hash-Rocket", "title": "Ruby5 Roundup - Episode 468", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/ruby5-roundup-episode-468", "abstract": "Ruby5 Roundup - Episode 468 by\nPaul Elliott\n\non\nMay 30, 2014 Lynn and I are back to bring you the latest news in the Ruby and Rails communities. Here is a quick roundup of what's new in this episode of the Ruby5 podcast. http://ruby5.envylabs.com/episodes/505-episode-468-may-30th-2014 Magical Routes http://www.reinteractive.net/posts/188-rails-discovery-magical-routes-part-1-major-usages This blog post from the folks at ReInteractive covers all the different ways you can form routes in Rails. If you are new to the framework or haven't used polymorphic paths before, you need to read this post. REST Client https://github.com/rest-client/rest-client Unsure of which http client to use with that REST API you are trying to talk to? The rest-client gem provides a simple DSL for interacting with RESTful services and some more advanced features for when things get a little crazy. PostGIS and Google Maps http://climber2002.github.io/blog/2014/05/18/postgis-and-google-maps-in-rails-part-1/ This blog series walks you through using PostGIS and Google Maps with Rails. It has lots of examples and explanations as well as a companion project on GitHub for you to play with. SSL for Rails with Heroku and DNSimple http://robots.thoughtbot.com/ssl-for-rails-with-heroku-and-dnsimple Setting up SSL has never been easier thanks to services like Heroku and DNSimple. This new blog post from thoughtbot walks you through the process of setting it up. Rails vs. Sinatra https://blog.engineyard.com/2014/rails-vs-sinatra This blog post from PG Hagerty at Engine Yard provides reasonable explanations of what it is like to get started with each of the two major web frameworks for Ruby. If you are just getting started or haven't used Sinatra before, you should definitely check this out. How to Get Developers to Write a Blogpost https://netguru.co/blog/posts/how-to-get-developers-to-write-a-blogpost The fine folks at Netguru wrote an excellent blog post about blogging. If you haven't been sharing your ideas publicly for any reason, you should take a look and hopefully have your mind changed. So that's it for this episode of Ruby5. If you haven't already, subscribe to the podcast and keep yourself up to date. Thanks for listening! Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-05-30"},
{"website": "Hash-Rocket", "title": "Ruby5 Roundup - Episode 434", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/ruby5-roundup-episode-434", "abstract": "Ruby5 Roundup - Episode 434 by\nPaul Elliott\n\non\nJanuary 23, 2014 Fellow Rocketeer Jonathan Jackson and I bring you the latest news in the Ruby and Rails communities this week. Here is a quick roundup of what's in this episode of the Ruby5 podcast. http://ruby5.envylabs.com/episodes/470-episode-434-january-24th-2014 Selecta https://github.com/garybernhardt/selecta Gary Bernhardt released a new open source project called Selecta. It provides a command line utility for fuzzy finding the things most important to you. It is really simple to use and written in Ruby! Check out the examples in the README to see how you can integrate it into your workflows. Goworker http://www.goworker.org If you have slow resque workers, Benjamin Manns has your back with Goworker. It is a worker library written in Go that is compatible with your existing resque queues. It may be able to process your background tasks considerably faster than your current Ruby workers. DevDocs http://devdocs.io Been looking for a single clean place to look up all your documentation? Look no further now that DevDocs is released. It gathers together the docs for all your favorite libraries and frameworks allowing you to search them all at once. The interface is really simple and easy to use. Front-end Job Interview Questions https://github.com/darcyclarke/Front-end-Developer-Interview-Questions Need to hire some front-end devs but don't know what questions to ask? A new open source project from Darcy Clarke collects great questions on a number of topics so you can interview those candidates like a pro. rubyconferences.org http://rubyconferences.org There's a new site written by Rocketeer Jon Allured that lists all the upcoming conferences in the Ruby community. It will keep you up to date on when and where they happen as well as which ones are available for talk submissions and registration. The whole site is open source so you can add new conferences with a pull request. PhantomJS Update https://groups.google.com/forum/#!topic/phantomjs/0GkXtTO6l4Q Been irritated by all the warnings coming out of PhantomJS since you upgraded to Mavericks? Version 1.9.6 of the library has the fix that will silence it all! So that's it for this episode of Ruby5. If you haven't already, subscribe to the podcast and keep yourself up to date. Thanks for listening! Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-01-23"},
{"website": "Hash-Rocket", "title": "Ruby5 Roundup - Episode 463", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/ruby5-roundup-episode-463", "abstract": "Ruby5 Roundup - Episode 463 by\nPaul Elliott\n\non\nMay  9, 2014 Today I am joined by the wonderful Lynn Elliott to bring you the latest news in the Ruby and Rails communities. Here is a quick roundup of what's new in this episode of the Ruby5 podcast. http://ruby5.envylabs.com/episodes/500-episode-463-may-9th-2014 Untangle Spaghetti Code With a Simple Change of Perspective http://www.justinweiss.com/blog/2014/05/05/untangle-spaghetti-code-with-a-simple-change-of-perspective/ This new blog post from Justin Weiss discusses the refactoring pattern of reversing the caller and callee. Flipping around how you think about the problem can often be just what you need to keep things simple. Atom is OSS http://blog.atom.io/2014/05/06/atom-is-now-open-source.html The guts of the Atom editor are now open source. This is a great move for the editor but the biggest win for the community is in the release of the Atom Shell framework that lets you build desktop applications with HTML, CSS, and JavaScript. DumbDelegator http://blog.atom.io/2014/05/06/atom-is-now-open-source.html Looking for a simple delegation framework that works with Rails' URL helpers? Look no further! The DumbDelegator gem has you covered. osxc http://osxc.github.io OSXC is a configuration framework for OS X that lets you script the setup of your workstations. Rails Caching http://www.rubytutorial.io/rails-caching-again Wondering how to get started with view caching in Rails? This two part blog post will have you up and running in no time. Eldritch https://github.com/beraboris/eldritch The eldritch gem makes concurrent processing in Ruby even easier with a simple DSL to run methods and blocks asynchronously. So that's it for this episode of Ruby5. If you haven't already, subscribe to the podcast and keep yourself up to date. Thanks for listening! Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-05-09"},
{"website": "Hash-Rocket", "title": "Best of TIL Year One: Rails", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/best-of-til-year-one-rails", "abstract": "Hashrocket Projects Ruby Best of TIL Year One: Rails by\nJake Worth\n\non\nJuly 12, 2016 Here are some of the top Rails posts from Today I Learned . My goal with this series is to highlight some of the top posts from\nthe first year of Today I Learned . Today we'll\nlook at Rails, our fifth-most active channel. Hashrocket was one of the first Ruby on Rails consultancies, and Rails is still very\nimportant to our business. Many of our projects are Rails applications, and we strive to maximize\nthis ever-evolving framework to meet client needs. Today I Learned is itself\nwritten in Rails ( source ); some of these posts came directly from working on it. Here are the top five most liked Rails posts, in order, from the first year of\nToday I Learned. Enjoy! Rails Sandbox 🏖 (Dillon Hafer) When you are working with a complicated database structure, and find yourself needing to debug a complex or dangerous (delete) action, you might be hesitant to experiment. Keep experimenting! Don't want to setup all that data again? No worries. You can use a sandbox: $ rails c -s Usage: rails console [environment] [options]\n  -s, --sandbox      Rollback database modifications on exit. The sandbox flag will keep all database changes in a database transaction when you start the rails console and automatically issue a rollback when you quit the console. Truncate Almost All Tables (Josh Branchaud) The database_cleaner gem is a handy way to make sure you have a consistent database context for\neach test example or suite. One database_cleaner strategy that can be used\nis the truncation strategy. This truncates the data from all the tables by\ndefault. This is not ideal for fixed tables that contain domain-specific\ndata because you end up having to do way more test setup than should be\nnecessary. Fortunately, specific tables can be excepted by the truncation\nstrategy using the except option. For instance, if we have a standard set of roles for users of our\napplication, we can except that table from truncation with a line like the\nfollowing in our rails_helper.rb file: DatabaseCleaner . strategy = :truncation , { :except =& gt ; %w[roles] } ActiveRecord subselects (Micah Woods) So you want to find all the rocketeers who wrote blog posts in a date range. Blog :: Post . where ( published_at: 15 . years . ago .. 4 . years . ago ). includes ( :rocketeer ). map ( & amp ; :rocketeer ) # Blog::Post Load (0.6ms)  SELECT \"blog_posts\".* FROM \"blog_posts\" #   WHERE (\"blog_posts\".\"published_at\" BETWEEN '2000-06-12 14:40:06.429288' AND '2011-06-12 14:40:06.429498') # Rocketeer Load (0.7ms)  SELECT \"rocketeers\".* FROM \"rocketeers\" #   WHERE \"rocketeers\".\"id\" IN (12, 13, 14, 16)  ORDER BY \"rocketeers\".\"name\" But you want to do it in one query! Rocketeer . where ( id: Blog :: Post . where ( published_at: 15 . years . ago .. 4 . years . ago ). select ( :rocketeer_id ) ) # Rocketeer Load (0.9ms)  SELECT \"rocketeers\".* FROM \"rocketeers\" #   WHERE \"rocketeers\".\"id\" IN ( #     SELECT \"blog_posts\".\"rocketeer_id\" FROM \"blog_posts\" #       WHERE (\"blog_posts\".\"published_at\" BETWEEN '2000-06-12 14:42:20.005077' AND '2011-06-12 14:42:20.005317'))  ORDER BY \"rocketeers\".\"name\" Interact with Rails via Runner (Jake Worth) The rails runner feature of the Ruby on Rails command line interface is pretty awesome. The documentation states: runner runs Ruby code in the context of Rails non-interactively. Use it like the ruby command to execute Ruby code in the context of your Rails environment. Take this file: # rails_runner_in_action.rb puts Developer . count # 5 puts Post . count # 40 puts Channel . count # 17 And run it with: $ rails runner rails_runner_in_action.rb\n5\n40\n17 It also runs Ruby code right in the terminal, so this works ( rails r is an alias): $ rails r \"puts Developer.count\" 5 http://guides.rubyonrails.org/command_line.html#rails-runner Return an Empty Active Record Collection (Micah Cooper) You can use .none in a scope to short circuit the query in the event you don't have all the data. Imagine this query but the project_type on a Project is nil class User scope :active -> { where ( archived: nil } scope :by_project , -> ( project ) do return none unless project . type . present? where ( project_guid: project . guid , role: project . type ) end end Just return none . The cool thing about this is it's chainable. So you can still do something like: project = Project . new ( project_type: nil ) User . by_project ( project ). active Conclusion Thanks to Dillon , Josh , Micah Woods , and Micah Cooper for these posts. Today I Learned had a spike in traffic near the beginning of the year, and\nthese posts are mostly from that time. But there's a lot of great Rails tips\nfrom earlier. See them all here: https://til.hashrocket.com/rails Thanks for reading this series of posts, and keep learning every day. This blog post is part five of a series; here's part one , two , three , and four . Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-07-12"},
{"website": "Hash-Rocket", "title": "When To Use Redux", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/when-to-use-redux", "abstract": "Javascript React When To Use Redux by\nJosh Branchaud\n\non\nFebruary 20, 2018 Have you had a conversation about Redux recently that has left you wondering — despite all of its praise and the assured recommendations — if you made a big mistake when you typed the words, yarn add redux ? Just got home from the React thought leader meeting. Bad news, we apparently all hate Redux now so please start rewriting your app as soon as convenient, thanks — Jani Eväkallio (@jevakallio) February 12, 2018 Jani is being a bit facetious here. But if you’ve been following along on twitter, there has been a lot of discussion about redux. The crux of the conversation is aimed at whether most apps even call for the use of redux and consequently if we, as developers, are using it when we don’t need it. Realization: Putting Redux in our company framework by default was a mistake. Result: 1 People connect *every* component. 2 People embed Redux in \"reusable\" components. 3 Everyone uses Redux. Even when they don't need it. 4 People don't know how to build an app with just React. — Cory House 🏠 (@housecor) February 11, 2018 I see this sentiment time and time again. We’ve been presented with this powerful tool and set of patterns for managing state in our apps. Then with wild abandon we plaster our entire app with reducers, actions, connected components, and tangled dispatch functions. Once you’ve mastered the Redux flow, everything starts to look like global state. In my mind the conversation boils down to this: we tend to overuse Redux and that has some consequences for our apps and how we build them. I’ve had my share of moments staring at the beginnings of a component wondering if component state will suffice or if I might as well wire it up to Redux seeing as most everything else is. I’ve also spent plenty of time navigating between the reducer file, action file, container, and presentational component wondering what all the ceremony is getting me besides cramped fingers. So, let’s leave the thought-leadery platitudes behind and (as Scott Rogowsky would say) get down to the nitty gritty.\nWhen should we use Redux? In the midst of a chorus of “don’t use Redux unless you need it”, “don’t use Redux for everything”, and “only pull in Redux once you feel the pain of managing state without it”, it is easy to be left scratching our heads wondering, “well, geez, I was told how great this library is, but I’m not sure if I’m even supposed to be using it.”\nIt’d be helpful to have a set of guidelines. I think Ken Wheeler gets us pointed in the right direction. FWIW, I think a lot of it comes from people using it for EVERYTHING. My personal state preferences: Server data: Abstracted away or via something like a graphql client where it's managed. Form state: setState Local state: setState Global state: Redux or Mobx or Unstated — JavaScript Kanye (@ken_wheeler) February 12, 2018 If we have a component, such as a toggle element, that has some basic state that only it cares about, keep that in component state. Similarly, when building out an interactive form, we can treat the parent form component as the arbiter of the state of the form. Component state updated with setState() will again suffice. The point at which we want to start thinking about a more managed solution, like Redux, is when that state is global. Most of our state probably isn’t global though. Which brings us to an important take away: we can and should use both local and global state in our apps. Managing everything in Redux is overkill. It may have negative performance implications, it will increase the complexity of your app, make it hard to refactor, and likely reduce the reusability of many of your components. So if we ought to only use Redux for global state, what do we consider to be global state? Most common example: I see people using Redux to hold form values. Rule: If only a single component cares about the data, use local state. https://t.co/9LVAkBwNvN — Cory House 🏠 (@housecor) February 12, 2018 Let’s invert this rule from Cory House. “If more than one component cares about the data, use global state.” Before we get carried away let’s expand on this a bit more. Technically speaking, people were build big fancy complex React apps before Redux came along. When the same piece of state was needed in disparate parts of the app, they just pushed the state up and up into some parent component until that state could flow down to wherever it was needed. This works. But it can be unpleasant. Component after component has to pass pieces of the state that it doesn’t care about down because some great-great-grandchild needs it. This situation has a name — prop drilling. This is the situation where we want to bring in Redux. Let’s make it a bit more concrete. Assume we are building an e-commerce site. We have a checkout page that shows the items we have purchased with their respective prices and the total shopping cart price. We manage all of this state with local component state in the Checkout component. Then, we go to implement the NavBar component which shows the number of items in the cart and the total shopping cart price. We quickly realize not only that we already have this data managed and rendered in the Checkout component, but also that we want both the Checkout component and NavBar component to stay in sync as the contents of the cart change. In other words, we have two disparate components that need to read and possibly update the same state. This is a perfect time to pull the shopping cart state of our Checkout component up into Redux. Once this data is in Redux, the Checkout and NavBar components can individually connect to Redux with the state and dispatch functions they need. We can assume that our e-commerce site has a bunch of other React-based functionality all of which at this point is managed in local component state. Leave it as is! There is no need to rewrite any of the existing codebase with Redux. If you’re feeling the urge to go on a Redux-ification spree, stifle it. Whatever development-time savings you were able to promise with the introduction of Redux will soon vanish with that ill-conceived refactoring. Redux is a powerful tool. It is most powerful if we know when to use it and we have a grasp of what our other options are. Our apps, especially SPAs, can get really complex really quickly. Much of our app state is local state and as such ought to be managed by a component. Once a slice of our state is needed in disparate components — components in distinct component hierarchies or components that are more than two levels apart — it may be time to bring in Redux. Epilogue It is worth noting that there are many other options for state management in the React ecosystem. As mentioned above, it can be perfectly reasonable to manage state with nothing more than setState() . A popular alternative to Redux is Mob . Certainly there are many others. This whole conversation is further complicated by the introduction of the Context API in React 16.3.0 as a formalized, first-class feature. Find an approach and tool that works for you, your team, and the particulars of your project. There is no perfect tool / framework / language / process.  All software is created within a context, and trade-offs are made based on that context. Learning to see and evaluate technical decisions from this angle will help you ask better questions, and build better systems — Caitie McCaffrey (@caitie) February 19, 2018 Was this post helpful? Share it with others. Tweet Share Post", "date": "2018-02-20"},
{"website": "Hash-Rocket", "title": "Up, Down, Up with Ecto Migrations", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/up-down-up-with-ecto-migrations", "abstract": "Elixir Up, Down, Up with Ecto Migrations by\nJosh Branchaud\n\non\nDecember  1, 2016 At Hashrocket, most of what we build are applications backed by relational databases. Our relational database of choice is PostgreSQL. We tend to offload as much work as possible on Postgres and we take joy in the opportunity to use all the features Postgres has to offer. Of course, using Postgres to this extent has implications for how we develop applications, in particular for the way we write database migrations. Though frameworks tend to come with fairly full-featured ORMs, these ORMs limit what we can do with Postgres. A cool way to get around these limitations is by writing migrations in SQL 1 . Up, Down, Up Writing migrations in SQL means we no longer get the down migration for free, as we would when using an ORMs DSL. This means we have to write custom down migrations along with our up migrations. For the basic kinds of migrations this is a straightforward task. When the migrations become more complex, perhaps even irreversible, we really have to think through and work out the exact details. Once both the up and down migrations are written, we like to migrate up , back down , and then up again. This helps us ensure with some level of confidence 2 that our migration is fully functional. For years we've had an alias for Rails projects to migrate up , down , up -- it's called twiki . I decided it's about time that our Elixir projects using Ecto get a twiki of their own. A Twiki Mix Task I wrote the following mix task to make it just as easy to migrate up , down , up in Elixir projects. defmodule Mix . Tasks . Ecto . Twiki do use Mix . Task @shortdoc \" Ecto Migration: Up, Down, Up\" @moduledoc \"\"\"\n    This will migrate the latest Ecto migration, roll it back, and then\n    migrate it again. This is an easy way to ensure that migrations can go\n    up and down.\n  \"\"\" def run ( args ) do Application . ensure_all_started ( Mix . Project . config [ :app ]) repos = Mix . Ecto . parse_repo ( args ) twiki ( repos ) end defp twiki ( repo ) when is_atom ( repo ) do migration_dir = repo |> Mix . Ecto . source_repo_priv |> Path . absname |> Path . join ( \" migrations\" ) count = down_count ( repo , migration_dir ) Enum . each ([ :up , :down , :up ], fn ( direction ) -> migrate ( direction , repo , migration_dir , [ step: count ]) end ) end defp twiki ([ repo ]) do twiki ( repo ) end defp twiki ([ _repo | _more_repos ] = repos ) do Mix . shell . info \"\"\"\n      Ecto.Twiki only supports migrating a single repo.\n      However, we found multiple repos: #{inspect repos}\n    \"\"\" end defp migrate ( direction , _repo , _migration_dir , [ step: 0 ]) do Mix . shell . info \" Already #{ direction } \" [] end defp migrate ( direction , repo , migration_dir , opts ) do Mix . shell . info \" Migrating #{ direction } \" Ecto . Migrator . run ( repo , migration_dir , direction , opts ) end defp down_count ( repo , migration_dir ) do direction_count ( :down , repo , migration_dir ) end defp direction_count ( direction , repo , migration_dir ) do Ecto . Migrator . migrations ( repo , migration_dir ) |> Enum . filter ( fn ({ status , _ , _ }) -> status == direction end ) |> Enum . count end end It functions in the following ways: If there are no pending migrations, it reports as much and does nothing. If there are one or more pending migrations, it migrates that many migrations, rolls them back, and then migrates them again. In this way, it works well if you have a couple new migrations ready to go. If there are issues with any of the migrations, you will see the error messages reported just as if you'd run the migration by hand. Lastly, because it is integrated with Ecto using the Migrator module (as opposed to simply shelling out), it is extensible. To twiki your latest Ecto migration, add this task to lib/mix/tasks/ecto.twiki.ex and run mix ecto.twiki . One tradeoff to using SQL instead of an ORM's DSL is that your application's migrations are not necessarily portable across relational databases. We are fine with this tradeoff because we find that in practice we rarely, if ever, switch off of Postgres. ↩ Depending on how closely the data in our development database mimics the data in our staging and production databases, we may experience a false positive of sorts. Everything migrates fine in our development environment, but there are conflicts, perhaps in the form of constraint violations, when we try to migrate elsewhere. To avoid this, we like to use scrubbed copies of production data in our development environment when possible. ↩ Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-12-01"},
{"website": "Hash-Rocket", "title": "Embrace the Database with Ecto", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/embrace-the-database-with-ecto", "abstract": "Elixir PostgreSQL Embrace the Database with Ecto by\nJosh Branchaud\n\non\nApril 13, 2017 The database is a hugely important, and often times underappreciated, part\nof the development stack we use to build most web applications. Web frameworks, such as Ruby on Rails, and their database adapter layers can\nsometimes lull us into a false understanding of the world. They leave us so\nfar removed from the actual database that we sometimes forget it is even\nthere. It becomes a mysterious black box that we can shovel data into and\nthen ask very rigid standard questions to get that data back out. When we\ntreat the database in this way -- as a dumb data store -- we miss out on all\nit has to offer. However, if we seek to understand the database and how to\nfully utilize it, we can harness guarantees about the correctness of our\ndata, build a solid data model, and write expressive, powerful, and highly\nperformant queries. The Elixir community has built Ecto , a tool and DSL for interacting\nwith database engines like PostgreSQL . Unlike\nmany adapters and ORMs, Ecto encourages us to understand the queries we\nwrite and to take advantage of many features a database engine like\nPostgreSQL has to offer. I recently spoke at both ElixirDaze (in St.\nAugustine, FL) and Rome Erlang Factory\nLite . I presented on these concepts\nshowing how Ecto makes it easy to constrain our data, query like we would\nwith SQL, and pull in the SQL and PostgreSQL-specific features that we\ndepend on. This topic is important to me because poorly modeled and ill-constrained\ndatabase schemas are dangerous. They can dictate poor design and defensive\ncoding throughout our application. Once it is allowed into production, this\ntechnical debt becomes extremely hard to manage and fix. And in some cases,\nit can mean incomplete or incorrect data. Ecto encourages and empowers us to\naddress these things head-on early in the development of our web\napplications. If you'd like to know more of the specifics, check out the recording of my\ntalk at ElixirDaze . Also, feel\nfree to follow along with the\nslides . You ought to come away from this talk with a renewed appreciation for what\nthe database can do for you. I hope it also piques your interest around\nElixir and Ecto. Enjoy! Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-04-13"},
{"website": "Hash-Rocket", "title": "Ruby5 Roundup - Episode 414", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/ruby5-roundup-episode-414", "abstract": "Ruby Ruby5 Roundup - Episode 414 by\nPaul Elliott\n\non\nOctober 25, 2013 This week I am back on the mic with my favorite Herokai, Matthew Conway, to bring you the latest news in the Ruby and Rails communities in another episode of the Ruby5 podcast. Here is a quick roundup of what's new this week. http://ruby5.envylabs.com/episodes/450-episode-414-october-25th-2013 casting https://github.com/saturnflyer/casting The casting gem is another approach to delegating methods to an \\\"attendant\\\" class. Similar behavior can be achieved with SimpleDelegator or by finagling things yourself with Forwardable. The most intriguing thing to me about this gem is the README, which contains an excellent discussion of delegation. If you haven't checked out the Clean Ruby book where much of this comes from, you really should. autoprefixer-rails https://github.com/ai/autoprefixer-rails This gem is a wrapper for the autoprefixer library written in coffeescript that automatically prepends browser prefixes to the css properties that need them. It works with the asset pipeline so you can just include the gem and it will start working. ivar_encapsulation https://github.com/citizen428/ivar_encapsulation The ivar_encapsulation gem makes it easy to have a mix of public and private accessors in your classes. You can declare lists of attributes and toggle the visibility of the getters and setters with some options to the macro. termit https://github.com/pawurb/termit Google Translate is an important part of all our lives at this point. Up until now, we have had to use our browsers to interact with it but the termit gem gives us all the goodness of that service right on the command line. It will even let you speak the translations right from the terminal! Ruby on Mavericks http://dean.io/setting-up-a-ruby-on-rails-development-environment-on-mavericks Wondering how to get a Ruby development environment up and running fast on Mavericks? This blog post from Dean Perry contains a checklist with all the steps to be developing in no time. Copy and paste commands FTW! So that's it for this episode of Ruby5. If you haven't already, subscribe to the podcast and keep yourself up to date. Thanks for listening! Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-10-25"},
{"website": "Hash-Rocket", "title": "Andrew Dennis is a Rocketeer!", "author": ["\nJon Allured\n\n"], "link": "https://hashrocket.com/blog/posts/andrew-dennis-is-a-rocketeer", "abstract": "Andrew Dennis is a Rocketeer! by\nJon Allured\n\non\nJanuary 14, 2013 Yesterday Andrew Dennis joined Hashrocket and we couldn't be happier to have him aboard. Last year he was among the first graduates of DevBootcamp and then went on to join Groupon as an apprentice. He joins us in Chicago and in addition to bringing some sweet, sweet technical skills, he's officially the 5th funniest person in the office.  He loves Ruby and sleeves of Oreos.  On his todo list - Obj-C, Cocoa, and Rubymotion. You can find him on Twitter as @alzonedout or GitHub as @adennis4 or in Darth Vader's bathroom. Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-01-14"},
{"website": "Hash-Rocket", "title": "Building Ember CLI Addons Simply", "author": ["\nJonathan Jacks"], "link": "https://hashrocket.com/blog/posts/building-ember-addons", "abstract": "Ember Building Ember CLI Addons Simply by\nJonathan Jackson\n\non\nJune 23, 2014 Before we begin, a backstory.  Ember is a framework for building ambitious web applications. Ember CLI is a project that enables you to get your ambitious web applications into the browser with a minimum amount of fuss.  It makes a lot of decisions for you, that allow you to get up and running quickly.  Build tools, ES6 module transpilation, and QUnit just work.  Which allows you to work on the important stuff.  It has a thrumming community that really cares.  Anyways, it's amazing. But, I'll stop gushing on Ember CLI now and get on with what I want to talk about.  Addons. edit: rjackson the Ember core team member who submitted the PR that brought addons to Ember CLI has just written a post about the topic.  Go read it here for some more awesome information One of the things that most excited me about Ember CLI was the promise of an easy way to share sections of reusable code between ember apps.  Recently, addons made it in. (in 0.0.35 ) I had a component that I'd been working on within an Ember CLI app that look like a perfect fit. It was integration / unit tested, and the method of sharing was copy and pasting the relevant sections (not ideal).  So I turned it into an addon.  Let's take a look at it! Roadmap So the skeleton of things we have to do to prepare an Ember app to become an Ember addon is pretty straightforward: Setup package.json Write ember-addon-main file to include files Setup folder structure Setup package.json Ember CLI looks for the ember-addon keyword within the package.json to identify your package as an addon and kick things off. \"keywords\" : [ \"ember-addon\" ], \"ember-addon-main\" : \"lib/ember-addon-main.js\" When Ember CLI sees ember-addon it looks for an ember-addon-main property (defaulting to main if not found).  Now we get to the fun stuff.  Ember CLI is going to read in lib/ember-addon-main.js and use the constructor it exports to create an instance with two methods; treeFor and included . Write ember-addon-main.js I'm going to be using the ember-addon-main.js file from ember-cli-super-number .  If you'd like to follow along with the full file you can here . EmberCLISuperNumber . prototype . treeFor = function treeFor ( name ) { var treePath = path . join ( 'node_modules' , 'ember-cli-super-number' , name + '-addon' ); if ( fs . existsSync ( treePath )) { return unwatchedTree ( treePath ); } }; EmberCLISuperNumber . prototype . included = function included ( app ) { this . app = app ; this . app . import ( 'vendor/ember-cli-super-number/styles/style.css' ); }; The treeFor method is called once for app , once for styles , and once for vendor .  The return from this should be a tree that can be merged into the including application's app , styles , or vendor folder. If you are curious about where the magic is happening (from the original PR): 'app' 'styles' 'vendor' In our case, we are looking inside the addon (from within node_modules) for ember-cli-super-number/app-addon . which is returned on line 39.  The unwatchedTree function ensures that the watcher doesn't monitor changes on included files. You might be wondering why ember-cli-super-number suffixes '-addon' to the app directory within treeFor . Understanding the '-addon' suffix I made this section because I need to make a few caveats.  Ember CLI addons are moving fast.  As such, the conventions surrounding how these addons are being implemented are still being established.  While moving ember-cli-super-number to an addon we wanted to be able to keep the integration/unit testing as is.  And we didn't want to bring any non-essential files used for testing (and the like) into the including application.  So, we decided to have an app-addon directory. Locally to ember-cli-super-number , we'd merge app and app-addon before new EmberApp was called in our Brocfile. This would allow us to return only the relevant files to treeFor by constructing the tree (inside ember-addon-main script) by appending -addon , and still keep the application running as before locally. This provides two major benefits.  We only include files that are relevant to the including application (as mentioned above).  And, we are able to act as if the addon is simply another Ember CLI app locally.  This makes testing quite simple and easy, in fact, the tests for ember-cli-super-number didn't have to change as a result of turning it into an addon. Merging these trees together is made quite simple with Broccoli: var mergeTrees = require ( 'broccoli-merge-trees' ); var appTree = mergeTrees ([ 'app' , 'app-addon' ], { overwrite : true }); var vendorTree = mergeTrees ([ 'vendor' , 'vendor-addon' ]); var EmberApp = require ( 'ember-cli/lib/broccoli/ember-app' ); var app = new EmberApp ({ trees : { app : appTree , vendor : vendorTree } }); I've done the same thing for vendor-addon, and would for styles if that was necessary.  The above simply merges app and app-addon before including newing up the EmberApp. I really want to stress that these conventions are still in flux.  Another option is to merge an app directory and tests/dummy .  In that scenario we'd end up with the same usability, but app would then contain only addon files, and tests/dummy would contain the rest of the Ember CLI app.  (this idea is influenced by the way Rails engines are tested) The included hook Our ember-cli-super-number component also needs to apply css. In order to accomplish this, we need to use the included hook.  This hook is called by the EmberApp constructor and given access to this as app. This is the perfect place to vendor our css. You can see where it is called here (from the original PR) You'll remember from earlier that we had a vendor-addon folder.  This is where we'll place our css. Then in our included hook we'll use our access to app to import into vendor .   We'll do this here, because if we add it here it will automatically be concatted to the assets/vendor css link in the default app/index.html file (of the including application). This seemed like a sensible default for the super-number component because it requires some css to be functional, but leaving it out could lead to great flexibility for the including app as it would force the including application to implement their own css (or copy it).  This is a decision left to each individual use case. Publishing Once you've done the above you have to remove the \"private\": true flag from package.json and call npm publish .  And now you can install your addon with npm install ember-cli-<your-addon-name-here> --save-dev and use your included stuff. You should probably add an .npmignore file to ignore the app , vendor , and tests so that they aren't shipped with the package (making for quicker downloads) In our example, you can now call: npm install ember-cli-ember-super-number --save-dev And within your tempates you can call {{super-number}} to get the super number component. Fin Personally, my conclusion is that this is really awesome stuff.  The people who are pushing the addon story through are trying to make this stuff easy and approachable while at the same time establish conventions that will work for a very wide range of people.  For this reason, I'd like to extend some gratitude to them.  Thanks! Hopefully, I didn't overcomplicate this topic.  It is actually quite simple to setup.  I wanted to go into a little more detail because documentation is so sparse on addons due how new it is.  Anyways, I hope this helps. Cheers photo credit Robert S. Donovan on Flickr Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-06-23"},
{"website": "Hash-Rocket", "title": "Edge of Tomorrow, Explained in Git", "author": ["\nVic Ram"], "link": "https://hashrocket.com/blog/posts/edge-of-tomorrow-explained-in-git", "abstract": "Edge of Tomorrow, Explained in Git by\nVic Ramon\n\non\nJune 19, 2014 Many people seemed to enjoy my recent post where I related X-Men: Days of Future Past to Git. So I've decided to do it again with Edge of Tomorrow . This is now the second installment in The Git Movie Series: Time Travel Movies Explained in Git . SPOILER ALERT Yeah, I'm about to discuss the entire movie. In The Beginning Major William Cage, played by Tom Cruise, meets with a British general in London. The general wants to send him to film the upcoming invasion on French beaches. Cage, not wanting to be anywhere near actual fighting, refuses and threatens to blackmail the general in order to get out of it. That really pisses the general off, so Cage is stripped of his rank and sent to a base at Heathrow Airport with orders to fight on the front lines of the invasion. Cage gets minimal training on the exoskeleton suit and is dropped onto the beaches of France the next morning with the rest of the invasion force. It goes terribly. The mimics are ultrafast killing machines. In fact, I don't really know how any people are still alive at this point in the story. The mimics could have steamrolled the whole planet by now. Anyways... $ git commit -m 'Wake up at the Heathrow base' $ git commit -m 'Land on the beach' The Magical Blue Blood Cage gets obliterated, but right before he's about to die he grabs what looks like a Claymore and blows up one of the blue Alpha Mimics. $ git commit -m 'Blow up an alpha mimic along with myself' The mimic's magical blue blood (the power of Git) flows into Cage and he dies... only to immediately RESPAWN at the base at Heathrow Airport the day earlier with all of his memories intact. Let's assume that Cage's memory is located in app/characters/cage/memories.yml . First let's do a soft reset on that last commit. That way we can isolate the memory file. $ git reset head ^ Git resets are soft by default, meaning that the files that are reset will still exist as unstaged files. With a hard reset the files would be removed entirely. Now let's stash the memory file: $ git add app/characters/cage/memories.yml $ git stash And now we want to reset hard all the way back to where Cage wakes up at the airport. $ git reset --hard wake-up-at-heathrow-base-commit-id And Cage still has his memories: $ git stash pop The Time Loop The first time Cage respawns he is in a total daze. He goes back into battle and dies again. $ git commit -m 'Land on the beach' $ git commit -m 'Get killed by a mimic' And now we repeat the soft reset, stash, hard reset, and stash pop: $ git reset head ^ $ git add app/characters/cage/memories.yml $ git stash $ git reset --hard wake-up-at-heathrow-base-commit-id $ git stash pop This same process happens over and over as Cage keeps dying. Finding the Omega Cage learns about the Omega from Dr. Carter, Rita's friend. Cage starts to have visions of the Omega in Bavaria, but when he goes to kill it there he finds out it's a trap . He narrowly escapes and is able to reset. Cage and Rita go to retrieve the transponder. When Cage finally gets it and stabs it into himself, he obtains the power of GIT GREP. He can now search the world (the codebase) for the Omega. He discovers that it's located in the Louvre. $ git grep omega\napp/countries/france/paris/louvre/underground.yml: \"omega\" Found the sneaky bugger. Killing The Omega After discovering the true location of the Omega, Cage gets in a car wreck and is knocked unconscious. He wakes up in a hospital where, to his horror, he sees that he has received a blood transfusion. He has now lost his ability to respawn (hard reset). Cages sets out with Rita and the ragtag crew of misfits from Heathrow to kill the Omega. Only Cage and Rita make it inside the Louvre. Cage swims down to the Omega and blows it up. In an incredible stroke of luck, the blood from the Omega (git juice) enters Cage, giving him the ability to respawn one last time. $ git commit -m 'Recruit ragtag gang' $ git commit -m 'Kill the Omega' $ git commit -m 'Cage and Rita die' Let's check out a branch where we can save all this work we did to kill the Omega: $ git checkout -b kill-omega The Last Reset This time Cage respawns at an even earlier time -- he's flying on a helicopter to London to meet with the general. But he soon finds out that the mimics have been destroyed, making this meeting rather unnecessary. First checkout master and reset hard to the helicopter scene: $ git checkout master $ git reset --hard cage-gets-on-helicopter-commit-id Now let's cherry-pick the single commit from the kill-omega branch where Cage killed the Omega: $ git cherry-pick kill-the-omega-commit-id This commit would also contain the most updated version of Cage's memories, so the codebase is now in the right spot. Cage goes to find Rita, ending the movie with a big cheesy grin. $ git commit -m 'Cage goes to talk to Rita' $ git commit -m 'Everyone lives happily ever after' Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-06-19"},
{"website": "Hash-Rocket", "title": "The Story of Hashrocket's Pairing Setup", "author": ["\nChris Erin\n\n"], "link": "https://hashrocket.com/blog/posts/hashrocket-s-pairing-setup", "abstract": "Community The Story of Hashrocket's Pairing Setup by\nChris Erin\n\non\nDecember 20, 2016 A symbolic world. The Morning The metallic train rumbles past the office windows, spreading concentrated sun briefly onto a pair of developers. \"Where should we begin?\", asked Josh. Ah yes, beginnings; days have them, projects have them, companies have them. Our protagonist company had a beginning, no one in this room was there, and that beginning was shaped like a glass globe, molten then cool, then hot, then cold and finally formed. Settled into this metaphorical spheroid, Josh and Jon set about their task. A single glowing flat screen provided the warmth of possibilities, absorbed by their faces, which animated their shifting eyes, left, then right, then left. From the blackness of the screen, symbols formed, at first very rapidly, then not at all. \"What are you thinking Josh?\", said Jon. Josh made motions, a finger in the air to indicate he was going to write more. The symbols began to form again, rapidly. Josh's hands were resting on a set of keys and they moved with practiced coordination while the developers' eyes moved again, left, then right, then left. Jon had an identical set of keys in front of him, just resting. When Josh stopped moving, Jon began, at the same time saying, \"It's like this, I think\". The symbols were grouped into larger symbols called code, the code into lines, and the lines into files that were endlessly created, removed, rearranged and changed to fit a dual purpose. The symbols should instruct a computer to do a very specific thing, and the symbols should clearly convey the instructions to a human. Josh thought through the symbols from the computer's perspective, Jon from the human's, and in tandem they spiraled towards a small set of complex instructions that satisfied both. They outlined thoughts through the act of editing code, because that was faster and clearer than talking. They had to be fast, a deadline was always approaching, and fast meant knowing a large set of movements and commands that could create, remove, rearrange and change code to fit the dual purpose. A History They used Vim , an ancient tool who's genesis came before either of them were born. Vim is the amalgamation of tools created when computers were not powerful and before glowing screens.  Proto-developers viewed their creations on paper with ink mechanically spread over them and changes were scrupulously planned in advance to use the minimum amount of computer effort necessary.  The necessity for efficiency was its power and even as computing grew this power never dissipated. Vim had come to be regarded as a dusty curiosity buried deep within the layers of pleasantries needed for a human and a computer to work together. Glitzier tools had been developed that were more adept at easing a human into the morass of miscellany that is computer programming. These newer tools relied on pointing, not typing.  Specialized pointers were created that humans moved with gentle lateral movements and sharp clicking movements. Slowly, the intelligentsia of the symbol world came to realize that writing code by pointing was just as inarticulate as humans communicating by pointing. The protagonist company had come into being at a time of dissatisfaction. Creating art and tools through symbols was ponderous. Hundreds of symbol warriors were expected to agree on what a computer should do by pointing at it. They argued about how to point and a hierarchy of pointing masters developed.  This hierarchy was  determined not by how effectively they could create with pointing but by how artfully they pointed at each other. The conceit of the protagonist company was that hundreds of symbol warriors were unnecessary, the pointing masters were unnecessary; it could all be done by two quick and practiced symbol manipulators working in tandem and communicating through actions and words. A reason for being existed, but the beginning was a hot time in this molten unshaped company and pointing and clicking was still in vogue even among these intelligent practitioners. Old habits, ya know. It was at this time that a legend came to be. A man of rare symbolic vision was hired. The tale has been passed down by word of mouth, and with all old stories, the words have changed but the truth remains. Tim Pope walked into the coastal Floridian office one morning and said, \"We shall all use Vim \", and it was so. From that time, the pointing ceased and work began in earnest to achieve ever faster rates of symbol manipulation. They decided that it was important to all agree on a large set of actions that would shorten all common tasks. Things moved quickly at this time, but with the symbolic leadership of Tim Pope, the actions formed and cooled and became accepted and known throughout the company. The Evening Josh and Jon were not there when these formations took place, but took comfort in the idea that a common language of creating, removing, rearranging and changing had been created and agreed upon. They learned this language, and though it evolved as all languages do, they used it to solve problems large and small, and it felt effective. As a train crawled past in the dark of a winter evening, passengers could see through the big windows a pair of software developers staring at a glowing flat screen, each one's hands resting on a set of keys. \"Man, we got a lot done today; I'm exhausted\", said Jon. \"Yeah, and we did some cool stuff too\", said Josh. \"Yeah we did; same thing tomorrow?\" \"For sure, see ya tomorrow, Jon.\" Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-12-20"},
{"website": "Hash-Rocket", "title": "DES+DEV: ASSEMBLE!", "author": ["\nCameron Daigle\n\n"], "link": "https://hashrocket.com/blog/posts/des-dev-assemble", "abstract": "Community DES+DEV: ASSEMBLE! by\nCameron Daigle\n\non\nMay  5, 2014 Next Thursday, Hashrocket will be headed to downtown Jacksonville to join KYN in welcoming two veteran talents for an evening of speaking & learning. Plus, it's going to be hosted in Jacksonville's historic Barnett Building. Here are the pertinent details: The speakers of note will be Mig Reyes and Javan Makhmali , designer and developer (respectively) of Basecamp (formerly 37Signals). They'll be speaking about a topic near and dear to our hearts at Hashrocket: collaboration between design and development. In addition to a top-tier speaker lineup, DES+DEV: ASSEMBLE! will also feature: A social hour with local designer & developer groups Demos from KYN apprentices featuring their food truck app \"Truck'n'Seek\" The KYN Apprentice Graduation After-Party featuring local food trucks Plus, you'll be treated to the epic interiors of the historic Barnett Building , which is in the midst of a complete restoration and has some of the most gorgeous elevators you'll ever see. (No, really.) We're partnering with KYN for this event – KYN is the premiere startup incubator in downtown Jacksonville, and features a couple of Hashrocket alumni at the helm. This is just the latest in a growing list of meetups & events they've been hosting in downtown Jax. tl;dr: DES + DEV: ASSEMBLE is Thursday, May 15 from 6:00pm to 9:00pm. Tickets are $20, but you can use the promo code HASHROCKET to save 50%. Seats are limited, so get moving (and hey, feel free to pass on the link): http://bit.ly/desdevassemble We're thrilled to be part of this event, and hope to see you there. Just look for anybody in a Hashrocket t-shirt and come say hi! Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-05-05"},
{"website": "Hash-Rocket", "title": "Introducing bdubs.vim", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/introducing-bdubs-vim", "abstract": "Vim Introducing bdubs.vim by\nJosh Branchaud\n\non\nDecember 15, 2016 I'm excited to announce my latest edition to our Hashrocket toolbelt, bdubs.vim . For years, our dotmatrix setup has included a Vim plugin called bufonly.vim . This plugin is not exactly essential, but definitely a very handy addition to our Vim workflow. It allows us to delete all the buffers in a Vim session except for the current one. It has certainly found a place in my brain's collage of Vim keybindings and commands. I've used its :BO command constantly, but come to realize that I need something more. To help understand why this kind of functionality is useful, I will explain a common scenario. At Hashrocket, we pair on dedicated development machines. We open Vim when we start working on a project and intend to leave it open indefinitely, only closing it on system update or by accident. This means our Vim sessions usually live for days, sometimes even weeks. As you might imagine, as we work on feature after feature in a large code base, the list of buffers hanging around (see :ls! ) can grow quite large. Every once in a while, perhaps in between features, we'd like a clean slate in order to maintain a more useful buffer list. A well-groomed buffer list makes it easier to navigate between and tab-complete buffers that we care about. This is where bufonly.vim comes in. We run :BO and our buffer list gets cleared out. Great! This plugin served my needs for a long time. As I started to pay attention to some of my pain points though, I realized I needed something more. First, I prefer to delete buffers with :bwipeout instead of :bdelete . The bufonly.vim plugin only supports :bdelete . This leaves around a ton of unlisted buffers. The :bwipeout command completely removes them. Second, I've found that deleting all but the current buffer tends to be overkill. I'd prefer to be able to preserve some of the most recently accessed buffers. As I work on a feature, I alternate between source-diving through numerous files and actually editing code in a few files. This means my buffer list gets filled up with a bunch of one-time items with the actual buffers of interest floating to the top. At various points in the development of a feature I usually want to clear out all the noise, but still have access to those recent buffers that I've been iterating on. I solved both of these issues by writing bdubs.vim . This plugin provides both the :BD and :BW commands so that I can choose between performing :bdelete and :bwipeout on all the buffers in my Vim session. I prefer to stick with :BW , but :BD is available for those that want to stick with the functionality of bufonly.vim . Additionally, I can append a number to either of these commands. For instance, if I run :BW5 , I will wipeout all but the five most recently accessed buffers. This plugin is now part of our Vim setup. It can be part of yours as well. Check it out on Github and browse the docs for more details on the available commands. Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-12-15"},
{"website": "Hash-Rocket", "title": "How to Switch from TextMate to Vim", "author": ["\nVic Ram"], "link": "https://hashrocket.com/blog/posts/how-to-switch-from-textmate-to-vim", "abstract": "Vim How to Switch from TextMate to Vim by\nVic Ramon\n\non\nSeptember  8, 2013 Switching to Vim from an editor like TextMate or SublimeText can be intimidating.  You might find yourself dreading your time editing text, but I'm here to tell you the effort is worthwhile.  When starting at Hashrocket, I found myself immersed in a Vim culture.  I decided to make the switch after seeing a hint of what's possible. It was difficult, but I've become quite proficient, and want to share my insights with you! What surprised me the most about Vim is that it is genuinely fun. Learning new commands is like learning magical incantations. Short keystrokes can do powerful things. This makes improving your knowledge of Vim fun. Myth Busted: The Vim Learning Curve is Not That Steep You often hear that Vim is a beast to master. While that may be true, I think you can become productive in a short time. In fact, I think you become more productive than you were in your old editor in just a couple of weeks with some work and focus. I think the 80/20 rule applies to Vim: you can be very productive by just knowing 20% of all the things that Vim does. That's not to say that you should be ignorant of the other 80%, just that it will take time to learn it all. You don't need to know everything to make Vim work for you. How to Get Started Here are some things you can do to become productive in just a few days: Open up a terminal and type vimtutor . This opens up Vim's built in tutorial. It does a great job covering all the basics. Do the whole thing. You'll want to come back to it in a few days and do it over again to help cement all the commands in your memory. Check out VimGenius . It's a flashcard-style learning application for Vim that I created to go along with vimtutor. Print out a copy of this Vim commands cheat sheet: here . Read over the whole thing and keep it handy. Take notes and circle commands that you want to remember or that you particularly like.  Use post-its, or flashcards, pen/paper, chalkboard, well...anything.  Just do it by hand! Next, I would recommend doing some kind of task requires a lot of movement. I did the Ruby Koans , which worked extremely well for me. If you try and work on an actual project at first you might feel flustered, so doing something practice oriented like the koans is a good idea just to get your handles down. If you're not a Rubyist then try and find some other kind of task, e.g. Python Koans for python developers. There are also a few games that can help you learn Vim: Vim Adventures and Vim Snake are great at helping you master the hjkl movements. These movements require some muscle memory to get right. does a great job at showing the direction and relative distance each command will take you. You could set it as your desktop background if you were so inclined, or just print it out and keep it handy. Chris Hunt gave an excellent talk called \"Impressive Productivity with Vim and Tmux\" at Hashrocket's Ancient City Ruby Conference, which you can watch here: http://www.youtube.com/watch?v=9jzWDr24UHQ Once you've done all of this and feel comfortable getting around, I recommend that you start actually using Vim for your work. It will be slightly painful at first, but you'll get the hang of it more quickly than you expect. Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-09-08"},
{"website": "Hash-Rocket", "title": "Finding Failure: Git Bisect", "author": ["\nJonathan Jacks"], "link": "https://hashrocket.com/blog/posts/finding-failure-git-bisect", "abstract": "Finding Failure: Git Bisect by\nJonathan Jackson\n\non\nDecember 19, 2013 Any sufficiently large development team will assuredly run into a situation where a failure occurs in a codebase that isn't exposed by the test suite.  Depending on the teams' commitment to test coverage this situation might be all too common.  When this happens it helps to have something to reach for, something to help you narrow down that sea of incoming code into a manageable chunk to be debugged.  With only a few team members this is something that you'd likely do without thinking. \"Oh, it's broken?  Must have been that commit from yesterday.\" -Joe Developer What happens when Joe's team is too large for him to review every commit?  Well, that's what I'd like to talk about.  There is a tool for that: Git bisect! Git bisect is a tool that performs a binary search (more on this later) across a range of commits to help you find where an error was introduced.  Unfortunately, its use is not as wide spread as it should be. Personally, I found it quite intimidating at first, though now I find myself utilizing it everytime an issue like this arises. Let's get started If you're anything like me you want to get started with git bisect straight away.  With that in mind, we'll cover basic functionality first then move into what is actually happening later. The first command is git bisect start , which begins the bisect.  With the bisect begun, you'll now need to mark a bad commit.  You'll do that with: git bisect bad That will mark HEAD as a bad commit.  Alternatively, you could pass the sha of a specific commit like so: git bisect bad <sha> Then, unsurprisingly, you'll want to set a good commit.  This will be the last known good commit. These two commands will set the outer bounds of the binary search.  After the git bisect good <sha> command, git will split the revisions and load the first guess. At this point you'll need to find a way to determine if that commit is broken or not.  Usually this can be accomplished by running a test or loading up your develpment environment.  When you know the state of the commit run git bisect good or git bisect bad to load the next revision.  Eventually git will tell you which commit the error was introduced. Ta da!  Git bisect in a nutshell. A little deeper Git bisect is performs a binary search across your commits.  A binary search is also know as a half-interval search, and in my opinion, better describes what it does.  I'm going to run through an example to illustrate how bisect works, but if you'd like to get a better understanding of binary search look here . Let's assume that we have 5 commits.  They'll look something like: We notice that we have a failure at commit five.  We start git bisect off with the commands above: git bisect start\ngit bisect good <c1-sha>\ngit bisect bad Git will do somethings under the hood (more on that later), and mark the commits like so: After that last command git will transition your current working branch to the next revision up for testing: We run our test to find that c3 is good and we tell git that it's good by running git bisect good again: And again git will take current working branch to the next revision up for testing: We run our spec and discover that c4 is the commit the error was introduced in. In this example, with five commits, we were able to find the commit that introduced the error without having to test each individual commit.  We were able to omit testing of commit two.  Over a larger range of commits this saving will be, obviously, greater. (1 + log2 N) [3] A little deeper, still I run Hashrocket's dotmatrix on my development machine.  It sets a prompt that has information relevant to git when in a git directory.  I noticed, during a debugging session, that the prompt was aware of when a bisect was happening.  Naturally, this sparked my curiosity so I delved a little deeper to find out what git was doing under the surface. So what is git doing to facilitate the bisect?  Let's explore with an actual example of git bisect. If you'd like to follow along you can pull down this repo . Let's assume we've been developing a calculator program that adds two numbers together.  Everything is looking great, until we notice that our Rspec suite no longer passes.  In our haste to create the world's best calculator in Ruby we've pushed bad code up to master.  We know to use bisect in this scenario so we start it up. We run git bisect start and to know what files git is creating we follow that with tail -n +1 -- .git/BISECT_* to see the following output: We know that master is bad so let's mark it git bisect bad and again follow up with our tail command: We can now see that git has logged the commit we've marked as bad.   We remember explicitly that rspec ran green on our initial commit so we let git know that by typing git bisect good HEAD~4 Git tells us the commit that we've jumped to for test, and how many more steps we have to go. We can see that git uses .git/BISECT_EXPECTED_REV to set which revision is currently under test.  Running our test suite verifies that this revision is good.  So we tell git git bisect good .  Git again tells which revision we're on, and how many we have left. Our test suite fails on this commit.  So we mark it as bad with git bisect bad To which Git tells us that this was the commit that introduced the error and prints the log for that commit. In this example we could have automated the whole thing with git bisect run rspec calculator.rb Which would have run the test suite against each revision and marked good or bad based on the exit code of the command. Conclusions We've learned that Git's bisect command is a powerful way to diagnose problems as they arise in a code base.  Though it can appear intimidating at first the command is actually quite simple.  So next time you run into an error remember to reach for this, it may save you some time.  :) TL;DR git bisect start git bisect bad git bisect good HEAD~4 git bisect run rspec <test_file> References: Linux Bisect Man Page Git Bisect - Thoughtbot Article Good Stack Overflow Answer Fighting regressions with git bisect Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-12-19"},
{"website": "Hash-Rocket", "title": "Anonymous Functions & Lambdas: Elixir = Ruby", "author": ["\nJohnny Winn\n\n"], "link": "https://hashrocket.com/blog/posts/elixir-functions-ruby-lambdas", "abstract": "Elixir Anonymous Functions & Lambdas: Elixir = Ruby by\nJohnny Winn\n\non\nJanuary 16, 2014 This week in the Elixir Cauldron we are going to compare Elixir's anonymous functions to Ruby's lambda expressions. Although we don't think of Ruby as a functional language, there are features of Ruby that resemble its functional cousins, including its use of lambda expressions. Let's look at examples in both languages. First, define an anonymous function in Elixir and then look at the Ruby equivalent. iex ( 1 ) > say = fn () -> IO . puts \" Hello\" end iex ( 2 ) > say . () Hello :ok Here is the same functionality replicated in Ruby: irb ( main ): 001 : 0 > say = -> { puts \"Hello\" } irb ( main ): 002 : 0 > say . () Hello => nil Those were oversimplified examples, but they do illustrate the syntactical similarities in the two languages. The similarities don't stop there. Like other functional languages, Elixir functions are first class citizens. What does that mean? It means that functions not only yield values but they can also be assigned to variables, passed as arguments, or returned from other functions. We've seen functions assigned to variables, so how does passing as an argument or returning a function compare? To see this in action, we will define a function that accepts a function as an argument and executes it: defmodule Simple do def call ( func ) do func . () end end Now in iex call the function, passing an anonymous function as an argument: iex ( 1 ) > Simple . call fn () -> IO . puts \" Hello\" end Hello :ok How would that be implemented in Ruby and irb ? class Simple def self . call ( func ) func . () end end irb ( main ): 001 : 0 > Simple . call -> { puts \"Hello\" } Hello => nil What about returning a function from another function? defmodule Simple do def return do fn () -> IO . puts \" Hello\" end end end Now call the function from iex : iex ( 1 ) > say = Simple . return iex ( 2 ) > say . () Hello :ok And again in Ruby and irb : class Simple def self . return -> { puts \"Hello\" } end end irb ( main ): 001 : 0 > say = Simple . return irb ( main ): 002 : 0 > say . () Hello => nil These are simple examples but their purpose is to illustrate the similarities between the two languages. Uncovering syntactical similarities can ease a transition between languages. Next week, we will expand our view of Elixir functions by addressing a feature Ruby doesn't include, pattern matching. Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-01-16"},
{"website": "Hash-Rocket", "title": "Ruby5 Roundup - Episode 459", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/ruby5-roundup-episode-459", "abstract": "Ruby5 Roundup - Episode 459 by\nPaul Elliott\n\non\nApril 25, 2014 Herokai Matthew Conway and I are back again to bring you the latest news in the Ruby and Rails communities. Here is a quick roundup of what's new in this episode of the Ruby5 podcast. http://ruby5.envylabs.com/episodes/496-episode-459-april-25th-2014 letter_opener_web https://github.com/fgrehm/letter_opener_web The letter_opener_web gem provides a handy web interface on top of the awesome letter_opener gem from Ryan Bates. No longer will you have to redo an action to see what that email looked like in the tab you already closed. mint-exporter https://github.com/toddmazierski/mint-exporter Ever wanted to get at your mint.com data programatically? The mint-exporter gem lets you extract your transactions form the service utilizing some private APIs. mongoid-observers https://github.com/chamnap/mongoid-observers Mongoid 4 comes with a lot of changes, including the removal of observers from the core library. A new gem called mongoid-observers will add that functionality back if your app is relying on it. Interview with DHH https://www.youtube.com/watch?v=Wp_tTfoCXYg Hot on the heels of DHH's RailsConf keynote, Mike from UGtastic interviews DHH about TDD. ids_please https://github.com/gazay/ids_please The ids_please gem makes it easy to extract social network ids from profile page URLs. Ruby Heroes 2014 http://rubyheroes.com/ As always, the 2014 Ruby Heroes were announced during RailsConf this week. Congratulations to all the winners! So that's it for this episode of Ruby5. If you haven't already, subscribe to the podcast and keep yourself up to date. Thanks for listening! Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-04-25"},
{"website": "Hash-Rocket", "title": "Ruby5 Roundup - Episode 429", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/ruby5-roundup-episode-429", "abstract": "Ruby5 Roundup - Episode 429 by\nPaul Elliott\n\non\nJanuary  2, 2014 Kick off the New Year with the latest news in the Ruby and Rails communities. Here is a quick roundup of what's in this episode of the Ruby5 podcast. http://ruby5.envylabs.com/episodes/465-episode-429-january-3rd-2014 Writing a Compiler in Ruby from the Bottom Up http://www.hokstad.com/compiler Vidar Hokstad just released parts 30 and 31 of his long running blog series walking you through the process of building a compiler from the ground up. It is a very interesting read with lots of great information for anyone interested in how the code you write is actually processed. The Omega Simulation Framework https://github.com/movitto/omega Ever wanted to run your own game universe? The Omega Simulation Framework is an open source project written in Ruby that lets you do just that. It is a configurable universe simulator with all the pieces you need to build your own galactic empire. If you're interested in game development, check out the source on GitHub! RubyGems 2.2.0 Released http://blog.rubygems.org/2013/12/26/2.2.0-released.html RubyGems 2.2.0 is finally out with a lot of changes and upgrades. It is now aware of ruby versions, gem installations, and Gemfiles so it should integrate better with the tools you're already using. Debugging an HTTP Client Library http://devblog.avdi.org/2013/12/29/debugging-an-http-client-library Ever tried to debug an HTTP client library? It is a quite challenging endeavor. Lucky for us, Avdi took it upon himself to do it and write an in-depth blog post about the adventure. Check it out and learn what's going on during your HTTP requests. Detect Similar Images http://www.amberbit.com/blog/2013/12/20/similar-images-detection-in-ruby-with-phash/ AmberBit has a new blog series about detecting similarities in images with Ruby. This is one of the more challenging tasks one might have to undertake as a developer, so give it a read and level yourself up a bit! Lotus http://lucaguidi.com/2014/01/01/announcing-lotus.html We've been long overdue for another web application framework to come on the scene. Luckily we now have Lotus! It promises to be a lightweight and flexible solution that lets you implement your application however you'd like, without all the opinionated-ness and monkey patching of other extremely popular frameworks. So that's it for this episode of Ruby5. If you haven't already, subscribe to the podcast and keep yourself up to date. Thanks for listening! Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-01-02"},
{"website": "Hash-Rocket", "title": "You can build SOAP requests with HAML", "author": ["\nMicah Cooper\n\n"], "link": "https://hashrocket.com/blog/posts/you-can-build-soap-requests-with-haml", "abstract": "Ruby You can build SOAP requests with HAML by\nMicah Cooper\n\non\nDecember 12, 2013 HAML can produce markup that looks terse, simple, and pleasant. SOAP requires\nmarkup can that look verbose, complex and unpleasant. If you combine the two\nand add just a touch of sugar, you can give your application some medicine that\ngoes down quite smoothly. Let's explore. If I'm making a soap request I usually have this markup from the documentation of the api. Take that markup and save it as a fixture so you can test against it later. spec/fixtures/<soap_service>/<operation>/xml <soapenv xmlns= \"foo\" > <header> <security> <username value= \"bar\" /> <password> baz </password> </security> </header> <soapbody> <foo:content> rem ipsum dolor sit amet </foo:content> </soapbody> </soapenv> So that's the xml. Let's see the HAML representation of the same thing: !!! XML %soapenv ( xmlns= \"foo\" ) %header %security %username ( value= \"bar\" )/ %password = password %soapbody %foo:content rem ipsum dolor sit amet Cleaner? Yeah? I think so too! Be sure to test that your HAML returns your xml in the proper format. Note about testing: HAML orders your attributes alphabetically so it's hard to do a direct == comparison on the two docs. One suggestion is to use Nokogiri in your tests. Now let's actually use this all as we would in a class that calls the service: require 'faraday' require 'haml' class SoapService def response Faraday . post ( < url > , request_body ) end def request_body #returns the xml document for the body of the request #pass #self to haml_engine.render #so your HAML can call all the methods #in this SoapService class. (very convenient!). haml_engine . render ( self ) end def haml_engine Haml :: Engine . new ( request_file , { attr_wrapper: '\"' , format: :xhtml }) end def request_file #called in request body File . read ( \"path/to/haml/file\" ) end def password #this method is called in the HAML document \"baz\" end end Now use this class like so: SoapService . new . request_body This will return: <soapenv xmlns= \"foo\" > <header> <security> <username value= \"bar\" /> <password> baz </password> </security> </header> <soapbody> <foo:content> rem ipsum dolor sit amet </foo:content> </soapbody> </soapenv> And that's it. I have found this to be a most pleasant way to build soap requests with ruby. Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-12-12"},
{"website": "Hash-Rocket", "title": "Using Datomic as a Graph Database", "author": ["\nAndrew Dennis\n\n"], "link": "https://hashrocket.com/blog/posts/using-datomic-as-a-graph-database", "abstract": "Using Datomic as a Graph Database by\nAndrew Dennis\n\non\nApril 10, 2014 Datomic is a database that changes the way that you think about databases. It\nalso happens to be effective at modeling graph data and was a great fit for\nperforming graph traversal in a recent project I built. I started out building a six degrees of Kevin Bacon project using Neo4j , a popular\nopen-source graph database. It worked very well for actors that were a few hops\naway, but finding paths between actors with more than 5 hops proved problematic.\nThe cypher query language gave me little visibility into the graph algorithms\nactually being executed. I wanted more. Despite not being explicitly labeled as such, Datomic proved to be an effective\ngraph database. Its ability to arbitrarily traverse datoms, when paired with\nthe appropriate graph searching algorithm, solved my problem elegantly. This\ntechnique ended up being fast as well. Quick aside: this post assumes a cursory understanding of Datomic. I won't cover\nthe basics, but the official tutorial will help you get started. 6 Degrees Kevin == Cool; 6 Degrees Kelvin == Cold The problem domain should be fairly familiar: the 6 degrees of Kevin\nBacon . I wanted to create an app where you could pick an\nactor and find out what their Bacon Number was. That is, given an actor,\nI wanted to answer the question \"how many degrees of separation is there\nbetween that actor and Kevin Bacon?\" Using information freely available from IMDb , I developed the following\nschema: [ ;; movies { :db/id # db/id [ :db.part/db ] :db/ident :movie/title :db/valueType :db.type/string :db/cardinality :db.cardinality/one :db/fulltext true :db/unique :db.unique/identity :db/doc \"A movie's title (upsertable)\" :db.install/_attribute :db.part/db } { :db/id # db/id [ :db.part/db ] :db/ident :movie/year :db/valueType :db.type/long :db/cardinality :db.cardinality/one :db/doc \"A movie's release year\" :db.install/_attribute :db.part/db } ;; actors { :db/id # db/id [ :db.part/db ] :db/ident :person/name :db/valueType :db.type/string :db/cardinality :db.cardinality/one :db/fulltext true :db/unique :db.unique/identity :db/doc \"A person's name (upsertable)\" :db.install/_attribute :db.part/db } { :db/id # db/id [ :db.part/db ] :db/ident :actor/movies :db/valueType :db.type/ref :db/cardinality :db.cardinality/many :db/doc \"An actor's ref to a movie\" :db.install/_attribute :db.part/db } ] In a nutshell, movies have titles and years. Actors have names and movies.\nThe \"relationship\" of actors to movies is many-to-many, so I've declared the :actor/movies attribute as having a cardinality of many. Using datalog queries Using datalog and datomic.api/q , we can make graph-like queries fairly easily.\nBecause the :where clauses of a datalog query form an implicit join, we can\njoin from our starting point to our ending point with relative ease. As an example, what if we wanted to know the shortest path or paths from Kevin\nBacon to Jon Belushi? Let's use a query to find out: ( require ' [ datomic.api :as d :refer [ q db ]]) ( def conn ( d/connect ... )) ( q ' [ :find ?start ?title ?end :in $ ?start ?end :where [ ?a1 :actor/name ?start ] [ ?a2 :actor/name ?end ] [ ?a1 :actor/movies ?m ] [ ?a2 :actor/movies ?m ] [ ?m :movie/title ?title ]] ( db conn ) \"Bacon, Kevin (I)\" \"Belushi, John\" ) ;=> #{[\"Bacon, Kevin (I)\" \"Animal House (1978)\" \"Belushi, John\"]} That is fine when actors have worked together in a movie (a Bacon Number of 1),\nbut doesn't help us solve Bacon numbers when there are 2 or more movies between\nthe actors. We could add more where clauses to join over two movies, but that\nisn't sustainable. The queries would quickly become too long to reason about.\nThis is a prime opportunity to use Datomic's rules. ( def acted-with-rules ' [[( acted-with ?e1 ?e2 ?path ) [ ?e1 :actor/movies ?m ] [ ?e2 :actor/movies ?m ] [( != ?e1 ?e2 )] [( vector ?e1 ?m ?e2 ) ?path ]] [( acted-with-1 ?e1 ?e2 ?path ) ( acted-with ?e1 ?e2 ?path )] [( acted-with-2 ?e1 ?e2 ?path ) ( acted-with ?e1 ?x ?pp ) ( acted-with ?x ?e2 ?p2 ) [( butlast ?pp ) ?p1 ] [( concat ?p1 ?p2 ) ?path ]]]) ( q ' [ :find ?path :in $ % ?start ?end :where [ ?a1 :actor/name ?start ] [ ?a2 :actor/name ?end ] ( acted-with-2 ?a1 ?a2 ?path )] ( db conn ) acted-with-rules \"Bieber, Justin\" \"Bacon, Kevin (I)\" )) ;=> #{[(17592186887476 17592186434418 17592187362817 17592186339273 17592186838882)] [(17592186887476 17592186434418 17592188400376 17592186529535 17592186838882)] [(17592186887476 17592186434418 17592187854963 17592186529535 17592186838882)] [(17592186887476 17592186434418 17592186926035 17592186302397 17592186838882)]} This time we get back a collection of paths with entity ids. We can easily\ntransform these ids by mapping them into entities and getting the name or title,\nusing a function like the following: ( defn actor-or-movie-name [ db eid ] ( let [ ent ( d/entity db eid )] ( or ( :movie/title ent ) ( :person/name ent )))) So, putting the query together with the above function, we get: ( let [ d ( db conn ) name ( partial actor-or-movie-name d )] ( ->> ( q ' [ :find ?path :in $ % ?start ?end :where [ ?a1 :actor/name ?start ] [ ?a2 :actor/name ?end ] ( acted-with-2 ?a1 ?a2 ?path )] d acted-with-rules \"Bieber, Justin\" \"Bacon, Kevin (I)\" ) ( map first ) ( map ( partial mapv name )))) ;=> ([\"Bieber, Justin\" \"Men in Black 3 (2012)\" \"Jones, Tommy Lee\" \"JFK (1991)\" \"Bacon, Kevin (I)\"] [\"Bieber, Justin\" \"Men in Black 3 (2012)\" \"Howard, Rosemary (II)\" \"R.I.P.D. (2013)\" \"Bacon, Kevin (I)\"] [\"Bieber, Justin\" \"Men in Black 3 (2012)\" \"Segal, Tobias\" \"R.I.P.D. (2013)\" \"Bacon, Kevin (I)\"] [\"Bieber, Justin\" \"Men in Black 3 (2012)\" \"Brolin, Josh\" \"Hollow Man (2000)\" \"Bacon, Kevin (I)\"]) The rules above are defined statically, but they are simply clojure data\nstructures: it would be trivial to generate those rules to an arbitrary\ndepth. For an example of doing just that, see the Datomic mbrainz\nsample . Low-level traversal for better performance Having to know the depth at which to traverse the graph is cumbersome. Datomic\nhas a distinct advantage of being able to treat your data as local, even if its\npermanent storage lives somewhere else. That means that we can bring our own\nfunctions to the problem and execute locally, rather than on a database server.\nWe can leverage Datomic's datoms function to search the graph using\nour own graph-searching algorithm, rather than relying on the query engine. Our IMDb actor data is essentially a dense unweighted graph. Because of its\ndensity, a bidirectional breadth-first search is probably the most\nefficient alogrithm for finding the shortest paths from one point to another. A\ngeneric bidirectional BFS returning all shortest paths might look like this. ( defn paths \"Returns a lazy seq of all non-looping path vectors starting with\n  [<start-node>]\" [ nodes-fn path ] ( let [ this-node ( peek path )] ( ->> ( nodes-fn this-node ) ( filter # ( not-any? ( fn [ edge ] ( = edge [ this-node % ])) ( partition 2 1 path ))) ( mapcat # ( paths nodes-fn ( conj path % ))) ( cons path )))) ( defn trace-paths [ m start ] ( remove # ( m ( peek % )) ( paths m [ start ]))) ( defn- find-paths [ from-map to-map matches ] ( for [ n matches from ( map reverse ( trace-paths from-map n )) to ( map rest ( trace-paths to-map n ))] ( vec ( concat from to )))) ( defn- neighbor-pairs [ neighbors q coll ] ( for [ node q nbr ( neighbors node ) :when ( not ( contains? coll nbr ))] [ nbr node ])) ( defn bidirectional-bfs [ start end neighbors ] ( let [ find-pairs ( partial neighbor-pairs neighbors ) overlaps ( fn [ coll q ] ( seq ( filter # ( contains? coll % ) q ))) map-set-pairs ( fn [ map pairs ] ( persistent! ( reduce ( fn [ map [ key val ]] ( assoc! map key ( conj ( get map key # {}) val ))) ( transient map ) pairs )))] ( loop [ preds { start nil } ; map of outgoing nodes to where they came from succs { end nil } ; map of incoming nodes to where they came from q1 ( list start ) ; queue of outgoing things to check q2 ( list end )] ; queue of incoming things to check ( when ( and ( seq q1 ) ( seq q2 )) ( if ( <= ( count q1 ) ( count q2 )) ( let [ pairs ( find-pairs q1 preds ) preds ( map-set-pairs preds pairs ) q1 ( map first pairs )] ( if-let [ all ( overlaps succs q1 )] ( find-paths preds succs ( set all )) ( recur preds succs q1 q2 ))) ( let [ pairs ( find-pairs q2 succs ) succs ( map-set-pairs succs pairs ) q2 ( map first pairs )] ( if-let [ all ( overlaps preds q2 )] ( find-paths preds succs ( set all )) ( recur preds succs q1 q2 )))))))) There's a lot of code here, including some optimizations and helper functions.\nThe important function here is bidirectional-bfs . I won't explain the details\nof the algorithm, but at a high level, it takes in a start and end node and a\nfunction to be called on any node to get it's \"neighbors\". This is a generic, pure function, agnostic of Datomic or our data. In fact, I used\na simple map as the \"graph\" while developing this: ( def graph { :a [ :b ] :b [ :a :c :d ] :c [ :b :e ] :d [ :b :c :e ] :e [ :c :d :f ] :f []}) ( bidirectional-bfs :a :e graph ) ;=> [[:a :b :c :e] [:a :b :d :e]] To use this generic algorithm with our database, we need a neighbors function.\nDepending on whether a node is an \"actor\" or a \"movie\", we need to return its\nappropriate counterpart. A naive \"or\" condition is actually good enough here: ( defn movie-actors \"Given a Datomic database value and a movie id,\n  returns ids for actors in that movie.\" [ db eid ] ( map :e ( d/datoms db :vaet eid :actor/movies ))) ( defn actor-movies \"Given a Datomic database value and an actor id,\n  returns ids for movies that actor was in.\" [ db eid ] ( map :v ( d/datoms db :eavt eid :actor/movies ))) ( defn neighbors \"db is database value\n  eid is an actor or movie eid\" [ db eid ] ( or ( seq ( actor-movies db eid )) ( seq ( movie-actors db eid )))) Gluing everything together is a simple matter of partial application: ( defn find-id-paths [ db source target ] ( bidirectional-bfs source target ( partial neighbors db ))) Given a source entity id and a target entity id, this will return all shortest\npaths (ids), much like the query example above. From there, we could map them\nto Datomic entities, get their names, or sort the paths using a domain-specific\nheuristic. Plugging in the previous example, we might do something like the\nfollowing: ( let [ d ( db conn ) d ( d/filter d ( without-documentaries d )) biebs ( d/entid d [ :actor/name \"Bieber, Justin\" ]) bacon ( d/entid d [ :actor/name \"Bacon, Kevin (I)\" ]) name ( partial actor-or-movie-name d )] ( map ( partial mapv name ) ( find-id-paths d biebs bacon ))) ;=> ([\"Bieber, Justin\" \"Men in Black 3 (2012)\" \"Jones, Tommy Lee\" \"JFK (1991)\" \"Bacon, Kevin (I)\"] [\"Bieber, Justin\" \"Men in Black 3 (2012)\" \"Segal, Tobias\" \"R.I.P.D. (2013)\" \"Bacon, Kevin (I)\"] [\"Bieber, Justin\" \"Men in Black 3 (2012)\" \"Brolin, Josh\" \"Hollow Man (2000)\" \"Bacon, Kevin (I)\"] [\"Bieber, Justin\" \"Men in Black 3 (2012)\" \"Howard, Rosemary (II)\" \"R.I.P.D. (2013)\" \"Bacon, Kevin (I)\"]) This returns the same set of paths as the query method did. However, this\nversion has the advantage of going to an arbitrary depth. This is just one example of graph searching with Datomic. Different kinds of\nproblems and domains could use other algorithms. The idea, though, is that\ngeneric graph searching functions can be used directly, since the data is\neffectively local to the peer machine. For more Clojure implementations of generic graph searching algorithms, loom's alg_generic namespace is a great starting point. Performance I'm using the above ideas and functions on IMDB's dataset to power\na six degrees of Kevin Bacon project. Once the peer's index caches are warmed, the performance\nis quite good: most searches I've performed between well-known actors complete\nin under a second, and in many cases, under 100 ms. I never got results that\ngood with Neo4j's cypher query language. Source The code in this post is based on the source for a six degrees of Kevin Bacon project. Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-04-10"},
{"website": "Hash-Rocket", "title": "The Elusive MVP", "author": ["\nChris Cardello\n\n"], "link": "https://hashrocket.com/blog/posts/the-elusive-mvp", "abstract": "Process The Elusive MVP by\nChris Cardello\n\non\nJuly 16, 2012 Minimum Viable Product, commonly abbreviated to MVP is a term almost everyone working in software is familiar with (or should be). It’s something everyone starting an application from scratch sets out with the intent to build though very rarely is actually achieved. The reasons are numerous and usually come so naturally that you don’t even realize that you’ve slipped into the feature bloat abyss before it is too late. Some of the more common reasons I’ve seen and fallen victim to myself are: Keeping up with zuckerbergs:  Facebook has this cool thing, Pinterest has this neat widget, Amazon does this other thing, my competitor has this feature, and we should do it too! The ever-moving finish line:  Features cannot be delivered faster than new ones are placed into scope. Fear of completion:  This is my life’s work and dream, I’ve been planning it for years and I don’t want anyone to see it until everything is perfect. It’s an Epic journey At Hashrocket most clients who walk through our door already have heard the term “Minimum Viable Product” and believe that the feature sets they have in mind are truly their MVP. Once we sit down and start learning their domain and talking through the workflows and roles in the system we typically find that in reality the expectation is anything but an MVP. In most cases clients want an application that is bigger than their budget. It is our job to make sure every story-card is in support of a feature that works towards allowing users of the system to achieve a core business value. The first day of story-carding we identify what the app is supposed to do, what’s the itch it is scratching. Once we know what the goal of the application is we capture the core feature sets needed to achieve that goal in one-line epics. We then talk through each of those epics capturing the individual stories required to complete them. When we identify a “risky” type of interaction we collaborate to determine the simplest solution that still achieves the goal of the feature. Many times we will write the cards both around the simple solution as well as the riskier interaction, tagging the riskier interaction as “Enhancement” or “Nice to have”. This allows us to help the client spend their budget on the items that are core to their MVP deferring superfluous scope until after launch. As an example, if we were building a simple online bookstore we might have core business value stories defined as: User browses through books User searches books User purchases a book Each of these “epics” would have a number of child stories which enumerate through the functionality needed to complete that epic. The ‘epic‘ feature in Pivotal Tracker makes this easy to organize and manage. Can I supersize that? During story-carding, discussion may turn to providing the ability to add books to favorites, rate books, review books, etc. We make it clear to clients that we will challenge them on scope that does not directly work towards achieving the core business goals of the application. Given this domain the aforementioned features are the type that we would typically push back on. Depending on how important these stories are to the client we will either defer the writing of these stories to later, or write the story under a separate epic. When we work with the client to prioritize the stories, those stories can be quickly identified and ordered after the launch release if the client so chooses.  We are sure to remind the client that those stories may or may not get done depending on how rapidly we progress through the backlog. It forces the client to come to the realization that they may not have the budget for everything they want, but also introduces them to prioritization. About that finish line We always tell clients that our goal is to get them to a feature complete product as fast as possible, meaning users can perform every action they need on the site to complete the goal of the system. We can then build upon that foundation incrementally in their desired order knowing that we can stop work at any point and they will have a completed application. The last thing we want is for a client’s budget to be consumed and they only have a 75% complete application. Best case scenario, we get to feature completeness well before a client’s expected budget is consumed and then they can make the decision of whether they want to continue development with enhancements to core features or go get user feedback and do the due diligence in truly tailoring the application to their users’ needs. Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-07-16"},
{"website": "Hash-Rocket", "title": "Book Club with Russ Olsen", "author": ["\nAndy Borsz\n\n"], "link": "https://hashrocket.com/blog/posts/book-club-with-russ-olsen", "abstract": "Book Club with Russ Olsen by\nAndy Borsz\n\non\nJuly  1, 2013 This month is all about design patterns as the Rocketeers read Russ Olsen's Design Patterns in Ruby and the Gang of Four's seminal Design Patterns: Elements of Reusable Object-Oriented Software . There will be design pattern posts along the way and the month will conclude with an interview with author and speaker Russ Olsen. Why Design Patterns? Design patterns are proven, reusable techniques to solve common programming challenges. By implementing a design pattern you aren't just hammering the same code out over and over again but applying a solution in a recognizable way that is familiar to other programmers. Understanding design patterns can tremendously ease programming when you recognize that the problem you face can be appropriately solved by applying a pattern. A solid understanding of design patterns is critical if you're a professional programmer. Imagine a future programmer reviewing your code and saying \"Looks like someone applied the proxy pattern poorly here without knowing it.\" Make sure this never happens to you! Join us this month as we explore and review this classic work and it's Ruby update. Which should I read? If you're a newer Rubyist we recommend following along with Russ' Ruby take on design patterns. For advanced Rubyists, we recommend giving the original Gang of Four version a try and using the Ruby version as a companion edition. Whichever you choose you'll be glad you did. Hope you enjoy and happy reading! Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-07-01"},
{"website": "Hash-Rocket", "title": "Phil 'er Up: Better Markup Through Content Generation", "author": ["\nCameron Daigle\n\n"], "link": "https://hashrocket.com/blog/posts/phil-er-up-better-markup-through-content-generation", "abstract": "Ruby Design Phil 'er Up: Better Markup Through Content Generation by\nCameron Daigle\n\non\nDecember 16, 2013 The design team at Hashrocket writes the vast majority of the HAML/SASS on our projects, and we're always looking for ways to better simulate what our designs will look like with real data in place. It's a common occurrence: those repeating blocks of \"Lorem ipsum title here\" don't properly reflect the random, fluid nature of real content. Enter Phil: our shiny new markup generation gem. As you might have noticed from my UI Controller blog post series ( part 1 , part 2 , part 3 ), I'm constantly working to find ways to more easily generate properly simulated data. In the above blog posts, I talked about using Ffaker and some one-off helper methods to assist in that goal. Well, now all that stuff is in a gem, and here's how it works: gem install phil Faker++ Phil started out as a cleaner way to get at some common Faker methods (e.g. Faker::Lorem.words(3) became Phil.words(3) ), but it's more than that: Phil focuses on giving you more varied content by accepting ranges for a ton of methods. So you can say Phil.words(1..100) to get between 1 and 100 words. Convenient? You bet. Designers, you'll sigh with relief (or frustration) the first time Phil reveals that you weren't prepared for a particularly long post to screw up your column alignment. These things happen. I know. Never miss a tag again. Faker spits out words, but it's intended mostly for populating databases – and on the frontend, we're more interested in populating markup. So there are a bunch of methods in Phil that spit out whatever content tags you might want. Here are just a few: .my_cool_content = Phil . paragraphs 1 .. 5 # outputs between 1 and 5 <p> = Phil . blockquote 1 .. 5 # outputs a <blockquote> with 1 to 5 <p> = Phil . ul 1 .. 5 , 10 .. 20 # <ul> with 1 to 5 <li>, 10 to 20 words apiece If you're styling the markup for something that will contain a ton of different tag types, like an article or blog post, don't clutter your code with tons of method calls – just use Phil.markup and pass it a string of the tags that you want. Here's a simple blog post: %article %h1 = Phil . words 1 .. 20 .content = Phil . markup \"h2 p ul p blockquote h5 p h6 p p\" Voila – less time spent copying & pasting filler content means more time to catch layout bugs. Iterate, iterate, iterate. Here's a situation that comes up all the time for us: maybe you're styling a news feed, or a list of followers, or something else where the content of each item might change dramatically. Outputting and debugging the layout of each possible content combination is a pain – and that's where Phil.pick , Phil.sometimes and Phil.loop can help. None of these methods are particularly fancy under the hood – they're mostly just syntax sugar – but they're easy to type and read. So here's a pretty beefy hypothetical example I whipped up for this blog post. This is a list of 1 to 20 comments from the last 30 days. Each comment has an avatar image, some content, and some author data, some of which might or might not be available to the view, and might even have some attachments. Take it away, Phil! - Phil . loop ( 1 .. 20 ) do %article %h1 = Phil . words 1 .. 15 %p .date = \"posted on #{ Phil . date ( 30 ). strftime ( '%A, %B %e, %Y' ) } \" .content = Phil . markup 'h1 p p ul p' .author_info .avatar = Phil . image '200x400' .author = \"posted by #{ Phil . name } \" - Phil . sometimes do .email = Phil . email - Phil . sometimes do .phone = Phil . phone '###-####' - Phil . sometimes do .location = \" #{ Phil . city } , #{ Phil . state } \" %ul .attachments - Phil . loop ( 0 .. 5 ) do %li = link_to \" #{ Phil . words 1 .. 5 } \" , \"#\" Check out the Github page for more examples and complete documentation, and may Phil help you save some keyboard wear and tear in the near future! Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-12-16"},
{"website": "Hash-Rocket", "title": "Module.prepend: a super story", "author": ["\nMicah Woods\n\n"], "link": "https://hashrocket.com/blog/posts/module-prepend-a-super-story", "abstract": "Ruby Module.prepend: a super story by\nMicah Woods\n\non\nApril 28, 2014 I was super excited when I first heard about the prepend feature included in the Ruby 2.0 release, but never had a chance to use it.  Micah Cooper and I were working in an existing code base recently, and one of the project's requirements was to log API requests that took longer than a given threshold. The project talked to multiple APIs via XML, JSON and SOAP. We didn't want to change the existing code, because it had been tested and was working in production. Our approach was to make the smallest change possible to the code base, so we came up with a simple DSL that would wrap the current methods without making any modifications. Here is the DSL we wanted. class Service include Bubot watch :request , timeout: 1 do # |instance, time, result| # execute this block when the request method # takes more than the timeout (threshold) in # seconds. end def request # make api request and return results end end Reaching into our Ruby toolbox, we immediately thought of the 'around alias' pattern.  If you're not familiar, the around alias pattern requires the developer to define a new method, rename the original method, and then rename the new method to the original method name.  Here is an example of how you would implement the pattern. class Model def save puts \"saving model\" end def save_with_validations puts \"validating model\" save_without_validations end # around alias pattern alias_method :save_without_validations , :save alias_method :save , :save_with_validations end __END__\n  > m = Model.new\n  => #<Model:0x007fa3a45e5d90>\n  > m.save\n  validating model\n  saving model\n  => nil At one point, Rails heavily depended on this pattern – in fact ActiveSupport.alias_method_chain does this exact thing.  If you'd like more info, check out the doc's here alias_method_chain .  However, the alias_method_chain method is no longer used in the Rails code base. (In fact, it was deprecated, removed, and then added back.) Furthermore, the around alias pattern is generally frowned upon – and it should be avoided, because you are redefining the original method and can't call super anymore. Before using this the around alias pattern, a developer should really ask themselves if inheritance or (better yet) composition could be used.  Here's an example with inheritance. module Validations def save puts \"validating model\" end end class Model include Validations def save super puts \"saving model\" end end __END__\n  > m = Model.new\n  => #<Model:0x007fa3a45df468>\n  > m.save\n  validating model\n  saving model\n  => nil Here's an example using composition & dependency injection (this is preferred over the previous example): class Validator < SimpleDelegator def save puts \"validating model\" __getobj__ . save end end class Model def save puts \"saving model\" end end __END__\n  > m = Validator.new(Model.new)\n  => #<Validator:0x007fa3a45dcbc8>\n  > m.save\n  validating model\n  saving model\n  => nil Given that we did not want to change the existing code base, we implemented the around alias pattern with what we think is a useful and elegant DSL. Here's the first pass (naive approach) at creating this DSL. module Bubot # included is a hook ruby calls when a module is included def self . included ( base_klass ) # the class that is being included to is being passed in # and we are extending that class with class level methods base_klass . extend ( ClassMethods ) end module ClassMethods # this is the method we want on our DSL def watch ( method_name , timeout: nil , & bubot_block ) define_method ( \" #{ method_name } _with_bubot\" . to_sym ) do |* args , & block | start_time = Time . now method_return_value = save_without_bubot ( * args , & block ) if ( total_time = Time . now - start_time ) >= timeout bubot_block . call ( self , total_time , method_return_value ) end method_return_value end alias_method \" #{ method_name } _without_bubuot\" . to_sym , \" #{ method_name } \" . to_sym alias_method \" #{ method_name } \" . to_sym , \" #{ method_name } _with_bubot\" . to_sym end end Problems with this approach and the DSL we wanted Generally alias_method_chain is considered a nasty approach, and I agree.  Most ruby dev's would reach for composition or inheritance.  However, we wanted this simple DSL, and we did not want to go through every service in the application and modify existing code.  We wanted the code that was already in production – the code that was tested and used – to remain unchanged (as far as our commits were concerned).  For this particular problem, because we were redefining the method at runtime, around alias seemed like the most elegant solution. There are problems however, in order for the around alias to work the method has to be already defined.  All calls to Bubot.watch would have to be called after the method definition. class Service include Bubot def response # make api request and return results end watch :response , timeout: 1 do # do stuff end end And although this is OK, we'd really prefer calls to .watch to be at the top of the file so they can be easily identified. So now we have to make the code a little uglier: module ClassMethods # this is the method we want on our DSL def watch ( method_name , timeout: nil , & bubot_block ) define_method ( \" #{ method_name } _with_bubot\" . to_sym ) do |* args , & block | start_time = Time . now method_return_value = save_without_bubot ( * args , & block ) if ( total_time = Time . now - start_time ) >= timeout bubot_block . call ( self , total_time , method_return_value ) end method_return_value end alias_method_chain_or_register ( method_name ) end private # if we already defined the method, alias_method_chain # otherwise add it to a list that we can evaluate later def alias_method_chain_or_register ( method_name ) if method_defined? ( method_name ) alias_method_chain ( method_name ) else ( @method_names ||= []). push ( method_name ) end end def alias_method_chain ( method_name ) alias_method \" #{ method_name } _without_bubot\" . to_sym , method_name alias_method method_name , \" #{ method_name } _with_bubot\" . to_sym end # method_added is a ruby hook that is called anytime a method is defined def method_added ( method_name ) if ( @method_names ||= []). delete ( method_name ) alias_method_chain ( method_name ) end end end More problems And now we have a working version of bubot, except for when the method being watched makes a call to super . The request method is now called request_without_bubot , and that method doesn't exist on the parent.  Luckily our codebase didn't need to call super , but the fact that this could break production code in the future made me very uncomfortable. So what to do? Luckily, I was able to spend a day pair programing with Paolo Perrotta (if you haven't read his book Metaprogramming Ruby , stop reading right now and get a copy, you will thank me).  One of the things I wanted to ask him about is how I could get rid of the around alias pattern we were using.  His solution was simple: \"Use prepend\", he said. Prepend is a newer feature to ruby and is only available in ruby >= 2.  Here is an example of how it works. Imagine we have the following code. module A def hello super if defined? ( super ) puts \"hello from A\" end end module B def hello super if defined? ( super ) puts \"hello from B\" end end class C include A prepend B def hello super if defined? ( super ) puts \"hello from C\" end end __END__\n  > c = C.new\n  => #<C:0x007fa3a45d62c8>\n  > c.hello\n  hello from A\n  hello from C\n  hello from B\n  => nil\n  > c.class.ancestors\n  => [B, C, A, Object, Kernel, BasicObject] So even though we initialized a C, when we send messages to an instance of C, Ruby looks first to B to see which methods to call.  This was a revelation to me about how Ruby works.  Ruby's class ancestors are nothing more than a lookup chain for method calls – this is much different than other languages.  We created a new C, but Ruby is looking to B first, then to C, then to A and then up the rest of the chain. Armed with this knowledge, we could now extend Bubot onto class, which will set Bubot as a parent so .watch can be called anywhere in the child (our objects) code.  But more importantly, we prepended a module after the class so that any message sent to the class will look at the prepended module first.  It's that simple: we create a method with the same name and call super . module Bubot def self . included ( base_klass ) base_klass . extend ( ClassMethods ) interceptor = const_set ( \" #{ base_klass . name } Interceptor\" , Module . new ) base_klass . prepend interceptor end module ClassMethods def watch ( method_name , timeout: nil , & bubot_block ) interceptor = const_get \" #{ self . name } Interceptor\" interceptor . class_eval do define_method ( method_name ) do |* args , & block | start_time = Time . now method_return_value = super ( * args , & block ) if ( total_time = Time . now - start_time ) >= timeout bubot_block . call ( self , total_time , method_return_value ) end method_return_value end end end end end And if we look at our original desired DSL, here is what the ancestors look like. class Service include Bubot watch :response , timeout: 1 do # execute this block when the response method # takes more than the timeout (threshold) in # seconds. end def response # make api request and return results end end __END__\n  > s = Service.new\n  #<Service:0x007fa3a45cf9a0>\n  > s.class.ancestors\n  => [ServiceInterceptor, Service, Bubot, Object, Kernel, BasicObject] Now we have a working DSL, we can still make calls to super from our original method and we don't have to worry about when the call to watch happens.  If you'd like to see the full source or contribute, the repo is here bubot . Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-04-28"},
{"website": "Hash-Rocket", "title": "An Ancient Recap", "author": ["\nJonathan Jacks"], "link": "https://hashrocket.com/blog/posts/an-ancient-recap", "abstract": "Community An Ancient Recap by\nJonathan Jackson\n\non\nApril 14, 2014 I went to Ancient City Ruby this past week, and had an amazing time.  There was a great turnout, and it was a lot of fun meeting everyone.  I'd like to take a few minutes to summarize the talks, and get you excited for their imminent release online.  So with that, let's dig in. Leon's Allegory of the Cave Leon Gersing got things started with his allegory of the cave.  He talked on, power distributions in organizations, and the training of young people to conform to a specified mentality.  Drawing connections from Plato's allegory of the cave , Leon explained to us that the reality that we are taught is not necessarily real.  That we have a responsibility to challenge these conventions everyday in order to progress.  And, like in the allegory of the cave, once we've seen something more we cannot return to the cave.  It was a really fun talk, that piqued my interest. Breaking Up (With) Your Test Suite This talk was great. Justin Searls explained an approach to organizing your suite into a few well defined test types in order to more immediately expose the meaning of the test.  Generally, there are two types of tests.  Confidence tests, which ensure that the code does what you want.  And Understanding tests, which are designed to drive design and ensure usability.  Luckily you don't have to rely on me to explain this talk, because Justin wrote a great blog about the process by which he created this talk.  His blog post also has a link to the slide deck, which is awesome btw.  Check it out here .  Thanks @searls. Let's Write Some Wierd Ruby David Copeland decided to challenge our assumptions in his talk.  First, he theorized a world that was bereft of the billion dollar mistake (the creation of nil) and wrote interesting Ruby without the use of nil.  Then he continued in a world without if and else .  It was fun to see the clever use of Ruby to solve these problems. Overkill In this talk Katrina Owen drew upon her experience with exercism.io's Bob example.  Having reviewed many of these examples from Rubyists of varying skill levels, she was able to show the transformation over time.  Moving from a naive implementation to a robust solution. The talk showed how beginner programmers will use a chainsaw (overkill) to solve their problems, and as you advance you'll begin reaching for more subtle tools.  Also, it was great to hear her explain how much trouble she takes naming things.  During Q&A she talked about how she struggled to name the beer for a \"99 bottles of beer on the wall\" program for six months, eventually going with inventory Be sure to check out exercism.io . Testing the Untestable Richard Schneeman talked about his experience testing the untestable.  Which in this case was the Heroku ruby buildpack.  He explained MVP, which in this case means Minimum Viable Patch, which led to a very difficult testing enviroment.  He showcased some black box testing techniques that enabled him to have confidence that things were working as needed.  I especially liked his approach to dealing with network testing issues, which involves retrying flaky network interactions.  I like it primarily because of his rrrrtry gem, which has an awesome name.  In case you were wondering, I think rrrrrtry and rrrtry are still available.  :)  Awesome talk @schneems. Fast, Testable, and SANE APIs Ben Lovell explained how to write sane APIs.  He walked us through everything from ActiveModel::Serializers to JSON API to Rails-API.  During this time he explained that the common themes that all SANE APIs have in common are Fast Standardized Intuitive And he showed us how Haley Duff and Heath Ledger are making our lives easier... Ruby & You Terrence Lee showed us how he got into ruby-core and how we can contribute to Ruby.  He walked us through using SVN to pull down the source and create a working patch for Ruby.  Then he talked about the goals for Ruby.  This included possibly moving Ruby to Git from SVN and updating the information out there to enable contributions from more people. One of the main takeaways (for me) was that when you submit a patch you should use bullet points and code snippets to make your case because english is not everyones first language (and code is universal).  Also, Friday hug: Postgres Performance For Humans Craig Kerstiens Gave an awesome run down of Postgres.  Scans, Cache, indexes, and hstore were just a few of the features he went over.  Craig took an advanced look at all of these and more. One of the major takeaways (for me anyways) here is that when 9.4 comes around we'll be able to use JSONB to store json.  This will make JSON more performant (queryable and indexable).  Pretty neat! Introduction to Elixir for Rubyists Josh Adams gave an introduction the Elixir.  Elixir is a functional programming language created by José Valim built on top of the EVM.   The functional paradigm is becoming more and more common.  And Elixir is a wonderful functional language that will feel familiar to most Rubyists as it shares quite a bit of syntax.  I reccomend checking out Elixir Sips if you want to see a bit more of the language. Oh, Oh, Oh, It's Magic In this talk Aaron Patterson explained his Magic, The Gathering interest.  He has a large collection that he wanted to categorize.  In order to achieve his goal he built a system that would take a photo of a card, normalize it, then compare it to a corpus of cards for likeness.  As he scanned more cards the corpus grows and the guesses get better.  This talk really got me thinking about applications of OpenCV (computer vision software).  Not only was this an interesting topic area, but it was also a truly entertaining talk.  You can see the code that he used for his project here .  Thanks Aaron! Hack Me If You Can Konstantin Haase talked about internet security.  Given recent security issues, this seems almost a prescient choice for a talk.  We learned about several different attack vectors that black hats will use if you are unaware.  From CSRF to Compression attacks we heard it all.  Each of these attack vectors were intricate and subtle.  I think my main take away was how difficult it is for framework maintainers to stay ahead of the curve security wise. Juggling children ... and Rubies In this talk Evan Machnic relayed his experience being both developer and parent.  Specifically, his journey to learning how to work without negatively impacting his parenting (and vice versa).  Part of this is to simply separate the two.  Don't bring work home, and try to remove distraction while at work. Conclusions Ancient City Ruby is one of my favorite conferences for a number of reasons (not the least of which is that I work at Hashrocket), but this year was particularly amazing.  I was able to meet many of the speakers, who were all very kind and welcoming as well as insightful.  And the talks have definitely stoked my creative fires. Thanks to Marian Phelan and everyone on the ACR team at Hashrocket - and thanks to all the speakers for taking the time to give such wonderful talks. Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-04-14"},
{"website": "Hash-Rocket", "title": "Running Out Of IDs", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/running-out-of-ids", "abstract": "PostgreSQL Running Out Of IDs by\nJosh Branchaud\n\non\nApril  6, 2017 People have fingerprints. Products have bar codes. Cars have vehicle\nidentification numbers (VINs). Internet connected devices have IP\naddresses. Anything that needs to be uniquely identifiable tends to have a\nmechanism for uniquely identifying it. It makes sense. This is particularly\nimportant for the data that we store as part of the applications we build.\nEvery user, every product, every financial transaction should all have\nsome kind of identifier that we can use to uniquely identify it. The limits of these identifiers is the subject of this post. Though we can utilize fancier schemes like UUIDs and composite keys, the\nmost common approach to uniquely identifying records is an id -- an\ninteger that we guarantee is unique. But where do these unique integer id s\ncome from? A Sequence In PostgreSQL, we generally create an id column as a primary key using serial . create table products ( id serial primary key , ... ); The serial numeric data type is what we call an auto-incrementing integer.\nWithout going into all of the details of what serial does, I will point\nout that upon creating our table, we also create a sequence -- products_id_seq . A sequence is a special kind of table with a single-row that we use to keep\ntrack of what unique integer we should assign next when adding a record to a\ntable. Let's take a look at the description of our new products_id_seq . > \\ d products_id_seq Sequence \"public.products_id_seq\" Column | Type | Value ---------------+---------+--------------------- sequence_name | name | products_id_seq last_value | bigint | 1 start_value | bigint | 1 increment_by | bigint | 1 max_value | bigint | 9223372036854775807 min_value | bigint | 1 cache_value | bigint | 1 log_cnt | bigint | 0 is_cycled | boolean | f is_called | boolean | f Owned by : public . products . id The sequence tells us a bunch of things. The parts that are interesting to\nus right now are last_value and increment_by . But before we can make any\nsense of those two values, we need to check out the nextval function. Why is it that we care about the nextval function? Well, it is being used\nto generate the default value for our id column. > \\ d products Table \"public.products\" Column | Type | Modifiers --------+---------+------------------------------------------------------- id | integer | not null default nextval ( 'products_id_seq' :: regclass ) ... Each time we insert a record into our products table (without specifying\nthe id ), nextval is invoked to determine the next available value in our\nsequence. The name of the sequence to be used is specified as the\nargument. nextval looks at the last_value and increment_by columns for\nthe specified sequence to compute the next value that it should return. When nextval is invoked for the first time, the is_called column is\nstill false , so the start_value (in this case, 1 ) is returned. The is_called column is then toggled to true . Now that is_called is true,\nall subsequent invocations of nextval on that sequence will add the value\nof the increment_by column to the value of the last_value column. The last_value column is updated accordingly and this value is returned to\nwhoever called it -- in our case, it is used as the default value for our id column in products . If we wanted to do something fancy and non-conventional with our sequence,\nwe could alter parts of it. For example, if we only wanted to assign even\nnumbered id values, we could set the start_value to be 2 and change\nthe increment_by value to be 2 . In this case, each invocation of nextval would yield an even number, monotonically increasing 2 at a time. We could even setup a sequence that starts at 1,000,000 and counts\nbackward by 1 until it reaches -1,000,000 . I can't imagine why, but it\nis nice to know we have the flexibility. Generally speaking, we can utilize our sequence and the default value of our id column as is. Exceeding A Sequence Another column that appears in the description of our sequence is max_value . A little curiosity may lead us to wonder, \"What happens when we\nreach that max_value ?\" Let's find out. Counting to and exceeding 9223372036854775807 (a number I don't know how\nto pronounce) is going to take longer than you or I have time for. To make\nthis experiment a bit more manageable, let's alter our sequence a bit. > alter sequence products_id_seq maxvalue 3 ; ALTER SEQUENCE The max_value of our sequence is now 3 . Let's do a couple inserts. > insert into products default values ; INSERT 0 1 > insert into products default values ; INSERT 0 1 > insert into products default values ; INSERT 0 1 Three insertions means we've reached our max_value of 3 , so what happens\nif we do one more insert ? > insert into products default values ; ERROR : nextval : reached maximum value of sequence \"products_id_seq\" ( 3 ) Reaching the \"maximum value\" of our sequence is a real problem in a\nproduction system. This means we will no longer be able to insert records\ninto the relevant table until we've dealt with the issue. How do we remedy this situation? Well, that is a topic for another blog\npost. A better question for now is, how likely or practical is it that we do exceed\nour sequence? 9 Quintillion I've since looked up the pronunciation. A bigint can support counting up\nto about 9.2 quintillion. That is a lot of records. So, you are concerned about reaching 9.2 quintillion unique IDs for the\nrecords in your database? That is kind of like being concerned about\nreaching $9.2 quintillion in your bank account. Generally speaking, this is\nwhat you might call \"a nice problem to have.\" In other words, running a\nbusiness that is pushing the bounds of your software, hardware, etc.\nhopefully means business is good. But what does this look like in practical terms? Let's say business is so good that we are inserting 10,000 records per\nminute into our table. So, how long would it take to max out our sequence?\nLet's compute it in years. > select 9200000000000000000 / ( 10000 * 365 * 24 * 60 :: bigint ) as years ; years ------------ 1750380517 That's a lot of years. This means that the size of your storage, the write speed of your hardware,\nand the lifespan of both your business and the human race are all\nconsiderations before exceeding the sequence. There is one minor issue to consider though. When you generate a sequence as we did at the beginning of this post, using serial , a bigint (64 bit) is used by the sequence, but the id column\nonly utilizes integer (32 bit). Let's do the same math, but compute it in\nweeks. > select 2100000000 / ( 10000 * 7 * 24 * 60 :: bigint ) as weeks ; weeks ------- 20 Twenty weeks is a much different story. So, when creating tables using serial , we may not be in danger of\nexceeding our sequence, but if we have serious amounts of incoming data, we\nmay max out our id column. Closing Thoughts Running out of IDs is probably not a concern in most web applications, but\nit is important to know the limitations of the software on which you depend.\nKeep on using serial for most use cases and keep bigserial in your back\npocket if a real need arises. Also, don't forget that the UUID type is a solid option for primary\nkeys ,\neven at scale. Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-04-06"},
{"website": "Hash-Rocket", "title": "Ruby5 Roundup - Episode 391", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/ruby5-roundup-episode-391", "abstract": "Ruby Ruby5 Roundup - Episode 391 by\nPaul Elliott\n\non\nAugust  1, 2013 Matthew Conway and I teamed up again for an episode of the Ruby5 podcast. Here is a quick roundup of this week's episode. http://ruby5.envylabs.com/episodes/427-episode-391-august-2nd-2013 VimGenius http://vimgenius.com VimGenius is a new service from Rocketeer Vic Ramon that will help you learn vim. It has different courses depending on your skill level so you are sure to learn some new tricks no matter how well versed in vim you are. No sign up necessary so go check it out today! When Screenshots Change http://cl.ly/QaNM Sometimes unit and integration tests just aren't enough. They can catch a lot, but what about breaking changes in stylesheets or markup? They page may \"work\", but it may not look right to users. Well a few new options are available for testing with screenshots, such as the huxley gem from Facebook and the wraith gem from the BBC. PSD.rb http://cosmos.layervault.com/psdrb.html Ever needed to crack open a PSD in Ruby? I'll bet you wish you hadn't. Well now you can easily get at the data inside with a nice DSL thanks to the folks at LayerVault. They released the PSD.rb gem to facilitate working with PSD files programmatically and it couldn't get any easier than this. NSA Panel https://github.com/goshakkk/nsa_panel If you're like us, you wish you could do more to support PRISM and the NSA's effort to invade the privacy of trouble-making Americans. Well thanks to the efforts of the Ruby community you can drop this gem into your app, mount the engine in your routes file, and provide an easy way for the NSA to keep your users in check. No more need to bother with answering pesky request warrants. The NSA can now be completely self-serving! Maildown https://github.com/schneems/maildown Maildown is a new gem from @schneems that lets you write emails in markdown. It automatically generates both html and plain text emails so you don't have to duplicate anything in your codebase. exercism.io http://exercism.io Exercism.io is a new service from Katrina Owen that provides you with exercises in javascript, ruby, elixir, and clojure to help you learn or just getter better with those languages. Your solutions are reviewed by others who have completed them, creating a community of feedback. Sign up today and become a better programmer with your peers. So that's it for this episode of Ruby5. If you haven't already, subscribe to the podcast and keep yourself up to date. Thanks for listening! Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-08-01"},
{"website": "Hash-Rocket", "title": "Pairing in Practice", "author": ["\nChris Erin\n\n"], "link": "https://hashrocket.com/blog/posts/pairing-in-practice", "abstract": "Process Pairing in Practice by\nChris Erin\n\non\nDecember 13, 2016 Two Keyboards, Two Mice, One Monitor.  This is a little bit of what pairing looks like in practice, constant discussion, trials, rollbacks, re-thinks all looking for a best solution.  An all-day coding marathon every day that reduces unwelcome pauses in momentum. Agreements Agreements are the lifeblood of pairing.  What to work on, how to work it, what to name this, how to solve that, a pair has to agree on everything.  If a developer is making decisions without explaining those decisions, without asking for input and without expecting input, then it's not pairing, its soloing.  Agreements are everything. Not all agreements are easy!  The time it takes to reach an agreement corresponds with the difficulty of the problem you are trying to solve.  But that is where pairing starts to shine.  With even moderately difficult problems, pairs discuss the different trade-offs in a joint search for the most elegant and simplest solution. Its a discussion of how things work that might necessitate some research, or a different perspective from a co-worker.  If you can agree on how things work (not as easy as it sounds) that creates a platform for solving the problem. Testing Provides Focus Pairing works well when both developers are thinking about the same narrow problem set at the same time.  The way Hashrocket collects and writes requirements leads directly into writing an automated test that describes a very particular user experience.  We focus on one user experience at a time, and writing the automated test for that user experience leads us into writing the code necessary to realize that user experience.  This is test first development. Test first development always starts red.  It always starts with a failure, and that's always the first problem you need to solve.  And then there's another failure, and another failure and another.  Each test provides a goal and each failure is the next step towards reaching that goal.  This is incredibly important in pairing as it provides the focus for the pair to think about the same problem at the same time.  Without it, each developer will naturally start thinking about different aspects of the system and different use cases.  When we lose focus, we lose momentum.  Test first development helps guide a pair from the first step to the last and when you're done you have a green test, code to deliver and a victory to celebrate. Pairing is Intense and Exhausting It's amazing what kind of energy pairing brings out of both developers.  For a pair the day starts with a discussion of how to proceed.  And once coding starts, it doesn't stop.  It's very rare for a pair to not know what to do next or to become blocked.  If I don't have the next line of code at my fingertips, my pair does, and vice versa.  Maybe because it's easier to think about the next step when you're not typing, or its easy to think about the implications of  what's being written as its being written.  And when as a pair you reach a point where you don't know what to do next, well, that starts a new discussion! When I first started, this constant discussion about what to do and how to do it wore me out!  I would come home exhausted every single day in a way I hadn't experienced since I worked in a greenhouse picking lettuce as a teenager.  Over time I got used to it, developers can build up a tolerance for communication and structured decision making. Multi-Tasking is Concurrent Having one set of hands on the keyboard at all times frees the other set of hands to take care of other tasks without stopping momentum.  Throughout the day we communicate with stakeholders, ask other developers questions and check-in with our project manager, all of which can occur while still developing.  Searching for technical answers on the internet is another thing that can be done concurrently.  I can look through the Rails docs to determine what obscure arguments of obscure methods do while my pair focuses on the programming task at hand. Being able to do two things at once has obvious time saving benefits, but the greatest benefit is maintaining context on a particular problem.  Tough problem sets involve many different variables and loading those into your head is part of solving the problem.  When pairing, there is less risk of losing the problem set (a phenomenon called context switching) when engaging with a different problem.  This enables us to solve problems faster and be responsive communicators which helps our team members do their own jobs better.  Pair multi-tasking enables to maintain focus as a whole. In Conclusion This is what its like to pair program on a regular basis, I hope it provides some insight into how pairing works at Hashrocket. Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-12-13"},
{"website": "Hash-Rocket", "title": "Ruby5 Roundup - Episode 426", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/ruby5-roundup-episode-426", "abstract": "Ruby5 Roundup - Episode 426 by\nPaul Elliott\n\non\nDecember 13, 2013 This Friday the 13th, Matthew Conway and I team up again to bring you the latest news in the Ruby and Rails communities. Here is a quick roundup of what's this episode of the Ruby5 podcast. http://ruby5.envylabs.com/episodes/462-episode-426-december-13th-2013 thoughtbot on Stack Overflow http://robots.thoughtbot.com/moving-open-source-project-mailing-lists-to-stack-overflow The fine folks at thoughtbot wrote an article discussing why they are moving their open source project communication to Stack Overflow. I think it is a great idea and will be joining them on this one. The article is well reasoned and worth reading for any open source maintainers still using mailing lists. New in RSpec 3: Verifying Doubles http://rhnh.net/2013/12/10/new-in-rspec-3-verifying-doubles RSpec will be releasing version 3 very soon and it comes with a lot of cool features. One of which is the ability to verify doubles against the underlying object, which will go a long way towards making our test suites less brittle when we use mocks. Stagehand http://camerond.github.io/stagehand The stagehand project from Rocketeer Cameron Daigle allows you to easily set up multiple states and switch between them in your static page mockups. This is a great tool to allow designers to communicate intent without having to write a bunch of throw-away javascript. Bundler Not Dead Yet http://andre.arko.net/2013/12/07/the-rumors-of-bundlers-death-have-been-greatly-exaggerated So you've heard that bundler and rubygems will be merging soon, but the reality of that situation is not all it's been rumored to be. Andre Arko from the bundler core team talks about the rumors and what's actually going on. roar https://github.com/apotonick/roar The roar gem just got a new README. I like to highlight changes like this because the README is well done and that is something that gem authors often don't take seriously. Having a well-written README is extremely important for adoption of your gem as most people won't actually look at the code to see how it works. Whether you use roar or not, check out the README and take some pointers. So that's it for this episode of Ruby5. If you haven't already, subscribe to the podcast and keep yourself up to date. Thanks for listening! Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-12-13"},
{"website": "Hash-Rocket", "title": "Top Tweets of 2016 with Elixir and Ecto", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/top-tweets-of-2016-with-elixir-and-ecto", "abstract": "Elixir Top Tweets of 2016 with Elixir and Ecto by\nJosh Branchaud\n\non\nJanuary 10, 2017 As you look ahead to 2017, you may find yourself looking back on 2016. What\nare the goals that you achieved? What are the books that you read? If you're an avid Twitter user you may also be wonder what your most popular\ntweets of the year were. With the help of Elixir and Ecto , we can figure that out. Roadmap Determining the most favorited and retweeted tweets of the year is going to\nrequire some investigative work. Here is a roadmap of what we will need to\ndo. Create a database schema for representing and storing tweets in\nPostgreSQL 1 . Import Twitter statuses for a user via the Twitter\nAPI using something like ExTwitter . Insert all the imported tweets into the database. Ask our questions of the Twitter status data using Ecto's schemaless\nqueries. The focus of this blog post is on Ecto, so we will only walk through the\ndetails of steps 1, 3, and 4. For the sake of completeness, the code for\nimporting Twitter statuses with ExTwitter has been posted to github . Step 1: A Schema For Tweets A database schema is often times a middle ground. We may have one or more\nsources of data on the one end. On the other end we have some questions we'd\nlike to be able to ask of the data. We need to create a schema that puts us\nreasonably between those two ends. If we look at the documentation for GET\nstatuses/user_timeline we can see an example response. There is way more data there than we are\nprobably going to need. Let's narrow it down. We will need favorite_count and retweet_count of each tweet to answer\nour original questions. By including retweet_status we can also\ndifferentiate between retweets and tweets authored by the given user. To\nensure that a given tweet is from 2016, we'll want created_at . Lastly, to\nidentify each tweet, we will want the id , screen_name of the user , and\nof course the text of the tweet. Here is a first look at an Ecto migration that meets our needs: defmodule TopTweets . Repo . Migrations . CreateTweetsTable do use Ecto . Migration def change do create table ( :tweets ) do add :tweet_id , :bigint , null: false add :author , :varchar , null: false add :text_content , :varchar , null: false add :tweeted_at , :timestamptz , null: false add :retweeted_count , :integer , null: false add :favorited_count , :integer , null: false add :retweeted , :boolean , null: false , default: false end end end There are a couple details of note. Tweets have very, very large id s. We cannot just use the integer data\ntype for tweet_id . We need something that can handle a much larger range\nof values, so we use bigint .\nAnything that is a valid data type in our backing data store can be\nspecified in an Ecto migration as the column's data type. I likely could have used tweet_id as the primary key for this table. I\ninstead chose to let Ecto give me an integer primary key for free. This is\nsufficient for the purposes of this blog post. We expect to always have values for all of our columns when inserting a\nrecord. Using null: false on all columns ensures that we never leave gaps\nin our data when inserting records. If something is missing, our database\nwill refuse to insert the record and complain. I don't like throwing away timezone information, so I always choose timestamptz over timestamp . I did just that with the tweeted_at column above. That sums up the interesting parts of the migration so far. We have a big\nissue though. With this table description, there is nothing to stop us from\ninserting duplicate tweet records. Duplicate data is detrimental to data\nintegrity. The addition of an index will go a long way in preventing\nduplicate tweets from getting into our database 2 . Let's add this index to our migration. create unique_index ( :tweets , [ :tweet_id ]) Lastly, we are working with a rate-limited API. In the event that we need\nadditional data about a tweet later on, it would be handy to have the entire\nbody of the API response in the database. Let's add a jsonb column. Here is the final iteration of our migration: defmodule TopTweets . Repo . Migrations . CreateTweetsTable do use Ecto . Migration def change do create table ( :tweets ) do add :tweet_id , :bigint , null: false add :author , :varchar , null: false add :text_content , :varchar , null: false add :tweeted_at , :timestamptz , null: false add :retweeted_count , :integer , null: false add :favorited_count , :integer , null: false add :retweeted , :boolean , null: false , default: false add :data , :jsonb , null: false , default: \" {}\" end create unique_index ( :tweets , [ :tweet_id ]) end end To make sure everything is on the up and up, we will want to run the\nmigration. I prefer to use mix\necto.twiki for this. Step 2: Import Twitter Statuses The details of importing Twitter statuses are out of the scope of this blog\npost. You can check out the relevant code on github . To follow along with the rest of this post, you may want to import some\nsample data. We are going to use @elixirlang 's tweets 3 . You can get a\nsnapshot of the data either as an elixir data\nstructure or as a SQL\ndump . Step 3: Insert All Those Tweets At this point, we have loaded a bunch of tweet data into memory. We figured\nout the structure of our tweets table in step 1. Now we just need to\ninsert the data. A naive approach would be to iterate over the thousands of tweets inserting\neach one with a separate insert statement. That's not ideal and with Ecto\n2.0 we can do better. Ecto 2.0 introduced much stronger support for schemaless\nqueries including the addition of the insert_all function .\nThat's what we are going to use. The beginning of the documentation for insert_all/2 reads as follows. Inserts all entries into the repository. It expects a schema (MyApp.User) or a source (\"users\") or both ({\"users\",\nMyApp.User}) as the first argument. The second argument is a list of\nentries to be inserted, either as keyword lists or as maps. We aren't working with a schema, so we use \"tweets\" , the name of our\ntable, as the first argument. The second argument is a list of keyword lists\nrepresenting the records we want to insert. We need to do some massaging of\nthe tweet data before we can pass it into insert_all . We'll map over the tweet data to produce the keyword lists. Most of the\npieces of data will map directly, but the tweeted_at timestamp needs help\nand the retweeted_status needs to be converted to a boolean for the retweeted column. The harder part is getting the timestamp right, so let's\nstart there. The Twitter API gives timestamps in a non-standard format. We need to\nconvert it into an Ecto.DateTime struct. Some pattern matching and Timex will help. def cast_timestamp ( timestamp ) do [ dow , mon , day , time , zone , year ] = String . split ( timestamp , \" \" ) rfc1123 = Enum . join ([ \" #{ dow } ,\" , day , mon , year , time , zone ], \" \" ) rfc1123 |> Timex . parse! ( \" {RFC1123}\" ) |> Ecto . DateTime . cast! () end I cover the details of this conversion in this blog\npost . Translating the retweeted_status data into a boolean is a bit more\nstraightforward. tweet_data . retweeted_status != nil The process of translating one instance of tweet data into a keyword list\naligning with our tweets table description can be encapsulated in the\nfollowing function. def bundle_tweet ( tweet_data ) do [ tweet_id: tweet_data . id , author: tweet_data . user . screen_name , text_content: tweet_data . text , tweeted_at: cast_timestamp ( tweet_data . created_at ), retweeted_count: tweet_data . retweet_count , favorited_count: tweet_data . favorite_count , retweeted: tweet_data . retweeted_status != nil , data: tweet_data ] end The list of keys in this keyword list are the names of the columns we\ndefined in step 1. For each key we pull out the associated piece of data\nfrom the tweet_data map. As we discussed earlier, the entire tweet_data map can be inserted directly into our jsonb field so that we have a\nbackup of the data for each tweet. With all of this in place, we can finally batch insert all our tweets. def insert_tweets ( tweets ) do bundled_tweets = Enum . map ( tweets , & bundle_tweet / 1 ) TopTweets . Repo . insert_all ( \" tweets\" , bundled_tweets ) end We have put in a lot of work so far; it's time for some answers. Step 4: Ask Questions With Queries We started off this blog post with two questions in mind. What are our most favorited tweets of 2016? What are our most retweeted tweets of 2016? It will help to be a bit more specific. Let's rework these questions a\nlittle. What are the 10 most favorited tweets authored by @elixirlang in 2016? What are the 10 most retweeted tweets authored by @elixirlang in 2016? To answer these questions, we'll need to construct queries with Ecto's query\nAPI. We will also need a TopTweets.Repo module which is part of the standard setup when getting started with Ecto.\nLet's open up IEx and iterate on a solution. First, $ iex -S mix and now let's start simple. Can we just grab all of our tweets? If we were working with a table that has a corresponding Ecto schema, we\ncould do something like: TopTweets . Repo . all ( TopTweets . Tweet ) We aren't working with Ecto schemas though. We are sticking to schemaless\nqueries here. Since we don't have a module to reference, we could try\nreferencing the name of our table. TopTweets . Repo . all ( \" tweets\" ) That doesn't work, but it is promising. If we take a close look at the\nerror, we'll see what the issue is. ** (Ecto.QueryError) PostgreSQL requires a schema module when using selector\n\"t0\" but none was given. Please specify a schema or specify exactly which\nfields from \"t0\" you desire in query:\n\nfrom t in \"tweets\",\n  select: t When provided with a schema, Ecto knows exactly which fields to request from\nthe database. Without one, we have to specify the fields ourself. We can do\nthis using the query API. The Ecto.Query.from\\2 function is a good place to start. > import Ecto . Query Ecto . Query > from ( t in \" tweets\" ) #Ecto.Query<from t in \"tweets\"> This constructs the from portion of our SQL query. The second argument to from\\2 is a keyword list that defaults to [] . We can further fill out\nour query by specifying what the select clause should look like: > from ( t in \" tweets\" , select: t . tweet_id ) #Ecto.Query<from t in \"tweets\", select: t.tweet_id> Piping what we have so far to TopTweets.Repo.all will give us a result\nwith all of our tweets -- or at least all the tweet_id s for our tweets 4 . from ( t in \" tweets\" , select: t . tweet_id ) |> TopTweets . Repo . all 15 : 34 : 52.816 [ debug ] QUERY OK source = \" tweets\" db = 16. 4 ms decode = 2. 7 ms SELECT t0 . \" tweet_id\" FROM \" tweets\" AS t0 [] [ 815582938918223872 , 815549333236027392 , 815549228818792448 , 814143389759340544 , 813698248888217600 , 813484243498270720 , 813172504730750976 , 812589506394279936 , 811867334860218368 , 811318252148232193 , 811234616178831360 , 811218704944001025 , ... ] This is great output. Not only does it give us a list of tweet_id s, but it\nprovides timing information and the SQL query that was run against our\ndatabase. This also gives us a query to iterate on. A natural next step is to limit the results to 10 items. The limit clause\nis what we need. from ( t in \" tweets\" , select: t . tweet_id , limit: 10 ) |> TopTweets . Repo . all 17 : 29 : 15.103 [ debug ] QUERY OK source = \" tweets\" db = 9. 4 ms SELECT t0 . \" tweet_id\" FROM \" tweets\" AS t0 LIMIT 10 [] [ 815582938918223872 , 815549333236027392 , 815549228818792448 , 814143389759340544 , 813698248888217600 , 813484243498270720 , 813172504730750976 , 812589506394279936 , 811867334860218368 , 811318252148232193 ] That has trimmed back the results a bit. Adding a where clause is another way to trim down and shape our result\nset. We'll want to start with a where clause that ensures only tweets from\n@elixirlang are in our result set. This is especially important if we have\nimported tweets from other users already. from ( t in \" tweets\" , select: t . tweet_id , limit: 10 , where: [ author: \" elixirlang\" ] ) |> TopTweets . Repo . all 17 : 46 : 10.144 [ debug ] QUERY OK source = \" tweets\" db = 12. 7 ms SELECT t0 . \" tweet_id\" FROM \" tweets\" AS t0 WHERE ( t0 . \" author\" = 'elixirlang' ) LIMIT 10 [] [ 815582938918223872 , 815549333236027392 , 815549228818792448 , 814143389759340544 , 813698248888217600 , 813484243498270720 , 813172504730750976 , 812589506394279936 , 811867334860218368 , 811318252148232193 ] The resulting query now includes the first part of our where clause. We\nneed to do more than limit the result set by author. We also want to ensure\nthe tweets are only from the year of 2016. Let's add constraints on the tweeted_at column to the where clause. from ( t in \" tweets\" , select: t . tweet_id , limit: 10 , where: [ author: \" elixirlang\" ], where: t . tweeted_at > type ( ^ \" 2016-01-01T00:00:00Z\" , Ecto . DateTime ) and t . tweeted_at < type ( ^ \" 2017-01-01T00:00:00Z\" , Ecto . DateTime ) ) |> TopTweets . Repo . all 09 : 53 : 36.972 [ debug ] QUERY OK source = \" tweets\" db = 12. 7 ms SELECT t0 . \" tweet_id\" FROM \" tweets\" AS t0 WHERE ( t0 . \" author\" = 'elixirlang' ) AND (( t0 . \" tweeted_at\" > $ 1 :: timestamp ) AND ( t0 . \" tweeted_at\" < $ 2 :: timestamp )) LIMIT 10 [{{ 2016 , 1 , 1 }, { 0 , 0 , 0 , 0 }}, {{ 2017 , 1 , 1 }, { 0 , 0 , 0 , 0 }}] [ 814143389759340544 , 813698248888217600 , 813484243498270720 , 813172504730750976 , 812589506394279936 , 811867334860218368 , 811318252148232193 , 811234616178831360 , 811218704944001025 , 810909398143029248 ] Constructing SQL queries can sometimes require you to bend your mind a bit.\nIt can be particularly challenging to construct an entire query in one take.\nBy taking this kind of iterative approach, we have a working query every\nstep of the way that gets us closer and closer to the solution we want. But\nas we go, we only have to think about any one part of it at a time. Our query looks to be almost done, but we don't have much context about our\nresult set yet. Let's update the select clause of our query to give us\nmore information. from ( t in \" tweets\" , select: { t . tweet_id , t . favorited_count , t . text_content }, limit: 10 , where: [ author: \" elixirlang\" ], where: t . tweeted_at > type ( ^ \" 2016-01-01T00:00:00Z\" , Ecto . DateTime ) and t . tweeted_at < type ( ^ \" 2017-01-01T00:00:00Z\" , Ecto . DateTime ) ) |> TopTweets . Repo . all 10 : 17 : 18.106 [ debug ] QUERY OK source = \" tweets\" db = 5. 5 ms SELECT t0 . \" tweet_id\" , t0 . \" favorited_count\" , t0 . \" text_content\" FROM \" tweets\" AS t0 WHERE ( t0 . \" author\" = 'elixirlang' ) AND (( t0 . \" tweeted_at\" > $ 1 :: timestamp ) AND ( t0 . \" tweeted_at\" < $ 2 :: timestamp )) LIMIT 10 [{{ 2016 , 1 , 1 }, { 0 , 0 , 0 , 0 }}, {{ 2017 , 1 , 1 }, { 0 , 0 , 0 , 0 }}] [{ 814143389759340544 , 0 , \" RT @ElixirConfEU: Looking forward to ElixirConf.EU 2017 - Very Early Bird starts 9 Jan # myelixirstatus https://t.co/3NjJhTpsjN\" }, { 813698248888217600 , 0 , \" RT @whatyouhide: Finally had time to send in a CFP for @ElixirConfEU in Barcelona, fingers crossed! # myelixirstatus\" }, { 813484243498270720 , 0 , \" RT @sasajuric: Great opportunity to grab yourself some Elixir in Action :-) https://t.co/GSKRnM76Qr\" }, ... ] Awesome. We can now see the favorite_count and content of each tweet in\nour result set. These don't appear to be our most popular tweets though.\nNone of them were favorited. We need to add an order_by clause: from ( t in \" tweets\" , select: { t . tweet_id , t . favorited_count , t . text_content }, limit: 10 , where: [ author: \" elixirlang\" ], where: t . tweeted_at > type ( ^ \" 2016-01-01T00:00:00Z\" , Ecto . DateTime ) and t . tweeted_at < type ( ^ \" 2017-01-01T00:00:00Z\" , Ecto . DateTime ), order_by: [ desc: :favorited_count ] ) |> TopTweets . Repo . all 10 : 37 : 49.487 [ debug ] QUERY OK source = \" tweets\" db = 47. 0 ms queue = 0. 1 ms SELECT t0 . \" tweet_id\" , t0 . \" favorited_count\" , t0 . \" text_content\" FROM \" tweets\" AS t0 WHERE ( t0 . \" author\" = 'elixirlang' ) AND (( t0 . \" tweeted_at\" > $ 1 :: timestamp ) AND ( t0 . \" tweeted_at\" < $ 2 :: timestamp )) ORDER BY t0 . \" favorited_count\" DESC LIMIT 10 [{{ 2016 , 1 , 1 }, { 0 , 0 , 0 , 0 }}, {{ 2017 , 1 , 1 }, { 0 , 0 , 0 , 0 }}] [{ 682890953212956672 , 226 , \" Elixir v1.2.0 is out - https://t.co/BuULFDtrsm! New guides and official blog post coming out soon. Happy New Year!\" }, { 745240093275873281 , 167 , \" Elixir v1.3.0 has been released and welcome @whatyouhide into Elixir's team: https://t.co/aw0aopnXxn\" }, { 753171413821976577 , 163 , \" Plug v1.2.0-rc.0 is out! Featuring safer encryption algorithms, better MIME support and a brand new debugging page! https://t.co/XuepyvTSQA\" }, ... ] That's more like it. By ordering the tweets by favorited_count in\ndescending order, we pull the most favorited tweets to the top. Lastly, our questions were specific when it came to pointing out that the\ntweets we are looking for ought to have been authored by @elixirlang. In\nother words, we don't want to include any popular retweeted tweets. We can\nmake an addition to our original where clause to accomplish this. from ( t in \" tweets\" , select: { t . tweet_id , t . favorited_count , t . text_content }, limit: 10 , where: [ author: \" elixirlang\" , retweeted: false ], where: t . tweeted_at > type ( ^ \" 2016-01-01T00:00:00Z\" , Ecto . DateTime ) and t . tweeted_at < type ( ^ \" 2017-01-01T00:00:00Z\" , Ecto . DateTime ), order_by: [ desc: :favorited_count ] ) |> TopTweets . Repo . all 21 : 37 : 35.068 [ debug ] QUERY OK source = \" tweets\" db = 24. 7 ms SELECT t0 . \" tweet_id\" , t0 . \" favorited_count\" , t0 . \" text_content\" FROM \" tweets\" AS t0 WHERE (( t0 . \" author\" = 'elixirlang' ) AND ( t0 . \" retweeted\" = FALSE )) AND (( t0 . \" tweeted_at\" > $ 1 :: timestamp ) AND ( t0 . \" tweeted_at\" < $ 2 :: timestamp )) ORDER BY t0 . \" favorited_count\" DESC LIMIT 10 [{{ 2016 , 1 , 1 }, { 0 , 0 , 0 , 0 }}, {{ 2017 , 1 , 1 }, { 0 , 0 , 0 , 0 }}] [{ 682890953212956672 , 226 , \" Elixir v1.2.0 is out - https://t.co/BuULFDtrsm! New guides and official blog post coming out soon. Happy New Year!\" }, { 745240093275873281 , 167 , \" Elixir v1.3.0 has been released and welcome @whatyouhide into Elixir's team: https://t.co/aw0aopnXxn\" }, { 753171413821976577 , 163 , \" Plug v1.2.0-rc.0 is out! Featuring safer encryption algorithms, better MIME support and a brand new debugging page! https://t.co/XuepyvTSQA\" }, ... ] That's it. This query gives us the 10 most favorited tweets authored by\n@elixirlang in 2016. Two small changes -- to the select clause and the order_by clause -- are all we need to get the most retweeted tweets. from ( t in \" tweets\" , select: { t . tweet_id , t . retweeted_count , t . text_content }, limit: 10 , where: [ author: \" elixirlang\" , retweeted: false ], where: t . tweeted_at > type ( ^ \" 2016-01-01T00:00:00Z\" , Ecto . DateTime ) and t . tweeted_at < type ( ^ \" 2017-01-01T00:00:00Z\" , Ecto . DateTime ), order_by: [ desc: :retweeted_count ] ) |> TopTweets . Repo . all 12 : 53 : 29.310 [ debug ] QUERY OK source = \" tweets\" db = 20. 6 ms SELECT t0 . \" tweet_id\" , t0 . \" retweeted_count\" , t0 . \" text_content\" FROM \" tweets\" AS t0 WHERE (( t0 . \" author\" = 'elixirlang' ) AND ( t0 . \" retweeted\" = FALSE )) AND (( t0 . \" tweeted_at\" > $ 1 :: timestamp ) AND ( t0 . \" tweeted_at\" < $ 2 :: timestamp )) ORDER BY t0 . \" retweeted_count\" DESC LIMIT 10 [{{ 2016 , 1 , 1 }, { 0 , 0 , 0 , 0 }}, {{ 2017 , 1 , 1 }, { 0 , 0 , 0 , 0 }}] [{ 682890953212956672 , 278 , \" Elixir v1.2.0 is out - https://t.co/BuULFDtrsm! New guides and official blog post coming out soon. Happy New Year!\" }, { 745240093275873281 , 199 , \" Elixir v1.3.0 has been released and welcome @whatyouhide into Elixir's team: https://t.co/aw0aopnXxn\" }, { 735157122430816256 , 124 , \" Elixir v1.3 introduces calendar types (Time, Date, NaiveDateTime and DateTime) as well as sigils for building them, e.g: ~D[2016-05-24]\" }, ... ] With our queries completed, let's take a look at the two result sets. @ElixirLang's Most Favorited Tweets of 2016 - 226 - \"Elixir v1.2.0 is out - https://t.co/BuULFDtrsm! New guides and official blog post coming out soon. Happy New Year!\"\n- 167 - \"Elixir v1.3.0 has been released and welcome @whatyouhide into Elixir's team: https://t.co/aw0aopnXxn\"\n- 163 - \"Plug v1.2.0-rc.0 is out! Featuring safer encryption algorithms, better MIME support and a brand new debugging page! https://t.co/XuepyvTSQA\"\n- 155 - \"Elixir v1.3 introduces calendar types (Time, Date, NaiveDateTime and DateTime) as well as sigils for building them, e.g: ~D[2016-05-24]\"\n- 123 - \"Announcing GenStage: https://t.co/bNOLdQaB2v\"\n- 118 - \"Ecto 2.1.0-rc.4 is out with https://t.co/PLUFjpi6Rb that streams query results without loading the whole result set into memory.\"\n- 106 - \"Elixir v1.3 will perform cross module checks to find undefined modules and functions! (tks to @antipax) https://t.co/DaMYMNu8cL\"\n- 104 - \"Elixir v1.3 will have new accessors for nested data structures. Here is how to upcase all languages names in a map: https://t.co/FNTTqjpDeP\"\n- 104 - \"José Valim's keynote on GenStage and Flow at @elixirconf is up: https://t.co/OOqeYKuHrt!\"\n- 103 - \"mix test --stale is now on Elixir master thanks to @antipax! It uses static analysis to run tests only for the source files changed on disk!\" @ElixirLang's Most Retweeted Tweets of 2016 - 278 - \"Elixir v1.2.0 is out - https://t.co/BuULFDtrsm! New guides and official blog post coming out soon. Happy New Year!\"\n- 199 - \"Elixir v1.3.0 has been released and welcome @whatyouhide into Elixir's team: https://t.co/aw0aopnXxn\"\n- 124 - \"Elixir v1.3 introduces calendar types (Time, Date, NaiveDateTime and DateTime) as well as sigils for building them, e.g: ~D[2016-05-24]\"\n- 110 - \"Announcing GenStage: https://t.co/bNOLdQaB2v\"\n- 106 - \"Plug v1.2.0-rc.0 is out! Featuring safer encryption algorithms, better MIME support and a brand new debugging page! https://t.co/XuepyvTSQA\"\n- 87 - \"Elixir v1.3.4 has been released with bug fixes for those using Dialyzer and Cover on Erlang 19. Release notes: https://t.co/cyntmGTbxG\"\n- 74 - \"mix test --stale is now on Elixir master thanks to @antipax! It uses static analysis to run tests only for the source files changed on disk!\"\n- 73 - \"Elixir v1.2.6 has been released with three bug fixes and support for Erlang 19: https://t.co/2NcBVJzRG3\"\n- 72 - \"Can't wait for Elixir v1.3? We have just tidied up the CHANGELOG with the upcoming release highlights: https://t.co/aFZbLwedWn\"\n- 71 - \"Official announcement for Elixir v1.2 is up as well as updated guides and docs: https://t.co/ST6lSXIVJz There is some overlap between the two sets of tweets, but if you look\nclosely, you'll see that there are some tweets that show up in one list but\nnot the other and vice versa. Nevertheless, it is clear that the big\nreleases of Elixir this year -- 1.2.0 and 1.3.0 -- stand out as the most\npopular tweets. And from the looks of the 1.4.0 release\ntweet , that is\neasily going to be one of the most popular of 2017. Conclusion If you've made it this far, then you've learned a lot about what Ecto can\ndo. We've seen how to create a migration that exercises a number of key\nfeatures of Ecto.Migration ,\nalbeit we didn't even touch on associations. Then we got to use insert_all , an\nexciting new feature hot off the Ecto 2.0 press. Lastly, we took an\niterative approach to building a query with Ecto's query API. Hopefully the results of all this work highlighted some interesting\n@elixirlang tweets and blog posts that you haven't seen yet. If you haven't\ndone so yet, give this a try on your own Twitter account. What were your\nmost popular tweets of 2016? PostgreSQL is my preferred relational\ndatabase. Though most of the concepts are applicable to any relation\ndatabase, there are some PostgreSQL-specific features that I use and the\nexample assumes you are using Postgres 9.5+. ↩ Sure, using tweet_id as the primary key would have warded this issue\noff from the outset, but then we wouldn't have had a chance to play with\nthe unique_index function. ↩ Twitter's API not only enforces standard rate-limiting, but it also\nlimits access to tweets from the statuses/user_timeline endpoint to the latest 3200 tweets. Because we are using @elixirlang's\ntweets, this won't be an issue. If you are following along with an account\nthat tweets much more frequently, then this may affect your results. ↩ If you are following along with the examples, depending on how your\ndata was inserted, you may be seeing different sets of tweet_id s in your\nresults. This will become deterministic once we add an order by clause. ↩ Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-01-10"},
{"website": "Hash-Rocket", "title": "Using SimpleDelegator for your Decorators", "author": ["\nMicah Cooper\n\n"], "link": "https://hashrocket.com/blog/posts/using-simpledelegator-for-your-decorators", "abstract": "Ruby Using SimpleDelegator for your Decorators by\nMicah Cooper\n\non\nOctober 28, 2013 Decorators are great - especially for building Ruby on Rails applications. They help separate the functionality from your models that some would consider \"not core\". Ruby has a neat class called SimpleDelegator that we've been using to implement the decorator pattern. It's clean, simple, attractive, and good with children which makes it a great candidate for this type of work. So let's use it! Lets say we have a Person class with #first_name and #last_name attributes and we want to add a #full_name method to this Person . Create our Person class: class Person attr_reader :first_name , :last_name def initialize ( first_name , last_name ) @first_name , @last_name = first_name , last_name end end Create our PersonDecorator that inherits from SimpleDelegator And add our #full_name method class PersonDecorator < SimpleDelegator def full_name first_name + \" \" + last_name end end So there is our decorator. But how would one actually use this? Actual use could look something like this: Create our person object person = Person . new ( 'Joe' , 'Hashrocket' ) person . first_name #=> 'Joe' person . last_name #=> 'Hashrocket' person . full_name #=> Method_Missing error Decorate our person object decorated_person = PersonDecorator . new ( person ) decorated_person . first_name #=> 'Joe' decorated_person . last_name #=> 'Hashrocket' decorated_person . full_name #=> 'Joe Hashrocket' All SimpleDelegator is really doing is giving you method missing. Good stuff... Now, I want to add a way to handle collections with this decorator. That is something often done with decorators. So, let's add the class method `.wrap to this decorator. Our final version looks like this: class PersonDecorator < SimpleDelegator def self . wrap ( collection ) collection . map do | obj | new obj end end def full_name first_name + \" \" + last_name end end And that's it! Have fun with your new decorator. Of course, there are many other  ways to use SimpleDelegator. And there are just as many ways to implement the decorator pattern but I think SimpleDelegator is a pretty clean solution. But, it seems to be lacking in documentation department. Hopefully, this helps you gain a better understanding of SimpleDelegator. Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-10-28"},
{"website": "Hash-Rocket", "title": "Refactoring Minical: a cup of Coffeescript", "author": ["\nCameron Daigle\n\n"], "link": "https://hashrocket.com/blog/posts/refactoring-minical-a-cup-of-coffeescript", "abstract": "Refactoring Minical: a cup of Coffeescript by\nCameron Daigle\n\non\nMarch 24, 2014 This past week I pushed a pretty thorough rewrite & cleanup of Minical, my jQuery datepicker plugin. I originally opened it up just to add one little feature, but quickly became sidetracked by how lousy the codebase was and how much better I could write it now (note: this happens with everything I've ever coded or designed, and is completely normal). So I ended up rewriting far more of it than would fit in a blog post – but here's a breakdown of how I rewrote one particular method. showCalendar : ( e ) -> mc = if e then $ ( e . target ). data ( \"minical\" ) else @ $other_cals = $ ( \"[id^='minical_calendar']\" ). not ( mc . $cal ) $other_cals . data ( \"minical\" ). hideCalendar () if $other_cals . length return true if mc . $cal . is ( \":visible\" ) or mc . $el . is ( \":disabled\" ) offset = if mc . align_to_trigger then mc . $trigger [ mc . offset_method ]() else mc . $el [ mc . offset_method ]() height = if mc . align_to_trigger then mc . $trigger . outerHeight () else mc . $el . outerHeight () position = left : \" #{ offset . left + mc . offset . x } px\" , top : \" #{ height + offset . top + mc . offset . y } px\" mc . render (). css ( position ). show () overlap = mc . $cal . width () + mc . $cal [ mc . offset_method ](). left - $ ( window ). width () if overlap > 0 mc . $cal . css ( \"left\" , offset . left - overlap - 10 ) mc . attachCalendarKeyEvents () This was the showCalendar method. It's dense and baffling – clearly written by my evil twin from 2011, as he cackled malevolently from the depths of his island volcano fortress. showCalendar has a ton of responsibilities: conditionally use either this or the event's target's data store for its this reference (because sometimes it's a handler) hide other calendars on the page bail out if the calendar is already showing or the text input is disabled position itself rebuild the calendar element adjust for overlap attach events Sigh. To make matters worse, it was being called as an event handler ... @ $el . on ( \"focus.minical click.minical\" , @ showCalendar ) ... directly, from within a keydown handler function ... preventKeystroke : ( e ) -> mc = @ if mc . $cal . is ( \":visible\" ) then return true key = e . which keys = 9 : -> true # tab 13 : -> # enter mc . showCalendar () false ... and to reposition the calendar on window resize. if @ move_on_resize $ ( window ). resize (() -> $cal = $ ( \".minical:visible\" ) $cal . length && $cal . hide (). data ( \"minical\" ). showCalendar () ) That is too many places! There are performance implications here (rebuilding the entire calendar every time the window resizes is gross), and the code is just downright confusing. Let's fix all the things! First off, the positioning of the calendar should be its own method. Let's abstract that and reference it directly, and while we're at it, let's condense the overlap adjustment code in there too. It's still verbose, but hey, positioning is complicated. At least it's in its own little area now. positionCalendar : -> offset = if @ align_to_trigger then @ $trigger [ @ offset_method ]() else @ $el [ @ offset_method ]() height = if @ align_to_trigger then @ $trigger . outerHeight () else @ $el . outerHeight () position = left : \" #{ offset . left + @ offset . x } px\" , top : \" #{ height + offset . top + @ offset . y } px\" @ $cal . css ( position ) overlap = @ $cal . width () + @ $cal [ @ offset_method ](). left - $ ( window ). width () if overlap > 0 @ $cal . css ( \"left\" , offset . left - overlap - 10 ) @ $cal And we'll reference THAT instead in our resize event and our showCalendar method. showCalendar : ( e ) -> mc = if e then $ ( e . target ). data ( \"minical\" ) else @ $other_cals = $ ( \"[id^='minical_calendar']\" ). not ( mc . $cal ) $other_cals . data ( \"minical\" ). hideCalendar () if $other_cals . length return true if mc . $cal . is ( \":visible\" ) or mc . $el . is ( \":disabled\" ) mc . render () @ positionCalendar (). show () mc . attachCalendarKeyEvents () if @ move_on_resize $ ( window ). on ( 'resize.minical' , $ . proxy ( @ positionCalendar , @ )) Okay, that's already way better. showCalendar is far shorter and is being called one fewer time. But that conditional redefining of this could be done in a few different ways that are far more clean. It'll make far more sense to separate the event handler functionality into its own method. We'll do this by defining a show.minical event that calls showCalendar in the proper context. @ $cal . on ( \"show.minical\" , $ . proxy ( @ showCalendar , @ )) Now every other call to showCalendar can instead trigger the event on the element, and showCalendar itself is only being called in a single place. Plus, if we do the same for the hide.minical event, we can reduce the \"hide other calendars\" functionality to one line, so these find/data/method lines ... $other_cals = $ ( \"[id^='minical_calendar']\" ). not ( mc . $cal ) $other_cals . data ( \"minical\" ). hideCalendar () if $other_cals . length ... become a single event trigger. $ ( \".minical\" ). not ( @ $cal ). trigger ( 'hide.minical' ) Hey, our showCalendar method is almost sensible now: showCalendar : -> $ ( \".minical\" ). not ( @ $cal ). trigger ( 'hide.minical' ) return if @ $cal . is ( \":visible\" ) or @ $el . is ( \":disabled\" ) @ render () @ positionCalendar (). show () @ attachCalendarKeyEvents () However, there's still one big problem: the calendar is re-rendering itself unconditionally every time it shows. We could add logic to showCalendar to combat this, but I don't think showCalendar should have to care if the calendar needs to be redrawn or not. It's only responsible for showing the calendar, after all. To fix this, I ended up rewriting the render method and a bunch of the other event bindings. That's out of the scope of this blog post, but suffice to say I ended up with a new highlightDay method that is responsible for knowing when to redraw: highlightDay : ( date ) -> # try and find the day to highlight $td = @ $cal . find ( \". #{ date_tools . getDayClass ( date ) } \" ) # bail if the day is illegal return if $td . hasClass ( \"minical_disabled\" ) return if @ to and date > @ to return if @ from and date < @ from # rerender the proper month and call itself again if the day isn't found if ! $td . length @ render ( date ) @ highlightDay ( date ) return # highlight the day klass = \"minical_highlighted\" @ $cal . find ( \". #{ klass } \" ). removeClass ( klass ) $td . addClass ( klass ) So here's the current incarnation of showCalendar . ( selected_day is an internal variable recording which day has been chosen and written to the input.) showCalendar : ( e ) -> $ ( \".minical\" ). not ( @ $cal ). trigger ( 'hide.minical' ) return if @ $cal . is ( \":visible\" ) or @ $el . is ( \":disabled\" ) @ highlightDay ( @ selected_day ) @ positionCalendar (). show () @ attachCalendarEvents () e . preventDefault () showCalendar is now less than half its original size. It doesn't need to know if or when the calendar is being redrawn, or even if the day it's supposed to be showing is legal. It's just responsible for highlighting the selected element (if there is one) and positioning the calendar element. Plus, the highlightDay implementation assures ow the calendar is only rendering when it needs to switch months. And there's just one conditional. Relief! You can check out more about Minical over at the Minical Github site . Overall, the rewrite process resulted in a rewrite of about 60% of the plugin, with a drastic increase in readability. I'm sure future me won't be quite as mad at present me as present me was at past me. Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-03-24"},
{"website": "Hash-Rocket", "title": "Ruby5 Roundup - Episode 446", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/ruby5-roundup-episode-446", "abstract": "Ruby5 Roundup - Episode 446 by\nPaul Elliott\n\non\nMarch  6, 2014 Herokai Matthew Conway and I are back again to bring you the latest news in the Ruby and Rails communities. Here is a quick roundup of what's new in this episode of the Ruby5 podcast. http://ruby5.envylabs.com/episodes/482-episode-446-march-7th-2014 Drone & Docker http://jipiboily.com/2014/from-zero-to-fully-working-ci-server-in-less-than-10-minutes-with-drone-docker CI servers are all the rage, but the setup and management of some just cause us to break out in rage and frustration. This blog post from Jean-Philippe Boily will walk you through setting one up with Drone and Docker, which provides a simple self-hosted solution for your CI needs. Under OS http://under-os.com Under OS is a RubyMotion library that lets you build your views in html and css instead of whatever crazy layouting library you've been using. You still get to use all the RubyMotion goodness under the hood, too! Hello Ruby book funded https://www.kickstarter.com/projects/lindaliukas/hello-ruby The kickstarter campaign for the Hello Ruby book was successfully funded! Linda Luikas asked for $10,000 to develop an e-book to help children learn to program and the community came out in force to donate over $380,000! This is a huge success for Linda and the book will be a win for the whole community. Thanks to Linda and everyone who donated. RubyGems.org Cost https://news.ycombinator.com/item?id=7344503 Running rubygems.org costs way more than you probably thought. It stays up thanks to funding from Ruby Central and the hard work of a lot of volunteers. If you haven't thanked them yet, you really should. All our livelihoods depend on it! Rails 4 Assets on Heroku https://devcenter.heroku.com/articles/rails-4-asset-pipeline Want to know all about how to leverage the Rails 4 asset pipeline on Heroku? A new article from Richard Schneeman on Heroku's Devcenter explains it in great detail! Flippit http://www.flippit.us The days of right-side-up text are finally behind us thanks to a new website from Rocketeer Jonathan Jackson. Just put in your normal text and it will flip it over for you to use wherever you please. The site itself is open source using Sinatra and the asset pipeline, so be sure to take a look under the hood! So that's it for this episode of Ruby5. If you haven't already, subscribe to the podcast and keep yourself up to date. Thanks for listening! Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-03-06"},
{"website": "Hash-Rocket", "title": "In Conversation: Sandi Metz", "author": ["\nAndy Borsz\n\n"], "link": "https://hashrocket.com/blog/posts/in-conversation-sandi-metz", "abstract": "Ruby In Conversation: Sandi Metz by\nAndy Borsz\n\non\nJune  9, 2013 Last week, Sandi Metz took time out of her Memorial Day to Skype with me for over an hour sharing her ideas on programming, design, frameworks, learning, and life without if statements. Along the way I found myself frequently laughing over her self-deprecating humor and amazed by her down-to-earth attitude. AB: So the last time I saw you was at RailsConf -- you won a Ruby Hero award. SM: How cool is that? Yeah, I had a -- I sort of knew -- they don't really tell you that you've won. What they do is confirm with you that you'll be at the Ruby Heroes awards -- and then you get followup emails that say, \"we want to make sure that you'll be sitting near the front.\" AB: [laughing] SM: Right? And so when you get the first email, well, you don't want to presume -- it seems impossible. And so I was thinking maybe they just have finalists, right? Because they really will not say that you've won. And so, yeah, it was cool. AB: I definitely agree. I thought that was really fantastic and it was very fun. SM: I mean, I think of people who work on open source projects as being the people who would get Ruby Hero awards. Historically that's what it's been, people who've spent tons and tons of time on software that we all use -- so when they first talked to me, I thought \"that's ridiculous, I don't deserve it\". I haven't done much open source at all -- I work on my own little apps behind firewalls. AB: I think that education about open source definitely is contribution in and of itself. SM: I thought, this is a commercial book -- the book isn't freely available -- and I went through all those reasons why I didn't deserve it. But then I thought: I spent two damn years on this! AB: Absolutely! SM: [laughing] Yeah, so, it was really cool. AB: I'm glad that you don't have those doubts, because I think it's well deserved. SM: I'm just not used to the idea that I deserve it. [both laughing] AB: So when I saw you there, I knew you were headed off to Paris and it seemed like you had come from about five places. SM: Yeah. AB: You're pretty busy right now. SM: So here's the deal -- I'm going to Toronto tomorrow. When I get back, I'm going to New York the next week, and when I get back the day after that I'm going to Washington. Back to Washington. I was in D.C. last week. I try to just go someplace once a month. I've turned down twenty speaking gigs this year. AB: Wow. SM: I can't even remember the list. I finally quit making tags in Gmail for conferences that I've declined because the list got too long. AB: [chuckling] That's incredible. SM: So I turned down a bunch, but I took one per month and didn't really figure out ahead how this would all work. The people who talk at conferences all the time -- like Jim Weirich, he does that for a living. Jim works for whoever it is, not New Context -- they changed their name to Neo or something now. AB: I think there was a joke on Ruby Rogues about not being able to keep track. SM: Yeah so I talked to him at Ancient City and I asked him, what is it that you do for a living? Because people who I see a lot at conferences -- I'm wondering, how are they leveraging this into an actual income? AB: Haha, yeah. SM: Right? And so he's marketing for them. AB: Yeah, he's getting the name out. SM: Exactly. But there's no organization behind me. [both laughing] SM: Anyway, Paris. They called me at the last minute, but how do you say no to Paris? AB: I don't think you do. I think that's one of the rules about Paris, right? SM: And even if you're already overbooked, it doesn't matter. I don't know. I'm supposed to be working on the POODR videos right now. AB: What's that about? SM: They want a video series for the book – the people who own the copyright. The people who own it. AB: Well, that's awesome. What exactly do they want you to do? SM: Okay, so this phenomena is very confusing to me. They want me to not exactly read the book out loud, but they want me to explain the book in a way that maps very closely to the text. Like, Michael Hartl has a series for his Rails Tutorial -- he's got ten or fifteen hours of video that are companion video for the book. But the economics of this escape me -- for the buyer, I mean. You can go to Amazon.com and get the e-version of POODR for $17 and you could read it in an afternoon. Now, for people who have less experience, there's digestion involved. So they're probably not reading it in an afternoon -- but still, it's cheap and relatively fast. Or you can spend more money on many hours of video and watch it over a long period of time. AB: I think that, uh, because you're as versed with the material as you are -- as the author and somebody who has studied this for some time -- you're not seeing how great the benefit of additional explanation can be. SM: [laughing] I am clearly not. AB: I think that kind of repetition is the kind of experience people are looking for. So the reader gets this new idea, and they think \"oh my goodness, I want to know everything about this\" and you might feel like it's more of the same, but when the concepts are new, every new example of the same idea helps solidify it. SM: I mean, I do believe that, I'm willing to accept that -- but really what people seem to want is for me to tell them what's in the book. [both laughing] AB: I just think that different modes of learning sink in in different ways. But I think there's a sentiment there that I also hear when you speak at conferences -- that that you have this knack for explaining things. SM: It's like my only skill, it's true. I will totally accept that. Like, I have this leaky thing in my brain where lots of apparently unrelated things get connected -- it has cons and benefits, because I can also say the most inappropriate things, I promise you. AB: [laughing] Does this ever get you into trouble? SM: Oh yeah, totally. At the same time, if I let it go, I get really good analogies that help me explain things. A couple of weeks ago, actually, at Ancient City Ruby, two things happened. A guy came up and asked me if POODR was ever going to be on Audible.com -- AB: [makes a face] SM: -- and I, I did that. AB: [laughing] SM: But now that I don't have a job, every question -- I don't just say \"that's stupid.\" AB: They're all opportunities. SM: Yes, exactly. So I asked him, how would that work -- it's full of code? I don't understand -- how would you use that? AB: I think that's why I made the face that you detected. I don't know -- are there tech books on Audible? I was unaware of this. SM: I didn't even see how it was conceivable. So I asked the guy, why would that be useful? Am I going to read the code? In what way are you imagining that it would be useful for you? And he said that there's lots of explanation in it, and that all the explanation stands alone. AB: Yes. SM: And he does that same thing that we all do -- he lets the audio run until he hears something interesting, and he stops and backs it up and he listens to that part. He said that even if I just said \"there's code here\" and \"there's code here\", if there was just a thing where I read it, he'd want to hear it in my voice, all of the explanations, read directly from the book. So, this same day, I got a tweet from someone that said \"I just watched your talk from somewhere, is there a transcript?\" AB: So it seems like people really want it all, then. We want written content in video, and video content in written form, and if I could get this to listen to in the background while I'm at work that'd be perfect. So you got a green screen for these videos? SM: I don't, but I have some software. They don't really want video of me -- they want screencasts -- so that helps. And there's some headshot thing that they want to do in a professional studio that I'll have to go to. So it seems totally lame. AB: [laughing] SM: It's completely lame. Like, I've watched a bunch of sample videos -- they're big sellers -- and I just found them appallingly boring. Maybe I'm too just ADD to watch videos. AB: Well, that could turn out to be an asset for making good ones, you know? SM: Yeah, well -- anyway, enough about me. AB: I'm just going to continue to ask you questions about you, Sandi. SM: Okay, well, I can talk as long as you're asking. AB: So you were at Duke for many years, and in some of your talks you've mentioned working on very large, long-running applications there. That environment sounds like where you honed a lot of the ideas that you're now writing and speaking about. Can you talk a bit about that time? SM: You know, the longest running app that I've been associated with at Duke is an app that handles sponsored projects. At universities, faculty do research and that research is sponsored by someone or some business. People who aren't associated with public universities probably are unaware of how much of the money that drives public institutions comes in the form of grants or contracts. And so getting funding is a big deal -- you're working on a project that's funded by a grant, and many of those involve competitions, especially with the government. AB: [chuckling] Competitions with the government? SM: Ha, no, contracts with the government are things you'd compete for. NIH wants a cure for cancer and they have an idea about how to do it -- so they put that idea up there, put a bunch of money in a bucket, and they tell everybody go ahead and submit a proposal to get some of this money for research. There can be a ton of money involved. So there's an app at Duke that we wrote twenty years ago in Smalltalk that basically manages how people submit proposals to funding entities. It was a waterfall design, and they put people in a hotel for like a year to figure out what the app would do. It's of massive importance to the institution - it's been through a couple of big rewrites in Smalltalk, segued through a little bit of Java, and now most of it is has been rewritten in Ruby. So this is an app where the same database has been through three different frameworks. AB: That's pretty cool. SM: Yeah. So that's a kind of experience where stuff doesn't go away -- you just keep on adding to it and changing it. And for an application like that, the data matters much more than the application itself. The data has long term value over many years, and the way users interact with the data is a skin we put on top of it. So whether it's a fat-client Smalltalk app sitting on people's desktops or whether it's some kind of web app, the frameworks come and go a lot more than the data does. I have a lot of interest and concern in making apps that people can maintain over long periods of time -- making apps that can switch frameworks without losing all the business value and years of effort that we put into them. And so, yeah, I mean, I recognize that we're all victims or products of our own experience. I have a very specific perspective on what a good app looks like, and it's because I haven't spent thirty years writing apps that we threw -- I mean, if I were part of a startup and the funding was coming and going, and we were throwing apps at the wall and if they stuck they made money and if they didn't stick, maybe we just abandoned them and went on to the next thing -- I might have a different bias. That style is totally legitimate; people write a lot of code that looks just like that. The kinds of things that you care about if you're doing that are different than if you think that your app is going to win and exist for a long period of time. AB: If it has to win. SM: Yeah, it has to win. That's probably the biggest example of an app that's lived a long time but there are tons of them. There's app after app after app at Duke that has been around a long time, and the data matters. We think of the app as being embodied in the data, not embodied in the framework. For example -- that fat-client Smalltalk app? Parts of it are still in Smalltalk. AB: Parts of it are in Ruby and parts of it are in Smalltalk? SM: Yeah, so the first part of it that went on the web was the faculty end, and so right now there's a multi-year project underway to get all of the administrative parts of that app on the web so they can quit paying Smalltalk licensing fees. They're trying to get rid of Smalltalk because open source has won so thoroughly that people are a lot less interested in paying annual licensing fees by the seat. Despite the fact that, with Ruby, strangers improve the software such that we have to do upgrades regularly -- and that's a cost that people complain about for open source -- the price of open source is still far lower than the price of closed source. AB: So you were beginning these apps -- did you work on them from the start? SM: Yeah, for many of these apps I was there at the beginning. AB: Do you feel like some of these lessons you learned came from doing some of these things the hard way, or were there mentors with you at Duke at the beginning who were able to help steer you from their own experience? SM: No, we learned it all the hard way. AB: [laughing] Amazing. SM: Well, it was back in the day. When we first started, we'd go to the Smalltalk and OOPSLA conferences, and so we were aware of people like Kent Beck, Rebecca Wirfs-Brock, Robert Martin. There was information out there about how to write object-oriented code, but we didn't understand any of it. When I think now about what I knew then, it's a miracle anything worked. AB: You needed more examples and videos, Sandi. SM: [laughing] Maybe. I mean, there was a lot of guidance in the fat-client MVC Smalltalk frameworks that we used.  Like in terms of being exemplary, they were very exemplary. So there was this browser that you'd go write code in, but the browser itself was written in Smalltalk, and so anything you couldn't figure out how to do, you could just look at the source code. If you could think of a thing like it in the browser, you could go and look at the source code in the browser and do it --  so we learned a lot from the tools we had. AB: Do you think that there are lessons from the Smalltalk community that have not yet reached the Ruby community? SM: Yeah. I don't want to -- I am in no way a Smalltalk snob! If I was, I'd be writing Smalltalk today, right? Smalltalk didn't win, and I don't think it's ever going to win. It missed it's chance because it was closed source, but Ruby is -- I mean I don't want to say it's like Smalltalk but better, because it's not quite. I mean, there are things I can do in Smalltalk that I can't do in Ruby, which I find a little bit frustrating, and I'll give you -- should I give you an example of that? AB: Yeah, that sounds fantastic. SM: Okay. So in the book, there's Ruby source code to explain a bunch of stuff and, you know, I write Ruby every day, but the truth is I'm very much not the best Ruby person around. So I have bias. I'm so biased by my Smalltalk background that I made assumptions about things that ought to work in Ruby, and when I tried them and they appeared to work, I believed that they were right. One of those things was to subclass Array. AB: Oh. SM: I just did it. I just assumed that if I have a thing that's sort of an array -- if I want to add behavior -- I'll just subclass Array. I didn't really understand that there were very specific reasons why doing that in Ruby was a bad idea. I have a lot of bad habits left over from my little silver bubble that I lived inside -- one is that I tend to think I can use everything as a first class object, so I have to be careful about that. The other is, well I'm used to living in this world where it's so closed source that you don't really share code with anybody else, so I'm far too likely to just rip open a base class in Ruby and make a change in it. This world where we're all collaborative and you should be exercising a little care about stomping on others messages, We were writing applications, not frameworks, so as an application writer you're at the end of the line. If you make base class changes to String, your team knows that. AB: Hopefully. SM: Yeah. There's many ways in which that can go badly. We did it for years in Smalltalk without a thought and never had a problem ... but there's not a whole gem ecosystem for Smalltalk. You're dealing with the library or you're dealing with your own code. But because Ruby has won -- and because Rails has won -- you have to take the rest of the world into consideration when you write code in a way that we never did when we were writing Smalltalk.  Anyway, the thing about Smalltalk, the example that I always give people is -- I don't want to get too nerdy here -- let me see if I can explain this quickly and sensibly. Imagine you didn't have an if statement. Imagine, Andy. AB: Okay. SM: Imagine there's no if statement. AB: I can keep my eyes open, right? SM: [laughing] You can. So yeah, there's no if statement. So: how do you switch on booleans if there's no if statement? It seems impossible, doesn't it? This is not a test. Does anything leap to mind? AB: Something like instantiating an object based on something you know. SM: Uh, huh. So imagine this: imagine you had a true object -- which you kind of do in Ruby -- there's a TrueClass and true is an instance of TrueClass . And imagine it implemented two new methods, right? Actually, let's leave true out of it for a minute. Imagine yourself as an object. You're an Andy and you're a positive guy. AB: Okay, this is a lot easier. SM: This is a lot easier. So you're an instance of Andy , and because you're a positive guy you believe that everything is true and nothing is false, right? You're just that way. AB: Sure. SM: So let's imagine that I'm negative. I'm negative Sandi. AB: It's getting harder to imagine again. SM: I'm just the opposite. I believe that everything is false and nothing is true. Alright? Now, if I wanted to be the Sandi object, the negative Sandi object, I could implement an if_true and if_false method, and they could take a block. So if you sent if_false(some_block) to me, I would always say, sure it's false, right? Because I believe that everything is false. AB: You are False. SM: Yes I am. And you, positive Andy, are just the opposite. If someone sends you if_true and passes you a block, you evaluate the block, but if they send you if_false you just ignore it, you do nothing. Right? AB: Okay. SM: And so, if you just have some way to make that boolean, and you can get the language to evaluate that to an instance of that class, you now have an object that will always evaluate the block they get for true or ignore the block they get for false appropriately. So we're talking about having true and false objects, and sending them messages. In Smalltalk, you don't even have control structures in the language -- you just have objects that have messages. And then your whole world is different. The syntax is to send a message, and there's nothing else. So you can imagine how it would shape your thoughts if you grew up in an object-oriented language like that. And how if you came from a procedural language to a language like Ruby where you have those control structures how it's easy to hang on to the procedural parts of your past. AB: The procedural past. SM: [laughing] Yeah. AB: When did you know that you wanted to be a programmer? SM: Oh, I don't know, like, I'm a woman of a certain age, and that age sort of pre-dates the profession. AB: We don't need the year -- I just mean when in your life. SM: I was a failed music student looking for a job until I could decide on a new major when I ran into the local vocational technical school at the mall. At the time, I thought vo-tech was for boys who were going to do auto body work underneath the football stadium, but I happened to be with someone who said, oh, vo-tech schools, they'll teach you how to make money, they'll teach you a job. And I was working at a WalMart-ish kind of thing -- so I visited the vo-tech school and saw that they offered drafting classes. I thought drafting could be cool, I can be an architect, right, I'd love that. So I took the little pre-test and they insisted that I sign up for data processing. AB: They insisted on data? SM: Yeah and I argued with them because I didn't even know what that was -- I mean this was, 1979, nobody was a 'programmer'. So they say I belong in data processing, and I didn't know a soul that -- I mean, this was in the days of punch cards in the IBM 370. This was before you get a degree doing this -- you couldn't really go to school, The reason vo-tech schools were teaching it was because employers needed programmers and they couldn't get them. But this was in the era of pocket protectors. I didn't know anybody that was a computer programmer, and it just seemed like the most boring thing I could imagine. I'm going to sit a desk and type all day long? That's insane. And so I had a big argument with them and I told them no, but they were really insistent. And so I went home -- but I was actually living with someone at the time who had done a little programming, but I didn't know it. And when we were washing up after dinner, out of the blue he said, \"you know, just listen to them -- I think you might be good at data processing.\" And on day one of the first class, we wrote three FORTRAN programs and I fell in love. And that's when I knew I wanted to be a programmer, but it was totally an accident. I never looked back from that point on because I thought, wow -- this is the job? You get to do this for the job? You get to play with logic puzzles all day long? How can this be work? So yeah, it was an accident. AB: Do you feel like there's anything outside the context or theme of your book that you'd like programmers to know? SM: I have this really strong desire for us to demystify -- to simplify our explanations to one another. And I think the book is an instance of that, or at least an attempt at that. Whether or not it has succeeded is of course an exercise left to the reader, but I want -- let me just say this -- I have a whole pile of technical books on my desk that I cannot read. I know that there are amazing ideas in them, and every now and then I crack them open -- and now more and more, because people think now that I have read everything. AB: All of it! Everything! SM: All of it, right? And so I have all those books -- I do work on them more rigorously now than I ever have, but I find it very tough going. And I don't know what it is about our community or our history, but why did things have to be complicated? Right? I mean, they're not. They're not innately hard, I don't think, and yet ... I don't want to assume that people make overly complicated explanations in order to keep people from understanding. AB: No, I don't think so either, but it seems to be a thing that could almost come across that way sometimes. SM: Yeah. They're not bad, they're not evil. Some of them come from an academic world where it seems like maybe there's a circle where everyone understands those explanations. That clearly seems true, right? AB: For sure. SM: But then where does that leave the rest of us? Because I think that pyramid is really wide at the bottom and pretty narrow at the top. And somewhere near the top are the academic folks who get what they're talking about, but I can promise you that I don't. And so that struggle of trying to figure out what they mean, what they're saying, how it applies to me -- if I could wave a wand and change the world, that'd be the thing I would change. I'd make people who have smart ideas able to explain them to the rest of us so that we can understand them and apply them usefully. It feels to me like there's a big gap. It feels like we're failing each other in this business. It feels like we're failing to provide. AB: Are you referring specifically to books and educational material or about communication in our industry in general? SM: Yeah, I don't know. One specific case is definitely the books. I don't know about the general case, because I don't think I have standing in that argument. I don't have standing to speak to that -- it may be true. But I can name some books that I found very hard going to begin with that I still think everyone should read. Everyone should read and understand the true meaning and techniques of Refactoring. That means going all the way through the Martin Fowler book, or going all the way through the Jay Fields Ruby version of it. And those books aren't terribly hard to read -- I just find that their story is uninspiring, because it's just detail detail detail detail detail, right? AB: Yeah. I think that can happen anytime you have a greater point to get across and you know you need some code -- that can be a challenge. SM: Yeah, yeah. And I'm not saying that they didn't have a really difficult challenge. Like, how do you tell a story inside the Refactoring book? [laughing] I don't know -- it seems kind of impossible, doesn't it? But I just had a hard time understanding the bigger context when I first tried to read that book. Another book I think is really useful and has tons of amazing ideas in it is Eric Evans' Domain Driven Design. But I just have a very hard time grasping the details. Again, I don't want in any way to seem critical, because I think the ideas are absolutely amazing and I think we'd all be better if we all read it and understood it -- but that's a big step. The reading and understanding is difficult. But I want us to get it. I feel there are a lot of arguments and tension right now in the Rails and the Ruby community around this idea of design. How are we going to do design -- how are we going to arrange code? And I so firmly believe that there are ways to understand the consequences of the choices we make about code arrangements -- and that knowledge will enable me to choose the consequence I like. So it doesn't seem like design is a bad thing to me. I just want people to get it so they'll know what they're picking. And yet it feels like there are a lot of heated arguments over different alternative arrangements, and the parties involved may not necessarily understand the consequences of all the choices they're fighting about. It just feels like we could have a higher level of conversation and produce better quality code if we all understood that. AB: I think a lot of readers are excited about this, and that there's sometimes some frustration with applying these concepts within Rails. I don't mean to speak poorly of Rails, because it's provided me a lot enjoyment as well as a livelihood. However, many people get excited about design and SOLID principles, and then get stuck. Like, how does a Rails controller or ActiveRecord model conform to the Single Responsibility Principle? So I think a lot of readers are very curious about this and are wondering ... what does a Sandi Metz Rails app look like? SM: So here's the thing. Let me just say this first: if I were not a slacker I would write a blog post about this, but I am, so I get this question all the time.  People are always telling me they can totally see how object-oriented design matters in Ruby, but they're writing Rails, and so what should they do -- and I don't necessarily feel qualified to answer that question officially, but I do have opinions. I definitely have opinions. And so I think of myself as someone who writes 'applications that use Rails', not as someone who writes 'Rails apps' -- and I have a different definition for those two things. Like you, I love Rails. And I think if you have a Rails app, as opposed to an app that uses Rails, then the Rails conventions are perfect. And when I say 'Rails app', I mean an app where a GUI pretty much maps over a database. AB: Yes. SM: Right? And so in a Rails app, there's an expectation built into the framework of this kind of pattern: a request comes in, and it's going to be a RESTful route that will wake up a controller action, that action is going to know about the name of one class, that class is going to be its model, that model class is going to be a subclass of ActiveRecord, and the controller action is going to know the name of that class. Let's ignore the index action for awhile -- let's say we're talking about the show method. So it knows the name of the class, it's got some id, and there's a little factory in that class -- find is a factory method. It's going to generate a new instance of the class, that instance is going to be put in a single instance variable, and that single instance variable is going to be passed back to the views. That's the simplest kind of Rails app. The controller knows the name of one class and it passes a single instance variable back to the views. Now if I have a more complicated application -- which lots of people do; it's a tribute to the strength and flexibility of Rails that many people write apps that use Rails when they don't have the simplest kind of Rails applications. So in that situation, let's say I have a controller action that doesn't map one to one to an instance of an ActiveRecord model. Then really what I want my controller to handle is some kind of procedure that's going to require the coordination of a whole bunch of ActiveRecord models. So I have a couple of choices: I can put all the code in the Rails framework -- I can make my controller know a whole bunch of classes and have all the logic for all the coordination between them and I can set a whole bunch of instance variables and pass them back to my view. So I can basically break the Rails pattern. Or, I can honor the Rails pattern by making up new objects, shoving the edges of the framework apart, and putting new objects in the middle. Now when you do that, all of those objects in the middle obey all those standard rules of Ruby object-oriented design, and the framework does nothing but let me deal with input and write to persistent store. So I write apps that use Rails, and I put very little logic in subclasses of the Rails framework, but I follow the Rails pattern. I want my controllers to know the names of two classes: the service business object, and the presenter object. And so I'll often instantiate a presenter class in my controller that takes my business class as an argument that then takes params as an argument. That lets me honor the Rails pattern of knowing the name of one business object while at the same time honoring the Rails pattern of having one variable to pass back to the views -- but it lets me get all of my logic out of the Rails framework classes so that I can do standard Ruby things. This kind of pattern is easy to test, I can use dependency injection, I don't have bunch of logic in helpers or controllers, I can write a test suite that evolves -- and because I completely trust that the Rails objects behave correctly, I don't have to do much in the way of ActionController or ActiveRecord tests if I can get all of my business logic out of those things. I can use small, easily composable objects that sit in the space between the edges of the framework, and have fast running tests about objects -- and the next time my app moves from Smalltalk to Java to Rails or whatever, all of my logic isn't tied up in the framework so it's easy to reuse or port. I know that's not much help for people who's apps are just absolutely insanely complicated. When you're starting with a greenfield app, it's easy to keep your logic away from the Rails framework. In a legacy app that has a lot of entanglements, then the only way to fix them is just to nibble away at them. When you add new code, don't make it worse. When you touch code, make it better. And my advice to everyone -- whether you're dealing with a Rails app or a Ruby app -- is that problems can be solved by breaking them down into smaller pieces. So break things down into smaller pieces in your Rails app, and quit putting logic in controllers. Make your ActiveRecord models smaller. There's a lot business logic that's not really part of the framework, and if you can get it out of the framework and into objects you own, then everything gets easier. There's no reason our Rails apps have to make us suffer even if they're big. AB: Is there such a thing as Impractical Object–Oriented Design in Ruby? SM: [laughing] I think that there are things that people call design that seems very impractical to me. I mean, the things that give design a bad name by definition are impractical, right? I'm not without sin, though. Most of us are nerds and we want to follow that flight of fancy and do the cool thing, but you've got to put a leash on yourself and not overengineer things. We owe it to the people that pay the bills to not indulge ourselves. If it's your money, you can do whatever you want for the sheer fun of it, no matter how big a waste of time it is -- but in the big world, we have ethical obligations to the people who are paying for our time to provide value. I don't think people overdesign because they're evil -- it's more of a symptom of having more care than experience. They're trying to do the right thing, and there's always this tension between making the best decision given available time and resources, and learning. It's taken me a long career to get comfortable with 'good enough' -- to leave code that's not very pretty, to leave code that I feel a little bad about, to leave code that's not quite right. But I've learned that doing too little is almost always better than doing too much. Almost always. It's much harder to change an abstraction that's wrong than to eliminate duplication. And so there are tons of impractical things that are often justified in the name of design. By my definition, I don't think of overdesign as being 'design' at all, whereas I know that there's a bunch of people in our community who think that the word 'design' means overdesigned, as if anything that could be called 'design' is bad by definition. I use design to mean 'however the code is arranged', and so, yeah, there's plenty of impractical design, depending on what definition you're using. If you think of all 'design' as bad, then it's all impractical. I would say just the opposite -- all design is good and that anything that is overdesigned is not designed at all. It's all semantics. - Sandi Metz has 30 years of experience working on projects that survived to grow and change. She’s the author of the recently published Practical Object–Oriented Design in Ruby and (as all who read the book know) an avid cyclist. Fundamentally, however, she is someone who writes object–oriented code. Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-06-09"},
{"website": "Hash-Rocket", "title": "Go 0 to 60 with create-react-app", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/go-0-to-60-with-create-react-app", "abstract": "Javascript React Go 0 to 60 with create-react-app by\nJosh Branchaud\n\non\nAugust  3, 2017 I remember the first time I tried out React.js . It was several years back. The promise of painlessly building interactive UIs was too enticing to ignore. As I worked through the first couple introductory tutorials, my computer screen began to shimmer in a way that it had never done before. The pure functions and composable components brought the browser to life. Just as my excitement for React was mounting, I was stopped in my tracks. Before I could go much further I was faced with tons of tangential decisions about how the project would be configured. Can't someone just make these decisions for me? This is the impetus for create-react-app . Create React apps with no build configuration. This tagline is another way of saying, \"Create React apps! We made all the build configuration decisions for you.\" So, what kinds of things do we get for free with create-react-app ? ES6 features compiled by Babel JavaScript code linting with ESLint Live-reloading with Webpack Dev Server Optimized production builds Zero create-react-app is a CLI tool that we can install globally and use anytime we want to bootstrap a new React project. Install it using yarn (or npm ). $ yarn global add create-react-app Then create your first CRA-bootstrapped project. $ create-react-app my-app\nCreating a new React app in /Users/dev/hashrocket/my-app.\n\nInstalling packages. This might take a couple of minutes.\nInstalling react, react-dom, and react-scripts...\n\nyarn add v0.27.5\ninfo No lockfile found. [ 1/4] Resolving packages... [ 2/4] Fetching packages... [ 3/4] Linking dependencies... [ 4/4] Building fresh packages...\nsuccess Saved lockfile.\nsuccess Saved 894 new dependencies.\n...\nSuccess! Created my-app at /Users/dev/hashrocket/my-app Once the project is set up and all the dependencies are pulled in, CRA let's us know about the main commands available to us. Inside that directory, you can run several commands:\n\n  yarn start\n    Starts the development server.\n\n  yarn build\n    Bundles the app into static files for production.\n\n  yarn test\n    Starts the test runner.\n\n  yarn eject\n    Removes this tool and copies build dependencies, configuration files\n    and scripts into the app directory. If you do this, you can’t go back!\n\nWe suggest that you begin by typing:\n\n  cd my-app\n  yarn start\n\nHappy hacking! Sixty The yarn start command is where the magic is. This video highlights the main features of the Webpack Dev Server that is kicked off by yarn start . First off, Webpack is configured with live-reloading. Make some changes to your app, save the file, and see your app update instantly in the browser. This includes state persistence. Second, linting of our JavaScript is provided by ESLint . Any linting issues, such as an unused variable, will be flagged in the output of the dev server. Third, the entire app is being compiled by Babel . This means we have access to the latest ES6 features . If there are any issues with our JavaScript that prevent compilation, those will also be flagged in the output of the dev server. The idea behind CRA is to present a fairly standard configuration. If you need to go beyond what it provides, you can yarn eject your app. This will unpack all of the hidden away parts of CRA such as the Webpack config files. Read more about that here . Onward! Go! Build that awesome React.js app. CRA gives you a great starting point. An app bootstrapped with CRA can do a lot more than we've discussed here. The project README.md has a comprehensive table of contents and goes into lots of detail in each section. Check it out. Are you building something bootstrapped with CRA? Tell me about it on Twitter . Cover photo by Geran de Klerk on Unsplash.com Other reading on create-react-app : Create Apps with No Configuration What's New in Create React App Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-08-03"},
{"website": "Hash-Rocket", "title": "Hashrocket and Code Platoon", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/hashrocket-and-code-platoon", "abstract": "Hashrocket and Code Platoon by\nJake Worth\n\non\nSeptember 21, 2017 Hashrocket's collaboration with Code Platoon continues this October! For the past three cohorts, Hashrocket has taught an intensive all-day workshop at the Code Platoon bootcamp. Here's a\ndescription of this unique developer training program, from their website: Code Platoon is a Chicago-based non-profit program exclusively for veterans.\nWe’ll teach you full-stack development over the course of 20-weeks (6 weeks\nremotely, 14 weeks in our Chicago classroom), and train you to be eligible\nfor paid internships and full-time employment as a web developer. No previous\nprogramming experience is necessary. In this post, I'd like to talk about this important program, and what we offer to it. About Code Platoon The world needs more\nprogrammers; there's a mountain of work to be done. Code bootcamps rise\nto this challenge by recognizing that a college degree isn't enough, or perhaps\neven required, to do what we do, and that the fundamentals can be learned in a\nshort time. Code Platoon is unique because they only select military veterans who pass a\nrigorous selection process. These veterans have served our nation, and for that\nwe owe them our gratitude. They come to the program with unique life\nexperiences, maturity, and character that makes them a special group. When I transitioned from the military to programming, Code Platoon didn't\nexist. I taught myself and learned by making mistakes. On a personal level, I am\ngrateful that other veterans are finding a path to this industry,\none that is well-lit, less haphazard, and can be navigated as a team. What We Offer Our workshop in October is our fourth with Code Platoon. Jack Christensen and I will be teaching an in-depth workshop about\ndatabases, a favorite subject in our office. We'll start with an overview of persistence, move into hands-on work covering the\nmajor SQL concepts, wind down with a data-modeling exercise, and wrap up with\ndeveloper Q & A. I'm excited to meet the fourth cohort, Delta Platoon, and share our passion for\ndatabases. Conclusion Teaching is an integral part of Hashrocket's culture, and we relish the\nopportunity to share our expertise. This combined with a mission we believe in\nis a win-win. Thank you to Code Platoon for inviting us back for a fourth workshop, to\nHashrocket for sponsoring this ongoing engagement, and to Alpha, Bravo,\nCharlie, and Delta Platoons, and all veterans for your service. Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-09-21"},
{"website": "Hash-Rocket", "title": "Ruby5 Roundup - Episode 419", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/ruby5-roundup-episode-419", "abstract": "Ruby Ruby5 Roundup - Episode 419 by\nPaul Elliott\n\non\nNovember 15, 2013 This week Matthew Conway and I bring you the latest news in the Ruby and Rails communities in another fantastic episode of the Ruby5 podcast. Here is a quick roundup of what's new this week. http://ruby5.envylabs.com/episodes/455-episode-419-november-15th-2013 marco-polo https://github.com/arches/marco-polo Ever truncated a table in the wrong environment? Ever added a user to the wrong application through the console? Install the marco-polo gem in your apps and never wonder again! It puts the app name and environment in your console line so you'll never have to wonder again. Honey, I shrunk the internet! http://robots.thoughtbot.com/content-compression-with-rack-deflater The fine folks at Thoughtbot wrote up a very informative blog post about deflating server responses. Chock-full of comparisons and detailed data, they cover all the pros and cons of adding Rack::Deflater to your app. A great read for people at all skill levels with Rails. Bitters http://robots.thoughtbot.com/announcing-bitters-a-dash-of-sass-stylesheets-for-bourbon-and-neat Thoughtbot released a new gem called Bitters to get your started faster with your sass stylesheets. It works great with Bourbon and Neat to give you a solid foundation to work with. Heroku Postgres 2.0 https://postgres.heroku.com/blog/past/2013/11/11/heroku_postgres_20 Heroku launched the next evolution in their Postgres as a service offering. It comes with some cool new features and new pricing structures. Check out their blog for all the details. Heroku Open-Sources Auth https://blog.heroku.com/archives/2013/11/14/oauth-sso Ever wondered how exactly Heroku's authentication system works? Well now it's open source! This is your change to find some security exploits in a popular system and please, for all our sakes, professionally and discreetly report them. MotionInMotion Screencasts http://blog.rubymotion.com/post/66769533905/new-rubymotion-screencasts-motioninmotion Everyone's favorite iOS development platform is getting its own line of screencasts! The MotionInMotion screencasts are going to launch soon and they released the first one for free on the RubyMotion blog. Check it out and sign up for the mailing list for a lifetime discount. So that's it for this episode of Ruby5. If you haven't already, subscribe to the podcast and keep yourself up to date. Thanks for listening! Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-11-15"},
{"website": "Hash-Rocket", "title": "RailsConf 2011: Q&A", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/railsconf-2011-q-a", "abstract": "Community RailsConf 2011: Q&A by\nPaul Elliott\n\non\nMay 22, 2011 Hashrocket came and conquered RailsConf 2011. We laughed, we cried and we walked away feeling inspired and challenged. Here are just of the thoughts of the Rocketeers that attended RailsConf. Who was the most interesting Rubyist you met at RC2011? Matt Conway: Probably Wayne. He has a very vibrant personality and wore sesame street boxers. You could hear him laughing all the way down the hall. What was the biggest thing you missed out on while you were at RC2011? Dave Ott: I didn't have any crab cakes and didn't make it to BoatConf [a few Rocketeers rented a room on a boat and dubbed it BoatConf - ed] . I really wanted to pair with some people but no one seemed to want to. Although BohConf was a lot of fun, it would have been cool to see some of the talks as well. Probably the biggest thing I'm upset about is missing the music jam. I just totally spaced on it. What was your favorite interview? Paul Elliott: It's hard to narrow it down to just one. I really enjoyed talking to Chad Pytel from Thoughtbot about how they manage themselves for both client and product work. I talked to Jeff Casimir from JumpStart Labs about the upsurge of training offerings in the industry. Jeff is great...I love talking with him. I also had a great conversation with Brian Doll from [NewRelic][] about some of the new features on their platform, which I am really looking forward to trying out. I was really interested in the comments by Anthony Aeden from DNSsimple and LivingSocial about the mentality on InfoEther 's side of the aquisition. I don't think I can pick just one. What was the coolest camera equipment you saw while you were at RC2011? Kaz Sheekey: Mine. Did you learn any new testing tricks from the conference sessions? Thais: I really liked how Chelimsky explained his point of view and how to remove duplication. He showed some examples of RSpec and some things he doesn't like about the library. Even though everything he mentioned is not new to me, it was still good to watch because he is a very good speaker. Another cool thing he talked about was how to think about being DRY. Sometimes being DRY causes you to tightly couple pieces of the system that shouldn't be coupled and this can cause problems down the road. He said that a little bit of duplication is OK in certain circumstances. What was different about this year's RailsConf versus previous years? Thais: This year had a big focus on frontend development. There were talks on backbone-js , coffeescript , and page load performance. This is a topic that has been largely overlooked by the industry for a long time. Even the organization of javascript in Rails 3.1 has had an overhaul and is greatly improved. The asset pipelines are much more sophisticated. DHH discussed some of this in his keynote and it was followed up by a few of the talks throughout the conference. Describe your conference experience in 1 word. Paul: inspiring Caleb: welcoming Thais: motivador Dave: connecting Aaron: enlightening Matt: gin Was this post helpful? Share it with others. Tweet Share Post", "date": "2011-05-22"},
{"website": "Hash-Rocket", "title": "Ruby5 Roundup - Episode 436", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/ruby5-roundup-episode-436", "abstract": "Ruby5 Roundup - Episode 436 by\nPaul Elliott\n\non\nJanuary 31, 2014 I'm back with Herokai Matthew Conway to bring you the latest news in the Ruby and Rails communities. Here is a quick roundup of what's new in this episode of the Ruby5 podcast. http://ruby5.envylabs.com/episodes/472-episode-436-january-31st-2014 Elixir Fountain http://elixir-fountain.com Staying up to date with the goings-on in the elixir community has never been easier thanks to a new website by Rocketeer Johnny Winn. You can subscribe to weekly updates via email or follow on twitter to get all the information. The archives are browsable through the site as well. Argus https://github.com/jimweirich/argus Have a Parrot AR Drone collecting dust in your closet? Well the Argus gem from Jim Weirich makes it really easy to fly it straight from  your Ruby console. STI + Hstore http://www.devmynd.com/blog/2013-3-single-table-inheritance-hstore-lovely-combination This blog post explains how to leverage the postgres hstore to make STI a more palatable option. No longer do we have to suffer through swaths of nil columns in our database! Rails Errors and Validators http://monkeyandcrow.com/blog/reading_rails_errors_and_validators This blog post will walk you through the Rails error subsystem and how validators work. If you've been wanting to start making your own custom validators but weren't sure how to get started, this is the place to look. Sparkr https://github.com/rrrene/sparkr The sparkr gem brings all the awesomeness of the spark utility by Zach Holman to the Ruby console. You can even colorize the output! HandCooler http://sanemat.github.io/hand_cooler/#/ HandCooler is your one-stop shop for gem README's. You can enter the name of your favorite gem and see the README in all of it's markdown processed splendor. So that's it for this episode of Ruby5. If you haven't already, subscribe to the podcast and keep yourself up to date. Thanks for listening! Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-01-31"},
{"website": "Hash-Rocket", "title": "Three jQuery Plugins We Wrote (And One We Love) ", "author": ["\nCameron Daigle\n\n"], "link": "https://hashrocket.com/blog/posts/three-jquery-plugins-we-wrote-and-one-we-love", "abstract": "Javascript Design Three jQuery Plugins We Wrote (And One We Love) by\nCameron Daigle\n\non\nAugust 15, 2012 Here in Hashrocket Front-End Coding Land, we like our plugins lightweight, manageable and practical. Inevitably, this has resulted in some custom plugins that tend to find their way onto pretty much every project in one form or another. Here are four of those plugins, in no particular order. Hopefully one or more of these will help make your life easier as well. Dummy Image This is Shane's newest creation – a lightweight solution to an issue that's been plaguing us for a while. We tried dummyimage.com , but it would occasionally bog down. In a moment of desperation, I even switched over to placekitten.com for a while, and that - while undeniably adorable - also had performance issues. One of our guys wrote a Ruby gem to generate placeholder images, but one day Shane realized that this problem could be solved in a platform-independent manner using Canvas, and dummy_image.js was born. Check out the Dummy Image site here and throw it into your next project. It's a piece of cake to use and has become a standard part of every new project here. jQuery.modal We work on a lot of management UI, and inevitably it'll make sense to show something in a modal (lightbox, popup, whatever you want to call it). After messing with an assortment of prebuilt plugins, Shane rolled our own a while back, and it's been evolving from project to project. The latest version has been ported to CoffeeScript and weighs in at a trim 3kb when minified. Check it out on Github – while other plugins may have more options & features, we love this one because it generates minimal markup, is easily customizable, and does exactly what we need it to do. Chosen Chosen is a select-box enhancer written & maintained by the good people at Harvest . It takes a standard dropdown and replaces it with a prettier one, complete with a search field and very nice multiple-select capability. We run across scaling issues with dropdowns fairly often in our projects, and Chosen's autofill UI has become our standard fallback with any situation where a select box has to handle more than a handful of options. Although it's not particularly easily restylable, the design is tasteful enough that a restyle isn't really needed; it fits in nicely with standard browser controls. Here's the homepage for Chosen – check it out, you'll undoubtedly have a use for it. jQuery.minical Ah, the infamous date picker. Every once in a while, a bit of UI will require the user to choose a date, and that's where the fun begins. Back before Minical, we'd wrangle together some jQuery UI codeball, plop in the jQuery UI Datepicker , and hope for the best. I finally couldn't take it anymore, realized that a good datepicker plugin was well within my capabilities as a designer-who-also-does-frontend-coding, and Minical was born. I have a little site for it here with more info and examples. I recently rewrote it from the ground up in CoffeeScript, which resulted in a good learning experience and a vastly improved codebase. It's easy to customize and super-lightweight; I've been adding features as we've needed them. If you just need a calendar and don't need all of the extra features that jQuery UI provides, Minical just might be for you. Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-08-15"},
{"website": "Hash-Rocket", "title": "Ruby5 Roundup - Episode 385", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/ruby5-roundup-episode-385", "abstract": "Ruby Ruby5 Roundup - Episode 385 by\nPaul Elliott\n\non\nJuly 14, 2013 Last week I released another episode of the Ruby5 podcast with my old friend, Matthew Conway. Here is a quick roundup of this episode. http://ruby5.envylabs.com/episodes/421-episode-385-july-12th-2013 RSpec 2.14 http://myronmars.to/n/dev-blog/2013/07/rspec-2-14-is-released We've seen a lot of changes in rspec over the years. Last week version 2.14 was released and marks the end of the 2.x line. It also comes with some cool new features like spies, mock support for the new expectation syntax, new configurations, and some bug fixes. Upgrading should not impact your existing test suite, so do it today and start spying! Rails Security Course http://railssecurity.com CodeClimate is offering a free month-long Rails security course via email. It promises to contain some tips not in the official guides, explanations of how things we thought were taken care of aren't exactly, and tricks you thought were cool but are actually really dangerous. Did I mention this is all free? Heroku Pipelines https://blog.heroku.com/archives/2013/7/10/heroku-pipelines-beta Heroku released a new labs feature called pipelines. If you have staging and production environments, it lets you hook them together then promote the slug from staging to production. It should reduce deployment times and help to streamline your deployment process. You can also diff the two servers to see what would be deployed. Bogus https://github.com/psyho/bogus Bogus is a gem that provides some safely to the world of mocking and stubbing. It lets you make test doubles that have the same interface as the class it is doubling and can alert you if you stub a method that doesn't actually exist. Drink Menu https://github.com/joefiorini/drink-menu The best part of RubyMotion is the strong ecosystem around the technology. Another new addition is Drink Menu, which lets you build OS X menu bar applications with RubyMotion. That means nicer DSLs, easier development and testing, and no Objective-C! Vimscript And You! http://blog.hashrocket.com/posts/vimscript-and-you Fellow Rocketeer Jonathan Jackson wrote a blog post explaining how to use RSpec to test your vim plugins. It leverages a library called vimrunner by Andrew Radev to link everything together. I never thought testing vim plugins could be this easy, but you'll see in this blog post's TDD'd example that it is! So that's it for this episode of Ruby5. If you haven't already, subscribe to the podcast and keep yourself up to date. Thanks for listening! Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-07-14"},
{"website": "Hash-Rocket", "title": "Easier Atomic Commits", "author": ["\nJoshua Davey\n\n"], "link": "https://hashrocket.com/blog/posts/easier-atomic-commits", "abstract": "Easier Atomic Commits by\nJoshua Davey\n\non\nMay 16, 2012 How do you keep your commits atomic easily? Let's explore one possible approach. Problem As a practitioner of good source control, you and your team have decided\nto make all of your git commits atomic within your projects. That is,\nevery commit has a green test suite, and you prefer small, incremental\ncommits to large, monolithic ones. Keeping commits small and atomic has\ntons of benefits, from more consistent continuous integration results,\nto better team cohesion (have you ever gotten upset with another team\nmember for committing red?). But in practice, keeping all of your commits\natomic can present some challenges. After doing a bunch of work, making incremental, atomic commits along\nthe way, it's time to push your work up. However, when you run git pull\n--rebase , you find that another team member has made changes since you\nlast pushed. Your commits are now sitting on top of a different git\nhistory. Are all of your commits still atomic? Short of checking out\nevery single commit and running the suite, how can you be certain that\nevery commit is atomic? What a pain! I don't want to check out every\ncommit by hand. Solution Enter atomically , a simple shell script designed to take the pain out\nof checking every commit between your upstream and you. Before pushing,\nyou can ensure every commit is atomic by running the script. To use, just pass atomically the command as arguments: $ atomically rake The above command will start at the current branch's HEAD and run rake.\nAfter that, it will check out the previous commit and run the command\nagain. It will do so for all commits between you and origin. If you are confident that nothing in your spec suite changed, you can\nrun only your cucumber features the same way: $ atomically cucumber Or just your spec suite: $ atomically rspec Regardless, keeping atomic commits is a vital part of good source\ncontrol, and this tool makes it slightly easier to do so. Here's the source of atomically : #!/bin/bash if [ -n \" $( git status --porcelain ) \" ] ; then echo \"ERROR: You have a dirty working copy. This command would remove any files not already checked in\" exit 1 fi b = \" $( git symbolic-ref HEAD 2>/dev/null ) \" branch = \" ` basename $b ` \" program = $* reset_branch () { git co $branch --quiet } git rev-list origin/ ${ branch -master } .. ${ branch -master } | while read rev ; do trap \"exit 1\" SIGINT SIGTERM echo\n  echo \"Running at revision $rev \" echo git co $rev --quiet && git clean -fd && $program echo\n  trap - SIGINT SIGTERM done reset_branch To use, just drop that in a file in your $PATH , and make sure it is executable. Thanks to Gary Bernhardt for the scripts' inspiration, run-command-on-git-revisions , which you can see in his\ndotfiles . Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-05-16"},
{"website": "Hash-Rocket", "title": "Trip Report: Magic Ruby 2011", "author": ["\nRogelio J. Samour\n\n"], "link": "https://hashrocket.com/blog/posts/trip-report-magic-ruby-2011", "abstract": "Community Trip Report: Magic Ruby 2011 by\nRogelio J. Samour\n\non\nMarch 10, 2011 On February 4th, 2011 I got to go hang out with my fellow Rubyists at Magic Ruby Conf in Orlando, FL! It was fun getting to see lots of familiar faces and to hear what other peeps are working on. Former Rocketeer, Les Hill's talk on 'Cuking it right' was a great summary of the integration testing we've been doing here at Hashrocket. Chad Fowler's keynote was, as always, insightful and thought-provoking! Avdi Grimm's Exceptional Ruby talk was a journey through Ruby's exception mechanism, exploring many ways of taming this beast. The conference was held at Disney's Contemporary Resort in Orlando, FL -- we were able to get an awesome group rate for the resort hotel, which also provided quick access to the parks. It's definitely worth the trip for any level of Rubyist and their family! Make sure to check out: Les' Cultivating Cucumber slides Avdi's Exceptional Ruby slides Was this post helpful? Share it with others. Tweet Share Post", "date": "2011-03-10"},
{"website": "Hash-Rocket", "title": "Ruby5 Roundup - Episode 500", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/ruby5-roundup-episode-500", "abstract": "Ruby5 Roundup - Episode 500 by\nPaul Elliott\n\non\nOctober  3, 2014 Lynn and I are back after a longer-than-normal break to being you the latest news in the Ruby and Rails communities. Here is a quick roundup of what's new in this episode of the Ruby5 podcast. http://ruby5.envylabs.com/episodes/537-episode-500-october-3rd-2014 Greenscreen.io http://greenscreen.io Greenscreen.io is a new open source project from Groupon that lets you command the Chromecasts on your network via a handy web interface. It is easy to set up and has lots of cool options. If you have some TVs in your office, you can easily turn them into anything you want with this little node.js app! Onboarding Junior Developers https://ninefold.com/blog/2014/09/23/onboarding-junior-developers As more and more people graduate from boot camps, more and more of our ranks are filled with junior developers. This blog post from NineFold provides and overview and commentary of a great conference talk given on the topic. 10 Skills to Make You a Better Rubyist https://www.amberbit.com/blog/2014/9/29/10-skills-that-will-make-you-better-ruby-developer We need to know quite a bit beyond programming to be successful professionals in this industry. This blog post from AmberBit is great for junior devs wondering where to get started. Adding a Staging Environment to Rails http://emaxime.com/2014/adding-a-staging-environment-to-rails.html This blog post talks about how to set up a staging environment to test changes before pushing to prod. He has some good tips in here, most notably how to override email delivery in staging. Nothing worse than accidentally inviting real users to your volatile staging environment! Absolutes as an AntiPattern http://brandonhilkert.com/blog/absolutes-as-an-antipattern I never want to hear you talk in absolutes! This post from Brandon Hilkert talks about the use of absolutes when discussing software and why you shouldn't. So that's it for this episode of Ruby5. If you haven't already, subscribe to the podcast and keep yourself up to date. Thanks for listening! Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-10-03"},
{"website": "Hash-Rocket", "title": "Deferring constraints in PostgreSQL", "author": ["\nJoshua Davey\n\n"], "link": "https://hashrocket.com/blog/posts/deferring-database-constraints", "abstract": "Ruby PostgreSQL Deferring constraints in PostgreSQL by\nJoshua Davey\n\non\nAugust 22, 2013 Database constraints are essential to ensuring data integrity, and you\nshould use them. Allowing them to be deferrable during transactions\nmakes them even more convenient. A common scenario in which the database\ncan help us is in a sortable list implementation. This post outlines\nthe how and why of deferring database constraints, using a sortable list\ndomain as an example. Modeling lists of sortable items Imagine that you have an application with multiple lists. Each list has\nitems that can be reordered with a drag-and-drop interaction. This can\nbe modelled in a fairly straightforward manner. Each list has_many list items, which are ordered by the position column. Each list's items have a position beginning with 1 and\nincrementing with each subsequent item. # app/models/list.rb class List < ActiveRecord :: Base has_many :items , -> { order :position }, class_name: \"ListItem\" validates_presence_of :name end # app/models/list_item.rb class ListItem < ActiveRecord :: Base belongs_to :list validates_presence_of :name , :list , :position before_validation :ensure_position def self . update_positions ( ids ) ids . each_with_index do | id , index | where ( id: id ). update_all ( position: index + 1 ) end end private def ensure_position self . position ||= self . class . where ( list_id: list_id ). maximum ( :position ). to_i + 1 end end A couple things are worth noting about the ListItem class. Firstly, we\nhave update_positions , a class method that accepts an array of ids and\nupdates each. This method will be called in a sort controller action\nas such: class ItemsController < ApplicationController expose ( :list ) def sort # list item ids is an ordered array of ids list . items . update_positions ( params [ :list_item_ids ]) head :ok end end Secondly, new items don't necessarily know what position they should\nhave, so we put list items that don't have position at the end of\ntheir respective list, just before validating that the position is\npresent. Here are the migrations that we used to create the models' database\ntables: class CreateLists < ActiveRecord :: Migration def change create_table :lists do | t | t . string :name t . timestamps end end end class CreateListItems < ActiveRecord :: Migration def change create_table :list_items do | t | t . belongs_to :list t . integer :position t . string :name t . timestamps end end end Notice anything missing? If you said database constraints, you're\ncorrect! Our application is enforcing presence for most attributes, but\nour corresponding columns are missing NOT NULL constraints. Also, the list_id column on list_items is missing a foreign key constraint. But I'd like to focus on another missing constraint. Our domain model has\nan implicit requirement that we haven't enforced with either validations\nor database constraints: each list item's position should be unique per\nlist . No two list items in a list should have the same position. That\nwould make the ordering non-deterministic. We could add a uniqueness validation for position , scoped to\nthe list_id . However, as thoughtbot recently warned ,\napplication-level uniqueness validations are insufficient at best , and\nfail completely in concurrent environments. The position column needs a database-level constraint. Adding constraints Adding the uniqueness constraint to position is fairly straightforward\nin PostgreSQL. We'll just create a new migration with the following: class AddUniquenessValidationOnListItems < ActiveRecord :: Migration def up execute <<- SQL alter table list_items\n        add constraint list_item_position unique (list_id, position); SQL end def down execute <<- SQL alter table list_items\n        drop constraint if exists list_item_position; SQL end end Let's wrap our UPDATE statements in a transaction so that any failed UPDATE of the position column will result in none of them being\nupdated: class ListItem < ActiveRecord :: Base # ... def self . update_positions ( ids ) transaction do ids . each_with_index do | id , index | where ( id: id ). update_all ( position: index + 1 ) end end end end This ensures at the database level that positions of items are unique\nper list; no two items in the same list can occupy the \"1\" position.\nWith regard to data integrity, this is a huge improvement over our\ninitial implementation. But it has one drawback: it doesn't work . To illustrate why, imagine a list with the following items: id | position | name\n13 | 1        | Eggs\n18 | 2        | Milk\n35 | 3        | Bread To move Bread to the top of the list, we would pass an array of ids, [35,13,18] to the update_positions method. This method does a series\nof UPDATE statements to the database. For the first id, the one for\nBread, we end up sending an update statement that would look like the\nfollowing: UPDATE list_items SET position = 1 WHERE id = 35 ; After this statement is executed in the database, but before we move on\nto the next id in the list, Postgres will fail its constraint checks. At\nthe moment that the UPDATE happens, the data would be: id | position | name\n13 | 1        | Eggs\n18 | 2        | Milk\n35 | 1        | Bread With both Eggs and Bread occupying the same position, the UPDATE fails.\nOf course, we know that we want to change the position of Eggs as well,\nso that its position would be \"2\", and that collision would not happen.\nBut at the time that the constraint-check happens, the database doesn't\nknow this. Even within a transaction, database uniqueness constraints are enforced\nimmediately per row . It seems our dreams of data integrity are\nsmashed. If only there were a way to enforce uniqueness constraints at\nthe end of the transaction, rather than the end of each statement... Deferrable Constraints As mentioned before, constraints are immediately enforced. This\nbehavior can be changed within a transaction by changing a constraints\ndeferrable characteristics. In PostgreSQL, constraints are assumed to\nbe NOT DEFERRABLE by default. However, constraints can also behave as deferrable in one of two ways: DEFERRABLE INITIALLY IMMEDIATE or DEFERRABLE INITIALLY DEFERRED .\nThe first part, DEFERRABLE , is what allows the database constraint\nbehavior to change within transactions. The second part describes what\nthe default behavior will be within a transaction. With a constraint that is deferrable, but initially immediate, the\nconstraint will by default behave just like a non-deferrable constraint,\nchecking every statement immediately. A constraint that is initially\ndeferred will, by default, defer its checks until the transaction is\ncommitted. Both of these can change their behavior per-transaction with a\ncall to SET CONSTRAINTS ( documentation ). With that information, let's change the definition of the constraint we\ndefined before: class AddUniquenessValidationOnListItems < ActiveRecord :: Migration def up execute <<- SQL alter table list_items\n        add constraint list_item_position unique (list_id, position)\n        DEFERRABLE INITIALLY IMMEDIATE; SQL end def down execute <<- SQL alter table list_items\n        drop constraint if exists list_item_position; SQL end end The only thing we've changed from before is the DEFERRABLE INITIALLY\nIMMEDIATE bit. I think it is a good idea to use the INITIALLY\nIMMEDIATE option. This will ensure that other parts of our app, and\nother consumers of the database will not be surprised by the behavior of\nthe constraint; it will continue to act a like a normal, non-deferrable\nconstraint, until we explicitly opt in to the deferral. We now need to change our transaction block. In our case, the first\ndatabase statement within the transaction must be the SET CONSTRAINTS statement: class ListItem < ActiveRecord :: Base # ... def self . update_positions ( ids ) transaction do connection . execute \"SET CONSTRAINTS list_item_position DEFERRED\" ids . each_with_index do | id , index | where ( id: id ). update_all ( position: index + 1 ) end end end end Having now opted in to deferring our uniqueness constraint, reordering\nthe items now works as expected. The constraint still ensures that we\ndon't have two items that occupy the same position, but waits until the\nend of the transaction to do that check. We can have our data integrity\ncake and eat it too. Bonus: deferrable introspection Having to name the constraint in two places is a bit of a bummer, and\nintroduces a coupling that could bite us if the constraint name ever\nchanged. Knowing that, we leverage PostgreSQL's introspective abilities\nto query the constraint names instead. For example, we can add the following module to our codebase: # lib/deferrable.rb module Deferrable def deferrable_uniqueness_constraints_on ( column_name ) usage = Arel :: Table . new 'information_schema.constraint_column_usage' constraint = Arel :: Table . new 'pg_constraint' arel = usage . project ( usage [ :constraint_name ]) . join ( constraint ). on ( usage [ :constraint_name ]. eq ( constraint [ :conname ])) . where ( ( constraint [ :contype ]. eq ( 'u' )) . and ( constraint [ :condeferrable ]) . and ( usage [ :table_name ]. eq ( table_name )) . and ( usage [ :column_name ]. eq ( column_name )) ) connection . select_values arel end def transaction_with_deferred_constraints_on ( column_name ) transaction do constraints = deferrable_uniqueness_constraints_on ( column_name ). join \",\" connection . execute ( \"SET CONSTRAINTS %s DEFERRED\" % constraints ) yield end end end And now change our model to use it: class ListItem < ActiveRecord :: Base extend Deferrable # ... def self . update_positions ( ids ) transaction_with_deferred_constraints_on ( :position ) do ids . each_with_index do | id , index | where ( id: id ). update_all ( position: index + 1 ) end end end end And, boom! Less coupling. NOTE That's a lot of Arel! Use at your own risk. ;-) Example application While writing this post, I created a sample Rails app to iterate\nquickly. I used TDD to write the initial approach, and reused the specs\nwhile I \"refactored\" the implementation to the subsequent approaches.\nEach commit on the master branch more or less follows the\nsections above. Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-08-22"},
{"website": "Hash-Rocket", "title": "Hashrocket Stands Up", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/hashrocket-stands-up", "abstract": "Process Hashrocket Stands Up by\nJake Worth\n\non\nDecember  8, 2016 At Hashrocket, a vital part of our process is standing up together every day. In 2006, Jason Yip wrote a thorough overview of the Agile/Scrum meeting\nstyle known as the 'standup'. It's available here . Yip concludes the article with the accurate observation that \"[i]t’s really\njust standing up together every day\". While true, his arguments reveal the subtlety behind this simple idea. It would be hard to improve on this article; I'd rather talk\nabout an example of this technique in the real world, here at Hashrocket. In this blog post, I will share the details of the two (and usually only two)\nmeetings our team members participate in each day— a standup with the\nHashrocket team, and a standup with our clients. Why We Stand Up Yip lists outcomes a standup can achieve; they are, to\nparaphrase: Shared understanding of goals Coordinated efforts Sharing problems and improvements Identify as a team At this point, I'd like to separate our standup procedures into the\nHashrocket internal standup, and the client standup, because they are \na little bit different. Hashrocket Internal Standup Our internal standup happens via Skype each day at 9 AM CST/10 AM EST,\nto accommodate employees in multiple time zones. We meet where work happens; right\nin our development spaces. It's an all-hands event; the only people who don't participate are those on\nclient sites or already engaged in client standups. We strive to get\neverybody on the call, even those working remotely. This includes visiting\nclient developers, candidates participating in our week-long hiring audition,\nand even our CEO. The order is round-robin with no set starting position. Speaking control is\npassed off by social cues, or, when the entire team is in Jacksonville Beach,\nvia a deflated and surprisingly heavy miniature basketball. This is the 'Pass\nthe Token' technique Yip describes. Since almost nobody is working on the same project or application, we\navoid the 'Yesterday Today Obstacles' pattern and focus instead on\nour general work, excitement, frustrations, office announcements, and\npersonal news. Weather reports, puns, and long-running jokes are common.\n'Identify as a team' is the focus of this meeting, and it works. We keep the energy level high by standing up, and keeping it brief. The entire\nmeeting, with often fifteen people talking, lasts a few minutes. Hashrocket Client Standups Every day, each of our clients has their own standup with their dedicated\nteam of developers, designers, and a product manager. It's also an all-hands\nevent for each team. We connect with clients on their terms— Skype, Google Hangout, GoToMeeting;\nwhatever provides a sustained connection. We meet at the same time every day,\nas close to the start of business as possible. Often our developers speak first, sticking to the 'Yesterday Today Obstacles'\npattern. Here's how I would deliver a report in this style, using the Agile\nterm story to represent a unit of work: \"Yesterday we delivered the admin inventory deletion stories and started work\non the admin inventory create stories. Today we plan to finish those stories\nand deploy to production. In order to deploy, we're going to need\naccess to your Heroku application and AWS.\" Pending questions from the client, that's it. This is\nfollowed by reports from other developer teams and designers, and a wrap-up by\nthe product manager. Energy level is harder to control on a client standup, because almost nobody is\nin the same room as one another, so it's easier to lose track of time. We\nemploy skilled product managers who do a great job at keeping everyone on task\nand pushing side conversations offline. These calls are all about the stakeholder— delivering an update, explaining\nobstacles, and asking the rare question that must be addressed in\nreal-time. It's a level playing field, but the client drives the meeting and\nhas the final say about when it is complete. Client standups also ensure that they always know what we are working on. It gives the client the opportunity to course-correct and allows us the chance to quickly eliminate any confusion we might have about work for that day. Conclusion Our standup practice is one of my favorite things about Hashrocket. It helps us\nunderstand goals, coordinate effort on problem areas, share problems and\nimprovements, and coalesce as a team. Within the tools described in Jason Yip's article, there lies an effective\nstandup strategy for any group. Experiment and find something that works. Cited: Yip, Jason. \"It's Not Just Standing Up: Patterns for Daily Standup Meetings\", martinfowler.com , 21 February, 2016, http://www.martinfowler.com/articles/itsNotJustStandingUp.html . Accessed 6 December 2016. Photo Credit: Margarida CSilva, unsplash.com , https://unsplash.com/photos/cQCqoTjr0B4 . Accessed 7 December 2016. Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-12-08"},
{"website": "Hash-Rocket", "title": "Why Vim?", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/why-vim", "abstract": "Vim Why Vim? by\nJake Worth\n\non\nSeptember 28, 2017 Hashrocket is known as a Vim shop; what does that mean? When visitors learn Hashrocket is a \"100% Vim shop\", questions follow: Does everyone use Vim? Is Vim mandatory at Hashrocket? Why Vim? These are fair, and I admit that from the outside, our company's\npreference for Vim might seem unusual. When we evangelize Vim to the development\nworld, via the Vim Chicago Meetup and\nothers, the questions become more acute. In a recent\ntalk I tried to sell Vim to\nReact.js developers. I feel obligated to have compelling, succinct\nanswers on hand. I'll tackle this subject in three parts: why Hashrocket uses Vim, what\nVim is, and why we think every developer should try Vim. Few subjects\nare as contentious in the development world as text editors. Whatever you use,\nlearn and make it your own. I hope to simply add Vim to the list of things\npeople are willing to consider, if it's not there already. Hashrocket + Vim Hashrocket and Vim have a long history. We love this 25-year-old text editor.\nMany of our blog posts , TILs , and open source projects over the years\nhave been Vim-focused. According to Hashrocket lore, Vim was brought to our team by an early\nHashrocket employee, Tim Pope. Tim is the author of many widely used Vim\nplugins. His passion catalyzed a shift from the editor of the day, TextMate,\ntoward its retro predecessor. We are known as a Vim shop; what does that mean? It means that everybody\nwho works at Hashrocket either is or becomes a Vim expert. Vim isn't required\nfor our job applicants, but it doesn't hurt and can be an impressive data point\nwhen used well. Who cares what tools people use? Because we pair program most of the time. People can and do\nprefer other editors, but when client code is being written involving almost\nany two developers on our team, Vim is the shared workbench. In a profession of nonstop decision making, it's one less to make. Our passion translates into sponsorship of the Vim Chicago\nMeetup . We meet on the third Tuesday of\nthe month for long form presentations, lightning talks,\nand hack nights. Let's backtrack: what is Vim? What is Vim? According to its website, Vim is: A FOSS advanced text editor released by Bram Moolenaar in 1991. Based in\nthe Vi text editor. Vim has three facets that make it stand out. It is: Modal . Highly configurable . Legendarily challenging . Let's explore each in greater detail. Vim is Modal Modality was Vim's big idea. To paraphrase my coworker Chris Erin, developers\nspend more time reading code than writing it. Thus, an editor should be\noptimized for what we do most often . Your mileage may vary; I feel that I spend, on a tough bug, maybe ten minutes\nreading code for every one minute that I spend writing it.  Most text editors\nare optimized for writing, so the easiest thing to do is the thing I do the\nleast. Vim is Highly Configurable Vim has many paths to customization: Settings (via your ~/.vimrc ) Mappings (such as nnoremap ) Functions Plugins Each of these could fill a book on its own. Customization is a feature of most\ntext editors, but the vibrant OSS ecosystem and bottomless customizability of\nVim is unmatched. Vim is Legendarily Challenging Vim is perceived as an editor where the \"basics are hard\", and I don't\ndisagree, at first. See 'How to exit the Vim\neditor?' ,\na trivial question with over 1 million views on Stack Overflow. This is the\nso-called 'vertical learning curve'; it's real and every developer who uses Vim\nhas had\nto confront it. Why Vim? There a lot of potential answers to this question: Vim's speed, wide adoption,\nand depth are all attractive. For me, the best argument is\nflexibility. Vim is a blank, endlessly configurable slate. It's written in a language called\nVimScript, which itself has plenty of dark caverns you can spelunk to your\nheart's content. The vibrant OSS community behind Vim means it's constantly\nrising to every development challenge that appears. A developer's Vim setup\nis a reflection of the things they value, as personal as a fingerprint. There are a lot of other reasons to consider, and if you come to Vim Chicago\nMeetup , you'll be exposed to and able to\noffer more of them. Conclusion I hope this post has helped give some insight into what Vim means to Hashrocket. If you come pair with us, prepare to learn some new Vim tricks,\nand teach us some, too. Photo credit: https://unsplash.com/photos/w5inNVfSyNU Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-09-28"},
{"website": "Hash-Rocket", "title": "Ruby5 Roundup - Episode 493", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/ruby5-roundup-episode-493", "abstract": "Ruby5 Roundup - Episode 493 by\nPaul Elliott\n\non\nSeptember  5, 2014 I know you weren't expecting to hear us again so soon, but Lynn and I are back with the latest news in the Ruby and Rails communities. Here is a quick roundup of what's new in this episode of the Ruby5 podcast. http://ruby5.envylabs.com/episodes/530-episode-493-september-5th-2014 Reading Rails - TimeWithZone http://monkeyandcrow.com/blog/reading_rails_time_with_zone Working with time zones in Rails causing you to drink? A better understanding of how they work may force you to find another reason for it! This blog post is just what you need. descriptive_statistics https://github.com/thirtysixthspan/descriptive_statistics Performing statistical functions has never been easier thanks to the descriptive_statistics gem from Derrick Parkhurst. ActiveJob and GlobalID http://dev.mikamai.com/post/96343027199/rails-4-2-new-gems-active-job-and-global-id Rails 4.2 comes packed with new features. This blog post will walk you through ActiveJob and GlobalID, two of the most exciting parts of the release! Paperdragon http://nicksda.apotomo.de/2014/08/paperdragon-explicit-image-processing Tired of uploader libraries that do all the hard work for you? Paperdragon enables you to do it your way when you need to work outside the standard box. Ruby's English Operators http://devblog.avdi.org/2014/08/26/how-to-use-rubys-english-andor-operators-without-going-nuts Avdi Grimm released a RubyTapas episode on Ruby's English and and or operators for free to everyone. You NEED to watch it! So that's it for this episode of Ruby5. If you haven't already, subscribe to the podcast and keep yourself up to date. Thanks for listening! Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-09-05"},
{"website": "Hash-Rocket", "title": "Mocaroni", "author": ["\nMicah Cooper\n\n"], "link": "https://hashrocket.com/blog/posts/mocaroni", "abstract": "Mocaroni by\nMicah Cooper\n\non\nOctober  2, 2014 With the help of the team here at Hashrocket, I've developed a product called Mocaroni for working with APIs. The idea came from the issues I've run into while working on projects dealing with unfinished APIs. So, here's the story. I hope it gets your noodle going. As a consultant at Hashrocket, I get to work on many different projects.  Oftentimes, these projects involve one or more Web APIs. If I'm lucky, I get to work with JSON and REST. If I'm less lucky, it's XML, SOAP, or SOAP requests that have been munged into some form of JSON. Better yet, is an API that calls itself RESTful but is really just every parameter stuck into the url ... but I digress – working with APIs is actually quite fun, but comes with its own set of challenges, and any help I can find is a plus. Now, depending on the project, I'm either creating or consuming an API.  And no matter what side of the API I'm on, it can be tough to coordinate the development of these services so they line up properly in the end. Without a well established and disciplined process of communication, publishers of an API end up wasting time guessing what a consumer might need while the consumer is stuck trying to develop against a moving target. I think this happens with many projects because this kind of discipline is very difficult to establish – and it takes a long time. I've run into this a number of times over the years, and I wanted to try and solve this problem (as I do with anything that is painful), so I started thinking: Wouldn't it be nice if the consumer of an API could sit down with the producer and figure out how to meet their respective needs? (I know it sounds simple, but you'd be surprised ...) What if there was some way for everyone to collaborate on the behavior of their new API?  (And I don't mean just sharing a Github repo with a bunch of JSON requests and responses - that seems like a great idea at first, but will quickly become unwieldy.) Enter Mocaroni . It makes planning and sharing your Web API easy. Really easy.  It's a tool that lets you collaborate on the request/response cycles of your API. But Mocaroni isn't just a place for planning. It's a tool for execution. At any point, Mocaroni can generate a single-file Sinatra app of your entire project. Don't wanna deal with a download? No problem, ping Mocaroni's service directly. That's right - your Mocaroni project is a real, live API that you can use in your test suite, ember app, mobile app, or what ever else you're building. So, what's another pain point of writing an API? How about keeping all your docs in a row? It's so easy for documentation to fall out of sync with its  app. And there are tools out there like RSpec API Documentation . But what if you didn't even have to do that?! Well with Mocaroni, you don't - public documentation is generated for you automatically. In conclusion, creating an API is a pain: planning out the service, then writing a simple static api with that plan, then documenting the API, and keeping it all in sync? Whoof . Too much work.  Just do it once with Mocaroni and never look back. When when you go to actually build the thing, it'll be much more pleasant. So check out Mocaroni and its documentation . See if it's a useful tool for your Web APIs. If you wanna see new features for it, just ask - I'll stick them in. Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-10-02"},
{"website": "Hash-Rocket", "title": "Drop-in Responsive Styles With Sass", "author": ["\nCameron Daigle\n\n"], "link": "https://hashrocket.com/blog/posts/drop-in-responsive-styles-with-sass", "abstract": "Design Drop-in Responsive Styles With Sass by\nCameron Daigle\n\non\nSeptember  4, 2014 Organizing responsive styles into your existing stylesheet is always a challenge, but a simple Sass mixin can provide you with a dead-simple way to inject responsiveness wherever it's needed. Here's the scenario. You've got most of your responsive styles specified within @media calls, but sometimes, even nesting responsive styles feels like overkill. When only one or two items are changed responsively, lines of code can add up quickly. For example, say I have an element with a bunch of styles that only needs margin and font size to change at 800px or smaller: .my_cool_element font-size : 24px margin-bottom : 40px // a ton of other stuff here @media screen and ( max-width : 800px ) font-size : 18px margin-bottom : 20px There's a simple readability issue here: there might be a lot of lines of code in between the original and the responsive styles. Plus, if I want new styles at a second breakpoint (e.g. 480px for mobile), I'll need to list each attribute a third time under a new @media call. This got me thinking. Given that I was finding myself using at most two breakpoints (roughly tablet-sized and roughly mobile-sized), and very often was only changing a few attributes per selector, was there a way to knock out all possible attribute values on one line? Enter the responsive mixin. = responsive ( $ attr , $ full , $ mid :false , $ narrow :false ) #{ $attr } : #{ $full } @if $mid @media screen and ( max-width : 800px ) #{ $attr } : #{ $mid } @if $narrow @media screen and ( max-width : 480px ) #{ $attr } : #{ $narrow } Here's the magic: with this mixin, I can specify all of my breakpoint values in one place. I just pass the attribute to responsive , along with each breakpoint's value. .my_cool_element +responsive ( font-size , 24px , 20px ) +responsive ( margin-bottom , 40px , 20px ) font-size : 24px margin-bottom : 40px So if I need to add more attributes at the 480px breakpoint, I can just pass a third value to the appropriate +responsive call: .my_cool_element +responsive ( font-size , 24px , 20px , 16px ) +responsive ( margin-bottom , 40px , 20px , 10px ) font-size : 24px margin-bottom : 40px I've found this technique to be tremendously useful for sprinkling my stylesheet with responsiveness. (I also recommend abstracting your breakpoint sizes into Sass variables, but that's outside the scope of this post.) Obviously you'll encounter scenarios in which you're changing 5 or 6 attributes, at which point that element will have outgrown this technique – but once I started approaching responsiveness with this mixin, I found it was cutting down on lots of verbose @media nesting. So, give +responsive a shot! Maybe it'll make your life easier. Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-09-04"},
{"website": "Hash-Rocket", "title": "Setting up SSL with Dnsimple and Heroku's Endpoint SSL", "author": ["\nMicah Cooper\n\n"], "link": "https://hashrocket.com/blog/posts/ssl%20with%20dnsimple%20and%20heroku%20endpoint%20ssl", "abstract": "Ruby Setting up SSL with Dnsimple and Heroku's Endpoint SSL by\nMicah Cooper\n\non\nSeptember 20, 2012 Setting up SSL on a recent app proved to be more of a headache that I initially thought. It's a fairly straightforward but for one who is unfamiliar, it can seem quite daunting. Perhaps this will help. Setting up SSL with Heroku and DNSimple The other day I found myself working on an app that needed SSL. As I scoured the internet for helpful instructions, I had more trouble than I seemed necessary. So here is a guide. This is for using the Endpoint SSL with Heroku which allows you to a SSL to a subdomain like: https://secure.example.com Steps Buy a domain from DNSimple.com Add the CNAME for the record you want to apply the SSL to. (like \"www\" or \"secure\") Buy an SSL Certificate for that subdomain domain. (This can be done on the domains detail page). The Host Name is the same as the CNAME record you created in step note : If you buy the wildcard domain, the following instructions will not apply. This set of instructions is only for the hostname SSL. Go here for the wildcard domain. Now you should receive a series of emails (3 total) from RapidSSL. Once you have finished this process go back to Dnsimple and view the details of the SSL you just purchased. Copy the contents of \"Private Key\" (on your SSL details page) and paste them into a file called \"server.orig.key\" Follow the link called \"Instructions for Building a Rapid SSL Bundle\" Download the RapidSSL CA Bundle into a file called \"intermediate_ca.pem\". This download is located in the \"Apache, Plesk & CPanel\" section of the RapidSSL site. Remove the password from your server.orig.key file. open ssl -in server.orig.key -out server.key Now, using the heroku CLI (command line interface) Add the domain to your project: heroku domains:add secure.example.com -a myapp Add the SSL addon heroku addons:add ssl:endpoint -a myapp Configure your ssl on heroku. Add the server.key as the first argument, then the ssl.pem, then the domain name for the SSL heroku cert:add server.key ssl.pem secure.example.com -a myapp Now thats it. As far as getting your app to force the SSL. Thats another story\nand will be told another time. But this should take care of everything outside\nof the app. Clear as mud? Perfect... Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-09-20"},
{"website": "Hash-Rocket", "title": "Pairing Increases Code Quality", "author": ["\nChris Erin\n\n"], "link": "https://hashrocket.com/blog/posts/pairing-is-caring-about-code-quality", "abstract": "Process Pairing Increases Code Quality by\nChris Erin\n\non\nDecember  6, 2016 Pair programming reduces the amount of defects in a system. Pairing Increases Code Quality At Hashrocket we pair.  We pair by default, which means 80% of the time we pair 100% of the time.  It works great for us, but have you heard anybody espouse the productivity gains of pair programming recently?  There's not much on the internet about pair programming and what there is always attaches an adjective to pairing like, \"controversial\" or \"extreme\".  But at Hashrocket its not controversial, its not extreme, its productive.  It's productive because the design of the software gets better -- significantly -- and the number of bugs go down -- significantly. When a bug is discovered by a customer a range of things can occur.  In the best case, the bug is reported by the user, which takes time and patience from both the user and the organization to deal with effectively.  In the worst case, the user stops using the software, the bug is not reported, and this gets repeated as more users encounter the bug.  Bugs range from moderately expensive to very expensive, but in all cases a bug encountered by a user are much more expensive than bugs encountered and fixed before they reach production. Pairing is entirely counter intuitive.  Its hard for a product manager to realize the many smaller design challenges in every feature or even the trade-offs among the bigger design challenges.  Without those its hard to accept that a feature will take roughly the same amount of time to build but will cost twice as much.  If there were an imaginary defect count that could be reduced by investing in the software it would be straight forward, but the expectation in both non-pairing and pairing situations is that every developer create defect free software.  That's unrealistic. Defects are inherent in all software and even the biggest companies with the biggest budgets create amazing, head-scratching, dumb-founding bugs.  We've learned to tolerate it.  When an application unexpectedly closes, well, we just re-open it!  So if the biggest companies with the deepest pockets have issues with quality, can you expect an ordinary developer with ordinary skills to create extraordinary work?  Programming is a profession with big talent differences across all its practitioners, but rarely do the most talented think through all edge cases and even more rarely do they resist the myriad of web-based, human-based, work-based and life-based distractions that mass on the edge of every quiet, creative moment. Pair Programming reduces distraction based defects because a pair is always insistent that focus be given to the task at hand.  Its easier to follow software best practices when there is an ongoing discussion of best practices between the pair.  When a design choice is necessary, there are many more options and an even discussion of pros and cons for each because that's a natural thing to do.  Every code change must be packaged nicely and wrapped with a bow, when has anybody ever wanted another persons sloppiness to be reflective of them?  When pairing, the right thing must be done and the right questions asked.  And above all its progress.  Evenly-paced consistent progress. There are many advantages to pairing and I only touched on some of them.  It takes the right people and a number of cultural and structural elements to make it work.  The developers at Hashrocket like to pair and know how to work well together, and we have the code quality to prove it. Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-12-06"},
{"website": "Hash-Rocket", "title": "Developing with small tools", "author": ["\nMicah Cooper\n\n"], "link": "https://hashrocket.com/blog/posts/developing-with-small-tools", "abstract": "Ruby Developing with small tools by\nMicah Cooper\n\non\nOctober  9, 2012 Recently I've been exploring the idea of integrating very small, specific gems and generators into my development workflow. The idea here is to have a toolset of many small, single use, non configurable libraries to add convenience and speed to my development. I think most gems start out this way and then become larger and more configurable as more people use them. This is what I want to avoid. Not that big configurable gems don't have their place. To be honest, this works for most problems. But not always. Imagine an adjustable wrench. With an adjustable wrench you can make it fit any size nut and it works fine most of the time. But as soon as you try to unscrew that rusty bolt on the oil pan under your car with an adjustable wrench and strip it, you'll throw that stupid wrench through the first window you can find... stupid wrench... So, here are a few examples where I prefer something much smaller and non (or less) configurable. Authentication More often than not all I need for authentication is email, password, and maybe remember me. That's it. So I want a tool that does only that. If I need more, I can write it myself. Authem is a good small tool for this. It only does what I need and nothing more. It's very specific to my needs which makes the decision of whether or not to use less consequential (single responsibility). If it doesn't fit exactly, meaning I use every feature it contains, I'll use something else. Forms At Hashrocket, we have front end developers that write beautiful markup. They have a standard way of writing forms and there are parts of it that are the exact same for almost every form. So it seems natural to use a form builder that allows back end developers to quickly wire up a form that adheres to their standards. I know there are gems out there for writing forms. But some seem like more work than they are worth- I do not prefer learning a DSL that allows me to use one form helper that is endlessly configurable to create a <label> and a <input/> . I feel like I could have just written the two default rails helpers in the same amount of code and saved myself (and others) from learning a new DSL. Call me lazy, but I think the decision to spend a client's money learning, teaching, and implementing a new library that touches many parts of the app should be warranted. Many times it very much is. But not in this case. Not for me anyways. All that to say I prefer to use a simple form builder called formal that saves a little bit of time when writing forms. I can add it to a form with the option builder: Formal::Builder where it applies and write a normal form where it doesn't. It saves me the work of handling form errors, and wrapping the <input /> tags in a <dd> and that's it. It will never be more than this and I don't want it to. I would rather write another small library to handle different aspects of the form than add more features to this one. Anyways... This is a new way of thinking for me. Usually I just choose the de facto tool for the problem, learn it and mold it to my app. But between these solutions, I feel there is a place for much smaller and more specific tools and that's what I'm working for. Other ideas include using generators to build a very small and specific cms - Buildybuild . Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-10-09"},
{"website": "Hash-Rocket", "title": "Setting budgetary expectations, this is how we do it", "author": ["\nChris Cardello\n\n"], "link": "https://hashrocket.com/blog/posts/setting-budgetary-expectations-this-is-how-we-do-it", "abstract": "Process Setting budgetary expectations, this is how we do it by\nChris Cardello\n\non\nOctober 23, 2012 While time and materials is the right way to bill clients, it doesn't always make estimating cost easy. This is how we do it. So… how much do you think this is going to cost?  After a 30 to 90 minute conversation with a prospective client the question will inevitably be asked. It’s the moment any self respecting and honest consultant fears because you know your chances of landing the project could hang in the balance. So how do you respond? The fear is that any number you provide could be used against you should the project end up going over budget and at this point you just don't know enough about the project to provide any type of estimate. However, you have to tell them something or else they will take their business to someone who is willing to make up a number. So again.. what do you do? Tell the truth of course; politely inform the client that you do not have enough information to accurately gauge the scope of work. It is important to understand that at this point they are most likely trying to weed out shops who are completely out of their ballpark. So give them a number based on what it “feels” like compared to previous projects with the caveat that the number you provide is no more than an educated guess. If the number you provide is completely out of their ballpark than chances are it wouldn’t have been a good fit anyway. Reiterate that you will have a clearer view of the scope of work after a story carding session and even then the estimate will still only be an educated guess as the stories captured are only a snapshot into the requirements for the app at that time. In most cases that snapshot will change and evolve from week to week as both the project and client teams become more informed on the domain and feedback starts to trickle in from users. There are typically two times a client is going to ask about cost. The first time is outlined above, during the “sales process”. The reality is that a client does need an idea at the outset of a project of what the overall scope is going to be, no matter how flawed that estimation may be. Our job is to make the client aware of why the estimates are flawed, and that while projecting total scope is important it is much better to manage scope on an ongoing basis. The second time is post story carding prior to development kick off. Hopefully, you’ve been able to make this more manageable by having scope conversations during story carding . All throughout the story-carding and sales process we repeatedly communicate the difficulty in accurately projecting the cost of an application. By the time they get the estimate they should be aware that any numbers provided are merely conjecture. So how do you provide an estimate with any type of accuracy and feel confident about it? We are acutely aware that any number provided will be used as a point of discussion should budget become a concern. As such, what we like to do is paint the “worst case scenario” for a client and prepare them for it at the outset. So given that all stories average out to 2 points a piece we look at the number of feature stories and multiply by 2 to determine the amount of points which comprise the project as it sits at that time. We then increase the point total based on the average amount of unaccounted for scope we've seen over the 170+ projects we've worked to date. We then divide that point total by a conservative estimate of our weekly velocity. This will give us a projection for the amount of weeks in the project. We then multiple that by our weekly burn rate to estimate overall cost. This may be significantly higher than the client anticipated, however it makes them aware of what could happen... and we feel like we’ve been honest in setting expectations. We then tell them that we’ll most likely get the project done for less than the estimate. The key to which is working together to manage scope and always being aware of where we are in relation to feature completion and budget consumption. The idea isn’t to prepare the client to spend more than they want to but to prepare for that possibility from the outset, so they are diligent in working with us to manage scope. Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-10-23"},
{"website": "Hash-Rocket", "title": "S3 Asset Hosting Walkthrough", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/s3-asset-hosting-walkthrough", "abstract": "Ruby S3 Asset Hosting Walkthrough by\nPaul Elliott\n\non\nAugust  9, 2012 Are your servers hosting your static assets as well as your Rails app? That's a lot of unnecessary load if they are. The good news is that setting up your app to serve them from S3 is really easy with the asset pipeline. Follow these steps and you'll take a significant load off your server. Rails Configuration The first change is to enable the asset pipeline in your config/application.rb if you haven't already. config . assets . enabled = true config . assets . digest = true The next step is not necessary but will speed up the rake task tremendously. It will prevent the Rails app from initializing when you run the precompile task. This will be fine unless you are accessing Rails resources in your asset files. You'll find out real fast if you need to leave this on by running rake assets:precompile locally. config . assets . initialize_on_precompile = false Now you need to tell the pipeline what to include in the precompile. By default it will include the application.js and application.css manifest files and anything they reference. Typically these will include all your SASS and CoffeeScript files so you won't need to do anything else. If you have other assets to include, just add them to this array in your configuration. config . assets . precompile += %w(otherfile.js otherfile.css) Asset Sync Config Setting up the asset_sync gem is very easy. Of course we start with the Gemfile. Put the asset_sync gem in your :assets group along with the other asset related gems. group :assets do gem 'asset_sync' gem 'coffee-rails' gem 'sass-rails' gem 'uglifier' end Next you need to set up your production.rb to tell the asset pipeline where the assets will be located. config . action_controller . asset_host = \"// #{ ENV [ 'FOG_DIRECTORY' ] } .s3.amazonaws.com\" config . action_mailer . asset_host = \"http:// #{ ENV [ 'FOG_DIRECTORY' ] } .s3.amazonaws.com\" If you have a staging.rb or any other server environment, be sure to add it there as well. Now we need to set up the environment variables. Asset sync looks for certain ones by default, so if you name them this way then it will just work. You can use these to configure your file upload gem as well. AWS_ACCESS_KEY_ID = xxxx AWS_SECRET_ACCESS_KEY = xxxx FOG_DIRECTORY = <your s3 folder name> FOG_PROVIDER = AWS ASSET_SYNC_GZIP_COMPRESSION = true ASSET_SYNC_MANIFEST = true ASSET_SYNC_EXISTING_REMOTE_FILES = keep If you want to configure it in a different way or see the other options, check out the readme for the asset_sync gem. Using Assets Properly Setting everything up properly means keeping all your assets under app/assets . Make sure you don't have assets under public because they won't work once you switch over. You also need to use the tag helpers whenever you reference any assets. One cool thing is that setting asset host up for action mailer makes all the tag helpers work again. Without it they are all relative paths and broken for your users. Be sure to check out the complete docs on proper asset usage. Once you set up your staging environment, browse around and make sure you aren't getting any 404's where images should be. http://guides.rubyonrails.org/asset_pipeline.html#how-to-use-the-asset-pipeline Deployment Make a commit and deploy your app. If you are using Heroku the precompilation will happen automatically. If you have your own deployment script like capistrano then you will need to add the precompile step after the code deployment. Once it is set up then you should see a lot of activity in the log when the asset precompile step runs. DON'T USE CLOUDFRONT I know you'll be tempted to. I mean, it's a CDN, right? That's faster! Indeed it is, but it also caches resources for 24 hours, meaning that your app will be broken for a while after you deploy while you wait for the cache to refresh. Congratulations! That should be it. Make sure you read up on using assets correctly in your app, as it is very important to get the most out of the pipeline. Congrats on your performance boost! Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-08-09"},
{"website": "Hash-Rocket", "title": "Conference Wrap-Up: jQuery San Francisco", "author": ["\nShane Riley\n\n"], "link": "https://hashrocket.com/blog/posts/jquery-san-francisco", "abstract": "Javascript Community Conference Wrap-Up: jQuery San Francisco by\nShane Riley\n\non\nJuly 11, 2012 After many years of watching jQuery conference dates come and go without attending, this year I finally got to go, and as a speaker to boot. At jQuery conferences, you'd expect to meet all the big names in jQuery development, from Ben Alman to Jörn Zaefferer , and San Francisco did not let me down in this regard. Many of the speakers were people whose work I've followed for years, so it was nice to finally meet the people behind the innovations. By far, the most memorable person I met was Adam Sontag , a character and a half and someone who was made for conferences like this one. Nearly every talk felt targeted towards an intermediate to expert level audience, which I greatly appreciated but could see how that might have put some potential attendees off. Given the Bocoup training session that was held just before, I would have figured there would be one or two basic-level talks to start off day one. The event was single-track with a breakout room for lightning talks to whomever wanted to present, which I hope is a pattern more and more conferences adopt. I would have been a bit upset if I didn't get to see both the deferreds and game dev talks if it were a multi-track conference. The highlight of the conference, however, was the very first announcement that jQuery 2 would drop all support for IE8 and below. I couldn't tweet that information fast enough. We've never really had issues with supporting IE8 at Hashrocket, but I know that dropping support leaves a lot of room for more features while keeping the code footprint the same. I can't wait to see what new goodies are introduced once this takes place. It was also mentioned that a new custom builder would be released for jQuery similar to what is currently on the jQuery UI site. I don't know how fine-grained it will be, but it will be great to pare down the library to a core subset for my quick demos and leave methods like $.ajax out. For the evening of the first day, a semi-impromptu (scheduled during the speaker dinner the night before) hackathon was organized just after the drinkup in the first floor of the conference center. Over 60 people were hacking away at coding all of the newly designed jQuery websites, from the main site to the jQuery Foundation site. I chose to lead a group to code the jQuery Foundation site, jquery.org. Despite some initial hurdles, we were able to get a good start, and I was left so motivated to continue to help that I coded the remaining pages assigned to my group while waiting in the airport the morning after the conference. I'd love to show you the new designs, but there's still a lot of work to be done. However if you're curious enough, you can probably see the pages somewhere in my GitHub account ;) Overall, I think it was well worth the trip just for the conversations I had with other jQuery team members. I'd love to at least attend some of the other 12(!) jQuery conferences that are to be planned throughout the next year around the world, if not speak at them. Hopefully my next one will be the Toronto date! It's been a long time since I've been to Canada. Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-07-11"},
{"website": "Hash-Rocket", "title": "How to win developers and influence features as a designer.", "author": ["\nDaniel Ariza\n\n"], "link": "https://hashrocket.com/blog/posts/how-to-win-developers-and-influence-features-as-a-designer", "abstract": "Design How to win developers and influence features as a designer. by\nDaniel Ariza\n\non\nApril 24, 2012 Jokingly at Hashrocket we have defined the cycle of development as a three step process. We toss it around jokingly but the it's sort of ground in truth. First the devs complain about the feature. Then they implement it with great speed. Finally they congratulate themselves on a job well done. It's funny but there is some truth in there. As designer it's my job to set the project team up for success wherever I can. Design aesthetics are important but just as important is working with your project team. Even this development cycle allows for that ideal but most of all it comes down to trust. Unfortunately that's the part of the equation that takes the most time but here are some simple concepts that will put you on track. First the devs complain about the feature. If the dev team is complaining about a new feature it's a product of one of several conditions. They were totally unaware of the features existence. They disagree with the features value. It's a tedious task that they're not looking forward to it. The first scenario shouldn't happen. Not only should the entire team be aware of upcoming features but as a designer I should be having discussions with the dev team personally about the interactions I'm designing. I need to have enough humility to listen to others but the confidence to stand up for what I believe to be the right decisions. The second condition also is usually a product of poor communication. Getting the whole team on board with new features is invaluable. No one want's to work on something they see as being worthless. The \"why\" for any feature is important to the whole team not just me the designer. If you can get complete \"buy in\" from everyone it will go a long way. The last scenario is not always avoidable. Some features are necessary but not glamourous or fun to develop. Just like it's not always fun for me to design forms, it's not always fun to implement them. Sometimes we just have to \"suck it up\" and do the work that needs to be done. Don't worry. There are always some cool new problems to solve just over the horizon. They implement it with great speed. When it comes time to develop a new feature everything should be in place. By this time I should have briefed the developers on the feature, designed any and all assets and provided them with the markup, styles and javascript they're going to need. If you have everything ready for your devs, they will love you for it. They get to focus on what they're good at and you know everything will be prepared to your visual standards. Leaving things to chance or guess work is frustrating for your team and also yields a high rate of misfires. This leads me to something everyone hates. Doing things over. There is a difference between changing a feature based on user feedback and something that was implemented wrong. If this happens it's on you as a designer. More than likely you failed to communicate or didn't provide all that was necessary. Take responsibility, learn from it and do it better next time. No excuses. Finally they congratulate themselves on a job well done. It's time to celebrate your victory. And more than likely you were a part of it since you set your team up for success. More than just a high-five will go a long way. Inquire about the challenges the devs faced. Don't just act interested. Be interested. It's invaluable for building relationships and by now you should have checked your ego at the door. Caring and respecting each others roles and work is the only way to build trust. If you want to step on toes and belittle your teams roles then you're creating unnecessary hurdles for yourself and working with your project team. The simple conclusion. It seems so simple. Communicate, provide the necessary assets and celebrate your victories. But more often then not when we have visiting developers or designers they are blown away by the relationships and dynamic we have within our team. It may appear easy on the outside but I assure you we've worked hard to have the level of mutual respect and trust that exists between us. So designers, embrace the cycle of development and set your developers up for success. In return you'll have a better product and mutual respect from your team. Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-04-24"},
{"website": "Hash-Rocket", "title": "Javascript, Rails, Google Maps!", "author": ["\nJohnny Winn\n\n"], "link": "https://hashrocket.com/blog/posts/javascript-rails-google-maps", "abstract": "Ruby Javascript Javascript, Rails, Google Maps! by\nJohnny Winn\n\non\nAugust 29, 2012 For anyone familiar with a certain little backpack wearing explorer, the layout of this post will seem eerily reminiscent of an episode. Not only because of her favorite tool, the map, but also because we are following her three step method: JavaScript, Rails, Goo-gle-maps! (I can hear her now) Lately at Hashrocket we have had a number of projects heavily dependent on the Google Maps API so it seemed like a great time to discuss one of these implementations, as well as some of the key points when integrating with that API. However, before we can begin we need to \"map\" out our journey. First we will geolocate our position using the Google Map API and javascript. Then we will use those results to query our rails services API for additional markers in the area. Finally we will return to JavaScript and add the markers and events to our map. So, come on vámonos! Every journey starts with the first step, and our first step is to include Google's JavaScript API URI path in our page heading. = javascript_include_tag \"http://maps.googleapis.com/maps/api/js?sensor=false\" This will give us all the Google-y goodness needed for our adventure. JavaScript & Google Geocoding API - the First Stop The Google Geocoding API is provided as part of the Google Maps API in order to retrieve geolocation data. The API provides access to the geocoder object through HTTP requests. The service can geolocate points by both a standard or a reverse lookup with either an address or coordinates. Once the Map API URI is included as a JavaScript tag, accessing the Geocoder object is simple but we need to look at the results set that is returned. var geocoder = new google . maps . Geocoder (); geocoder . geocode ({ address : 'Jax Beach, FL' }, function ( results , status ) { // Do something with the results }); The API delivers back some valuable data in the form of a JSON object (XML is also an option) that can be easily parsed to provide some essential information. One side effect is address sanitization so the address Jax Beach, FL returns Jacksonville Beach, FL. In addition to having a clean address the response includes the geometry details like bounds, location point and suggested viewport bounds. Both the bounds and viewport are points in the Northeast and Southwest quadrants and define the rectangle that our supplied address appears within. The API provides a couple methods to get to those boundary points: getNorthEast() and getSouthWest() . The two methods will extract the bounding box points so that we can use them in out ajax request. var geocoder = new google . maps . Geocoder (); geocoder . geocode ({ address : 'Jax Beach, FL' }, function ( results , status ) { var bounds = results [ 0 ]. geometry . bounds , center = results [ 0 ]. geometry . location ; if ( bounds ) { var ne = bounds . getNorthEast (), sw = bounds . getSouthWest (), data = { sw : [ sw . lat (), sw . lng ()], ne : [ ne . lat (), ne . lng ()]}; // ajax call to rails service API } }); The Rails API - The Other Side Now that we have arrived on the other side of the tracks, we can discuss a couple of gems used to ease the implementation of our Rails API. Because querying the data comes before delivering it, we should start with the geokit-rails3 gem. This is a port of the geokit gem and provides a common interface for geolocation applications. It also allows us to make ActiveRecord models mappable simply by adding acts_as_mappable to our model. To query the database based on our supplied bounds, we use the geo_scope method. This is the gem's underlying method for scoped searches and we can chain methods to refine our result set like any other ActiveRecord query. def geo_search ( sw , ne ) self . geo_scope ( bounds: [ sw , ne ]). where ( active: true ) end Once we have retrieved the data from our database we can use the jBuilder gem to easily generate the JSON views. The setup is simple for our scenario and we can abstract some of the formatting to an application helper. Here is the format we will use in our result set: json . results do | json | json . array! ( searchables ) do | json , result | json . id result . id json . type result_type ( result ) json . name result . name json . lat result . latitude json . lng result . longitude json . url url_for ( result ) end end When you are testing the Rails service API with rspec, remember to add a call to render_views in your spec. Otherwise, the tests will not return the JSON views in the response. describe SearchablesController do render_views # add Fabricated records for testing context \"search spots\" do it \"returns our spot\" do get :index , { format: :json , sw: sw , ne: ne } data = JSON . parse ( response . body ) data [ 'results' ]. count . should eq ( 1 ) data [ 'results' ][ 0 ][ 'name' ]. should eq ( 'Jacksonville Beach Pier' ) end end end Putting Our Map Together Now that we have our results, we need to add them to a map object. To do that we need to define a map and give it a few options. Note the map center point is being set through the options. This can also be set after the map is drawn using the setCenter() method. (For the full list of options check out the Google Maps API Map Options ) var opts = { zoom : 10 , max_zoom : 16 , scrollwheel : false , center : new google . maps . LatLng ( center . lat (), center . lng ()), mapTypeId : google . maps . MapTypeId . ROADMAP , MapTypeControlOptions : { MapTypeIds : [ google . maps . MapTypeId . ROADMAP ] } }; var map = new google . maps . Map ( $ ( '#map' ), opts ); At this point we need to iterate through our results set and add markers for each point to the map. The Google API also provides a Marker object to define a marker and these can be customized to level your application's needs, including the addition of custom images. var marker = new google . maps . Marker ({ id : result . id , title : result . name , position : new google . maps . LatLng ( result . lat , result . lng ), cursor : 'pointer' , flat : false , icon : new google . maps . MarkerImage ( '/assets/our_custom_icon.png' , new google . maps . Size ( 32 , 35 ), new google . maps . Point ( 0 , 0 ), new google . maps . Point ( 32 / 2 , 35 ), new google . maps . Size ( 32 , 35 )) }) ); And there you have it! A map that has all our results that fall within a boundary. What more could you want? Oh yeah, when you move the map or zoom in/out the bounds change but our results don't. For that we need to use the Google Map API events, most notably the \"idle\" event. This event is fired when the map display goes idle. Although there are specific events for panning and zoom, we have found that the \"idle\" event works best for this scenario. google . maps . event . addListener ( map , 'idle' , function () { // call your geolocation method }); Our journey is now complete. We have a Google map that displays all the stored results that fall within a bounding box. The code here has been modified to fit neatly into a single post so its purpose is to be an example of Google Maps integration. We would suggest defining your map object using object literal notation so that the map functionality can be encapsulated and functions can be reused. Swiper, go swiping and have your own adventure with JavaScript, Rails, Goo-gle maps! Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-08-29"},
{"website": "Hash-Rocket", "title": "Two Coffeescript Oddities", "author": ["\nCameron Daigle\n\n"], "link": "https://hashrocket.com/blog/posts/two-coffeescript-oddities", "abstract": "Javascript Two Coffeescript Oddities by\nCameron Daigle\n\non\nFebruary  7, 2013 Here are a couple of interesting situations I encountered a while back while writing some Coffeescript. I was writing the QUnit test suite for jQuery.minical as an exploratory exercise to become more familiar with the language, and I ran across a couple of unexpected behaviors – not bugs, just ways Coffeescript's more minimal syntax can trip you up in ways that aren't immediately apparent if you're coming from the world of vanilla Javascript. $.map() and Coffeescript Aren't Friends One particular test in this plugin required that I verify the contents of the days of the week (it's a calendar plugin), which are contained in the <th> elements of the calendar (a table). This is essentially what we want to verify: $ ( \".calendar th\" ). text () == \"SunMonTueWedThuFriSat\" Well, that's fine, but kind of ugly. Just because .text() mashes everything together doesn't mean I should just compare against a mashed-up string. So, I thought a nice touch would be to compare the elements in an array, like this: $ ( \".calendar th\" ). getTextArray () == [ \"Sun\" , \"Mon\" , \"Tue\" , \"Wed\" , \"Thu\" , \"Fri\" , \"Sat\" ] Now, $.getTextArray() is the function I wrote for this, and it's kind of awkward in Coffeescript. The Javascript implementation is easy: use $.map() to create a jQuery object, and use $.get() to extract the native array. As the jQuery docs say: \"As the return value is a jQuery object, which contains an array, it's very common to call .get() on the result to work with a basic array.\" $ . fn . getTextArray = function () { return $ ( this ). map ( function () { return $ ( this ). text (); }). get (); } However, as far as I can tell, that output is undeniably awkward to execute in Coffeescript. This should work, right? $ . fn . getTextArray = -> $ ( @ ). map -> $ ( @ ). text (). get () Nope, that will call .get() within .map() : $ ( this ). map ( function () { return $ ( this ). text (). get (); }); Your options are to add parentheses, which is the most verbose javascript-like option ... $ ( \"li\" ). map ( -> $ ( @ ). text () ). get () ... outdent, which feels super-awkward (especially if you don't indent the third line at all, which evaluates accurately but looks confusing) ... $ ( \"li\" ). map -> $ ( @ ). text () . get () ... or use an extra set of parentheses (which actually outputs an extra set into the Javascript as well, but works fine): ( $ ( \"li\" ). map -> $ ( @ ). text ()). get () This isn't a deal breaker by any means, but if you're used to vanilla Javascript, you'll run into these issues where jQuery expects to operate in ways that require more parentheses than Coffeescript usually needs, and your code will fail in unpredictable ways. Update: I stand humbled by Tomas Carnecky's comment below with another (superior) option: $(\"li\").map(-> $(@).text()).get() does the job. Just goes to show how many suboptimal ways there are to refactor Javascript into Coffeescript. Don't Under-Parenthesize, if \"Parenthesize\" Is Even A Word Here's a more complex example. I wanted to put together an array of every day I expected to be in a particular calendar month view. This would require handwriting an array or running a bunch of loops in Javascript, but Coffeescript's comprehensions and array operations allows us to slam together the whole array in one line. days = (( day + \"\" ) for day in []. concat ([ 25 .. 30 ],[ 1 .. 31 ],[ 1 .. 5 ])) Woohoo! I love that to death. Coffeescript is occasionally great for reducing distracting code around something minor like constructing an arbitrary array. However, note the enclosing parentheses around the whole thing. If you remove those: days = ( day + \"\" ) for day in []. concat ([ 25 .. 30 ],[ 1 .. 31 ],[ 1 .. 5 ]) CRAZY THINGS happen, the results of which I will leave as an exercise for the reader . The bottom line here is that Coffeescript's minimal syntax will sometimes sneak up on you when you're not carefully keeping track of how it's evaluating your code. It's pretty easy to drop one too many sets of parentheses in your zeal to write concisely. This sort of extreme code breakage in the CS to JS handoff is the riskiest thing about using Coffeescript, as it can be difficult to bug fix, but if you've used Coffeescript at all, you're well aware of that, and hopefully this article has better prepared you for those situations. Happy Coffeescripting! Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-02-07"},
{"website": "Hash-Rocket", "title": "Ember on S3 with CloudFront - Less Hash More Cache", "author": ["\nMicah Cooper\nand\nTaylor Mock\n\n"], "link": "https://hashrocket.com/blog/posts/ember-on-s3-with-cloudfront-bash-the-hash", "abstract": "Ember Ember on S3 with CloudFront - Less Hash More Cache by\nMicah Cooper\nand\nTaylor Mock\n\non\nJanuary 28, 2016 How to host your ember application using Amazon S3 and Cloudfront and maintain your pretty URLs without having to redirect to a hashed url. We like deploying our Ember app to S3 with ember-cli-deploy . While cheap and fast, there is a caveat to this approach: When a user goes directly to a page on your Ember site, say https://emberweekend.com/episodes , S3 doesn't have the capacity to serve your index.html file. We initially solved this by setting redirect rules in our s3 static web hosting settings: <RoutingRules> <RoutingRule> <Condition> <HttpErrorCodeReturnedEquals> 404 </HttpErrorCodeReturnedEquals> </Condition> <Redirect> <HostName> emberweekend.com </HostName> <ReplaceKeyPrefixWith> #/ </ReplaceKeyPrefixWith> </Redirect> </RoutingRule> </RoutingRules> So, when we visit: https://emberweekend.com/episodes S3 will not be able to find that page (404) and redirect us to: https://emberweekend.com/#/episodes This returns the index.html file, which loads the Ember app. Then, the Ember app uses pushState() to change the url back to: http://emberweekend.com/episodes . Please note: This is how it's currently working as of this article's publish date. Subject to change. This works, but the frequently changing URLs bothered us, the status code still returned a 301 redirect, and our friends at Ember Weekend noticed Google was not indexing pages that were created after taking this approach. Attempting to reconcile these issues led us to Amazon CloudFront's custom error page configuration. We simply create a Custom Error Response that returns our /index.html with a status of 200 when we get a 404 : CloudFront -> <distribution-id> -> Error Pages -> Create Custom Error Response So now, if we were to visit: https://emberweekend.com/episodes Our S3 bucket will still return a 404 , but now Cloudfront will handle that 404 and return the /index.html file with a status of 200 instead. No redirects, no Hash URLs, and all is right with the world. Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-01-28"},
{"website": "Hash-Rocket", "title": "Using Ruby's Method Class for Fun and Profit", "author": ["\nNick Palaniuk\n\n"], "link": "https://hashrocket.com/blog/posts/using-rubys-method-class-for-fun-and-profit", "abstract": "Ruby Using Ruby's Method Class for Fun and Profit by\nNick Palaniuk\n\non\nFebruary  1, 2016 Lets try to use some of the methods in Class: Method First some setup, we will create a Foo module with a bar method\nand include it in the String class. module Foo def bar ( x ) self << \" #{ x } \" end end String . send ( :include , Foo ) We will start with a String instance. pry ( main ) > str = \"foo\" => \"foo\" We can create an instance of the Method class like so: pry ( main ) > bar_meth = str . method ( :bar ) => #<Method: String(Foo)#bar> If we wanted to we could invoke this with Method#call , but\nwe are more interested in the Method instance. pry ( main ) > bar_meth . call ( \"baz\" ) => \"foobaz\" We often want to know where a method is defined. We can use Method#source_location to find out. I loaded the initial Foo module\ninto the REPL in a file called 'wut.rb' pry ( main ) > bar_meth . source_location => [ \"wut.rb\" , 2 ] If the method is defined in the core library Method#source_location will return nil. pry ( main ) > str . method ( :length ). source_location => nil We could wrap this in a predicate method called core? and\nadd to Method module MethodHelpers def core? self . source_location . nil? end end Method . send ( :include , MethodHelpers ) pry ( main ) > bar_meth . core? => false We can check where the method of interest is scoped with Method#owner pry ( main ) > bar_meth . owner => Foo This means we can check the instance methods en masse for any\nmodules or external libraries in our inheritance chain. Here I will\ncreate a Hash with the owners as keys and the values an array of their \nmethods. pry ( main ) > str . methods . each_with_object ( Hash . new { | h , k | h [ k ] = []}) do | m , h | h [ str . method ( m ). owner ] << m end => { String => [ :<=> , :== , :=== , :eql? , :hash , :casecmp , ... Foo => [ :bar ] ... ]} We can add a little more information to this if we throw in the Method#arity and Method#parameters methods in there as well. pry ( main ) > str . methods . each_with_object ( Hash . new { | h , k | h [ k ] = []}) do | m , h | h [ str . method ( m ). owner ] << [ m , str . method ( m ). arity , str . method ( m ). parameters ] end => { String => [[ :<=> , 1 , [[ :req ]]], [ :== , 1 , [[ :req ]]], [ :=== , 1 , [[ :req ]]], [ :eql? , 1 , [[ :req ]]], [ :hash , 0 , []], [ :casecmp , 1 , [[ :req ]]], ... Foo => [[ :bar , 1 , [[ :req , :x ]]]], ... ]} Now let's create a new Object#methods . We will call this #method_list and I'll put it on object as well. It will look\nsimilar to the previous hash. I'm going to add Method#source_location to the values array and a parameter for \npassing a boolean if you want to exclude core methods from the \nlist. We can also significantly clean it up by iterating over\nan array of Method objects instead of the method name. module MethodList def method_list ( core = true ) total_meth = self . methods . map { | m | self . method ( m ) } non_core_meth = self . methods . map { | m | self . method ( m ) }. select { | m | ! m . core? } list = core ? total_meth : non_core_meth list . each_with_object ( Hash . new { | h , k | h [ k ] = []}) do | m , h | h [ m . owner ] << [ m . name , m . arity , m . parameters , m . source_location ] end end end Object . send ( :include , MethodList ) pry ( main ) > str . method_list ( false ) =>... Foo => [[ :bar , 1 , [[ :req , :x ]], [ \"wut.rb\" , 2 ]]], ... And there we have it - usefulness debatable. I find Method#source_location invaluable for looking up the source in gems, but I would be curious to hear \nhow other people are using any of the other methods in the Method class. Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-02-01"},
{"website": "Hash-Rocket", "title": "Create and Publish Your Own Elixir Mix Archives", "author": ["\nMicah Cooper\n\n"], "link": "https://hashrocket.com/blog/posts/create-and-publish-your-own-elixir-mix-archives", "abstract": "Elixir Phoenix Create and Publish Your Own Elixir Mix Archives by\nMicah Cooper\n\non\nOctober 25, 2016 A workflow for creating, publishing, and sharing mix archives. When I first got into Elixir , one thing that\nimmediately jumped out as me was the convenience of working with Mix . Mix makes elixir\ntooling, fast, simple, and fun! Working with Phoenix made this apparent to me. The\nworkflow to get started was a simple mix archive install and mix\nphoenix.new . So, when I started bulding my own elixir archive, Gatling , I wanted the setup to be that\neasy. Overview Let's learn about Mix archives; what they are, how we can create them, and what\nseems to be a good way to share then with the community. Mix Archives A Mix archive is essentially a .zip of your elixir project in the Erlang\nArchive format . But what does that mean\nfor us? Simply put, this allows us to install a mix arhive and use it\nlike a command line tool on our system. This is how Phoenix achieves the mix\nphoenix.new task on your system. We run mix archive.install\nhttps://github.com/phoenixframework/archives/raw/master/phoenix_new.ez , and\nMix downloads the archive from the given url, and adds it to ~/.mix/archives on your system. Build Your Own Archive Any time we create a new Elixir project with Mix by running mix new\n<project_name> , we can also create an archive with mix archive.build . This\nwill create an archive called <project_name>@x.x.x.ez . Share Your Archives Once you have built an archive, anyone can download and use it with mix. So, we\ncould just push it up to Github and call it a day. But we should do our\ncustomers/users justice by versioning our software; allowing users to revert\nback to an old version if they so choose. So, I'd like to have a separate\nrepository to store just the archives, and I want to make it convenient to add\nnew versions. So lets create a mix project and walk through a nice workflow to\nfacilitate the management of our mix archives. $ mix new foo #create a new mix project Now that we have our new project, here is what I want to do every time I\nrelease a new version of our archive. Update the version of our project Commit and push our project Build a new archive ( foo-x.x.x.ez file`). This will be the\nversioned/historical version of our archive. Build a new archive ( foo.ez file`) This will be the \"current version\" of\nour archive. Move both archives into a separate git repo called foo_archives Commit and push/publish the archive repository separately from project How it's Made Move into our ./foo dir and let's get started: $ cd foo\n$ mkdir foo_archives\n$ echo \"foo_archives/\" >> .gitignore # ignore our archives directory. It will be it's own repo\n$ git add .\n$ git commit -m 'Initial Commit'\n$ mkdir foo_archives\n$ cd foo_archives\n$ git init\n$ cd ../ Now, let's create a custom mix task in our project to simplify the creation\nof releases. Inside our our ./mix.exs file our project/0 function should look like this: def project do [ app: :foo , version: \" 0.1.0\" , elixir: \" ~> 1.3\" , build_embedded: Mix . env == :prod , start_permanent: Mix . env == :prod , deps: deps ()] end We're going to clean it up a little, pull the \"version\" out into a function so\nwe can use it later, and add an aliases option to it. def project do [ app: :foo , version: version , elixir: \" ~> 1.3\" , build_embedded: Mix . env == :prod , start_permanent: Mix . env == :prod , deps: deps (), aliases: aliases , ] end Define version/0 : def version , do : \" 1.0.0\" Define aliases/0 : defp aliases do [ build: [ & build_releases / 1 ], ] end This creates a mix task called build to our project. When we call $ mix\nbuild it will call a function named build_releasese/1 so lets define that: defp build_releases ( _ ) do Mix . Tasks . Compile . run ([]) Mix . Tasks . Archive . Build . run ([]) Mix . Tasks . Archive . Build . run ([ \" --output=foo.ez\" ]) File . rename ( \" foo.ez\" , \" ./foo_archives/foo.ez\" ) File . rename ( \" foo- #{ version } .ez\" , \" ./foo_archives/foo- #{ version } .ez\" ) end Because we want to create our archive in production mode, we explicitly compile\nin the build task since running mix tasks with MIX_ENV=prod does not compile\nautomatically. Then we create our two archives, and move them into foo_archives With this new build task, the workflow for publishing a new release would\nlook like this: $ MIX_ENV = prod mix build ` $ cd foo_archive $ git add . $ git commit \"Release vx.x.x\" $ git push origin master Now when users want to install the most recent version of your mix project from\n,say Github , the command would consistently look\nsomething like like: $ mix archive.install mix archive.install https://github.com/<username>/foo_archives/raw/master/foo.ez This is a very simple way to maintain historical versions as well as a default\nversion of your mix archives for fun and profit! Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-10-25"},
{"website": "Hash-Rocket", "title": "Survival Tips For Live Wireframing", "author": ["\nCameron Daigle\n\n"], "link": "https://hashrocket.com/blog/posts/survival-tips-for-live-wireframing", "abstract": "Design Survival Tips For Live Wireframing by\nCameron Daigle\n\non\nOctober 11, 2012 Hashrocket projects kick off in a big way: the client comes to town (or occasionally we go to them), we all sit in a big room, and we make wireframes and write story cards for up to 3 days solid. But it hasn't always been this way. The 5-Day Slog In the earlier days of Hashrocket, wireframing and story carding took place in independent sessions. When the sessions were finally combined into one hybrid session – something I personally pushed for on my first projects here, as I couldn't imagine not being able to communicate visually with the client when discussing functionality – the meetings were monsters, commonly lasting 4-5 days. As you can imagine, after the 3rd day or so, energy levels dipped, burnout kicked in, and – most importantly – we would find ourselves diving into features that were too far away: we've learned since then that, inevitably, requirements and features will change and mature, rendering story cards set 2-3 months into the future completely obsolete by the time they're needed. So, here's what we do these days: our team (designer, developers, project manager) and their team, 3 days, one room, looking to hash out the MVP (minimum viable product) of the client's app. We have a big TV hooked up to a Mac Mini, with half of the display showing the project manager's VIM window as he writes story cards, and the other half showing our wonky-but-lovable wireframing tool of choice, Balsamiq . 1. Wireframe interactions, not layout. The first thing I tell the client when we get to the point of actually starting up Balsamiq and building our first wireframe (usually about halfway through the first day) is that Balsamiq is deliberately ugly . Seriously. The typeface is some bastardized Balsamiq-only thing that replaced Comic Sans / Chalkboard in earlier versions. I have no idea if the Balsamiq team intends that we treat \"ugliness\" as a feature in the way we do, but: for all of its design flaws, I love that Balsamiq is ugly and lo-fi. It helps me (and the client) focus on what the wireframes are representing from a functionality standpoint and not get into discussions about whether the nav should be on the left or the top, or whether the heading should be centered, or whether there should be a ... you get the idea. We're in a story carding session to hone down what their app does – we don't want to be going off on a visual tangent as to how that should look . This rule isn't set in stone – sometimes visual design conversations are absolutely necessary in story carding. And sometimes it's hard for me to just wireframe a simple table when my designer sense knows the final solution will be much more interesting than that. But the designer's role in story carding is to help with the concept, flow, and interactions of the app, and staying focused on that is what helps keep the session on track. 2. Only wireframe what exists Here's how not to begin a wireframe: Logo, navigation element, and asking the client \"okay, what're the items that go into the navigation here?\" – because the nav should be empty at the start of the day: it's there to support navigation between features, and those features don't exist yet. We always wireframe from the perspective of a set of features and let that dictate the outer structure of the app. For example, in an app that allows simple management of clients & companies, we'll walk through the wireframes for listing, creating, and viewing a company first, at which point I'll add an item to the navigation: \"Companies\". And then we'll do the same for clients, and I'll add that – or maybe I won't, because maybe clients belong to companies, in which case we'll add a subnav to a company's page and go from there. See what I mean? Features dictate structure. And if we need to wireframe a dashboard, or homepage, or other page that's dependent upon other content, we wireframe those last. This makes sense enough for design, but is absolutely essential to story carding, as we try to write stories in the order that they need to be delivered (for example, the developers can't deliver a \"user views company\" story until they've done the \"user creates company\" story). This approach can also be a challenge to design, because you have to let go & relax about what the more design-heavy pages (dashboard, homepage, etc) will look like until you've gone through the supporting pages. This approach will inevitably lead (in Balsamiq, anyway) to a bunch of inconsistencies between files: navigation & other shared elements in various states of completion. However: 3. Get messy – you can tidy up later. Story carding with a client can and should be a messy process, where feature sets are changed, wireframes are scrapped, conversations go back to the whiteboard, and (this has happened multiple times) the team lets an idea gestate overnight and comes back the next day with a fundamental shift in the app's concept. We roll with the punches – when something changes the nav, for example, the current wireframe is adjusted and we update the rest later, after the session is over. Look, the last thing I as a designer want is 8 pairs of eyes staring at me while I noodle with the last 15-odd wireframe files and make sure everything's up to date. I'm still guilty of that occasionally (cut me some slack, I'm a designer and that button was misaligned and it was LITERALLY KILLING ME) but the name of the game in story carding is not to let the process get in the way of the discussion. So files are scrapped, quick sticky notes are thrown into the corner of wireframes, and when the client's not physically present in the room, I'll go back in, tidy up the joint and post a professional, organized set of wireframes to Basecamp. Now get to it! Lo-fi now, high-fi later! After story carding is complete, we get to dig into those ugly, simplistic wireframes and work on real mockups, and it's a blast every time. The wireframes have done their job – they've provided a visual representation of all those long feature conversations – and when the client sees the real comps, the only surprises will be good ones (it's so pretty now!) instead of bad ones (what's this button doing here?) and the project will have taken its first steps toward success. Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-10-11"},
{"website": "Hash-Rocket", "title": "Observing Change: A Gold Master Test in Practice", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/observing-change-a-gold-master-test-in-practice", "abstract": "Ruby Observing Change: A Gold Master Test in Practice by\nJake Worth\n\non\nSeptember 14, 2017 How do we test a system that seems untestable ? Untested legacy code is the bane of many a developer's existence. In a\npermissive language like Ruby, continued development in an untested legacy\ncodebase requires adding tests. Users expect certain behavior to survive each release intact. For some systems, the traditional pyramid of test types (unit, API, and\nintegration) is insufficient. Unit and API tests must presumably cover hundreds\nor thousands of functions; which matter?  Integration tests could cover a wide\nrange of user interactions; which are likely? Familiarity with the project is a luxury we don't all enjoy. I recently worked on an application that took a complex input (a\nproduction database), ran it through a large SQL function, outputting a\ngiant CSV file representing thousands of permutations of data. Each stage of\nthe transformation was complex enough that visual inspection would fail to\ndetermine if something was amiss. None of the traditional test types could handle this challenge. Unit tests came\nclose, but the isolation of the test database could not rival the complexity of\na large, messy production database. Unfortunately, not testing was not an\noption. Like every application, development needed to keep moving forward.\nSo, what did we do? After much brainstorming, we found a solution: an obscure technique called a\nGold Master Test. I wrote about the\nexperience not long\nago. Continued discussion led to a talk that I presented at RailsConf\n2017 in Phoenix, AZ.\nMy presentation included finding a clear definition of a Gold Master Test,\nwriting a test, and working with an existing test. Want to learn more? Check out the recording of my talk at\nRailsConf , and follow along with\nthe slides . Test writers of every experience level should leave this talk with a better\nunderstanding of this technique, and a broader conception of what a test can\nbe. Thank you to Hashrocket for continuing to sponsor my public speaking endeavors. Photo Credit: Christoph Gockel Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-09-14"},
{"website": "Hash-Rocket", "title": "Using Tiled and Canvas to Render Game Screens", "author": ["\nShane Riley\n\n"], "link": "https://hashrocket.com/blog/posts/using-tiled-and-canvas-to-render-game-screens", "abstract": "Design Javascript Using Tiled and Canvas to Render Game Screens by\nShane Riley\n\non\nNovember 15, 2012 A while back, I came across a great application called Tiled that can be used for creating layered scenes using a sprite map, and thought it would be a great way to quickly create scenes in Canvas-based games. The idea behind this block of code is creating a way to go from Tiled data to the same scene rendered in Canvas, like this one for use in top-down view Javascript games. To start with, let's grab a nice-looking sprite sheet from our friends at Open Game Art . We'll use a nice Zelda overworld-like sprite sheet . You can check out the Tiled tutorial for how to do the basics. We'll start by creating a new tileset. From the map menu, select new tileset. Find the sprite sheet we grabbed from Open Game Art, give it a name, and leave the default 32 x 32 tile size. You should see the tileset in the bottom right. This gives us the sprite stamps we need to create our scene. Next, we'll make the layers we'll use to create our scene. Click the add layer icon below the layers pane or choose add tile layer from the layer menu. For this example, I created three layers: forest, mountain, and ground. The ground layer contains any grass or pathways, forest contains trees, tall grass, and small stones. Mountain contains the larger rock formations, including the cave entrance. Once we've got the scene drawn, we can export the data necessary to render it to a canvas in JSON format. From the file menu, choose export as. From here, we can start writing the code necessary to generate the scene in canvas. We'll start by creating a scene object to hold our layers and methods for rendering them. var scene = { layers : [], renderLayer : function ( layer ) { }, renderLayers : function ( layers ) { }, loadTileset : function ( json ) { }, load : function ( name ) { } } Let's create a method to load the scene using jQuery's Ajax method. We'll assume the path will be to the maps directory, and we'll create an argument to pass in the scene name. When we're done loading the JSON data, we'll call the loadTileset method to preload our sprite sheet. load : function ( name ) { return $ . ajax ({ url : \"/maps/\" + name + \".json\" , type : \"JSON\" }). done ( $ . proxy ( this . loadTileset , this )); } Note that we are using jQuery's proxy method to assign the context that the loadTileset method is being run with. We'll receive the same arguments passed to loadTileset as we normally would with the done callback, but this will refer to the scene object. Why did we use the deferred .done() method rather than the success method in the Ajax object? This allows us to add multiple callbacks on the same Ajax request and manage the callbacks if necessary. Since the load method is returning the deferred object created by jQuery, we can attach any scene-specific callbacks to the returned object when we tell the scene to load. The loadTileset method is easy enough. Create the tileset image, store a reference to it, then attach an onload event to render the scene's layers. loadTileset : function ( json ) { this . data = json ; this . tileset = $ ( \"<img />\" , { src : json . tilesets [ 0 ]. image })[ 0 ] this . tileset . onload = $ . proxy ( this . renderLayers , this ); } We assign the JSON data to the data property of our scene object, create an img DOM element and store it as scene.tileset, then set our proxied renderLayers method as our onload callback. This means our next method of interest will be renderLayers. renderLayers : function ( layers ) { layers = $ . isArray ( layers ) ? layers : this . data . layers ; layers . forEach ( this . renderLayer ); } I've set up renderLayers to allow an optional argument to specify which layers we should be rendering. This allows us to call the method later and render either a subset of the layers of that scene or a mixture of layers from this scene and others. Finally on to the bulk of the work, the renderLayer method. Let's start by making sure the layer we've been passed can and should be rendered. If so, we'll set up a scratch canvas to render to for a slight performance improvement. This method assumes you've created a canvas rendering context and assigned it to variable c , like var c = $(\"canvas\")[0].getContext(\"2d\"); if ( layer . type !== \"tilelayer\" || ! layer . opacity ) { return ; } var s = c . canvas . cloneNode (), size = scene . data . tilewidth ; s = s . getContext ( \"2d\" ); Next we'll check to see if we've previously rendered the layers and stored them as images. If we haven't, we'll start rendering the tiles for that layer to the canvas. if ( scene . layers . length < scene . data . layers . length ) { layer . data . forEach ( function ( tile_idx , i ) { if ( ! tile_idx ) { return ; } var img_x , img_y , s_x , s_y , tile = scene . data . tilesets [ 0 ]; tile_idx -- ; img_x = ( tile_idx % ( tile . imagewidth / size )) * size ; img_y = ~~ ( tile_idx / ( tile . imagewidth / size )) * size ; s_x = ( i % layer . width ) * size ; s_y = ~~ ( i / layer . width ) * size ; s . drawImage ( scene . tileset , img_x , img_y , size , size , s_x , s_y , size , size ); }); scene . layers . push ( s . canvas . toDataURL ()); c . drawImage ( s . canvas , 0 , 0 ); } Our forEach method received the tile ID from the JSON layer object and an iteration index. If the tile ID is 0, there is no sprite to be rendered in that location, and we won't need to proceed further. If there is a tile ID, we'll start into the drawing method by setting our x and y coordinates for the sprite. To properly calculate this based on the tile ID, we'll need to use a 0-based index and as such we'll decrement the tile ID before performing our calculations. The x and y coordinates of the sprite within the sprite sheet are calculated by thinking of the canvas as a two-dimensional matrix that we write to from left to right, top to bottom. The x value, therefore, is derived from the modulus of the tile ID and the sprite sheet's width divided by the width of the sprite, multiplied by the width of the sprite. As an example, if we were told to use sprite 4 for the current block in the scene matrix, our sprite sheet's width is 512 and our sprite width is 32, we'd have an equation that looks like this: img_x = ( 3 % ( 512 / 32 )) * 32 ; We do the same for the y value. The tile ID is divided by the result of the sprite sheet width divided by the size. This gives us the row we're grabbing the sprite from. Multiply that by the sprite size and you get the starting y value of the sprite. To ensure we don't use a floating point number and receive a blurred version of our sprite, we use the double bitwise not operator to drop the decimal places. Depending on your browsers, you may have a slight performance improvement by using the left bitwise shift, img_y = (tile_idx / (tile.imagewidth / size)) << 0 * size , but I've found that more developers know what you're doing with the double not. If both are confusing, pass it in to Math.floor instead. The sprite's x and y position are now calculated in a similar fashion to the sprite sheet's x and y position. Finally, we get to rendering the portion of the sprite sheet to the canvas. We use the tileset image as the source, set the starting x and y position on the image, the width and height we're taking from that point, then use that cut of the image to draw from s_x, s_y at the same sprite size. Once we've rendered each of the sprites to the canvas for that layer, we push a base-64 encoded version of the canvas in PNG format to the layers array on our scene object for quick redrawing. Using our scratch canvas as an image, we draw it to the game canvas starting at point 0, 0. If our layers are already rendered to PNGs, we simply draw them to the canvas. else { scene . layers . forEach ( function ( src ) { var i = $ ( \"<img />\" , { src : src })[ 0 ]; c . drawImage ( i , 0 , 0 ); }); } And that's it! You should now see the same scene you created in Tiled rendered to the canvas, like my example from before. In my example, I'm using a 640 x 480 canvas to make it more like the screen size of the 16-bit era games. You can now use this backdrop for whatever manner of game you're making. I recommend keeping this as a separate canvas layered underneath the game canvas via CSS or exporting the contents of the canvas-rendered scene to a PNG and using that to redraw your scene. This will give you a tremendous performance boost allowing you to do more with your game without experiencing frame rate issues. There's more that can be done to this to make it more universal, such as turning it into a prototype or giving it a new method to create separate instances (for now I use $.extend) and adding flipped sprite output. Adding options like sprite sheet path, map data path, and scene overrides (modify opacity, sprite dimensions, etc.) will make it easier to control the output. If time and desire permit, I'll be adding these features in later and making a formal repository for it. For now, you can grab the code we've reviewed from this Gist . Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-11-15"},
{"website": "Hash-Rocket", "title": "A Journey to gSchool", "author": ["\nMicah Cooper\n\n"], "link": "https://hashrocket.com/blog/posts/gSchool", "abstract": "Community A Journey to gSchool by\nMicah Cooper\n\non\nJuly 18, 2013 I had the opportunity to spend a couple days in Denver, Colorado with the students and teachers of gSchool. gSchool is a intensive 6 month program that engulfs people of all work and experience backgrounds in a workshop/programming course/adventure teach you how you build applications with Rails the right way. The students only had a few weeks left before graduation and I have to say - I was really impressed with what I observed. There are a several things about gSchool that stand out in comparison to other training programs I've heard of. The students gSchool is a 6 month program, whereas most other development programs only last about 8 weeks. It's is also a bit more expensive, which I think is a good thing. The kind of people who are willing to quit working for 6 months and spend a the money to learn how to program are the kind of people a company would want to hire. The length & cost barrier is a strong filtering mechanism for acquiring talent. From talking to gSchool students, it's clear that they aspire to become expert software craftsmen, not just (as Paul Elliott puts it) business people who try to learn code because they don't want to pay a developer. Some of the students may have started that way but I get the feeling the ones that did either left the program or have since refocused their passion and enthusiasm towards mastering the craft of software development, not just making money. The teachers gSchool is a group of really smart and interesting people being taught by some really smart and interesting people. Jeff Casimir , Katrina Owen , and Franklin Webber have done a great job of guiding their students in the right direction. These instructors actually know what they're talking about because they are active in the field. Additionally, they know what type of developer the industry needs and designed gSchool to satisfy that need. There are a lot of tremendously successful self-taught developers in our industry, but my guess is that the ones who learned the fastest had a mentor. There's really no substitute for having someone more experienced show you the things you didn't know that you didn't know. The concepts There are many habits and philosophies that even the most dedicated veteran developers can overlook. One of the most obvious examples is test-driven development . We here at Hashrocket are huge proponents of TDD, but some developers still aren't sold on the benefits of always writing tests first – and old habits die hard. gSchool students don't have those habits to break – they're introduced to the value of TDD and proper version control from the very beginning. That's huge! The Culture The gSchool students were all very lively, well spoken, encouraging and supportive of each other. There was no sense of competitiveness or \"bad vibes\" - the positive atmosphere reminded me of our own offices here at Hashrocket. Everyone was laid back, yet possessed the confidence and discipline to tackle the challenges of the day. It was really encouraging to get this feeling from a place that is creating developers whom I will very likely be working with someday. So, to my new friends in gSchool, you're doing something cool. I almost wish I didn't already know Rails so I could go through your program. You've created a culture that you should be proud of. Keep up the good work – I look forward to seeing the awesome software that emerges from your fingers. Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-07-18"},
{"website": "Hash-Rocket", "title": "Nuggets of wisdom from Future Insights Live ", "author": ["\nCameron Daigle\n\n"], "link": "https://hashrocket.com/blog/posts/nuggets-of-wisdom-from-future-insights-live", "abstract": "Community Nuggets of wisdom from Future Insights Live by\nCameron Daigle\n\non\nMay  9, 2012 Daniel and I flew out to Las Vegas last week to attend Future Insights Live, and proceeded to violate the city motto by bringing back some knowledge, just for you. Here we go: It's about the client, not the user. Paul Boag went straight for the reactionary bullet-point in his talk, but Daniel and I couldn't agree more. We trust our clients to know their business domain; while we can tell them what might or might not make sense for a user to do on a website, we're not experts on their business model and target market. So it's a collaborative experience, of course. But it's crucial to remember that when you're working with a client, it shouldn't be a battle – you're providing a service, and you're both on the same side. Always approach interaction challenges from the viewpoint of a persona. Don't use the words \"I\", \"you\", or \"we\" when you're talking about an interaction (\"I'm searching for a widget on your site\") – instead, create a persona (\"Bob is searching for a widget on Sharon's site\") and talk through your scenario from their point of view. That way, the conversation can be about a hypothetical third party, so you and your client can be looking at the problem from the same side of the table, so to speak. This tip came from Tyler Crowley's talk about how to pitch your startup to investors, but it applies to what we do as well, because when we talk with clients about their project, we're essentially pitching our idea to them. You shouldn't be writing plain CSS/HTML anymore. Seriously! When I joined Hashrocket a little over two years ago, I was given a crash course in HAML and SASS and have never looked back. Preprocessors were a recurring theme throughout the conference, from Chris Coyier's day 1 presentation to [Dan Cederholm's] day 2 walkthrough of the Dribbble design process. So, even if you have to resort to LESS , download a preprocessor and wrap your mind around it, because it's no longer bleeding-edge – it's the standard for a strictly better markup-writing experience. (Also, I heartily recommend disregarding SCSS and moving straight to the bracket-and-semicolon-free world of pure SASS.) Good UX balances challenge, anxiety and boredom. I'm almost doing Leonard Souza and Sean Coulter 's idea-packed talk (\"Physical Architecture meets Interaction Design\") an injustice here, but I'm going to specifically call out this one graphic: the flow channel. I love this thing. . It encapsulates the challenge of every interaction, from simple apps to complex video games. ( Here's a related article about it if you're intrigued.) Django is friendly. We're obviously a Rails shop, but Jacob Kaplan-Moss gave a very personable and informative high-level overview of Django, and it definitely seems to have its own strengths. I came away with a pretty positive impression of the tech and the community as a whole. Reviews are eliminating mediocre products more quickly than ever. Jason Calacanis' keynote was a whirlwind of success stories, horror stories, doomsaying, and general intensity. He predicted that the feedback loop of good (or bad) reviews will continue to get more accurate and faster, knocking out mediocre products (and workers) more quickly than ever, and this will result in nationwide unemployment levels of 25% in just a few years, as middling businesses struggle to survive. It's hard to argue with him. So: Especially in contrast to the madness that was SXSWi 2011, Future Insights Live was a solid experience. If I could make one wish, I'd love to see the FILive team take some cues from RailsConf and provide more in the way of tangential community events such as lightning talks & organized meetups. But hey, we came away energized, enjoyed visiting Las Vegas, and got to visit the KISS Museum / Glow-in-the-dark Minigolf Course / Wedding Chapel , so obviously I consider the trip a grand success. Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-05-09"},
{"website": "Hash-Rocket", "title": "SXSW Interactive 2011: Q&A", "author": ["\nCameron Daigle\n\n"], "link": "https://hashrocket.com/blog/posts/sxsw-interactive-2011-q-a", "abstract": "Community SXSW Interactive 2011: Q&A by\nCameron Daigle\n\non\nMarch 21, 2011 Our design team (plus Kaz and Marian) journeyed to SXSW this year, and came away with full brains and fuller stomachs. Here's just a tiny bit of what happened, from each person's (unique) perspective. What was your most memorable moment? Daniel: Picking a single moment as the best is difficult because they range from technical learning experiences to the totally absurd. On Sunday night Hashrocket sponsored BeerCamp 2011 -- it was great to cut loose with our team and evangelize Hashrocket. Doris: Memorable? Good choice of words ... because chances are i don't remember everything ;) Cameron: I ate barbecued pork on a donut, and it was FREE. I'm proud to be an American. (Thanks to Gordough's for the donut, and the Squarespace food truck for the price.) Marian: A pedicab ride uphill with three of us on board! Kaz: Most memorable: First seeing Jake Gyllenhaal in Uchi then getting rejected when trying to get a photograph with him.\nMost favorite: Being in a really cool town with loads of great food, meeting awesome people, hanging out with friends and of course going to see The Foo Fighters. What was your least favorite experience? Daniel: Deciding which panel discussions to attend is difficult. It's a real hit or miss process to choose one. I attended one panel that featured some well known names in our industry only to find that there seemed to be no preparation, agenda or theme to the discussion. It ended up becoming a weak discussion of the obvious. It was quite disappointing. Doris: Missing our flight and being fearful of missing a day of SXSW. That would have been tragic. Cameron: Getting five minutes into a panel and realizing it's a bust, which happened a few times. Every year I root for the conference portion to become more well-executed. Marian: Airport drama! And waiting in line for coffee at Starbucks in the Hilton Lobby. Kaz: Missing the airplane to Austin. [ed: obviously we made it there, after much grief and confusion. Thanks for nothing, TSA] What was your favorite panel? Daniel: On the flip side of the hit or miss process, there were some real winners in the bunch. I attended an iPad UI talk that was quite informative, another on using visual data to discover music that kept me thinking and the geek in me loved the panel where sci-fi film UI designers discussed their work. Doris: GeekFit. Robert Jolly, Derek Featherstone and I have been trying to come up with a way to promote health/fitness in the workplace for a few years now. The panel explored some of the long-term health risks associated with our obsession with all things digital and too little time maintaining an active, healthy lifestyle. Lots of ideas were tossed out and we're continuing the conversations -- search Twitter for #geekfit Next year at SXSW, expect some bigger and more exciting things surrounding this topic! Cameron: John Gruber , Jim Coudal , and Michael Lopp hosted a panel about their personal approaches to creativity & writing. It took place approximately 739 blocks away in the basement of the Sheraton, but was absolutely worth the early time slot, taxi drive, overcrowding, and heatstroke. Marian: The Lean Start-Up panel. Kaz: A Conversation with James Gunn, Ellen Page and Rainn Wilson. I also enjoyed the various 15 minute closing panels on using video to help your business. What was the most delicious thing you ate? Daniel: The best food I consumed by far was from a Japanese fusion restaurant call Uchi . Apparently the chef has competed on Iron Chef and is culinary wizard from the future. Everything I ate seemed to be prepared with magical unicorn dust and continued to get better with each plate they brought to the table. I also ate a donut filled with potato salad and smothered in BBQ pulled pork. I felt very guilty afterward ... but not that much. Doris: That's a hard question to answer. I'm big into raw foods and walked myself to Whole Foods every single day to indulge on some incredible eats. One of my favorites was the pad thai Kaz and I made in our hotel room from julienned zucchini, carrots, yellow squash, peppers and pad thai sauce from the raw bar. The best thing i ate though, had to be the dinner of high caliber amazingness that is Uchi. Cameron: Yes, Uchi was even more delicious than the pork-on-a-donut. Even more . Marian: Sushi! Kaz: Whole Food's Raw Pad Thai, oh and ALL of the food at Uchi. Austin has great food! Sum up your SXSWi experience in one word. Daniel: QR Codes Doris: Ridiculous. Cameron: Meat. Marian: Inspiring. Kaz: Epic. Was this post helpful? Share it with others. Tweet Share Post", "date": "2011-03-21"},
{"website": "Hash-Rocket", "title": "Ruby5 Roundup - Episode 408", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/ruby5-roundup-episode-408", "abstract": "Ruby Ruby5 Roundup - Episode 408 by\nPaul Elliott\n\non\nOctober  4, 2013 This week fellow Rocketeer Jonathan Jackson and I bring you the latest news in the Ruby and Rails communities in another episode of the Ruby5 podcast. Here is a quick roundup of what's new this week. http://ruby5.envylabs.com/episodes/444-episode-408-october-4th-2013 Nestive https://github.com/rwz/nestive Using content_for in Rails templates is a really handy feature, but it is lacking in some respects. Enter the nestive gem, which allows you to specify an area in your view then append and prepend markup to it. You can also specify the layout to extend right in the markup! Ruby Resty http://growingdevs.com/ruby-resty.html We all know and love resty. It is a command-line tool that makes it easy to make multiple curl requests with complex settings. With so many people switching to zsh lately, this tool has fallen out of favor. Luckily there is a Ruby port of the library now that works in multiple shells and in Ruby itself. You can also configure it with a yml file! rescue_from http://www.rubytutorial.io/rails-rescue_from I know you've cringed a little every time you had to write a rescue block. I do too, it's ok. The good news is that there is a better way. Rails provides a macro called rescue_from that allows you to specify a method to execute in the event of an error instead of hard-coding a rescue block. This way, you can easily test the error routes and extend them in subclasses. When it's the right thing to do, this is a much cleaner way. binding.repl https://github.com/robgleeson/binding.repl For those of you not using pry, your REPL of choice just got a lot better. A new library called binding.repl gives you many of pry's features in IRB and ripl. Sandi-Meter https://github.com/makaroni4/sandi_meter So you've heard of Sandi Metz. You've likely seen her conference talks and if you haven't, you need to go on Confreaks right now and watch one. It's ok, I'll wait. Anyways, she published some coding style and design rules that are good common guidelines for us all. Thanks to the quick reactions of the Ruby community, we now have a tool that will provide metrics on your compliance to those guidelines called sandi-meter . It comes with cool visualizations, too. Check it out! Faye 1.0 http://blog.jcoglan.com/2013/10/01/announcing-faye-1-0 Faye has been around for a long time and is finally celebrating its 1.0 release. If this is the first you're hearing of it, Faye provides a pub/sub messaging system for Ruby and node.js. It is a dependency for some popular gems, so you may already be using it indirectly. In any event, this is a big milestone for the team. Congratulations! So that's it for this episode of Ruby5. If you haven't already, subscribe to the podcast and keep yourself up to date. Thanks for listening! Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-10-04"},
{"website": "Hash-Rocket", "title": "Rails Quick Tips: Easy ActiveRecord Optimizations", "author": ["\nMicah Woods\n\n"], "link": "https://hashrocket.com/blog/posts/rails-quick-tips-easy-activerecord-optimizations", "abstract": "Ruby Rails Quick Tips: Easy ActiveRecord Optimizations by\nMicah Woods\n\non\nMay 27, 2015 Recently, we had the opportunity to write an API endpoint that would \"bulk\" create thousands of users. The API needed to quickly return errors if any existed, so a background job wouldn't work in this case. Bulk insert was easy thanks to the activerecord-import gem. However, validation had to be done before bulk inserting users into the database. We had to ensure that the screen_names were unique (amongst other things). The code we produced was a bit more involved, but here is a minimal version of what an screen_name validation might look like. class BulkUserScreenNameValidator attr_reader :users def initialize ( emails ) # emails is an array ['jane@example.com','john@example.com'] # it comes from the API endpoint and potentially has thousands of emails @users = User . where ( email: emails ) end def valid? User . where ( screen_name: users . map ( & :screen_name )). blank? end end On the surface this looks like clean idiomatic Ruby. However, when you run the code you quickly see the performance problems. Thousands of users are instantiated and loaded into an array to check if it's blank? Thousands of users are instantiated and loaded into an array to map screen_name Multiple queries are made. Ok, let's go down the list and fix these problems.  Most of the time when you see blank? at the end of an active record relation, you should use empty? instead.  Let's look at the difference. # Using `blank?` User . where ( screen_name: [ 'user1' , 'user2' ]). blank? # 1. Queries database for all user data #   SELECT \"users\".* FROM \"users\" WHERE \"users\".\"screen_name\" IN ('user1','user2') # 2. Loads users into an array #   [<#User:0x007fbf6413c510>,<#User:0x007fbf65ab1c70>] # 3. Checks to see if the array size is zero #   => true # Using `empty?` User . where ( screen_name: [ 'user1' , 'user2' ). empty? # 1. Queries database for ONLY a count #   SELECT COUNT(*) FROM \"users\" WHERE \"users\".\"screen_name\" IN ('user1','user2') # 2. Checks to see if the count is zero # => true So, blank? will load the entire array, then check to see if the array is empty.  On the other hand, empty? asks the database for a count, and checks to see if that count is zero or not.  This might not make a difference in small datasets (like development), but it can make a big difference in databases with large datasets (like production).  It will also make a huge difference in memory consumption when thousands of records are loaded vs a single integer. Next, let's look at our mapping of the screen_names.  When you notice that you're mapping an activerecord relation, you should consider using pluck instead.  We will talk more about this later. # Using `map?` User . where ( email: [ 'jane@example.com' , 'john@example.com' ]). map ( & :screen_name ) # 1. Queries database for all user data #   SELECT \"users\".* FROM \"users\" WHERE \"users\".\"email\" IN ('jane@example.com','john@example.com') # 2. Loads users into an array #   [<#User:0x007fbf6413c510>,<#User:0x007fbf65ab1c70>] # 3. Iterates over users to collect screen_names #   ['user1','user2'] # Using `pluck?` User . where ( email: [ 'jane@example.com' , 'john@example.com' ]). pluck ( :screen_name ) # 1. Queries database for only screen_names #   SELECT \"users\".\"screen_name\" FROM \"users\" WHERE \"users\".\"email\" IN ('jane@example.com','john@example.com') # 2. Returns those screen_names in an array #   ['user1','user2'] So, map will load the entire array, then iterate to collect the screen_names.  Alternatively, pluck asks the database for exactly what it needs and returns an array of just those items.  Once again, performance is gained and less memory is used for large datasets (like production). Now, we have two fairly efficient queries. How do we transform it into just one efficient query.  In the past, I would drop down into Arel, or just give up and write SQL.  However, thanks to a recent blog post, ( Rails: Don’t “pluck” Unnecessarily ), I now have another option. So the new rule is \" think twice before you use pluck \". Any time you use pluck you should consider using select instead.  Lets look at select and how it works. User . where ( email: [ 'jane@example.com' , 'john@example.com' ]). select ( :screen_name ) # 1. Queries database for only screen_names #   SELECT \"users\".\"screen_names\" FROM \"users\" WHERE \"users\".\"email\" IN ('jane@example.com','john@example.com') # 2. Returns and array of User instances with only the \"screen_name\" populated #   [<#User:0x007fbf6413c510>,<#User:0x007fbf65ab1c70>] Ok, well that seems less efficient than pluck , and it is, until you pass it into a where clause. # Using `pluck` with a `where` emails = [ 'jane@example.com' , 'john@example.com' ] User . where ( screen_name: User . where ( email: emails ). pluck ( :screen_name )). empty? # 1. Queries for just the emails #   SELECT \"users\".\"email\" FROM \"users\" WHERE \"users\".\"email\" IN ('jane@example.com','john@example.com') # 2. Queries for the count #  SELECT COUNT(*) FROM \"users\" WHERE \"users\".\"screen_name\" IN ('user1','user2') # Checks if the count is zero # => true # Using `select` with a `where` emails = [ 'jane@example.com' , 'john@example.com' ] User . where ( screen_name: User . where ( email: emails ). select ( :screen_name )). empty? # 1. Makes one query for count with subquery #   SELECT COUNT(*) FROM \"users\" WHERE \"users\".\"screen_name\" IN ( #     SELECT \"users\".\"screen_name\" FROM \"users\" WHERE \"users\".\"email\" IN ('jane@example.com','john@example.com') #   ) # 2. Checks if the count is zero # => true And that's it!!  We just saved a ton of performance and memory.  Let's look at the new code and how little it changed. class BulkUserScreenNameValidator attr_reader :users def initialize ( emails ) @users = User . where ( email: emails ) end def valid? User . where ( screen_name: users . select ( :screen_name )). empty? end end Here's a reminder of the rules. Use empty? or any? instead of blank? or present? . Never use map on active record relations, use pluck instead. If you're using pluck to pass values to a where use select instead. Note, email validator was updated to screen name validator. User.where(email: emails).empty? would be enough to check if emails already existed.  Thanks to Pavel for pointing this out. And even in the example of a screen name validator, the query could be User.where(screen_name: screen_names).empty? I tried to pull a small piece of code from a large application, and simplify it.  Unfortunately, somethings get lost in translation.  So my hope is that you take away some simple rules, that can improve your application.  Of course if you are doing this in your own app, please write some bench mark tests, import production data, and ensure that you are actually optimizing your code. Was this post helpful? Share it with others. Tweet Share Post", "date": "2015-05-27"},
{"website": "Hash-Rocket", "title": "Looking Back At ACR 2016", "author": ["\nCameron Daigle\n\n"], "link": "https://hashrocket.com/blog/posts/looking-back-at-acr-2016", "abstract": "Community Looking Back At ACR 2016 by\nCameron Daigle\n\non\nApril 12, 2016 Ancient City Ruby 2016 has come and gone, and I already miss it. Since our first Ancient City Ruby in 2013 – yes, we've somehow pulled off FOUR of these things – we've seen our humble idea mature from a meeting of the minds into a gathering of friends. We're thrilled beyond measure at the reception to ACR this year. Clear night skies help kick off the ACR registration reception ACR IV was a new high in terms of attendance, sponsorship, and energy level. After four years, we're starting to get the hang of this: booking speakers is easier. Printing and assembling of signage, swag and collateral is becoming smoother. Local businesses are helping out through breakfast vouchers and discounts. ACR is hitting its stride, and we're proud to see it coming together in bigger and better ways each year. Ben Lovell speaks to a packed house in the Casa Monica ballroom We started a tradition last year: each attendee receives a button in their swag bag representing their years of ACR attendance. The buttons (like your badge, tickets, and even the ACR stamp on your canvas swag bag) were created in-house: we laid out button sheets in quantities proportionate to the totals of each type, printed a bunch of sheets, and cried softly as we cut buttons for hours. Here's this year's button sheet: This means a couple of things. Yes, we're stuck making an increasingly elaborate set of buttons every year – but also: 65% of that button sheet is composed of 2, 3, or 4-year buttons, meaning roughly 65% of our attendees have attended ACR previously and determined that it was worth their time to return. We're proud enough that ANYONE would take time to come see us every year, but we're absolutely ecstatic about this percentage of returnees. You out there – if you've returned, or are planning to return: you're the lifeblood of ACR. We're nothing without a friendly, inclusive community, and the Ruby community has embraced us. We look forward to crafting special 5-year buttons next year. Thank you, literally everyone. What's Next If you attended the conference, you hopefully saw one of these sitting on your chair: That's right, we offered up a deep discount for anyone who wants to reserve their ticket for ACR 2017. That $200 ultra-discount is now expired, but never fear: for the next 30 days, we're offering a $50 reservation to get your 2017 pass at the still-crazy-low price of $250. This offer will expire on May 12, so visit bit.ly/acr2017 , and we'll see you in 2017! On behalf of the entire Hashrocket team, thanks to everyone who attended, and we'll see y'all next year! Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-04-12"},
{"website": "Hash-Rocket", "title": "Screencast: Clojure + vim basics", "author": ["\nJoshua Davey\n\n"], "link": "https://hashrocket.com/blog/posts/screencast-clojure-and-vim-basics", "abstract": "Vim Screencast: Clojure + vim basics by\nJoshua Davey\n\non\nJune 13, 2014 I've released a screencast detailing getting vim setup for Clojure development. This screencast covers how to do basic evaluation and get Clojure documentation from within vim. In this screencast I cover how to do basic evaluation and get Clojure documentation \nfrom within vim. I'm also including the transcript below. Vim is a powerful text editor. Clojure is a powerful programming\nlanguage. While its been possible to edit Clojure code in vim for years,\nthe toolchain has improved greatly over the past year. Today we're going\nto see how we can integrate vim with our Clojure REPL environment. Life without integration In a shell session, let's fire up a Clojure REPL. I'm going to use lein\nrepl to do this. In another shell session, let's start vim and edit a\nclojure file. As I edit my file, I can copy code from the editor, switch to the window\nwith the REPL in it, and paste that code in. This works, but it's an\nawkward, slow process. REPLs are supposed to be all about fast feedback.\nWe can do better than copy and paste. Plugins Before we get started, we should get the some basic plugins for clojure\ndevelopment. Using your preferred vim plugin manager, add these plugins: guns/vim-clojure-static\ntpope/fireplace.vim Setup After you've installed the necessary Vim plugins, enter a project\ndirectory. For example, if you have a leiningen project, cd into the\ndirectory. In one shell session, fire up a REPL with lein repl . In\nanother shell session, cd that that folder once again, and then open\nvim. Fireplace is able to detect when you are in the same directory as an\nactive REPL, and will attempt to automatically connect for you. This\nprocess is transparent, but should be obvious once we attempt to to send\na command to the connected REPL. Evaluation The most basic fireplace command is :Eval . :Eval takes an arbitrary\nclojure expression, sends it off to the REPL, and prints the result\nfor you. For example, we could run :Eval (+ 1 1) , and we would, as\nexpected, see 2 printed out. This emulates typing at REPL prompt\ndirectly, but there's much more we can do with our REPL-connected vim\nsession. Let's stay with :Eval for a bit longer. :Eval without any arguments\nwill send eval and print the outermost form on the current line. For\nexample, let's look at a simple expression. ( map inc [ 1 2 3 ]) When we have our cursor on this line and type :Eval with no arguments,\nwe'll see (2 3 4) printed back. :Eval , as with many vim commands, can also take a range. So, :1,3Eval would evaluate all of lines 1 through 3. All of the normal\nspecial ranges work here, such as % for the entire file, and '<,'> for the current selection in visual mode. :Eval works well, but there's a quicker way to get feedback. cp is\nthe normal mode mapping for doing a simple eval and print. By default, cp expects a motion. The form that I use most though is cpp , which\nwill eval and print the innermost form from the cursor's current\nposition. To demonstrate what this means, let's look at that expression again. ( map inc [ 1 2 3 ]) When our cursor is on the m of map , and we type cpp , we'll see (2 3 4) , just as when we did the plain :Eval . But if we move our\ncursor inside the vector and type cpp again, we'll see that inner form\nevaluated. Something unique to fireplace is its concept of a quasi-REPL. This is a\ncousin of the cp mappings, but with an intermediate editing window. To\ndemonstrate this, let's consider the following example. ( ->> [ 1 2 3 ] ( map str ) reverse ( mapv dec )) In this trivial example, we want to reverse a sequence and decrement\neach number. There's a bug in here, but it's in the middle of the\nthread-through macro. We could just edit the line directly and\neval/print using cpp , but there's another way to do one-off iterative\ndevelopment like this. Type cqc in normal mode. A commandline window will open. This is very\nmuch like a normal vim buffer, with a few notable exceptions: It cannot be modified or saved Pressing Enter in normal mode sends the current line to the REPL\nfor eval-ing. As you run commands, they are added to this buffer. tpope calls this the \"quasi-repl\", and indeed that is the mnemonic for\nthe mapping itself: cq is the \"Clojure Quasi-REPL\". While we're in this special window, let's type the following, and hit\nenter: ( map str [ 1 2 3 ]) Immediately, we can see the issue. Converting each number to a string\nprevents dec from working later on. Having to type the whole line again isn't always convenient. For those\ncases, there's cqq , which is like cqc except that it pre-populates\nthe command window with the innermost form under the cursor. We can\nsee this in action by putting our cursor near the beginning of the\nthread-through macro, and typing cqq . You can think of cqq as being very similar to cpp , but with a chance\nto edit the line or lines before sending it off to the REPL. Documentation One of the great things about Clojure is that documentation is a\nfirst-class citizen, and builtin functions have documentation attached\nto them. With a standard REPL, we can use the doc function to get the\nsignature and documentation for a given function. With fireplace, we get this with the :Doc command, and it works just\nlike doc . To see the documentation for map , for example, type :Doc\nmap . We immediately see the documentation for the map command printed. There's an even shorter way to look up documentation for a function.\nWhen your cursor is on a word, you can press K , that is Shift and K . We can try this again with the map function by placing our cursor\non the function itself, and pressing K . We can also use the :Source command to show the source for a function.\nWhen we do this with map , we see the source code for map from clojure.core . Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-06-13"},
{"website": "Hash-Rocket", "title": "Avoiding Common Traps of Product Building", "author": ["\nChris Cardello\n\n"], "link": "https://hashrocket.com/blog/posts/avoiding-common-traps-of-product-building", "abstract": "Process Avoiding Common Traps of Product Building by\nChris Cardello\n\non\nJanuary 11, 2016 Years of working with companies in various stages of maturity has taught us a lot about how businesses push their products forward. Regardless of the business model and methodologies, there's a common trap I see many clients fall victim to: building for where you see yourself, and not for where you are. Regardless of what stage your business is in, you always want to build for the future: to save yourself from tomorrow's pain by making the right decisions today. This kind of thinking is ingrained in our brains from a young age. \"He who fails to plan, plans to fail\", after all. Wouldn’t it be great to predict the future needs and wants of your customers and provide solutions before they ever becomes a pain point? Of course it would! The problem is that none of us have been given the gift of clairvoyance, so we're relegated to making the most informed and educated guesses we can. While we may hit on some ideas, there are bound to be misses as well. Depending on how successful your business is, the cost of these misses can either be absorbed, a mild inconvenience, painful, or damning. Let’s look at traps from the perspective of a few different business maturity stages:\n​ Pre-Launch Startup This is the stage where I believe you need to be most disciplined in defining the scope of what you want to bring to market. Whether you need an idea built or you're actively building it, the shape of your product is still starting to form. During this phase you should have identified your market, identified how you will provide value to that market, scoped out the competition, and narrowed your product down to the bare essentials you’ll need to fulfill that value proposition to your future customers. Stay The Course ​\nWhere I see people falter is staying the course with the core decisions that were made about the product. Too many times the need to match the competition feature-for-feature causes the project’s timeline to balloon past where it needs to be. Competition is healthy—it proves your market exists, after all—but trying to match a successful business with more existing manpower and revenue is a losing proposition. Identify what makes you different and let that inform your decisions.\n​ Learn, Don't Guess ​\nAnother danger is the attempt to predict what people will want from your product. We often hear people say \"I know my users are going to want this\" or \"my customers won’t use my product without X\", etc. It’s easy to sit around a table and come up with features, but the hard part is figuring out which ones to actually include in the product. A better exercise is to determine how to reach your market. Once you have people using the system, how can you best engage them to learn which features they actually want? At this stage, the trick is getting to market and then engaging your customers to let them drive the shape of your application moving forward, so that the decisions you make are informed ones and not hypotheticals. Post-Launch Startup Congratulations—you’ve launched! You have traffic (and all of the glorious support requests that come with it), and you may even have investors (and all of the accountability that comes with them). In either case, you now have a sample you can utilize to gather data and help inform your decisions. The danger you run into here is trying to do all of the things at once. Everyone has an opinion and most aren’t afraid to speak it, but it's important to remember that negativity is a much greater feedback motivator than contentedness—your unhappy users are inevitably going to be the loudest. Staying Focused Your gut reaction to feedback will probably be to try and remediate whatever concerns you've received. Unfortunately, responding to every request with a feature will quickly add up, and as a result, your much larger business-focused initiatives will be pushed further into the future. So you need to be disciplined in choosing which features get in and which are left out—what can people live with, and what will cost you users. The big picture must always drive your decisions. Sure, not having X feature now may cost you users in the near term, but if analytics and research tell you that finishing your bigger feature set instead will introduce even more new users to the system, then you need to stay focused. Look, we understand: your product is your baby, and you probably feel a deep sense of responsibility to it. You feel like with all the blood, sweat, tears, and dollars you’ve invested in it, everyone who uses it should be in love with it—but that will just never be the case for every user, and it's extremely important that you thicken your skin and avoid having a knee-jerk reaction to every bit of negative feedback you receive. Don't Over-Pivot Depending on the success of your launch, you may or may not be realizing that users are not flocking to your product the way you had anticipated. You may even be second-guessing your business model, and might even be feeling the need to pivot. There’s nothing wrong with pivoting—it’s saved more startups than I can count—but it’s important not to become a serial pivoter. If you’ve just recently gone to market and you’re not getting the traction you anticipated, rather than investing in large functional changes to your product, you might see better results investing that capitol in a more extensive marketing strategy. Reach out to bloggers and ProductHunt, invest in AdWords, banners, social media, your existing email list, etc. Trust in the decisions you’ve made up until this point, and invest in reaching your market, not changing it. Stable User Base If you're the proud owner of a successful Web business, the name of the game is growth. Luckily, you’ve got a tech team able to deliver, and a product team deeply in tune with your users' needs. You can afford to swing for some home runs and strike out every once in a while. What you should be focusing on at this stage of the game is not necessarily which features get in and which features are left out, as you should have metrics to support these decisions. Instead, your focus now should be on ensuring efficiency across your team. Are product and development communicating in a healthy way? Is design setting up development for success? Is design being given the direction they need? Getting multiple teams to collaborate in a way that ensures ever-increasing efficiency is different for every company. In many cases a process needs to be tailored to fit the company, rather than a company adopting a methodology and attempting to force a square peg into a round hole. Piles of books have been written on all of the different ways in which you can organize a software organization, but I’ve always found it most useful to identify a mentor and lean on them for advice. Find people who have been there and done that, and look to them for guidance. Avoid spinning your wheels on solved problems—while your situation may be unique, chances are someone you respect has experienced something similar, and you can utilize that knowledge to your benefit. A lot of this information may appear to be common sense, but it’s amazing how fast common sense can go out the window once you’re under pressure. It’s vitally important that no matter what stage your business is in, you remain disciplined in your decision-making, and whenever possible, always rely on concrete data to inform your decisions. Understand where you are as a business, the cost of every feature you add, and the risk/reward associated with that cost to ensure a bright future. Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-01-11"},
{"website": "Hash-Rocket", "title": "How Can We Help You?", "author": ["\nChris Cardello\n\n"], "link": "https://hashrocket.com/blog/posts/how-can-we-help-you", "abstract": "Process How Can We Help You? by\nChris Cardello\n\non\nMarch 31, 2016 Here at Hashrocket, our designers and developers build kickass software that businesses depend on. But delivering at a high level is the result of more than just having a talented team – that team also must be given direction. We've refined a process over the years which allows us to deliver and ship quality software at a high rate, consistently. We'd like you to leverage that process and our experience to help you deliver at a similar pace. Process Management Hashrocket began as a shop that completed projects: a client would arrive with requirements, we'd lead them through storycarding, and we'd deliver design & code. But we quickly realized that there was one final step: to take our process & ethos and help our clients adapt it as their own. Nothing makes us happier than seeing our clients succeed, and we realized that applying the principles we have developed to client teams is another way we can help set them up for success. I’m pleased to introduce a service we're offering to any software organization at any stage of maturity. Companies attempting to implement a process, identity inefficiencies in their existing process, or make the change to a more agile development approach will get the most benefit out of working with us. What we'll do is work with you to develop a process that best suits your team. Every team and business has different needs, different strengths and weaknesses, and different goals. We'll tailor a process specifically for your team. So many Agile/Kanban/XP/Scrum workshops try to force you to adopt their methodology via a curriculum designed without you in mind, but we believe in pulling the most relevant aspects of each methodology and applying them to your team. We'll help you create a hybrid process designed specifically for your team – one that fits your needs and can evolve with your business. Project Kick-Off Before we begin designing or developing any project we typically engage in an in-depth story carding session with our clients. This allows us to immerse ourselves in the business domain of the application, define the overall interface and functionality, discover edge cases, and walk through user work flows. We think of this as a chance to take a snapshot of what the needs of the application are at that point in time. The artifacts of this exercise are an extensive set of story cards defining the software's functionality, and a robust set of wireframes defining the interface and experience. We’ve recently had a number of people reach out and ask us if we would offer this exercise as a standalone service offering. Well, as it turns out, we DO! Whether it’s helping with ideation, planning out the next version of your existing application, or bringing your next big idea to fruition, we can help. We’ll co-locate with you and collaboratively work through the process we’ve refined to capture your initial requirements and design your interface. You can use our experience to help identify areas to reduce scope in your app, solve complex problems with simple interactions, and uncover just how much work will be involved in materializing your ideas. After that initial session, if desired, we can continue to work with you in whatever capacity you need. If you’d like high fidelity mockups, our design team will work with you to establish a look and feel for your application and design comps based off of your new wireframes. Similarly, if you'd like ongoing project management or development assistance, we can help there as well. Throughout the 8 years we have been building software, we’ve been presented with almost every kind of challenge and restriction a software organization can endure. Chances are, some of the challenges you're facing are challenges we've seen before, and have experience addressing. So rather than spinning your wheels by going through expensive iterations of trial and error, reach out to us and we’ll be more than happy to develop a plan to set your business up for continued success. Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-03-31"},
{"website": "Hash-Rocket", "title": "Managing Design Handoffs with the UI Controller ", "author": ["\nCameron Daigle\n\n"], "link": "https://hashrocket.com/blog/posts/managing-design-handoffs-with-the-ui-controller", "abstract": "Design Ruby Managing Design Handoffs with the UI Controller by\nCameron Daigle\n\non\nJune 28, 2012 What's the best way for a design to make its way from PSD to working implementation? Ask a dozen development teams and you'll get a dozen different answers (and probably start a fight). Here's our answer out of the dozen (the other 11 are up to you – stay frosty out there). It's lo-fi, but helps keep everyone on the team on the same page. Our technique boils down to this: every new page in the application gets built first in a directory of static views, which we always store under /ui . The Life Cycle of a View Here's the simplest scenario: when a new design comp is approved, it's handed off to one of our front-end guys to slice. They then create a new static view in /ui. The developers can then take the completed markup to use as a reference for the implemented page. Of course, not every page is that simple. For example, a link on the page might load a modal form that pops over the page. In that case, we'd use Shane's nifty jQuery.modal plugin to load in a partial (also of static markup) to simulate that interaction – and since his modal plugin is reliant upon loading according to the href of the anchor that triggers it, no Javascript has to be changed in the fully-implemented version. The beauty of the UI directory is that it allows us to have a living, organized representation of how the site should ideally behave. This helps us in terms of Agile-ity, as well: if a feature or requirement changes, we can make the change in the UI view, allowing the developers to know exactly what needs to change where. (And if the change breaks an existing view, we just push it to a branch.) We also use separate, static layouts, allowing us to make changes to the page container & nav without impacting the implemented site. The UI Controller Here's an example UI controller from LensRentals.com , a recently released Hashrocket site: # in ui_controller.rb class UiController < ApplicationController layout :choose_layout private def choose_layout case action_name when /(home)|(cart)|(^email)|(^_)|(^[45])/ false when /^(gift)|(account)|(checkout)/ \"no_nav\" else \"ui\" end end end # in routes.rb match 'ui(/:action)' , controller: 'ui' # in /views/ui/index.html.haml % ul - Dir . glob ( 'app/views/ui/*.html.haml' ). sort . each do | file | - wireframe = File . basename ( file , '.html.haml' ) - unless wireframe == 'index' || wireframe . match ( /^_/ ) % li = link_to wireframe . titleize , action: wireframe And here's what that looks like for LensRentals . Just a list of views (a lot of them, as it's a fully-launched product). It's pretty trivial to make make the index view smarter if views pile up; for example, another project of ours splits up views into sections based on their first word (e.g. user_account_show & user_account_edit would both be grouped under an \"User\" heading). That's the ultra-basic overview of our UI directory. It's a simple, organic tool that has developed over time to do exactly what we need it to do, without requiring a lot of infrastructure or maintenance. Plus, it allows me as a front-end guy to poke around in a UI view and use Ruby loops to generate lots of example content quickly: % table % thead - [ \"First name\" , \"MI\" , \"Last name\" , \"Phone\" , \"Email\" , \"Location\" ]. each do | th | % th = th % tbody - 20 . times do % tr - [ \"John\" , \"Q\" , \"Doe\" , \"555-555-1234\" , \"john@email.com\" , \"Chicago, IL\" ]. each do | td | % td = td Whee! Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-06-28"},
{"website": "Hash-Rocket", "title": "has_one view", "author": ["\nJoshua Davey\n\n"], "link": "https://hashrocket.com/blog/posts/sql-views-and-activerecord", "abstract": "Ruby PostgreSQL has_one view by\nJoshua Davey\n\non\nFebruary 12, 2013 Sometimes, aggregating data can become overly complex in a normal\nActiveRecord model. Because Rails works well with SQL views, we can\ncreate associations to SQL views that aggregate data for us, simplifying\nour models and potentially speeding up queries. Modeling an inbox I've got an inbox. A cat inbox. For real. There are many possible implementations for modeling an inbox. I've\ngone with a relatively simple approach. Two users participate in a\nconversation, sending messages back and forth to each other. The\nConversation model has a subject, but the body of the initial message is\npart of the Message object. # app/models/conversation.rb class Conversation < ActiveRecord :: Base # fields: to_id, from_id, subject belongs_to :to , class_name: \"User\" belongs_to :from , class_name: \"User\" has_many :messages , dependent: :destroy , inverse_of: :conversation end # app/models/message.rb class Message < ActiveRecord :: Base # fields: user_id, conversation_id, body belongs_to :conversation , inverse_of: :messages belongs_to :user end After the initial message, the two participants on the conversation send\nmessages back and forth. A user may have any number of conversations\nwith other users. As such, the main inbox view must list the\nconversations a user is a participant on, as well as some summary\ninformation about that conversation. For our purposes, we've decided on an HTML table view with the following\ncolumns: From - Who the original message was sent from To - The original recipient of the message Message - the Subject of the conversation, as well as the first line\nof the most recent message Last post - The date/time of the most recent message Replies - The number of replies on the conversation (excluding the\nfirst message) Although the subject is part of the conversation itself, everything else\ncomes from its various associations. This is the view, which reveals the\nexpected interface each conversation object should have: %table #inbox %thead %tr %th From %th To %th Message %th Last post %th Replies %tbody - conversations . each do | conversation | %tr %td = conversation . from_name %td = conversation . to_name %td %p %strong = conversation . subject = conversation . most_recent_message_body %td = time_ago_in_words ( conversation . most_recent_message_sent_at ) ago %td = conversation . reply_count Let's explore a typical way to model this in our model directly. A typical Ruby implementation # app/models/conversation.rb class Converation < ActiveRecord :: Base # associations, etc... def most_recent_message_body most_recent_message . body if most_recent_message end def most_recent_message_sent_at most_recent_message . created_at if most_recent_message end def reply_count messages . size - 1 end def to_name to . name end def from_name from . name end private def most_recent_message @most_recent_message ||= messages . by_date . first end end # app/models/message.rb class Message < ActiveRecord :: Base # associations, etc... def self . by_date order ( \"created_at DESC\" ) end end This approach is fairly straightforward. We obtain the most_recent_message_body and most_recent_message_sent_at from the\nmost recent message, which is trivial after we've ordered the messages association by date. The to_name and from_name methods are delegated\nto their respective associations. And reply_count is simple the total\nnumber of messages, minus one (the initial message doesn't count as a\n\"reply\"). This approach offers a number of advantages. For one, it is familiar.\nI believe most Rails developers would be able to understand exactly\nwhat's going on above. It also locates all of the domain logic within\nthe Conversation model, making it easy to find. Having everything in the Conversation model is actually a blessing\nand a curse. Although everything is easy to find, the model is also\nquickly becoming bloated. It may not seem like much right now, but as\nmore information is added to the inbox, it will become unruly. The other problem with the above is the multitude of N+1 queries that\nit has introduced. With only 3 conversations in play, loading the inbox\noutputs a log like this: Started GET \"/\" for 127.0.0.1 at 2013-02-11 09:49:02 -0600\nConnecting to database specified by database.yml\nProcessing by InboxesController#show as HTML\n  User Load (12.8ms)  SELECT \"users\".* FROM \"users\" LIMIT 1\n  Conversation Load (0.6ms)  SELECT \"conversations\".* FROM \"conversations\" WHERE (1 IN (from_id, to_id))\n  User Load (18.3ms)  SELECT \"users\".* FROM \"users\" WHERE \"users\".\"id\" = 2 LIMIT 1\n  User Load (0.5ms)  SELECT \"users\".* FROM \"users\" WHERE \"users\".\"id\" = 1 LIMIT 1\n  Message Load (12.4ms)  SELECT \"messages\".* FROM \"messages\" WHERE \"messages\".\"conversation_id\" = 7 ORDER BY created_at DESC LIMIT 1\n   (0.6ms)  SELECT COUNT(*) FROM \"messages\" WHERE \"messages\".\"conversation_id\" = 7\n  User Load (0.4ms)  SELECT \"users\".* FROM \"users\" WHERE \"users\".\"id\" = 3 LIMIT 1\n  CACHE (0.0ms)  SELECT \"users\".* FROM \"users\" WHERE \"users\".\"id\" = 1 LIMIT 1\n  Message Load (0.4ms)  SELECT \"messages\".* FROM \"messages\" WHERE \"messages\".\"conversation_id\" = 8 ORDER BY created_at DESC LIMIT 1\n   (0.4ms)  SELECT COUNT(*) FROM \"messages\" WHERE \"messages\".\"conversation_id\" = 8\n  CACHE (0.0ms)  SELECT \"users\".* FROM \"users\" WHERE \"users\".\"id\" = 1 LIMIT 1\n  CACHE (0.0ms)  SELECT \"users\".* FROM \"users\" WHERE \"users\".\"id\" = 2 LIMIT 1\n  Message Load (0.5ms)  SELECT \"messages\".* FROM \"messages\" WHERE \"messages\".\"conversation_id\" = 9 ORDER BY created_at DESC LIMIT 1\n   (0.4ms)  SELECT COUNT(*) FROM \"messages\" WHERE \"messages\".\"conversation_id\" = 9\n  Rendered inboxes/show.html.haml within layouts/application (683.9ms)\nCompleted 200 OK in 691ms (Views: 272.5ms | ActiveRecord: 418.1ms) Eager-loading We can definitely cut down on the N+1 query problem by introducing eager\nloading. In our controller, the conversations exposure is currently\ndefined thusly: # app/controllers/inboxes_controller.rb class InboxesController < ApplicationController expose ( :user ) { User . first } expose ( :conversations ) { user . conversations } end Let's change that to eagerly load its associations: expose ( :conversations ) { user . conversations . includes ( :messages , :to , :from ) } With eager-loading in place, the log now looks slightly more reasonable: Started GET \"/\" for 127.0.0.1 at 2013-02-11 09:55:24 -0600\nProcessing by InboxesController#show as HTML\n  User Load (0.3ms)  SELECT \"users\".* FROM \"users\" LIMIT 1\n  Conversation Load (0.3ms)  SELECT \"conversations\".* FROM \"conversations\" WHERE (1 IN (from_id, to_id))\n  Message Load (0.3ms)  SELECT \"messages\".* FROM \"messages\" WHERE \"messages\".\"conversation_id\" IN (7, 8, 9)\n  User Load (0.2ms)  SELECT \"users\".* FROM \"users\" WHERE \"users\".\"id\" IN (1, 2)\n  User Load (0.2ms)  SELECT \"users\".* FROM \"users\" WHERE \"users\".\"id\" IN (2, 3, 1)\n  Message Load (0.4ms)  SELECT \"messages\".* FROM \"messages\" WHERE \"messages\".\"conversation_id\" = 7 ORDER BY created_at DESC LIMIT 1\n  Message Load (0.4ms)  SELECT \"messages\".* FROM \"messages\" WHERE \"messages\".\"conversation_id\" = 8 ORDER BY created_at DESC LIMIT 1\n  Message Load (0.3ms)  SELECT \"messages\".* FROM \"messages\" WHERE \"messages\".\"conversation_id\" = 9 ORDER BY created_at DESC LIMIT 1\n  Rendered inboxes/show.html.haml within layouts/application (9.5ms)\nCompleted 200 OK in 13ms (Views: 10.0ms | ActiveRecord: 2.4ms) There are more optimizations we could make here in Ruby land. But data\ntransformation and aggregation is something that databases are good at.\nWe can use a native feature of SQL to aggregate information for us:\nviews. SQL views A SQL view is essentially a virtual table. It can be queried just like a\nnormal table, but does not physically store anything itself. Instead, a\nview has a query definition that it uses to represent its data. In our case, SQL views can allow us to treat a complex SQL query as a\ntable, abstracting away the complexity into view itself. SQL views are\nalso read-only, and therefore are usually only used for querying, but\nnot updating data directly. ActiveRecord plays nicely with SQL views out of the box. It considers a\nSQL view a normal table, and all associations and querying methods work\nlike they would with a normal table, with one exception: the records are\nread-only. Add a migration for the view Let's create a view to handle the to_name and from_name methods on\nconversation. We can do this in a normal migration, but it needs to be\ncreated with raw SQL: class CreateConversationSummaries < ActiveRecord :: Migration def up execute <<- SQL CREATE VIEW conversation_summaries AS\n        SELECT ... SQL end def down execute 'DROP VIEW conversation_summaries' end end This is the basic syntax for adding a view with ActiveRecord migrations.\nOur view needs to incorporate to_name and from_name , so let's add\nthose fields: CREATE VIEW conversation_summaries AS SELECT c . id , f . name as from_name , t . name as to_name FROM conversations c inner join users t on t . id = c . to_id inner join users f on f . id = c . from_id After we migrate our database, we can use our database console to verify\nthat we see what we expect: mailbox_development=# select * from conversation_summaries;\n id |    from_name    |     to_name\n----+-----------------+-----------------\n  7 | Felionel Richie | Cat Stevens\n  8 | Nelly Purrtado  | Cat Stevens\n  9 | Cat Stevens     | Felionel Richie\n(3 rows) Cool. The id corresponds to the conversation, and to_name and from_name columns come from the users table, but it's all displayed to\nus as one table. ActiveRecord associations for views Now that our view exists, we can integrate it into our application: class Conversation < ActiveRecord :: Base class Summary < ActiveRecord :: Base self . table_name = \"conversation_summaries\" self . primary_key = \"id\" belongs_to :conversation , foreign_key: \"id\" end has_one :summary , foreign_key: \"id\" end Let's break down what's going on here. I've chosen to nest the Summary model within the Conversation\nnamespace, mostly to call out the fact that we're doing something\nnon-standard. Also, the Summary class only makes sense in the context of\na Conversation. For that reason, we need to manually set the name of the\ntable. We must also choose a primary key, because Rails cannot infer it for SQL\nviews. The association itself should be familiar. It works like a normal has_one / belongs_to relationship, except that we override the foreign\nkey. Now that the relationships are set up, let's actually take advantage\nof our new view by changing the implementation of the to_name and from_name methods. class Conversation < ActiveRecord :: Base # ... def to_name # Used to be to.name summary . to_name end def from_name # Used to be from.name summary . from_name end end One the biggest benefits about this approach is that we can eager-load\na view assocation . We no longer need the to or from associations\neager-loaded, since we are no longer using any attributes from them in\nthe view. Let's update our controller's exposure to only eager-load the\nnecessary parts: expose ( :conversations ) { user . conversations . includes ( :summary , :messages ) } And when we visit the inbox again, the log looks like this: Started GET \"/\" for 127.0.0.1 at 2013-02-11 14:26:12 -0600\nProcessing by InboxesController#show as HTML\n  User Load (0.5ms)  SELECT \"users\".* FROM \"users\" LIMIT 1\n  Conversation Load (0.4ms)  SELECT \"conversations\".* FROM \"conversations\" WHERE (1 IN (from_id, to_id))\n  Conversation::Summary Load (0.6ms)  SELECT \"conversation_summaries\".* FROM \"conversation_summaries\" WHERE \"conversation_summaries\".\"id\" IN (7, 8, 9)\n  Message Load (0.3ms)  SELECT \"messages\".* FROM \"messages\" WHERE \"messages\".\"conversation_id\" IN (7, 8, 9)\n  Message Load (0.4ms)  SELECT \"messages\".* FROM \"messages\" WHERE \"messages\".\"conversation_id\" = 7 ORDER BY created_at DESC LIMIT 1\n  Message Load (0.4ms)  SELECT \"messages\".* FROM \"messages\" WHERE \"messages\".\"conversation_id\" = 8 ORDER BY created_at DESC LIMIT 1\n  Message Load (0.4ms)  SELECT \"messages\".* FROM \"messages\" WHERE \"messages\".\"conversation_id\" = 9 ORDER BY created_at DESC LIMIT 1\n  Rendered inboxes/show.html.haml within layouts/application (10.4ms)\nCompleted 200 OK in 13ms (Views: 9.5ms | ActiveRecord: 2.9ms) That's definitely an improvement, albeit a small one. We've pushed data\nfrom the user model into our SQL view, but we don't need to stop there. Push it down! Let's update our view migration to include more aggregated information\nabout each conversation. class CreateConversationSummaries < ActiveRecord :: Migration def up execute <<- SQL CREATE VIEW conversation_summaries AS\n        SELECT c.id,\n        f.name as from_name,\n        t.name as to_name,\n        m.body as most_recent_message_body,\n        m.created_at as most_recent_message_sent_at,\n        (select count(*) from messages m2 where m2.conversation_id = c.id) - 1 as reply_count\n        FROM conversations c\n        inner join users t on t.id = c.to_id\n        inner join users f on f.id = c.from_id\n        left outer join (\n          select distinct on(conversation_id) conversation_id, body, created_at\n          from messages m1\n          order by conversation_id, created_at desc\n        ) m ON m.conversation_id = c.id SQL end def down execute 'DROP VIEW conversation_summaries' end end After running rake db:migrate:redo , we can verify that everything is\nstill working as expect in the database console: mailbox_development=# select * from conversation_summaries;\n id |    from_name    |     to_name     |        most_recent_message_body        | most_recent_message_sent_at | reply_count \n----+-----------------+-----------------+----------------------------------------+-----------------------------+-------------\n  7 | Felionel Richie | Cat Stevens     | Say you. Say meow.                     | 2013-02-08 02:45:27.07712   |           2\n  8 | Nelly Purrtado  | Cat Stevens     | Except that I'm a cat                  | 2013-02-05 16:45:27.088292  |           0\n  9 | Cat Stevens     | Felionel Richie | I'm sorry that you're feeling that way | 2013-01-30 16:45:27.092443  |           1\n(3 rows) That's a lot of SQL! But actually, all I've added are one join to a\nsubquery, and a subselect. Let's review both of these changes. There are many ways to grab the most recent message for a conversation\nin SQL, including using window functions . The method I've opted for\nhere is a subquery in the table expression . The subquery\nalone would return rows for only the most recent messages for each\nconversation: conversation_id |                  body                  |         created_at\n----------------+----------------------------------------+----------------------------\n              7 | Say you. Say meow.                     | 2013-02-08 02:45:27.07712\n              8 | Except that I'm a cat                  | 2013-02-05 16:45:27.088292\n              9 | I'm sorry that you're feeling that way | 2013-01-30 16:45:27.092443 By joining with only the most recent message per conversation, we avoid\nduplicate rows and only get the body and created_at columns from\nthe most recent message. Then, joining against this subquery, we can\nadd the body and created_at to the list of projections, naming\nthem most_recent_message_body and most_recent_message_sent_at ,\nrespectively. The other thing we've added to the view this iteration is the reply_count column, which is a subselect to get the count. We also\nsubtract 1, just as before. Let's take a look at our Conversation model now: # before class Conversation < ActiveRecord :: Base belongs_to :to , class_name: \"User\" belongs_to :from , class_name: \"User\" has_many :messages , dependent: :destroy , inverse_of: :conversation def most_recent_message_body most_recent_message . body if most_recent_message end def most_recent_message_sent_at most_recent_message . created_at if most_recent_message end def reply_count [ messages . size - 1 , 0 ]. max end def to_name to . name end def from_name from . name end private def most_recent_message @most_recent_message ||= messages . by_date . first end end # after class Conversation < ActiveRecord :: Base class Summary < ActiveRecord :: Base self . table_name = \"conversation_summaries\" self . primary_key = \"id\" belongs_to :conversation , foreign_key: \"id\" end belongs_to :to , class_name: \"User\" belongs_to :from , class_name: \"User\" has_many :messages , dependent: :destroy , inverse_of: :conversation has_one :summary , foreign_key: \"id\" delegate :most_recent_message_sent_at , :most_recent_message_body , :reply_count , :to_name , :from_name , to: :summary end With much of our data transformation and aggregation in our SQL view,\nour model has become trivially simple. It literally only contains\nassocations and delegation now. We update our exposure to only\neager-load the conversation summary: expose ( :conversations ) { user . conversations . includes ( :summary ) } Now, reloading the page yields the following log output: Started GET \"/\" for 127.0.0.1 at 2013-02-11 15:37:49 -0600\nProcessing by InboxesController#show as HTML\n  User Load (1.0ms)  SELECT \"users\".* FROM \"users\" LIMIT 1\n  Conversation Load (0.2ms)  SELECT \"conversations\".* FROM \"conversations\" WHERE (1 IN (from_id, to_id))\n  Conversation::Summary Load (0.8ms)  SELECT \"conversation_summaries\".* FROM \"conversation_summaries\" WHERE \"conversation_summaries\".\"id\" IN (7, 8, 9)\n  Rendered inboxes/show.html.haml within layouts/application (5.5ms)\nCompleted 200 OK in 8ms (Views: 6.0ms | ActiveRecord: 2.0ms) Now we see some real improvement. All N+1 queries are gone, replaced\ninstead with the eager-loading of the the Conversation::Summary model. Real World Benefits I used this technique in a real-world application. It helped abstract\nsome of the mundane details of the inbox and allowed us to think about\neach conversation at a higher level with a summary. In fact the app included even more business rules than I've included\nhere. Each conversation had to include a read/unread status that updated\nwith each sent message. Although it was easily implemented in pure Ruby,\nit cluttered the model and created yet more N+1 queries in the app view. The inbox also had to be sorted by the most recent message date,\nso that the conversation with the most recent activity would\nappear first in the list. This kind of sorting without SQL is both\ncumbersome and inefficient in Ruby; you have to load all messages\nfor each conversation. With the SQL view, it was as simple as\nchanging the scope from user.conversations.include(:summary) to user.conversations.include(:summary).order(\"conversation_summaries.most\n_recent_message_sent_at DESC\") . Conclusion Any time that we push stuff into the database, we make a tradeoff.\nIn this case, when we move data transformation into the SQL view, we\nsacrifice the co-location of the conversation model and the definition\nof its summary. With the summary definition located in the database,\nthere's one extra layer of indirection. The other tradeoff is that any time we'd like to make a non-trivial\nchange the view, we actually have to create an entirely new view,\nreplacing the old one. If for example, we knew that our inbox was likely\nto change or add fields, the SQL view approach might be too brittle. On the other hand, we effectively removed N+1 queries from our\napplication and simplified our model considerably. By abstracting the\nconversation's summary into a model backed by a SQL view, we're able\nto think of the Summary as an object in its own right. This provides\na cognitive simplification, but also yields performance gains as the\ndataset grows. It may not be right for every situation, but knowing and understanding\nhow we can use SQL views in our Rails applications adds another tool to\nour toolbelt. Example app As before, while writing this post, I created a sample Rails app to\niterate quickly. I used TDD to write the pure-ruby approach, and reused\nthe specs while I \"refactored\" the implementation to the subsequent\napproaches. Of particular note is the history of the Conversation\nmodel , which mirrors the code above. Was this post helpful? Share it with others. Tweet Share Post", "date": "2013-02-12"},
{"website": "Hash-Rocket", "title": "Dropping IE7 Support? Here's What to Look Forward to", "author": ["\nShane Riley\n\n"], "link": "https://hashrocket.com/blog/posts/dropping-ie7-support-here-s-what-to-look-forward-to", "abstract": "Design Dropping IE7 Support? Here's What to Look Forward to by\nShane Riley\n\non\nMay 24, 2012 Thinking about dropping support for IE7 in your applications and wondering what (relatively) new CSS features you can start using? Maybe you've already dropped support and have just spent too long supporting it to know when to start using more progressive CSS. In either case, I'd like to go over some of the nifty new (actually kind of old) CSS tricks you can start using as a result. First off, you can now reliably use the inline-block value of the display property. This means you can now put those nicely styled rounded-corner, gradient-background buttons one after another without having to float them. The primary difference between floating and making them inline-block is that you can declare a vertical alignment for all or each rather than set margins to position them. Check out an example Next is custom counters. Using counter-reset, counter-increment, and content properties you can output your own ordered list with custom list markers on any collection of elements. This works wonders when radically styling your counters in conjunction with the next CSS feature you're allowed to use. Here's a basic example of styling counters. Finally, my favorite. The :before and :after pseudo-elements are finally available! Yay! This eliminates the need for those extraneous spans and divs that you've been putting in your markup for the sake of crazy design ideas for years. Need three different colored bottom borders for an element? No problem! Just absolutely position the :before and :after pseudo-elements to render borders at the bottom of the parent element! Make sure that when you use these elements that you add a content property, even if it's an empty string. Otherwise your pseudo-elements will be a no-show. How about some bitchin' 80s pinstriping ? There's also items in Javascript land that you'll now be able to use, like the localStorage object and querySelector. You can check out what you can and can't reliably use in the browsers you support at When can I use... . You can quickly compare IE7 and IE8 under the tables tab. Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-05-24"},
{"website": "Hash-Rocket", "title": "MySQL Has Transactions, Sorta", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/mysql-has-transactions-sorta", "abstract": "MySQL Has Transactions, Sorta by\nJosh Branchaud\n\non\nAugust 10, 2017 I've used PostgreSQL almost exclusively for several years. I've chosen it again and again for my own projects and guided many of my clients to it for theirs. In the process I have definitely taken my share of jabs at MySQL's deficiencies. A recent brownfield project required me to use MySQL and so I took some time to revisit some of my assumptions about the tool. I hope to write about those soon. This post, however, focuses on a particular issue, that of transactions. \"If you're that Postgres person who's angry that MySQL doesn't have transactions, it's had them 4 years so get over it\" - @mfdii #devopsdays — Matt Stratton (@mattstratton) July 25, 2017 It's true. If you are using MySQL with the InnoDB engine, you'll have support for transactions when working with data. There is a catch though, an implicit one. One that can certainly suprise you if you aren't aware of it. MySQL does not support transactions for DDL changes. To understand the implications of this, we can turn to the MySQL documentation on the Transaction Life Cycle . The normal transaction is committed in a similar way (by going over all engines in thd->transaction.all list) but at different times: When the user issues an SQL COMMIT statement Implicitly, when the server begins handling a DDL statement or SET AUTOCOMMIT={0|1} statement It is that second condition that should catch our eye. Transactions with Data and DDL If we are in the midst of a transaction -- perhaps we have deleted some data from one table and modified rows on another -- and we then execute any DDL statement, such as adding a table or changing a default value, the transaction will be implicitly and silently commited. Any data changes that we have made, regardless of whether or not we wanted them, will have been committed. Let's look at an example of that. mysql > create table persons ( id serial primary key , first_name varchar ( 20 ) not null , last_name varchar ( 40 ) not null ); Query OK , 0 rows affected ( 0 . 02 sec ) mysql > insert into persons ( first_name , last_name ) values ( 'Liz' , 'Lemon' ); Query OK , 1 row affected ( 0 . 01 sec ) mysql > select * from persons ; + ----+------------+-----------+ | id | first_name | last_name | + ----+------------+-----------+ | 1 | Liz | Lemon | + ----+------------+-----------+ 1 row in set ( 0 . 00 sec ) mysql > begin ; Query OK , 0 rows affected ( 0 . 00 sec ) mysql > insert into persons ( first_name , last_name ) values ( 'Jack' , 'Donaghy' ); Query OK , 1 row affected ( 0 . 00 sec ) mysql > alter table persons modify column last_name varchar ( 40 ); Query OK , 0 rows affected ( 0 . 12 sec ) Records : 0 Duplicates : 0 Warnings : 0 mysql > rollback ; Query OK , 0 rows affected ( 0 . 00 sec ) mysql > select * from persons ; + ----+------------+-----------+ | id | first_name | last_name | + ----+------------+-----------+ | 1 | Liz | Lemon | | 2 | Jack | Donaghy | + ----+------------+-----------+ 2 rows in set ( 0 . 00 sec ) Though it is uncommon to be simultaneously making data and DDL changes in the same transaction, this example clearly illustrates the potential for a transaction to be unexpectedly committed. As you may have noticed, MySQL gave us no indication that the transaction was implicitly committed. In fact, when we issued the rollback; statement, it again gave us no signal that data was committed. In this simple case, we can select all rows and clearly see that data changes were committed. However, in a typical database, there is far too much data to glance at. You'd have to trust that the rollback worked. MySQL is not gaining much of my trust here. Transactional Migrations Another place where this can trip us up - data migrations within a framework like Ruby on Rails. When executing migrations for an app, Rails will wrap the contents of an entire migration in a transaction. The reasoning is that you want all or nothing. If any part of a migration is invalid, the whole thing should be rolled back so that you can fix it. Now, imagine we are putting together a migration to replace an author column with a reference to a new author table. Let's say we end up with the following lines of DDL in the up method of a migration. --  assuming we already have the following posts table --  > describe posts; --  +--------+---------------------+------+-----+---------+----------------+ --  | Field  | Type                | Null | Key | Default | Extra          | --  +--------+---------------------+------+-----+---------+----------------+ --  | id     | bigint(20) unsigned | NO   | PRI | NULL    | auto_increment | --  | title  | varchar(50)         | YES  |     | NULL    |                | --  | body   | text                | YES  |     | NULL    |                | --  | author | varchar(40)         | YES  |     | NULL    |                | --  +--------+---------------------+------+-----+---------+----------------+ create table authors ( id serial primary key , first_name varchar ( 20 ), last_name varchar ( 20 )); alter table posts add column author_id bigint ( 20 ) unsigned not null , add constraint fk_author_id foreign key ( author_id ) references authors ( id ); alter table posts remove column author ; The first couple statements are fine, but the third has a typo ( remove should be drop ). When you run that migration it executes the first couple statements and then fails. Rails does a rollback on that transaction. Or at least it attempts to. Because the migration is a series of DDL statements, each is committed implicitly irrespective of the surrounding transaction. This is a cause of confusion and unnecessary friction. The migration, from Rails' perspective, is considered down . However, it is actually half up and half down . Fixing that third statement and running the migration again will result in a different error -- the authors table already exists and cannot be created again . You're going to have to rollback the first bits of the migration manually before proceeding. Avoiding These Gotchas First, I'd suggest not mixing data changes and DDL changes. Regardless of the database engine you are using, this is probably not a good practice. If you can remain disciplined on this front, you'll at least avoid unintentionally committing data changes. When working with migrations, there aren't a lot of great options. You can be meticulous about sticking to single-statement migrations. This may make migrations confusing and hide intention. Alternatively, you can carefully piece together each part testing out statements from the MySQL shell as needed. Ultimately, my recommendation is, if you have the choice, to move to a database with a more robust transaction system, such as PostgreSQL. Cover image by chuttersnap on\nUnsplash.com Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-08-10"},
{"website": "Hash-Rocket", "title": "X-Men: Days of Future Past, Explained in Git", "author": ["\nVic Ram"], "link": "https://hashrocket.com/blog/posts/x-men-days-of-future-past-explained-in-git", "abstract": "X-Men: Days of Future Past, Explained in Git by\nVic Ramon\n\non\nJune  5, 2014 I went to see X-Men: Days of Future Past last weekend. While having a heated discussion about the logic behind the time travel in the movie with fellow Rocketeer Lucas Galego, we realized that Git provides a great analogy for time travel. So here you have it, the new X-Men movie explained in Git. SPOILER ALERT If you haven't seen the movie yet then you probably shouldn't read this post. There are definitely spoilers here. If you have seen it then read on... In The Beginning Fade in. The X-Men are on the main timeline (the master branch), but something has gone terribly wrong. Super-powerful shape-shifting sentinels (bugs) have emerged and are killing all the X-Men (the working features). Luckily, Kitty Pryde, aka Shadowcat, has the ability to send people's consciousnesses back in time (she's a Git master). Finding The Problem The group of remaining X-Men think back in their memories (Git history) to determine at which point in time (which commit) the problems started. In Git, you can use git bisect to quickly determine which commit introduced a problem. $ git bisect start\n$ git bisect good some-time-far-in-the-past\n$ git bisect bad time-when-sentinels-are-coming-to-murder-us Git bisect uses binary search to pick which commit to show you each time. You then tell Git which commits are good and which are bad... $ git bisect good \n$ git bisect bad And through a quick process of elimination Git will pinpoint which commit started all the trouble: Author: Mystique <mystique@x-men.com>\nDate:   1973\n\n    Kill Trask, get my DNA stolen. Ah yes, that's the one. Mystique just had to go getting her DNA stolen. Good job Mystique. It's okay though, we can fix this. Going Back in Time Now that we know where the problem started we can go back in time and change history (write new code). The X-Men choose to send back Wolverine (a programmer), because the trip will be a rough one for his mind and he is the only one with instant healing power (you have that too, right?). Kitty Pryde places her hands next to Wolverine's head, concentrates really hard and... boom! Wolverine from the future wakes up in his 1973 body. That's just like git checkout: $ git checkout wolverine-wakes-up-in-random-bed-1973\nYou are in 'detached HEAD' state.... We are in a detached HEAD state. That sounds morbid, but it really means that we are detached from the main timeline (master branch), and we have the capability to start a new timeline (a new branch) if we choose. Changing History Wolverine wakes up and quickly dispatches some nameless henchmen (squashes minor bugs). They should know better than to mess with Hugh Jackman. Wolverine wants to be on a new timeline, which is like checking out a new branch: $ git checkout -b prevent-sentinels Now the movie goes on and Wolverine stops Mystique from killing Trask, twice. The sentinel program is stopped and the dystopian future is prevented. $ git commit -m 'Pull Charles out of depressed state'\n$ git commit -m 'Free Eric from super-max prison'\n$ git commit -m 'Stop Mystique from killing Trask at the summit'\n$ git commit -m 'Stop Mystique from killing Trask at the press conference'\n$ git commit -m 'Get rebar-ed by Magneto' Stashing Memories Wolverine now is lying unconscious at the bottom of a river with rebar laced throughout his body. Mystique, disguised as Stryker, revives Wolverine. At this time he doesn't have the memories from the dystopian future, he's just his original 1973 self. Let's stash the changes to Wolverine's memories so we can get them later: $ git add app/characters/wolverine/memories.yml\n$ git stash The New Timeline Becomes the Main Timeline We've got totally new events on this timeline, and we want it to become the official timeline (the master branch). To do this we need to remove all commits from master that occur after the timelines diverge, then pull in all the new commits from our new branch. We can do this with git reset --hard and git rebase: $ git checkout master\n$ git reset --hard wolverine-wakes-up-in-random-bed-1973\n$ git rebase prevent-sentinels Now master is the exact same as the prevent-sentinels branch, so we can add code directly to master. We don't need the prevent-sentinels branch anymore so we can delete it: $ git branch -D prevent-sentinels Wolverine's Memories Return At the end of the movie Wolverine's memories from the dystopian future return: $ git stash pop\n$ git add .\n$ git commit -m 'Wolverine's memories return' Roll credits! Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-06-05"},
{"website": "Hash-Rocket", "title": "Nordic Ruby Wrap-Up", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/nordicruby-wrap-up", "abstract": "Community Nordic Ruby Wrap-Up by\nPaul Elliott\n\non\nJuly  2, 2012 Representing Hashrocket at Nordic Ruby this year were Shane Riley, Daniel Ariza, and myself. We hadn't been to Nordic Ruby before or even to Europe. It was definitely the best conference I have been to in a long time and it exceeded my expectations in more ways than one. If you've only been to the larger conferences, then experiencing the magic of a regional conference is really amazing. It was a single track event so we didn't have to pay too much attention to the schedule or figure out which talk to go see. The quality of the talks was very high and I think that stems from the selection process. They judged the submissions based solely on the title and content of the talk and didn't attach any information about the speakers until they had it significantly narrowed down. That level of unbiased selection is refreshing to see and it certainly payed off. The talks were thirty minutes each with a thirty minute break in between. There were no keynote speeches but the opening talk from Joseph Wilk felt like it could have been one. His discussion of rhetoric and logical arguments had me searching for things to add to my reading list. The opposite end of the conference program was Steve Klabnik talking about the inconsistencies in Rails and how teaching helps to illuminate areas that need improvement. He presented some interesting ideas on how we can improve the Rails router API and it felt like a much friendlier version of a DHH keynote. The only weird thing about the whole experience was traveling from the US to Europe to go to a Japanese spa. That said, the venue suited the conference well and although we couldn't really go out and roam Stockholm, we enjoyed the hotel bar and amenities. The fine folks at eLabs even brought their favorite award-winning barista from Goteborg and had her making all the espresso you could drink throughout the event. As a coffee connoisseur I can tell you that her latte art is top notch. Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-07-02"},
{"website": "Hash-Rocket", "title": "Implementing a Macro in Ruby for Memoization", "author": ["\nNick Palaniuk\n\n"], "link": "https://hashrocket.com/blog/posts/implementing-a-macro-in-ruby-for-memoization", "abstract": "Ruby Implementing a Macro in Ruby for Memoization by\nNick Palaniuk\n\non\nDecember 21, 2015 One of my favorite ways to learn is digging into existing libraries and trying to reimplement the functionality. We were recently implementing our own memoization solution at work which led me to checking out some\nof the existing solutions out there. Memoist2[1] is a small implementation and I thought would be fun to \nwalk through a similar implementation to see how it works. For the uninitiated, \"memoization\" is: an optimization technique used primarily to speed up computer \nprograms by storing the results of expensive function calls and \nreturning the cached result when the same inputs occur again.\n[2] For example: def foo @foo ||= expensive_method end or def foo @foo ||= begin expensive_stuff end end We'll create a MemoRedux module that is included to provide\nmacros named memoize and memoize_class_method , which\nwill be used like this: class Foo include MemoRedux def bar end memoize :bar def self . baz end memoize_class_method :baz end (A ‘macro’ in this context is a class-level method that generates code.) Here's a pretty similar looking API to the aforementioned gems.\nThe first step will be to add the method definitions. We'll\nmake it so multiple methods can be memoized at once. module MemoRedux def self . included ( klass ) klass . extend ( Macros ) end module Macros def memoize ( * methods ) end def memoize_class_method ( * methods ) end end end The next step will be to utilize Module#prepend . The Module#prepend method was added to Ruby 2.0 [3], and will add the module before it hits the current class, as opposed to\nthe typical Module#include , which puts the module above the class in the call chain.\nWe'll get or set a module called MemoMod + the object id of self. Here self will be the class where\nMemoRedux is included. This will help us avoid collisions with the random chance that a dynamically\ncreated module has the same name as an existing constant somewhere else. At the end of the memoize method we \nprepend this module. pry ( main ) > module A ; end pry ( main ) > module C ; end pry ( main ) > module B ; prepend A ; include C ; end pry ( main ) > B . ancestors => [ A , B , C ] module MemoRedux def self . included ( klass ) klass . extend ( Macros ) end module Macros def memoize ( * methods ) const_defined? ( :\"MemoMod #{ self . object_id } \" ) ? const_get (: \"MemoMod #{ self . object_id } \" ) : const_set ( :\"MemoMod #{ self . object_id } \" , Module . new ) mod = const_get ( :\"MemoMod #{ self . object_id } \" ) prepend mod end def memoize_class_method ( * methods ) end end end We'll loop through the methods to memoize, and\ncall define_method for each one to create a new\nmethod to read the cached value. module MemoRedux def self . included ( klass ) klass . extend ( Macros ) end module Macros def memoize ( * methods ) const_defined? ( :\"MemoMod #{ self . object_id } \" ) ? const_get (: \"MemoMod #{ self . object_id } \" ) : const_set ( :\"MemoMod #{ self . object_id } \" , Module . new ) mod = const_get ( :\"MemoMod #{ self . object_id } \" ) mod . class_eval do methods . each do | method | define_method ( method ) do end end end prepend mod end def memoize_class_method ( * methods ) end end end Inside define_method , we'll create a hash to store the \ncomputed values for the memoized methods, or call super() \nto get the original method. module MemoRedux def self . included ( klass ) klass . extend ( Macros ) end module Macros def memoize ( * methods ) const_defined? ( :\"MemoMod #{ self . object_id } \" ) ? const_get (: \"MemoMod #{ self . object_id } \" ) : const_set ( :\"MemoMod #{ self . object_id } \" , Module . new ) mod = const_get ( :\"MemoMod #{ self . object_id } \" ) mod . class_eval do methods . each do | method | define_method ( method ) do @_memo_methods ||= {} if @_memo_methods . include? ( method ) @_memo_methods [ method ] else @_memo_methods [ method ] = super () end end end end prepend mod end def memoize_class_method ( * methods ) end end end We'll fill out the memoize_class_method next. We open\nup the eigenclass by sending class_eval to singleton_class . This creates a \nclosure so we can use the method arguments *methods in the eigenclass.\nMemoRedux is included here and we use the memoize method again. module MemoRedux def self . included ( klass ) klass . extend ( Macros ) end module Macros def memoize ( * methods ) const_defined? ( :\"MemoMod #{ self . object_id } \" ) ? const_get (: \"MemoMod #{ self . object_id } \" ) : const_set ( :\"MemoMod #{ self . object_id } \" , Module . new ) mod = const_get ( :\"MemoMod #{ self . object_id } \" ) mod . class_eval do methods . each do | method | define_method ( method ) do @_memo_methods ||= {} if @_memo_methods . include? ( method ) @_memo_methods [ method ] else @_memo_methods [ method ] = super () end end end end prepend mod end def memoize_class_method ( * methods ) singleton_class . class_eval do include MemoRedux memoize ( * methods ) end end end end It would be nice to be able to get a fresh value if we want,\nso we'll add the ability to pass true to skip the cached method. instance . foo ( true ) To do this we'll add an optional arg, skip_cache , to the \ndynamic method definition and add a check for it in the if statement. module Memoist def self . included ( klass ) klass . extend ( Macros ) end module Macros def memoize ( * methods ) const_defined? ( :\"MemoMod #{ self . object_id } \" ) ? const_get (: \"MemoMod #{ self . object_id } \" ) : const_set ( :\"MemoMod #{ self . object_id } \" , Module . new ) mod = const_get ( :\"MemoMod #{ self . object_id } \" ) mod . class_eval do methods . each do | method | define_method ( method ) do | skip_cache = false | @_memo_methods ||= {} if @_memo_methods . include? ( method ) && ! skip_cache @_memo_methods [ method ] else @_memo_methods [ method ] = super () end end end end prepend mod end def memoize_class_method ( * methods ) singleton_class . class_eval do include Memoist memoize ( * methods ) end end end end And with this, we've arrived at the final solution. You can memoize\ninstance methods, class methods, cache methods that return nil, and \nskip the cached method call. References https://github.com/matthewrudy/memoist2 https://en.wikipedia.org/wiki/Memoization http://gshutler.com/2013/04/ruby-2-module-prepend/ Was this post helpful? Share it with others. Tweet Share Post", "date": "2015-12-21"},
{"website": "Hash-Rocket", "title": "A Peek Into The Clojure Community", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/a-peek-into-the-clojure-community", "abstract": "Community A Peek Into The Clojure Community by\nJosh Branchaud\n\non\nDecember 24, 2015 Several weeks ago, folks from a myriad of technology backgrounds including\nJava, Ruby, Lisp, and academia in general converged on the Philadelphia\nneighborhood of Society Hill which is situated just a few blocks southeast\nof Independence Hall and the resting place of the Liberty Bell. It was in\nthis historic setting that the 2015 Clojure Conj would take place. As someone intrigued by Clojure , but still very new to the language and\nunfamiliar with the community, attending Clojure Conj seemed like the best\nnext step forward. My hope was to glean both excitement and perspective from\nthe talks and the community at large. There are a few talks in particular\nthat stand out to me as I look back on the conference. Solving Problems with Automata The Conj kicked things off with a talk from Mark and Alex Engleberg, titled Solving Problems with Automata . The premise of the talk is that as\nprogrammers we have to solve problems day to day and some, if not many, of\nthose problems can be modeled as automata. If we can model a problem with\nautomata, then ultimately the problem can be boiled down to a graph problem.\nThis is an exciting revelation because there have been decades of research\ninto solving graph problems with graph theory. In other words, we've got a\nlot of time tested theory and techniques with which to work.\nMark and Alex split the talk time in half each\npresenting an interesting programming problem, showing how the problem could\nbe modeled with automata, and then using techniques like constraint solving\nto quickly move to a general solution. Debugging With The Scientific Method The first day was capped off with an excellent keynote from Stuart Halloway on being better debuggers. Though\nthis was a Clojure conference, the improvements to the debugging process\nthat Halloway suggests are applicable in the context of any programming language. All too often when our\nprograms break or behave in unexpected ways, we take a tedious, error-prone,\nhaphazard approach to tracking down the problem. In Debugging With The Scientific Method , Halloway challenges us to\napply the scientific process by stating what we know, forming a hypothesis,\nand then validating or, more often than not, invalidating our hypothesis. I\nthink this is a talk that every programmer should watch. By incorporating\nthese practices into our existing development workflow, we will become more\nintentional and efficient programmers. ClojureScript For Skeptics and Om Next The Clojure and ClojureScript communities continue to refine their web\nstory. Derek Slager, in his talk ClojureScript For Skeptics , makes the\ncase for why ClojureScript is increasingly becoming a viable choice for web\napplication development. He addresses many of the concerns that skeptics raise and also goes over key weaknesses and shortcomings that the community\nneeds to address as they move forward. Slager's talk couples well with David Nolen's talk, Om Next , which focuses on the rapidly stabilizing next major version of Om . Nolen describes the current state of\nOm Next, recent breakthroughs, and the rationale behind the significant evolutions in how Om works. He then goes on to discuss the future of Om Next. It's clear\nthat Om Next is not simply a port of React.js to Clojure but is in fact the\nnext step in functional reactive web frameworks taking inspiration from React , Relay , and Falcor . I'd be remiss in not also mentioning the talk given by Maria Geller in which she\ntakes us on a fascinating and accessible tour of the internals of the\nClojureScript compiler. For those interested in ClojureScript and its\ncompiler, The ClojureScript Compiler - A Look\nBehind The Curtain is a must\nwatch. The Takeaway Though there were many other excellent talks at Clojure Conj (check them out here ), these were the talks that set themselves apart for a number of reasons. In addition to simply being full of engaging content, these talks revealed for me a lot about Clojure Conj and the Clojure community in general. By starting the conference with the Solving Problems With Automata talk, the organizers of Clojure Conj made plain that this conference seeks to find the intersection between the practical and the theoretical; between fun problems and academic rigor. This father-son duo shows that Clojure has a broad appeal and can make an excellent language for cutting one's teeth on programming. Stuart Halloway emphasized a value that showed up time and again throughout the conference. Rationale. The Clojure community values always having a rationale behind what they do. There should be a rationale behind each step in the debugging process. There should be a rationale behind the tools that they build. There should be a rationale behind each and every talk. All this rationale leads to intentionality and quality. The talks on ClojureScript and Om Next emphasize that the Clojure community has an eye on the future. Not only is it valuable to be able to write full-stack Clojure, but this community is helping push forward the state of the practice when it comes to how we build increasingly large and complex frontend web applications. There is a strong mindset that lots can be learned from other languages and technologies. Photo Credit: Box of Hardwood Prototypes, Jared Tarbell, flickr Was this post helpful? Share it with others. Tweet Share Post", "date": "2015-12-24"},
{"website": "Hash-Rocket", "title": "All in with Ember and Phoenix: A (mostly) Daily Code Journal", "author": ["\nJason Cummings\n\n"], "link": "https://hashrocket.com/blog/posts/all-in-with-ember-and-phoenix-a-daily-code-journal", "abstract": "Ember Phoenix All in with Ember and Phoenix: A (mostly) Daily Code Journal by\nJason Cummings\n\non\nMarch 17, 2016 I never wrote about my development experiences until I started working here at Hashrocket, but it's something I would highly encourage developers at any level to start doing, for a few reasons: When you have to write down what you're doing and explain it, you go to extra lengths to really understand what you're talking about out of fear of embarrassing yourself in front of your piers.  I remember in detail everything I've blogged about, but I can't remember what I ate for breakfast, or the method signature for collection_select.  I'd bet that if I blogged about collect_select, I'd never have to look at those docs again. Everyone in the open source community have benefitted from the blog posts, packages, frameworks, and tutorials created by other developers.  You may not have the time to write your own package, or regularly commit to an open source project, but you can contribute by blogging. A few days ago, I started a personal project.  Inspired by Hashrocket's TIL , a collection of mini blogs where rocketeers write about something they learned that day, I decided to blog my daily experiences as I go. The project will start as a test driven Ember app with a mock API, then I'll build the Phoenix API.  It won't be a very complex app, but there will be lots of testing, some web scrapping, and probably some PDF parsing, which means I have to trudge through the depths of the JVM using either Clojure or JRuby.  Follow along in the blog folder of my repo . Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-03-17"},
{"website": "Hash-Rocket", "title": "Gold Master Testing", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/gold-master-testing", "abstract": "Gold Master Testing by\nJake Worth\n\non\nNovember  8, 2016 Gold Master\nTesting is a technique for evaluating complex legacy systems. The general idea is that you take a known input, such as a database,\nrun it through a function that changes the data, and\nthen compare the output with an approved version of the output. It's ideal for systems that are mature, where little change in the output is\nexpected. And it's ideal for systems that are complex and difficult\nto test. This week my pair and I wrote a test like this, and I wanted to share the\nexperience. Our work developed through four general phases: preparation, testing, evaluation, and\nmaintenance. 1. Preparation The first step we took was to acquire a production database dump and restore it\ninto a local Postgres database. Once we had the database dump, we wrote a small Rake task to transform it into\nplaintext SQL. namespace :gold do desc \"DATABASE will be used to generate the new gold test database\" task :update_db do db_name = ENV . fetch ( 'DATABASE' ) destination = Rails . root . join ( 'spec/fixtures/gold_master.sql' ) sh \"pg_dump #{ db_name } --attribute-inserts --column-inserts --no-tablespaces --disable-triggers --data-only > #{ destination } \" end end Some noteworthy flags affecting our pg_dump . --attribute-inserts and --column-inserts to add explicit column names --disable-triggers to temporarily disable triggers on the target tables\nwhile the data is reloaded --data-only to dump only the data, not the schema We chose to use Ruby's sh because it echoes the command before running it. With a valid data dump, we were ready to test. 2. Testing The focal point of the test is the Transformer.transform function. Here's the RSpec test that covers it: describe 'gold master' do it 'produces a consistent result' do ActiveRecord :: Base . connection . execute <<- SQL truncate schema_migrations; #{ Rails . root . join ( 'spec/fixtures/gold_master.sql' ). read } SQL actual = Transformer . transform gold_master_file = Rails . root . join ( 'spec/fixtures/gold_master.txt' ) gold_master = gold_master_file . read if gold_master != actual gold_master_file . write ( actual ) end expect ( actual ). to eq ( gold_master ) end end Breaking this down, we start with a database transaction, which truncates the\nschema table and then executes the plaintext SQL statements. The result\nis our production data dumped into the test database. Next we call Transformer.transform and assign the output\nto the variable actual . Then, we read an approved version of the output, generated on a previous run.\nIf it doesn't match the Gold Master, we write the file with the same name.\nWriting the file adds unstaged changes to Git, which has some interesting\nbenefits we will look at in the next step. Finally, we make our assertion— if the two outputs don't match, the test\nfails. Using the Database\nCleaner transaction\nstrategy was an crucial part of making this all work. The production database\ncan only exist in the scope of the single test we care about. 3. Evaluation On a mature system, this test should pass most of the time. If it fails, we\nbroke accepted behavior. Like any good test, it is a safeguard against\nthat happening, but on an exacting scale. If it does fail, we've written the file, so our Git log has unstaged changes to the\nGold Master we must address. If the changes are desired, great; we can commit them; if not, we can abandon\nthem. 4. Maintenance Like any test, this one is only as good as the maintenance that supports it. It\nis more brittle than the usual test by its nature. Disciplined upkeep would\ninclude running all migrations on the local database and restoring a fresh\nproduction dump from time to time. Data changes shouldn't affect the result\nbecause we are testing behavior, not data. Conclusion This was a fun test to write. Thank you to Brian Dunn, who paired with me\nthrough the implementation. If you have a mature, complex application, consider Gold Master Testing. With the proper preparation,\ntesting, evaluation, and maintenance, it can be a valuable addition to a robust test suite. Image Source: https://commons.wikimedia.org/wiki/File:400-oz-Gold-Bars-AB-01.jpg Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-11-08"},
{"website": "Hash-Rocket", "title": "Broccoli: The Build Tool, Not the Vegetable", "author": ["\nChase McCarthy\n\n"], "link": "https://hashrocket.com/blog/posts/broccoli-the-build-tool-not-the-vegetable", "abstract": "Javascript Ember Broccoli: The Build Tool, Not the Vegetable by\nChase McCarthy\n\non\nApril  8, 2015 Broccoli is the blazingly fast build tool used by Ember.js and Ember CLI. Though as we'll see it has uses in any JavaScript project and maybe beyond that. In your Ember CLI app If you have created an Ember CLI application you are probably using Broccoli very minimally. Your app is initialized with a Brocfile.js that looks something like this: // Brocfile.js var EmberApp = require ( 'ember-cli/lib/broccoli/ember-app' ); var app = new EmberApp (); module . exports = app . toTree (); Typically the only interaction you will have with the Brocfile.js of an Ember CLI app is when you need to include a vendored library installed with Bower. // Brocfile.js var EmberApp = require ( 'ember-cli/lib/broccoli/ember-app' ); var app = new EmberApp (); app . import ( app . bowerDirectory + '/moment/moment.js' ); // <--- + module . exports = app . toTree (); There are some more advanced use cases but it's likely that anything you do in your Brocfile.js should be moved to an Ember CLI addon. A great example of the wrong way to do this is the RSS feed generator currently being used by Ember Weekend . If you take a look at the Brocfile.js for that repo,  you'll see a whole mess of things going on. This will eventually be moved out into an Ember CLI addon because it's functionality that can be tested and maintained outside of the main repository and would be useful in other applications. Even the app.import(...) statements can be moved to addons. You can see this in ember-moment . In your Ember CLI addon Addons use Broccoli to a somewhat greater extent than apps. In the setupPreprocessorRegistry hook you add preprocessors to handle transpiling things like CoffeeScript , ECMAScript 2015 , or HTMLBars . The preprocessor object is basically just this: { name : '...' , ext : '...' , toTree : function ( tree , inputPath , outputPath ) { // build output tree return outputTree ; } } This object is simple enough to construct so some addons simply build it in place, such as ember-cli-babel . Others extract this into a separate module, as is the case with ember-cli-coffeescript . In this case, the instance of MyCustomPreprocessor has the same properties and behavior as above: setupPreprocessorRegistry : function ( type , registry ) { var preprocessor = new MyCustomPreprocessor (); registry . add ( type , preprocessor ); // common types are 'js', 'css', and 'template' }, Now in the toTree function of the preprocessor we see where Broccoli comes into play. For clairity and to comply with the single responsibility principle , the building of the output tree should take place in another object. toTree : function ( tree , inputPath , outputPath , options ) { return new MyCutomTreeBuilder ( tree ); } Again this can live in the same repo like ember-cli-htmlbars does, or can be extracted to a generic Broccoli library, as broccoli-babel-transpiler does. All of the preprocessors I've mentioned build upon a simple base prototype called broccoli-filter . As you can see, there is no implementation of read for this tree. You'll see why that is important when we discuss the tree API later. The broccoli-filter plugin handles much of the work for us, accepting simple configuration data and a processString callback to be executed for each file in the input tree. var Filter = require ( 'broccoli-filter' ); function MyCutomTreeBuilder ( inputTree , options ) { this . inputTree = inputTree ; } MyCutomTreeBuilder . prototype = Object . create ( Filter . prototype ); MyCutomTreeBuilder . prototype . extensions = [ 'js' ]; MyCutomTreeBuilder . prototype . targetExtension = 'js' ; MyCutomTreeBuilder . prototype . processString = function ( string , relativePath ) { ... }; We could keep going down this rabbit hole, but I find it is easier to understand Broccoli from the bottom-up at this point. We will come back to visit broccoli-filter later. Broccoli Everywhere It's worth mentioning that many other libraries such as my Haml-like template parser, hbars , use Broccoli to manage transpilation, linting, and building distributions entirely outside of the Ember ecosystem. Broccoli Core The Broccoli tool actually handles quite a few tasks. There are utilities that find/load the Brocfile.js , a file watcher, a server to issue live reload commands, and other related functionality. We will concentrate on just the Builder . But first... What makes a tree a tree Here is an example of a minimal Broccoli tree according to the Broccoli Plugin API Specification: var tree = { read : function ( readTree ){ var tmp = quickTemp . makeOrRemake ( this , 'tmpDestDir' ); // or use promise-map-series return readTree ( subtree ). then ( function ( dir ){ // subtree has finished proccesing now // read from subtree if needed and write to tmp files return tmp ; }); }, cleanup : function (){ quickTemp . remove ( this , 'tmpDestDir' ); } }; Note: quickTemp is from node-quick-temp , and is used in various Broccoli plugins to generate temporary directories As you can see there are only two functions, read and cleanup , defined on this object. This API allows for tremendous flexiblility. Something so simple can be easily implemented by plugin authors. The seemingly complicated broccoli-filter mentioned earlier is just an extension of these two functions. Builder At the core of Broccoli is the Builder . The builder is constructed with a path string or a tree: var broccoli = require ( 'broccoli' ); var tree = broccoli . loadBrocfile (); var builder = new broccoli . Builder ( tree ); // or var builder = new broccoli . Builder ( 'someDir' ); Ember CLI creates a builder just like this. Build The build command is called to kick off the process of building the output. Instead of blocking here while the output is built, a promise to an output is returned. builder . build (). then ( function ( output ){ // finished building }); This output is an object containing the output directory, the node representation of the tree and the time it took to build. var output = { directory : 'outputDirname' graph : node , totalTime : timeInMillis }; The node is the top level tree and its associated metadata: var node = { tree : tree , subtrees : [ subtreeNode ], selfTime : timeInMillis , totalTime : timeInMillis , directory : 'outputDirname' }; Let's step through the process used to build this output. Recurse The builder simply calls readAndReturnNodeFor calls read on the given tree and returns a promise to the node for that tree to be resolved once it's done being built. This is an abbreviated version: function Builder ( tree ) { this . tree = tree ; } Builder . prototype . build = function ( willReadStringTree ) { ... return RSVP . resolve () . then ( function () { return readAndReturnNodeFor ( tree ); }) . then ( function ( node ) { return { directory : node . directory , graph : node , totalTime : node . totalTime }; }) . catch (...); }; The readAndReturnNodeFor does exactly what is says, calling read on the given tree. function readAndReturnNodeFor ( tree ){ ... var node = { tree : tree , subtrees : [], selfTime : 0 , totalTime : 0 , directory : null // <-- populated once reolved }; ... return RSVP . resolve () . then ( function () { return tree . read ( readTree ); // <-- call `read` on input tree }). then ( function ( treeDir ) { node . directory = treeDir ; return node ; }); } If a tree consumes other trees as input it should call readTree for each inside the read function. The readTree function returns a promise that must be fulfilled before the next call to readTree . A convienient way to enure this is provided by the promise-map-series library, also by Jo Liss . Internally, readTree actually calls readAndReturnNodeFor recursively for all subtrees of the input tree. This of course calls read for their trees, and so on. function readTree ( subtree ) { ... return RSVP . resolve () . then ( function () { return readAndReturnNodeFor ( subtree ) // <-- recursive }) . then ( function ( childNode ) { node . subtrees . push ( childNode ); return childNode . directory ; }) . finally (...); } Cleanup Once the build is complete you call cleanup on the builder to cascade the cleanup through the subtrees. Subtrees can sometimes just be a directory name, and in that case cleanup is not necessary. Builder . prototype . cleanup = function () { function cleanupTree ( tree ) { if ( typeof tree !== 'string' ) { return tree . cleanup (); } } return mapSeries ( this . allTreesRead , cleanupTree ); }; The mapSeries is from promise-map-series . It will call cleanup for each of the trees in allTreesRead sequentially, calling the next only after the promise from the last one has resolved. Everything changes The tree API has recently changed in Broccoli versions 0.14.x and 0.15.x, and you may have noticed some deprecation warnings. There was an issue discussed on Github that highlighted how read was nondescript.  The job of the function is to build the output of the tree and return the directory where the output is written. Jo Liss agreed and suggested that rebuild would be a more descriptive name. While fixing the minor naming issue, the larger issue of having to sequentially call readTree was fixed. With the new rebuild syntax, input trees are easier to manage. You simply add a property called inputTree or inputTrees to your object. Broccoli will handle calling readTree internally. var tree = { rebuild : function (){ var tmp = quickTemp . makeOrRemake ( this , 'tmpDestDir' ); return RSVP . resolve (). then ( function (){ // write to tmp files return tmp ; }); }, cleanup : function (){ quickTemp . remove ( this , 'tmpDestDir' ); }, inputTrees : [ subtree ] }; Conclusion This is the information I wish I had when I was diving into Ember CLI addons for the first time. In fact, I originally put this information together as notes while deep diving into Broccoli's source and I'm glad I invested the time. So while I think broccoli is a delicious vegetable I think it makes an even better build tool! Hope you found this helpful. Thanks for reading! Related https://github.com/broccolijs/broccoli#plugin-api-specification http://simonwade.me/intro-to-broccoli/ http://moduscreate.com/better-builds-begin-with-broccoli/ Was this post helpful? Share it with others. Tweet Share Post", "date": "2015-04-08"},
{"website": "Hash-Rocket", "title": "Stop Tinkering and Prioritize!", "author": ["\nChris Cardello\n\n"], "link": "https://hashrocket.com/blog/posts/stop-tinkering-and-prioritize", "abstract": "Process Stop Tinkering and Prioritize! by\nChris Cardello\n\non\nFebruary  8, 2016 So you're in full-scale development on your app. Congratulations! Now you have a new problem: A huge pile of upcoming features, bugs and user requests. Wise prioritization is crucial to ensure you achieve your goals. Here's the ideal flow of a new idea into your project: Idea Curation: Management reviews research and metrics to determine which initiatives to pursue. Marching orders: Product owners work with design, development, and project management to define how those initiatives will manifest themselves within the product. Delivery: Those requirements are turned into functional features. That's how everything is supposed to work, in a vacuum – ideas come in, they're defined, they're delivered, wash, rinse and repeat. Back in reality, though, there are typically multiple initiatives all coming at once, each of ostensibly equal importance. Throw bug fixes, maintenance, and minor enhancements to appease your user base into the mix, and you're at risk of nothing getting done on time at all. Find your Epics Here at Hashrocket, we use the term \"Epic\" to refer to a collection of features which equal a fully functional enhancement to your application. An example of an epic would be \"Customer Manages Payment Options\".  This epic could have individual features which support managing credit cards, managing addresses, loyalty options, default payment options, etc. Find how each new idea and feature request fits into the big picture, and organize them into epics. Soon you'll have a much more manageable and shorter list of items to prioritize. Define your Workstreams A workstream is a set of features within the same epic which can be worked in tandem with another set of features without interference. In the example above, managing addresses and implementing loyalty features could be two different workstreams in the \"Customer Manages Payment Options\" epic, as developers could potentially work on these two things independently without affecting each others' code. Don't Forget About Bugs There's one workstream that will always exist in some form: bugfixing. It’s always smart to have a developer (or a pair) dedicated to bugfixes and minor user-generated enhancement requests. Your userbase is your lifeblood, and when you can address their issues quickly, you'll increase their confidence in your product. Allocate Wisely How do you allocate your developers across epics so that your goals are achieved in the shortest time possible? It'd be great to just assign everyone to the highest-priority epic, but reality rarely allows this. Typically there are 2-3 epics all of equal or close to equal priority, but now that we're thinking in terms of workstreams, we can identify which epics can support larger teams without conflict and overlap. Your stakeholders, after all, are most interested in delivery of epics. Defining features at such a macro level enables you to set straightforward expectations with stakeholders to ensure they know the estimated timeline for delivery. If you don't have the staff to support all of your desired workstreams and bugfixes in tandem, try to schedule in terms of 'dedicated days'. For example, feature work gets done on Mondays, Tuesdays, and Wednesdays, with production support, hotfixes, deploys, etc. happening on Thursdays and Fridays. Test, Don't Tinker Endless tinkering has been the death of far too many companies. I’m all for being agile, and am a big proponent of being able to switch gears and focus at a moment's notice, but even the best-laid plans can devolve into chaos when unanticipated scope is introduced. There's something to be said for making a plan and sticking with it, and there's nothing unhealthier for an application than having multiple features started and unfinished because priorities changed too often. Make sure any added features will directly support your value proposition, and do not interfere with an active work stream. Vet any new ideas without development effort whenever possible. If the new idea is a design or user experience change, build it out in a clickable prototype outside of the app so you can get a feel for the change without having to actually implement features and disrupt your existing workstream. For high-risk features, timebox and research implementation prior to moving directly to development so that you can get feel for the time investment required. Focus Means Momentum Never underestimate the cost of switching context. Whenever a developer is hopping from one epic to another, or moving from support to feature work and back again, there is an inherent loss of efficiency. If you stay focused, you'll build momentum, and disrupting that momentum by making people move back and forth across areas of responsibility will negatively impact delivery. After all, it boils down to this: people can only work on one thing at a time, so pick that thing wisely. Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-02-08"},
{"website": "Hash-Rocket", "title": "Solving Ara Howard's Metakoans", "author": ["\nNick Palaniuk\n\n"], "link": "https://hashrocket.com/blog/posts/solving-ara-howard-s-metakoans", "abstract": "Ruby Solving Ara Howard's Metakoans by\nNick Palaniuk\n\non\nDecember 17, 2015 Taking a look at Ruby Quiz #67 Inspiration for this post comes from an old ruby quiz that I recently came upon.\nFor more info check here . A test suite is provided \ndescribing the behavior of an attribute method and the challenge is to get \nthe provided tests passing. There is a solution write-up already there, but I thought \nI'd write up my own summary of steps taken to solve this. This really boils down to an \nexercise in keeping track of self. From the intro: metakoans.rb is an arduous set of exercises designed to stretch\nmeta-programming muscle.  the focus is on a single method 'attribute' which\nbehaves much like the built-in 'attr', but whose properties require delving\ndeep into the depths of meta-ruby.  usage of the 'attribute' method follows\nthe general form of class C attribute 'a' end o = C :: new o . a = 42 # setter - sets @a o . a # getter - gets @a o . a? # query  - true if @a The following will be a fairly quick walkthrough of my solution. Our test descriptions (taken from the quiz spec): 'attribute' must provide getter, setter, and query to instances 'attribute' must provide getter, setter, and query to classes 'attribute' must provide getter, setter, and query to modules at module level 'attribute' must provide getter, setter, and query to modules which operate correctly when they are included by or extend objects 'attribute' must provide getter, setter, and query to singleton objects 'attribute' must provide a method for providing a default value as hash 'attribute' must provide a method for providing a default value as block which is evaluated at instance level 'attribute' must provide inheritance of default values at both class and instance levels into the void I'm going to define this new attribute method in a module called Attribute and include it in Module.\nSince Module is the superclass of Class, [ 5 ] pry ( main ) > Class . ancestors . include? ( Module ) => true we will have access to the attribute method in classes or modules. I'm going to define #attribute on the instance level in the module which\nwill create the ::attribute method class level after Attribute is included\nin Module. module Attribute def attribute ( name , & block ) end end Module . class_eval do include Attribute end A quick sidebar to help show this behavior. Below Foo is\nactually an instance of class Module so Foo responds to \nthe defined instance method #baz in class Module class Module def baz ; end end module Foo ; end [ 1 ] pry ( main ) > Foo . respond_to? ( :baz ) => true Next we will define getter, setter, and query methods for the instance variable\nbased on the first argument to ::attribute module Attribute def attribute ( name , & block ) define_method \" #{ name } \" do instance_variable_get ( :\"@ #{ name } \" ) end define_method \" #{ name } =\" do | value | instance_variable_set ( :\"@ #{ name } \" , value ) end define_method \" #{ name } ?\" do instance_variable_defined? ( :\"@ #{ name } \" ) end end end Module . class_eval do include Attribute end This will actually make the first 5 tests pass now. \nFor the sake of brevity, I won't show all of the code from these passing tests, \nbut I'll use pry to show that ::attribute will exist in the call chain \nin a couple of these different contexts. # 'attribute' must provide getter, setter, and query to instances c = Class . new { require 'pry' ; binding . pry ; attribute 'a' } [ 1 ] pry ( #<Class>)> self.respond_to?(:attribute) => true # 'attribute' must provide getter, setter, and query to classes c = Class . new { class << self require 'pry' ; binding . pry ; attribute 'a' end } [ 1 ] pry ( #<Class>)> self.respond_to?(:attribute) => true # 'attribute' must provide getter, setter, and query to modules at module level m = Module . new { class << self require 'pry' ; binding . pry ; attribute 'a' end } [ 1 ] pry ( #<Class>)> self.respond_to?(:attribute) => true To pass the 6th test we need to allow ::attribute to receive a hash to set a default \nvalue and we need to return false if a defined instance variable is set to nil def koan_6 c = Class :: new { attribute 'a' => 42 } o = c :: new assert { o . a == 42 } assert { o . a? } assert { ( o . a = nil ) == nil } assert { not o . a? } end We add a conditional to check for value and set\nthe instance variable if it exists and add a guard clause \nto the query method for nil module Attribute def attribute ( name , & block ) if name . is_a? ( Hash ) name , value = name . first end define_method \" #{ name } \" do if value instance_variable_set ( :\"@ #{ name } \" , value ) end instance_variable_get ( :\"@ #{ name } \" ) end define_method \" #{ name } =\" do | value | instance_variable_set ( :\"@ #{ name } \" , value ) end define_method \" #{ name } ?\" do return false if instance_variable_get ( :\"@ #{ name } \" ). nil? instance_variable_defined? ( :\"@ #{ name } \" ) end end end Module . class_eval do include Attribute end Being able to use a block to set a default value will get us past tests 7 and 8. def koan_7 c = Class :: new { attribute ( 'a' ){ fortytwo } def fortytwo 42 end } o = c :: new assert { o . a == 42 } assert { o . a? } assert { ( o . a = nil ) == nil } assert { not o . a? } end def koan_8 b = Class :: new { class << self attribute 'a' => 42 attribute ( 'b' ){ a } end attribute 'a' => 42 attribute ( 'b' ){ a } } c = Class :: new b assert { c . a == 42 } assert { c . a? } assert { ( c . a = nil ) == nil } assert { not c . a? } o = c :: new assert { o . a == 42 } assert { o . a? } assert { ( o . a = nil ) == nil } assert { not o . a? } end And the code: module Attribute def attribute ( name , & block ) if name . is_a? ( Hash ) name , value = name . first end define_method \" #{ name } \" do if value instance_variable_set ( :\"@ #{ name } \" , value ) elsif block instance_variable_set ( :\"@ #{ name } \" , instance_eval ( & block )) end instance_variable_get ( :\"@ #{ name } \" ) end define_method \" #{ name } =\" do | value | instance_variable_set ( :\"@ #{ name } \" , value ) end define_method \" #{ name } ?\" do return false if instance_variable_get ( :\"@ #{ name } \" ). nil? instance_variable_defined? ( :\"@ #{ name } \" ) end end end Module . class_eval do include Attribute end To get the final test to pass we need check if the instance variable\nis already defined in the getter. def koan_9 b = Class :: new { class << self attribute 'a' => 42 attribute ( 'b' ){ a } end include Module :: new { attribute 'a' => 42 attribute ( 'b' ){ a } } } c = Class :: new b assert { c . a == 42 } assert { c . a? } assert { c . a = 'forty-two' } assert { c . a == 'forty-two' } assert { b . a == 42 } o = c :: new assert { o . a == 42 } assert { o . a? } assert { ( o . a = nil ) == nil } assert { not o . a? } end module Attribute def attribute ( name , & block ) if name . is_a? ( Hash ) name , value = name . first end define_method \" #{ name } \" do unless instance_variable_defined? ( :\"@ #{ name } \" ) if value instance_variable_set ( :\"@ #{ name } \" , value ) elsif block instance_variable_set ( :\"@ #{ name } \" , instance_eval ( & block )) end end instance_variable_get ( :\"@ #{ name } \" ) end define_method \" #{ name } =\" do | value | instance_variable_set ( :\"@ #{ name } \" , value ) end define_method \" #{ name } ?\" do return false if instance_variable_get ( :\"@ #{ name } \" ). nil? instance_variable_defined? ( :\"@ #{ name } \" ) end end end Module . class_eval do include Attribute end Now with all our tests passing we can do some refactoring and pull\nout the instance variable logic into a new method and get a final\nsolution. class Module def attribute ( name , & block ) name , value = name . first if name . is_a? ( Hash ) create_attribute ( name , value , & block ) end def create_attribute ( name , value = nil , & block ) define_method ( \" #{ name } \" ) do unless instance_variable_defined? ( :\"@ #{ name } \" ) instance_variable_set ( :\"@ #{ name } \" , block_given? ? instance_eval ( & block ) : value ) end instance_variable_get ( :\"@ #{ name } \" ) end define_method ( \" #{ name } =\" ) do | value | instance_variable_set ( :\"@ #{ name } \" , value ) end define_method ( \" #{ name } ?\" ) do !! instance_variable_get ( :\"@ #{ name } \" ) end end end ~/hashrocket/projects/ruby $ ruby metakoans.rb knowledge.rb\nkoan_1 has expanded your awareness\nkoan_2 has expanded your awareness\nkoan_3 has expanded your awareness\nkoan_4 has expanded your awareness\nkoan_5 has expanded your awareness\nkoan_6 has expanded your awareness\nkoan_7 has expanded your awareness\nkoan_8 has expanded your awareness\nkoan_9 has expanded your awareness\nmountains are again merely mountains Was this post helpful? Share it with others. Tweet Share Post", "date": "2015-12-17"},
{"website": "Hash-Rocket", "title": "RubyJax Recap - Ember CLI Rails with Jonathan Jackson - March 12, 2015", "author": ["\nMicah Cooper\n\n"], "link": "https://hashrocket.com/blog/posts/rubyjax-recap-ember-cli-rails-with-jonathan-jackson-march-12-2015", "abstract": "Ember Community RubyJax Recap - Ember CLI Rails with Jonathan Jackson - March 12, 2015 by\nMicah Cooper\n\non\nMarch 19, 2015 This is a recap of the most recent RubyJax meetup. On the second Thursday of every month, RubyJax gathers to listen to a talk or lecture about something new and awesome. This month, Jonathan Jackson gave a talk on his new gem: Ember-CLI-Rails . He did a great job explaining what Ember.js is and the history of the project, starting with Ember, then Ember Appkit, then ember-rails, and finally Ember CLI. He explained how each iteration of the project, though they definitely had their shortcomings, helped move the future of Ember along. Now, with Ember CLI being the next step forward with the Ember project, Jon shows us how Ember CLI can play nicely with Rails by using his gem. Usually RubyJax attracts primarily Ruby developers, but this meetup attracted people from all different disciplines: PHP, Pearl, and .Net developers all attended. Some were totally unfamiliar with Ember and frontend frameworks in general. Fortunately, they were happy to have their minds blown by Jon's explanation of how ember utilizes the History API. With a thorough introduction to the joy of Ember.js, and details of how to use the power of Ember CLI inside your Rails application made, Jon Jackson made this month's meetup one to remember and aspire to. Thanks, Jon! RubyJax is a meetup group hosted in Jacksonville, FL and sponsored by Starfield TMS and ourselves (Hashrocket). We usually meet the first three Thursdays of every month according to the following agenda: First Thursday: Book Club Second Thursday: Lecture/Presendation Third Thursday: RubyJax OpenHax Check out the Meetup page , and come hang out sometime! Was this post helpful? Share it with others. Tweet Share Post", "date": "2015-03-19"},
{"website": "Hash-Rocket", "title": "Creative South 2015", "author": ["\nRye Mas"], "link": "https://hashrocket.com/blog/posts/creative-south-2015", "abstract": "Design Community Creative South 2015 by\nRye Mason\n\non\nApril 14, 2015 The Hashrocket design team recently returned from Creative South , a creative conference in Columbus, GA. We'd heard a lot of good things about this conference in the past and had wanted to check it out, so this year we finally made the five–hour road trip (past lots of trees, farms, and tiny towns we'd never heard of — Ty Ty, anyone?). Kicking Things Off The party started on a pedestrian bridge overlooking the Chattahoochee River, with food, drinks, music, and an array of 8–foot–tall canvases for artists who would later compete in Ink Wars. This hourlong competition pitted six artists against each other, armed only with permanent markers, to illustrate a certain theme (this year it was \"moonshine\"). The final artwork ranged from cute to scary, incorporating typography, cats, divers, and werewolves, to be voted on by the audience at large later during that weekend (and eventually won by defending champ Dan Christofferson ). The night ended with a massive display of fireworks over the rapids of the Chattahoochee. With a variety of talks ranging from business to process to work–life balance, a live Typefight event (where designers went head–to–head to design a single letter in an hour), and afterparties galore, what impressed me most about Creative South was that the conference motto, \"come as friends, leave as family,\" really rang true. Night after night we met a number of other creatives who walked right up and introduced themselves. The atmosphere was open and welcoming — you may not have known anyone when you got there, but darn if you didn't by the time you left. We bumped into fellow designers from Jacksonville and Orlando, as well as creatives from Colorado, Oregon, Wisconsin, Texas, California, New York, Georgia, and North Carolina. It was truly a gathering of community, regardless of whether or not we'd met each other before. Takeaways As a team, we drew inspiration from the wide variety of disciplines represented at Creative South. Our primary focus at Hashrocket is UI design, so it was refreshing to rediscover print, lettering, and illustration, which motivated us to try new things in our line of work. The work from Typefight, Ink Wars, products by vendors, and even on talk slides was impressive (and, I'll admit, made me kind of jealous). But the conference wasn't about competition. Multiple speakers drove home the point that your worth isn't found in your work — it's not about what you do, but who you are, because you're a person first and job title second. Speaker Octavius Newman of B3ar Fruit Design posed the question: what do you believe, and how do your beliefs fuel who you are and what you do? Whether you're a designer, illustrator, lettering artist, developer, photographer, or fill–in–the–blank–here, it's practically impossible not to find inspiration and encouragement at Creative South. Thanks to the Creative South team for assembling such a great group of people and organizing a conference that is truly inspiring. Was this post helpful? Share it with others. Tweet Share Post", "date": "2015-04-14"},
{"website": "Hash-Rocket", "title": "Behind the Curtain: a Design Workshop Recap", "author": ["\nRye Mas"], "link": "https://hashrocket.com/blog/posts/design-workshop-recap", "abstract": "Design Behind the Curtain: a Design Workshop Recap by\nRye Mason\n\non\nFebruary 26, 2015 I recently led a design workshop for a client team onsite in Birmingham, Alabama. This introduction to design was an all-day event that led over 20 participants through the elements of design, from line and color to hierarchy and typography, with group activities, critiques, and interactive exercises. While Hashrocket’s training is typically geared more toward development, we found there is also a demand for design training. This in-depth design course proved to be helpful for a group of people who, for the most part, weren’t really designers or developers, and wanted to learn more about the core fundamentals of design. (No matter how long you’ve been in the industry, though, it never hurts to review the basics. Developing the curriculum was a refresher course for me on things I’ve known but haven’t thought about in a while — and then I started seeing examples everywhere.) Creating Curriculum Our attendees ranged from journalism majors to self-taught artists, all of whom work with content layout on a daily basis. We avoided computers (technical difficulties are inevitable) and broke out paper, scissors, pens, and glue like it was grade school all over again. (Turns out adults still like arts and crafts.) It can be daunting to figure out where to start for something as broad as design, especially when members of your audience are all at different skill levels, but starting at the beginning — with something as a simple as a line — and building step-by-step on that foundation turned out to be a good roadmap for everyone. Engaging the Crowd It was impressive how quickly people picked up concepts they just learned. Some of the most common feedback centered around knowing a certain design looked good, but not knowing why. Attendees had seen white space, alignment, and proximity before, but didn’t know what those things were. “Now I have a name to go with those elements,” said one participant. Most everyone was used to organizing content inside boxes with borders and using all available space, kind of like a jam-packed car dealership ad you see in the classifieds. Using white space as its own intentional element to create breathing room, guide the eye, and separate content really resonated with the group as a whole. Another hot topic was hierarchy: when everything’s important, nothing’s important. Several participants gave examples of how they marked what was important with notes — which ended up being everything — and then never read those notes again, so nothing was important. That was their own work, so who knows what their audience did with that information? Chances are, the end user didn’t know what was important and glossed over everything. Attendees told me it was eye-opening to review real-world examples of hierarchy, which they connected to their own work in an “aha!” moment. People were also able to identify goals that designers set and how they accomplished those goals using specific elements such as color, line, and shape. This led to a conversation around being intentional and guiding the eye. One participant said he was used to centering all of his content because, well, that’s what he had always done, and thought things were supposed to be laid out that way. He referenced a real-world advertisement we critiqued and noted how they led you to their logo using negative space, lines, and left alignment. In doing so, he summed up the ideal learning scenario: reviewing the reference, attaching the information he just learned, then explaining it in his own words. Making it Click A key element to the success of this event was having participants articulate the design decisions they saw and made. It’s one thing to know a line of gray buttons makes a green button stand out, but it’s an entirely different thing to explain to someone the green button breaks the repetition and creates a focal point. You’ve probably heard the quote, “Those who know, do. Those that understand, teach.” (I don’t know if Aristotle actually said that, but it’s true nonetheless.) You learn so much more when you explain it to others, which was something I learned by preparing and leading the course, and something my attendees learned by explaining design elements to their coworkers. Driving it Home All in all, it was empowering for these newly minted designers to see what was “behind the curtain” and how they’d reacted to design elements all along. They told me they planned to use what they’d learned — to sketch instead of going straight to the computer; to set an intention for their design; to rely on proximity and alignment and white space instead of borders and nonstop text — and I believe them. Armed with the ability to identify and implement the fundamental elements of design, they have a greater sense of confidence going forward, and I’m confident that will translate to happier designers — and happier users. If you're so inclined, you too can book a design workshop with Hashrocket . Was this post helpful? Share it with others. Tweet Share Post", "date": "2015-02-26"},
{"website": "Hash-Rocket", "title": "Sass Button Mixins for Fun & Profit", "author": ["\nCameron Daigle\n\n"], "link": "https://hashrocket.com/blog/posts/sass-button-mixins-for-fun-profit", "abstract": "Design Sass Button Mixins for Fun & Profit by\nCameron Daigle\n\non\nApril 17, 2014 Here's a SASS pattern for constructing mixins that I've been enjoying lately – and has saved me from a number of CSS-related headaches. We'll be looking at strategies for styling buttons (and by buttons, I mean links, submits, actual button tags – anything that'll look like a styled action in your design). Buttons are fantastic place to use Sass mixins – there are a number of ways that button mixins can not only clean up our code, but also avoid unhealthy patterns and unexpected consequences. From Classes to Mixins Suppose I needed to make a general button style. Without mixins, I'd probably make a class definition of button , as coders have been doing since the dawn of time ... a .button // green background background : #00aa00 // with a slightly lighter green on hover: & :hover background : #33cc33 ... and suppose I need to add a delete button that uses the same button style, but is red. a .button // base button styles & .delete // red background background : #ff0000 & :hover // slightly lighter red background : #ff3333 So each additional class modification needs a class with adjustments for the regular state as well as others ( hover , active , etc). However, Sass lets us implement the same styles without all of the repetition and classes. We can define a button mixin, and then use that mixin for a second button – now we won't need to add two classes to our 'delete' button. = button // button styles a .button +button a .delete +button background : #ff0000 & :hover background : #ff3333 Better, but we're still having to rewrite the hover state color when we modify our button. We can streamline this one step further by moving our background color into the mixin, and creating the :hover state programmatically using Sass' powerful color math functions: = button ($ bg : blue ) background : $bg color : #fff & :hover background : lighten ( $bg , 10% ) a & .button +button & .delete +button ( #ff0000 ) Burned By Inheritance Now, we've constructed a nice mixin pattern for any future buttons, there's one other potential problem. In the great class inheritance rulebook, a single class is the weakest selector – that is, it's overridden by literally anything else that might accidentally point to your button. For example, you might not intend for this code to override the font size on your button, but it will: .bully_container %a ( href= '#' class= 'button' ) I'm not supposed to be 30px a .button font-size : 12px .bully_container a font-size : 30px There's a fundamental issue here that you might have noticed: our buttons are global classes with the shallowest possible selector, yet the button itself will always be deep within your DOM structure – the deepest, in fact (given that there will probably not be any elements within your button). A button is essentially a CSS inheritance endpoint. Well, this is great news for mixins. One of the dangers of mixins is that they can inject styles anywhere and disregard the actual cascade, so sloppy usage can result in unexpected style inheritance. However, since the button is the end of the chain, we don't have to worry about its nonexistent children. Try not using global classes (no, really) What I've started doing lately takes this system to its logical conclusion: avoiding global button classes and instead scoping mixins to specific buttons (or containers that have buttons). For example, our forms typically have a set of actions at the end (to submit, cancel, etc). We'll put those actions in a fieldset.actions and scope mixins to that area. So here's how that would look for a submit button, a grey 'cancel' button, and a red 'delete' button: fieldset .actions input [ type = 'submit' ] +button a & .cancel +button ( #888 ) & .delete +button ( #ff0000 ) I typically even abstract out the buttons into a set of mixins, which allows us to DRY our button colors even further. The mixins end up looking something like this: = button-base ( $ color ) // button styles = primary-action +button-base ( #000 ) = secondary-action +button-base ( #888 ) = destroy-action +button-base ( #ff0000 ) And, for our styles: fieldset .actions input [ type = 'submit' ] +primary-action a & .cancel +secondary-action & .delete +destroy-action So there you have it. We've written refreshingly minimal Sass, we're inheriting any state changes ( hover , active , etc.) automatically, and we're avoiding any unexpected inheritance from the rest of the DOM chain. As a plus (I consider it a plus, anyway), our class names are now more semantic – they indicate the action taken – and we don't need to throw a global 'button' class on everything. This class-light and mixin-heavy philosophy might be a bit out of the norm, but it's been serving me well as of late. Moving specific button styles from a class inheritance structure to a mixin inheritance structure results in fewer classes and less markup overall, and also results in much more flexibility in terms of semantic class naming. Give it a shot – it might just work for you. photo credit: Breibeest on Flickr Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-04-17"},
{"website": "Hash-Rocket", "title": "Breaking Down Barriers Between Design and Development", "author": ["\nChris Cardello\n\n"], "link": "https://hashrocket.com/blog/posts/breaking-down-barriers-between-design-and-development", "abstract": "Process Breaking Down Barriers Between Design and Development by\nChris Cardello\n\non\nJanuary 21, 2016 People ask me all the time how Hashrocket manages the give and take between design and development. After being involved in a number of external teams over the years, I've realized that this is a challenge which many companies struggle with. One could say that well crafted software is the product of a balance between intuitive design and easily-maintained code. Design and development are essentially two sides of the same coin, but problems arise when one side is treated as more valuable than the other. We've probably all witnessed or been part of one of the following two situations; Developers make a thing and hand it off to a designer to \"make it look pretty\" Designers design a thing and hand off to developers to \"make it work\" When writing software, it is of the utmost importance that all project team members are treated as having equal value, and treat each other with respect and appreciation for the value they bring. So this us-then-them mentality is fundamentally broken: ideally, anyone who is going to be contributing a project for its duration should be involved in all phases of the lifecycle. Collaboration from the beginning is key, because it sets the tone for the working relationship that will occur over the coming months. Wireframes and storycards should be created concurrently, so both perspectives can be voiced. This ensures that decisions made at this phase of the project are informed by both perspectives. This approach helps minimize the situations where something is designed in a way that's costly or time-consuming to implement, or a clunky interaction is developed where a more intuitive interface may have improved the user experience from the start. This level of collaboration shouldn't stop there. Your company should have a \"no fences\" culture. This means that at no point is something thrown over a fence to become \"your problem\". When a designer is working on a design, it shouldn't be an oddity for them to grab a developer and ask their opinion on how a given thing will be implemented. Likewise, when a developer is implementing a design, they should feel free to question the design and voice their concerns or get clarification from the designer. Occasionally, something that everyone agreed upon in a wireframe or static view doesn't \"feel right\" once implemented. If this happens, no matter what side of the coin you're on, it's your responsibility to bring up the issue before any more time is spent on a potentially less-than-ideal experience for the user. Too often the walls we build between each other cause us to fall back on assumptions which hurt the project in the long term. Even when the entire project team is all working towards the same goal, there are bound to be differing opinions on matters. This is why having strong leaders in place who can make the hard decisions is paramount. Not everyone is going to get their way every time, but it's essential that everyone feels that their opinion is valued, so that they aren't deterred from providing it in the future. All of this may sound obvious or idealistic, but the truth of the matter is that it all starts with the culture of your company. In order to work together this closely, everyone needs to stand on equal footing and leave their egos at the door. No one should be fearful of providing constructive feedback due to unwarranted backlash. Active collaboration is the key to a successful and healthy design & development team, and that can only be achieved through humility, conversation and mutual respect. Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-01-21"},
{"website": "Hash-Rocket", "title": "Better Icon Fonts with Sass", "author": ["\nCameron Daigle\n\n"], "link": "https://hashrocket.com/blog/posts/better-icon-fonts-with-sass", "abstract": "Design Better Icon Fonts with Sass by\nCameron Daigle\n\non\nNovember 23, 2015 When I think about all of the different ways I've made icons over the past dozen or so years, I start to feel a few different emotions. Old, primarily. But nostalgic? Hardly. We've come a long way, folks. I'm Going to Talk About Obsolete Stuff For a While In the (well, my) early days, if I wanted an icon that changed color on hover, I'd make two versions of an icon ( icon_check.jpg , icon_check_hover.jpg ), and then preload them with Javascript, to prevent my early-2000s-era Internet from causing the button to flicker on hover as the second image was loaded in. (Remember how MM_preloadimages() was on the body tag of half the Internet back then? Yeah, that.) Swing on down to the heady days of 2004: A List Apart publishes an article on CSS sprites . That article changes everything. I start dropping every version of every icon into one gigantic image, each positioned via boatloads of background-position coordinates scattered throughout my CSS. Hooray for progress! Fast forward a handful (okay, a lot) of years, and RETINA SCREENS appear. Congratulations, your icons now look like blurry crap! I sigh heavily, admit that spritesheets always felt kind of hacky anyway, and accept a future in which I must redraw every icon at @2x and even @3x sizes. So now instead of maintaining one spritesheet of 20 icons, I now maintain 40 separate images. Hooray, for, uh, progress? Okay, This Is The Useful Part But it's 2015, and thankfully, we've since moved on from those dark times. Icons are generally vector data now, allowing for unlimited rescaling and recoloring. Font Awesome and other prebuilt icon fonts are there for the taking, reducing a decade's worth of technical knowledge to semi-nostalgic-blog-post status. I generally prefer to start from scratch on new projects rather than tossing in an entire icon library, and Fontastic has been our tool of choice for creating custom icon fonts. I'm not crazy about the CSS it spits out, though – the globs of repeating code it generates (a separate CSS class for each icon, etc. etc. is just begging for some Sass streamlining. So let's go for it! First of all, characters map to icons, which is the perfect job for a Sass map: $icons : ( \"plus\" : \"a\" , \"minus\" : \"b\" , \"email\" : \"c\" , \"arrow-right\" : \"d\" ); Now that we've set up a map, each new icon is just a new line in the map. Everything else we're going to do can remain the same, which makes me happy. Next up, here's a set of icon mixins that append the icon as either a :before or :after element, with the actual inclusion code abstracted into its own mixin in case we need to inject it in different ways: // if we just need the content attribute @mixin icon-content ( $icon ) { content : map-get ( $icons , $icon ); } // if we need the content attribute with our default styling @mixin icon ( $icon ) { display : inline-block ; vertical-align : middle ; text-transform : none ; font-family : \"my-sweet-icons\" ; @include icon-content ( $icon ); font-weight : normal ; } // if we want it in a :before or :after, with some default spacing @mixin icon-after ( $icon ) { & :after { @include icon ( $icon ); margin-left : 1rem ; } } @mixin icon-before ( $icon ) { & :before { @include icon ( $icon ); margin-right : 1rem ; } } And finally, let's generate a set of classes, for situations where we don't want to inject the icon directly via a mixin. // generates .icon-plus, .icon-minus, and so on @each $icon , $char in $icons { .icon- #{ $icon } { @include icon-before ( $icon ); } } There we are. Regardless of your chosen font generation method, this method allows you to update your icon font with a simple change to your $icons map, and allows you to drop in your icon at your leisure via mixin or class. Check back in 2020 when all of this becomes obsolete! We'll laugh and laugh, you and I. Was this post helpful? Share it with others. Tweet Share Post", "date": "2015-11-23"},
{"website": "Hash-Rocket", "title": "Entry Level? This will help!", "author": ["\nMicah Woods\n\n"], "link": "https://hashrocket.com/blog/posts/entry-level-this-will-help", "abstract": "Community Entry Level? This will help! by\nMicah Woods\n\non\nMarch  3, 2016 The Ruby community is surging with new developers. Thanks to bootcamps and a drive for diversity, the community is booming and becoming better than ever. A little over a year ago I started reviewing applications at Hashrocket. While talking to new developers the same questions keep coming up: What can I do to improve my resume/github? How can I make myself more marketable? This article is my advice for entry level developers. Github Don't leave your github avatar as the default. This screams \"brand new\". A professional looking photo of yourself is best, but even a silly photo will do. Have at least one personal project that you're proud of. Most of the time new developers have several bootcamp, tutorial or how-to github projects and that's okay; it shows you're learning. My advice is to write at least one small library, preferably tested, that you are proud of and link to it in your cover letter or resume. Don't let the first github experience be an untested tutorial. Resume Send a PDF. Not everyone has Word. If you wrote it on Windows their Mac may not have your font. Sending a docx may look horrible to the person reading it. PDF is universally accepted and you'll have peace of mind knowing it looks great everywhere. Don't use buzzwords. If you use a buzzword on your resume be prepared to be questioned about it. I suggest not using buzzwords unless you're an expert. Don't do this: Full stack developer experienced with TDD/BDD, Angularjs, Emberjs, Rails, CanCan, Devise, Postgresql, MongoDB, CouchDB, Async, Nodejs. Do this: I really like Javascript. I blog at someblog.example.com. My favorite blog post is someblog.example.com/favorite. It is about different ways to use Javascript's call and apply. Example code can be found at github.com/handle/call_vs_apply. Don't try to make it sound like you have more experience than you do. People will see the amount of experience you have. Be honest. The goal of the resume should be to pique an employer’s interest. You just need to get to the next step. Get that phone call and/or interview. You’ll have a chance to talk more about your experience later. Don’t add too many spoilers. Making yourself more marketable Below you'll find ideas that, I believe, will make you more marketable. You can add these to your Github or resume. However, if you are doing the following, you will probably get noticed by companies and recruiters without having to apply anywhere. Tinker. Have a personal project. Something that shows you enjoy what you do. The best developers almost always code in their free time. This is easy to add and show on Github. If you don't code in your free time, but you tinker in other ways, let people know. Blog about your woodwork, quadcopters, catapults, whatever. Speak. Give a presentation or a lightning talk at your local meetup. Meetups are a great way to find other developers and businesses in the area. If you're shy, or don't like speaking, then blog. Write about the thing you're tinkering with. Or, do a \"today I learned\" style blog. Everyday post a sentence or two, and an example of something you learned. This is a much better way to show you are learning than a Github filled with tutorials. Contribute. Give something back to the community. Find a small project and add something to it. Add documentation or a bug fix, or spend some time helping others. Test. This is the hardest skill to learn. Find a mentor, or read other people's tests (Github is full of expert-level tests). Then practice, practice, practice. Add tests to your projects and make sure you link them in your resume. If you do any of the above you will be way ahead of the competition. If you do all of them, you can get a job anywhere. Last bit of advice: Don't stop once you get a job. Continuing to do these things will only help you grow as a developer and serve to promote you later. Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-03-03"},
{"website": "Hash-Rocket", "title": "2000 Today I Learned posts", "author": ["\nChris Erin\n\n"], "link": "https://hashrocket.com/blog/posts/2000-today-i-learned-posts", "abstract": "Community Hashrocket Projects 2000 Today I Learned posts by\nChris Erin\n\non\nMay  7, 2019 2000 TILs is a landmark but is just a small window into what we do at Hashrocket every day. Two Thousand This week, we published the 2000th post to our daily learning site, Today I Learned . The statistics page gives a sense of the scope and scale of this ever-evolving project. It is the result of continual small contributions by the entire Hashrocket team. Two thousand TILs began on April 13th, 2015. Springtime: a great time of year to begin something new. We've averaged a bit under two posts a day since starting to document these things in a digital, shareable way. Of course, before we had TIL as a platform there was a fair amount of non-shareable, non-documented learning. Okay, a \"fair amount\" is an understatement; Hashrocket's entire history has been about teaching and learning and craftsmanship and sharing. You can't be a great developer without obsessing over all the small ways that code can be written more elegantly and efficiently, and Hashrocket has seen more than a handful of great developers grace the keyboards and pairing stations of our two main offices. Web development was once just text, tags, and the network, but every year since the early nineties has seen higher expectations for what a website should be. Our eyes recoil at 1995's Space Jam or Bob Dole's 1996 Campaign Web Site . In 2006, creating a website had become prohibitively expensive for small organizations. Creating the expansive websites of that era took big teams of developers working with seemingly insane managerial overhead and software that was most definitely not free. Hashrocket's conceit in 2008 was that great developers using smart open source software (Ruby on Rails) and subtracting that managerial overhead would bring great websites to life for anybody with an idea. There is not a website built in 2008 that would come close to meeting expectations today. There's been at least five generations of JavaScript technologies since then! Security and performance requirements have multiplied, data sizes have gone from GB to TB, and of course the number of screen sizes that must be accommodated has grown as the screen sizes themselves (and bandwidth) have shrunk. Hashrocket's advantage in 2008 was retaining a set of developers that were at the cutting edge of web development, wanting to stay there, and setting up structures that promoted learning and expanding the craft of web development. From bookclubs in 2010 to internal conferences in 2012, to mentoring in 2018, the developers at Hashrocket have continually pursued the active structures that facilitate sharing and learning. Today it's our definitive strength that we learn together and share what we learn. Every Friday we have Show 'n Tell, and of course we pair. Pairing is continual positive reinforcement of sharing what you know. There is an inherent joy in showing somebody something new, and that in and of itself can drive you to explore further each of the endlessly deep technologies that we use on a daily basis. From that perspective, 2000 posts in Today I Learned is really not all that impressive. It could easily be 5000, or, if every little detail that each of us has internalized over the last three years were documented it would easily be 10,000. TIL serves as a small window into our company and represents very well our collective interests and joint journey in becoming ever better web devs. Was this post helpful? Share it with others. Tweet Share Post", "date": "2019-05-07"},
{"website": "Hash-Rocket", "title": "Switch vs. Map: Which is the Better Way to Branch in Go?", "author": ["\nJack Christensen\n\n"], "link": "https://hashrocket.com/blog/posts/switch-vs-map-which-is-the-better-way-to-branch-in-go", "abstract": "Switch vs. Map: Which is the Better Way to Branch in Go? by\nJack Christensen\n\non\nDecember 28, 2015 While working on pgx , a PostgreSQL driver for Go, I have run into multiple occasions where it is necessary to choose between 20+ branches. Typically, I have done this with a switch statement. A more readable alternative could be a map of functions. However, I would assume that the simple branch of a switch statement would be faster that a map lookup and function call. Performance in a database driver is of paramount concern, so it was prudent to investigate the performance implications before making any changes. TL;DR Micro-benchmarks show substantial performance differences. But the ultimate answer is it will very rarely matter to the program as a whole. But if you want to understand the research and testing behind this assertion, read on. Initial Research Searching the internet did not reveal much. The few posts I found suggested that maps may be faster if there are enough branches. A discussion of possible switch optimizations from 2012 included Ken Thompson's opinion that there was not much room for improvement. I decided to write a benchmark measure the performance in modern Go. A Basic Benchmark The following results were produced from a Intel i7-4790K running go1.5.1 linux/amd64 on Ubuntu 14.04. The benchmark source code and results are on Github . The most basic benchmark of a switch is as follows: func BenchmarkSwitch ( b * testing . B ) { var n int for i := 0 ; i < b . N ; i ++ { switch i % 4 { case 0 : n += f0 ( i ) case 1 : n += f1 ( i ) case 2 : n += f2 ( i ) case 3 : n += f3 ( i ) } } // n will never be < 0, but checking n should ensure that the entire benchmark loop can't be optimized away. if n < 0 { b . Fatal ( \"can't happen\" ) } Benchmarks, in particular micro-benchmarks like this are notoriously hard to get right. For example, if a block of code has no side effects an optimizer may omit the code entirely. The purposeful side effects to n are so the entire benchmark cannot be optimized away. There are several other pitfalls I will cover during this article. The basic benchmark of a map of function is as follows: func BenchmarkMapFunc4 ( b * testing . B ) { var n int for i := 0 ; i < b . N ; i ++ { n += Funcs [ i % 4 ]( i ) } // n will never be < 0, but checking n should ensure that the entire benchmark loop can't be optimized away. if n < 0 { b . Fatal ( \"can't happen\" ) } } A Ruby erb template is used to create benchmarks for 4, 8, 16, 32, 64, 128, 256, and 512 branches. Initial results showed that the map version was about 25% slower than the switch version for 4 branches, roughly equivalent at 8 branches, and continued to gain until it reached over 50% faster at 512 branches. Accounting for Inlineable Functions The previous benchmark gave some initial results, but it is not sufficient to stop here. There are multiple factors that have not been accounted for yet. First of these is whether the function can be inlined or not. A function call may be inlined in a switch statement, but that will never happen in a map of functions. It is worth determining how much of an impact that has on performance. The following function does a trivial amount of work to ensure that the entire call cannot be optimized away, but the function as a whole can be inlined by the Go compiler. func f0 ( n int ) int { if n % 2 == 0 { return n } else { return 0 } } At time of writing, the Go compiler cannot inline functions that can panic. The following function includes a panic call that will never be hit making it impossible for the function to be inlined. func noInline0 ( n int ) int { if n < 0 { panic ( \"can't happen - but should ensure this function is not inlined\" ) } else if n % 2 == 0 { return n } else { return 0 } } When the function cannot be inlined, performance changes substantially. The map version is ~30% faster at only 4 branches and over 300% faster at 512 branches than the switch version. Compute or Lookup Branch Destination The above benchmarks choose which branch to take based on the index of the benchmark loop. for i := 0 ; i < b . N ; i ++ { switch i % 4 { // ... } } While this lets us measure just the branch performance, in a real-world scenario the branch discriminator would typically incur a memory read. To simulate this, I introduced a simple lookup to find the branch. var ascInputs [] int func TestMain ( m * testing . M ) { for i := 0 ; i < 4096 ; i ++ { ascInputs = append ( ascInputs , i ) } os . Exit ( m . Run ()) } func BenchmarkSwitch ( b * testing . B ) { // ... for i := 0 ; i < b . N ; i ++ { switch ascInputs [ i % len ( ascInputs )] % 4 { // ... } // ... } This change drastically reduced performance. The best performing branch benchmark went from 1.99 ns/op to 8.18 ns/op. The best performing map benchmark went from 2.39 ns/op to 10.6 ns/op. The exact numbers varied somewhat for the different benchmarks, but in general the lookup added about 7 ns/op. Unpredictable Branch Order Astute readers may have noticed that the benchmark is still unrealistically predictable. It always follows branch 0, then 1, then 2, etc. To fix that, the slice of branch choices is randomized. var randInputs [] int func TestMain ( m * testing . M ) { for i := 0 ; i < 4096 ; i ++ { randInputs = append ( randInputs , rand . Int ()) } os . Exit ( m . Run ()) } func BenchmarkSwitch ( b * testing . B ) { // ... for i := 0 ; i < b . N ; i ++ { switch randInputs [ i % len ( ascInputs )] % 4 { // ... } // ... } This change further hinders performance. At 32 branches, the map lookup goes from 11 ns to 22 ns. Exact numbers vary based on number of branches and whether inlineable functions are used but performance is typically cut in half. Diving Way Deeper While the loss of performance from computed to looked up branch destination is somewhat expected due to the additional memory read, the switch from predictable to random order of visiting branch destinations is more surprising. To discover the underlying reason we turn to the Linux perf tool. It can give CPU-level statistics like cache misses and branch-prediction misses. To avoid profiling the compilation of the test, the test binary can be precompiled. go test -c Then we can ask the perf tool to give us statistics on one of our predictable lookup benchmarks. $ perf stat -e cache-references,cache-misses,branches,branch-misses ./go_map_vs_switch.test -test.bench=PredictableLookupMapNoInlineFunc512 -test.benchtime=5s The interesting part of the output is the branch prediction statistics: 9,919,244,177      branches\n10,675,162    branch-misses  # 0.11% of all branches So branch prediction is working great when the order is predictable. But something different happens when we run the unpredictable branch benchmark. $ perf stat -e cache-references,cache-misses,branches,branch-misses ./go_map_vs_switch.test -test.bench=UnpredictableLookupMapNoInlineFunc512 -test.benchtime=5s Relevant output: 3,618,549,427 branches                                                    \n  451,154,480 branch-misses  # 12.47% of all branches The branch miss rate goes up by over 100 times. Conclusion The original question I wanted to answer was if there would be a performance impact in converting a switch statement to a map lookup. I assumed that switches would be faster. I was wrong. Maps are usually faster, often several times faster. Does this mean we should prefer using maps over switches? No. While the percentage changes are substantial, the absolute time difference is small. It will only matter if the branch is executed millions of times per second and the actual work performed is minimal. Additionally, even if that is the case, the memory access pattern and branch prediction success rate have a bigger impact on performance than the choice of a switch or map. The choice of switch or map should be made based on whichever produces the clearest code. Was this post helpful? Share it with others. Tweet Share Post", "date": "2015-12-28"},
{"website": "Hash-Rocket", "title": "Document Your Confusion", "author": ["\nIfu Aniemeka\n\n"], "link": "https://hashrocket.com/blog/posts/document-your-confusion", "abstract": "Document Your Confusion by\nIfu Aniemeka\n\non\nNovember 28, 2017 Like a diary, but for code On a few occasions, I've started work on a brownfield project and found that, either due to poor programming practices or because there was no elegant way to implement a feature, it's been necessary to create documentation beyond what is normally included in a README. This documentation often took the form of glossaries, diagrams, or additional exposition on the codebase. Your typical README contains all or some of the following: a short introductory paragraph, a list of dependencies, instructions on how to install the app, configuration, how to run the test suite, database creation and migration, services, and how to deploy the app. If it's an open source project, there will likely be sections devoted to listing authors, rules around contributing, the license under which it can be used, as well as additional documentation around how to use the package. There are contexts in which this is inadequate.  Let's describe such a situation in detail. You've recently been assigned to a brownfield project. You're looking at the app for the first time, and you notice fairly quickly that the codebase looks like a Jackson Pollock painting. For starters, the company pivoted a couple years ago, so large portions of the app remain that aren’t relevant anymore. Perhaps entities are named in a way that no longer accurately describes them. Maybe there are references to features that have long since been abandoned. Obscured by all of this cruft are the operative parts of the app, which aren't much to look at, either. There are models named Article, Post, and Essay as well as models named Writer, Scribe, and Author. For business logic reasons, an Article can belong to an Author or a Writer, but not a Scribe, a Post can belong to all three, and an Essay can belong only to an Author. There's a \"God\" model that encompasses multiple concepts, another model that should really implement a state machine, etc. In short, this app needs a serious overhaul, but maybe you've been brought on to implement just a few features or the client simply doesn't have the budget to pay for an overhaul right now or the powers-that-be don't see the value in well-crafted software and aren't willing to pay you to refactor the code, no matter how many times you explain that substandard code actually costs more in the long run. Heck, maybe the client thinks the code is just fine and not in need of refactoring at all. For whatever reason, refactoring this code is not something you'll be able to do right now and you suspect that the technical debt that's accumulated will only increase for the foreseeable future, making the app ever more brittle and difficult to add features to. In an ideal world, this codebase would have been cleaned up ages ago. In the meantime, you still have to help your client. Unfortunately, the mental overhead of working on this app makes development a grind. You're constantly having to remind yourself of the subtle differences between an Author, a Writer, and a Scribe and how they relate to the other models in the application. What's needed is documentation that will reduce the mental overhead that comes with working with certain kinds of apps; basically, an augmented README. For simplicity, I'll refer to this 'superREADME' as a VELLUM ('cause it's fancy ).* I’ll also stop screaming README(!) and VELLUM(!) at you from here on out. A vellum and a readme are quite similar (really, the former could be thought of as a 'superset' of the latter). The difference is that a vellum adheres to a longer, more expository style of documentation than is typically found in readmes and there is an expectation that the author(s) of the vellum will reference it with some frequency. A vellum is the place where you document your confusion. A vellum can serve as a guide through less-than-ideal codebases, but it also has a place in well-architected projects (brownfield or greenfield) that have complex domains and require a bit of explication. You would use the vellum as a way to note difficult or subtle concepts around the domain or the codebase that any developer coming into the project should know before starting work. The remainder of this blog post will cover what should go into a thorough vellum file and some of the pitfalls of over-reliance on such documentation. It will be written from the perspective of a consultant working on a client application, but that isn't to say that vellums can't be carried over to open-source contexts. They can certainly be used to convey ideas that will help other or new contributors better understand the codebase. Introduction The first part of the vellum should consist of an introduction to the application including its purpose, its audience, and details around the business itself, such as how the app makes money. At this point, you're providing context. In the example below, we write an introduction to an application that helps users manage their stock portfolios. This application is called SmartMoney. It enables the casual trader to manage their portfolio of stocks. The app makes money by charging traders a flat per month fee of $5, in contrast to the payment structure of other services where users are charged per trade. The app allows users to make a wide variety of orders. Namely: market orders limit orders stop-loss orders stop-limit orders bracketed orders trailing stop-loss orders The app also provides a simple interface for shorting stocks. Some other key features of the app include the ability to set up notifications for when a particular stock's worth has dropped below or risen to a certain value. The app also has a recommendation engine that suggests stocks to invest in that are \"hot\" and in an industry in which the trader has shown interest. When development first began, the app was geared towards only sending notifications based on changes in stock prices. However, the company decided to make a more fully featured app as customers wanted an experience that integrated setting up notifications with making orders. You'll note that the introduction also lays out some terms (the types of orders that are available in the app) that the next developer should understand before writing any code (more about that soon). It also mentions the app's history, which helps to explain any cruft that might be left behind. Your goal is to convey enough salient information about the app that any developer could just open the vellum and know who this app is for, why certain quirks of the app are present, etc. You don't want to go into excruciating detail. Save that for storyboarding. Glossary You should follow this introductory paragraph with a glossary of terms. The glossary should include any terms you think a developer stepping into this project should be aware of, but reasonably might not know. ... ETF (Exchange Traded Fund): a collection of assets (stocks, bonds, etc.) that can be traded like a common stock, i.e. partial ownership in a corporation. P/E Ratio (Price-Earnings Ratio): measures the amount of money traders are willing to pay for each dollar of earnings from a single share. It is equal to the stock price divided by the earnings per share. PEG Ratio (Price-Earnings to Growth Ratio): It is similar to the P/E ratio, except that it takes the growth of the company into account. It is equal to the P/E ratio divided by the growth rate of the company. Stop-Loss Order: an order to sell a stock if its price drops below a specified value ... Pitfalls Your vellum file should also contain a section devoted to potentially confusing aspects of the app. You want to provide clarification on things like complex models or relations between models confusing or unusual model names distinctions between similar or closely-related models non-obvious relations between what a developer is dealing with on the backend versus what the user sees. Bracketed Order Model: This model corresponds to bracketed orders. On the backend, it is a bracketed order, but for the user, it is a 'One-Cancels-the-Other Order'. Architectural Decisions You should also take the time to justify non-obvious architectural decisions. This may include talking about design patterns you used: We opted for using the Factory design pattern to handle the creation of order objects. Order types are quite similar and we want the client to be order-agnostic. README The final section should discuss the topics you would cover in a readme, including things like configuration, how to run the test suite, database creation and migration, services, and how to deploy the app, etc. I assume you've already written your fair share of readmes. If not, read this brief tutorial on how to write a kick ass readme . Caveats You don't want to over-rely on your vellum file to the extent that you abandon good consulting and programming practices. If you're walking into a codebase badly in need of a refactor, this should be conveyed to the client. In such a situation, a vellum is stopgap documentation, i.e. explanation that makes up for code that is not self-explanatory. If you're writing the application from scratch, a vellum is used to elucidate complexity. It's not an excuse to write opaque code or inexact stories. Certainly, not every project requires a vellum file or all of the parts of a vellum detailed here. Simpler applications will generally not require one as there isn't enough mental overhead to justify having this extra, rather involved document you have to maintain. A good rule of thumb is that if you don't have to consult the vellum very often, then you can go ahead and delete it. Conversely, if you constantly find yourself checking your notes while coding, your notes likely contain information that should be extracted into some kind of documentation. Of course, all of the topics I described could simply be appended to a readme file. The important thing is to remember to reduce confusion for yourself and subsequent developers. * Vellum is a high-quality parchment made from the skin of calves Related articles: Acceptable Technical Debt Photo by Patrick Fore on Unsplash Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-11-28"},
{"website": "Hash-Rocket", "title": "The UI Controller, part 2: Faking It", "author": ["\nCameron Daigle\n\n"], "link": "https://hashrocket.com/blog/posts/the-ui-controller-part-2-faking-it", "abstract": "Ruby Design The UI Controller, part 2: Faking It by\nCameron Daigle\n\non\nJuly 26, 2012 It's an occasional, yet frustrating problem: I design a page with some temporary names and Lorem Ipsum filler content, only to see something wrap or look weird after implementation because the real content is unexpectedly long or annoyingly short. Plus, even when the content doesn't cause a problem, the actual workflow of pasting in filler content is suboptimal – I mean, I'm pasting Lorem Ipsum all over the page (which fills up the code with text that literally doesn't mean anything) and having to either come up with a bunch of names (\"John Doe, Jim Namerson, Roger Thisisalongnametotestspacing\") or just get lazy and repeat the same name over and over. Look, frontend slicing is time-consuming enough as it is; typing filler content shouldn't make it more so. Since writing about the UI controller process that we use for clean design handoffs, I've added another piece to the puzzle: a cleaner way to generate useful filler content through the Faker gem. Let Faker do the faking so you can focus on the work Faker's a little Ruby gem that's actually intended primarily to generate data for testing databases or whatever, but it's perfect for filling up static UI views. I'm not going to dig into the docs here, because they're a piece of cake for our usage here. Just call Faker:: + something; since this is just a static view, we don't care how inefficient that might be. Here's an example straight from a current project of mine: %table %thead - [ \"School Name\" , \"Students\" , \"District\" , \"City\" , \"State\" ]. each do | th | %th = th %tbody - 5 . times do %tr %td = link_to Faker :: Lorem . words ( 3 ). join ( ' ' ), \"#\" %td 3943 %td 4 %td = Faker :: Address . city %td = Faker :: Address . state_abbr Hey, look at that! Thanks to a simple each loop and some calls to Faker, we now have a table with some reasonably realistic content. Writing your filler copy with Faker But you might notice that I had to do a .join call after requesting Faker::lorem.words() . That's because Faker returns arrays when asked for multiple words & paragraphs, so I then had to join them together. That's incredibly suboptimal for generating body content – and that's the main thing we want Faker to help with, because we're tired of pasting in Lorem Ipsum, right? So let's break this down: if we just call Faker::Lorem.paragraphs(3) , we're going to get back an array of 3 strings. We'll need to wrap those in a <p> tag and concatenate them together, and call .html_safe() (so the paragraph tags don't get filtered to &lt;p&gt; ). So if we want to make 3 paragraphs, our call will look something like this: Faker :: Lorem . paragraphs ( 3 ). map { | text | content_tag ( :p , text )}. join . html_safe Yuck, I don't know about you, but I don't want to ever type that more than once. Let's put it in a helper. module UiHelper def lorem ( paragraphs ) Faker :: Lorem . paragraphs ( paragraphs ). map { | text | content_tag ( :p , text )}. join . html_safe end end Hooray! Now you can write one clean line in your HAML and Faker will do the work for you. = lorem ( 3 ) Beautiful. No more trips to the Lorem Ipsum Generator for me. Was this post helpful? Share it with others. Tweet Share Post", "date": "2012-07-26"},
{"website": "Hash-Rocket", "title": "Advanced Queries with ActiveRecord's From Method", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/advanced-queries-with-activerecord-s-from-method", "abstract": "Ruby Advanced Queries with ActiveRecord's From Method by\nJosh Branchaud\n\non\nSeptember 12, 2017 For those of us that write database-backed Rails apps, ActiveRecord is a\nfamiliar beast. We know its various query building methods well: where , order , limit , and even group . The most important SQL clause is\nconspicuously missing from our repertoire: the from clause. Chances are you can't think of many, if any, times that you've used ActiveRecord 's from method. This is to be expected. The from clause in\nSQL specifies the target table of a query. In the context of ActiveRecord, the\ntarget table is explicitly expressed by the model from which you query. Consider the times you use one-liners such as Developer.first , Developer.all , and Developer.count . With each of these, the developers table as the target of the query is expressed by the use of the Developer model. So, where does the from method come into the picture? The docs describe the from method as such. Specifies table from which the records will be fetched. Specifying the table of a query against an ActiveRecord model is going to feel\na bit redundant. > Developer . from ( 'developers' ). to_sql => \"SELECT \\\" developers \\\" .* FROM developers\" As you've probably noticed we get this same behavior with nothing more than the all method. > Developer . all . to_sql => \"SELECT \\\" developers \\\" .* FROM \\\" developers \\\" \" So, again, where does the from method come into the picture? Our from clauses aren't always going to look as simple as from developers .\nWe sometimes need the \"table\" from which we are fetching to be a collection of\ncomputed values. The key insight here is that the result of a query is itself a\ntable. That means that we can query against the result of another query. This\nis ostensibly how subqueries work. The from method comes into play when we are building up a complex query\nin SQL and need to transition back to the land of ActiveRecord and Rails. Let's work with the developer_scores.sql query from the hr-til codebase . select developers . id , username , posts , likes , round ( likes :: numeric / posts , 2 ) as avg_likes , round ( log ( 2 , ( 1022 * (( score - min ( score ) over ()) / (( max ( score ) over ()) - ( min ( score ) over ()))) + 2 ):: numeric ), 1 ) as hotness from developers join ( select developer_id as id , sum ( score ) as score from hot_posts group by developer_id ) developer_scores using ( id ) join ( select developer_id as id , count ( * ) as posts , sum ( likes ) as likes from posts group by developer_id ) stats using ( id ) This query is used to compute both the average number of likes and the hotness score for each developer's posts. Because of the complexity of the\nquery, we are better off writing it in SQL rather than using ActiveRecord's\nDSL. This query does a complex computation that results in a bunch of data. It\nwould be nice if we could further refine the results as needed with\nActiveRecord's DSL. And herein lies the challenge. How do we transition from\nthis SQL query to something compatible with ActiveRecord? The from method -- the topic of this post -- is, of course, the answer. We\ncan treat the above query as a \"table\" that we can query against (i.e. a\nsubquery). This allows us to then use order , limit , group , etc. to\nfurther refine the result set. We can create a class ( app/models/developer_ranker.rb ) to see how this can work. class DeveloperRanker RANKED_DEV_SQL = <<- SQL (select\n       developers.id,\n       username,\n       posts,\n       likes,\n       round(likes::numeric / posts, 2) as avg_likes,\n       round(log(2, (1022 * ((score - min(score) over ()) / ((max(score) over ()) - (min(score) over ()))) + 2)::numeric), 1) as hotness\n      from developers\n      join (\n        select developer_id as id, sum(score) as score\n        from hot_posts\n        group by developer_id\n      ) developer_scores using(id)\n      join (\n        select\n        developer_id as id,\n        count(*) as posts,\n        sum(likes) as likes\n        from posts\n        group by developer_id\n      ) stats using(id)\n      ) as developers SQL def self . top_developers ( count = 1 ) Developer . from ( RANKED_DEV_SQL ) . order ( hotness: :desc ) . limit ( count ) end def self . bottom_developers ( count = 1 ) Developer . from ( RANKED_DEV_SQL ) . order ( hotness: :asc ) . limit ( count ) end end We treat the complex query as a subquery by wrapping it in (...) as\ndevelopers assigning it as a string to a constant. This constant is then\navailable to any methods that want to build on this query without duplicating\nall the core query logic. As you can see in DeveloperRanker.top_developers , we hydrate an ActiveRecord::Relation with the results of the query. We can then chain those\nfamiliar ActiveRecord query methods on the end. In this case, we order the\nresults with the developers most on fire at the top and then limiting by the\ngiven count .  Note that because we are querying against the Developer model, it is essential that the subquery's name matches the name that\nActiveRecord uses for that model's table. We create DeveloperRanker.bottom_developers to perform a similar query with\nvery little duplication. An additional benefit of this approach is that the Developer objects that are\nbeing hydrated are decorated with the additional values returned from the\nquery. Not only do we know the id and username of each developer, we can\nalso access the avg_likes and hotness attributes. These hydrated Developer objects can be passed to the view layer like any other decorated\nobject would be. This technique is great for generating reports and stats that require lots of\ncomputation or complex joins, but are ultimately tied to particular domain\nconcepts (e.g. a developer). Ultimately, this frees you to write some SQL without completely cutting\nyourself off from the world of ActiveRecord . Enjoy! Cover photo by Samuel Zeller on Unsplash.com Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-09-12"},
{"website": "Hash-Rocket", "title": "Get Started with redux-form", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/get-started-with-redux-form", "abstract": "Javascript React Get Started with redux-form by\nJosh Branchaud\n\non\nJuly 27, 2017 The redux-form library bills itself as the best way to manage your form state in Redux. It provides a higher-order form component and a collection of container components for dealing with forms in a React and Redux powered application. Most importantly, it makes it easy to get a form up and running with state management and validations baked in. To get a feel for what redux-form can do for us, let's build a simple Sign In form. You can follow along below or check out the resulting source code directly. Start with React We are going to start by getting the React portion of this application setup. The easiest way to spin up a React app is with create-react-app . Install it with npm or yarn if you don't already have it globally available. $ yarn global add create-react-app Let's generate our project. $ create-react-app redux-form-sign-in The create-react-app binary that is now available on our machine can be used to bootstrap a React app with all kinds of goodies -- live code reloading in development and production bundle building -- ready to go. Let's jump into our project and kick off a live reloading development server. $ cd redux-form-sign-in $ yarn start At this point you should see your browser pointed to localhost:3000 with a page reading Welcome to React . Is This Thing Plugged In? We can see the live-reload in action by altering src/App.js . return ( < div className = \"App\" > < div className = \"App-header\" > < img src = { logo } className = \"App-logo\" alt = \"logo\" /> < h2 > Redux Form Sign In App < /h2 > < /div > < p className = \"App-intro\" > To get started , edit < code > src / App . js < /code> and save to reload . < /p > < /div > ); Change the text in the h2 , save the file, and then switch back to the browser to see the changes almost instantly. create-react-app made it really easy to get to a point where we can just iterate on our app. We didn't have to fiddle with Webpack configurations or any other project setup. Next, let's change the prompt in the app intro. < p className = \"App-intro\" > Sign in here if you already have an account < /p > Our app is now begging for a form which brings us to redux-form , the focus of this post. Satisfying Some Dependencies Let's add the redux-form dependency to our project. $ yarn add redux-form It is now available to our app, so we can import the reduxForm function at the top of src/App.js right after the react import. import React , { Component } from 'react' ; import { reduxForm } from 'redux-form' ; If we save the file, our development server will have a complaint for us regarding an unmet dependency. Module not found: Can't resolve 'react-redux' in '/Users/dev/hashrocket/redux-form-sign-in/node_modules/redux-form/es' The same issue will arise for the dependency on redux itself. So, you'll want to add both to the project. $ yarn add react-redux redux Trigger a reload by saving and you'll see that the code is successfully compiling again. Not without warnings though. reduxForm is never being used, so let's use it. A Basic Form The reduxForm function is the higher-order component (HOC) that creates our super-charged form component. In particular, it wires this component up to redux for management of our form's state. First, let's add an empty form in a presentational component. // src/App.js let SignInForm = props => { return < form /> ; }; Then, let's transform it into a redux-connected form using the reduxForm HOC. // src/App.js SignInForm = reduxForm ({ form : 'signIn' , })( SignInForm ); The only required configuration property is form which specifies a unique name for the form component. If you wanted to create multiple instances of the same form on a page, each would need a separate name in order to manage their separate form states. Let's render our new form component into our existing React app. // src/App.js return ( < div className = \"App\" > < div className = \"App-header\" > < img src = { logo } className = \"App-logo\" alt = \"logo\" /> < h2 > Redux Form Sign In App < /h2 > < /div > < p className = \"App-intro\" > Sign in here if you already have an account < /p > < SignInForm /> < /div > ); Saving results in successful compilation, but there is a runtime error. The details of which are displayed in your browser. A Redux Store The first part of the error reads: Could not find \"store\" in either the context or props of \"Connect(Form(SignInForm))\". We are missing a redux store to which reduxForm can connect our form component. The store is the place where redux will manage our form's state. The error helpfully suggests two ways to address this problem. Let's do the first and add a provider to src/index.js . // src/index.js // ... leave existing import statements intact import { Provider } from 'react-redux' ; import { createStore , combineReducers } from 'redux' ; import { reducer as formReducer } from 'redux-form' ; const rootReducer = combineReducers ({ form : formReducer , }); const store = createStore ( rootReducer ); ReactDOM . render ( < Provider store = { store } > < App /> < /Provider> , document . getElementById ( 'root' ) ); // ... The Provider is a component from react-redux that makes our store available to child components. The exact details are abstracted away by redux-form . The react-redux docs have more details if you are interested. The Provider requires one prop, the store . We set up our store with createStore from redux. createStore is given a top-level reducer that it can use to update any of our state in response to actions, such as changes to the form. We construct this top-level reducer, rootReducer , using combineReducers . Don't worry if you are not familiar with Redux's core concepts, that is the extent of what we need to know for this post. If you want to know more I do highly recommend Dan Abramov's course, 'Getting Started with Redux' . Save these changes and our form will render successfully. Because we didn't add any form elements yet, you'll have to inspect the DOM with dev tools to confirm that. A Form with Fields A form without any fields isn't much of a form. So, let's add some with the Field component provided by redux-form . Because this is a Sign In form, we'll need an Email field, a Password field, and a submit button to sign in. // src/App.js let SignInForm = props => { return ( < form > < label > Email < /label > < div > < Field type = \"text\" name = \"email\" component = \"input\" /> < /div > < label > Password < /label > < div > < Field type = \"password\" name = \"password\" component = \"input\" /> < /div > < button type = \"submit\" > Sign in < /button > < /form > ); }; The label and button parts of the above JSX are pretty standard parts of a form. The interesting bit is the Field component. For each we specify the component prop as input because these, for the time being, are both simply input tags. The type prop gets passed down to each of the inputs specifying one as a standard text input and the other as a password input. The name is important because it will be used to identify the state of the fields in the redux store. Our app won't recognize the Field component until we import it, so let's update this statement at the top of the same file. // src/App.js import { reduxForm , Field } from 'redux-form' ; If we save our file and check out the re-rendered page, we'll see our form. Go ahead and inspect the DOM to see how the Field components were translated to <input> tags. Unfortunately, this form isn't much to look at right now. I'll leave styling it as an exercise for the reader. Form State Clicking our Sign in button at this point isn't all that satisfying. We want to know that the values that have been entered into our form fields are being managed by redux. Let's pass down a callback function that our form can call when it is submitted. // src/App.js handleSignIn = values => { console . log ( 'Submitting the following values:' ); console . log ( `Email: ${ values . email } ` ); console . log ( `Password: ${ values . password } ` ); }; This function will simply spit out the form values with console.log . This is more or less where you'd want to integrate the app to some backend system. We won't deal with a backend in this post. If we give this handler function to our redux form, SignInForm , via the onSubmit prop, it will be available to us in the props of our form component. // src/App.js render () { return ( < div className = \"App\" > < div className = \"App-header\" > < img src = { logo } className = \"App-logo\" alt = \"logo\" /> < h2 > Redux Form Sign In App < /h2 > < /div > < p className = \"App-intro\" > Sign in here if you already have an account < /p > < SignInForm onSubmit = { this . handleSignIn } / > < /div > ); } The callback function that we passed in to our form is wrapped in the handleSubmit function. This can be destructured from our form's props and passed in to the onSubmit of our <form> . // src/App.js const { handleSubmit } = props ; return ( < form onSubmit = { handleSubmit } > // ... < /form > ); Enter in a value for Email and Password and hit submit. You should see those values in the console output. Awesome! We don't want our form to allow just any values though. Do You Validate? redux-form comes with built-in support for both synchronous and asynchronous validations. To start, let's add synchronous validations to require both the email and password; that is, we won't allow blank values. This can be done by defining a validate function. // src/App.js const validate = values => { const errors = {}; if ( ! values . email ) { console . log ( 'email is required' ); errors . email = 'Required' ; } if ( ! values . password ) { console . log ( 'password is required' ); errors . password = 'Required' ; } return errors ; }; This function starts with errors as an empty object. If the function ultimately returns an empty object, then there were no errors and the form is valid. In fact, if you don't define the validate function, it defaults to (values, props) => ({}) -- always valid. The conditional logic in the middle of our function is what decides whether to flag any validation errors. Our current implementation checks for the presence of each field. Additionally, we console.log every time a validation check fails so that we can see it working as we develop. To override the default implementation, we have to tell SignInForm to use our implementation. This is done in the form constructor using es6's object literal property shorthand . // src/App.js SignInForm = reduxForm ({ form : 'signIn' , validate , })( SignInForm ); If you open up the dev tools console and then try typing into the fields, you'll notice that every single change to the form triggers the validate function. This ensures that we always have fresh information about the validity of our form. It also means that our form defaults to invalid. As such, we need to be discerning in how and when we display those validation errors. Metadata provided to our Field component allows us to do that as we'll see in the next section. Additionally, we may want a validation that ensures the value entered into the email field looks like an email address. To achieve that, we can update our validate function like so. if ( ! values . email ) { console . log ( 'email is required' ); errors . email = 'Required' ; } else if ( ! /^.+@.+$/i . test ( values . email )) { console . log ( 'email is invalid' ); errors . email = 'Invalid email address' ; } It is a rather crude check of the email address field, but it will suffice for this post. Our validate function will now ensure that even if the email is present that it still conforms to the email address regex we've laid out. As long as there are validation errors on the errors object, we will not be able to submit the form. That is, our handleSignIn function will not be triggered. But beyond that and our console.log statements, there is no outward expression of the validation errors. Displaying Errors Earlier on I hinted that using input as the component for Field was only temporary. Let's extract a functional component with logic for displaying our errors. We can use that custom component, instead of input , with Field . // src/App.js const InputField = ({ input , label , type , meta : { touched , error , warning }, }) => < div > < label > { label } < /label > < div > < input {... input } type = { type } / > < /div > < /div> ; The props passed into InputField include the input object, a label which we will need to specify, the type of input it is (e.g. password ), and some redux-form specific metadata. Using InputField , we are able to greatly simplify and dry up the JSX in SignUpForm . // src/App.js return ( < form onSubmit = { handleSubmit } > < Field type = \"text\" name = \"email\" component = { InputField } label = \"Email\" /> < Field type = \"password\" name = \"password\" component = { InputField } label = \"Password\" /> < button type = \"submit\" > Sign in < /button > < /form > ); We are now in a position to decide how we want to render our errors. Better yet, the JSX for it only needs to be defined in one place, inside the InputField component. // src/App.js const InputField = ({ input , label , type , meta : { touched , error , warning }, }) => < div > < label > { label } < /label > < div > < input {... input } type = { type } / > < /div > { touched && error && < div > { error } < /div> } < /div> ; We can render a div with the error message when there is an error. That isn't quite good enough though. Remember when I mentioned being discerning about when to render errors. The user experience of seeing an error the second the page loads is not great. Fortunately, redux-form lets us know through the touched flag in the metadata if a particular field has been focused and then blurred. By combining touched and the presence of an error we are able to display an error message only when necessary. Check it out in the browser to see the various validation messages we programmed into our form. Conclusion and Next Steps That's it. We made it. A lot was covered in this post. Having gone all the way through, you should now have a solid foundation for getting redux-form set up with create-react-app , building out a simple form, adding validations, and responding to a form submission. redux-form is capable of a lot more than what we have explored in this post. There are a number of examples on their site that I'd recommend checking out to see what is available. Looking for a next step? Try connecting the form to a backend system. Then, see if you can get Async Form Validation working. Cover image by Steinar Engeland on Unsplash. Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-07-27"},
{"website": "Hash-Rocket", "title": "Do The Shuffle", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/do-the-shuffle", "abstract": "Ruby Do The Shuffle by\nJosh Branchaud\n\non\nJune 29, 2017 How should we break up teams for this game? In what order should we do\npresentations? Which lunch spot should we go to this week? The need for some\nrandomization arises often. With irb at our fingertips, we often reach for Ruby's shuffle . But have you ever wondered how shuffle works? I can imagine writing a loop that randomly shuffles around the contents of\nan array, but what would I need to do in order to sufficiently and\nefficiently shuffle an array? What I need is an algorithm. An Algorithm When I do a google search for \"algorithm to shuffle a\nlist\" , the first result\nis a Wikipedia article on the Fisher-Yates shuffle. The Fisher–Yates shuffle is an algorithm for generating a random\npermutation of a finite sequence—in plain terms, the algorithm shuffles\nthe sequence. The algorithm effectively puts all the elements into a hat;\nit continually determines the next element by randomly drawing an element\nfrom the hat until no elements remain. The algorithm produces an unbiased\npermutation: every permutation is equally likely. That description tells me that this algorithm will be both sufficient and\nefficient. Great, let's proceed. Here is the pseudocode for the Fisher-Yates shuffle: -- To shuffle an array a of n elements (indices 0..n-1):\nfor i from n−1 downto 1 do\n     j ← random integer such that 0 ≤ j ≤ i\n     exchange a[j] and a[i] An Implementation We can implement that in Ruby no problem. Something like this should work. class Array def fisher_yates_shuffle upper = self . length - 1 upper . downto ( 1 ). each do | current_index | random_index = rand ( current_index + 1 ) temp = self [ random_index ] self [ random_index ] = self [ current_index ] self [ current_index ] = temp end self end end The idea behind the algorithm is to go from the end to the beginning\nreplacing the value at each position in the array with a value from a random\nposition in the remaining unset portion of the array. This approach will\nfinish in linear time and we can expect to have a well-shuffled array. But does this approach have anything to do with the way Ruby's shuffle works? A Real-World Implementation We can do some source-diving to find out. The challenge here as we'll\nquickly find out is that Ruby's Array class is entirely implemented in C.\nAfter cloning Ruby's git repository , we can\nopen up array.c . Searching for shuffle , we find a function called rb_ary_shuffle which makes a call to rb_ary_shuffle_bang . Navigating to rb_ary_shuffle_bang , we find the meat of the implementation (I've omitted\nsome less interesting bits): rb_ary_shuffle_bang ( int argc , VALUE * argv , VALUE ary ) { ... i = len = RARRAY_LEN ( ary ); RARRAY_PTR_USE ( ary , ptr , { while ( i ) { long j = RAND_UPTO ( i ); VALUE tmp ; if ( len != RARRAY_LEN ( ary ) || ptr != RARRAY_CONST_PTR ( ary )) { rb_raise ( rb_eRuntimeError , \"modified during shuffle\" ); } tmp = ptr [ -- i ]; ptr [ i ] = ptr [ j ]; ptr [ j ] = tmp ; } }); /* WB: no new reference */ return ary ; } If we stare at it for a moment and let the C sink in a bit, we may start to\nnotice the familiarity of this function. We have an i value that starts as\nthe length of the array and is decremented each loop. We produce a random\nindex value from i . We do some array value swapping each step of the way with tmp . Lastly, we return the shuffled array ( ary ). As it turns out, Ruby's shuffle implementation also uses the Fisher-Yates\nShuffle. Cool! A Conclusion We've looked at pseudocode, Ruby, and C. We've also learned a bit about\nalgorithms and added the Fisher-Yates Shuffle to our algorithm-toolbelt.\nNext time you go to shuffle an array, you'll know exactly what is going on\nbehind the scenes. So, where should we go for lunch? restaurants = [ 'Umami Burger' , 'Chicken and Farm' , 'De Cero Tacos' , 'Bad Hunter' ] restaurants . fisher_yates_shuffle . first #=> 'Chicken and Farm' Cover photo by Anders Jildén on Unsplash Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-06-29"},
{"website": "Hash-Rocket", "title": "Swap Two Column Values in SQL: Part 1", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/swap-two-column-values-in-sql-part-1", "abstract": "PostgreSQL Swap Two Column Values in SQL: Part 1 by\nJosh Branchaud\n\non\nSeptember 19, 2017 I recently needed to write a PostgreSQL snippet that would massage some\ndata. More specifically, I needed to swap the ordering of two different\nrecords in the pages table.  These records are made up of an id , a name , and an ordering . Here is a snapshot of the data in pages . select * from pages order by ordering ; id | name | ordering ----+--------+---------- 3 | Green | 1 1 | Red | 2 4 | Yellow | 3 2 | Blue | 4 We've been informed that we need to swap the ordering of Red and Blue .\nWe can see here in this snapshot of the data that Red has an ordering of 2 and Blue has an ordering of 4 . Doing a swap based on the data that we have locally would be most easily\naccomplished with a couple update statements. update pages set ordering = 4 where name = 'Red' ; update pages set ordering = 2 where name = 'Blue' ; Unfortunately, we've been informed that these exact ordering values cannot\nbe guaranteed in the primary data set. All we know is that Red and Blue need to be swapped. So, how do we write a general purpose swapping statement that works both\nwith our local dataset and with the primary data set? We can combine the updates into a single statement doing the value swapping\nbased on the evaluation of a case statement that utilize subqueries. update pages set ordering = case name when 'Red' then ( select ordering from pages where name = 'Blue' ) when 'Blue' then ( select ordering from pages where name = 'Red' ) end where name = 'Red' or name = 'Blue' ; We can run it and then check that it works: select * from pages order by ordering ; id | name | ordering ----+--------+---------- 3 | Green | 1 2 | Blue | 2 4 | Yellow | 3 1 | Red | 4 But how does it work? At first glance, it seems like updating one record's ordering could mean\nthat we clobber the ordering value that we need in order to update the\nother. Shouldn't we need some sort of temporary variable? We've probably\nseen imperative code statements that look something like the following: let temp = a\na = b\nb = temp Why don't we need a temporary value with our PostgreSQL statement? The answer has to do with the consistency guarantees that PostgreSQL\nprovides us. Throughout the entire course of a single update statement,\nPostgreSQL is going to provide a single, consistent snapshot of our data. If\nthis wasn't the case, then the subqueries inside of the case statement\nwould have data changing out from underneath them. This technique not only massages our data as requested, it shows us the\ndynamism of update statements in PostgreSQL. We are able to utilize case statements and subqueries to get exactly what we need out of the statement.\nThis statement isn't flawless though. In part 2 of this post, we'll explore\nanother use case and see how we can build on this technique. Supporting SQL Create the pages table. create table pages ( id serial primary key , name varchar not null , ordering integer not null ); Insert some records into the pages table. insert into pages ( name , ordering ) values ( 'Red' , 2 ), ( 'Blue' , 4 ), ( 'Green' , 1 ), ( 'Yellow' , 3 ); Cover image by Dmitri Popov on Unsplash.com Shout out to Jake Worth with whom I worked on this particular solution. Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-09-19"},
{"website": "Hash-Rocket", "title": "Writing Prettier JavaScript in Vim", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/writing-prettier-javascript-in-vim", "abstract": "Javascript React Writing Prettier JavaScript in Vim by\nJosh Branchaud\n\non\nJuly 13, 2017 JavaScript is an incredibly powerful and fickle language. In the face of\nquickly evolving frameworks and tooling, it can be cognitively challenging\nto grapple even with a familiar codebase. Even just having one less thing to\nthink about goes a long way in helping us deal with the rest. Figuring out\nhow to format each line of code is accidental complexity and one less thing\nwe ought to think about. As Brian Dunn put it, \"Once you stop formatting your code, life becomes\nbeautiful.\" And so does your code, but without any of the effort. This is where Prettier comes in. At Hashrocket, Vim is an essential part of our development tooling. neoformat makes it easy to\nincorporate prettier into our existing workflow. As you can see in the\nabove gif, Prettier formats the file on save. Here is how we set it up. Step 1 Install prettier if you haven't already. $ yarn global add prettier (or npm install -g prettier if you prefer) Step 2 Install the neoformat vim plugin. \" ~/.vimrc Plug 'sbdchd/neoformat' This plugin is a general-purpose formatter that can be configured to work\nacross many filetypes. You can check out the repository README for the full\nlist of supported filetypes. Step 3 Next we need to configure Neoformat to use Vim's formatprg as its\nformatter. let g:neoformat_try_formatprg = 1 This way in the following step we can specify some options for prettier . Step 4 We want neoformat to be configured specifically to format our JavaScript\nfiles with prettier using a few configuration flags of our choosing. We\ncan set up an autogroup in our ~/.vimrc file for that. \" ~/.vimrc augroup NeoformatAutoFormat\n    autocmd ! autocmd FileType javascript setlocal formatprg = prettier\\ \\ -- stdin\\ \\ -- print - width\\ 80 \\ \\ -- single - quote\\ \\ -- trailing - comma\\ es5\n    autocmd BufWritePre * . js Neoformat\naugroup END The FileType line sets prettier as the format program whenever a javascript file is encountered. formatprg will provide the file contents\nto prettier via stdin , so the --stdin flag tells prettier to expect\nas much. The --print-width flag tells prettier to keep lines under 80\ncharacters. And so on. The rest of the details are in the Options section of the\nPrettier README. Step 5 We work on a lot of React.js projects, so having support for JSX is\nessential. The time it takes to reformat long lines of JSX means that we can\nfeel the time savings quite acutely when prettier does the formatting for\nus. To accommodate React, we can update our existing autogroup by adding support\nfor the jsx filetype. This requires that you already have the vim-jsx plugin installed. \" ~/.vimrc augroup NeoformatAutoFormat\n    autocmd ! autocmd FileType javascript , javascript . jsx setlocal formatprg = prettier\\ \\ -- stdin\\ \\ -- print - width\\ 80 \\ \\ -- single - quote\\ \\ -- trailing - comma\\ es5\n    autocmd BufWritePre * . js , * . jsx Neoformat\naugroup END All Set Give this setup a try and enjoy some consistent, well-formatted code. Forget\nabout the formatting and focus on the function. Shoutout to Dorian Karter for\nintroducing me to Prettier in Vim. Cover photo by JOHN TOWNER on\nUnsplash Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-07-13"},
{"website": "Hash-Rocket", "title": "Represent Your Work", "author": ["\nDaniel Ariza\n\n"], "link": "https://hashrocket.com/blog/posts/represent-your-work", "abstract": "Design Represent Your Work by\nDaniel Ariza\n\non\nNovember 16, 2015 As this magical thing called the internet has connected us all, the world has become much smaller. Even though we are all seemingly more connected it often results in a disconnect between us. One of the ways this impacts us as designers, at Hashrocket, is we are rarely in the same physical room with our clients to talk about our work. Since we are big on presenting our work directly to our stakeholders this can present some challenges. So we built Compton to better represent our work and now we're looking for some beta users . We believe it’s important to do presentations because a lot of thought and care went into each decision. We want our clients to not only understand the thought process that went into it’s creation but be a part of the dialog throughout it’s evolution. We have found that doing a formal presentation always helps us arrive at a solution much faster and yields better results overall. So the question is… how do you do this when your clients are in another region or country? To overcome these issues we use tools like Basecamp, Invisi/on, Flinto, Slack, email, and Google Hangouts to name a handful. These are all great tools but we haven’t been able to solve some of the issues we have had with doing remote presentations.  So we decided to build our own solution, a lightweight presentation tool that helps us to show our work at it’s best. To do this we had to start where everyone does… at the beginning. What are our goals and how do we define success? Here is a list of some of our initial goals for the application and solutions. We want a presentation tool, not a collaboration tool. There are already a lot of great choices for collaboration software. Invision and Basecamp are great at this. They allow for asynchronous feedback. It’s a conversation. This is great once we get past a certain point but in the beginning it’s more important to establish some rapport and kick off that conversation on a good note. Too often we see clients bypass that eloquent explanation we wrote and go straight for the link to the images or PDF. We want the opportunity to communicate about our work, so our client can understand and appreciate the care that went into it. We want our work shown at its best. Client: “Why do the fonts look weird? Things look pixelated.” Designer: “Well the browser is scaling the image down to fit in your browser. Use the magnifying glass to zoom in.\" We don’t want these kinds of conversations to happen. You, the designer, choose the best way to display your work and the software should get out of the way. Upload your slides at the size you want them to be viewed. We want to control what the client sees and when they see it. Ever have a client looking at page 3 of your document when you are still on page 1? It’s understandable. They are probably just excited to see your new work. It just makes it hard to present to someone who is off looking at something shiny. We want them to be on the same page. There is a flow to a presentation and we want to control it. We want to be able to call attention to items of interest. Since there may be a lot to look at, it would be helpful to be able to highlight an area of interest or discussion. We don’t want to clutter up your design with little icons. How about the ability to highlight an area with a selection tool and remove when you are ready to move on? We don’t want to forget to explain certain details. We have been in the habit of making notes for ourselves before the presentation begins. Depending on how much you have to cover, it could be easy to forget something. It would be great to have notes associated with each slide in your presentation. We don’t want our viewers to need a username and password. We all have too many passwords already. Let’s lower the bar for our viewers and not require them to register. Just invite them to the presentation and you will be notified when they arrive. Start the presentation whenever you are ready. After some really fun work and experimentation we have built the beta version of our dream presentation tool. We call it Compton. You know… because you present… comps. It has already helped us internally create and share better presentations. We hope it will help you too. We are asking for the help of some beta testers to give it a try . Tell us how you like it and where it could be improved. We already have some additional features in the works but your voice would be valuable. Was this post helpful? Share it with others. Tweet Share Post", "date": "2015-11-16"},
{"website": "Hash-Rocket", "title": "Datetime select", "author": ["\nPavel Pravosud\n\n"], "link": "https://hashrocket.com/blog/posts/datetime-select-in-cucumber", "abstract": "Ruby Datetime select by\nPavel Pravosud\n\non\nOctober  7, 2014 Sometimes, default Date and DateTime Rails selects might be hard to use from your Cucumber tests. Luckily, we can always build some custom steps to work around this problem. Consider a trivial user registration form with a birthday field: = form_for User . new do | form | = form . label :birthday = form . date_select :birthday It typically generates HTML similar to this: <form accept-charset= \"UTF-8\" action= \"/users\" method= \"post\" > <label for= \"user_birthdate\" > Birthdate </label> <select id= \"user_birthdate_3i\" name= \"user[birthday(3i)]\" ><option> Day </option> <select id= \"user_birthdate_2i\" name= \"user[birthday(2i)]\" ><option> Month </option> <select id= \"user_birthdate_1i\" name= \"user[birthday(1i)]\" ><option> Year </option> </form> There are two problems with this code: The label does not actually point to any of the selects. Select ids are\ndifferent from labels for attribute. There are 3 separate selects for a single database field that could go in\nany arbitrary order. Both problems make these types of fields super inconvenient to access from\nCapybara/Cucumber tests. Your usual select \"1986/08/25\", from: \"Birthday\" is not\ngoing to work here. Custom Cucumber step to the rescue: When /^I fill in \"(.*?)\" date field with \"(.*?)\"$/ do | field_name , date_components | label = find ( \"label\" , text: field_name ) select_base_id = label [ :for ] date_components . split ( \",\" ). each_with_index do | value , index | select value . strip , from: \" #{ select_base_id } _ #{ index + 1 } i\" end end Now you can fill in date selects like this: When I fill in \"Birthdate\" date field with \"1986, Aug, 25\" This step could also easily be adopted to work with DateTime select fields. Notice that you have to\npass date and time components in the order Rails expects it: year goes first, followed by month and then day. Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-10-07"},
{"website": "Hash-Rocket", "title": "Understanding the Buffer List in Vim: Part 1", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/understanding-the-buffer-list-in-vim-part-1", "abstract": "Vim Understanding the Buffer List in Vim: Part 1 by\nJosh Branchaud\n\non\nMarch  2, 2016 Perhaps the most important component in editing text with Vim is the buffer. Understanding buffers, how Vim organizes them, and how you can manage them is essential to effectively using Vim. A buffer is the in-memory text of a file. Any time we open an existing file or create a new one using Vim, a buffer will be allocated as the in-memory representation of said file. Any changes we make will be tracked within the buffer. When we are ready to update the file, the buffer can be written to disk. Vim allows us to work with a bunch of different buffers at once. When working on large projects, the list of buffers can quickly grow out of control. Being able to read, understand, navigate, and manage the list of buffers can be challenging. With the right tools at our fingertips, we should be able to get by. At the end of this series, we will know how to leverage many built-in Vim features and a handful of plugins to this end. In Part 1, we will explore the basics of the buffer list. In particular, we will walk through a step by step scenario to see how Vim populates the buffer list and to learn what each part of the buffer list means. Let's get started. Listing Buffers During a long running Vim session, we will quickly accumulate more than a few buffers. We can use ls at any time to get a handle on the current list of buffers. To see this in action, let's navigate to a new directory, touch a new file, and then open it up in Vim. $ mkdir understanding-buffers $ cd understanding-buffers $ touch file.md $ vim file.md We now have a Vim session running with what we can expect to be a single buffer. Let's see. :ls\n  1 %a   \"file.md\"                      line 1 At this point, the ls command reveals that we only have the one buffer open -- file.md . There is some other information displayed. The 1 is the buffer number. This is the first and only buffer, so it was assigned 1 . Other buffers opened during this session will be assigned numbers counting up from there. The %a is two pieces of information. The % meaning that this line (buffer 1 ) corresponds to the buffer in the current window. Vim uses windows as viewports for displaying buffers. In this example, there is only one window displaying a single buffer, so this must be the buffer in the current window. The a has a similar meaning. It means that this is an active buffer that is loaded and visible. In a bit, we will add more buffers and see these indicators change. Lastly, the line 1 part means that the cursor is currently on line 1 of that buffer. The ls command isn't the only way to list the current buffers. We are also free to use files and buffers which are synonymous commands. Another Buffer We can make things more interesting by creating a new file. We can do that with the edit command. : edit data . csv Let's execute ls again to see what has happened to our buffer list. :ls\n  1 #h   \"file.md\"                      line 1\n  2 %a   \"data.csv\"                     line 1 The buffer list now contains two items. The first item in the list, buffer 1 , now has different indicators: # and h . The h means that it is now a hidden buffer 1 . Though the buffer is still loaded, it is no longer being displayed. Don't worry though, we can always switch back to it. The # indicator means that buffer 1 is the alternate buffer 2 . For now, we can think of the alternate buffer as the previous buffer, but we will see later that this analogy isn't quite right. The second item in the list, buffer 2 , is for the new data.csv file. It has the % and a indicators which we recognize from the previous example. This makes sense because it is the current, active buffer. More Buffers Let's use the edit command to create a couple more buffers. First, how about a ruby file : e code . rb and then a SQL file : e schema . sql At this point, we might want to take another look at our buffer list. :ls\n  1  h   \"file.md\"                      line 1\n  2  h   \"data.csv\"                     line 1\n  3 #h   \"code.rb\"                      line 1\n  4 %a   \"schema.sql\"                   line 1 Buffers 1 through 3 are all hidden (notice the h indicator) because none of them are currently in view. The buffer for code.rb is the previously accessed buffer, so it gets the alternate file indicator ( # ). Buffer 4 , as expected, gets % and a because it is the current, active buffer. It seems like we've exhausted the set of indicators that can come up by just editing new buffers. So, what else can we do? Perhaps we can modify one of the buffers. Modifying A Buffer Let's make some changes to the buffer for schema.sql and see what the buffer list has to say. We can enter Insert mode by hitting i and then type something like the following: select 1; Now, hit <esc> to return to Normal mode and then invoke the ls command. :ls\n  1  h   \"file.md\"                      line 1\n  2  h   \"data.csv\"                     line 1\n  3 #h   \"code.rb\"                      line 1\n  4 %a + \"schema.sql\"                   line 1 We've dirtied buffer 4 by making an unsaved change to it which is indicated by a + . As soon as we write the buffer, this + indicator will go away. :w\n:ls\n  1  h   \"file.md\"                      line 1\n  2  h   \"data.csv\"                     line 1\n  3 #h   \"code.rb\"                      line 1\n  4 %a   \"schema.sql\"                   line 0 New Files We've created buffers for a couple named files, but what about if we use the new command. :new\n:ls\n  1  h   \"file.md\"                      line 1\n  2  h   \"data.csv\"                     line 1\n  3  h   \"code.rb\"                      line 1\n  4 #a   \"schema.sql\"                   line 0\n  5 %a   \"[No Name]\"                    line 1 Vim doesn't have a name for the file represented by buffer 5 , so it just uses [No Name] . The other interesting thing here is that Vim opens a split window with the new command. If we look closely at that buffer list, we see that there are two buffers with the a indicator. This is because both 4 and 5 are loaded and visible. All the others remain hidden. We can then write the contents of the buffer to disk by providing a filename. :w callbacks.js\n:ls\n  1  h   \"file.md\"                      line 1\n  2  h   \"data.csv\"                     line 1\n  3  h   \"code.rb\"                      line 1\n  4  a   \"schema.sql\"                   line 0\n  5 %a   \"callbacks.js\"                 line 1 The file for buffer 5 now has a name, callbacks.js . If we are really observant, we'll also notice that the # indicator is no longer present. Apparently we no longer have an alternate file. This is where our understanding of the alternate file as the previous buffer starts to fall apart. It turns out that \"an alternate file name is remembered for each window.\" 3 The new callbacks.js file has no alternate file yet. Back To One window We have a split window situation. The top split is the buffer for schema.sql , the bottom for callbacks.js which has focus. If we are to quit out of the current window which contains the callbacks.js buffer, the other window, which contains the schema.sql buffer, will be able to reclaim the entire Vim window. We can do this with the quit command. :quit\n:ls\n  1  h   \"file.md\"                      line 1\n  2  h   \"data.csv\"                     line 1\n  3 #h   \"code.rb\"                      line 1\n  4 %a   \"schema.sql\"                   line 1\n  5  h   \"callbacks.js\"                 line 0 Listing the buffers reveals that the schema.sql buffer is the only active buffer now. The callbacks.js buffer is now hidden. Something more interesting is revealed though. The alternate file indicator is back, but perhaps not where we might have expected it. The alternate file is now code.rb . Remember above where we said that each window remembers its own alternate file. The window for buffer 4 had code.rb as its alternate file all along. Now that it is the window in focus, code.rb is again the alternate file. In Summary We should now have a pretty solid handle on how to access and read the buffer list. We learned about each piece of information contained in the buffer list. In particular, we explored the most common indicators and saw the contexts in which they occur. Perhaps the most interesting part was the alternate file and the nuances of its definition. In Part 2 of this series we will take a deeper dive into the buffer list. We will see how we can navigate between buffers based on the buffer list. Up to now the concept of an unloaded buffer has been obscured. We will take a look at some types of buffers that Vim keeps unloaded and see how we can unload buffers ourselves. Unmodified buffers can be hidden by default. Modified buffers, however, depend on the hidden option. If hidden is on, then modified buffers can be hidden. If it is off, then Vim will raise an error and not let the buffer be hidden until saving. By default hidden is set to off, but most .vimrc configurations have hidden set to on. This is also what sensible.vim does. I recommend having set hidden somewhere in your Vim configuration. ↩ The alternate buffer indicator is not a meaningless distinction. We can quickly switch to the alternate buffer by hitting CTRL-^ or by typing :e # . This comes in particularly handy when you are jumping back and forth between a file with some code and its unit tests. ↩ See :help alternate-file for more details. ↩ Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-03-02"},
{"website": "Hash-Rocket", "title": "Swap Two Column Values in SQL: Part 2", "author": ["\nJosh Branchaud\n\n"], "link": "https://hashrocket.com/blog/posts/swap-two-column-values-in-sql-part-2", "abstract": "PostgreSQL Swap Two Column Values in SQL: Part 2 by\nJosh Branchaud\n\non\nSeptember 26, 2017 In the first part of this post we looked at a technique for swapping two\ncolumn values for existing records in a single statement. It is a relatively\nsound solution that takes advantage of subqueries and a case statement.\nUnfortunately, it has a couple drawbacks which we will address in this post. This post builds off of Part 1 . If you haven't already, give it a\nread. The first drawback can be demonstrated by building on our existing example,\nthe pages table which has the following data. select * from pages order by ordering ; id | name | ordering ----+--------+---------- 3 | Green | 1 1 | Red | 2 4 | Yellow | 3 2 | Blue | 4 In the interest of maintaining data integrity, it is reasonable to expect\nthat a table like this would have a uniqueness constraint on ordering .\nLet's add that to ensure that no two pages can end up with the same\nordering value. alter table pages add constraint orderings_are_unique unique ( ordering ); ALTER TABLE With that in place, let's see how our update statement from the previous\npost holds up. update pages set ordering = case name when 'Red' then ( select ordering from pages where name = 'Blue' ) when 'Blue' then ( select ordering from pages where name = 'Red' ) end where name = 'Red' or name = 'Blue' ; ERROR : duplicate key value violates unique constraint \"orderings_are_unique\" DETAIL : Key ( ordering ) = ( 4 ) already exists . Though this single query may have been able to rotate those values in place,\nit runs into trouble when confronted with a uniqueness constraint. What to\ndo? One possible solution to this I picked up from my colleague, Jack\nChristensen. It will allow us to swap the values despite the uniqueness\nconstraint and still maintains that we don't need to know the exact ordering values at the time we write the query. update pages set ordering = - ordering where name = 'Red' or name = 'Blue' ; UPDATE 2 select * from pages order by ordering ; id | name | ordering ----+--------+---------- 2 | Blue | - 4 1 | Red | - 2 3 | Green | 1 4 | Yellow | 3 update pages set ordering = - ( case name when 'Red' then ( select ordering from pages where name = 'Blue' ) when 'Blue' then ( select ordering from pages where name = 'Red' ) end ) where name = 'Red' or name = 'Blue' ; UPDATE 2 select * from pages order by ordering ; id | name | ordering ----+--------+---------- 3 | Green | 1 2 | Blue | 2 4 | Yellow | 3 1 | Red | 4 This solution uses two statements. The first negates the values that will be\nswapped. The second makes them positive again as it swaps them using the\nsame technique from the previous approach. The negative values act as\nlossless placeholders to prevent violation of the uniqueness constraint. This is a bit of a trick that takes advantage of two key facts. First, ordering is an integer column, so it can be negated. Second, we are\nassuming that only the range of positive numbers represented by this integer\ncolumn are being utilized. If this is the case, then at any point in time we\ncan use the range of negative numbers as a temporary scratch pad for\nswapping these values. This frames the second drawback. What do we do if the column we are swapping\nhas a unique constraint but is not an integer column? To illustrate this, let's also put a uniqueness constraint on the name column which we will now treat as the target of the swap. alter table pages add constraint names_are_unique unique ( name ); ALTER TABLE In this scenario, we want to swap the name values of the pages that have ordering values of 2 and 4 . This is going to take a number of steps,\nso let's walk through them one by one. First, we create a temporary table that we call the swap_table . The\nintention of this table is to serve as the temporary scratch pad that holds\non to the values that need swapping. In fact, in the subsequent insert\nstatement into swap_table , the values and their keys (the ordering value) are inserted in the swapped order. create temporary table swap_table ( swap_key integer , swap_value varchar ); CREATE TABLE insert into swap_table ( swap_key , swap_value ) select p1 . ordering , p2 . name from pages p1 join pages p2 on p1 . name != p2 . name where p1 . ordering in ( 2 , 4 ) and p2 . ordering in ( 2 , 4 ); INSERT 0 2 table swap_table ; swap_key | swap_value ----------+------------ 2 | Blue 4 | Red To see really clearly how this swapped order is achieved for the swap_table , let's isolate just the select statement.  Because we are\nonly dealing with two records and we know the values are unique, we can join\nthe table against itself based on the name values not being equal. select p1 . ordering , p1 . name as original , p2 . name as swapped from pages p1 join pages p2 on p1 . name != p2 . name where p1 . ordering in ( 2 , 4 ) andp2 . ordering in ( 2 , 4 ); ordering | original | swapped ----------+----------+--------- 2 | Red | Blue 4 | Blue | Red This swap_table is necessary in order to deal with the uniqueness of the name column as well as the possibility that it is a not null column. Because we cannot simply negate the original name values as we did in the\ninteger example, we need to find another way to uniquely obfuscate them.\nPostgres' built-in md5() function is a great choice here since we are dealing with string values. We\nupdate the target records in the pages table accordingly. Again, this is\nso that we avoid violating the uniqueness constraint on name in the\nsubsequent update statement. update pages set name = md5 ( name ) where ordering in ( 2 , 4 ); UPDATE 2 select * from pages order by ordering ; id | name | ordering ----+----------------------------------+---------- 3 | Green | 1 1 | ee38e4d5dd68c4e440825018d549cb47 | 2 4 | Yellow | 3 2 | 9594 eec95be70e7b1710f730fdda33d9 | 4 Then we update the target records in pages with the swapped values from\nthe swap_table based on corresponding ordering values. update pages set name = swap_table . swap_value from swap_table where swap_table . swap_key = ordering ; UPDATE 2 select * from pages order by ordering ; id | name | ordering ----+--------+---------- 3 | Green | 1 1 | Blue | 2 4 | Yellow | 3 2 | Red | 4 Lastly, we can drop our temporary table in an effort to be tidy. Because it\nis a temporary table, we can also count on it being cleaned up when the\ncurrent connection is terminated. drop table swap_table ; DROP TABLE The queries needed for this scenario may feel overwhelming, but they\ndemonstrate the power and flexibility of SQL and, in particular, PostgreSQL. In this post we saw a couple tricky ways of swapping column values when\nconstrained by uniqueness constraints in the case of both integers and\nstrings. We took advantage of the range of negative numbers that is often\nignored for an integer column, we created a temporary table, and even used\nthe md5() string function provided by Postgres. Surely none of these\nsolutions are a perfect fit for every situation, but what they demonstrate\nis the capacity that SQL has for solving all kinds of problems that arise. Supporting SQL Create the pages table. create table pages ( id serial primary key , name varchar not null , ordering integer not null ); Insert some records into the pages table. insert into pages ( name , ordering ) values ( 'Red' , 2 ), ( 'Blue' , 4 ), ( 'Green' , 1 ), ( 'Yellow' , 3 ); Cover image by Dmitri Popov on Unsplash.com Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-09-26"},
{"website": "Hash-Rocket", "title": "Integrating React Components with a Phoenix Application", "author": ["\nChad Brading\n\n"], "link": "https://hashrocket.com/blog/posts/integrating-react-components-with-a-phoenix-application", "abstract": "Elixir Phoenix React Integrating React Components with a Phoenix Application by\nChad Brading\n\non\nSeptember 22, 2015 In this blog post we’re going to integrate React components into a Phoenix application. For this example we will implement a simple chat interface with multiple rooms for users to join. To get started, let’s create a new Phoenix application. (Note we will be using Phoenix version 1.0.2. For installation instructions visit https://www.phoenixframework.org/docs/installation .) mix phoenix.new chat Then cd into the directory and run the following commands to include React. bower init\nbower install react --save Before we start adding React code we need to set up the Phoenix application to render our template. Let’s first create the controller to handle requests. // web / controllers / room_controller . ex defmodule Chat . RoomController do use Chat . Web , :controller def index ( conn , _params ) do render conn , \" index.html\" end end And the view to render our template. // web / views / room_view . ex defmodule Chat . RoomView do use Chat . Web , :view end Next let’s add the basic template which our React code can use to render its components into. <!-- web/templates/room/index.html.eex --> <div id= \"dashboard\" ></div> Then update the router to direct root requests to our room controller. // web / router . ex get \" /\" , RoomController , :index At this point we can check that our template is rendering correctly by running mix phoenix.server. But before we go any further, let’s go ahead and add our room channel, which we will use to send messages between clients. // web / channels / room_channel . ex defmodule Chat . RoomChannel do use Phoenix . Channel def join ( \" topic:\" <> _topic_name , _auth_msg , socket ) do { :ok , socket } end def handle_in ( \" message\" , %{ \" body\" => body }, socket ) do broadcast! socket , \" message\" , %{ body: body } { :noreply , socket } end end Notice that we are allowing users to join a channel with any topic, which will let us have any number of chat rooms. Next let’s define this channel in the user socket. // web / channels / user_socket . ex channel \" topic:*\" , Chat . RoomChannel For this example we will add our JavaScript code to the app.js file. First add the following line so we will have access to our socket. // web/static/js/app.js import socket from \"./socket\" Now we are ready to start adding our React code. Let’s begin by adding a Dashboard component which will be the parent of the rest of our components. // web/static/js/app.js\n\nlet Dashboard = React.createClass({\n  render() {\n    return( <div> <RoomList /> <ActiveRoom /> </div> )\n  }\n})\n\nReact.render( <Dashboard /> , document.getElementById(\"dashboard\")) Here you can see that Dashboard has two child components, RoomList, which will hold a list of the possible chat rooms users can join, and ActiveRoom, which will include the name of the active chat room and a list of its current messages. The last line is where we are rendering our React components into the template we previously created. We will also need to pass a list of rooms to RoomList and an active room to ActiveRoom, so let’s update the Dashboard component to take care of this. // web/static/js/app.js\n\nlet rooms = [\"general\", \"mix\", \"ecto\", \"plug\", \"elixir\", \"erlang\"]\n\nlet Dashboard = React.createClass({\n  getInitialState() {\n    return {activeRoom: \"general\"}\n  },\n  render() {\n    return( <div> <RoomList rooms= {rooms}/ > <ActiveRoom room= {this.state.activeRoom}/ > </div> )\n  }\n}) Next let’s create the RoomList component. // web/static/js/app.js\n\nlet RoomList = React.createClass({\n  render() {\n    return ( <div> {this.props.rooms.map(room => {\n          return <span><RoomLink name= {room} /> | </span> })} </div> )\n  }\n}) Here we are looping over the list of rooms and creating subsequent RoomLink components, which will act as triggers to change the active room. The RoomLink components will look like this. // web/static/js/app.js\n\nlet RoomLink = React.createClass({\n  render() {\n    return( <a style= {{cursor: \"pointer\"}} > {this.props.name} </a> )\n  }\n}) You may have noticed that we haven’t added an event handler for when our link is clicked. This will include a callback function that is passed down from the parent Dashboard component. Let’s add the handler to RoomLink and then see how it is passed down from its parent components. // web/static/js/app.js\n\nlet RoomLink = React.createClass({\n  handleClick() {\n    this.props.onClick(this.props.name)\n  },\n  render() {\n    return( <a style= {{ cursor: \"pointer\"}} onClick= {this.handleClick} > {this.props.name} </a> )\n  }\n}) The onClick property will be passed down from the RoomList component. // web/static/js/app.js\n\nlet RoomList = React.createClass({\n  render() {\n    return ( <div> {this.props.rooms.map(room => {\n          return <span><RoomLink onClick= {this.props.onRoomLinkClick} name= {room} /> | </span> })} </div> )\n  }\n}) And the onRoomLinkClick property of RoomList is being passed down from the Dashboard component, where it is also being defined. // web/static/js/app.js\n\nhandleRoomLinkClick(room) {\n  let channel = socket.channel(`topic:${room}`)\n  this.setState({activeRoom: room, channel: channel})\n},\nrender() {\n  return( <div> <RoomList onRoomLinkClick= {this.handleRoomLinkClick} rooms= {rooms}/ > <ActiveRoom room= {this.state.activeRoom} /> </div> )\n} Here you can see that we have passed the name of the link that was clicked up to the Dashboard component, where we are using it to change the active room to the link that was clicked and to change the channel to reflect the active room. We haven’t created a channel yet up to this point, but we will be using a separate channel for each room a user can join. Now is a good time to show how we will initially set up the channel when our Dashboard is first loaded and how we will update it upon joining a new room. Add a channel attribute to our getInitialState function and include the following componentDidMount function, which will execute when the Dashboard component is first loaded. // web/static/js/app.js getInitialState () { return { activeRoom : \"general\" , channel : socket . channel ( \"topic:general\" )} }, componentDidMount () { this . configureChannel ( this . state . channel ) }, Notice we are calling a configureChannel function, which will describe how we join a channel. Let’s define it now. // web/static/js/app.js configureChannel ( channel ) { channel . join () . receive ( \"ok\" , () => { console . log ( `Succesfully joined the ${ this . state . activeRoom } chat room.` ) }) . receive ( \"error\" , () => { console . log ( `Unable to join the ${ this . state . activeRoom } chat room.` ) }) }, We will also need to call this function when a new room is joined, so let’s add the following line to our handleRoomLinkClick function. // web/static/js/app.js handleRoomLinkClick ( room ) { let channel = socket . channel ( `topic: ${ room } ` ) this . setState ({ activeRoom : room , channel : channel }) this . configureChannel ( channel ) }, Next add the following placeholder for our ActiveRoom component. // web/static/js/app.js\n\nlet ActiveRoom = React.createClass({\n  render() {\n    return ( <div> </div> )\n  } Now if you navigate back to the browser and open up the console, you can see that we are able to successfully join different chat rooms. However, we still haven’t implemented our ActiveRoom component, so there is no way to submit or view messages. Let’s do that now. // web/static/js/app.js\n\nlet ActiveRoom = React.createClass({\n  render() {\n    return ( <div> <span> Welcome to the {this.props.room} chat room! </span> <MessageInput /> <MessageList messages= {this.props.messages}/ > </div> )\n  }\n}) Here we have the child components MessageInput, where new messages will be submitted, and MessageList, where the current messages for the active chat room will be displayed. Notice we are passing MessageList a list of messages from an ActiveRoom property, which we haven’t yet defined. This property will be passed to ActiveRoom from the Dashboard component, so let’s add that now. // web/static/js/app.js\n\ngetInitialState() {\n  return {activeRoom: \"general\", messages: [], channel: socket.channel(\"topic:general\")}\n},\nrender() {\n  return( <div> <RoomList onRoomLinkClick= {this.handleRoomLinkClick} rooms= {rooms}/ > <ActiveRoom room= {this.state.activeRoom} messages= {this.state.messages} /> </div> )\n} We are setting the initial state of the messages to an empty array and will update it whenever a new message is submitted. But before switching to how messages will be submitted, let’s finish implementing how messages are displayed. // web/static/js/app.js\n\nlet MessageList = React.createClass({\n  render() {\n    return ( <div> {this.props.messages.map(message => {\n          return <Message data= {message} /> })} </div> )\n  }\n}) Here we are looping over our messages and returning a Message component, which will look like this. // web/static/js/app.js\n\nlet Message = React.createClass({\n  render() {\n    return ( <div> <div> {this.props.data.text} </div> <div> {this.props.data.date} </div> </div> )\n  }\n}) Also, when we switch rooms we want to reset our message list, so update the handleRoomClick function in the Dashboard component. // web/static/js/app.js handleRoomLinkClick ( room ) { let channel = socket . channel ( `topic: ${ room } ` ) this . setState ({ activeRoom : room , messages : [], channel : channel }) this . configureChannel ( channel ) }, To allow users to actually submit messages we need to implement our MessageInput component. // web/static/js/app.js\n\nlet MessageInput = React.createClass({\n  handleSubmit(e) {\n    e.preventDefault()\n    let text = React.findDOMNode(this.refs.text).value.trim()\n    let date = (new Date()).toLocaleTimeString()\n    React.findDOMNode(this.refs.text).value = \"\"\n    this.props.onMessageSubmit({text: text, date: date})\n  },\n  render() {\n    return ( <form onSubmit= {this.handleSubmit} > <input type= \"text\" ref= \"text\" /> </form> )\n  }\n}) Here you can see that we have added an input text box for users to enter their messages, as well as a handleSubmit event handler that is resetting the text box and calling an onMessageSubmit function. We haven’t yet defined the onMessageSubmit function. It is a callback from the Dashboard component that is passed down through the ActiveRoom component. Let’s first add it to ActiveRoom. // web/static/js/app.js\n\nlet ActiveRoom = React.createClass({\n  render() {\n    return ( <div> <span> Welcome to the {this.props.room} chat room! </span> <MessageInput onMessageSubmit= {this.props.onMessageSubmit}/ > <MessageList messages= {this.props.messages}/ > </div> )\n  }\n}) And then let’s define it in Dashboard and pass it to ActiveRoom. // web/static/js/app.js\n\nhandleMessageSubmit(message) {\n  this.state.channel.push(\"message\", {body: message})\n},\nrender() {\n  return( <div> <RoomList onRoomLinkClick= {this.handleRoomLinkClick} rooms= {rooms}/ > <ActiveRoom room= {this.state.activeRoom} messages= {this.state.messages} onMessageSubmit= {this.handleMessageSubmit}/ > </div> )\n} Now that we are pushing messages through our channel, we need to add a handler for when our channel receives a message. Add the following to our configureChannel function in the Dashboard component. // web/static/js/ap configureChannel ( channel ) { channel . join () . receive ( \"ok\" , () => { console . log ( `Succesfully joined the ${ this . state . activeRoom } chat room.` ) }) . receive ( \"error\" , () => { console . log ( `Unable to join the ${ this . state . activeRoom } chat room.` ) }) channel . on ( \"message\" , payload => { this . setState ({ messages : this . state . messages . concat ([ payload . body ])}) }) }, If you navigate back to the browser and open up a few tabs you should see that your messages are being pushed to other clients in the same chat room. To take a look at the sample repository visit https://github.com/chadbrading/phoenix-react-chat . Was this post helpful? Share it with others. Tweet Share Post", "date": "2015-09-22"},
{"website": "Hash-Rocket", "title": "Implementing Video Chat in a Phoenix Application with WebRTC", "author": ["\nChad Brading\n\n"], "link": "https://hashrocket.com/blog/posts/implementing-video-chat-in-a-phoenix-application-with-webrtc", "abstract": "Elixir Phoenix Implementing Video Chat in a Phoenix Application with WebRTC by\nChad Brading\n\non\nSeptember 16, 2015 In this blog post we’re going to cover how to implement video chat in a Phoenix application with WebRTC. By the end of this post we will have enabled two remote clients to connect with each other and engage in a video conversation. We will use Phoenix channels to communicate messages between our two clients so they can establish a remote peer connection. WebRTC allows clients to establish a direct peer to peer connection with each other, but before they can establish this connection they need to communicate certain information about themselves, and this is what our Phoenix channels will facilitate. For a more in-depth explanation of how WebRTC works, visit https://www.html5rocks.com/en/tutorials/webrtc/basics . To begin, let’s create a new app (Note that we will be using Phoenix version 1.0.2. For install instructions visit https://www.phoenixframework.org/docs/installation ). mix phoenix.new video_chat Let’s first create a basic call controller to handle our requests. // web / controllers / call_controller . ex defmodule VideoChat . CallController do use VideoChat . Web , :controller def index ( conn , _params ) do render conn , “ index . html \" end\nend Then we’ll need to create a corresponding call view to render our template. // web / views / call_view . ex defmodule VideoChat . CallView do use VideoChat . Web , :view end For our template we will just include two video elements, one for the local stream and another for the remote stream. We will also need some buttons to invoke our actions. <!--  web/templates/call/index.html.eex --> <div> <video id= “localVideo\" autoplay ></video> <video id= “remoteVideo\" autoplay ></video> <button id= “connect” > Connect </button> <button id= \"call\" > Call </button> <button id= \"hangup\" > Hangup </button> </div> Next we need to update our router to redirect to our call controller. Change the root path from PageController to CallController. // web / router . ex get “ / “ , CallController , :index Now we can run mix phoenix.server and navigate to localhost:4000 to make sure our template is rendering correctly. With that in place we are ready to set up our channel. Create a call channel with a join/3 function to allow clients to join the channel, as well as a handle_in/3 function to handle incoming events. // web / channels / call_channel . ex defmodule VideoChat . CallChannel do use Phoenix . Channel def join ( \" call\" , _auth_msg , socket ) do { :ok , socket } end def handle_in ( \" message\" , %{ \" body\" => body }, socket ) do broadcast! socket , \" message\" , %{ body: body } { :noreply , socket } end end Next we need to define our call channel in our socket handler. Add the following line to web/channels/user_socket.ex. // web / channels / user_socket . ex channel \" call\" , VideoChat . CallChannel Now we are just about ready to implement our JavaScript code to enable our clients to establish a connection. First add the following line to our application template to enable our WebRTC methods to work across different browsers (Chrome, Firefox, and Opera all currently support WebRTC). <!-- web/templates/layout/app.html.eex --> <script src= \"//cdn.temasys.com.sg/adapterjs/0.10.x/adapter.debug.js\" ></script> For this example we will put our JavaScript code in app.js. Let’s first import our socket and then establish a connection with our call channel. // web/static/js/app.js import socket from \"./socket\" let channel = socket . channel ( \"call\" , {}) channel . join () . receive ( \"ok\" , () => { console . log ( \"Successfully joined call channel\" ) }) . receive ( \"error\" , () => { console . log ( \"Unable to join\" ) }) Note that our socket is imported from web/static/js/socket.js. If you take a look at that file you will see that is where our socket is created and connected. You can also comment out or delete the code attempting to join a new channel since we have implemented that on our own. Now we can wire up our buttons. //  web/static/js/app.js let localStream , peerConnection ; let localVideo = document . getElementById ( \"localVideo\" ); let remoteVideo = document . getElementById ( \"remoteVideo\" ); let connectButton = document . getElementById ( \"connect\" ); let callButton = document . getElementById ( \"call\" ); let hangupButton = document . getElementById ( \"hangup\" ); hangupButton . disabled = true ; callButton . disabled = true ; connectButton . onclick = connect ; callButton . onclick = call ; hangupButton . onclick = hangup ; And then begin to define how our clients will establish their connections. // web/static/js/app.js function connect () { console . log ( \"Requesting local stream\" ); navigator . getUserMedia ({ audio : true , video : true }, gotStream , error => { console . log ( \"getUserMedia error: \" , error ); }); } Here we are using the getUserMedia function to capture our local video stream and then call the callback function gotStream. // web/static/js/app.js function gotStream ( stream ) { console . log ( \"Received local stream\" ); localVideo . src = URL . createObjectURL ( stream ); localStream = stream ; setupPeerConnection (); } In gotStream we are setting our local stream and then calling setupPeerConnection. // web/static/js/app.js function setupPeerConnection () { connectButton . disabled = true ; callButton . disabled = false ; hangupButton . disabled = false ; console . log ( \"Waiting for call\" ); let servers = { \"iceServers\" : [{ \"url\" : \"stun:stun.example.org\" }] }; peerConnection = new RTCPeerConnection ( servers ); console . log ( \"Created local peer connection\" ); peerConnection . onicecandidate = gotLocalIceCandidate ; peerConnection . onaddstream = gotRemoteStream ; peerConnection . addStream ( localStream ); console . log ( \"Added localStream to localPeerConnection\" ); } setUpPeerConnection creates a new RTCPeerConnection and then sets callbacks for when certain events occur on the connection, such as an ICE candidate is detected or a stream is added. Then we add our local video stream to the peer connection. Next we will add our call function to send a message to other clients connected on our channel with a local peer connection. // web/static/js/app.js function call () { callButton . disabled = true ; console . log ( \"Starting call\" ); peerConnection . createOffer ( gotLocalDescription , handleError ); } We are passing the createOffer function the following gotLocalDescription callback. // web/static/js/app.js function gotLocalDescription ( description ){ peerConnection . setLocalDescription ( description , () => { channel . push ( \"message\" , { body : JSON . stringify ({ \"sdp\" : peerConnection . localDescription })}); }, handleError ); console . log ( \"Offer from localPeerConnection: \\n \" + description . sdp ); } The createOffer function created a description of the local peer connection and then sent that description to any potential clients. Once a client receives such a description it then calls the following gotRemoteDescription function. // web/static/js/app.js function gotRemoteDescription ( description ){ console . log ( \"Answer from remotePeerConnection: \\n \" + description . sdp ); peerConnection . setRemoteDescription ( new RTCSessionDescription ( description . sdp )); peerConnection . createAnswer ( gotLocalDescription , handleError ); } Here it sets the remote description on its local peer connection so it can connect to that remote client. It then replies with an answer containing its own description so that remote client can connect back to it as well. The descriptions being sent back and forth between our clients also contain the streams that we added to their peer connections. Once a client receives a remote stream it will call the following function. // web/static/js/app.js function gotRemoteStream ( event ) { remoteVideo . src = URL . createObjectURL ( event . stream ); console . log ( \"Received remote stream\" ); } Here we are just setting the remote stream we receive to the video element in our template. Also, when we create our local description we are also creating a local ICE candidate, which will call the following function. // web/static/js/app.js function gotLocalIceCandidate ( event ) { if ( event . candidate ) { console . log ( \"Local ICE candidate: \\n \" + event . candidate . candidate ); channel . push ( \"message\" , { body : JSON . stringify ({ \"candidate\" : event . candidate })}); } } This sends information about the local ICE candidate over the channel to any potential clients. When a client receives a description about an ICE candidate it will call the following function. // web/static/js/app.js function gotRemoteIceCandidate ( event ) { callButton . disabled = true ; if ( event . candidate ) { peerConnection . addIceCandidate ( new RTCIceCandidate ( event . candidate )); console . log ( \"Remote ICE candidate: \\n \" + event . candidate . candidate ); } } This function will add information about the remote candidate to its local peer connection. When our channel receives a message from the server it needs to know how to process that message. If the message it receives is a description of the remote peer connection we need to call the gotRemoteDescription function, but if it is a description of the remote ICE candidate we need to call the gotRemoteIceCandidate function. We will implement our channel’s event handler to account for these two scenarios. // web/static/js/app.js channel . on ( \"message\" , payload => { let message = JSON . parse ( payload . body ); if ( message . sdp ) { gotRemoteDescription ( message ); } else { gotRemoteIceCandidate ( message ); } }) Let’s also include a hangup function so a user can close the connection and stop the video chat session. // web/static/js/app.js function hangup () { console . log ( \"Ending call\" ); peerConnection . close (); localVideo . src = null ; peerConnection = null ; hangupButton . disabled = true ; connectButton . disabled = false ; callButton . disabled = true ; } And finally add our handleError function. // web/static/js/app.js function handleError ( error ) { console . log ( error . name + \": \" + error . message ); } Now if you navigate back to the browser, open up two tabs, and click the connect button in each tab you should see that each one has created a local peer connection and added its local video stream. If you click the call button from one of the tabs then it will send a description of its local peer connection to the other peer connection, and after they exchange the necessary information they will establish their remote connection. Now you can see their video chat session. You can view the sample repository at https://github.com/chadbrading/phoenix-webrtc . Was this post helpful? Share it with others. Tweet Share Post", "date": "2015-09-16"},
{"website": "Hash-Rocket", "title": "Encryption with gpg, a story (really, a tutorial)", "author": ["\nChris Erin\n\n"], "link": "https://hashrocket.com/blog/posts/encryption-with-gpg-a-story-really-a-tutorial", "abstract": "Encryption with gpg, a story (really, a tutorial) by\nChris Erin\n\non\nApril  4, 2017 GPG is a compatible replacement of PGP.  That, in and of itself, is confusing. That each acronym is sufficiently obtuse on its own is enough to prompt a less\nprivacy minded developer to give up.  GPG stands for Gnu Privacy Guard and PGP\nstands for Pretty Good Privacy.  Pretty Good Privacy came first and prompted\nthe government to prosecute the creator, Phil Zimmerman, for exporting\nmunitions illegally.  He published the code in a book, and then he was\nexporting a book not munitions. The first thing to understand about gpg is that it is not like openssl . openssl takes inputs and delivers outputs and maintains no state inbetween\nrequests. gpg , on the other hand, functions as a key server along with\nencrypting, decrypting and key generation.  It runs in the background, a daemon , and manages your private keys and the public keys of others.  It is a\nprogram intended to encrypt emails and therefore uses option names like recipient to indicate which public key to encrypt a message with. Its a key manager When you understand gpg as key manager before you understand it as an\nencryption tool then it is easier to learn how to use.  Therefore, you should install the program with brew or apt-get and then use it to list all the keys that its managing. > gpg --list-keys\ngpg: directory `/Users/chriserin/.gnupg' created\ngpg: new configuration file `/Users/chriserin/.gnupg/gpg.conf' created\ngpg: WARNING: options in `/Users/chriserin/.gnupg/gpg.conf' are not yet active during this run\ngpg: keyring `/Users/chriserin/.gnupg/pubring.gpg' created\ngpg: /Users/chriserin/.gnupg/trustdb.gpg: trustdb created It has created files and issued a warning.  It needed to have the resources to manage your keys and now it has them.  If we list keys again: > gpg --list-keys Nothing. Great. Lets get started.  Here's a public key that you can manage. -----BEGIN PGP PUBLIC KEY BLOCK-----\nVersion: GnuPG v2\n\nmQENBFjceoMBCADDgOGwaQAVv53E+vT1RhYtUBdU7igdKb+K1cBb/0y7SV9A7zBi\n/N5z2GhsL6cU7vXoX2QoMpDhQ2MYP48nWCmVeff9izinqrjXA05ViSMnPhpEGuq0\njoiAy3QE1xzdiYdvmOcr4PR2rDuz7kIybJi41a+4TPA2fqkGtOsaXfwm6qH5KwhE\noEu1QuZi8X7aiHA0A/tN2sKSos7JSf7G/Ps1XQlkCgF8hQ19jSy3uaZ3d69Rplqf\nUMMukPzlQYQCXltuAyqac86G/jVHJPMmL4ttAagC9vQMnHdQyOhzXhjERGSOCt6w\nMW+sv1JGUROB+rNIYBktF7sviQBwwDyxODX9ABEBAAG0OkNocmlzIEVyaW4gKEJl\nIEtpbmQgVG8gT3RoZXJzKSA8Y2hyaXMuZXJpbkBoYXNocm9ja2V0LmNvbT6JATkE\nEwEIACMFAljceoMCGwMHCwkIBwMCAQYVCAIJCgsEFgIDAQIeAQIXgAAKCRBaN+T0\nfHMfFK8cCACtkhMQ0AumUL9M3eDR8P5DPhGOH14SuobeL5fl/iLTaho2P9ie+R+f\nwFG6m1s0+HxNGMqHTqDdkRYejfwlovPh96aDWz7rwzlECw5zDoLKvxnbk7Lkcy8I\nSl+9JwzAEra2wn+V2iTD9kBNBrzB6IqLny6Q/+i6xaLamANH3ZYgwVQbxt05nUPO\nanGiex5Gu0i5O9FfuHwUjiFdkdtpP5pHX1Lbq0rW8c4rt/6uTQ4DAISIKgphZDw5\nBcg1QytVQ4jdB7Wjyf6uTtDc01pDj79v0VA2O4r/cA6Qk5UwdCmBbKYFARFEEUQT\nCdVKPP/TfOdmPbLjhbn4A3d5V8AsCi1TuQENBFjceoMBCACy7x3s5kprHmNiKpga\n8YP4BhZKG29UTd5TSiPi2M66pXTEz7HHFcW/sZH0i9o5UGS2ueh7m5tRQk8+REVp\nCn7MCOOaaQuS3ISRbZxJIloojHwT7W1SV25KH2Xf2Hs/R//Evu9ajdX5ohfVsZ55\nDFyptEi7BX1dz445+htCMUwAFwjkWnUSG3D1ueFOG/bhZbytfthBiInqHgVh6gYQ\ne18ond4fcsFqzwj2FpfoXa7DYl/RiywfYRkZFjaddJ8+cqdU3Vrt57lRoLd206US\ns5ql77yam3pzWWif+nmiSnPJujj5PzLZWlcUSz3cQ8rAs6ZVFFoxULbBu97ZQ3T6\n/lOXABEBAAGJAR8EGAEIAAkFAljceoMCGwwACgkQWjfk9HxzHxRU/Af9HkN3dqAx\nKsIyXQRnPq33JHTw82urcpRlytAozDhym5PJzUeQsVZ8EZYrjHMa+YgjJ6OR2d4R\naH7cAJUHgig7DZog7ilPJDv/JOtMlHkHBqY7O8SNicuNVr6SNRbO5FRhaXTi4pIE\nWMYl/Jni+0DL0GyIhnzIpxzJGYtY88EJ0obpqWENj829gqFUaHjPs7k0L/1uPTWY\nAT8jdSukbKLc4xtFTa9R/8G8QLW+DtQiDWk+cuVpnF4YiCOIKWCp/rVtdv55EytH\ng+4Xod7FGO4t8WQTPEXb08DLae6ikLMp7gMXDs0kXnk3P1w646nwbpBUi1z0AkZv\nSUlCJaGiY+Eclg==\n=qZVn\n-----END PGP PUBLIC KEY BLOCK----- You can copy this public key to a file named chriserin.pub and import it: > gpg --import chriserin.pub Now when you run list-keys you can see a key is listed: > gpg --list-keys\npub   2048R/7C731F14 2017-03-30\nuid       [ unknown] Chris Erin (Be Kind To Others) <chris.erin@hashrocket.com>\nsub   2048R/1CC31A83 2017-03-30 Encrypting messages Now we have a key to encrypt with.  Please encrypt a message for me that only I can read. > echo \"a really interesting message\" | gpg --armor --encrypt --recipient chris.erin@hashrocket.com Now, if you send me the output of that message, only I will be able to read it. There is an interesting option there, --armor .  The man pages describe --armor as: Create ASCII armored output Its something you need if you plan on transmiting this message in ascii formats like email. Decrypting messages When I get your message I'll be curious and want to decrypt it.  Decrypting is a little bit more envolved then encrypting but still relatively easy. First, I need to make sure that my private key is managed. > gpg --list-secret-keys Nothing!  I'll need to import it. > gpg --import chriserin.priv\n> gpg --list-secret-keys\nsec   2048R/7C731F14 2017-03-30\nuid                  Chris Erin (Be Kind To Others) <chris.erin@hashrocket.com>\nssb   2048R/1CC31A83 2017-03-30 Now lets decrypt the message.  What's unique about the decryption process on my\nend is that I need to enter a passphrase.  I will be prompted to enter this passphrase when the below command is run. > gpg --output interesting.txt --no-tty message.gpg I should now have a file that has the content a really interesting message Creating a key pair Necessary if you expect to receive encrypted messages is a key pair.  A public key that you can distribute on the internet as freely as you see fit and a private key which should be guarded and protected. gpg provides a command option for this: > gpg --gen-key Which asks you for a set of information including your name, email, an optional comment, and a passphrase.  As we've seen while encrypting a message, the email is the unique identifier by which the user specifies a recipient and through which the gpg program identifies which public key to use for encryption. One tricky thing about this key generation on linux is that when generating the\nkey itself the program complained about not having enough entropy and hung\nuntil the entropy requirement was met.  When googling I discovered that this\nwas a common complaint and most often solved by installing the rng-tools with\napt-get as described here . If you are able to create a key successfully it should be in your list of keys, both the secret list and the public list: > gpg --list-keys\n> gpg --list-secrety-keys You'll want to have a public key to distribute which you can obtain with: > gpg --export -a \"Chris Erin\" > chriserin.pub And potentially you'll want to move the private key to a different location.\nIt can be exported with: > gpg --export-secret-keys -a \"Chris Erin\" > chriserin.priv Finally, knowing that you can manage keys with gpg you are probably assuming that they are deleteable.  Delete them with: > gpg --delete-secret-keys chris.erin@hashrocket.com\n> gpg --delete-keys chris.erin@hashrocket.com Now you're back where you started.  Happy encrypting! Was this post helpful? Share it with others. Tweet Share Post", "date": "2017-04-04"},
{"website": "Hash-Rocket", "title": "Avoiding Code Catastrophes", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/avoiding-code-catastrophes", "abstract": "Process Avoiding Code Catastrophes by\nJake Worth\n\non\nNovember 27, 2018 \"Am I going to make a huge mistake in my first dev job (e.g., delete the production database) and get fired?\" I've been asked this question a few times. It's a concern that new programmers have: that they'll end up in a situation where they make the wrong call in the heat of the moment, harming their company and ending their fledgling career. Cutting the red wire fixes the production bug; cutting the green wire sets our servers on fire, or vice versa. Which one will you cut, junior developer? I think this is a valid fear when you're new, and it's something that once crossed my mind, as a self-taught beginner. Is it possible for me to make such a gigantic mistake in my first programming job? I think the answer is no. When you start working as a developer you will have differing levels of power based on the team. At a startup, your level of power may be high. On a large team it will be lower. On any sized team, there should be processes in place that prevent mistakes. Not just mistakes made by junior people, but by people on all levels. When anybody on the team has the power to make a giant mistake, that's a process failure. As your career progresses, you'll begin to perceive invisible boundaries between safe and dangerous technical choices. Running an SQL query in a console on the production database is dangerous. You might make a mistake and delete a record, or all the records. Writing a database migration in development is safe. That's the type of work we all want to be doing. New programmers should be put in positions where failing to see those boundaries isn't detrimental to themselves or the team. Effective leadership does that. Here's a practical example: the feared 'bug in production'. Imagine a bug that is affecting user data. A nullable column is leading to bad data that breaks the user experience. What should we do? A tempting solution might be to connect to the database console and fix the records. The problem goes away in seconds, so it's the fastest choice. Doesn't that also make it the best choice? The answer is no. Connecting to the production database console is risky; you're just as likely to do more harm as good, regardless of your level of experience. More importantly, you won't learn anything. You won't learn what caused the bug, and you won't be able to prevent it from happening again. And that information is much more valuable than any quick fix. We want to put ourselves in a position where we can learn, and fix the problem instead of the symptoms. Here's a better way to approach the issue. We have a bug in production. Step one: don't panic! Unless your software is powering a shuttle to Mars, the significance of the bug is probably lower than you think. Is it a bug that affects customers, or just internal users? If it does affect customers, how many? Is the bug in a feature that is used by many people, or just a handful? What does our error monitoring software say? As you start to ask questions like these you realize that few bugs are a five-alarm catastrophe. No matter how bad the situation seems, you almost always have time. Time, to fix the problem safely, and to ensure it doesn't happen again. Here's a list of steps I might take to fix this contrived emergency: Get a copy of the production database, sanitize with the sanitization script you already have Load the production data into a development database Try to reproduce the bug! Use every tool you have to recreate the issue. The bug was created with the data and code in front of you; think! Fix the bug. If this requires you to change data or the database, could you write a script? Test it in development. Does it work, and also clean up after itself if something goes wrong? Write a test that describes the correct behavior. Verify your bugfix by running it before and after your changes Test out your script in multiple environments. One of these environments should be staging where the data is the same or almost the same as production Fix in production Does this take longer than the shortcut I proposed? Absolutely. But once you've done it a few times, that delay becomes insignificant. By taking these few extra steps, you have fixed the issue in a low stakes environment, and you know why it happened so your solution prevents it from reoccurring. You might think that solutions like these are what they teach you in school, but not what people do the real world. If you find yourself at a company that thinks this way, start looking for a new job. Get on a team that takes the time to do it right. There, you will learn these techniques and more as your career progresses. By doing your work in a safe and predictable environment, even when the stakes are high, you'll keep your users happy and your career secure. Photo by darkroomsg on Unsplash Was this post helpful? Share it with others. Tweet Share Post", "date": "2018-11-27"},
{"website": "Hash-Rocket", "title": "Introducing Metabot", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/introducing-metabot", "abstract": "Hashrocket Projects Introducing Metabot by\nJake Worth\n\non\nJanuary  4, 2016 We made a robot. Our foray into chatbots began with Steambot. Rocketeer Josh Davey wrote a Markov Chain text generator in Clojure, then trained it on some Kafka, Russell, Aristotle, Nietzsche, and a special ingredient: Steam, Its Generation and Use by Babcock & Wilcox Company (35th edition, copyright 1919). He named his program Steambot and configured it as an outgoing webhook in Slack . This was Steambot's first message, triggered with the keyword 'steambot': 'The Mean man will be vexed, but only moderately and as their age changes so likewise do their pleasures.' -Steambot (November 6, 2014) We pinged Steambot for a few days, asking it Turing Test-style questions like 'Steambot, what is love?'. Steambot responded: 'It is the test of a K. W. steam engine turbine unit, Mr. H. G. Stott and Mr. R. G. S.' -Steambot (December 18, 2014) Pretty soon there was so much traffic that we needed a dedicated Slack channel just for these inane conversations. The #steamroom was born. Soon there were other bots, written by Rocketeers across the company. We made Madbot, a robot that returns slightly wrong proverbs such as: 'When the going gets loud, the shaky get going.' We made Artbot, a robot who draws you an abstract picture. We made Fightbot, a robot who creates hypothetical battle matchups like this: 'A hipster, can turn to steel, has a hostage ​ VS ​ an evil doll, afraid of nachos, hands glued to hips' -Fightbot (December 18, 2015) Halobot, Knightbot, Bardbot, and more. Within a few months we had created over a dozen outgoing webhooks, pinging Clojure, Go, JavaScript, and Ruby web apps. Like any good sitcom, a spin-off was inevitable. Wouldn't it be cool, we thought, to build a web app that made it easy and fun to create chat bots? One meta-bot to rule them all. We are proud to introduce Metabot , a new side project from Hashrocket. How It Works Metabot makes it easy to create your own bot. Simply click 'Make a bot', choose between the two bot types we've built (Markov Bot or Substitution Bot), feed your bots any text files you choose, and we'll provide you with an endpoint you can easily integrate into Slack. Soon your chat log will be alive with a fun, randomly coherent robot that you created. There's no limit to the personality you can give your bot, or the amount of bots you can create. Technology Technically, Metabot was a chance for us to showcase our process and hone competency in new technologies. The backend is written in Go with a PostgreSQL database. We saw this as an opportunity to build a small code base using solid design patterns such as the Respository Pattern and a custom responsive caching system. The Markov and Substitution algorithms are our own and were refined over months of dogfooding. The frontend features a Middleman site generator and was written in React.js. It includes some cool CSS tricks that add personality to the robot. On the devops side, Metabot features a rapid deployment with Nginx using Mina and a custom database backup. Look for more detail about the technology in forthcoming blog posts. In the future we plan to add more bot types and integrations, so everybody can have a bot. Conclusion To quote Steambot: 'As this is growing wearisome, I would now be generally adopted.' -Steambot (October 5, 2015) Make your own bot with Metabot , and let us know what you think. Was this post helpful? Share it with others. Tweet Share Post", "date": "2016-01-04"},
{"website": "Hash-Rocket", "title": "Ruby5 Roundup - Episode 479", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/ruby5-roundup-episode-479", "abstract": "Ruby5 Roundup - Episode 479 by\nPaul Elliott\n\non\nJuly 11, 2014 You missed us, didn't you. Well Lynn and I are back again to bring you the hottest news in the Ruby and Rails communities. Here is a quick roundup of what's new in this episode of the Ruby5 podcast. http://ruby5.envylabs.com/episodes/516-episode-479-july-11th-2014 Time Travel Movies Explained in Git http://hashrocket.com/blog/posts/edge-of-tomorrow-explained-in-git Fellow Rocketeer Vic Ramon wrote a very entertaining and informative blog series about some of git's more advanced history manipulation features. He tells the story of some of this summer's hottest blockbusters in the context of a git log! Introduction to Programming with Ruby http://www.gotealeaf.com/books/ruby Are you or someone you know trying to learn to program? Well a new online book published by Tealeaf Academy beautifully explains the basics of programming using examples in Ruby. Did I mention it is completely free? Adequate Exposure https://github.com/rwz/adequate_exposure Tired of \"@\" variables all over your views and controller actions? A new and lightweight gem from fellow Rocketeer Pavel Pravosud gives you a simple interface for defining resources in your controllers. Zero to Smoke Test with Sinatra http://devblog.avdi.org/2014/07/08/zero-to-smoke-test-with-sinatra Implementing a test suite on an app that doesn't already have one is no easy task. This new blog post from Avdi Grimm will walk you through the process on a Sinatra app he wrote! App Server Arena https://blog.engineyard.com/2014/ruby-app-server-arena-pt1 Let's get ready to rumblllleeeeee! A new blog series from Engine Yard pits all our favorite application servers against one another in an arena battle. Lots of great information and benchmarks in the posts. Check it out! Informant https://addons.heroku.com/informant A new Heroku add-on called Informant will track server-side validation errors and give you metrics on which forms cause your users the most hassle. This is very valuable information for any design team supporting a production environment. So that's it for this episode of Ruby5. If you haven't already, subscribe to the podcast and keep yourself up to date. Thanks for listening! Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-07-11"},
{"website": "Hash-Rocket", "title": "Lets Write Some Bad Ruby", "author": ["\nJason Cummings\n\n"], "link": "https://hashrocket.com/blog/posts/lets-write-some-bad-ruby", "abstract": "Ruby Lets Write Some Bad Ruby by\nJason Cummings\n\non\nOctober 26, 2015 One of my favorite things to do with Ruby is to write a quick throw away script to automate some mechanical changes to my code that can't easily be done with a text editor.  Unlike normal code, the code can be downright bad and it's perfectly fine, because once the script runs, we can just delete it. Sometimes I Don't Want To Type All That All we care about is the computer doing something for us in a few seconds that would take us much, much longer manually. Here's an example in Rails: Rails . application . routes . draw do namespace :foo do resources :bars resources :users # plus 20 more resources end end Let's say that for whatever reason, namespacing our resources under 'foo' seemed like the correct decision when we were building our app, but now we've decided not to. We're going to move them out. Rails . application . routes . draw do resources :bars resources :users # plus 20 more resources end We know this is going to break our code in a few ways: The route helpers. Our bars/index route was foo_bars_path , but now it's simply bars_path . The same concept applies to every other route, and these are bound to be all over our code. Controller namespacing. Our BarsController looks like, class Foo::BarsController < ApplicationController , so we need to drop the Foo:: namespace off of our controllers. We need to move everything out of app/**/foo/** , to app/**/** When dealing with a couple of resources, this isn't really a big deal. But when you have 10, 20, or more resources, doing all that by hand is slow. Sometimes we can accomplish these changes with a text editor, but since we'll have a bunch of path helpers that will all be different (and we'll also be moving files), there will still be a good chunk of manual editing. Luckily, we can write a throwaway script to do it for us! The Throwaway Script # cd to root of app $ touch tmp/move_stuff.rb && cd tmp We know that our changes are primarily breaking controllers and views. Let's start with views. We want to recursively loop through each view namespaced under foo . For this, we'll use Dir.glob . We'll glob our directories, check to make sure the current file in the iteration isn't a directory, and then find & change our path helpers. #tmp/move_stuff.rb Dir . glob '../app/views/foo/**/*' do | file | if ! Dir . exist? file text = File . read file text . gsub! /(\\w*)foo_(\\w+)path/ , '\\1\\2path' #puts text.gsub /(\\w*)foo_(\\w+)path/, '\\1\\2path' File . open ( file , 'w' ) { | f | f . write text } end end Here we read the file, find anything that matches the format of a path helper that had the foo namespace, gsub it with the first and second captures, and append 'path' to it. Then, we write the file back. Notice, I included the code I used to test the regex when I ran the script, which logged it to the console instead of actually changing anything. I don't want to make any changes until I'm sure I have it right. This applies to any step here. We also we want to move the file. The current path is the current item in the iteration, file . We can simply gsub out 'foo/' to get the path we want. file # '../app/views/foo/bars/index.html.haml' new_path = file . gsub \"foo/\" , \"\" # '../app/views/bars/index.html.haml' We can get the directory in the new path by matching anything that has word characters followed by a trailing forward slash, like this: dir_to_move_to = new_path . gsub /.+\\// , \"\" # '../app/views/bars/' We can't move to a directory that isn't there, so make it if it doesn't exist: Dir . mkdir dir_to_move_to unless Dir . exists? dir_to_move_to And finally, backtick a console mv command to move the file to the new_path: `mv #{ file } #{ new_path } ` # `mv ../app/views/foo/bars/index.html.haml ../app/views/bars/index.html.haml` The final product: Dir . glob '../app/views/foo/**/*' do | file | if ! Dir . exist? file text = File . read file text . gsub! /(\\w*)foo_(\\w+)path/ , '\\1\\2path' File . open ( file , 'w' ) { | f | f . write text } new_path = file . gsub \"foo/\" , \"\" dir_to_move_to = new_path . gsub /.+\\// , \"\" Dir . mkdir dir_to_move_to unless Dir . exists? dir_to_move_to `mv #{ file } #{ new_path } ` end end Then go to the /tmp directory and run the file (it needs to be in /tmp in order to use the relative pathnames we wrote, which is probably bad, but I don't care. I'm deleting this in 12 seconds.) $ ruby move_stuff.rb And that takes care of the views. In Conclusion: Yuck WOW that is some ugly code!  The future reader of this code would have to stare at it for a minute or two before getting a handle on what's going on. But here, ugly code is the right tool for the job, because it can be written quickly, and once this script runs, we'll never need to run this again, so we can just delete it. I think this is much easier than doing it by hand (assuming you have a lot of files), plus, the more you write these types of scripts, the quicker you can write them in the future. For the controllers, you could literally copy and paste (remember, we're deleting this!) the same chunk of code, replace views with controllers , and add a gsub for removing the \"Foo::\" namespacing. When you're done, delete this file and never speak of it again! Was this post helpful? Share it with others. Tweet Share Post", "date": "2015-10-26"},
{"website": "Hash-Rocket", "title": "Test Driving a Stubbed API in Ember with Ember-CLI-Mirage", "author": ["\nJason Cummings\n\n"], "link": "https://hashrocket.com/blog/posts/test-driving-a-stubbed-api-in-ember-with-ember-cli-mirage", "abstract": "Javascript Ember Test Driving a Stubbed API in Ember with Ember-CLI-Mirage by\nJason Cummings\n\non\nOctober 19, 2015 When developing a client side javascript app, you won’t always have an API available before you start. Even when you do, you probably don’t want to have your tests reliant on the the API end-points. Luckily, there is a great solution to stubbing out an API while building your Ember app; Ember CLI Mirage . Mirage works great when Ember Data is expecting a REST API, but there's some manual conversion that must be done if you want to consume JSON API 1 , which I ran into recently on a project. In this tutorial we will leverage QUnit and Mirage's factories and API DSL to craft explicit acceptance tests as we build our application. I’m going to assume you have some basic knowledge of Ember for this. Setup $ ember new mirage-tutorial $ cd mirage-tutorial Vim users who use Vim Projectionist can curl a set of projections from my Github repo. $ curl -G https://raw.githubusercontent.com/jsncmgs1/ember-vim-projections/\nmaster/.projections.json -o .projections.json We will use Ember/Ember Data 2.1.0 for this app, so let's update. In your bower.json file: change:\n\"ember\": \"{your version}\" to \"ember\": \"2.1.0\" and \"ember-data\": \"{your version}\" to \"ember-data\": \"2.1.0\" Then nombom with: $ npm cache clear && bower cache clean && rm -rf node_modules bower_components && npm install && bower install Ember and Ember Data should be updated.  To check, start your ember server and go to localhost:/4200 and you’ll see the “Welcome to Ember” page. Pull up the Ember\ninspector, and click the left sub-nav “Info” button. Ember and Ember-data should both be at 2.1.0. We will use the JSONAPI adapter, generate your adapter: $ ember g adapter application In the adapter file, change RESTAdapter to JSONAPIAdapter. Now install mirage, then restart your server. $ ember install ember-cli-mirage Mirage will create a mirage directory under app/. It contains a config.js file, a factories directory, and a scenarios directory. Config file : Mirage wraps Pretender, which intercepts requests that would normally hit your API, and allows you to specify the response that should be sent back.  This file is where you specify your API end-points. Mirage gives you shorthand syntax for simple routes, but you can create manual routes when shorthand won’t work. Mirage docs have a\nshort and clear description of how to handle your routes. Scenarios : Mirage creates a default.js scenario for you.  Inside the scenario you declare all the data you want to seed your development environment with.  This data will not be in the test environment. Factories : Your mirage scenario will use the factories you define to generate your data, and you should use them in your tests as well. We will create a simple app that will list our cars and let us create new ones.  Our cars also contain parts, which can also be created. While the API team builds their their end, we’ll get started on our end. Listing our cars Let's create a cars acceptance test. $ ember g acceptance-test cars Ember generates a test for us at tests/acceptance/cars-test.js , with a generated test which checks to make sure our route functions. Let's change it to test a link to the cars index on the application template.  When writing QUnit, you'll simulate all your user navigations ('click', 'visit', etc), which run asynchronously. Assertions are called in the andThen() callback, which will run after all the async operations are complete. 2 //app/tests/acceptance/cars.js test ( 'visiting /cars' , function ( assert ) { visit ( '/' ); click ( '#all-cars' ); andThen (() => { assert . equal ( currentURL (), '/cars' ); }); }); Our tests run at localhost:4200/tests. When you go to that page, in the Module drop down in the upper right and corner, choose 'Acceptance | cars'. We will get an error because we don’t have the #all-cars link. Lets make our test pass. First, we need to create the link. <!-- app/templates/application.hbs --> <h2 id= \"title\" > Welcome to Ember </h2> {{link-to 'Cars' 'cars.index'}}\n\n{{outlet}} Now QUnit tells us there's no cars.index route. $ ember g route cars Ember will add the route for you in the router.js file.  It adds the empty object, but we also need to pass an empty function so that an cars/index route is generated. Unfortunately, this.route('cars', {}) would not create it. //router.js Router . map ( function () { this . route ( 'cars' , {}, function (){}); }); Now check your test page, it passes. Lets test that when we go to the cars page, we will actually see some cars. At the bottom of your cars acceptance test: //tests/acceptance/cars.js test ( 'I see all cars on the index page' , ( assert ) => { server . create ( 'car' ); visit ( '/cars' ); andThen (() => { const cars = find ( 'li.car' ); assert . equal ( cars . length , 1 ); }); }); server.create('car') is telling Mirage to find a factory named 'car', create 1 of those cars, and put them in the Mirage database. When you run the test, it will die due to a Mirage error.  I recommend running\nyour tests with the Chrome debugger open so you can see the errors. Mirage will log an error saying it tried to find a ‘car’ factory, and it was not defined.  Lets make one at app/mirage/factories/car.js . // /app/mirage/factories/car.js import Mirage from 'ember-cli-mirage' ; export default Mirage . Factory . extend ({ name ( i ) { return `Car ${ i + 1 } ` ;} }); This will create a car with a name attribute. This (i) syntax is used for Mirage sequences, the first name will be \"Car 1\", then \"Car 2\", etc. If we check our tests again, it will fail, finding 0 cars when expecting 1. To get the cars on the page, our car/index route will need to load the car model. Let’s create our car model. The Ember CLI generators are fantastic, but they will generate some tests that are not in the scope of this tutorial (unit tests).  You can remove them, or ignore them for now.  However, I wouldn't recommend leaving unused tests around. $ ember g model car // /app/models/car.js import DS from 'ember-data' ; export default DS . Model . extend ({ name : DS . attr ( 'string' ) }); And our route/template: $ ember g route cars/index // /app/routes/cars/index.js import Ember from 'ember' ; export default Ember . Route . extend ({ model (){ return this . store . findAll ( 'car' ); } }); <!--app/templates/cars/index.hbs--> <ul class= 'cars' > {{#each model as |car|}} <li class= 'car' > {{car.name}} </li> {{/each}} </ul> When we hit the model hook in our route, Ember Data sends out a GET request to /cars . If you let the test run, the test will seem frozen without the chrome debugger open.  Mirage will log an error to the console saying there's no end point for GET /cars . Let’s create a route for Mirage so it can intercept this request.  For the tutorial we will use the longer syntax, because Mirage doesn’t handle JSON API in the shorthand syntax - yet. When the json-api-serializer branch of Mirage gets merged (which should be soon), Mirage will be able to take care of a lot of the payload transforming itself. JSON API expects a response with a top level key named 'data', which contains an array of the resources returned.  Each resource should have a specified type,\nthe id of the resource, and the resource attributes. When Mirage responds to a request, it will log the response object in the console for inspection. The object should look like this: data : { [ { attributes : { id : 1 , name : 'Car 1' }, id : 1 , type : 'cars' }, { attributes : { id : 2 , name : 'Car 2' }, id : 2 , type : 'cars' }, //.... ] } There are other keys as well, such as errors, and relationships. We will expand on relations further in the tutorial. // /app/mirage/config.js export default function () { this . get ( '/cars' , ( db , request ) => { let data = {}; data = db . cars . map (( attrs ) => { let rec = { type : 'cars' , id : attrs . id , attributes : attrs }; return rec ; }); return { data }; }); }; When we run our tests again, they pass.  If you’d like to see it work in development, generate some cars in scenarios/default.js , and go to localhost:4200/cars . // /app/mirage/scenarios/default.js export default function ( server ) { // Seed your development database using your factories. This data will not be loaded in your tests. server . createList ( 'car' , 10 ); } Whats going on here? When we visit the cars route, ember sends us to the cars/index route. The route fires the model hook, where ember data sends out a GET request for all of the cars.  The mirage route in mirage/config.js intercepts the request, gets the cars that we generated in the test, adds them to a JSON API formatted object, and sends it back as the response.  No api needed! Now that we have a working acceptance test, lets create a car component for our cars to live in. $ ember g component a-car Ember created a component integration test, which we'll use. It's easy to setup Mirage for an integration tests.  Under tests/helpers/ , create a file called mirage-integration.js //tests/helpers/mirage-integration.js import mirageInitializer from '../../initializers/ember-cli-mirage' ; export default function setupMirage ( container ) { mirageInitializer . initialize ( container ); } and in your component test, import the setupMirage function, you will invoke in the moduleForComponent setup hook, passing in this.container. //app/tests/integration/components/a-car-test.js import { moduleForComponent , test } from 'ember-qunit' ; import setupMirage from '../../helpers/mirage-integration' ; import hbs from 'htmlbars-inline-precompile' ; moduleForComponent ( 'a-car' , 'Integration | Component | a car' , { integration : true , setup () { setupMirage ( this . container ); } }); test ( 'it renders' , function ( assert ) { const car = server . create ( 'car' ); this . set ( 'car' , car ); this . render ( hbs `{{a-car car=car}}` ); assert . equal ( this . $ (). text (). trim (), 'Car 1' ); }); In this test, we create a car, and a component ( this ) and set it on the component. Then we can actually render the template, and assert what the components text should be. Of course we haven't done anything with our component\nyet, so the test fails. In our cars/index template, we're rendering our component inside of an li, with a class of 'car'.  Add those attributes to the component. import Ember from 'ember' ; export default Ember . Component . extend ({ tagName : 'li' , classNames : [ 'car' ] }); Move the {{car.name}} expression into the component template, and render the component in the each loop, passing the model into the component. <!-- templates/components/a-car.hbs --> {{car.name}} <!-- templates/cars/index.hbs --> Cars/Index <ul class= 'cars' > {{#each model as |car|}}\n    {{a-car car=car}}\n  {{/each}} </ul> Run the tests, they should pass. Adding New Cars Now that our cars index is tested and working, we need to be able to add more cars to our collection. Let's make a test. //tests/acceptance/cars-test.js test ( 'I can add a new car' , function ( assert ){ server . createList ( 'car' , 10 ); visit ( '/cars' ); click ( '#add-car' ); fillIn ( 'input[name=\"car-name\"]' , 'My new car' ); click ( 'button' ); andThen (() => { const newCar = find ( 'li.car:contains(\"My new car\")' ); assert . equal ( newCar . text (). trim (), \"My new car\" ); }); }); Our test fails because there's no link with an id of add-car. This link should take us to the cars.new route. In your cars/index template at the bottom of the file, add: <!-- app/templates/cars/index.hbs --> <!-- ... --> {{#link-to 'cars.new' id='add-car'}}\n  Add new car\n{{/link-to}} Now our test fails because we don't have the specified input field. We'll need the cars/new template, we also know that we will need that route. Generating the route will create both for us, as well as adding the route to our router. ember g route cars/new The router should now look like: //router.js import Ember from 'ember' ; import config from './config/environment' ; var Router = Ember . Router . extend ({ location : config . locationType }); Router . map ( function () { this . route ( 'cars' , function () { this . route ( 'new' , {}); }); }); export default Router ; Add the form for creating a car to our cars/new template: <!--app/templates/cars/new.hbs--> New Car <form {{ action ' createCar ' name on= 'submit' }} > {{input name='car-name' value=name}} <button> Create Car </button> </form> We know we'll need an action to handle the creation of the car, so we'll go ahead and declare that now.  Our test will fail because there's nothing to handle the action named createCar yet. My preference is to handle anything related to data in the route when I can, so we'll do that. // /app/routes/cars/new.hbs import Ember from 'ember' ; export default Ember . Route . extend ({ actions : { createCar ( name ){ const car = this . store . createRecord ( 'car' , { name }); car . save () . then (() => { this . transitionTo ( 'cars' ); }). catch (() => { // something that handles failures }); } } }); Now our Ember pieces are hooked up, but the test fails because mirage doesn't see a route that specifies a POST request to /cars . Add it to the Mirage config file. // /app/mirage/config.js export default function () { //... this . post ( '/cars' , ( db , request ) => { return JSON . parse ( request . requestBody ); }); }; Our JSONAPIAdapter sends the serialized data in the correct format, so all we have to do is parse it, and return it. And with that our test should pass. Viewing Parts I mentioned earlier that our cars contain parts. We'll make it so that when we click our car, we will be taken to that that car's parts page. Let's generate a test for parts. $ ember g acceptance-test parts Delete the generated test and add the following. //tests/acceptance/parts.js test ( 'when I click a car, I see its parts' , ( assert ) => { const car = server . create ( 'car' ); const parts = server . createList ( 'part' , 4 , { car_id : car . id }); visit ( '/cars' ); click ( '.car-link' ); andThen (() => { assert . equal ( currentURL (), `/car/ ${ car . id } /parts` ); assert . equal ( find ( '.part' ). length , parts . length ); }); }); Our first breakage occurs because Mirage has no part factory. //mirage/factories/part.js import Mirage from 'ember-cli-mirage' ; export default Mirage . Factory . extend ({ name ( i ) { return `Part ${ i } ` ; } }); Now QUnit yells because we have no links. Turn our list of cars into links, so that when we click on one, we can see that car's parts. <!-- templates/components/a-car.hbs --> {{#link-to 'car.parts' car class='car-link'}}\n  {{car.name}}\n{{/link-to}} QUnit shames us for not having a car.parts route. $ ember g route car/parts The router should look like: //router.js import Ember from 'ember' ; import config from './config/environment' ; var Router = Ember . Router . extend ({ location : config . locationType }); Router . map ( function () { this . route ( 'cars' , function () { this . route ( 'new' , {}); }); this . route ( 'car' , function (){ this . route ( 'parts' , {}); }); }); export default Router ; We'll add a dynamic segment of id to the car path. //... this . route ( 'car' , { path : '/car/:id' }, function (){ this . route ( 'parts' ); }); //... }); export default Router ; Since our route is nested, we need to specify the model for the parent route. $ ember g route car In the car route, return the car specified by the id dynamic segment. //routes/car.js import Ember from 'ember' ; export default Ember . Route . extend ({ model ( params ){ return this . store . find ( 'car' , params . id ); } }); We also have to create a Mirage route to GET a single car. At this point in the app, we have had our cars loaded from visiting the index, but a user could go straight to a car/:id url, so we need to handle that.\nJSON API requires relationship information to be stored in a 'relationships' object. Add it to your mirage config file. //mirage/config.js export default function () { //... this . get ( '/cars/:id' , ( db , request ) => { let car = db . cars . find ( request . params . id ); let parts = db . parts . where ({ car_id : car . id }); let data = { type : 'car' , id : request . params . id , attributes : car , relationships : { parts :{ data :{} } } } data . relationships . parts . data = parts . map (( attrs ) => { return { type : 'parts' , id : attrs . id , attributes : attrs }; }); return { data }; }); } Additionally, in our Mirage /cars route, we are only\nreturning the car information, not the associated parts.  What this means is, if the first page we visit is the /cars page, those cars will already be loaded in the store (with no knowledge of any associated parts).\nWhen we go to the cars/part page, the store won't fetch the model, because it's already in the store, so there will be no parts available to render.  We should load a cars parts in the cars/index route. //mirage/config.js export default function () { this . get ( '/cars' , ( db , request ) => { let data = {}; data = db . cars . map (( attrs ) => { let car = { type : 'cars' , id : attrs . id , attributes : attrs , relationships : { parts : { data : {} } }, }; data . relationships . parts . data = db . parts . where ({ car_id : attrs . id }) . map (( attrs ) => { return { type : 'parts' , id : attrs . id , attributes : attrs }; }); return car ; }); return { data }; }); //.... We also need the Mirage end-points for getting a part. //mirage/config.js export default function () { //... this . get ( 'parts/:id' , ( db , request ) => { let part = db . parts . find ( request . params . id ); let data = { type : 'parts' , id : request . params . id , attributes : part , }; return { data }; }); //... Now we need a part model, and a factory. //models/part.js import DS from 'ember-data' ; export default DS . Model . extend ({ name : DS . attr ( 'string' ), car : DS . belongsTo ( 'car' ) }); import Mirage from 'ember-cli-mirage' ; export default Mirage . Factory . extend ({ name ( i ) { return `Part ${ i } ` ; } }); And update our car model to show the association. //models/car.js import DS from 'ember-data' ; export default DS . Model . extend ({ name : DS . attr ( 'string' ), parts : DS . hasMany ( 'part' ) }); And our template: <!-- car/parts.hbs --> Parts <ul> {{model.name}}\n  {{#each model.parts as |part|}} <li class= 'part' > {{part.name}} </li> {{/each}} </ul> And now our test should be green. I'll leave converting the part into a component with an integration test as an exercise for you to complete. The steps are the same as they were for cars. Adding Parts Our last test will cover adding parts. At the bottom of your parts acceptance test: //tests/acceptance/parts.js test ( 'I can add a new part to a car' , ( assert ) => { server . create ( 'car' ); visit ( '/cars' ); click ( '.car-link' ); click ( '.new-part' ); fillIn ( 'input[name=\"part-name\"]' , \"My new part\" ); click ( 'button' ); andThen (() => { assert . equal ( find ( '.part' ). text (). trim (), \"My new part\" ); }); }); Our test tells us we don't have a '.new-part' link. in our template: Parts <!-- car/parts.hbs --> <ul> {{model.name}}\n  {{#each model.parts as |part|}} <li class= 'part' > {{part.name}} </li> {{/each}} </ul> {{#link-to 'car.new-part' model class='new-part'}}\n  Add new Part\n{{/link-to}} $ ember g route car/new-part Now we need a 'car.new-part' route. //router.js Router . map ( function () { this . route ( 'cars' , function () { this . route ( 'new' , {}); }); this . route ( 'car' , { path : '/car/:id' }, function (){ this . route ( 'parts' , {}); this . route ( 'new-part' , {}); }); }); And a template for our route to render. <!--templates/car/new-part--> New Part <form {{ action ' newPart ' name on= 'submit' }} > {{input name='part-name' value=name}} <button> Create Part </button> </form> //routes/car/new-part.js import Ember from 'ember' ; export default Ember . Route . extend ({ actions : { newPart ( name ){ const car = this . modelFor ( 'car' ); const part = this . store . createRecord ( 'part' , { name , car }); part . save (). then (() => { this . transitionTo ( 'car.parts' , car ); }); } } }); And a Mirage endpoint: this . post ( 'parts' , ( db , request ) => { return JSON . parse ( request . requestBody ); }); And we're done! This process will be even easier once Mirage supports JSON API, which is on its way.  You can view the source at https://github.com/jsncmgs1/mirage-tutorial.git . Demystifying Ember Async Testing JSON API Ember CLI Mirage Was this post helpful? Share it with others. Tweet Share Post", "date": "2015-10-19"},
{"website": "Hash-Rocket", "title": "Design That's Not a Pain in the Neck", "author": ["\nMike Fretto\n\n"], "link": "https://hashrocket.com/blog/posts/design-thats-not-a-pain-in-the-neck", "abstract": "Design Design That's Not a Pain in the Neck by\nMike Fretto\n\non\nMay 14, 2015 Everyone has worn them—and let’s be honest, aren’t they embarrassing? They are clunky, they are dorky and they are poorly made, are they not? Conference lanyards have always bothered me, which is what set me on the path to design a better one. When I was tasked with designing them for Ancient City Ruby , an annual conference that Hashrocket hosts, I saw it as an opportunity to find more use for the lanyard. We designers at Hashrocket are in the business of creating useful and enjoyable experiences for people. The following is a glimpse into how I applied that approach to designing our lanyards for the conference. Creating the Booklet From the beginning, the team’s memories were fresh from ACR 2013, when they suffered through jamming name badges into plastic sleeves that didn’t fit, so I decided that a booklet format might work better—especially when we considered how much useful information we could include in them. After creating nearly a dozen mockups, I decided to use a durable heavyweight recycled craft paper for our covers (environmentally friendly was a huge plus). It was also a stock that could withstand two days of conference punishment. It was bound by hand with copper rivets that we sourced from a scrapbook manufacturer. The inside pages were printed on an uncoated lightweight stock that was chosen to allow our attendees to easily jot down notes in a dedicated “notes” section in the back of the booklet.  We even tested multiple lanyard ropes to ensure they’d be long enough for attendees to easily read the booklets without needing to take them off. A Look Inside Going with a booklet format gave us a ton of pages to work with, so we decided to put the entire speaker schedule in there, complete with talk summaries and speaker bios. Offering our attendees this information offline saved them from having to locate the speaker lineup on the website (especially given that conference Wi-Fi is often a little sketchy). St. Augustine is packed with great restaurants, so I naturally wanted our guests to experience them. We added a map to feature our favorite nearby restaurants, shopping spots, and coffee shops. One thing that’s special about ACR is its events. This year we had a pirate ship ride, distillery tour, game night, and a sponsored happy hour. Ready to sign up for the next Ancient City Ruby yet? Information about the events was listed in the lanyard booklets along with some slick photography. I had a lot of great images between a few gems taken in previous years and other photos that we got permission to use (thanks, Joe Mills ). Some Personal Touches As designers, we are constantly looking for opportunities to bring delight to people through our work; our conference lanyards were no exception. Here are a couple of other little details that we’re really proud of: There’s a reason why attendees’ names were printed on covers of the booklets as opposed to being handwritten: it feels more special—more valuable. I often feel rushed and unappreciated when waiting in line at the registration table, while a well-meaning conference worker asks me how my name is spelled. This is one of the first experiences at a conference event and is a huge opportunity to convey appreciation and value for your guests. Presenting an attendee with a lanyard that has their name pre-printed on it communicates something entirely different than fumbling to write a name down while being filed through a registration table. We here at Hashrocket wanted to do something special for those who have attended Ancient City Ruby in the past, so we designed unique buttons for first, second and third-year ACR attendees. It was a simple gesture to help our growing community of ACR faithful feel recognized and appreciated. Folks from all over the world were attending Ancient City Ruby. We wanted them to have a enjoyable experience, yes—but also wanted to show Hashrocket’s appreciation and add a personal touch. Designing a conference lanyard that provided value, meaning and ultimately enhanced the experience—now that was important, even if, in the end it still looks like a glorified necklace. Was this post helpful? Share it with others. Tweet Share Post", "date": "2015-05-14"},
{"website": "Hash-Rocket", "title": "Bulk Imports with Datomic", "author": ["\nJoshua Davey\n\n"], "link": "https://hashrocket.com/blog/posts/bulk-imports-with-datomic", "abstract": "Bulk Imports with Datomic by\nJoshua Davey\n\non\nSeptember 17, 2015 I've been really happy with Datomic, but doing an initial bulk import\nwasn't as familiar as SQL dump/restore. Here are some things that I've\nlearned from doing several imports. Use core.async The Datomic transactor handles concurrency by transacting datoms\nserially, but that doesn't mean it isn't fast! In my experience, the bottleneck is actually in the\nreshaping of data and formatting transactions. I\nuse core.async to parallelize just about everything in the import\npipeline. One example of how I've leveraged core.async for\nimport jobs can be found in my Kevin Bacon project\nrepository . Run the import locally I use DynamoDB as my storage backend in production. I used to try to\nrun my import tasks directly to the production transactor/storage.\nLately, though, I've found it really helpful to run my import tasks to\na locally-running transactor and the dev storage backend. Running an import locally means I don't have to worry about networking, which speeds the whole process up quite a bit; also, it give me a much more freedom to iterate on the database\ndesign itself. (I rarely get an import correct the first time.) And\nin the case of DynamoDB, I save some money, as I don't have to have my\n\"write throughput\" cranked way up for as long. Clean up the local database Bulk imports create some garbage, so manually reindexing before backing\nup is advantageous. Here's what a REPL session looks like: ( def conn ( d/connect \" datomic :dev://localhost:4334/database-name )) ( d/request-index conn ) ( ->> conn d/db d/basis-t ( d/sync-index conn ) deref ) ;; blocks until done indexing ( d/gc-storage conn ( java.util.Date. )) For more information on why this cleanup\nis important, see the relevant Datomic\ndocumentation . Use backup/restore Once everything looks good on the local production database, I use\nDatomic's builtin backup/restore facilities to send the database\nup to production. Assuming you've already deployed a production transactor and provisioned DynamoDB storage, here's the process I follow: Run the datomic backup-db command against the local import. Crank my \"write throughput\" on DynamoDB way up (on the order of 1000). Run the datomic restore-db command from the backup folder to the\nremote database. Turn the \"write throughput\" back down to whatever\nvalue I plan to use for ongoing use (see the Datomic\ndocumentation for more\ninformation). The heart of almost every business is its data. Datomic is a great\nchoice for business data, in part because it treats all data as\nimportant: nothing is overwritten. New things are learned, but the old\nfacts are not replaced. And knowing how to get your data into Datomic\nis half the battle. Go forth and import! Was this post helpful? Share it with others. Tweet Share Post", "date": "2015-09-17"},
{"website": "Hash-Rocket", "title": "Classical mixin inheritance in Sass", "author": ["\nCameron Daigle\n\n"], "link": "https://hashrocket.com/blog/posts/classical-mixin-inheritance-in-sass", "abstract": "Design Classical mixin inheritance in Sass by\nCameron Daigle\n\non\nOctober 27, 2014 I had a crazy idea the other day, and it's grown into something pretty awesome (but still a little crazy): a pattern for scalably 'extending' Sass mixins in a sort-of-classical way. The problem: extend and mixin aren't simultaneously awesome Here's the problem I wanted to solve. Suppose I have a button that gets slightly lighter on :hover . One way to do this would be via simple subclasses, or perhaps Sass' @extend feature, like so. %button-base // some other button styles here background : blue & :hover background : lighten ( blue , 10% ) a .button @extend button-base a .cancel-button @extend button-base background : red & :hover background : lighten ( red , 10% ) This is inconvenient and scales poorly - I don't want to have to write my color math logic again for each new button subclass. Sass isn't doing a lot for me here. But if I use @mixin instead of @extend , I get something new: arguments. I can pass a $color argument to my mixin, and it can operate on that one color to calculate the hover state. Like so: @mixin button-base ( $color : blue ) // some other button styles here background : $color & :hover background : lighten ( $color , 10% ) a .button @include button-base a .cancel-button @include button-base ( $color : red ) Cool, right? But I'm still only halfway there: my other-button class is having to pass an argument to a mixin, when that other button might make more sense as a child mixin with its own defaults, so it can be injected elsewhere into my styles. @mixin cancel-button @include button-base ( $color : blue ) Awesome. But here's the problem: @cancel-button doesn't know how to pass its own arguments to button-base unless you specify them directly ... so in an effort to expose properties consistently, you have to start writing some pretty silly stuff: @mixin cancel-button ( $color : blue ) @include button-base ( $color : $color ) Once you're dealing with lots of arguments, having to explicitly define those arguments becomes painful and redundant. So, that's the setup. Thanks for sticking with me so far. This is the problem I decided to try and solve: could I create a system in which Sass mixins inherited properties in a semi-classical way, instead of having to write all of that stuff out? Into the mouth of madness What I've written is a set of mixins that takes advantage of Sass' rendering order to store & retrieve parameters outside of mixins. Here's how it works: you only ever pass one argument to a mixin: a Sass map, which defaults to (). you call a mixin named override and pass it that single argument. This essentially tells Sass that this mixin is a child of another mixin, and stores the argument map in a global store. (If the mixin isn't the end of the inheritance chain, set $is_child to false .) you set default attributes for this child mixin by calling a default mixin and passing it an attribute & value pair. Anywhere along the way, instead of accessing variables directly, you can call fetch to retrieve them from the global storage. Here's what it looks like in action. @mixin button-base ( $args : ()) //button styles @include override ( $args , $is_child : false ) @include default ( color , blue ) background : fetch ( color ) & :hover background : lighten ( fetch ( color ) , 10% ) @mixin cancel-button ( $args : ()) @include override ( $args ) @include default ( color , red ) @include button-base There's a lot going on here, but this is the key takeaway: cancel-button no longer needs  to know about the arguments used by button-base . This means that as you add arguments to button-base , you don't have to change anything about its children: cancel-button will automatically pass all arguments up the chain. So, suppose I want to add support for a text-color argument, because I want to perform some color math on the text color and have it lighten on :hover . All I have to do is add two lines to button-base : a default value for text-color , and a style assignment. @mixin button-base ( $args : ()) //button styles @include override ( $args , $is_child : false ) @include default ( color , blue ) @include default ( text-color , yellow ) // set a new value background : fetch ( color ) color : fetch ( text-color ) // assign the new value & :hover background : lighten ( fetch ( color ) , 10% ) color : lighten ( fetch ( text-color ) , 10% ) // whee , color math I haven't changed cancel-button at all, and yet I can do this: .cancel-button @include cancel-button (( text-color : purple )) Voila! cancel-button is \"inheriting\" all of the properties of button-base (and passes along arguments to it) without those properties having to be explicitly redefined. In a perfect world, this would be accomplished by some sort of actual extends command implemented natively, but until that day, this solution gives me something pretty special. Under the Hood Once you wrap your head around it, this inheritance model can be really powerful. Here's what's going on (I'm going to switch to mixin shorthand syntax here, because I like it): Passing arguments as a Sass map When you pass arguments with this technique, you're actually passing one single argument that is being stored in a global variable. override stores its $args map in this global store, overwriting whatever's there. If $is_child is true, it's the beginning of a new mixin inheritance chain, so it initializes the global store first. $global_store : () = override ($ args :() , $is_child : true ) @if $is_child $global_store : () ! global $global_store : map-merge ( $global_store , $args ) ! global As Sass parses the mixin inheritance chain, you can add more values by passing a key/value pair (or even a map of key/value pairs) to default : // usage: // +default(color, red) // +default((color: red, background: blue)) = default ( $ key_or_map , $ value :null ) @if $value +store-default ( $key_or_map , $value ) @else @each $key , $value in $key_or_map +store-default ( $key , $value ) = store-default ( $ key , $ value ) @if map-has-key ( $global_store , $key ) @else $global_store : map-merge ( $global_store , ( $key : $value )) ! global Finally, fetch retrieves the value from the global store. // usage: // color: fetch(color) @function fetch ( $key ) @return map-get ( $global_store , $key ) That's that. I'm calling it inheritance.sass, and it's in a gist here. Grab it and try your hand at it yourself – I'm currently using this technique on a client project, and it's actively making my life easier. Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-10-27"},
{"website": "Hash-Rocket", "title": "CSS Bar Charts with Flex and attr()", "author": ["\nCameron Daigle\n\n"], "link": "https://hashrocket.com/blog/posts/css-bar-charts-with-flex-and-attr", "abstract": "Design CSS Bar Charts with Flex and attr() by\nCameron Daigle\n\non\nJuly 27, 2015 The Hashrocket blog has a little cousin now: Today I Learned is doing quite well for itself these days, with over 200 posts from 13 different Rocketeers since its launch in May. How do I know this? We've got a spiffy new statistics page. Given that TIL's format lends itself to flurries of bite-size posts from a variety of people, we decided that a statistics page would be a great way to help visitors get a handle on what (and how often) we're posting. We list channels, authors, and most popular posts, all capped by a bar chart showing posts per day for the last 30 days. After toying with some of our usual graphing libraries, I decided to see if I could pull off building a responsive bar chart, complete with mouseover labels, in just CSS – so I did. Here's how it works. Structuring the bar chart with flex I wanted a fully-responsive chart with 30 bars. This is where display: flex really shines. Before flex, I'd probably have to use some table-cell hackery, but no more. With display: flex and align-items: stretch , we can easily make a set of any arbitrary number of bars with 2 pixels in between each; flex handles stretching the bars to the proper width & height. ul .activity_chart display : flex height : 10vw align-items : stretch li flex : 1 1 margin : 0 1px background : silver (If you don't recognize vw units, 1vw is 1% of your viewport's width – check out my TIL post for more hot vw tips.) Positioning the bars Once I had the bar backgrounds in place, positioning the bars themselves was simple: I just stuck an absolutely positioned element inside each background element, with a percentage height output by our Rails view: ul .activity_chart // stuff from earlier li position : relative .activity_chart_bar position : absolute right : 0 bottom : 0 left : 0 background : navy With each bar positioned to the bottom of their respective bar background, the Rails view could then give each bar a percentage height as an inline style: %ul .activity_chart - posts_per_day . each do | ppd | %li ( data-date= \"#{ppd.date}\" data-amount= \"#{ppd.count}\" class= \"#{if ppd.count == 1 then 'singular' end}\" ) .activity_chart_bar ( style= \"height: #{compute_percentage(ppd)}%\" ) You'll note that the %li has two data attributes on it, as well as a class of singular . Those are our CSS graph labels in action. CSS label hovers: using attr() Lastly, I wanted the chart labels to not require JavaScript or extra elements of any kind. This is where the attr() property is really useful in CSS: I could set up labels as :before and :after elements on each bar, and define their content by accessing a data attribute using attr() . Here's a subset (minus aesthetic styling) of the code to show the day's date above each bar: ul .activity_chart li & :before display : none position : absolute left : 50% margin-left : -5rem bottom : 100% margin-bottom : .5rem width : 10rem text-align : center content : attr ( data-date ) & :hover & :before display : block As you'll see, there's some negative-margin stuff to center our label properly, some positioning to stick it on the top of the bar, and content: attr(data-date) , which simply takes the text from the aforementioned data-date attribute and sticks it in the :before element. Convenient! Our hover state also shows the number of posts on a particular day – to accomplish this, we use a lot of the same code from above, plus some different positioning to put it below the bar, and some different content. The content attribute conveniently concatenates attr with any other string info, so we can do this: & :after margin-top : .5rem top : 100% content : attr ( data-amount ) \" posts\" And because overlooked pluralization (\"1 posts\") drives me nuts, here's where that conditional singular class comes in. Having to do this is a little goofy, but hey, CSS doesn't know how to pluralize, so we just have a more robust language tell it what to do. & .singular :after content : attr ( data-amount ) \" post\" So there you have it – a responsive, CSS-only bar graph, using some nifty new(ish) features. I put together a basic Codepen with the example code from this post, and of course check out the real TIL Statistics page for the full version, which also includes some more hover effects and label-positioning intricacies. Was this post helpful? Share it with others. Tweet Share Post", "date": "2015-07-27"},
{"website": "Hash-Rocket", "title": "Reviewing Code", "author": ["\nJake Worth\n\n"], "link": "https://hashrocket.com/blog/posts/reviewing-code", "abstract": "Process Reviewing Code by\nJake Worth\n\non\nOctober  8, 2015 The dreaded code review. Nobody likes receiving or delivering a code review, but scrutinizing our code leads to a better product. \"In my reviews, I feel it's good to make it clear that I'm not proposing objective truth, but subjective reactions; a review should reflect the immediate experience.\" – Roger Ebert Code reviews are crucial – one set of eyes on a codebase is not enough. Modern applications are simply too complex for one person to manage alone. We review code to prevent regressions, document behavior, ensure quality, and share knowledge. The ability to justify and explain your design choices is an essential skill for any developer. (I'm using the term 'code review' here in the same way others might say 'code audit': I'm reviewing a pull request on a project I maintain from a developer I don't yet have a relationship with. I'm the gatekeeper, and and my goal is to approve the changes or suggest improvements.) In this post, I'd like to discuss the way I approach a code review. Everybody does this their own way, but I have a checklist, some tools, and general principles that help me as I review. My Checklist The first thing I ask is: do the existing tests pass? This is a no-brainer. The new feature can't introduce regressions or break tests. Next, I look for tests that document the new behavior. We keep our code test-driven at Hashrocket, and expect to see tests for any feature. Having tests goes a long way toward preventing regressions and conveying intention. If a model is introduced, I look for unit tests. If it's a feature, I look for integration tests. My next question is, does it work? Fire up your browser and walk through the story. If you can't manually test the feature in a few minutes, the story (or documentation) is unclear. I like to look at the commits, too. Good commits should be atomic: the tests should pass at every step, and the code should be moving toward a realized feature. The commits should be concise and intention-revealing. Anything vague during a review is going to be unintelligible in the future, because the author won't be there to explain. A good git rebase can make anybody look like a genius. Here is an example: commit 55jc0bc8e5f1ff4979699b400001f603011e2256\nAuthor: Developer Alpha and Developer Bravo <dev+deva+devb@hashrocket.com>\nDate:   Tue Sep 29 09:50:25 2015 -0500\n\n    Add middleman instructions to README\n\n    diff --git a/README.md b/README.md\n    index f02eeb3..93919a2 100644\n    --- a/README.md\n    +++ b/README.md This message explains exactly what the commit does, in the tense (present) we tend to use on our projects. At 35 characters, it's short enough that no content will be wrapped by any view on Github. It states that it only changed one file, and then only changes one file. If you checkout this commit in Git and run the tests, they pass. Tim Pope wrote about this in 2008, and the advice is still relevant today. Other factors to consider: does the code use design patterns already established in the project? Does it conform to company and technology-specific styles? Tools There are many tools to help facilitate a review. Github comments are useful for talking about specific lines of code. Code linters like Rubocop , HAML-lint , or Hound can be used to find style violations. There are static analyzers for code smells like Code Climate , security like Brakeman , redundant queries like Bullet , coverage like SimpleCov . Like Sandi Metz's ideas? There's a gem ( SandiMeter ) for that. Some of these can even be configured with Github to automatically check against any pull request. Make the computers do the work! Guidelines Be nice . It's easy to shoot down somebody's idea, but it's hard to build on an idea or provide a thoughtful alternative. Everybody is trying to write the best code they can. If you are receiving a code review, remember that you are not your code. Teach . Code reviews can be a great opportunity to teach. Some of my favorite techniques have come from people reviewing my code. Don't miss an opportunity to explain. Pair. Pair programming is like a continual, in-person code review, and it beats a traditional review because it's happening as you type. Conclusion Code reviews matter. With these guidelines, tools, and techniques, I can critique anyone's code productively. Better code helps our clients, company, and the open source community continue to thrive. Was this post helpful? Share it with others. Tweet Share Post", "date": "2015-10-08"},
{"website": "Hash-Rocket", "title": "Ruby5 Roundup - Episode 505", "author": ["\nPaul Elliott\n\n"], "link": "https://hashrocket.com/blog/posts/ruby5-roundup-episode-505", "abstract": "Ruby5 Roundup - Episode 505 by\nPaul Elliott\n\non\nOctober 24, 2014 We knew you were wondering what happened in the Ruby community over the last few days. It's ok, Lynn and I are back with everything you need to know. Here is a quick roundup of what's new in this episode of the Ruby5 podcast. http://ruby5.envylabs.com/episodes/542-episode-505-october-24th-2014 ENVied http://www.gertgoet.com/2014/10/14/envied-or-how-i-stopped-worrying-about-ruby-s-env.html The ENVied gem let's you be explicit about environment vars in your app. You can fail fast when required ENV's aren't present, set defaults, and specify automatic coercion. Payola https://www.petekeen.net/introducing-payola The payola gem makes it easier than ever to set up Stripe in your Rails app. It is an engine that comes ready to use and can be styled to fit with your existing look and feel. Pippi http://thomasleecopeland.com/2014/10/22/finding-suboptimal-api-usage.html The pippi gem performs code analysis at runtime and detects inefficient code execution loops. Run it as part of your test suite and see where you can improve performance and readability in your codebase. Multithreading in MRI http://www.csinaction.com/2014/10/10/multithreading-in-the-mri-ruby-interpreter This blog post talks about how threads are handled in Ruby and why it is important to you. It is a short read and will help clear things up if you don't quite get how it works. Practicing Rails http://www.justinweiss.com/blog/2014/10/20/writing-better-tests-with-the-three-phase-pattern Justin Weiss just released the beta version of his new book, Practicing Rails! If you are learning Rails and want to take your skills to the next level, this is a great place to look. His content is always top notch and the book is well worth the money. So that's it for this episode of Ruby5. If you haven't already, subscribe to the podcast and keep yourself up to date. Thanks for listening! Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-10-24"},
{"website": "Hash-Rocket", "title": "Rich UI Prototyping with Stagehand", "author": ["\nCameron Daigle\n\n"], "link": "https://hashrocket.com/blog/posts/rich-ui-prototyping-with-stagehand", "abstract": "Javascript Design Rich UI Prototyping with Stagehand by\nCameron Daigle\n\non\nJanuary 13, 2014 I'm a huge proponent of increasing designer/developer communication and easing the handoff between static and implemented markup – and Stagehand is my latest attempt to bridge the gap. Prototype UI without writing Javascript. Stagehand is a jQuery plugin that allows non-developer-types to simulate the states of a page simply by adding a few data attributes. For example, if a designer or frontend dev needs to slice a search page into HTML/CSS, they can easily slice the blank slate , search results, and 'no results' messaging all in one view. Then, using the Stagehand toolbar, anyone visiting the static markup of the Search page can flip through the various states: %section ( data-stage= 'search' data-scene= 'blank slate' ) %h1 Search for something! %section ( data-stage= 'search' data-scene= 'results' // cool search result listing % section ( data-stage= 'search' data-scene= 'no results' ) %h1 No results for this query = link_to \"try searching for something else\" , \"#\" That's just the tip of the iceberg. In the month since I released Stagehand, it's been used for toolbars, nav dropdowns, dynamic forms, and more. It's a new way to think about mocking interfaces – one that's resulted in saved time and increased markup quality for our projects. A solution like Stagehand was a long time coming. We've tried a number of different solutions over the years in efforts to figure out where to draw the line between simulating & implementing Javascript interactions: In which we stub out AJAX calls. Back in the day, slicing AJAX features just involved setting the code up to call a dummy URL, which would then be swapped out for a real call in the real app view. We'd find ourselves with broken static markup as the Javascript was implemented for the real app, but we treated that as a necessary tradeoff for how convenient the URL-swapping method was. To make matters worse, as client-side interfaces became more complex and frameworks (Backbone, Ember, etc.) became more common, we'd find unexpected layout bugs that were either deep within the implemented app or just plain hard to reproduce. In which we double down on Javascript files. We've also tried maintaining a separate Javascript file just for the static markup, full of quick & easy jQuery toggles and animations. But as projects grew in complexity, the dummy JS would grow as well, resulting a few hundred lines of essentially throwaway code – plus, we ran the risk of making Javascript implementation decisions prior to the implementation of the framework that'd be tying everything together. Unhealthy! Stagehand: decoupled and happy. Enter Stagehand. Now there can be a bare, bare minimum of JavaScript to go with our static markup, and interface states are simulated with Stagehand data attributes. Broken JavaScript is a thing of the past, and no throwaway JavaScript is written. Honestly, it was hard for me to let go after years of the Hashrocket design & frontend team being responsible for core of the Javascript interactions along with the HAML/SASS. But thanks to Stagehand, I find myself focusing on designing & slicing every possible interface state, because it's so easy to write them all at once. It's helped me catch layout issues sooner and reduces the amount of assumptions the developers need to make when implementing views. And I'm actively avoiding writing JavaScript, leaving 100% of the implementation to our capable team of developers, who've quickly and capably embraced JS & Coffeescript alongside Ruby, Elixir, Go and all the other stuff we're writing around here. As the nature of applications has changed, we must adapt – and it's my hope that Stagehand is the next step that we've been looking for. That's a bit of how Stagehand has changed our process for the better – maybe it can do the same for you. Check out the Stagehand homepage for documentation & more examples – and then go forth and slice! Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-01-13"},
{"website": "Hash-Rocket", "title": "10 Tips to Fine-Tune Your Photoshop Workflow", "author": ["\nRye Mas"], "link": "https://hashrocket.com/blog/posts/10-tips-to-fine-tune-your-photoshop-workflow", "abstract": "Design 10 Tips to Fine-Tune Your Photoshop Workflow by\nRye Mason\n\non\nSeptember 11, 2014 We use Photoshop for the majority of our visual design work here at Hashrocket. These 10 tips can help improve your process (and make your life easier) when you're working on a design. 1. Leave the Guidemaking to a Robot I usually create new documents for the web at ~1500px wide and create my final layout at 1000px. I used to drag guides to mark the center point and boundaries until I found GuideGuide (free), a Photoshop plugin that draws guides and grids for you. 2. Double-check Your Profiles If you've worked with Photoshop and the web for any length of time, you've probably come across color discrepancies that make you ask why, Photoshop, why? Turns out there are a few settings that need to be changed. I won't attempt to explain the process here, but I highly recommend reading this Bjango post to familiarize yourself with color management. 3. Pick any Color Without Banging Your Head Against the Wall Speaking of color, it can be frustrating to pick one from a source outside of Photoshop. (Let's not talk about the times I've pasted in screenshots to get hex codes.) There's a great little standalone tool called Sip (free, Mac-only), which lets you color pick anything anywhere on your screen and copies the hex, RGB, or a host of other options to your clipboard. 4. Don't Drown in Layer Metadata I like to keep the layers palette clean with a minimal amount of visible info. From the menu in your layers palette, choose \"Panel Options,\" select \"None\" for Thumbnail Size, and deselect both \"Expand New Effects\" and \"Add 'Copy' to Copied Layers and Groups\" if they're checked. This cuts down on all the thumbs, \"rectangle copy 327\" verbiage, and endless stair-stepping of effects that can quickly crowd your palette. (Just don't leave copied layers the way they are – adopt the habit of immediately naming them something specific.) Also, a quick way to collapse your groups (and vice versa) is to alt/opt+shift+click the parent group, which I wish I'd known about sooner. You can also target visibility for a specific layer by alt/opt-clicking its eye icon. 5. Create Hierarchy and Stay Specific Before I add any elements to my PSD, I like to create hierarchy with top-level layer groups. Start with the footer (or the lowest grouping of elements on the screen) and work your way up so you don't have to rearrange the groups later. Typically I'll have layer groups named footer, body, sidebar, nav, and header. Any elements that get added to the document are grouped within the appropriate parent group and organized top to bottom, left to right, and named specifically, which makes them a whole lot easier to modify later. 6. Stick with Vector Work in vector for every shape you possibly can. If I create an icon with multiple parts, I'll merge the final shapes when I'm done if the design allows. Smart objects can be helpful in situations when you might update an element's style across many instances of that element, but where possible, try to stay away from using them to cut down on file size and frustration (like trying to click the shape to find out where it is only to be foiled by its non-clickability). 7. Color Code all the Things Use color coding for different states (like Hover, Active, etc) – especially if their visibility will eventually be turned off – for quick visual indicators. 8. Get Rid of Bloat When your file is complete, turn on visibility for all the layers you'll use, leaving unused layers hidden. Then clean up those extraneous layers by choosing \"Delete Hidden Layers\" from the menu in your layers palette. 9. Export Easier (usually) Exporting slices can be tricky. I use Slicy ($29, Mac-only) for just about everything, and it's been well worth the money. There's also Photoshop's new built-in slicing tool, and, if all else fails, slicing by hand. Slicy is easy to use, and can handle pretty much anything you throw at it when you add a file extension to the end of your layer/group name. By default it slices directly around the bounds of your object, but you can customize boundaries for situations where you need uniformly- or specifically-sized assets – perfect for exporting multiple icons that need to have the same dimensions. You can choose where to save the slices and whether you want Slicy to automatically update them when your PSD is modified. Photoshop's Image Asset Generator works similarly, but isn't quite as feature-rich. After appending your layer names with extensions, you select \"Image Assets\" from File > Generate and save your file, and the appropriately named layers are then saved in the background to a folder alongside your PSD. The save location is kind of annoying because I'd rather choose where the slices end up instead of being forced to save them in what's usually my assets folder. Slicing by hand is my last resort when I can't get the results I want between Slicy and the Image Asset Generator. Sadly, nothing really helps with (good) retina image generation. You'll usually have to redraw those by hand. 10. Shrink Slices Outside of Photoshop This isn't about Photoshop per se, but rather a pair of post-Photoshop apps that are worth mentioning. Photoshop can't compress PNGs as well as ImageOptim and ImageAlpha (both free, Mac-only), so I shrink PNGs with these apps before sending them to the repo. These tips have increased my productivity in Photoshop, and hopefully they'll improve your workflow, too. Was this post helpful? Share it with others. Tweet Share Post", "date": "2014-09-11"}
]