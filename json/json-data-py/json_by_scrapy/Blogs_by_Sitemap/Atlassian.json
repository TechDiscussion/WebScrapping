[
{"website": "Atlassian", "title": "Try pair programming", "author": ["Lucy Bain"], "link": "https://blog.developer.atlassian.com/try-pair-programming/", "abstract": "Pair programming is great for getting started on a code base and an excellent way to get to know your teammates better. But if your team doesn’t pair regularly you might not know how to get started. If you’ve never paired before, give it a try and see how you like it! Share knowledge : you’ll learn, teach, and work with code you might have missed. Often you understand the code better at the end of a pairing session becuase each of you were asking “why?” more than when you work alone. Good introduction : pairing can be particularly useful for new hires and programmers fresh out of school . It’s a great way to get to know your team, learn about coding styles and expectations, and find who’s the right person to ask about a given topic. Stay focused : checking your social media site of choice is much less appealing when there’s a person right there to talk with. Better code : theoretically you don’t write code much faster with two people, but you write it with fewer bugs. There isn’t a lot of research to back this up. Personally, I think the other reasons are more compelling. You caught me! Pair programming is like code review . However, pair programming has a major advantage over code review: you review in real time. This article is about starting to pair in a culture that doesn’t do a lot (if any) pairing now. Some of the advice given will seem odd to people who pair regularly. My hope is that this gives a good starting point and introduction to pairing. Then people can choose to do more research into pairing and change their style to better fit them and their team. Anyone who wants (or is willing) to pair with you! A willing, happy pair is better than the “best matched” pair. Pair with a few (5+) people before you decide if pairing is for you. People don’t want you to waste your time by working on their task. Push back on this politeness, if it’s worth their time to do it, it’s worth you time to pair on it (not always true, but generally the case). For the first few pairing sessions, go to your pair. This will build up good karma before you ask them to come to you. Some people don’t love pairing, be willing to gracefully accept a “no.” Start by asking. No success? Send calendar invites to people. (People take you more seriously if you’ve got a super official calendar entry for your pairing session.) Send calendar invites to everyone on your team. Set up regular pairing sessions to learn, teach, and get caught up. People do everything from 90 min sessions to pairing all day, every day for a full sprint. One team found many short pairings was better (YMMV). Check with your pair on how long they want the pairing session to last. Start with 1.5 to 2 hour sessions and build up from there. It’s between you, your pair, and your team; just be sure that you’re all on the same page. If possible, begin with two sessions a week with different people on your team. That way you’ll see what it’s like to pair with different people. Driver : types, bounces ideas around; gets the micro view of the code Navigator : looks for logic problems, bugs, and better implementations, acts as a sounding board, and thinks ahead to potential problems; gets the macro view of the code Think of the navigator as the code reviewer. Constantly be on the look out for better solutions. Note: The names below are entirely my own invention, they’re not official pair programming vocabulary. Bring your keyboard and mouse, keep swapping the “driver” role with your pair. People usually swap every 20 minutes to hour; for beginners I’d err on the side of too short. My team has recently set up a pairing station. It has two keyboards, mice, and monitors – plug and play pairing! Same as above, but don’t bring your keyboard or mouse. Be the navigator the whole time. I love the noob. It opens opportunities to ask questions, learn how the team does things, and learn what your pair is particularly good at. Excellent for new hires like me! Like the noob, but you bring your laptop with you. Keep your laptop closed most of the time, only use it to check syntax, google solutions, or settle a debate. Don’t disengage from your pair for more than a few minutes. Start with the noob and fall back to the distracted when you need to look something up. Build up to the classic. Have good hygiene Make sure you’ve showered, put on deodorant, brushed your teeth, eaten a mint, and skipped the garlic. Be inclusive Talk a lot, seek first to understand, and make your pair feel welcome. Yay! Thanks for giving it a go. I’d love to hear about your experiences! If you hated it then I’m sorry to hear that. Did you try pairing with a few different people? If not, give it a couple more tries. Not everyone likes pairing, but I think it’s something you have to try first to know. This post is featured in our ebook, “Hello World! A new grad’s guide to coding as a team” – a collection of essays designed to help new programmers succeed in a team setting. Grab it for yourself, your team, or the new computer science graduate in your life. Even seasoned coders might learn a thing or two. Read it online now Click here to download for your Kindle 1minus1.com zenpayroll.com c1.staticflickr.com commons.wikimedia.org twicsy.com", "date": "2015-05-21"},
{"website": "Atlassian", "title": "The power of Git subtree", "author": ["Nicola Paolucci"], "link": "https://blog.developer.atlassian.com/the-power-of-git-subtree/", "abstract": "Git subtree allows you to insert any repository as a sub-directory of another one. It is one of several ways Git projects can manage project dependencies . People with good memory will remember I wrote about the usage and the advantages of the command in an earlier piece on Git submodule alternatives . Let’s review the basics so that you can decide if git subtree is useful for you. Imagine you want to add some external project to your own repository but you do not want to add too much to your daily process and the one of your peers. The subtree command works well in this case. For example to inject a vim extension in a repository that stores your vim setup you could do: This command will squash the entire history of the vim-fireplace project into your folder .vim/bundle/fireplace , recording the SHA-1 of master at the time for future reference. The result of a squashed “ git subtree add ” is two commits: If after a while you want to update that sub-folder to the latest version of the child repository, you can issue a “ subtree pull ” with the same parameters: That’s it for the basic usage. If you want to be more careful and structured you can add or pull only tagged revisions (e.g. v1.0 ) of your child project. This prevents you from importing code from a master that might not be stable yet. Note: git-subtree stores sub-project commit ids and not refs in the meta-data. But that’s not an issue since given a commit id ( sha-1 ), you can find the symbolic name associated with a commit with a command like ls-remote : If you use subtree commands often, you can shorten and streamline them with a couple of simple aliases in your $HOME/.gitconfig : The alias I use flips the original order of parameters because I like to think of adding a subtree a little bit like a scp command ( scp <remote src> <dest> ). You use them like this: I recently had a look at the implementation of git-subtree and boy is it clever! The first insight – deep I know – is that Git subtree is implemented as shell script and it’s nicely readable . The core technique of the command is the following: git-subtree stores extra meta-data about the code it is importing directly in the commits. For squashed pulls for example it stores these two values in the commit message before the merge: The “ git-subtree-split ” field records the commit id ( sha-1 ) of the subproject that has been injected at folder “ git-subtree-dir “. Simple enough! Using this information the subsequent git subtree pull can retrieve the previous integration point as base for the next squash/merge. How do you rebase a repository with sub-trees mixed in? From what I could derive from this Stack Overflow discussion , there is no silver bullet. A workable process seems to be just to basically do a manual rebase--interactive and remove the add commits, rebase--continue and re execute the git subtree add command after the rebase is done. One tiny thing that I found missing from the defaults of the command is that it does not store the URL of the original repository you are adding. I was reminded of this recently as I was trying to update all the vim extensions I track. I forgot all source repository URLs I had previously injected using git subtree add . Since attending Git Merge 2015 I’ve been energized to find ways to contribute to the project and so I said to myself: “instead of complaining about this, I can fix it!”. So I’ve started tweaking the git-subtree.sh script to do something extra. I changed git subtree add to annotate the squash commit with an extra field git-subtree-repo . So issuing: Results in a commit with that extra field: With this relatively small addition I can now write a new subtree command to list all the folders which have been injected from other repositories: Which helpfully outputs: Update 11th March 2016: As the “ list ” command finds commit ids for subtrees injected into the checked out branch the --resolve flag tries to look up the repositories at git-subtree-repo and retrieve the symbolic refs associated with the commit ids found. Example: The above changes and the “ list ” command implementation have been submitted to the Git mailing list for review and are currently sitting on my Git fork if you want to try them out. As soon as I have proper and solid tests to this change I’ll submit a patch to the core git mailing list and see if they find this addition useful. Hopefully yes! In any case I hope you enjoyed the above knowledge dump and ping me @durdn and @atlassiandev for more Git shenanigans. You might also enjoy our ebook, “Hello World! A new grad’s guide to coding as a team” – a collection of essays designed to help new programmers succeed in a team setting. Grab it for yourself, your team, or the new computer science graduate in your life. Even seasoned coders might learn a thing or two. Read it online now Click here to download for your Kindle", "date": "2015-05-18"},
{"website": "Atlassian", "title": "A git horror story: faking provenance with a loose nonce", "author": ["Tim Pettersen"], "link": "https://blog.developer.atlassian.com/git-horror-story-loose-nonces/", "abstract": "A few weeks ago, a “vulnerability” was discovered in a new security feature that shipped in Git 2.2.0: signed pushes . No need to panic though! It is vanishingly unlikely that anyone would be able to successfully exploit this particular problem. In fact, the issue was quietly fixed a few days ago in the 2.3.7 point release. The vulnerability does however make for a fascinating story, and serves as a cautionary tale for developers on limiting and sanitizing the content received over the wire from a server or client that you don’t completely trust. Push signing allows Git servers to guarantee that a particular client updated a branch or tag to point to a particular commit. This can be used, for example, to trace the origin of malicious code that has made its way into a repository. Roughly speaking, the signing process (that introduces our vulnerability) works as follows: The nonce in this case being a securely generated random string known only to the server. So, did you spot the problem? It’s pretty subtle. The nonce is used to prevent a particular man-in-the-middle (MITM) technique known as a replay attack . The server specifies a different nonce value at the beginning of each push process and will only continue when the client supplies a signed version of that nonce. The problem lies in the fact that the server specifies the nonce value to be used. If you’re familiar with other authentication systems that use nonces, for example OAuth, you might be used to the client generating a random nonce. This is not the case with the Git protocol. The reason a server generated nonce is problematic in this case is that a malicious server can use this mechanism to force a client to sign any arbitrary value that fits into a nonce with their private key. And it turns out the Git pack protocol is very loose when it comes to what values you can use as a nonce. Most modern Git clients will accept as a nonce any collection of bytes that is: For example, if I initiate a git push --signed , a malicious server could respond with the capability: And my Git client would automatically sign it with my private key and send it back to the serve r. This is something that only I should be able to willfully do, and observers (for example, my bank) could believe that I had authored or at least ratified the signed statement. Of course, the underscores look a bit weird. And the nonce is embedded in a push certificate, so there would be a bunch of junk in it that would tip off my bank before they forked over the cash (I hope). To illustrate, the signed push request would look something like: However, some binary file formats are very forgiving about the format of a file. Jann Horn , the security researcher who first reported the vulnerability, pointed out that PDF files are particularly problematic: If you replace all n with r and all spaces with f in a PDF file, then add lines above and below the PDF file, most PDF readers (e.g. evince and xpdf) will still treat it as a valid PDF file. So if the server had craftily encoded the request for $100,000 as a PDF and passed it as the nonce value, they would end up with a signed PDF that my bank would be much more likely to fall for. The vulnerability has been fixed by limiting the nonce size to 256 bytes and whitelisting the allowed characters [a-zA-Z0-9./+=_-] . It’s believed this will restrict the nonce content to the point where an attacker will be unable to force the client to produce a signed file of any exploitable value. Once again, it’s hard to label this issue a vulnerability, as the threat of exposure is so low. You can’t use this technique to make malicious changes to the repository, as the ref operations specified in the pack protocol are rigidly defined. It is only exploitable in that it can be used to generate signatures for content that fits into a nonce, and even then the signed content will contain push certificate cruft that will look very suspicious to most consumers. In the fraudulent bank transfer above, the looseness of the PDF specification is arguably just as much to blame as the nonce. In addition to this limitation, the attacker must have full control the Git server that the user is pushing to in order to exploit the problem. Or somehow spoof the server via MITM, which is difficult as most Git servers will be protected by TLS (particularly those frequented by Git users who sign their pushes). While this vulnerability is a neat trick and a fun hypothetical exploit, your Git servers and bank accounts are probably safe for the time being. However, there is a slightly more important take away for developers who dabble in security: if you’re ever programmatically using GPG keys, don’t sign arbitrary data from third parties! Keep an eye on @kannonboy for further musings about programming, git , and developer tools. The title of this post is borrowed from Mike Gerwitz’s excellent paper illustrating why tag, commit and push signing are valuable practices for security minded professionals.", "date": "2015-05-05"},
{"website": "Atlassian", "title": "From beta to GA: How to build a winning API", "author": ["tcrusson"], "link": "https://blog.developer.atlassian.com/how-to-build-winning-api/", "abstract": "When you think of an API, you probably don’t picture a bunch of drawings on a whiteboard. But exactly one year ago, our plans for HipChat Connect were just that. In a year this idea evolved through several stages, and finally made it to public GA with over 30 partners . We’ve learned a lot about shipping and partnership in the process, and we hope what we’ve learned will be helpful to other software development teams (like yours, maybe). Like many of you, we believe APIs are the gold standard of product and ecosystem development today. Having a platform that connects your product to a larger market can have huge benefits for your business, your partners who build on the platform, and your end customers. It’s a win-win-win. But with the sheer amount of players involved, developing an API is a unique and challenging experience. For context, a little bit about HipChat Connect . It’s an API that enables developers to write conversational apps for HipChat that are fully embedded in the chat window. You can add buttons, dialogs, forms, and interfaces, all using your own technology stack. It’s the first implementation of Atlassian Connect to be supported both in the cloud and server versions of the product, and works on the web, Mac, Windows and Linux HipChat Apps. Today, more than 35 HipChat Connect integrations are available in the Atlassian Marketplace , with more to come. Shipping an API is very different from shipping a product: once it’s out there, it’s out there. Forever. That said, doing things right should never be at the cost of speed. You need developers using your API early on in order to make it competitive. Shipping fast means cutting scope often, but it shouldn’t mean cutting corners. Whatever choices we made, we always decided against solutions that would hold us back later on. A few key decisions we took to help with this strategy: What we all love about cloud is that a cloud MVP can be much, much, leaner than a server-based MVP. You don’t have to live with the impact of your mistakes as long, you can run experiments and collect data to validate ideas, and you can decide the speed at which you release with techniques like feature flags. And, if you combine this with allowing each developer to release to production, with the unit of change being a pull request, it basically means you can test an idea in days instead of weeks. In our case, the first version of Connect in HipChat Cloud was out in 6 weeks, while the first version of Connect in HipChat Server took 12 months. To have any chance of shipping a public API in time, we knew we needed a lot of developers to use (and abuse) the API early on. There are several ways you can do this, including recruiting another team within your company or reaching out to developers from your favorite Meetup group. We started testing within our own company, and shipped an internal alpha to Atlassians to build on during our quarterly ShipIt hackathon. This first step gave us a ton of good feedback to mature the API. Having dogfooded internally, our next step was to take it to the streets, and by streets, we mean, an external Alpha for partners. Our short timeframe required outside-the-box thinking, which resulted in a Vendor Lab Week, where we invited 12 companies to our office for a week-long hackathon. This was one of the highlights of the project. Our office was buzzing with excitement and there were some great demos unveiled that week! It’s one thing to explain your vision to companies, but quite another when they’re selling it back to you. If this sort of event is within your realm of possibility, we highly recommend it. Launching an API without partners is meaningless, that much is clear. But what isn’t always clear is who the right partner would be. You can’t just team up with whoever is out there; it needs to be a mutually beneficial partnership. So, before spreading the word, sit down and define the objectives that will help you land the right partners. Then, do everything you can to win them over and serve their needs. Partners love it when you prioritize making them successful. One thing we realized very early on was that partners want guidance when it comes to building for you. After all, you know your products and challenges better than they do! We worked with our partners on the design and implementation of their add-on and it was a great experience for both parties. It forced us to think about the core value proposition of our product and company, as well as where third party apps could play a role. What helped us gain traction with a lot of partners was to paint a vision for the space they work in that showed we cared about their success. Here’s something everyone should know: shipping on a new platform doesn’t necessarily produce a sudden rush of users. This expectation can often result in partners falling into an add-on death loop, as illustrated by David Bland’s now famous tweet illustrating the “Product Death Cycle”: Realizing that continually piling features on top of an existing add-on was not going to cut it, we spent time discussing growth tactics with our partners. We advised them to make sure their integrations could be installed by anyone, not just an administrator. We encouraged them to delay asking for a sign up for an add-on until after it had been evaluated. And finally, we had them focus on onboarding, and making sure the user was given a proper tour of the add-on after installing in order to demonstrate its value. Your partner’s success may live and die by the permissions you set up on your API. While permissions may be set up with the best of intentions, you can potentially hamstring your partner’s integration if there’s too much red tape. For instance, if only a small subset of users can install integrations, and only in a specific context, how can a partner be expected to grow? Take the friction out of the process by keeping everything open by default, and offering the ability for admins to lock it down later if they want to. With APIs, you could keep building more features until the end of time! But first you have to make sure your API is put to good use. Here are some things to consider as you keep growing: Well there you have it; that’s our story. We’re excited about what our team has come together to build, and we’re looking forward to seeing other dev teams doing similarly great things in the world of APIs.", "date": "2016-06-06"},
{"website": "Atlassian", "title": "Tip of the Week: Hgwatchman to wrangle those huge Mercurial repos", "author": ["John Garcia"], "link": "https://blog.developer.atlassian.com/tip-of-the-week-hgwatchman/", "abstract": "Last week, we had an amazing 10th anniversary meetup for Mercurial ! Sean Farley invited Facebook ‘s Ryan McElroy told us about an exciting open source extension to Mercurial that the team at Facebook has been using for some time now. As with most commercial software endeavors, the code base at Facebook grows every day, and as a result specific operations can be time consuming. You can watch the talk here, or read my summary below. In response to this business need, Ryan and his crew invented the hgwatchman extension and released it on Bitbucket to the open-source community. If you have large latency with your repo operations in Mercurial, give it a spin. Instructions to build the extension are in the README; Ryan’s real-world observations of performance deltas are contained below. Hgwatchman isn’t just for status #happybdayhg @atlassiandev pic.twitter.com/0Q3ijyID86 — John Garcia (@bitbucketeer) May 7, 2015 Behind the scenes, hgwatchman is monitoring the filesystem for changes and keeping an index of those changes between operations, instead of stat on every file at runtime. It’s trivial to install, yet gives a performance boost to repos with large filesets. Check out the video above if you’d like know more. Let us know what you think in the comments! Watch out for news and info from our @AtlassianDev Twitter feed! Follow me at @bitbucketeer !", "date": "2015-05-18"},
{"website": "Atlassian", "title": "Common Dockerfile Mistakes", "author": ["Derek Chamorro"], "link": "https://blog.developer.atlassian.com/common-dockerfile-mistakes/", "abstract": "We live in a containerized world. As companies transition from monolithic builds to microservice architectures, we often overlook some common mistakes we make when we write our Dockerfiles. Most are simple mistakes, allowing a user to make use of build cache in a more pragmatic fashion. Others, should be avoided at all costs. The following are some common mistakes I’ve seen uploaded in the past and some ways to correct them. When Docker builds an image, it goes through each line (or instruction) of your Dockerfile. As each line is examined, Docker will look for an existing image in its cache that it can reuse, rather than creating a new (duplicate) image. So, as an example, with each occurrence of a RUN command in your Dockerfile, Docker will create and commit a new layer to the image and then commit it to disk. Any RUN command changes will update the build layer. If nothing is changed, Docker will use the cache of a previous build on the host for subsequent builds. Keep this in mind when using ADD or COPY commands. COPY will copy a file or a directory from your host to your image. ADD can do the same, but also has the ability of fetching remote URL’s, extracting TAR files, etc. As the range of functionality covered by ADD can be quite large, it is usually best to use COPY for copying files or directories into the build context with RUN instructions for downloading remote resources. Example: Running apt-get install is one of those things virtually every Debian-based Dockerfile will have. This is due to satiate some external package requirements in order to run your code. But, using apt-get as an example, comes with its fair share of gotchas: Example: apt-get upgrade . This will update all your packages to their latests versions, which can be bad because it prevents your Dockerfile from creating consistent, immutable builds. Example: Running apt-get update in a different line than running your apt-get install command. Running apt-get update as a single line entry will get cached by the build and won’t actually run every time you need to run apt-get install . Instead, make sure you run apt-get update in the same line with all the packages to ensure all are updated correctly. Many Dockerfiles use the FROM package:latest pattern at the top of their Dockerfiles to pull the latest image from a Docker registry. While simple, using the latest tag for an image means that your build can suddenly break if that image gets updated. This can lead to problems where everything builds fine locally (because your local cache thinks it is the latest) while a build server may fail (because something like Bitbucket Pipelines makes a clean pull on every build). Additionally, troubleshooting can prove to be difficult, since the maintainer of the Dockerfile didn’t actually make any changes. To prevent this, just make sure you use a specific tag of an image (example: alpine:3.3 ). This will ensure your Dockerfile remains immutable. ENVs should only be declared when you need them in your build process. If they are not needed during build time, then they should be at the end of your Dockerfile, along with EXPOSE . Below is an example of a Vault Dockerfile build. The ENV is declared for the release URL fetch required for the build and the EXPOSE is left at the end: Attempting to chain multiple images together by using multiple FROM statements will not work. Docker will only use the last declared FROM statement. Example: Running docker exec into that running container will net the following: Which leads to my next point…. Volumes in your image are added when you run your container, not when you build it. You should never interact with your declared volume in your build process as it should only be used when you run your container. Example: Creating a file in my build process and then running cat on the file once the image is run works fine: If I attempt to do the same thing for a file stored in a volume then it won’t work: Never store secrets (keys, certs, passwords, etc) in your actual image. It’s bad… like REALLY BAD . You could potentially store secrets encrypted in images, but then you still need a way of passing the decryption key, and you are unnecessarily giving an attacker something to work with. Secrets can be passed in environment variables, as it has been recommended in the 12 factor App , but there are caveats to this as well: Another used option is storing secrets in a shared volume: The problem with this solution is that you still keep your secrets in a file, which could potentially have many sets of eyes viewing it. One the better solutions is to use a key management system, like Vault or Keywhiz to keep secrets and retrieve them from the container at runtime. They can help you avoid an embarrassing situation . Most of us have grown accustomed to making Dockerfiles. By avoiding some simple mistakes, we can take advantage of the following: I hope these tips will help you as much as they’ve helped me and my team. And speaking of help, this post from the team at Runnable was really helpful as I was organizing my thoughts. Check it out if you’re keen to explore this topic further. Thanks for reading!", "date": "2016-06-30"},
{"website": "Atlassian", "title": "The HTML dialog element", "author": ["Travis Smith"], "link": "https://blog.developer.atlassian.com/html-dialog-element/", "abstract": "Did you know that there’s a dialog element in the proposed HTML 5.1 spec? Did you know that you can use this in Blink -based browsers natively or with a dialog polyfill for other browsers? I didn’t know this until just recently. Here’s what I’ve learned… The simplest example is just a page that opens and closes a modal dialog. To make that example happen, we need just a couple items in our page. A dialog element provides the content, and we are embedding a form with the method of “dialog” , which closes the dialog on submit instead of navigating to another page. A button’s click handler is setup to call [showModal]() on the dialog element, allowing the dialog to appear when desired. So that’s pretty cool! But that’s not all we can do. We can apply some styling to the backdrop of the dialog via the backdrop pseudo selector. This controls the ‘gray out’ effect when using a modal dialog. And that CSS in action produces a pretty red overlay to all the content. If you want to do this using the polyfill, you’ll need to adjust your selector slightly as the pseudo element doesn’t exist and instead the polyfill creates a new element to take the place of the ::backdrop . This is enough to get you started, but there’s a couple other things if you’re interested… You can call dialog.close() instead of using a form submit to dismiss the dialog. When you call close you may provide a return value that’s exposed as the returnValue property on the dialog object. This might be handy to ask for and return input in a modal, like [window.prompt] . The behaviour of ‘show’ and ‘showModal’ differs in display. And ‘show’ does not produce a ::backdrop pseudo element, covering over other content. If you open a modal dialog, the escape button will close the dialog. You can listen for this event and then use event.preventDefault() to disable this behaviour. dialog.oncancel = function(evt) { evt.preventDefault(); } will do that for you. You can make the dialog open on page load by adding the open attribute to the dialog element. This opens the dialog as if you used the show method on the element. Calling ‘show’ or ‘showModal’ on the dialog also changes this property. We can see what’s going on by registering a [MutationObserver] , which listens for changes to the DOM. The MutationObserver shows that the open value changed, but Chrome does not appear to fill in oldValue in this case. You still get notifications that it’s changing — the important part. Hitting open and close a couple times shows the following in Chrome. There are websites using dialog elements now , and you can too! Now you can replace all those app modal alert dialogs with page or tab modal dialog s! Thanks to @jasonkarns for bringing this to my attention. Without that, I wouldn’t have spent half a day poking around to bring you all the insight I learned. Follow me @TravisTheTechie on Twitter for more things about the web, LEGOs, and tech.", "date": "2015-04-15"},
{"website": "Atlassian", "title": "Open letter from an @ignored test", "author": ["Mauri Edo"], "link": "https://blog.developer.atlassian.com/open-letter-from-an-ignored-test/", "abstract": "We’ve all done it: a perfectly good test goes flakey, and instead of rolling up our sleeves right then and there, we slap an @ignore annotation on it. But do we ever take the test’s feelings into consideration? No. No, we do not. Here’s one @ignored test who thinks it’s high time we do. Dear developer, I've been wanting to talk to you for a while now, but words don't always come easy. We've had some really fun times together. I still remember the first time I warned you about a minor bug in your code, and how happy you were for having me in your life! Do you remember it? I also remember the first time you refactored me to make me more efficient and how well-written I felt afterwards… ah, great times! I owe you everything, I know. And I’m thankful for it. I wouldn’t exist if it weren't for you. You thought that I was needed so you created me, and from that moment on I am at your service, and I am glad to be, as you gave me a purpose. I want to catch bugs for you. I want to give you assurance that things will continue to work after your changes. I want to make your life easier, and you know I can do all those things, I know you do. But then, with no clear explanation, I started to fail sometimes, for no specific reason. Something broke a little inside of me. I was able to continue functioning almost normally, but I couldn't avoid causing red builds from time to time, it was simply out of my control. I became… flakey . My flakiness upset you, and I am not angry about that, as it upset me too. I was not reliable anymore. I lost my purpose. At this point, I have to say, it hurts me to remember how you reacted after some weeks of flakiness: instead of investing some love and dedicate a couple of hours to fix me and get me back to a good state, you annotated me as @ignore and abandoned me in an immense and desolate codebase. My statements and assertions can't help to shed a tear when I think of this. For an automated test, being flakey is bad – but at least I passed successfully from time to time, and my failures were a reminder that I needed some of your magic; but being ignored my friend… that is simply terrible. If there is a hell for automated tests, it definitely is being annotated as @ignore and forgotten, being surrounded by successful tests that go green and not being able to join them, watching builds pass by and not pick me up, sitting between infinite lines of code, hopelessly waiting, needing a fix and not being taken care of… I would never wish that even to my worst automated test enemies. Don't get me wrong, I understand that automated tests have a lifecycle, and eventually they get replaced by other automated tests, better and more modern. Sometimes our flakiness can't be resolved, so we need to be removed or replaced, and that's ok. Sometimes the code we are testing is simply retired, so we have no purpose anymore, and that's ok as well. It's part of who we are. But hey: I am code too, you know? I need attention! I need to be implemented and refactored properly to achieve my purpose! I need code reviews where you look at me with care and spot issues that I might have, because tests can have bugs too! It's simply unfair to only look after feature code and, when forgotten tests start to fail, annotate them as @ignore and continue your day as if nothing happened. It's outrageous! All I am asking is for you to make up your mind about me, either fix me or delete me, but do not forget about me! You break my statements when you do. Humans have issues with decisions, as we lines of code know, so if you need to get away with a green build and ignore me for a couple of runs, it's fine. Really! But if you are not going to come back immediately and find what's wrong with me and why have I been flakey recently, have some decency at least: raise an issue in your bug tracker, so that someone else can give me the attention I need to get back on track and provide some value again. It's not that hard, is it? Please? For all the green builds we've had together? I sincerely hope we can sort out our differences soon. Forever yours,", "date": "2015-05-12"},
{"website": "Atlassian", "title": "The secret life of ngrok", "author": ["Travis Smith"], "link": "https://blog.developer.atlassian.com/secure-localhost-tunnels-with-ngrok/", "abstract": "ngrok is a handy tool and service that allows you tunnel requests from the wide open Internet to your local machine when it’s behind a NAT or firewall. This is useful in a number of cases, such as when you want to test out an add-on you’ve been writing for HipChat or a custom webhook endpoint for Bitbucket , but you haven’t yet deployed your code to an Internet accessible host or PaaS. The most common usage of ngrok sets up a tunnel to localhost using the random hostname ngrok provides by default, e.g., 5a3e3614.ngrok.com. But that’s not all it can do… That works reasonably well, but perhaps you want to be able to reuse a hostname or use a specific hostname for the tunnel. As it turns out ngrok supports specifying the subdomain used , giving you a hostname of choice. This does require you to sign up for a ngrok account. It’s free, so I’d suggest signing up for ngrok to access this benefit plus the ability to password protect the tunnel, do non-http/https tunnels, and launch multiple tunnels at once. Awesome! ngrok also tracks requests through the tunnel. They are displayed in the console window you used to launch ngrok, but more importantly they can be seen if you visit the web console ngrok launches at http://127.0.0.1:4040/ . This console allows you to inspect the latest requests, look at the request (headers and content), the response (headers and content), and lets you replay requests. This is super helpful when you have API requests you are testing out and don’t want to bother doing whatever steps it takes to generate that request in your UI. There’s also a client REST API so you can query information about the ngrok tunnels active from your local machine. Need to set your app’s hostname? Hit the API to ask for the external hostname. By default ngrok forwards both http and https traffic to your local machine. This is great when testing out webhooks you’re building with services that require SSL and a valid, properly signed cert. This just touches the surface of the cool things you can do with ngrok. ngrok’s free tier is powerful enough for most use cases that I’ve come across in every day development. There’s also paid tiers for people teams that add custom domain names and reserved hostnames among other things. So keep ngrok in mind next time you’re working on an add-on for HipChat or one of our other cloud products . Follow me @TravisTheTechie on Twitter for more things about the web, LEGOs, and tech.", "date": "2015-05-07"},
{"website": "Atlassian", "title": "Triggering Deployment with Git Commands", "author": ["Ian Buchanan"], "link": "https://blog.developer.atlassian.com/git-based-deployment/", "abstract": "Deployment doesn’t get any simpler than just getting the latest versions of some files up to the production server. While rsync, sftp, and scp have long been the tools of the trade for such simple deployments, these approaches have their warts. Even if it is easy to recover, an remote copy that fails in the middle may leave a web site in an incoherent state. If you are already using Git to manage the files as source code, then you may benefit from using Git’s native ability to distribute versions of files. While this idea isn’t all that new, there is a new feature of Git that makes this much easier than in past. Read on to learn when Git-based deployments are appropriate and how you can use Git to deploy files. Here are some examples where deployment is as simple as making a copy on the production server: This kind of simple deployment makes most sense for web sites on a virtual private server (VPS), where the environment is controlled by a hosting provider and resources like memory are limited. Tools like Capistrano or Fabric may be overkill and/or inappropriate for the environment. The one caveat is that you may have to install Git into your VPS yourself (as I did with my provider). The main advantage of Git for deployment is its transactional nature. Consider a website where changes to the structure, content, and style. As the files are copied to production, a visitor might get a new page that has a link to an updated page that hasn’t yet been copied to production yet. While the copy is in progress, the partial changes are incoherent. To compensate for this problem, there is a notion of Blue Green Deployment . While Blue Green Deployment is essential when changes are made to multiple servers, it seems overkill for the kind of simple case that we’re considering. In contrast to ftp and the like, Git will first send all of the changes to the production repository, then it can quickly apply all of the changes. If, for some reason it can’t, Git will automatically roll-back. That’s just a basic capability of Git so no special compensation is necessary. Newer PaaS providers like Heroku and Azure offer push-to-deploy as the default deployment model. The deployment command is simple: Where remote-server is a Git repository living on the production server. If the capability isn’t built into your environment, you can set up Git on a VPS yourself. However, those instructions (and dozens of others) were written before Git 2.3 , when a small but significant feature was introduced. Prior to Git 2.3, Git refused to modify a branch that is currently checked out. There were many work-arounds: detached work tree , dual repositories , post-receive hooks , and even specialized tooling . Admittedly, these approaches cover other things. For example, some of these restart services upon configuration or code changes. Some also prevent the web server from sharing the .git directory but that can also be achieved by configuring the web server . Now, with Git 2.3, the following configures Git to override the normal behavior, making Git perform a git reset --hard after a push so that it updates the current branch. A common pattern of Git usage is to have a development branch with cutting-edge changes and a master branch that is always kept consistent with production. With this pattern, it is easy to use Git to see what code is currently in production. This pattern depends on automatically pushing the master branch to production so there is never any question that master means production. With a Git-hosting service like Atlassian’s Bitbucket , the Git automation is a simple matter of configuring a post-push webhook. The trick is that webhooks need to be translated to a local Git command on the target. For reference, I created a simple PHP script to adapt the webhook to a git pull . Although the example cases are simple, that is hardly an excuse to blindly deploy code into production. There are appropriate tools for automatically checking web sites and configuration files so use them. These Git-based deployment techniques should come after those checks, whether done by hand on a developing branch, or automatically by a continuous integration server like Bamboo . Even if your continuous integration server makes an automatic decision to deploy, the advantages of the Git-based approach remain.", "date": "2015-04-10"},
{"website": "Atlassian", "title": "Quick Tip: Getting Emacs and IntelliJ to play together", "author": ["Steve Smith"], "link": "https://blog.developer.atlassian.com/emacs-intellij/", "abstract": "As I’ve mentioned before , I’m gradually working towards my grey-beard badge so for most of my programming I tend to use Emacs. However when I moved into the order-systems team I adopted IntelliJ IDEA , which is our weapon of choice for Java development at Atlassian. This is because while Emacs is a great text editor, IntelliJ takes a holistic and semantic view of your project, something that is necessary with Java’s verbosity and file-based classes. In particular, its on-the-fly tracking of the project syntax tree enables complex refactoring and clean-ups, either automated or by the more brute method of just changing something and seeing what turns red in the editor. But like western musical notation enables complex harmonic structures at the cost of rhythmic structure , IntelliJ’s structured refactoring come at the cost of a really powerful text editor. Sure, IntelliJ has keybindings to match Emacs and Vim, but those editors have other features that enable complex text processing patterns. And sometimes you need to get down and dirty and hack on some text, be it mangling CSV or conforming to some baroque copyright header formatting requirements. In particular, Emacs’ keyboard-macros have helped me turn some annoying data and code transformation problems into a few key-presses in the past, as this screencast from Avdi Grimm shows: Right, enough rationalisation, let’s get on with it. What I want to do is do my coding in IntelliJ on a day-to-day basis, but immediately load a file into Emacs for any tricky text processing I want to do. This is how you do it… Rather than constantly starting up Emacs every time we want to use it, we’re going to keep an Emacs session running in the background and just tell it to load whatever file we want; most Emacs users tend to have a session open for months anyway. So start up an Emacs session if necessary and then tell it to listen for instructions from emacsclient . This is just a case of invoking M-x server-start in an existing Emacs window. IntelliJ has a feature called external tools that allows it to invoke external commands with some pre-defined variables, such as the current file path. To use this to send files to Emacs go to Preferences->External Tools . From there click the plus button to add a new tool. Then add the following details: /Users/ssmith/Applications/Emacs.app/Contents/MacOS/bin/emacsclient . Your new tool should now be available in the Tools main menu and under the context (right-click) menu in the tab for any open files. However we can do better than that and add a keyboard shortcut. To do this go to Preferences->Keymap->External Tools and double-click on your new tool. This will pull up a dialog allowing you to add a new keyboard-shortcut; I use Ctrl-Shift-O under Linux and Cmd-Shift-O under OS X, but whatever works for you. Now you can just invoke your keyboard shortcut in any file you’re working on and it will immediately load in Emacs. Once you’re done just save the Emacs buffer and switch to IntelliJ and it will pick up the changes. This is probably enough for most people, however there are a few more features we can add for the programmer who likes to tweak things (and if you’re using Emacs that’s almost certainly you). In particular, as Emacs is immensely programmable via its built-in Lisp engine we can override a few settings in the loaded buffer to better work with the already open file in IntelliJ. Some things we’d like to do: To do this we need to invoke Emacs lisp from emacsclient . While this is possible in the external tools dialog it would be messy, so we’re going to write a quick wrapper to make this neater… I use ~/bin/openinemacs but you can put it anywhere. You should make this file executable with chmod +x <YOURFILE> . The contents of the file should look like: (It would be nice to define this in a .el file and just invoke it or even put emacsclient on the shebang line, but emacsclient has no way of evaluating elisp from a file.) This is much the same as the previous version, except that we invoke the wrapper instead and pass it some extra parameters: That’s it. Of course, there are probably many more tweaks that could be performed in the evaluated lisp; feel free to add suggestions in the comments. Unfortunately I don’t know enough Vim magic to come up with a similar recipe for its users, but I’m sure it’s possible. Again, feel free to post tips in the comments below.", "date": "2015-03-31"},
{"website": "Atlassian", "title": "Stash plugin tutorial: tag list plugin", "author": ["Tim Pettersen"], "link": "https://blog.developer.atlassian.com/tag-list-stash-plugin-tutorial/", "abstract": "It’s that time of year again! Time for Atlassian’s San Francisco office to be insanely jealous of our counterparts down under. Specifically, we’re jealous of the Sydney office’s graduate program: the Atlassian HackHouse . For comparison, here’s what the San Francisco office looks like during Australia’s summer: However, one awesome thing did shake out of this year’s HackHouse that benefits everybody, no matter which hemisphere you live in! A member of the Stash development team wrote a neat Stash plugin tutorial for our grads to hack on. Presumably between mojitos. The Stash Tag List Plugin is a self-contained Bitbucket repository that iteratively teaches you how to build a fully functional Stash plugin. In five simple steps you’ll build a properly styled repository view that displays a list of tags in a Stash repository. The tutorial teaches you a range of useful plugin development skills, including: To get started, you’ll need a recent JDK and the Atlassian SDK installed. Then you can simply clone the repository from Bitbucket and start from the README.md! If you’re more of a visual learner, you can also check out this screencast that teaches you how to build, deploy, and debug a Stash plugin from scratch in 30 minutes. If you have any questions about Stash plugin development, leave a comment below or drop me a line on Twitter (I’m @kannonboy ).", "date": "2015-03-26"},
{"website": "Atlassian", "title": "So you want to speak at a conference…", "author": ["John Garcia"], "link": "https://blog.developer.atlassian.com/so-you-want-to-speak-at-a-conference/", "abstract": "Speaking at a conference is an important opportunity! Getting selected for a session, then writing and delivering a compelling presentation, can be a daunting and nerve-wracking task. I’ve collected my insights to help rookie Advocates understand the process of securing an opportunity and delivering a quality talk. Presentations at conferences are a key chance to build your portfolio as a developer, get review for your research or product ideas, foment excitement for a new product or service offering, and have a great time meeting other technical folks. Read on for insights into my experience as a speaker at this year’s Git Merge in Paris. #Atlassian loves git. #gitmerge pic.twitter.com/EYdrcVGiR0 — Twidi (@twidi) April 9, 2015 It all started with Nicola spotting a thread about a potential 10th Anniversary party in the Git mailing list. Grace , the head of my department, initiated discussions with the contributor and the Linux Foundation to brainstorm about the best way to celebrate. Through these discussions and the Linux Foundation, we were able to get together with the Git Merge organizer over a friendly lunch so we could sell him on the value of a presentation from our team. We identified a topic of general interest to the Git community that Atlassian had done recent research into, and agreed to submit a proposal. Writing the proposal was straightforward; a few hours of research on the internal project and a bit of word-craft to make it sound exciting were applied to the task. I wrote a two paragraph summary of the idea, and my team (h/t Tim ) helped fill it out into a proper pitch. We submitted as soon as we could turn it around. I found out that a winning submission will have several hundred words that make the value of your talk as clear as possible: the problem you intend to solve, the value of your solution to the audience, and the overall message of your talk. Once we had been approved for a session, I set out to build a presentation. Researching the topics, I built an outline in Confluence to store my ideas and keep them organized, and then created my Keynote deck based on that page. I found out later in the review stage that I had organized the presentation with an exposition frame rather than a story frame; this caused some significant re-work which could have been prevented by paying closer attention to the overall goal of the talk. I spent a fair amount of time recording a demo video, which turned out to be a great way to show off the project while being able to talk coherently; this also eliminated the chance that my equipment, or the equipment at the convention hall, would fail and leave me stranded. I was fortunate to be able to arrange several hours of public speaking training from one of San Francisco’s premier voice coaches, which was a critical part of our strategy to present a compelling session in Paris. In addition to this time, I logged upwards of 40 hours practicing the presentation in front of team mates, co-workers, other teams, developers, stakeholders, and even a tech writer and a data scientist for good measure. I even practiced in front of my technically-inclined wife, my non-technical personal friends, and a mirror. Because I didn’t initially have a very strong command of the subject matter, I needed a lot of feedback to get the presentation right! The best way to make sure I get reliable feedback, in my opinion, is to invite as many people as I can, with as diverse of a skill set as I can find. For me, the most valuable input beyond that of the extensive Advocacy experience of Grace came from Alison , the Head of Product Marketing and our Technical Writer, Dan. The method you use to take in feedback and synthesize it into your deck is your own to choose; however, I assure you that when it comes to reviewers, more is definitely better. Working with so many reviewers helped me to become more confident and comfortable with the subject matter. Once is an accident. Twice is coincidence… @bitbucketeer ‘s talk was the 4th one at #gitmerge about git troubles with large repos/files. — Vadim Zeitlin (@_VZ) April 9, 2015 Details about the first day of the event can be found at Nicola’s excellent synopsis Notes from Git Contributors’ Summit . The Git Merge organizers had a full agenda for the day, including a Walking Tour of Paris and a Speakers’ Dinner where the lineup of presenters were afforded a chance to get to know one another over wine. We had some great discussion, especially because so many of our talks had such similar topics. Crucially, this meeting gave me a chance to compare notes with a presenter who would be giving a very similar talk. This helped us avoid confusing the audience because we took the time to synchronize our messages. I brought a rather unfortunate head cold with me from San Francisco that had me sniffling and coughing throughout the plane ride and my time in Paris. Apologies to anyone who I transmitted it to! On the night before my talk, it’s critical that I get adequate sleep to function. I like to get just as much as I can manage, up to eight hours; this helps me stay alert and avoid excess consumption of caffeine on the morning of the event. In my case, the jetlag bug bit and I got a paltry three hours of sleep. That morning, I had a bit of coffee but then switched to orange juice for a more natural alertness. First thing after arriving at the convention and debugging the Null Coffee Exception in my brain, I made a point to run around and get all of the essential details of how and when I would be expected to go on stage, to ensure there was no confusion when I was called. I had a few last minute edits to make – there’s always last minute edits – and I made sure to practice the whole talk with Nicola and Steve , who were in town. To get this done, we retreated to Nicola’s hotel room and rehearsed everything during the lunch break. @bitbucketeer is tearing it up at #gitmerge pic.twitter.com/Jsa7KvepAb — Steve Smith (@tarkasteve) April 9, 2015 Thankfully, the organizers provided the crowd with a fifteen minute recess between talks, and this allowed time for me and the mic to become friendly with each other. It also gave me some time to test out the stage, which is a great way to get comfortable with speaking on it. I walked the length and breadth of the stage, testing out my remote control from all four corners. Confident it would work reliably throughout the presentation and that I had a good understanding of the space, I waited to be called up by Scott Chacon , the master of ceremonies for the event. Notably, my sniffles and stuffy nose symptoms cleared up as my excitement to be on stage grew; this is a normal reaction to that type of stress and one of the many benefits to it. Once I was called, I made my presentation. Even though I was nervous, the hours of preparation really paid off – it was a great talk! I presented the results of our research clearly and competently, and kept most everyone entertained the whole time. The resulting Tweets were very appreciative! I scanned the crowd while on stage, and while there were a few drowsy heads out of the three hundred or so developers in attendance, I felt like the crowd was engaged and excited to hear my story! After the talk, there were some great, encouraging words from many attendees as well as some very intriguing questions. I’ll admit I did forget to read some of the anecdotes I had prepared, but that’s okay. Great talk at #gitmerge from the awesome @bitbucketeer John Garcia pic.twitter.com/PWCYa84jNX — Dirk Lehmann (@doergn) April 9, 2015 Our hosts threw an amazing afterparty at a former crate factory, with amazing food and beverage on hand. I had a great time talking with guests and our hosts, swapping stories, and generally having a grand time. I had a bit of star power, and that makes for a fun party! Reports from my team in San Francisco were enthusiastic, so I’m happy to say it was a great result! A huge thank-you to GitHub for presiding over a most excellent convention and birthday party! Watch out for news and info from our @AtlassianDev Twitter feed! Follow me at @bitbucketeer !", "date": "2015-04-30"},
{"website": "Atlassian", "title": "I like to keep my Promises (when I Javascript)", "author": ["Travis Smith"], "link": "https://blog.developer.atlassian.com/i-keep-my-promises/", "abstract": "Promises are now here with ES6 in Node/iojs, Chrome, FireFox, and Safari . Easy polyfills exist for the browsers that don’t yet support A+ promises . So how would you use them? Promises arose from the need to handle nested callbacks and compose actions. Ever try to $.ajax in a success handler of a $.ajax — you end up half of the screen width taken up with just indentation! Or say you need the results of multiple async operations to finish computing something. You can collect all the operations into one promise and act upon the result instead of having to keep track of that yourself. Let’s work on an example of building a Promise API from an existing async API. The fs.readFile API in Node.js is an excellent example of a callback based API. I’m sure that code has a familiar ring to it — but what if we wanted to compose many file reads together? The easiest thing to do is turn this into a Promise based API, like… And we can exercise this API… You can chain promises together. Perhaps we need to query a config to find a file’s location before reading it. And the best part, you only need to write one error handler – an error in any part of the chain will end up executing the error handler. Promise.catch allows you to compose the code that leaves error handlers explicit. What if we just needed two files loaded? We can collect the promises together into a single promise. Sure, I’ll show you something real. Consider the case where you’re working on a Connect add-on for JIRA. Then you are using AP.request to load something about a project from JIRA’s REST API . But you don’t know the project key pragmatically – you just know it’s the first project. First we’d need to promisify (if it isn’t a word, it will be shortly) AP.request … And we can use a function like that to compose a chain of events together… Promises are now in Nodejs, and will be in all browsers real soon. There’s no reason for you not to use them if you see the value they provide. Go forth and Promise! Thanks for reading and let us know what you think at @atlassiandev or at @travisthetechie .", "date": "2015-03-25"},
{"website": "Atlassian", "title": "Notes from Git Contributor Summit (Git Merge 2015)", "author": ["Nicola Paolucci"], "link": "https://blog.developer.atlassian.com/git-merge-2015-wrap/", "abstract": "Thanks to the flawless organisation of the GitHub team, Git Merge 2015 happened last week in Paris. It hosted the Git Core Contributor Summit on the first day, one day of Git talks and the celebration of Git’s 10th anniversary on the second. I felt very humbled and fortunate to participate in the first day and to witness real discussions amongst the core contributors. Here’s a few notes I took during the discussions of the core team. Junio C. Hamano – chief maintainer of Git – gave the opening presentation. He went through the entire history of the Git repository, slicing it and dicing it to highlight different contributors. In his words not all commits are of equal value, you can’t just count the number of commits or count the lines of code to see who contributed more to the project. He used all sorts of cool commands to find interesting stats and all the names of people doing important contributions. For example translators did generally few commits but wrote many many lines of very valuable content. Then we went on asking: how valuable is a contribution that gets superseded later by a different implementation? This tells us commit counts don’t mean that much. Also people who borrowed code from other code bases have been instrumental and valuable, even though they have few commits. He praised the contributors who wrote the manual. He also noted that line counts skew results towards finishing touches. What if someone had a brilliant idea that got shadowed by many refinements and now appears as though someone else wrote that part? And finally he answered the question: Have we gotten rid of the original Git? Apparently only 0.2% of the original Git source code is left at the tip of Git’s master branch. To get to this stat he ran blame backwards and read the result in reverse. If my notes are right 195 files from early version are still in Git’s code base today. The most stable and untouched part of the code base is the index concept which is in the cache source file. Important contributors are also bug reporters, feature wishers, reviewers and mentors, alternative implementors, trainers and evangelists. He also praised the new initiative of creating a community owned newsletter “ Git Rev News ” to summarise the work of the mailing list and share the most relevant article and links from the Git sphere amongst enthusiasts and users alike. Jeff King gave a talk on Software Freedom Conservancy and the governance and income of the Git project. Right now there is no formal governance structure but he asked if the community felt a committee or a more formal organisation was needed. It was felt the current structure is working well. His talk and notes are here . The rest of Contributor’s Summit followed an unconference style with people bringing up topics they wanted to discuss amongst the contributors. Next up Jeff King and Ævar Arnfjörð Bjarmason lead a deep discussion on “Why is Git slow?” and what can be done do about it? Repositories can be very large in the number of bytes, but that can come from having large blobs. But once repositories have a large number of commits and trees – like Mozilla’s repository or FreeBSD ports’ tree – performance degrades sensibly. The latter has a tree with 50k entries. Then there are repositories that are not big but have a lot of refs which perform very poorly. It was reported that some people using Gerrit have 300k refs. Jeff said that at GitHub they have 10s of millions of refs as ones are created each time a repository is forked because of the way the organize their back-end. Ævar reported that Booking.com has 600k commits with 1.5Gb of text files and they used to have 150k refs. The latter number was caused by deployments creating tags. The immediate issue they had was solved by having a sliding window maintaining only 5k refs for tags. git pull was taking 15 seconds due to the number of refs. A pull would take 15s and under a second was in the actual data transfer. It’s really hard to tell users not to do something. Reading refs is slow. The contributors debated ways to solve the issue: Then they made a consolidated list of things that are slow in Git: Interesting topic came up next: the maintainers love discussing patches in public in the mailing list but several people lamented that it’s hard for newbies to learn to setup Git to send patches to the mailing list. A tool called patchwork was mentioned but there seemed to be consensus in building a web frontend, an unidirectional solution that would save new contributors from configuring the email sending process to contribute proper patches to the mailing list. Next up was a conversation on the topic of how to engage and nurture newbies and general Git users in the Git core mailing list. The group was generally happy to engage with users but it was made clear that the core mailing list is chiefly to discuss Git development, not primarily a help channel. The libgit2 team reported they use a StackOverflow tag to great effect and state clearly on their Wiki that they want people to ask questions on StackOverflow and it has been working for them. There is also a git-users mailing list on google groups which is quite active and is a better suited environment to ask beginner questions. Next Stefan Beller described his proposal to start work on the version 2 network protocol. Version one was not designed with a world of so many refs in mind. The core of the discussion was on how to bring the negotiation of the capabilities to the front of the protocol. The first line of the protocol should only be about compatibility. What needs changing for version 2? They want to design a protocol that is extensible in the future. The protocol is already extensible via capabilities but right now you can’t change the initial exchange. What they want is a way to do a negotiation at the start. Capability negotiation is the first topic and they will only cover static capabilities because they can cache the capabilities with a process like “last time I talked to you you had these capabilities now we can keep the same protocol”. What is a v2 protocol minimal change look like? They agreed that first point is to let the client speak first. The first stab could be send the capabilities first. Dynamic capabilities like nonces for signed pushes would break caching these initial capabilities advertisements. Carlos Martin brought up a proposal for a streaming objects format. For example to stream a blob out of a database can be problematic for delta-ed streams as they cannot zlib compress them. This is needed by the libgit2 team some of their backends store into databases. Johan Herland from Cisco has been working on a new prototype tool, that could improve what git subtree does but allows for remote fetches. He said that submodules and novice users don’t go along and as they are in their current state I agree and have written about it before . It was fantastic for me to pick the brains of the varied group of contributors. I enjoyed talking with Roberto Tyley author of bfg and agit , two of libgit2 and libgit sharp maintainers Jeff Hostetler , Ed thomson , GitHubber Brendan Forster and many others. It was also great to connect with GitMinutes’s host Thomas Ferris Nicolaisen and Christian Couder who invited me to help out on the grand plans for Git Rev News effort. If you haven’t read it yet edition 2 was just published! All in all it was incredibly inspiring for me to be part of the conversation with the diverse and very competent mix of Git contributors. I feel proud of being part of this community in the supporting role of trainer and evangelist ;). Stay tuned for the videos of the day of talks, all sessions were of real technical depth and hard earned scaling lessons. Thanks for reading and let us know what you think at @atlassiandev or at @durdn .", "date": "2015-04-17"},
{"website": "Atlassian", "title": "Tip of the week: when did that get in our repo?", "author": ["John Garcia"], "link": "https://blog.developer.atlassian.com/totw-when-did-that-get-in-our-repo/", "abstract": "As our repositories grow large and complicated, it can seem impossible to find when specific strings of text were introduced to the repository. While the git blame command does great for showing you the most recent modification of the line, how do we find the earliest , especially considering that line numbers shift as commits and merges happen? From our excellent Technical Account Manager Tim Wong comes this week’s tip, the solution to that question. In his words: “A common thing I want to know is, when did version x.y.z of some library enter the codebase? The only way I have found to do this is:” Note: –pickaxe-regex is an option that allows us to use POSIX regular expression matching, instead of the default string matching we would expect from -S. More info can be found at the associated commit in the Git project. Let us know in the comments if you’ve found an easier way! Follow me on Twitter for updates about Atlassian software, robotics, Maker art, and software philosophy at @bitbucketeer .", "date": "2015-03-23"},
{"website": "Atlassian", "title": "Welcome New Marketplace Partner Program Partners", "author": ["Justin Lau"], "link": "https://blog.developer.atlassian.com/welcome-new-marketplace-partner-program-partners/", "abstract": "Happy New Year! We hope you all had a safe and restful holiday season. 2020 was a unique year full of unforeseen events; however, as we enter the new year of 2021, we're optimistic for the light at the end of the tunnel for COVID-19 and continuing to grow and strengthen our partnership with our diverse ecosystem of developers and Marketplace Partners. In July 2020, we launched the Marketplace Partner Program to emphasize growth in cloud and Data Center and up-level security practices. Today, as part of the 6-month requirements refresh cycle , we are excited to announce the Marketplace Partner Program's newest members. Bolded are Marketplace Partners who are new to their respective tier. Congratulations! Justin drives the strategy & management of a portfolio of go-to-market & partner research programs & incentives for the Atlassian Marketplace.", "date": "2021-01-14"},
{"website": "Atlassian", "title": "Creating a Jira Cloud issue in a single REST Call", "author": ["Peter Van de Voorde"], "link": "https://blog.developer.atlassian.com/creating-a-jira-cloud-issue-in-a-single-rest-call/", "abstract": "1 call is all it takes to create an issue through the Jira Cloud REST API. But before you can make that call, there are a couple of steps you need to take. To be able to call the REST API endpoint you'll need to authenticate yourself, one way to do this is through using Basic Auth with an API token . That is how we'll do it in this example because we want to keep it simple. I do recommend using OAuth for anything bigger than a single REST call though. To create an API token you can simply go to your Atlassian account and create a new API token. TIP: be sure to copy your API token on creation, you will not be able to retrieve it afterwards. Now that you have an API token, you can then create a basic auth header.  This is needed for any REST API call that requires your account be logged in. Complete steps on this part are in https://developer.atlassian.com/cloud/jira/platform/basic-auth-for-rest-apis/#supply-basic-auth-headers We'll be creating a simple curl command to call our REST API endpoint. To create an issue we need to call the /rest/api/3/issue endpoint on your Jira Cloud instance. Take a look below to see how we need to structure to curl call: Let's go over this one line at a time: Now let's deepdive into the data content, here is an example: This will create an issue in the project with project key TEST with a summary of \"Summit 2019 is awesome!\" and a description of \"This is the description.\" We will also need to define the issue type based on the issue type id. You can find more information about how to find issue type ids further down in this blogpost. Sometimes you will need to know all the required fields for to create an issue of a certain type in a given project. At the bottom of this blog post you can find a tip on finding all fields on the issue create screen, including whether or not they are required. Be sure to check it out. If you want a full insight in all that's possible with this call take a look at our reference documentation . Now we are ready for the final step. Now you simply need to execute your curl command, and if everything was done well, you should get this kind of result back: This gives you the issue id, the issue key, and a REST API endpoint for the issue you've just created. Or you can just go and have a look in your Jira instance: Congratulations you've just created your first issue through the Jira Cloud REST API! Let me give you a couple of tips and tricks that might help you along. The easiest way to find the issue type ids is by calling the issuetype REST API endpoint . Like this: This will return a full list of all issue types, and their ids, for you to go through and select the one you need. The easiest way to find the project keys is by calling the project search REST API endpoint . Like this: This will return a full list of all projects, and their keys, for you to go through and select the one you need. Sometimes you want to figure out which fields are required or available during the creation of an issue. The easiest way to find this information is by (yet again) calling a REST API endpoint. Namely the issue create metadata one. Like this: This will return a full list of all projects, the available issue types in each project, and if requested it will return the create screen fields (both required and optional fields) for the selected issue type in the specified project. I hope this blog post helped you create your first Jira Cloud issue through the REST API and will get you started on building more interesting integrations with Jira Cloud. If you have any feedback or questions feel free to let us know on the Atlassian Developer Community ! Or reach out to us on twitter @pvdevoor or @atlassiandev.", "date": "2019-04-10"},
{"website": "Atlassian", "title": "End of new server sales: resources and support for the ecosystem", "author": ["Katrina Morales"], "link": "https://blog.developer.atlassian.com/end-of-new-server-sales-resources-and-support-for-the-ecosystem/", "abstract": "Hi developer community, As we recently announced , we're committed to delivering the best possible cloud experience for every single one of our customers. Because we believe that cloud will be the primary destination for the majority of our customers, we're simplifying our self-managed offerings. Atlassian will stop selling new server licenses effective February 2, 2021 (PT). Server customers can continue to purchase new server app licenses until February 2023, and purchase and receive maintenance until Feb 2024. While we are not changing our external facing 30-day refund policy for new server app customers, we do expect there to be a small increase in refund approvals that fall outside this policy timing. If a customer requests a refund under the following conditions, your monthly remittance will not be reduced by such refunds. Conditions for no remittance change to Marketplace Partners (all must be met): In simple terms – because it's out of the usual refund policy you agreed to as part of the Marketplace Partner Agreement, Atlassian will cover the cost, and ensure you are not out of pocket for such refunds. We know this is a big decision, and it will likely impact the future of the apps you have built. To that end, we are doing everything we can to help you through this transition. Below are some of the resources that are available right now, and more are coming. First, here is some context on this decision as discussed by Co-CEO Mike Cannon-Brookes, CTO Sri Viswanath, Head of Ecosystem Martin Suntinger, and Head of Marketplace Programs Warren Chen: You'll probably have questions about timelines, cloud product extensibility, and more. We've got that covered in this guide on developer.atlassian.com . More content will be coming to this section over time. For our Marketplace Partners with paid via Atlassian apps, we have put together a resource hub in the Partner Portal . These resources go into more specifics about the business implications of this change. (Access to the Partner Portal is an exclusive benefit of the Marketplace Partner Program .) After reviewing these resources, we ask all developers with access to the Partner Portal to please leave any outstanding business-related questions or comments on the resources in the Partner Portal , not in the online developer community. If you don't have access to the Partner Portal (i.e. you don't have paid via Atlassian apps), or your question is technical in nature (e.g. related to how to build cloud apps), you can post it in the online developer community . Please use the tag server-eos on your post to help us triage it faster. We hope these resources are helpful to you in the short term. We know there is a lot more that you will need to be successful in the long term, and we are working on it. For example, we are working on a program similar to the Data Center Apps Program for supporting apps in large enterprise cloud instances of our products. Stay tuned for more information on this. Here's to building the future in the cloud for all our mutual customers! Katrina is the Developer Community Manager for Atlassian's ecosystem.", "date": "2020-10-16"},
{"website": "Atlassian", "title": "Integrating DevOps into Automation for Jira", "author": ["Mo Beigi"], "link": "https://blog.developer.atlassian.com/integrating-devops-into-automation-for-jira/", "abstract": "In this post, I’m going to be giving you a brief overview of the great work the Cold Fusion team at Atlassian has been doing over the past 9 months on integrating DevOps into Automation for Jira. Automation for Jira (AFJ) is a simple and powerful feature used to automate Jira. It was created by Code Barrel which was acquired by Atlassian in October 2019. Post-acquisition, the app was made available natively in Jira Cloud. AFJ allows users to automate their Jira projects with a lot of flexibility. A handful of triggers are used to trigger certain actions that operate within your Jira board. For example, you can automatically transition, comment, or create issues based on some trigger. Atlassian realised the potential in extending this automation framework to add DevOps automation. The ability to action your Jira issues based on development triggers was a highly sought after feature from customers. At the very least users would be able to automatically transition their issues from the backlog → in progress → in review → done , all without ever manually transitioning an issue. No more “Hi team, has everybody updated their issues on the board” prompt from the team lead each standup. However, this is only scratching the surface! Thankfully, I was fortunate enough to be a part of the awesome Cold Fusion team that has worked hard over the past 9 months to make this desire a reality. We have added triggers for branches , commits , pull requests, builds and deployments to AFJ with support for all major git providers (including but not limited to Bitbucket, Github and Gitlab). While my team was working on adding DevOps integration to AFJ. The core AFJ team was working hard to make the app available natively. This was quite an interesting process as AFJ’s infrastructure lived in its own AWS account isolated from the Atlassian AWS account. The main challenge in this project was effectively and reliably delivering development events from git providers to the AFJ system. Once the deployment messages reached the AFJ system, they would be wired to fire the corresponding trigger. For example, a commit created message would travel from the git provider through our system and finally reach AFJ which would trigger all rules that utilised the commit created trigger. Luckily, there was some existing infrastructure that could be reused. We already have development information being sent from git providers like Bitbucket to Jira. A Jira issue reference is the way in which we link development information to a Jira issue. There is no easy way to automatically determine which development action (i.e. commit ) is linked to which Jira issue without a reference. The reference must exist in the name of the development action (branch name, commit name, pull request title etc). An example for a branch name might be PROJ-17-fix-null-pointer-exception which would cause the issue PROJ-17 to be linked (if it existed). Various pieces of metadata in the development events (i.e. the branch name in a branch creation event or destination repository name in a pull request merged event) were exposed to users using AFJ’s Smart Values feature. It was important the team carefully crafted the smart values API so that we could support it going forward without making breaking changes. Furthermore, we had to ensure the API was git provider agnostic and did not rely on any special terminology or functionality offered by a specific git provider. You can find documentation on DevOps/development smart values here . This is the final infrastructure diagram that was designed by the team. In this design, development events flow from the git provider through various internal services before finally landing onto Streamhub (an internal event bus). From here, events are consumed by an SNS topic. At this stage, the next step takes us outside the Atlassian AWS account and into the AFJ AWS account. SNS then pushes the events to an SQS subscriber. This SQS queue is polled by an event reader and stored in an event queue which is then processed by the AFJ engine. There are a few sources for the development events that we use. Our goal here is to simply push the incoming development events to Streamhub (internal event bus) so that they can be processed later. DSS is a constellation of microservices that is responsible for fetching information related to Jira tickets for a Jira project which has been integrated with a Bitbucket repository. Consider the development status panel which is shown on the issue view. This panel exposes development status information that has been linked to the issue via a Jira issue reference. In this example, we are using Bitbucket data retrieved via DSS. We utilised DSS to push branch, commit and pull request Bitbucket events to Streamhub. JSWDD is responsible for validating, transforming, storing and retrieving entity data associated with Jira issues. We utilised JSWDD to push build and deployment events to Streamhub. DevInfo allows us to enable third parties to push their development information to us. We utilise DevInfo to process development information originating from third-party git providers like Github and Gitlab. Once the development event is on Streamhub, we know it will be shortly processed. Streamhub is essentially a black box to us that takes in an event matching some predefined schema and pushes it to various consumers. In our case, the consumer was an SNS topic. It may seem odd that the SNS topic only has a single SQS consumer but keep in mind the SQS consumer was living in an entirely different AWS account. This provides us with some flexibility on the off chance that we need additional consumers in the future. Once the event had entered the SQS queue in the AFJ ecosystem, it would be polled by an Event Reader and stored on an Event queue for processing by the AFJ engine. We will ignore the AFJ internal workings for the purposes of this blog post. One concern for the team was scaling our solution as the number of users inevitably grew as we rolled out to customers. Streamhub is quite a mature service at Atlassian and we were confident we would not put any stress on it that it couldn’t handle as the goto event bus used across the company. We treated Streamhub as a black box and worked on ensuring that the rest of the pipeline on either side of Streamhub scaled well. Another optimisation that was made was to add a DynamoDB filter in our pipeline. We cached a list of enabled rules per tenant and filtered out events that would not result in an automation execution. In other words, we filtered out events for customers with no enabled DevOps rules. This allowed us to significantly reduce the amount of stress on the AFJ engine and RDS database. It was very important to the team that we had sufficient metrics and analytics in place as we rolled out these features to customers. For metrics, we wanted to measure reliability and correctness across each stage of the pipeline. As a result, our metrics were largely broken down into the following categories: Grouping our metrics like this allowed us to quickly determine which part of the pipeline was experiencing a disruption in the case of an incident. For analytics, we wanted to measure the uptick in usage as these features rolled out to customers. This allowed us to frequently re-evaluate priorities and focus on the areas that our customers were most interested in. The following graph shows the uptick in DevOps triggered AFJ rule executions over time. For more context please read the following announcement posts: If you have any feedback or questions on DevOps automation, please visit the Jira Automation space on the Atlassian community. Mo is a Software Engineer on the Jira Agile team.", "date": "2020-08-19"},
{"website": "Atlassian", "title": "Retrieving Schemes with Project Configuration APIs", "author": ["Lukasz Berezowski"], "link": "https://blog.developer.atlassian.com/retrieving-schemes-with-project-configuration-apis/", "abstract": "Handling the configuration of projects can be challenging – whether you're an admin auditing the configuration of multiple Jira projects, or a developer trying to set the application flow in order to accommodate potential differences in target projects. Our new set of comprehensive REST APIs aim to solve these issues and more. We're excited to share an overview of how to use the new APIs for retrieving various project configuration items. The ability to wrap one's head around effective classic project configuration is what often separates a Jira veteran from the casual user. As such, it constitutes one of Jira's most notable learning curves. At the root of what makes classic projects so notorious is also what makes them so versatile and customizable: schemes . Configuration schemes are the mappings between other configuration items and projects. A scheme is how a particular project uses selected issue types, which draw from a set of selected fields. Configuration items at their core are fairly simple entities; it's the schemes that bring the flexibility to treat them like atomic building blocks to tailor to a projects’ specific needs. To make this a bit less cryptic, let's dive in to some examples. Issue type scheme is a configuration that describes which issue types are available to projects and the order in which they are displayed. The issue type is selected by default when creating an issue. In essence, it's the Issue Types ↔︎ Project mapping. To retrieve all available issue type schemes: GET /rest/api/3/issuetypescheme The response returns a list of issue type schemes IDs: Learn more: Issue type schemes To retrieve issue type schemes associated with projects: GET /rest/api/3/issuetypescheme/project The response returns projectIds for each assigned issue type scheme: Learn more: Issue type schemes for projects To retrieve Issue types contained by an issue type scheme: GET /rest/api/3/issuetypescheme/mapping The response contains a list of issue types associated to issue type scheme: Learn more: Get issue type scheme mapping Now, let's look into a more complex scheme. Issue type screen scheme takes a screen scheme, applies it to an issue type and associates that relationship to a project: Project ↔︎ Issue Type ↔︎ Screen Scheme . The below screen scheme is a mapping between screens (containing a collection of fields organized into tabs) and four issue operations: default, create, edit, and view. To sum it all up, issue type screen scheme is responsible for the scenario where in project ABC, when creating a bug, there's a 'severity' field visible on the creation screen. To retrieve all available screen schemes: GET /rest/api/3/screenscheme The response contains a list of screen schemes: Learn more: Get all screen schemes To retrieve all available issue type screen schemes: GET /rest/api/3/issuetypescreenscheme The response lists Issue type screen schemes: Learn more: Get issue type screen schemes To retrieve issue type screen schemes associated with projects: GET /rest/api/3/issuetypescreenscheme/project The response lists issue type screen schemes with associated projectIds : Learn more: Get issue type screen schemes for projects To retrieve all available Issue Type Screen Scheme items: GET /rest/api/3/issuetypescreenscheme/mapping The response returns the mapping of issue type screen schemes with issue types and screen schemes: Learn more: Get issue type screen scheme items With the changes and new endpoints we have recently introduced, developers and admins can now utilize APIs to retrieve all project configurations schemes. Learn more about working with other configuration components by reading their corresponding doc sets, including: Scenario: You are building an awesome app that will enhance the way bugs are handled in the service channel. Before you deploy the functionality, your app can verify that the correct configuration context is available to apply changes and make necessary adjustments. Task: Check if in every project the assigned issue type scheme has a bug issue type. Solution : First, we need let's determine the id of the Bug issue type: Then, let's get all issue type schemes associated with the existing projects: We can see that there are two issue type schemes in scope. Let's check which issue types belong to these schemes: It's clear that the issue type scheme with Id 10001 does not contain a Bug issue type. You can add it by using our newly-released endpoint, add issue types to issue type schemes : Classic project configuration had been a significant gap in Jira Cloud extensibility. We aim to change this, as providing API coverage in this area is a top priority of the Jira Cloud Ecosystem teams. Stay tuned and follow along on our Developer Community to learn more about new features as we release them. Up next: APIs to assign schemes to projects, CRUD for issue types and issue type schemes Your feedback is critical in helping us deliver safe and comprehensive extensibility in Jira Cloud. What are your initial impressions about GET APIs for classic project configuration? Will you use it? Will it help you build better apps? Let us know by posting on the Developer Community (add tags: jira-cloud , rest-api and project-config ). Is there a classic project configuration API that you are waiting for? Let us know by filing a feature request . Learn more about developing for Jira Cloud from our documentation . Product Manager working with Jira Platform Extensibility", "date": "2020-07-23"},
{"website": "Atlassian", "title": "Announcing the winners of Codegeist 2020", "author": ["Shaziya Bandukia"], "link": "https://blog.developer.atlassian.com/announcing-the-winners-of-codegeist-2020/", "abstract": "Thank you to everyone who participated in Codegeist 2020 . Codegeist is Atlassian's longstanding remote hackathon where participants build innovative applications for their favorite Atlassian tools. This year, we got to see our developer community in action during our biggest ever Codegeist  – with over 1,300 participants, 300 app submissions and $315k in prizes. Each submission solved for key challenges: from productivity to project management, to DevOps for distributed teams and even ways to manage your favorite craft beer selections . The focus and dedication to build production-ready apps in just eight weeks paired with the creativity and passion behind each app is inspiring, especially during the unprecedented times we live in today. At the core of everything we build at Atlassian is the reminder to tackle important challenges with passion and urgency, while taking a step back to consider options fully and with care. This helps us strive for the best possible customer experiences. Codegeist provides unique opportunities for developers to learn about our products, build on our platform, and improve upon the tools tens of millions of users rely on every single day. This year, we were also able to use Codegeist as a testing ground for Forge , our new cloud app development platform. Developers can now build trusted, scalable apps in minutes while Forge takes care of infrastructure and security. With 132 Forge submissions and a whole lot of new solutions, we're more confident than ever about our decision to build a new way to develop cloud apps. Many former (and current!) Codegeist participants have also built successful businesses by listing their Codegeist submissions, both apps and integrations, on the Atlassian Marketplace . This transformation of developers into entrepreneurs has helped the Marketplace reach an important milestone of $1B in lifetime sales – a feat that has only been possible because of our community of more than 25,000 developers. Building on this momentum, I'm excited to share the winners of Codegeist 2020. It's clear that each submission was built with heart and balance, and an effort to creatively solve for a variety of use cases. We were looking for apps in two tracks: And for bonus prizes , participants were even able to use the Trello customization platform to build Trello Power-Ups. Each submission was evaluated based on its strengths in three key areas: The winners will share $315,000 in cash prizes, along with receiving official Atlassian Developer swag packs given to all eligible submitters. ✨ Grand prize ($55,000) Meetical for Confluence Cloud Meetical for Confluence is a meeting management tool that allows teams to enhance meeting planning, documentation, and review, through seamless integration with popular third-party calendars like Google Calendar and Outlook. Second place ($43,000) Lively Recorder for Confluence Lively Recorder for Confluence enables teams to create audio, video and screen recordings directly from Confluence. Third place ($33,000) ZenRPA Triager for Jira ZenRPA Triager for Jira makes it easy for teams to build simple triage flows without code. Fourth place ($22,000) Microsoft Teams for Jira Microsoft Teams for Jira enables teams to start conversations and join discussions directly in a Jira issue. Fifth place ($12,000) Pair Up Pair Up helps remote teams address their Jira issues efficiently by leveraging tribal knowledge held within their organization. Honorable mentions ($5,000 each) Grand prize ($20,000) Scrum Maister Scrum Maister is an intelligent helper that revolutionizes agile development through AI-powered issue grooming, sprint analytics and retrospectives. Second place ($15,000) Visualize with AWS Visualize with AWS enables users to use a variety of declarative diagram rendering engines, such as Vega/Vega-Lite, PlantUML, Mermaid, and Graphviz, to visualize any kind of data. Third place ($10,000) Predictions for Jira Predictions for Jira uses deep learning to predict the type and priority of a Jira issue based on the issue title. Honorable mentions ($1,000 each) Plus, the top 100 Forge submissions will each receive $500. Best open source Forge app ($5,000) Prototyper : Create interactive prototypes in Jira issues without an external tool Best app for remote working ($5,000) Board Mirror (by Placker) : Automatically link and sync cards across Trello boards Best app for remote DevOps ($5,000) DEVsheds : Instant workspaces for teams to build and run code Best Jira app ($5,000) ZenRPA Triager for Jira : Build simple triage flows without code Best Trello Power-Up ($5,000) Screenful Reports : Create fully customizable reports from Trello data Congratulations to all the winners, and thanks again to everyone who participated in Codegeist 2020! 🎉 Curious to see all the apps that were submitted? Check out the Codegeist submissions gallery . Forge and Codegeist 2020 are the first of many exciting ecosystem announcements coming from the Atlassian Developer Platform team in the coming months. For the latest, head to the Atlassian Developer Community and follow us @atlassiandev . We'll be back soon with another Codegeist, but that doesn't mean you should wait to build your next app. Get some inspiration from the apps that were built for Codegeist 2020, and build production-ready apps with Connect or get started with Forge . We can't wait to see what you build next. ❤️ Shaziya is the senior developer marketing manager for Atlassian's ecosystem. ✨", "date": "2020-07-29"},
{"website": "Atlassian", "title": "Update to Jira Cloud’s Swagger/OpenAPI docs", "author": ["Ben Kelley"], "link": "https://blog.developer.atlassian.com/update-to-jira-clouds-swagger-openapi-docs/", "abstract": "In a previous Atlassian Community article , I talked about some things that might block you in generating client code automatically from our published spec, and then using that generated code to call Jira. Well, we removed those blockers! Also, we heard your feedback that you didn't like the long and confusing method names. So here’s a quick recap on what Swagger/OpenAPI is and how you might use it to generate client code to call Jira, and some suggestions on best practices in integrating with a cloud service. I will also discuss our ongoing QA process to ensure that the spec is always usable. If you go to the documentation for the Jira Cloud REST API , you'll see the \"…\" in the top right. Click that, and you'll see the link for \"Download OpenAPI Spec.\" This is a JSON file that describes the REST API for Jira Cloud. The actual URL to this file is https://developer.atlassian.com/cloud/jira/platform/swagger-v3.v3.json . Although swagger-codegen can generate APIs directly from a URL, I recommend saving a local copy of this file. I am going to use a local copy in my examples. In this post I will deal with APIs common to all Jira project types, but the process is similar for Jira Software's APIs, and Jira Service Desk's APIs. While you are welcome to write code by hand to call Jira's REST APIs, there are a lot of APIs, and new ones are being added all the time. Keeping that code up-to-date by hand would be time consuming. Swagger is a way of documenting APIs in a standard way. I won't discuss that in detail here. You can read more at https://swagger.io/ – this is also called Open API. I am going to use the swagger-codegen tool to generate my client code. You can build this from source, but I just installed a version on my Mac using \"brew install swagger-codegen.” (Read more here) There are also integrations for swagger-codegen that work with popular build tools, like Maven. In theory this is the minimum command you'd need to generate some source code in the current directory from the spec: That will generate you a usable client project in the current directory. If you read my previous post on this , you will remember that there were a number of other manual steps at this point. Not any more. In theory you're good to go, but I am going to recommend a few more things as best practice. You will want to do things like choose package names and an HTTP client style that suits your project better. Create a config.json file for swagger-codegen. For example: I'm using the \"jersey2\" flavour of HTTP client. There are different options depending on the language you used. Check the documentation for swagger-codegen . To generate a client using a config file, use a command like: Configure swagger-codegen until you are happy with the code it generates. The above command generates a Maven/Gradle project under ~/src/jira-client. You will want to customise the pom.xml a bit so suit your development practices, and how you build things that are going to use this code. While the client code is generated, my recommended best practice is to version control the generated client code. Why? This helps you to know what changed since the last time you generated it. To test, I opened up the generated project directly in my IDE, and created a Main class. My recommended best practice is to build the generated client as its own library. That way you can refresh the client periodically in a self contained way. Specify this library as a dependency to your projects that need to call Jira. In this example I'm going to look up my user, find a project, find the bug issue type for that project, and create an issue. If you have used our Open API spec before, you will notice how the method names are nice and readable now. We know you depend on your Jira integrations continuing to work, and you would rather spend your time adding value to your organisation than figuring out how to integrate with Jira. For that reason we have put in place a continuous integration strategy that basically does all the things I described above. We regularly generate a client library, and then run tests using that generated code to make sure it does what it says on the box. Ben is a Senior Developer on the Jira Enterprise & Migration team.", "date": "2020-08-19"},
{"website": "Atlassian", "title": "Developer beta now live for app module support on Jira Cloud Android, iOS, and Mac clients", "author": ["Rayen Magpantay"], "link": "https://blog.developer.atlassian.com/developer-beta-now-live-for-app-module-support-on-jira-cloud-android-ios-and-mac-clients/", "abstract": "Attention Jira Cloud developers: issue glance and issue right context panel module support is now available on beta for all platforms, Jira Cloud iOS, Android and Mac! We are giving developers early access now to give you time to validate that your web app works as expected before we officially go live to all Jira Cloud users. If you experience any problems leave a comment here in the developer community or contact us at jira-cloud-native@atlassian.com directly. Rayen is a Product Manager on the Jira Cloud team.", "date": "2020-04-29"},
{"website": "Atlassian", "title": "Want to see your app in Jira Cloud mobile? Glances have arrived!", "author": ["Anmol Agrawal"], "link": "https://blog.developer.atlassian.com/want-to-see-your-app-in-jira-cloud-mobile-glances-have-arrived/", "abstract": "Don't just peek or squint – take a proper glance at your app within Jira Cloud for iOS and Android . That's right, third-party apps have arrived as glances on mobile. Is your app ready? (Of course, it is! Grab the bull by the horns. Or, rather, the code by the branch.) Currently, the web version of Jira displays third-party apps within issue details, showing which apps are connected and providing information to the issue (such as time-tracking with Tempo ). We call this feature a glance because users can take a quick glance at key information. Until now, glances have only been supported in Jira's web version. But now… Jira Cloud mobile has joined the glance party. Cue streamers! The glance includes your app's icon, content, and status. Clicking the glance opens a separate glance panel with more information. Maximize your app's warm welcome into the land of Jira Cloud mobile by testing it first. Connect with us directly by following the steps below. Follow these 3 steps : Step 1. Go here and complete the form ; you'll need to include your Jira Cloud test instance URL . We'll be in touch via email once we've enabled your instance. (Please allow approximately 2 business days.) Step 2. Test how your app's content appears as a glance, including the separate glance panel (if it has one). Make sure you've read these technical specifications . If you experience any problems , let us know via jira-cloud-native@atlassian.com with the subject line 'Jira app glance test'. Step 3. When you're finished testing your app as a glance, let us know by replying to the email we sent you. We'll enable your app, and it'll appear as a glance in Jira Cloud mobile for users around the globe. Anmol is a Developer Advocate for ecosystem developers.", "date": "2020-01-07"},
{"website": "Atlassian", "title": "Confluence Server 7.7 beta is available now", "author": ["Rachel Robins"], "link": "https://blog.developer.atlassian.com/confluence-server-7-7-beta-is-available-now/", "abstract": "This week we released a beta for Confluence 7.7. There are changes in this release that may directly affect 3rd-party apps. New in 7.7 To find out what’s in this release, check out the Confluence 7.7 beta release notes . Get started Download the beta from our site to start testing your app for compatibility with the changes. Be sure to take a look at our Preparing for Confluence 7.7 guide to find out what you need to do to make your app compatible. We’ll update that page regularly, so keep an eye on it to see any changes. Cheers, The Atlassian Confluence team Rachel is a technical writer on the Confluence Server and Data Center team.", "date": "2020-08-02"},
{"website": "Atlassian", "title": "10 design principles for delightful CLIs", "author": ["Natalie Johnson"], "link": "https://blog.developer.atlassian.com/10-design-principles-for-delightful-clis/", "abstract": "Forge is our new cloud app development platform — now, developers can build trusted, scalable apps in minutes without the need to manage infrastructure or security. Forge is currently available in beta for customers building apps and integrations for their teams. At the center of this new platform is the Forge CLI, a command-line interface that enables developers to create, develop, install, and manage Forge apps. We — designers Michael Belton and Natalie Johnson — were tasked with designing the new Forge CLI. Given its centrality to the developers' experience, our mission was to create a kick-ass CLI tool that made it delightfully easy for developers to build and run apps on the new platform. With thousands of apps created with the CLI so far and glowing feedback from Forge developers, here are our 10 principles for designing successful CLIs. If you're building a CLI to support your service, resource, or platform, we hope that you might use these principles, too! CLIs have pretty much been around since the dawn of computers themselves, so you don't need to reinvent the wheel to create a crowd-pleasing experience. There are already many established conventions and guidelines that you can use to design your CLI, such as Heroku's CLI Style Guide and the Microsoft Style Guide . By using existing patterns that your users are already familiar with, you ensure that it will be easy for them to adopt your tool. Adding a --help command to your CLI provides users with an essential piece of documentation. It lets new users discover all the commands and options available to them, while more experienced users can refer to it as a reference throughout their use of the CLI. To help your users accomplish their tasks, the --help section should provide a complete list of commands, subcommand, and any short-name equivalents with a simple description. You should also make sure that your users can run the --help flag after any specific command to quickly see the full syntax (usage, arguments, and options) they can use to work with the command. While the visibility of system status is an important heuristic to apply to the design of any interface, it's particularly relevant when designing for the text-only interfaces of a CLI. Without a Graphical User Interface (GUI) to provide immediate visual feedback, CLIs don't always do a good job of keeping users informed about what's going on behind the scenes. We suggest you show what's happening by using progress bars, spinners, and other visual devices. We also recommend you break long-running tasks into a series of meaningful steps to help communicate the status of the system to your users. CLIs don't always make it clear when an action has taken place. For every action a user performs, your CLI should provide an equal and appropriate reaction, clearly highlighting the current system status. For example, in the Forge CLI, if a logged-in user types forge logout a message is displayed, providing a clear indication that the action has been successful. Errors are expected and even (dare we say) an essential part of using any CLI. When testing the Forge CLI, we observed that errors are one of the key blockers that prevent developers from building their app. Our CLI would sometimes spit out error messages that were, as one of our engineers once described it, \"human-unreadable.\" These are error messages that come directly from the back-end system that created it, and provide little value to the user who's trying to figure out what's gone wrong, and how they can quickly recover from it. To help the user get back on track as quickly as possible, every error message that your CLI displays should not only contain a written description of what's gone wrong but also include suggestions for how to fix it. You might even include a link to find out more information about the error. While it can be tempting to include every piece of documentation that your users must know in the CLI, this can overwhelm a user who is trying to quickly progress through the task at hand. Throughout usability testing of the Forge CLI, for instance, we observed that many developers often skim-read the text, only looking for information that appears relevant to their immediate needs. To support the more action-oriented users, you should break information into digestible chunks that make it easy for users to scan. A general rule of thumb we followed was to keep any instructions accompanying the CLI command to no more than 3 sentences (or around 50–75 characters) in each paragraph. You should also emphasise important information by using visual devices such as text formatting, lists, and icons. We recommend keeping the set of icons to a minimum. This enables each icon to stand out as a useful wayfinding element. Throughout their use of your CLI, users will repeatedly run the same sequence of commands. For example, in the Forge CLI, a common sequence would be forge login and then forge create . While the more experienced users of your CLI will quickly become adept at running the same sequence of commands, new and less-experienced users of your CLI might sometimes need a bit of a reminder. By identifying common patterns of use, you can be smart about suggesting the next best step users should take at the end of each command, reducing the need for users to refer back to the documentation. When users run a command in your CLI, they'll likely need to also pass in some options to execute that command. While experienced users of the CLI may become adept at passing in all of the required information, other users might need a bit of prompting. Rather than throwing an error, your CLI should prompt the user to enter any outstanding information. Make sure to consider the cases where users provide some, none, and all of the required options so that you can anticipate the various ways that users will interact with your CLI. When possible, provide sensible defaults for options rather than asking for them each time. For example, most commands in the Forge CLI run in the context of an environment, and we believe the CLI usage will mostly happen during the development of the app. That's why the default for most commands is to run in the development environment without needing to prompt the user for this information. Unlike most GUIs, CLIs have no visible way to stop a task from running, other than closing the terminal window. To provide users with a sense of control, your CLI should have clearly marked exit pathways. For interactive commands, remind users there is a simple way to stop the task from running, and that returning to the prompt is only a short ^C away. As Heroku's CLI Style Guide specifies , rather than passing arguments directly into a command, the CLI should use flags to label the arguments. Using flags means the user doesn't need to memorize the argument order, they just focus on what arguments to provide. The labels also give context to each value, improving the readability. We recommend providing short-names for commonly used flags. For example, rather than writing a command like forge deploy production jira , developers write forge deploy --environment production --product jira . Experienced developers can also use the shortened version: forge deploy -e production -p jira . While many other factors contribute to creating successful CLI tools, if you start with these principles you will be well on your way to creating a CLI that your users find delightfully easy to use. But don't just take our word for it — try it yourself! The Forge CLI is available to use in the Forge beta. We're actively looking for members of our developer community to build trusted, scalable cloud apps and tell us about their experience. Natalie Johnson is a senior product designer at Atlassian.", "date": "2020-07-23"},
{"website": "Atlassian", "title": "Announcing the Marketplace Partner Program", "author": ["Warren Chen"], "link": "https://blog.developer.atlassian.com/announcing-the-marketplace-partner-program/", "abstract": "In 2012, the Atlassian Marketplace launched to provide new use cases for Atlassian's products. In 2014, the Atlassian Verified/Top Vendor Program was introduced to customers to formalize the benefits we offer to our partners and to showcase high-traction partners. Throughout this journey, our partners’ success and growth has seen the Atlassian Marketplace grow from handful of developers to more than 1,000 partners who have built businesses on the Atlassian platform. In 2019, we achieved $1 billion in lifetime sales and added more than 275 cloud apps to the Marketplace (an app every business day!). As the Atlassian Marketplace continues to mature and as customer requirements and expectations for cloud, Data Center, and security have up-leveled, we sought to redesign the partner program to incentivize cloud and Data Center app development and promote enhanced security practices. This is the next step in a continual journey of evolution to ensure the offerings created by our Atlassian ecosystem of partners meet and exceed customer expectations. Launching today is the Marketplace Partner Program. Beginning today, the Marketplace Partner Program replaces the Top Vendor Program. The Marketplace Partner Program comprises 3 partner levels – Platinum, Gold, and Silver. These scaling levels recognize each partner's individual investment in the Atlassian platform and alignment with our strategy. Furthermore, we recognize that our relationship with our ecosystem of developers and partners has evolved as well. Apps and partners are critical aspects of our mutual customers’ unique ways of working and fill a variety of needs and use cases. Thus, we are formalizing the terminology change from \"Marketplace Vendor\" to \"Marketplace Partner.\" Marketplace Partners are a key reason why many of our customers have become and remain passionate champions of Atlassian. Platinum, Gold, and Silver Marketplace Partners have met enhanced Atlassian requirements in areas such as: The new Cloud Security Participant badge identifies any app that has undergone additional security measures. Today, this includes cloud apps enrolled in the Marketplace Bug Bounty Program , a first of its kind that allows ecosystem partners to crowdsource vulnerability discoveries through a pool of talented security researchers. The new Cloud Security Participant badge will evolve over time to include industry-standard security assessments and more rigorous testing. The Marketplace Partner Program is intended to ensure that customers have great experiences and for Atlassian to reward partners who are most aligned with our objectives; see the program's requirements and benefits here . Warren is the Head of Marketplace Programs overseeing initiatives for the Atlassian Marketplace and partner relationships.", "date": "2020-07-07"},
{"website": "Atlassian", "title": "Codegeist is back in a big way", "author": ["Shaziya Bandukia"], "link": "https://blog.developer.atlassian.com/codegeist-is-back-in-a-big-way/", "abstract": "Codegeist is a remote hackathon where participants build innovative cloud applications for their favorite Atlassian tools. These apps can be for individuals and for teams, and participants may even want to list their apps on the Atlassian Marketplace. From newbies to experts, everyone is welcome to participate from May 19 (Tuesday) to July 13 (Monday) for a chance to win over $300k in prizes . Think about solutions that would help teams work smarter, improve workflows and make lives easier. We're looking for apps in two tracks: Plus, you can use the Trello customization platform to build Trello Power-Ups, too! There are plenty of opportunities to win big – pick a topic that inspires you and start building. Recently, the Atlassian Marketplace reached an important milestone: $1B in lifetime sales. This is all thanks to our community of more than 25,000 developers who have helped build the Atlassian Ecosystem into what it is today. What's even better is that we've seen 60% year-over-year growth in cloud app sales. This tells us something important: our customers rely on the cloud to get things done. Two cloud apps are added to the Atlassian Marketplace every business day, and we hope your Codegeist submissions are next. Over the years, Codegeist has received a stream of new apps and cool ideas that have been listed on the Atlassian Marketplace, including: Whether you're new to Atlassian Cloud or need a refresher, check out these resources to help you get started. When you're ready: All submissions will be evaluated by a panel of Atlassians through July 24. Winners will be announced by July 31. Don't worry – we'll keep you posted. We're here to help! Head to the Atlassian Developer Community , locate a relevant forum for your question and make sure to type in codegeist in the optional tags field of your topic. Mention us @atlassiandev , and tell us about what your progress using the hashtag #Codegeist2020. We're excited to see what you create! Shaziya is the senior developer marketing manager for Atlassian's ecosystem. ✨", "date": "2020-05-19"},
{"website": "Atlassian", "title": "Build your dream workflow using Forge", "author": ["Shaziya Bandukia"], "link": "https://blog.developer.atlassian.com/build-your-dream-workflow-using-forge/", "abstract": "Forge is our new cloud app development platform – now, developers can build trusted, scalable apps in minutes without the need to manage infrastructure or security. Forge is currently available in beta for customers building in-house apps and integrations for their teams. I had the opportunity to sit down with Anil Kumar Krisnashetty, IT Consultant at bitgrip GmbH, and learn more about his experience improving day-to-day workflows by building Forge apps. During my recent visit to Atlassian's Bengaluru office, I met Atlassian developer Manjunath Basaluru Srinivasa. We had a great conversation about the agile framework and issues I've faced with Jira cloud marketplace apps. That's when he suggested that I try out Forge – a new, easier way to build cloud apps. With my 10+ years experience in frontend web development and passion for APIs and serverless platforms, this was music to my ears. I received access to the beta in a few days and was on my way. I started by creating a hello world macro app for Confluence and got it up-and-running in just a few minutes. The easy, step-by-step instructions in the documentation are helpful in getting a quick start. Plus, the beta gives you access to an active Forge Slack workspace, where the Forge team provides quick responses to any questions I have. I was also able to rely on a few videos from Atlas Camp 2019 , which helped me understand the overall Forge architecture. A React-style declarative UI, access to data through APIs and the execution of your business logic on a Functions-as-a-Service (FaaS) platform; Forge lets you build more with less. With a set of Forge UI components and useful sample starter apps, I was able to see my ideas in action in less than five minutes. The Forge tunnel lets the user build apps locally and see the results instantly. The platform also takes care of infrastructure and security, enabling more time to focus on business logic, and less time on infrastructure. Here's an example of what I mean: This app renders your Figma design assets on a Confluence page. Check out the .README file for more information. You can also duplicate the Figma file and use this Bitbucket project to get started. Watch the demo to learn more WhatsApp is a widely used app, which made me think, \"what if we could create a Jira issue in WhatsApp?\" This app assigns issue type automatically based on the text description received in WhatsApp. I've used Twilio to send and receive messages from Whatsapp, Forge webhooks to receive the Whatsapp message, and Algolia to determine whether or not the WhatsApp message is a bug. Watch the demo to learn more Use a Confluence page as the single source of truth for your application's end-to-end automation tests. With this app, you can display automation results status – built using a Forge macro app, and the table, lozenge and text Forge UI components. Watch the demo to learn more This app enables you to view Figma artboards in a Jira issue panel as a carousel. I built this using the image, button and table Forge UI components. Check out the duplicate Figma file and watch the demo to learn more Collect product ideas from teams and display them in a prioritized table on a Confluence page using Typeform. Watch the demo to learn more Build user stories with a simple view of your target user in Jira issue panels. Rather than jumping from one platform to another, your user personas populate automatically in Jira. Watch this demo to learn more Let me give you an analogy. In many cities, for a person to go from A to B, they commit to buying a car and getting a driver's license, and after that, they still have to consider additional costs such as auto insurance, parking, and maintenance. Though, with a ride share option, folks can go from A to B with a single tap. Just like that, Forge enables developers to get an app up-and-running in just a few minutes. A few things stick out to me: I'm looking forward to building more apps that tackle team efficiency, solve developer workflows and improve the pull request process. I'm also excited to see what people are going to build after reading this post – maybe I'll see some of these submissions in Codegeist , as well! Of course! First things first: Think about issues you want to solve and workflows you want to improve. Forge can even be the perfect side project, whether you want to work individually or with a team. Check out these tips as you think about what to build: Many thanks to Anil for his contributions to Forge, and for consistently evangelizing the platform to his peers. If you'd like to provide Anil with any feedback, please use this link . For latest updates, on Forge and otherwise, catch up with Anil by subscribing to his Youtube channel , connecting with him on LinkedIn , and/or following him on Twitter . Shaziya is the senior developer marketing manager for Atlassian's ecosystem. ✨", "date": "2020-06-17"},
{"website": "Atlassian", "title": "Confluence Server 7.6 beta is available now", "author": ["Rachel Robins"], "link": "https://blog.developer.atlassian.com/confluence-server-7-6-beta-is-available-now/", "abstract": "This week we released a beta for Confluence 7.6. There are changes in this release that may directly affect 3rd-party apps. New in 7.6 To find out what’s in this release, check out the Confluence 7.6 beta release notes . Get started Download the beta from our site to start testing your app for compatibility with the changes. Be sure to take a look at our Preparing for Confluence 7.6 guide to find out what you need to do to make your app compatible. We’ll update that page regularly, so keep an eye on it to see any changes. Cheers, The Atlassian Confluence team Rachel is a technical writer on the Confluence Server and Data Center team.", "date": "2020-06-15"},
{"website": "Atlassian", "title": "Atlassian at the Okta ITERATE Conference", "author": ["Sheldon Callahan"], "link": "https://blog.developer.atlassian.com/okta-iterate/", "abstract": "In late February, Okta put on the ITERATE conference which they describe as a new type of conference. It was a one day experience themed around being the best developer you can be. The event was a free event centered around technical best practices, developer culture, security, productivity, and passion. I was able to sneak away from our booth for two sessions both of which had a significant impact on me as a developer. The keynote speaker was Jeff Atwood the co-founder of StackOverflow and Discourse . Jeff spoke about how games influenced his thinking when it came to building software. He exposed the difference in thinking that informed the building of these two platforms. Jeff stated that StackOverflow was designed to solve problems and not for discussions and chit-chat. He spoke to the fact that StackOverflow is a strict peer-based game focused on questions and answers and building reputation. Discourse conversely was described as a discussion game based on empathy, fun, and storytelling instead of solving peoples problems. At Atlassian, we use discourse for our community and this talk gave more context for why the tool works the way it does and how to maximize its use by understanding the underlying thinking. Also the idea of thinking of this platform as a multiplayer game that rewards types of behavior intrigued me as a developer. I was able to walk away with Jeff’s book Effective programming given to all attendees, and I can attest it is an informative read. Jeff Atwood: Keynote Video The other session I was able to attend was the API Throwdown with Nate Barbettini . His talk focused on API design, and he compared the design styles of RPC to REST and GraphQL. He addressed the debate about which pattern is best. While remaining neutral, he described the different use cases and advantages of each. He stressed there is yet no one size fits all solution and like any tool, they each work well for their purpose. Nate explained RPC as a good pattern of calling endpoints that relate to functions. This simple protocol works well for high-performance environments with medium network noise. RPC Applications become problematic because of its tight coupling to functions he advocated. RESTful application architecture, on the other hand, has a goal of decoupling the client and the server. He defined REST as not an endpoint that returns JSON using HTTP verbs but as a way of modeling resources and links to resources and metadata without the need for creating functions for every use case. That is to create stable systems that used for years after being built. Nate suggested using REST specifications like Ion, HAL, and JSON-API to help you with modeling your REST API. GraphQL is the newest kid on the block and is the most complex of the three mentioned here. Which uses queries to bring back the exact data you are looking for in fewer calls to the server. Some of the advantages are; create low network overhead to remove latency, the ability to query deep linked relationships between resources. I felt this talk was especially relevant for our developers as they design and maintain their APIs. To determine which style is best ends up depending on who consumes your API and why. Nate Barbettini: API Throwdown Video Atlassian partnered with Okta on a booth where we created a trivia game dubbed “Geek Out()” with prizes. ITERATE gave Atlassian a chance to build some awareness with the event attendees about our tools and developer opportunities. The team consisted of Neil Mansilla , Luke Kilpatrick , and myself Sheldon Callahan . We kicked around many ideas, but in the end, we came up with a repeatable and reusable game focussed on developers. The game consisted of 150 dev related, nerdy, and geeky trivia questions. Up to 4 players could play together for prizes, with the best prizes only available if there were 4 players. This incentivized people to get others to try and play our game, and created immediate social interaction. We put the questions and answers in a slide deck, question first then answer, and projected them on a screen. Each player was given a buzzer in a Jeopardy-like fashion to press when they knew the answer. The first player to press and guess correctly would score a point. The player with the most points would win the prize. Questions ranged from things like: People seemed to genuinely have fun, and in between sessions we had many repeat players. People would come back with a friend, or they would even ask a random person to play so that they could have a shot at the better prizes. There was an instant social impact on the crowd and people would stand around watching other people play with intrigue and then step up to play themselves. This was a successful, fun, engaging and repeatable way to engage the community. ITERATE itself was an informative conference with excellent content that complimented Atlassian.", "date": "2018-04-04"},
{"website": "Atlassian", "title": "Dreamforce 17 Streamline Cloud Deployments", "author": ["Sheldon Callahan"], "link": "https://blog.developer.atlassian.com/dreamforce-streamline-cloud-delpoyment/", "abstract": "Every year Salesforce puts on its annual Dreamforce conference. Dreamforce is a sales, business, technology, learning, self-actualization, rock concert, and philanthropic event. In the Developer Campground, I demoed Atlassian team collaboration tools. Showing how any developer can use our tools to deploy to the Salesforce Heroku platform . My talk, \"Streamline Cloud Deployment,\" mapped communication and collaboration strategies to Atlassian tools. Building integrated workflows with Jira, Confluence, Bitbucket, and our newest communication platform, Stride. Configuring Bitbucket Pipelines to auto-deploy to Heroku with every code commit. The audience was able to see how Atlassian Tools simplified CI/CD with endless possibilities.", "date": "2017-12-05"},
{"website": "Atlassian", "title": "Apply for the Stride API early access program", "author": ["Sheldon Callahan"], "link": "https://blog.developer.atlassian.com/apply-for-the-stride-api-early-access-program/", "abstract": "At Atlassian, our mission is to unleash the potential in every team. Today we announced Stride , the complete communication solution that empowers teams to talk less and do more. Our ecosystem team is excited to announce we are also offering developers the opportunity to apply for early access to the Stride API. While the Stride app empowers teams to turn conversations into actions, the Stride API enables you to quickly and easily build apps, bots and integrations that can be used by your teams, or published to the Atlassian Marketplace to make them available to other teams using Stride. If you are a developer who builds apps for our server products, HipChat Data Center remains our Enterprise team communication product and your apps will continue to be supported via the current HipChat API . On the new Stride platform, building apps and bots is a snap. Bots are treated just like users, and can be added to any conversations that users select, including private 1-on-1 conversations. Your apps and bots can: Send messages to users via a public or private conversation Watch for specific messages or events using webhooks Chat and interact with users Extend the Stride app with custom UI — to an end user, an app appears as a fully integrated part of Stride Built for scale, Stride can accommodate all sorts of teams, from small startups to enterprises with over 10,000 users. Stride offers developers unique opportunities to get their apps in front of more users and teams. Stride features an embedded version of the Atlassian Marketplace, showcasing useful Stride apps built by the developer community. This makes it easy for your apps to be discovered and installed by Stride users. Once an app is installed, a user only needs to @-mention your bot to have it join the conversation, helping your app to get more traction within teams. Today, we’re also excited to announce our new Atlassian API platform. This next generation platform is an evolution of app and API management at Atlassian. For developers, creating and managing apps that work across the suite of Atlassian products will be much easier. For Atlassian, the new platform improves how we publish, manage and secure APIs. Stride is the first Atlassian product API built using the new platform (with more to follow). You can learn more about the Stride API at developer.atlassian.com , where you’ll find: Reference documentation Tutorials and guides Sample reference app, complete with source code Interested in early access to the Stride API? Apply for the early access program below. From bots that become natural extensions to the team, to integrations with other essential apps that teams rely on, we are excited to see what you’ll build with the new Stride API to help teams do more.", "date": "2017-09-07"},
{"website": "Atlassian", "title": "Announcement: Changes to Username search term in JQL for Jira Cloud", "author": ["Ben Kelley"], "link": "https://blog.developer.atlassian.com/announcement-changes-to-username-search-term-in-jql-for-jira-cloud/", "abstract": "In line with our ongoing work to remove usernames in Atlassian Cloud, from October 1st 2020 we will be removing support for searching by usernames in JQL in Jira Cloud. This means that after that date, if you write a new JQL query that tries to match a user field using that person's username, it will no longer return any results. This means queries like this will no longer be supported: After this date, the following ways to find users will continue to be supported: *These methods depend on whether that person makes that information visible to you , as is currently the case . We are making this change in line with Atlassian's policy to remove usernames across Atlassian Cloud products, as announced in June 2018 at say goodbye to usernames in atlassian cloud . You will find that the JQL editor in Jira Cloud no longer writes queries that contain usernames, but if you have existing queries with usernames, those queries will continue to work up until the cutoff date. Before that date we will be updating queries stored in Jira Cloud that search by usernames to search for the same person using that person's account ID. If you have JQL stored outside of Jira Cloud that you use to run queries in Jira, you need to make sure that you migrate your queries to perform the same search using account IDs before the cutoff date. You may find the \"pdcleaner\" REST API useful in Jira Cloud to help you migrate queries. You can read more about this in Jira Cloud’s REST API documentation . Ben is a Senior Developer on the Jira Enterprise & Migration team.", "date": "2020-05-01"},
{"website": "Atlassian", "title": "Confluence Server 7.5 beta is available now", "author": ["Rachel Robins"], "link": "https://blog.developer.atlassian.com/confluence-server-7-5-beta-is-available-now/", "abstract": "This week we released a beta for Confluence 7.5. There are changes in this release that may directly affect 3rd-party apps. New in 7.5 To find out what’s in this release, check out the Confluence 7.5 beta release notes . Get started Download the beta from our site to start testing your app for compatibility with the changes. Be sure to take a look at our Preparing for Confluence 7.5 guide to find out what you need to do to make your app compatible. We’ll update that page regularly, so keep an eye on it to see any changes. Cheers, The Confluence Server team Rachel is a technical writer on the Confluence Server and Data Center team.", "date": "2020-05-06"},
{"website": "Atlassian", "title": "Artificial intelligence for issue analytics: a machine learning powered Jira Cloud app", "author": ["Katrina Morales"], "link": "https://blog.developer.atlassian.com/artificial-intelligence-for-issue-analytics-a-machine-learning-powered-jira-cloud-app/", "abstract": "This blog post was contributed by a group of students at the University of Wollongong, based on a project they did for their computer science class under the supervision of Associate Professor Hoa Dam. As Natural Language Processing matures, the value of a trusted AI interpreter to enhance a team's performance through business applications becomes more apparent. In this article, our team presents a Jira Cloud app that provides teams with an AI-powered \"assistant\" to assess Jira issues as they are submitted. Our team built this app as the capstone project for the Bachelor of Computer Science at the University of Wollongong, located in Australia on the south coast of New South Wales. The app, Artificial Intelligence for Issue Analytics (AIIA) , provides features that use machine learning (ML) and visualisation techniques that allow team members and managers to more effectively manage projects. The aim of the app is to help teams properly scope projects by providing accurate issue data and identifying conflicts and blocking tasks. The initial concept for the project came from the research of Associate Professor Hoa Dam . One of his focus areas, artificial intelligence for project management and software engineering, gave us insights and a goal to work toward for our project. We based our research and development primarily on the projects found in this technical paper . Additionally, Hoa encouraged the team to operate as if we were a company to give us some insight into what it will be like to work in a professional setting. Hoa acted as our manager, and we met with him weekly to receive feedback on what requirements we should prioritise. All of these factors combined provided a strong learning environment for project management, technical skills, and ideation. This project is a proof of concept on how to implement a machine learning model within the Jira Cloud environment. Since we had no experience in machine learning prior to this project, nor in developing apps for Jira Cloud, we recognised the strong learning outcomes that could be achieved. Initially, we implemented some traditional machine learning techniques before moving to more modern deep learning techniques. As we developed an understanding for deep learning we became familiar with the concept of transfer learning, however due time constraints we opted not to implement such a system, rather focusing on proving the end-to-end app implementation on Jira Cloud. The model is based on a Long Short Term Memory (LSTM) structure. This network provides a model that can retain information and learn from time-series data. A common application of a LSTM is text analysis, which is needed to acquire context from the surrounding words to understand patterns in the dataset. Understanding LSTM Networks provides a more in depth look at how an LSTM network operates. Data used to train the model was obtained from other open source projects available on Jira Cloud such as Clover (by Atlassian) . This data was downloaded through the Jira Cloud REST API using the jira-python library . The data was cleaned and structured in a jupyter notebook with Pandas. The experience of developing a model based on data obtained from a real-world source allowed us to develop stronger analytical skills and a practical understanding of how to approach machine learning topics. This is in contrast to building a model based on datasets from clinical learning environments that contain comprehensive, on topic data with minimal to no missing elements. One feature of our app is issue recommendations. Once a new issue has been created with a title and description, the recommendation feature appears in the glance sidebar. When \"Generate Recommendations\" is clicked, the app suggests values for different fields in the issue such as story points, labels, priority, issue type, and component. The machine learning model works as a recommendation engine for these values, and it bases its suggestions on data from other issues in the project. The user can then accept or reject the suggestions. Additionally, accepted suggestions can be reverted in the glance menu. In future versions this validation mechanism can be harnessed to train the machine learning model. The core components of our application were the Atlassian Connect Express (ACE) framework and a Flask webapp to provide the API to our model deployed using Keras. ACE provided the perfect development environment, allowing us to simply and easily add new pages and modules to our app. When the recommendations glance menu is activated, an AJAX request is made through ACE to the Flask API with the task's title and description. Flask passes the data from the request to the model for inference. The model resides on a remote server that is independent of the Jira Cloud app, allowing the model to be updated on restart of the server side program. The task is updated with the selected suggestions using the Jira Software Cloud REST API. This API is crucial in updating tasks, retrieving task information to send to the machine learning model, and obtaining project information to ensure that suggested values are valid within a task's context. Our team also implemented the ability for users to revert suggestions through the use of a history panel if they later decide to return to a task's original state. This was possible because the ACE framework allowed us to define routes used for storing and retrieving task information in a MongoDB database. Initially our app's user interface lacked cohesive elements that matched the Jira Cloud interface. After we discovered the Atlassian Design Guidelines (ADG) and the Atlassian User Interface (AUI) library, we were able to refactor the user interface to create a seamless experience in the Jira Cloud environment. For example, the recommendations menu was initially displayed as a web dialog with a plain checkbox beside each suggestion, however we were later able to incorporate these components into the glance menu with improved styling and AUI toggle buttons to replace the checkboxes. The other feature of our app is the visualiser. The visualiser aims to provide the team with a big picture understanding of how the project is progressing and identify potential blockers. In a separate panel, the app produces a network graph of all the issues in a project and their relations to other issues. These links show the types of dependencies issues have on each other such as blocking, derived, and others. Additionally, in the issue view we show a small version of the visualiser of related issues for immediate visual feedback. The visualiser stores preprocessed graph data in a MongoDB database, in which filters can then be applied to the data in the application window, while Atlassian Connect Express (ACE) formats the MongoDB query. The display of the data is via the JavaScript plugin cytoscape using the Cose-Bilkent display algorithm. The above mentioned Atlassian Design Guidelines and AUI library were also key to building the visualiser. They provide consistent styling and components, reinforcing a sense of familiarity within Jira while maintaining a high standard of usability. We knew that the data users place on the Jira platform could contain sensitive topics and IP, so we developed our app with its security in mind. To mitigate the inherent risks of dealing with external user data, our app minimises interactions with user data in two ways: For the foreseeable future, our platform can operate while only storing the issue IDs and request the data from Jira on an as needed basis. If a more performant implementation is required we will need to revisit this topic. This project was developed as a proof of concept for developing machine learning applications based on Jira Cloud. During the development we identified some features of the app that could be improved, but we kept our focus on developing an end-to-end product to reach a proof of concept required for the project. With our improved understanding of ML techniques, we now see many areas where the model could benefit from the application of modern networks such as Nvidia’s Megatron, GPT-2, and others via transfer learning. These are modern and computationally heavy models, but they and even their predecessors would provide many improvements to the performance of our model. This project shows the possibilities of implementing machine learning models on cloud based systems. As mentioned, there's plenty of room for improvement with our model, but overall we're quite pleased that Atlassian's comprehensive libraries and frameworks allowed us to develop a novel, artificial intelligence-powered solution in Jira Cloud in 6 months from ideation to a viable product. The frameworks available such as ACE and AUI provided extensive resources to produce high quality, consistent applications within the Atlassian environment. Without the user interface elements from AUI, the Atlassian Design Guidelines providing approachable design documentation, and the ACE framework, we would not have been able to produce an app with the detail and features that we did. Not to mention they were extremely easy to use! Personally, the team benefited extensively from this development experience. The variety of topics, technologies, and methods brought together for this project provided a somewhat realistic experience in developing a product idea from start to MVP, which will be invaluable experience in our careers. Katrina is the Developer Community Manager for Atlassian's ecosystem.", "date": "2020-04-08"},
{"website": "Atlassian", "title": "Confluence Server 7.4 Enterprise release beta is available now", "author": ["Rachel Robins"], "link": "https://blog.developer.atlassian.com/confluence-server-7-4-enterprise-release-beta-is-available-now/", "abstract": "Today we released a beta for Confluence 7.4. There are changes in this release that may directly affect 3rd-party apps. New in 7.4 To find out what’s in this release, check out the Confluence 7.4 beta release notes . Confluence 7.4 is an Enterprise release , so we're not including any new features or significant changes. This release is focused on bug fixes to support our largest customers. Many customers on Confluence 6.13, our last Enterprise release, have been eagerly awaiting a 7.x Enterprise release, and we are expect a lot of upgrades to this version.  So check out our Preparing for Confluence 7.4 guide, start your testing, and remember to mark you app as compatible as soon as you can after release day. Get started Download the beta from our site to start testing your app for compatibility with this version. Be sure to take a look at our Preparing for Confluence 7.4 guide to find out what you need to do to make your app compatible. We’ll update that page regularly, so keep an eye on it to see any changes. Cheers, The Atlassian Confluence team Rachel is a technical writer on the Confluence Server and Data Center team.", "date": "2020-04-09"},
{"website": "Atlassian", "title": "Refactoring Atlas Camp", "author": ["Andrew Golokha"], "link": "https://blog.developer.atlassian.com/refactoring-atlas-camp/", "abstract": "Hey there, developer community! We've made the decision to forego Atlas Camp in 2020, and this was decided prior to the effects of COVID-19. As a team, we'd like to take a step back and rethink our developer event focus and strategy. Attendee experience is top-of-mind for us, and we want to take this time to ensure we're refocusing our events strategy to be even more useful, engaging and informative for you. As of now, we are planning to relaunch Atlas Camp in 2021 in the US. As always, please let us know if you have any questions or feedback, by reaching out to us here . Cheers, Atlassian Developer Advocacy Team", "date": "2020-04-07"},
{"website": "Atlassian", "title": "Build a cloud app, take home 95% of the profits for the first year", "author": ["Katrina Morales"], "link": "https://blog.developer.atlassian.com/build-a-cloud-app-take-home-95-of-the-profits-for-the-first-year/", "abstract": "Anyone can use our development platform to create custom apps that personalize their experience with Atlassian products. For the entrepreneurial types who want to distribute their apps commercially, we have the Atlassian Marketplace . Marketplace makes merchandising seamless and helps expose your apps to our 160,000+ customers. Hundreds of companies have built successful businesses within our ecosystem. If you need a little inspiration, consider Alex Medved, founder of Vertuna. I spoke to him recently about his journey to becoming a Marketplace developer. He was working at Skype when he was tasked with building a shopping cart-like experience in Confluence for internal employees to choose their company provided work phones. That assignment gave him an idea for an app: ConfiForms . It was just a side project at first, but after landing a few heavy-hitting customers, he decided to quit Skype to work on it full time. \"I really believed in the potential of the Atlassian Marketplace,\" he said. Now he and two others build and support apps for over 3,000 customers and have earned over $2M in revenue. If you're new to listing an app on Atlassian Marketplace, here's a quick overview of the financial side of things: Atlassian takes a percentage of the revenue from apps in exchange for providing the e-commerce platform, handling taxes and billing, providing assistance in promoting the apps, and other benefits . Traditionally, this percentage has been 25% for every app listed, regardless of deployment option. Today, that all changes. For all new cloud apps listed on Atlassian Marketplace after April 1, 2020, we're reducing the revenue take from 25% to 5% for all sales generated in the app's first year. That's right, you now keep 95% of the profits in year one. And no, this is not an April Fools joke! Learn more here . If you already have cloud apps listed on Atlassian Marketplace, you're in luck too. We're reducing our percentage to 15% for those, also effective starting April 1, 2020. You don't need to do anything to receive your increased share. With these new financial incentives, now's the time to start innovating on our cloud products while taking home a larger piece of the pie. The 95% revenue share split is the among the most generous in the industry. Maybe you already created something for a hackathon, or you created something to make your personal workflow easier, and just haven't listed it on Atlassian Marketplace yet. If it's working well for you, chances are it will work for others! Who knows, it might turn into something big. Katrina is the Developer Community Manager for Atlassian's ecosystem.", "date": "2020-04-01"},
{"website": "Atlassian", "title": "Users that use Sign in with Apple may choose to not share their email address", "author": ["Matthew Ho"], "link": "https://blog.developer.atlassian.com/users-that-use-sign-in-with-apple-may-choose-to-not-share-their-email-address/", "abstract": "Atlassian is currently adding the Sign in with Apple social login option for our customers, for both login and signup in our Cloud products. This is a solution from Apple that is focused on privacy and security, with two-factor authentication being mandatory. Although this may prove to be a simpler solution for some iOS customers, it's worth highlighting that this mechanism also provides customers with an option to hide their email . This means that Atlassian will receive an email address that hides the customer’s real email address. When we send an email to this address, Apple will forward the email to the customer’s real email address. The email address that we receive will look like uniquecode@privaterelay.appleid.com These emails are only for Atlassian to use, as we need to register all domains from which we might send emails. This means that Marketplace apps that have access to the customer’s email, can only use the email as an identifier, not to communicate with. Come and talk to us and let us know. For more information about Sign In With Apple, please check out Apple's documentation .", "date": "2020-04-08"},
{"website": "Atlassian", "title": "Jira has Capabilities", "author": ["Celebi Murat"], "link": "https://blog.developer.atlassian.com/jira-has-capabilities/", "abstract": "A while back we were working on improving the performance of the Createmeta REST endpoint . The endpoint was deprecated and we added two new endpoints. But obviously, we couldn't remove the old one right away since that would break the API. The problem was the products (Confluence, Bitbucket, other third-party apps etc.) that integrate with Jira. They are more or less agnostic with regards to Jira versions when they are integrated, meaning they work with a range of Jira versions instead of a one to one version match. Since we wanted customers to see improvement as soon as possible, the change was released with Jira 8.4. We needed to give the integrated products a way to check whether the new and improved endpoints are available for them to use. After some digging, we found the solution we were after was already available: the capability module. The Capability module in Atlassian plugins is a very simple module with two fields: name and url . When you create a capability module, it is registered and when the /rest/capabilities endpoint is called the capability you created is returned. The answer of the capabilities endpoint responds with a list of JSON objects. A single entry will look like this: In a Jira plugin, you can add new capabilities in the atlassian-plugin.xml . In order to get the above entry in the response, you would need to add the following capability module:", "date": "2020-03-26"},
{"website": "Atlassian", "title": "Dev Chat – A look at the new Move & Copy Page APIs for Confluence Cloud", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/dev-chat-new-move-copy-page-apis-for-confluence-cloud/", "abstract": "In our first Dev Chat video, Ralph Whitbeck of the Developer Relations team sits down with Ryan Talusan, Senior Engineer on the Confluence Cloud Ecosystem Team to show us the new Move & Copy page REST APIs in Confluence Cloud. Ralph is a Developer Advocate for ecosystem developers.", "date": "2020-03-27"},
{"website": "Atlassian", "title": "App Week in June has been canceled", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/app-week-in-june-has-been-canceled/", "abstract": "As the coronavirus (COVID-19) situation has continued to evolve around the world, Atlassian has been actively assessing the risk and impact on a daily basis and monitoring guidance from public health authorities. Ultimately, the health and safety of Atlassians, our partners, and your families are our highest priority, so we have made the difficult decision to cancel our upcoming App Week in June. We always look forward to connecting with our Atlassian community at App Week and we are deeply saddened by the tough decision to cancel. In this uncertain environment, we believe this is the right decision to preserve everyone's health and safety. We will reevaluate when we can hold the next App Week as circumstances allow. In the meantime, if you have any questions or concerns, please don’t hesitate to reach out to us in the Developer Community . Ralph is a Developer Advocate for ecosystem developers.", "date": "2020-03-24"},
{"website": "Atlassian", "title": "How Atlassian helps you build apps that meet enterprise security standards", "author": ["Katrina Morales"], "link": "https://blog.developer.atlassian.com/how-atlassian-helps-you-build-apps-that-meet-enterprise-security-standards/", "abstract": "You don't need to look far to find examples of tech companies getting their hands slapped for mishandling user data . Oftentimes the company itself has done nothing nefarious with the data, but through an act of negligence, a third party is able to get access to it and misuse it. In today's world, data is gold, and where there is gold, there are thieves. Atlassian takes the responsibility of safeguarding user data very seriously. We have a number of compliance certifications in place today, and we continue to make concerted investments in this area. The high standards we have for our products extend to our ecosystem; we expect our third party developers who create apps for customers to install in their Atlassian products to take this responsibility seriously as well. Given our large customer base and the built-in infrastructure of the Atlassian Marketplace, there are tremendous opportunities for budding entrepreneurs to build businesses on top of Atlassian. But when you're starting a new venture with limited resources, how can you possibly be expected to build your product AND keep up with all the new security threats that are discovered every day? Atlassian has an Ecosystem Security (EcoSec) team for just this purpose. The EcoSec team is building programs to make it easy for our Marketplace partners listing commercial apps to build them securely from the start and also set them up for future success. This is a win-win for developers because it offloads some of the legwork required to stay on top of security best practices, and in doing so it unlocks potential customers who won't consider purchasing software that hasn't run through some sort of risk and compliance gamut. In this blog, we'll take a look at a few of the things the EcoSec team is doing to programmize our approach to security. When it comes to software that runs outside of the user's four walls, security becomes exponentially more important because of the inherent risk associated with sending information over the internet. That's why the EcoSec team created a minimum set of security requirements that all cloud apps must adhere to while being listed on the Atlassian Marketplace. These requirements address common root causes of security breaches and take the guesswork out of how to be reasonably sure that your app is secure while you are building it. Additionally, Atlassian has set SLAs for security issues to be fixed, which vary depending on the severity of the issue. This allows Marketplace partners to easily prioritize which issues should be fixed first after they have been identified. A step beyond these minimum requirements is our cloud security self-assessment program . The program includes completing the \"lite\" version of an industry standard self-assessment questionnaire called CAIQ. (CAIQ Lite is 73 questions long, whereas CAIQ is 300+.) These questions are intended to identify areas for improvement in regards to data security best practices. For example, do you have a disaster recovery plan in case the servers where you host the data are destroyed? How long will it take to recover the data? The program also includes access to a platform called Whistic , which provides an easy way to interact with the questionnaires and additional support interpreting what the questions mean to prospective customers. It also can be used as a way to communicate your compliance certifications to your current and prospective customers. As a bonus, Atlassian pays for Whistic on behalf of all Marketplace partners in the program. As we mentioned before, certain prospective buyers will require any software they purchase to go way beyond the basics in terms of security. This is where pass/fail certifications come in, such as SOC2 Type 2 and ISO 27001 , which are industry standard for SaaS companies. These types of certifications are granted by independent agencies and ensure that the company is held to the highest standards with respect to privacy, security, confidentiality, processing integrity, and availability. It should be noted that these certifications are not easy to get; if they weren't the certification wouldn't be worth much. But they are essential to landing those enterprise customers we talked about earlier. Also, if you're vying for a government contract, be prepared to go through separate and additional hurdles specific to the regional authority, such as FedRAMP in the United States . Even if you do all the right things from a security perspective while planning and building your app, no software is 100% bullet proof once it's in production. The reality is incidents can and often do happen. One of the most effective ways to increase the security of your app is to participate in a bug bounty program. Bug bounties offer cash incentives to security researchers who find and report vulnerabilities to the participating software company. The greater the potential risk of the vulnerability reported, the greater the payout. This model of outsourcing is particularly cost-effective because instead of paying full time engineers to find these bugs, you as a company only pay when an exploitable vulnerability has been discovered. The beauty of Atlassian Marketplace's bug bounty program is that, as with Whistic, Atlassian pays the platform fees to Bugcrowd for all participating ecosystem developers, saving even more $$$. The EcoSec team started the Marketplace bug bounty program as a trial in July 2019 with four Marketplace partners participating. Tempo , who has been in our ecosystem for many years, was one of the pilot participants in the program. They have a complex architecture which supports not only Atlassian Marketplace apps, but mobile apps, a Chrome extension, and integrations with other services like Google Calendar and Office 365 Calendar. They said of the program, “This has been a great opportunity to work with Atlassian to improve cloud security as it reveals wrong configurations, misalignment between systems, and bugs. It’s made our products better and safer.” In just six months, the bug bounty program identified 277 vulnerabilities across 32 Marketplace apps with an average reward of $480.82 per vulnerability reported. That’s 277 issues identified before customers were impacted! On the heels of this overwhelming success, the EcoSec team has opened up the program to all Marketplace partners and is rolling it into the requirements for being a Silver, Gold, and Platinum Marketplace Partner, along with the above mentioned self-assessment program. All apps, whether for cloud, server, or Data Center, can be listed in the bug bounty. Adaptavist , another participant in the pilot program, has a bit of advice for those looking to join the program: \"Make sure you spend enough time on your scope to determine what is in and out of scope. Spending that effort will guide the security researchers and Bugcrowd engineers so they don't need to rely as much on your engineers.\" If you want even more detail about the thinking that went into these programs, check out this talk at AppSec California by Hari and Jana from the EcoSec team. Whether you’re building your first app or looking for ways to increase adoption of your existing app on Atlassian Marketplace, investing in its security can have a big payout in the end. Our Ecosystem Security team is continuing to evolve our programs to ensure that the work you do in this arena is reflected in your Marketplace listings, helping you stand out in the crowd of other apps. Let us guide you! Katrina is the Developer Community Manager for Atlassian's ecosystem.", "date": "2020-03-05"},
{"website": "Atlassian", "title": "Using Forge to Build Insights into your Team", "author": ["Dugald Morrow"], "link": "https://blog.developer.atlassian.com/using-forge-to-build-insights-into-your-team/", "abstract": "Decision making with the right combination of robustness and speed is a challenge, but improving the process can have a dramatic impact. Within Atlassian, if we need to make a decision, we create a DACI page in Confluence. DACI stands for Driver Approver Contributors Informed which is detailed further in the Atlassian Team Playbook. We love DACIs so much that we even created a DACI Confluence app that simplifies the creation of DACI pages. If you're unfamiliar with the DACI framework , you may like to read up on it, but for the purpose of this post you really only need to think of a DACI as a Confluence page with a structure encouraging the adherence to a decision making process. This post also refers to the term DACI driver which is the person who is taking responsibility for ensuring the DACI process is followed. Since all DACI pages are based on a template provided by the DACI Confluence app and a process outlined in the DACI play , I figured I would be able to improve our decision making by creating an app that helps us follow best practices. After a short amount of development, I quickly realized that the most difficult aspect of developing the app would be the parsing of pages. Consequently, I needed to have a quick development loop whilst I implemented the parsing logic so I ensured the logic was encapsulated by a utility that could easily be unit tested. The combination of unit testing plus the Forge tunneling feature provided me with an efficient development flow, allowing me to implement the page parsing and DACI best practices logic rather quickly. I then added an editor macro to allow the Forge app to surface the results into DACI pages that the app is inserted into. The editor macro displays best practice messages such as: The macro also renders a banner summarising the status. At the time of developing the macro, Forge provided a fairly limited set of components so I implemented the banner by dynamically creating an SVG and including it as the src attribute of the Image component. Using an SVG allowed me to color the banner red or green depending on the number of issues found. This kind of workaround will not be necessary when Forge supports more composability options and possibly higher-order components such as banners and cards. In fact, whilst writing this blog, Forge has released several UI components that would have helped here. When creating the SVG representing the banner, I ran into a problem whereby the SVG would not display. It turned out that this was due to using hexadecimal coded colors in the SVG, but not URL encoding the SVG. The URL encoding is necessary since the SVG is passed in the Image src parameter. The following snippet shows how to create the Forge Image component with an SVG: Speaking of colors, I used the Atlassian design color palette to ensure the banner conformed to the Confluence styles as much as possible. Other aspects of the SVG can also look out of place if left to their defaults so I found I had to explicitly set a few properties such as the text size to 14px and the text stroke-width to 0px. Suffice to say, I would not recommend using SVG to mimic Confluence content elements, but it is a powerful feature that can unlock capability. The DACI Helper app code has been open-sourced at atlassian/forge-daci-helper . You can follow the instructions at https://developer.atlassian.com/platform/forge/example-apps/ to install it in your Confluence tenant, but you will need to be in the Forge beta program first. With the DACI helper app built, I then felt I needed to build awareness of it. Ideally, I would be able to modify the DACI template so that the app's macro is inserted into every new DACI page, however, the app had not yet proven itself useful so I figured that would be a little premature. More importantly, however, the DACI template is a feature of the DACI Confluence app which would be installed in tenants that the DACI Helper app is not installed in so Confluence would display an error indicating the macro is unavailable. Note, however, it is technically trivial to insert a Forge app into a Confluence template and is a great way to distribute apps within a tenant that relate strongly to certain kinds of content. My approach to promoting the DACI helper app involved building another Forge app with the purpose of creating awareness of the DACI helper app. This new app, DACI Helper Promoter , listens to page publish events to detect the creation of DACI pages. Within Atlassian, we use the convention of prefixing all our decision pages with \"DACI:\". The page title is provided in the event payload so this simple prefix check is inexpensive and avoids unnecessary load on APIs, etc. When the app detects a DACI page has been published, it creates a child page that informs the driver of the DACI that they may wish to insert the DACI Helper app in their DACI page. The content of the page provides an introduction to the DACI Helper app and also explains the page was automatically created by the DACI Helper Promoter app. The automatically generated page also mentions the driver of the DACI so they receive an email notification, bringing their attention to the page. Of course, I had to be careful not to notify any given person more than once because this would be very annoying. For this reason, the DACI Helper Promoter app had to keep a log of everyone who had been informed. Forge doesn't yet have its own storage mechanism so the app stores the log using Confluence entity properties . I was also careful with my rollout of the app by initially only enabling it for the Confluence spaces that my team was active in. With growing confidence, I gradually enabled it for more spaces. I was encouraged by receiving a number of likes by various DACI drivers for the auto-generated pages and observing the DACI Helper app's macro being installed in a number DACI pages. This also provided me with some valuable feedback such as a fix to ensure the app supports being inserted into draft pages. This fix involved passing the query parameter status=any when fetching the page content . Although the overwhelming feedback was positive, I subsequently thought of an alternate approach to pursue. At this point I had built code that could: I figured it would not take much effort to refactor the code to build an app that collects statistics about DACI pages. These statistics would hopefully provide some insights into decision-making behaviors and possibly lead to improvements in the guidelines. To avoid complicating the original open-source DACI helper app , I created a new Forge app and copied over the DACI page parsing logic. I called this new app DACI stats . Like the DACI Helper Promoter app, the DACI stats app also has a Confluence trigger that listens for page publish events. Forge supports the subscription to a growing list of different types of events from Atlassian products. Forge's reliable function based runtime provides the ideal mechanism to process events in near real-time. The initial product trigger logic was very similar to the DACI Helper Promoter app and also started its processing by filtering out pages that do not start with \"DACI:\". The product trigger processing then retrieves the page, parses it and records information about the page in the form of an analytics record. Once again, this app stores data using Confluence entity properties . Note that I could have used an external storage mechanism, but I wanted to keep the data within Confluence rather than egressing it to an external database. I used a Forge environment variable to identify the content ID of the page to store the properties against. Forge environment variables can be set on a per environment basis so this approach allowed me to test the app using a different page on my test tenant to the page I intended to use in the Confluence tenant we use at Atlassian for all of our work. The analytics events contain information such as: Each Confluence entity property has a size limit of 32KB, but there is no limit to the number of entity properties that can be stored against a piece of content. For these reasons, I chose to store each DACI analytics event in a separate entity property. With this code implemented, I deployed the app to the Forge production environment and installed it in the Confluence tenant we use at Atlassian for all of our work and it started logging the analytics to content properties. However, the app didn't yet have any features that provided insights or summary level information so I needed to develop a way to summarise data spread over all the content properties. After only a few days, the app had stored over 200 DACI analytics events so it was clear the next challenge was going to relate to the ability to query the data in order to summarise it. This challenge is due to Forge's 10 second invocation timeout combined with the fact that the APIs to retrieve Confluence entity properties are quite limited. A single entity property can be retrieved by its ID and multiple entity properties can be retrieved by specifying an offset and limit. With my ever growing number of entity properties storing analytics events, the app was going to have to make multiple queries to retrieve and process all the analytics events. It was obvious that it would be only a matter of time that this processing would exceed Forge's 10 second invocation timeout. I thought about modifying the app so that it continuously computes the summary data as each analytics event is recorded, however, I didn't have a clear idea about which information would be necessary to summarise. I also figured there would be an ongoing need to process the raw analytics events to provide new insights. In addition, some statistics operations such as computing the median are difficult or impossible to compute in an incremental manner. To solve this, I added a webtrigger to the Forge app to form the basis of a long running task. When the web trigger is invoked with a GET request, it initializes some context for the long running task to execute in. This context is used to store the information necessary to compute the summary data. It also stores information about where the task is up to so that it can retrieve the entity properties in small batches to avoid the chance of any given web trigger processing exceeding the Forge 10 second timeout. At the end of each web trigger invocation, it sends a POST request to the same webtrigger with the body containing the long running task context. Once all the analytics events have been retrieved, the final webtrigger invocation performs the summary computations and stores the summary data in a separate Confluence entity property. Similar to identifying the content ID to store the entity properties against, I stored the webtrigger URL in a forge environment variable so the app could invoke the correct web trigger depending on the environment it was running in. When I was developing the long running task, I was paranoid about creating an infinite loop of webtrigger invocations so I implemented a simple circuit breaker based on a pseudo random number. The processing stops if the number is below a certain value which I set to result in roughly a 2% probability. This was low enough that I could loop through all of my test data reasonably reliably. Sometimes I would see the circuit breaker trip and I'd have to start the task again. Forge has some abuse prevention logic, but this circuit breaker logic was trivial to add and provided peace of mind. After deploying my app to production once more, I discovered the long running task stopped part way through. It turned out that the app was accumulating too much context data for POSTing to the next iteration of the long running task. My development environment only had a small amount of data so I never hit that problem. I fixed this by culling unnecessary data, but I am aware that this issue will likely rise again as the app collects more statistics. A note on trust and environments . To restrict unnecessary access of app developers to personal and user generated data, Forge does not allow app developers to see logs from the production environment, nor tunnel in the production environment. This will soon be bolstered by the introduction of a permissions based scheme that will require apps to declare their intention to use certain capabilities such as being able to egress data. These are important features for building trust in Forge apps by customers, but can be problematic when problems occur in the production environment, but not in the development environment. To minimize impact, ensure the data you test against in your development environment is realistic and covers as many cases as possible. Also ensure your test/development data is similar in terms of quantity/volume where you believe your app may be pushing processing limits. The final step involved adding a macro to the app so that the summary data could be displayed. I developed the macro in my test tenant with the aid of tunneling. Once happy with the macro, I deployed the app to the production environment in seconds with forge deploy -e production, however, the macro had a bug causing it to crash. I found I had to disable the functionality I had introduced until the macro worked again. This process allowed me to narrow down the problematic code. To avoid this kind of problem, make sure your development environment has sufficiently realistic data that covers a variety of use cases. The DACI statistics app has only been running for a short time so there is not yet enough data to provide accurate insights, however, this blog is more concerned with demonstrating how Forge apps can illuminate insights into an organization’s behavior. For this reason, the following should be interpreted as preliminary insights . The app calculates the duration to make a decision by comparing the time that the DACI transitions to the DECIDED state with the time it first detects the DACI being drafted. Since this requires the entire lifecycle of a decision to be monitored, the app has not yet collected enough information to provide informative data. So far, however, the median time to make a decision is about 6 days. An important part of the DACI decision making framework involves defining a date by which the decision is due. The statistics collected thus far indicate that about 4% of our decisions are overdue. The DACI Confluence app suggests the valid DACI statuses are NOT STARTED, IN PROGRESS and DECIDED. The DACI helper app introduces another status of OBSOLETE, however, 31% of DACIs have some other status. One of the more interesting alternate statuses was \"READY FOR INPUT ?\". The median number of options for our decisions is 3, but up to 6 options have been observed in some decisions. Most of our decisions lump the pros and cons of each option together, however, a small number of DACI drivers have gone to the effort of splitting out pros and cons into distinct considerations to ensure each option is classified against each consideration. Confluence does not enforce the DACI process so it is not surprising that breaches of the DACI guidelines exist. On average our DACIs breach one third of the guidelines that are checked. The statistics show that our DACI process is rather loose, but effective at making decisions in a timely manner. There is an opportunity to improve adherence to the process and possibly introduce some kind of change to reduce the number of overdue decisions. Does your organization capture OKRs in either Jira or Confluence? If so, an app could be created which captures and analyses OKR data. If you use Confluence to capture OKRs, you could consider creating a Confluence OKR template and inserting the app in it to ensure it is automatically added to each of your new OKR pages. Use the team health monitors plays to record the health of your teams from time to time. Apps such as the Leadership Team Health Monitor will ensure your various teams capture results in a consistent format which will then allow a Forge app to capture trends across teams. Overall, developing in Forge was productive and a lot of fun. Here's a short summary of the best features of Forge and those areas that I'm sure will improve in the near future.", "date": "2020-03-24"},
{"website": "Atlassian", "title": "Improvements to your Jira 8.8 audit log", "author": ["Magda Michalak"], "link": "https://blog.developer.atlassian.com/improvements-to-your-jira-8-8-audit-log/", "abstract": "The auditing feature tracks key activities on the Jira instance, allowing administrators to get insight into the way the instance is being used. The audit log can be used to identify authorized and unauthorized changes or suspicious activity over a period of time. Jira 8.8 Server and Data Center introduced a number of important changes to the audit log: Additionally in Data Center: As part of the auditing feature, we have introduced a number of new REST API endpoints. You can see the compiled list here . So far, the audit log in Jira, Confluence and Bitbucket's had different layout and functionalities. Now, to help admins switch between products, we're creating a unified experience for all the 3 products. We are working hard for the changes to be available in Confluence 7.5 and Bitbucket 7.2. The audit log helps you become compliant with the NIST regulations (National Institute of Standards and Regulations). If you want to explore the feature a bit more, see Preparing for Jira 8.8 . Enjoy, Your Jira Team", "date": "2020-03-20"},
{"website": "Atlassian", "title": "Acquisitions and venture funding within an ecosystem", "author": ["Katrina Morales"], "link": "https://blog.developer.atlassian.com/acquisitions-and-venture-funding-within-an-ecosystem/", "abstract": "Google \"who acquired who this week\" and you will find a plethora of articles rounding up the latest mergers and acquisitions, notably in the tech sector. Apparently, Apple buys a company every few weeks . Atlassian, too, is no stranger to M&A , but something potentially more interesting than these high profile acquisition stories is the number of companies acquired by other companies within Atlassian's own ecosystem. Atlassian Marketplace is the app store for Atlassian products. The vast majority of the apps are built by third party developers, and many companies have formed around them as a result of the opportunity to tap into Atlassian's large customer base. How big of an opportunity? Well, Atlassian Marketplace just recently passed the $1B threshold for lifetime sales . 75% of this revenue has gone straight into the pockets of the developers who build the apps, and some of them have really made a killing: 50 of the companies that make and sell apps in the Atlassian Marketplace are grossing $1M or more in annually recurring revenue. All of this has made Atlassian's ecosystem ripe for venture capital money and acquisitions. One of the earlier acquisitions was Appfire’s purchase of Bob Swift Atlassian Apps in 2013. At the time, Appfire was a well known Atlassian Solution Partner providing services to Atlassian customers, but the founders were actively trying to become a 100% product-focused (i.e. app-focused) company. Bob Swift had created a range of automation, workflow, and business intelligence apps that made his own work easier as an Atlassian administrator, and as soon as the Atlassian Marketplace launched he listed them publicly. It quickly became a full time job to maintain them, so Bob was glad for the opportunity to gain additional traction and support with Appfire's team via an acquisition. Two years later, Appfire acquired Wittified , another up-and-coming Atlassian Marketplace partner. Wittified's story is similar to Bob Swift’s: before the acquisition, Wittified was a two-person shop with a growing portfolio of apps. They needed help expanding support, marketing, operations, and more — and these were all things Appfire was able to provide. Joining the Appfire family allowed them to help more customers scale up their Atlassian platforms, faster and more effectively. Now Wittified's cofounders, Daniel and Julia Wester, are a year into their new venture: 55 Degrees . On why they started over, Daniel says, \"Atlassian has grown a lot since we started Wittified and the opportunities have grown with it. The state of the Atlassian cloud today allows us the capability to build up a business primarily focused on cloud which is new and exciting.\" As for Appfire , they have grown to nearly 100 employees with over 60 apps in the Atlassian Marketplace and are continuing to pursue additional app and partner acquisitions in 2020 and beyond. This next acquisition is reminiscent of particularly famous Silicon Valley origin stories. Kanoah was founded by two developers in a garage in Brazil who sought to push the boundaries of what Jira can do. They developed Kanoah Tests, Kanoah CRM, and Kanoah Checklist as apps for Jira Server. 18 months after launching Kanoah Tests, they were acquired by Adaptavist and moved to HQ in London. Adaptavist was one of the first app developers to be really successful on Atlassian Marketplace with ScriptRunner, a tool that lets you customize the Jira workflow. Combine that with Test Management for Jira (rebranded from Kanoah), and you have a powerful automated test management framework right within your software planning tool. Since the acquisition, they have been able to serve even more customers by bringing TM4J to Jira Cloud. This wasn't the only acquisition in the test management space. In 2018, SmartBear added Zephyr to its portfolio of test management tools for software teams. Zephyr is a test management tool that works natively inside of Jira, and it was one of the top-grossing apps on the Atlassian Marketplace at the time of acquisition. It even has its own API, ZAPI, for further customization should teams require it. Given Jira's popularity in the agile software development space, a tool like Zephyr was a must-have in SmartBear's tool belt. Justin Teague, CEO of SmartBear, said, \"The acquisition of Zephyr will establish SmartBear as a leader in test management and broaden our portfolio of high-impact, easy-to-use tools, which includes SoapUI, TestComplete, SwaggerHub, CrossBrowserTesting, Collaborator, and AlertSite.\" Gliffy is another top performer in the Atlassian Marketplace. This popular diagramming tool can be used standalone, but its apps for Confluence and Jira have been a key part of its success since its early days as a company. In late 2018, Gliffy was acquired by Rogue Wave , which in turn was acquired by Perforce Software a few months later. Perforce offers tools for all stages of the software DevOps cycle. Because Gliffy helps teams visualize their projects with things like flowcharts and wireframes — a key part of the DevOps loop — it made them (via Rogue Wave) a prime candidate for acquisition. Tempo was an early pioneer in the Atlassian ecosystem. What started as an internal tool for IT consultants to track their time in Jira soon became a top-selling app in Atlassian Marketplace called Tempo Timesheets. Tempo has since developed other tools for Jira around planning and budget management and has its own set of APIs for these tools. Tempo's apps make it easy for companies to manage resources, all with the power and familiarity of Jira, and is used by a slew of household name companies like Disney, Starbucks, and BMW. Enter Diversis Capital, which invests in middle-market companies to help bring them to the next level. In 2018 they acquired a controlling interest in Tempo and are now the majority owners alongside Origo, Tempo's original founder. Diversis co-founder Kevin Ma says, \"Our investment into Tempo and the Atlassian Ecosystem has already been a strong addition to our portfolio. We are excited to see this accelerating early growth and have high expectations for 2020 and beyond.\" Tempo has NPS and CSAT scores in the top decile quarter after quarter. Most recently, Australia-based ThinkTilt secured seed round funding from two co-investors plus the Queensland government via its Business Development Fund, totaling more than $1M USD. ThinkTilt is the maker of ProForma, an app that allows Jira users to create custom forms without having to navigate Jira's custom fields settings. ProForma has quickly been gaining popularity amongst non-technical teams and has the potential to expand to other Atlassian products, and fast . Their secret to rapid development? A single codebase that allows each app to work across all three of Atlassian's deployment options (cloud, server, and Data Center). With this new funding they have been able to expand the team, make improvements to ProForma's UI, and develop a second product which will launch in 2020. As the proliferation of SaaS continues, we are seeing a landscape that is less a collection of monolith companies, independent and siloed, and more an interconnected web of ecosystems. The companies above are prime examples of this phenomenon, particularly those whose apps have their own APIs so that they can create connections with other Atlassian Marketplace apps (talk about meta!). They show that in order to be viable for mergers and acquisitions or capital investment, SaaS apps playing well with other SaaS apps is not a hindrance in today's market, in fact it's a requirement. In addition, several of the examples above show how the founders didn't need to go \"all in\" on their first Atlassian apps; they were able to create them as side projects in tandem with their day-to-day work. That's because Atlassian makes it easy to get started building apps and integrations with our products through comprehensive documentation, support, and a massive community of 25,000+ developers. Get started building on Atlassian today. Katrina is the Developer Community Manager for Atlassian's ecosystem.", "date": "2020-02-24"},
{"website": "Atlassian", "title": "Forging ahead with Atlassian Connect", "author": ["Joe Clark"], "link": "https://blog.developer.atlassian.com/forging-ahead-with-atlassian-connect/", "abstract": "Atlassian recently announced Forge , a new app development platform that introduces powerful capabilities for those looking to customize, extend or integrate with Atlassian cloud products. Forge is currently in private beta, and I encourage you to sign-up for the waitlist if you want to take it for a test drive! The Forge private beta is targeted at customers building in-house custom integrations and apps for their teams. Through this period we'll be working with customers, and eventually existing marketplace vendor partners, to capture feedback and iterate on the platform before we make it generally available. Meanwhile, we're continuing to invest in Atlassian Connect – our existing cloud app development framework that powers over 1,000 cloud apps that are publicly available on the Atlassian Marketplace, as well as thousands of private apps built by customers to address their team's unique ways of working. If you are looking to build a new app or integration for Atlassian cloud products (especially a commercial one), we recommend that you build with Connect. Connect is a stable, secure and feature-complete framework – and we're going to keep it that way. We are continuing to enhance Connect to address key gaps in functionality compared to server apps and add enterprise-ready features. There will always be a need for apps that access data and compute resources outside of the Atlassian cloud, and this core capability will always be part of our app development experience. If you are looking to customize an Atlassian product to help your team or to tinker with what's possible (especially if you are interested in custom Jira automations or Confluence macros), we encourage you try out Forge. It's easy to get started and prove out your idea without having to worry about hosting, authentication and security requirements. We're working to combine Connect and Forge into a single, unified development experience that enables self-hosted and Atlassian-hosted apps on a single platform. This means that any effort invested in Connect today will still pay off in the future. We're excited to share details later this year on how existing Connect apps can start taking advantage of Forge functionality. If you'd like to know more, please join me on the Atlassian Developer Community to discuss this announcement.", "date": "2020-02-25"},
{"website": "Atlassian", "title": "App Week Scottsdale Recap: Cloud App Migration & Trust", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/app-week-scottsdale-recap-cloud-app-migration-and-trust/", "abstract": "During the week of January 13, 32 Atlassians and 76 third-party ecosystem developers descended upon the Wekopa Resort and Conference Center (30 minutes from Phoenix, Arizona) for App Week Scottsdale. When determining the theme of an App Week, we look to our company goals for guidance. Our main mission is to provide the best possible customer experience in our cloud products, and ecosystem is a key part of that experience. We want customers who have enjoyed using Marketplace apps in their server products to continue using them in our cloud products without having to start from scratch because there is no portability of app data. That's why the theme for this App Week was Cloud App Migration and Trust. We brought members of the App Data Migration team to help developers build both data migration paths for customers of their server app to their cloud app equivalent. In addition, members of the Ecosystem Security team were on hand to help developers build with security in mind. Additional teams were on hand to help developers with any questions they may have around specifics of the product they were extending. This included teams from Jira Software, Jira Service Desk, Confluence, Bitbucket, Atlassian Access, and Strategic Partnerships. App Week is an invite-only event for our ecosystem partners. We review each application that comes in and invite only those who specify that they are going to work on the areas that match the theme. We also are looking for the customer impact of each partner based on the number of installs their apps have. For this event, we heavily favored partners with a high number of installs of their server app as well as partners that specified that they were working on app data migration paths. App Week starts on Monday with a series of presentations from Atlassians. For this event, we broke the presentations up so that the sessions before lunch were directly related to the theme of App Week, Cloud App Migration and Trust. After lunch, we had a few presentations from the product teams that were in attendance about Roadmaps, new features, and general evangelism of what's possible with our products. On Tuesday through Thursday, we would start the day with a stand-up. We asked a representative from each company to tell us what they worked on the previous day, what they planned to work on that day, and most importantly, if they were hindered by any blockers. Letting us know what blockers they had allowed us to be able to quickly address those issues immediately after the stand-up. Most of the day after standup is what we call hack time. This is heads-down time where they can work on their code for their app, seek help from Atlassians, or schedule time with specific teams in unstructured breakout sessions. At the end of each day, we organized 45-minute \"firepit chats.\" We crowdsourced topics during the day that would be of interest, and we had Atlassians at each table moderating the discussions. Friday is demo day, where the expectation is that everyone gives a quick 3-minute demonstration of what they accomplished during the week. If they didn't have a demo to share we asked that they come up and share what they learned. The demos at the end of the week are a great litmus test for how much value was driven by the event. In this case, 12 of the companies in attendance were able to show working demos of an app data migration path from server to cloud! Other demos showed net new cloud app concepts or new functionality for existing cloud apps. In addition, we sent out a survey to all attendees to find out how App Week went from their perspective. Our goal for a successful event was to achieve a 4.6 CSAT rating or higher. We asked other questions in the survey to see where we could do better and to understand a little how the week went for them. We reached our success goal and then some with a CSAT score of 4.7 ! Here are some quotes from attendees about the event: “Our team, with Atlassian guidance, was able to create a migration process in about three days, moving most of the data from the Xporter app in a Jira Server instance to Jira Cloud.” – Paulo Alves, Xpand IT “As a Platinum Solution Partner, our work centers around helping our clients get the most out of the Atlassian Tools and apps in the ecosystem. With the dramatic increase in requests for viability evaluations and implementations of Atlassian Cloud migrations, the information gained during App Week will be invaluable in continuing to craft the best solutions for our clients.” – Rodney West, Isos Technology App Weeks are invaluable sources of knowledge for Atlassians as well; many of the teams blog internally afterwards to share what they learned from the ecosystem community. The last App Week we held in June even inspired one Atlassian to change teams: Ecosystem Security's own Matthew Hart! While we did work hard, we made sure to include some downtime during the week. On Monday, we held a happy hour networking event after hours. With some hors d’oeuvres and drinks, attendees got to know one another better. On Wednesday, we took everyone off-site for a western experience. There was barbecue, s’mores , cornhole , and cowboy games all outside in the brisk Arizona night. Interestingly, s'mores was a new concept for many attendees and I wound up teaching them how to make one. In addition to the officially planned events, strategic partnerships manager Michael Lauricella, who lives in the Phoenix area, organized an outing to watch the Phoenix Suns NBA basketball team play. About 20 attendees went to the game, where they received a warm welcome on the jumbotron. We're in the midst of planning the next App Week. No contracts have been signed yet, but we’re targeting June 2020 in Eastern Europe. Mark your calendars and stay tuned for more details! Ralph is a Developer Advocate for ecosystem developers.", "date": "2020-02-19"},
{"website": "Atlassian", "title": "Bamboo 7.0 Early Access Program 2 (EAP) release – with divergent branches", "author": ["Martyna Wojtas"], "link": "https://blog.developer.atlassian.com/bamboo-7-0-early-access-program-2-eap-release-with-divergent-branches/", "abstract": "We're preparing for our Bamboo 7.0 release, which will bring a number of breaking changes . To make sure our app vendors and customers who have written in-house apps are prepared for that release, we're launching the second Bamboo 7.0 Early Access Program (EAP). This is the final one, including divergent branches. Please share any feedback or questions you might have in this post in Atlassian Developer Community forums , and our developer team will get back to you. Links to download installers and archives Release notes Atlassian Developer Community", "date": "2020-02-19"},
{"website": "Atlassian", "title": "Announcement: Reminder about deprecation of xdm_e usage and needing to load all.js from the CDN", "author": ["Anmol Agrawal"], "link": "https://blog.developer.atlassian.com/announcement-reminder-about-deprecation-of-xdm_e-usage-and-needing-to-load-all-js-from-the-cdn/", "abstract": "In October 2019, we announced a major change relating to the hosting location of all.js, the JavaScript library used by all Connect-based apps for Atlassian cloud products. Effectively immediately, all Connect apps are required to load all.js from this CDN location: https://connect-cdn.atl-paas.net/all.js If all.js does not load properly, the result will be a non-functional app experience for the customer. Specifically, all Connect framework functions and API calls made by your app will break. We announced that changes would take place on Feb 1, 2020, and have already started to roll them out into production. If your app is still loading all.js from the tenant URL, then your app is currently at risk of not functioning properly. If your app is built using one of our official toolkits, Atlassian Connect Express for Node.js or Atlassian Connect Spring Boot for Java, we recommend updating to the latest version by following the directions below. There are also directions below for apps built using other/custom toolkits. For Node.js apps built with Atlassian Connect Express: For Java apps built with Atlassian Connect Spring Boot: For apps built using different toolkits, or your own home-grown code: More detailed information about this change can be found on the announcement . If you have any additional questions, please reach out on the Developer Community forums. Anmol is a Developer Advocate for ecosystem developers.", "date": "2020-02-18"},
{"website": "Atlassian", "title": "Track your campaigns: Marketing funnel insights now available", "author": ["Malathi Vangalapati"], "link": "https://blog.developer.atlassian.com/track-your-campaigns-marketing-funnel-insights-now-available/", "abstract": "The Atlassian Marketplace is a diverse ecosystem of vendor partners who build apps and integrations for customers to use in their Atlassian products – these vendor partners could be as small as one person doing everything, or as large as a company with hundreds of employees. Some of them are able to find success using just the Marketplace itself, which provides the tooling and the exposure to reach hundreds of thousands of Atlassian customers from one place. Some, though, want to take it to the next level and actively market their apps outside of the Atlassian Marketplace. They might bid on Google AdWords, place ads on LinkedIn, or start their own blog to drive traffic to their listings. There are usually many touch points involved before a customer makes a decision to buy, and a marketer needs to know what’s working and what isn't. That's why we're excited to release the marketing funnel insights feature for the Atlassian Marketplace – available now. This new feature will let vendor partners attribute evaluations of their apps directly to their marketing campaigns and track ROI. This has been a top request from the Marketplace vendor partner community, and we're happy to finally deliver on it! Here is a quick demo video: Before releasing this feature to the public, we conducted a closed beta with a few Marketplace vendor partners. The initial feedback from it was overwhelmingly positive: We are able to see our campaigns coming through, which is a HUGE SUCCESS for optimizing our marketing capabilities across these apps. This feature is accessible via the license API . The license API now contains new parameters to help you evaluate the ROI of your marketing campaigns, like this: As we look to further enhance marketing funnel insights for Marketplace, we’re working to extend funnel attribution to final purchase. Future considerations include the need for multiple attribution models, granular data on Atlassian category, and expanding marketing tech options to more than just Google Analytics. We will post on the Atlassian Developer Community as we prioritize additional updates to the Marketplace roadmap. For more information on how marketing funnel insights works, check out our documentation on developer.atlassian.com . Malathi is a Senior Product Manager on the Atlassian Marketplace team.", "date": "2019-12-19"},
{"website": "Atlassian", "title": "Bamboo 7.0 Early Access Program (EAP) release", "author": ["Martyna Wojtas"], "link": "https://blog.developer.atlassian.com/bamboo-7-0-eap-release/", "abstract": "We're preparing for our Bamboo 7.0 release, which will bring a number of breaking changes . To make sure our app vendors and customers who have written in-house apps are prepared for that release, we're launching the first Bamboo 7.0 Early Access Program (EAP). This way app developers can update their apps in advance to ensure they will work with the public release of Bamboo 7.0. We want to give you a heads up that a second EAP will be released around 2 weeks before the release of Bamboo 7.0. Please share any feedback or questions you might have in this post in Atlassian Developer Community forums , and our developer team will get back to you. Links to download installers and archives: Release notes Atlassian Developer Community", "date": "2020-01-23"},
{"website": "Atlassian", "title": "Run background scripts in the new Jira issue view", "author": ["Giles Brunning"], "link": "https://blog.developer.atlassian.com/run-background-scripts-in-the-new-jira-issue-view/", "abstract": "We’ve released an issue background script module for the new Jira issue view . This module lets your app add an invisible panel to the issue view to listen for events, poll for changes, or run code that consumes the JavaScript API . As a result, your app can now execute its required functionality without adding unnecessary UI elements. To start using the new issue background script module, take a look at the issue background script documentation . For even more locations in the new issue view, check out new issue view locations .", "date": "2020-02-04"},
{"website": "Atlassian", "title": "Announcement: Bitbucket Server 7.0 Early Access Program (EAP) release", "author": ["Anmol Agrawal"], "link": "https://blog.developer.atlassian.com/announcement-bitbucket-server-7-0-early-access-program-eap-release/", "abstract": "We are pleased to announce the availability of the first Early Access Program (EAP) release of Bitbucket Server 7.0. This EAP release is so that app vendors and customers who have written in-house apps can update their apps to ensure they will work with the public release of Bitbucket Server 7.0. A second EAP will be released 1-2 weeks before the release of Bitbucket Server 7.0, so remember to retest your apps one more time on this release. Bitbucket Server 7.0, due February 2020, is our next platform release and will contain breaking changes . Important things to note about this EAP release: The following plugin points on the pull request page are being replaced with a new client-side extensions API. Please note, equivalent named plugin points not on the pull request page will remain unchanged. Details on how to use the new client-side extensions API on the pull request page can be found in our developer documentation 8 . Extension point discovery mode can be enabled by providing the ?clientside-extensions URL parameter on any pull request page. Once the page is loaded, you'll see the blue, interactive extension points. You can find out more about these new extension points in our community post . As work for Bitbucket Server 7.0 proceeds, the Bitbucket Server engineering team may discover work needed for other, smaller, breaking changes that have not been planned yet. We will continue to update these release notes to bring you the latest information as work progresses. Anmol is a Developer Advocate for ecosystem developers.", "date": "2020-01-15"},
{"website": "Atlassian", "title": "Trust, choice, migration: Atlas Camp 2019 in review", "author": ["Katrina Morales"], "link": "https://blog.developer.atlassian.com/atlas-camp-2019-developer-conference-in-review/", "abstract": "On the heels of our first stop of the Atlassian Open tour , we held our annual Atlas Camp developer conference in Vienna. Atlas Camp is our opportunity to bring the whole Atlassian developer community together and learn from each other. It's where you can learn best practices from the experiences of your developer peers, in addition to Atlassian presenting the latest and greatest advancements in our products that you can hook into. By the numbers: Atlas Camp is always a time for us to reflect on our humble beginnings. Martin Suntinger, Head of Product for Ecosystem, told us about how he got his professional start in Vienna, where he studied at university and met his business partner with whom he built the app Roadmaps for Jira. He told us that they chose Atlassian because it has a platform customers love, it solves the distribution challenge with a large user base, and it is both flexible and powerful enough to build what they envisioned. Today, we know that more than 60% of our customers across server and cloud use an app from the Atlassian Marketplace. And as we look at our largest customers, that number is well over 80%. One customer in the entertainment industry told us they use 101 apps! This appetite for apps (see what we did there? 🙂 ) is reflected in the growth of Marketplace – at the end of 2019, we crossed the $1B threshold for lifetime sales on Marketplace ! The numbers also show that our customers are taking us, and their apps, to the cloud. The cloud side of our ecosystem grew more than 60% year over year, and nearly 150 vendors saw triple digit growth in their cloud sales. Cloud truly is becoming a critical part of many businesses built on Marketplace! Martin also talked about the future state of ecosystem, and what developers can expect to see from us in the coming year. At Summit earlier this year, Atlassian co-founder Mike shared our commitment to making our cloud products deliver the best Atlassian experience for all types of teams. This extends to ecosystem as well. In order to do this, we need to build trust with customers, offer greater choice of cloud apps, and provide a clearer migration path as customers move from server to cloud. To address trust , we've started piloting a bug bounty program with a few Marketplace vendors. With 27 accepted vulnerabilities (including 2 criticals) and $11,600 paid out in rewards, it's proven to be a success so far with the vendors in the beta program, and we're looking to expand it to more vendors soon. We're also working to simplify and standardize the security assessment process, so that customers will better understand the investments you're making to secure their data. To address choice , we are focused on providing more cloud APIs to unlock critical use cases that developers can build for. For example, right now we're looking into configuration management uses cases in Jira and content organization use cases in Confluence. Since 90% of new Atlassian customers start in the cloud, and thousands of server customers are making the switch, we are confident that the demand for cloud apps across a variety of uses will continue to increase as customer growth in cloud increases. We've also held App Weeks to accelerate the development of cloud apps with hands on guidance from the Atlassian product teams, and because of that, more than 40 of our top server apps have made it to cloud since last year. Our next App Week is scheduled for January 2020 in Scottsdale, Arizona. (Details to be announced.) To address migration , we're working on an app assessment report to be included with our product migration assistant tool that will display whether the same app is available in cloud and whether it has feature parity. We'll need to source this info directly from Marketplace vendors, so be on the lookout for an \"app source of truth tool\" where you can provide details about your app availability, including if and when you’re planning to offer a cloud compatible version of your apps. This tool will not only help customers make informed decisions about migrating to cloud, it'll signal to us any challenges that you're facing with app migration readiness. There's even more we're doing to address these things that we believe will provide a greater experience for customers and developers, but you'll have to stay tuned to find out. Watch this space. 🙂 We've shipped new APIs and enhancements to our most popular cloud products. In Jira Software Cloud, we launched two new Connect modules that let apps unleash the full power of Jira workflows. This includes access to new elements like conditions, that check whether a transition should be performed by the user, and validators, that check input made to the workflow transition is valid before the transition is performed. You can read more about this here . We also shipped dynamic modules for Jira, and some are coming soon to Confluence Cloud. Modules can now be registered dynamically in the context of an app installation, giving you more flexibility in defining an app's behavior. For example, when a user gives a value to a field on a project in Jira, you can now index and store those values. In Confluence, we're working on dynamic modules for macros and webhooks, and we are considering adding web panels as well. In Jira Service Desk, we shipped an asset management API. This actually works across Jira Software and Jira Core as well, but we find Jira Service Desk to have the most compelling use case: say an employee’s laptop is constantly rebooting; agents want to know when it was purchased, the model number, OS version, patches applied to it. In addition, they want to be able to pull up a list of previous requests tied to that device. This API makes that possible. You can learn more about it here . In Trello, we added Enterprise actions and webhooks to help customers be aware of events as they occur. Webhooks in Trello are not new, but the addition of the enterprise actions are a great way to easily integrate with IT services so they can trust usage is managed. For example, until recently, there wasn't a good way to get notifications in Splunk about a team being added to a Trello Enterprise account. You'd have to write your own service to regularly poll the Trello Enterprise API. Now you can setup a webhook so that Trello tells you exactly when a team was added to an Enterprise account. And finally, something for everyone: marketing attribution in our Marketplace API. We're now exposing marketing attribution channels through the API so that you can tie an evaluation or conversion to the source of traffic. It gives you critical insights into how your marketing channels and campaigns are performing, so you can spend your money where it matters. This has been a long time coming and we're really excited to finally release it! Check out the details here . Want to watch the rest of the goodness that went on at Atlas Camp? Check out the recorded sessions. Katrina is the Developer Community Manager for Atlassian's ecosystem.", "date": "2019-09-24"},
{"website": "Atlassian", "title": "Confluence Server 7.3 beta is available now", "author": ["Rachel Robins"], "link": "https://blog.developer.atlassian.com/confluence-server-7-3-beta-is-available-now/", "abstract": "This week we released a beta for Confluence 7.3. There are changes in this release that may directly affect 3rd-party apps. New in 7.3 To find out what’s in this release, check out the Confluence 7.3 beta release notes . Get Started Download the beta from our site to start testing your app for compatibility with the changes. Be sure to take a look at our Preparing for Confluence 7.3 guide to find out what you need to do to make your app compatible. We’ll update that page regularly, so keep an eye on it to see any changes. Cheers, The Atlassian Confluence team Rachel is a technical writer on the Confluence Server and Data Center team.", "date": "2020-01-14"},
{"website": "Atlassian", "title": "A milestone moment: $1B in lifetime sales", "author": ["Martin Suntinger"], "link": "https://blog.developer.atlassian.com/a-milestone-moment-1b-in-lifetime-sales/", "abstract": "Today, I'm thrilled to announce that the Atlassian Ecosystem has reached an unprecedented milestone: $1B in lifetime sales. Yes, one billion . Few companies, let alone enterprise software marketplaces, reach a milestone of this caliber. Congratulations to each of you who have helped build the Atlassian Marketplace into what is today! As a former Marketplace vendor myself, I vividly recall the exact moment of our first license sale. That moment had a lasting impact on me and every single member of our team, both professionally and personally. From the humble startup days building a Marketplace app with my team, I don't think I could have imagined how far the Atlassian Ecosystem would come in just a few short years. When I officially joined Atlassian, the Ecosystem team had recently shipped Connect 1.0 and closed out the year with $50M in sales. Since then, this community has delivered hundreds of new apps, grown to more than 1,000 vendor partners, and over 25,000 developers. We ended last fiscal year with $300M in app sales, our strongest year yet. That's $225M paid out to developers – funding thousands of jobs in our communities, including 50 million-dollar businesses in the ecosystem! Your team's contributions to the Atlassian Marketplace and our customers cannot be understated. You’ve solved for an incredible number of customer use cases and have taken our products to places we couldn’t even think of. You’ve connected teams across departments, timezones, and languages, and you’ve saved customers countless hours of work by automating workflows and tailoring our products to fit exactly how these teams do their best work. You are a key reason why many of our customers have become and remain passionate champions of Atlassian. More than 60% of our customers use at least one app or integration from the Marketplace, and as we look towards our largest customers, that number is well over 80%. The work you do impacts the life of tens of thousands of Atlassian customers, and millions of users worldwide. It's an incredible feeling to be on this journey with you. As I look at the opportunities ahead and our big long-term mission to unleash the potential of every team, I feel confident to say: We’re still just getting started! To accelerate this mission together, we’ve recently made another big announcement: Forge . Forge is our new cloud development platform that will empower developers to more easily build and run enterprise-ready cloud apps that integrate with Atlassian products. Forge sets the path for the next stage in our journey together. We can't wait to see what your teams accomplish! Congratulations again to each of you from everyone at Atlassian! 🎉 Martin is Head of Product for Atlassian's ecosystem, including Atlassian Marketplace and the developer platform.", "date": "2019-12-16"},
{"website": "Atlassian", "title": "How we built the Jenkins to Jira Cloud integration (and what YOU can do with it)", "author": ["Christian Rolf"], "link": "https://blog.developer.atlassian.com/how-we-built-the-jenkins-to-jira-cloud-integration-and-what-you-can-do-with-it/", "abstract": "Several weeks ago we launched the Jira Software Cloud plugin for Jenkins, available now on the Jenkins Plugin Index . Similar to our other integrations with CI/CD tools , this integration allows teams to automatically send build and deployment information from Jenkins and display it across Jira issues and boards and query it via JQL. Teams using the integration will have up-to-date information about their Jenkins builds and any deployments on Jira issues with deep links to quickly investigate failed builds or deployments in Jenkins. In order to build an integration between a cloud product and a behind-the-firewall product, we had to build a new way to integrate with Jira Software Cloud that doesn't expose it to security breaches. The solution is a new OAuth credential (2LO) which Jira admins can create in their Jira Cloud site. When created, this OAuth credential is explicitly scoped for additional security, so that it can only be used to send build and deployment information and associate that information with Jira issues. This gives your Jenkins server, operating behind the firewall, a mechanism to securely send data one-way to your Jira Cloud site without having to open up any ports in your firewall. (By the way, this is open source, so if you are interested in contributing to or forking this plug-in, you can head over to our project on the Jenkins GitHub repo to get started .) The OAuth credential is currently scoped to only send build and deployment information via the Jira APIs, but we're excited about the potential to expand the scope of the OAuth credential to enable more behind-the-firewall apps to connect with our cloud products. For example, the API we're working on now includes commits, branches, and pull requests, which would allow customers to integrate Bitbucket Server/GitHub Enterprise/GitLab with Jira Software Cloud. Long-term, we hope to get a point where we can add support for new data types in less than a month, just by specifying the APIs. Stay tuned for more! In the meantime, here is how we built the Jenkins to Jira integration using this new credential . The Jenkins plugin was built using the Jenkins Pipeline API . The plugin consists of the following three components: The plugin provides an additional section on the Manage Jenkins → Configure system screen. It allows you to configure Jira Cloud sites with the corresponding OAuth credentials. Credentials are encrypted and securely stored in the Jenkins credentials store . The jiraSendBuildInfo step has been designed as a post-build action. When added to a Jenkinsfile , it will collect and send current build information to Jira Cloud. The target Jira Cloud site name is provided as the step parameter. The site name parameter is used to retrieve the corresponding Jira Cloud site configuration from the Jenkins Global configuration (described above). To associate Jenkins build information with Jira issues, the plugin looks for Jira issue keys in a branch name, e.g. branch name TEST-123-awesome-new-feature will be parsed and \"TEST-123\" will be extracted as a potential Jira issue key. By default, the branch name is fetched from the Jenkins SCM API. Alternatively, the branch name can be explicitly specified as an additional branch parameter. You can also use the branch parameter to post the build result to a specific Jira issue. To send the build information to Jira, the plugin uses Jira Cloud Builds API . The whole process of sending build information to Jira Cloud looks as follows: The jiraSendDeploymentInfo step has been also designed as a post-build action. When added to a Jenkinsfile , it will collect and send current deployment information to Jira Cloud. The jiraSendDeploymentInfo expects the following parameters: Similarly to jiraSendBuildInfo step, the site name parameter is used to retrieve the corresponding Jira Cloud site configuration from the Jenkins Global configuration (described above). To associate Jenkins deployment with Jira issues, the plugin looks for Jira issue keys in commit messages. Specifically, the plugin uses the Jenkins ChangeLogSet API . For example if the Jenkins ChangeLogSet contains the following entries \"TEST-123 First commit\" and \"TEST-345 Second commit\", the plugin will extract \"TEST-123\" and \"TEST-345\" as potential Jira issue keys to associate the deployment information with. To send the build information to Jira, the plugin uses Jira Cloud Deployments API . The whole process of sending deployment information to Jira cloud looks as follows: There were three main challenges with building the Jira side of the integration: Atlassian is currently in the process of moving all the front-end code into the Jira SPA (single-page application) built using REACT/Redux. When all of a user's navigation within the SPA, the perceived performance is almost an order of magnitude higher. To simplify the process of adding modules to the SPA, we have a lot of internal tooling. We were lucky enough to be one of the first teams to use the latest version. This meant that almost all state-management, analytics, and AtlasKit components were already wired up for use. Thanks to the internal front-end platform, we could focus on quickly shipping a simple design. This is generally a good approach, as it's hard to gauge what features customers will request next. One of the features we worried about cutting was the ability to edit. For example changing the name of an application. But so far, we haven't had any feature requests for it. The single most important aspect of the back-end was security. Before shipping to our initial cohort, we paired up with our security team to get a full audit of the microservice we built. This included ensuring that credentials cannot leak to logs, a common issue to miss if not filtering stack traces on errors. Early on, we decided not to store any credentials in our service. A stateless service with no persistent store is a much smaller target for hacking or security problems. We didn't manage to avoid storing any data, as this would limit our ability to create a full audit trail. However, the data we store is of no value to a hacker, as it only provides a means of traceability. It can't be used to access any private resources. The main way to create integrations with Jira is Atlassian Connect. However, it has two design flaws: In this project, we decided to solve both these problems. Removing the callback means customers can integrate with products like Jenkins that run behind a firewall. This was a must-have for our enterprise customers. The second issue was the security problem of coarse-grained scopes. We solved this by creating a separate OAuth scope for our integration. This means that if the credentials we create were somehow to leak, the data in Jira is still safe. Hence, the only thing that can be found out about Jira issues using credentials is whether or not a particular issue key is valid or not. The only data that can be read, written, or deleted is the data that was written with the credentials themselves. In case of credentials leaking, this data is likely already exposed in the application that's sending data. Our team is quite far along on our microservice journey. Whenever we start a new project, one of our goals  is to not write a single line of code in the Jira monolith. There are four reasons for this: As mentioned before, we created a new microservice to provide the back-end functionality. This service is built in Kotlin on top of Spring Boot, with a single Dynamo table for audit trails. All the functionality is provided by chaining together REST calls to our platform APIs. This meant we could build the core functionality in under 1000 lines of Kotlin. One of the biggest challenges we faced is actually low traffic volume. As the apps are used for system to system integrations, they have a lifecycle of years and many customers never create any. This meant that we had to add frequent semantic monitoring to catch any issues with our dependencies. Otherwise, it may be days between a dependency breaking its API and us noticing the problem when a customer has an issue. We're excited about the potential of the work we did to create this integration between Jenkins and Jira Software Cloud. Together we can unlock so many more use cases for our mutual customers with this new Server2Cloud integration type as we scope the API even further. For now, you can use the OAuth credential to create integrations between your behind-the-firewall CI/CD tool and Jira with build and deploy info, and we'll be sure to update you when more data types become available. To simplify development, here is a separate downloadable API specification . Christian is a Senior Developer on the Jira Software Cloud team.", "date": "2019-12-09"},
{"website": "Atlassian", "title": "Ecosystem support for custom domains", "author": ["Conor MacNeill"], "link": "https://blog.developer.atlassian.com/ecosystem-support-for-custom-domains/", "abstract": "Atlassian will soon add support for Custom Domains to our cloud products (see https://jira.atlassian.com/browse/CLOUD-6999 ). The custom domains feature allows a customer to configure their Atlassian Cloud products (Jira Software, Jira Service Desk's customer portal and Confluence) to be accessible via a custom domain that they own. The custom domain is configured on a per-product basis. For example customers can: In preparation for this feature we will be adding two new fields to the Connect installed lifecycle event payload soon: These fields may or may not be present on any particular installed payload. For Confluence instances, only the displayURL field will be added. For the most part we do not expect apps to require this information. It is provided for cases where apps perform server-side rendering of user visible links so links are in the user's expected domain. Confluence Example: Example for Jira (with ServiceDesk): Initially these fields, when present, will be based on the current baseUrl field. You will only begin to see these containing distinct values as we roll out custom domain support. We have also recently presented a detailed talk on custom domains at Atlas Camp in Vienna (our developer conference). The video recording and presentation slides are available via this link here. If you have any questions about supporting custom domains in your apps, please post them on the Developer Community forums .", "date": "2019-10-22"},
{"website": "Atlassian", "title": "Change notice – Jira Cloud Transition Issue API to support HTTP 409 conflict error code", "author": ["Anmol Agrawal"], "link": "https://blog.developer.atlassian.com/change-notice-jira-cloud-transition-issue-api-to-support-http-409-conflict-error-code/", "abstract": "NOTE: This blog post has been updated to reflect a future change date for this API method, from Oct 31, 2019 to May 4, 2020. The Jira Cloud API does not support simultaneous issue transitions. This helps to avoid storing incorrect or duplicate issue data. Currently, if multiple issue transitions are requested at the same time, only one of those transitions will succeed, and the remaining requests will return a 400 (“Bad Request”) error code. In the case described above, where transitions requested via the Transition Issue API ( POST /rest/api/3/issue/{issueIdOrKey}/transitions) are blocked due to an existing issue transition being executed, we will soon be replacing the 400 status code with 409 (“Conflict”). The reason for switching from the 400 status code to 409 is because the latter more accurately describes the reason for the API request failing. In the case of an issue transition request returning a 409, it means that an existing transition is still being executed, and that your app should employ a retry mechanism. To prepare for this change, we recommend that your apps are capable of handling both the existing 400 error case, and the new 409 error case. After the change takes affect, you should make the appropriate functional changes in the 400 error case, moving any “Conflict” handling to the 409 error case. The change will be rolled out on (or after) May 4, 2020. If you have any questions or need further clarifications about this change, please reach out in the Jira Cloud Development forum in the Developer Community . The changes have already been reflected in the documentation . Anmol is a Developer Advocate for ecosystem developers.", "date": "2019-10-25"},
{"website": "Atlassian", "title": "Applications open for App Week Scottsdale 2020: Cloud App Migration & Trust", "author": ["Katrina Morales"], "link": "https://blog.developer.atlassian.com/applications-open-for-app-week-scottsdale-2020-cloud-app-migration-trust/", "abstract": "The next App Week will be January 13-17, 2020 in Scottsdale, Arizona . This will be a week long event working side by side with Atlassian's product and engineering teams from Jira Software Cloud , Jira Service Desk Cloud , Confluence Cloud , Bitbucket Cloud , and Atlassian Access . By the end of App Week, you'll form strategies and make progress toward closing critical feature parity gaps between your cloud and server apps, learn potential strategies for migrating customer data from one to the other, and learn best practices for security and reliability that will earn the trust of enterprise customers. Due to the limited space, this event is by application. In order to make sure our goals are aligned, the answer to the question \"What app or integration will you be working on and how far along are you in the process of developing it?\" is what determines whether your application is accepted. Each person from the company interested in attending must apply individually. Maximum 3 people per company. Please apply at https://atlassian.swoogo.com/appweek2020scottsdale . Applications will close on December 2, 2019. Everyone will receive an answer to their application by December 9, 2019 at the latest. We hope to see you in Scottsdale! Katrina is the Developer Community Manager for Atlassian's ecosystem.", "date": "2019-10-25"},
{"website": "Atlassian", "title": "State of Atlassian Ecosystem changes to address privacy", "author": ["Alexandra Kassab"], "link": "https://blog.developer.atlassian.com/state-of-atlassian-ecosystem-changes-to-address-privacy/", "abstract": "A quick reminder: why privacy is critical for our users We here at Atlassian believe in working openly because when work is open we unleash the full potential in every team. And with the recent changes to data privacy legislation (i.e. GDPR), we really wanted to understand how privacy affects a team's ability to be open. So we conducted research with customers from around the globe and learned two important lessons: This research fundamentally changed our definition of \"open\". Open doesn't mean all access. In order to be open and to work openly, users need to be able to trust that they have control over their personal data. Only then does collaboration become less about the individuals on a team and more about the work that the team is trying to complete together. Providing users with greater control over their profile information requires changes to our platform. We're introducing a new identity model which centralizes user profile information into a single source of truth – the Atlassian Account. Atlassian Account uses a single global identifier – the Atlassian ID. The Atlassian ID is an alphanumeric string between 1 and 128 characters long and may contain characters such as dash and colon. It is, by design, opaque and safe to store. Legacy identifiers such as username and user key which have been used in Jira and Confluence are neither global nor opaque, so we're removing them from our product databases and Cloud REST APIs. This change requires apps built for Jira, Confluence, and Bitbucket to migrate to AtlassianID as well. In October 2018, we announced the formal deprecation of usernames and user keys from our Jira, Confluence, Bitbucket, and Connect Cloud REST APIs. See here for the announcements in our developer documentation: Per our deprecation policy, we committed to providing a 6 month period of time to complete the migrations and communicated that on March 29, 2019 legacy user references (username and user key) will be removed from those public APIs. To facilitate the migration we updated our APIs to include accountID and added opt-in mechanisms in order to enable testing. Our migration guides also mention changes to user objects which will come as a result of a new feature we're introducing to users in mid-April 2019, the profile visibility control screen. The profile visibility control screen will allow users to hide or unhide parts of their profile. Fields that are currently returned in the user object today, like email address, may not show up (or return null), depending on the user's profile visibility control settings. In addition to the changes we're making to our APIs, we've also added new requirements for apps listed on Atlassian Marketplace and registered on developer.atlassian.com/apps. As of December 2018, apps are required to disclose their data storage practices and add both a privacy policy and customer terms of use agreement. Those that are missing information have been de-listed from Atlassian Marketplace. Cloud apps storing personal data are now also required to provide regular reports on the list of users that have been stored. We've created a new API and service to respond to those reports with information about Atlassian Accounts that have been closed. We expect that when apps receive this information they immediately process data deletion. A trusted experience is something that we're building together. One bad customer experience can reflect poorly on our community as a whole so we will be regularly monitoring and enforcing these new requirements. We started with de-listing apps that have not provided sufficient information on Atlassian Marketplace, and we will continue to monitor the use of personal data and the tools we provide to stay in sync. Alex is a Senior Product Manager on the Ecosystem team.", "date": "2019-02-13"},
{"website": "Atlassian", "title": "Universal Plugin Manager (UPM) is now available for Crowd", "author": ["Brian Keough"], "link": "https://blog.developer.atlassian.com/upm-for-crowd/", "abstract": "In Crowd Server and Data Center version 3.7, we released support for the Universal Plugin Manager (UPM). The UPM makes it possible for customers to access the Atlassian Marketplace from within Crowd, improving integration of Crowd with Atlassian’s app ecosystem. UPM in Crowd will help remove unnecessary friction around app management and improve discoverability of available Crowd features. Check out the release notes below to learn more.", "date": "2019-10-22"},
{"website": "Atlassian", "title": "Confluence Server 7.1 beta is available now", "author": ["Rachel Robins"], "link": "https://blog.developer.atlassian.com/confluence-server-7-1-beta-is-available-now/", "abstract": "This week we released a beta for Confluence 7.1. There are changes in this release that may directly affect 3rd-party apps. New in 7.1 To find out what’s in this release, check out the Confluence 7.1 beta release notes . Get started Download the beta from our site to start testing your app for compatibility with the changes. Be sure to take a look at our Preparing for Confluence 7.1 guide to find out what you need to do to make your app compatible. We’ll update that page regularly, so keep an eye on it to see any changes. Cheers, The Atlassian Confluence team Rachel is a technical writer on the Confluence Server and Data Center team.", "date": "2019-10-11"},
{"website": "Atlassian", "title": "Design your next app with the Atlassian Vendor Sketch Plugin", "author": ["Huw Evans"], "link": "https://blog.developer.atlassian.com/design-your-next-app-with-the-atlassian-vendor-sketch-plugin/", "abstract": "I'm sure most designers would laugh if I approached them in a black trenchcoat & sleek sunglasses, leaned in thoughtfully, and said \"What if I told you I could make you work 3 times faster?\". It sounds absurd, right? I'm sure many of us have been aggressively customising keyboard shortcuts and browsing listicles of Sketch tips just to eke out a couple of milliseconds of extra time here and there. But hear me out for a moment, and let me show you how deep this rabbit-hole goes. We're really proud to introduce the Atlassian Vendor Sketch Plugin. A month or so ago, we rounded up a handful of designers and asked them  to build common Atlassian designs with and without our design tools. We learned that for every hour spent tab-switching between atlassian.design and copy-pasting from sticker sheets, a designer with the Sketch Plugin by their side only takes 21 minutes! Here's how they did it. We launched the Vendor GUI Pack last November — and today it's becoming a lot easier to get started and stay up to date. The Vendor Sketch Plugin automatically supplies and updates the Vendor GUI Pack for you, which now stays up to date with our internal components. That means less hassle and less inconsistencies with Atlaskit going forward! Templates are the start of any design project at Atlassian — they provide an empty background to frame your designs within a real product context. I genuinely can't walk three metres in the office without seeing a print-out of these on a wall somewhere, because they're so effective at visualising new features & concepts. As a bonus, if you work with the Atlassian Marketplace, they're perfect for building screenshots that really show off your product's features. Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Just kidding! But sample data does make your designs look stale and flat. Thankfully for you, we invented a fake company called Beyond Gravity that have the entire Atlassian suite installed, and we have a bunch of their data in Sketch! What's more, we used a neural network to generate a diverse set of employees — that means you can use them, license-, royalty- and obligation-free in all your designs! Have you ever thought to yourself: \"I need a light standard button with an icon on the right and a loading state, with an N70 transparent background\"? Neither, but apparently sifting through our 50+ Sketch libraries can be a hassle, so we invented the Symbol Palette. It's a scrollable, searchable, visual list that gives you the info you need to discover the perfect component for the job. Switching between a design reference website is a thing of the past. With the Vendor Sketch Plugin, all of our text styles & colors are automatically in Sketch for you to use. We even have our own section in the color picker just for the ADG palette. The Vendor Sketch Plugin is written entirely using modern JavaScript using the Sketch Plugin API . We've got async/await, destructuring, arrow functions, React Hooks, Webpack, Babel & Emotion. Since a lot of Ecosystem developers use these technologies every day, it's easy to customise the plugin to your team's needs — think adding sample data from your own product, or adding your own libraries to the Symbol Palette. It's completely open source, and you can find it on GitHub . We launched the Vendor Sketch Plugin at Atlas Camp 2019 . In that talk, we go into a lot more detail about specific use cases and include some video demos that you might find handy. We also have more formal documentation on our roadmap for some point in the future. Watch this space. Click here to download it — once it's installed, we'll periodically push out updates. All other information is on Github .", "date": "2019-10-14"},
{"website": "Atlassian", "title": "Get your kids coding with Minecraft", "author": ["Grace Francisco"], "link": "https://blog.developer.atlassian.com/get-your-kids-coding-with-minecraft/", "abstract": "If you have young children like I do between the ages of 5-10 you’re probably all too familiar with the Minecraft craze and may even have the books and foam toys related to Minecraft in your home. In the interest of trying to find some educational value out of this block building fascination I started looking into how you would get kids interested in coding alongside their Minecraft gaming. We did do some Scratch programming but it really didn’t hold their interest the way Minecraft has so I felt it was more important to find a way to piggy back off that. There are books and classes on modding with Minecraft using Java. I felt that might be too complex for my young kids at this point and stumbled on The Young Person’s Guide to Programming in Minecraft . There’s a ton of valuable content in this guide but I also quickly realized it wasn’t aimed at children in this age group, but felt it would still be valuable to provide some tips for parents wanting to leverage some of this content to get their young kids excited about coding. And for those of you wondering, yes I’ll also be looking to do a pull request to contribute some of this content directly on the ScriptCraft repository . This is an area of the guide which I did not do with my children. At this age I wanted to make sure we could focus on “the fun stuff” and show them the immediate gratification of coding versus setting things up so you can get to the coding part. I’m glad I did that because there were some quirks in setting this up that I had to sort through which would have easily frustrated the kids. Note that we did this on a Mac. Links for the latest: Make sure you are using versions that are compatible with each other else the ScriptCraft plugin will not load properly. These are the ones I used: One basic thing to note is when the instructions tell you to “start the server”, they mean the following: If all goes well you should see: Note that you should see “Found 1 plugin” if you also installed the ScriptCraft plugin correctly. The status ends with a command prompt for the running server. To exit gracefully from the server when you are done, go back to terminal and simply type “stop”. If you don’t do that and close terminal or Ctrl-z the related java process will hang onto the port which CanaryMod was started on – the default being 25565 and cause you problems when you try to start the server again. If you are not a regular Minecraft player as was my case, make sure you know how to navigate around the game. The one thing I could not find in that reference or in basic Google searches was how to open a door without destroying it. I eventually came across a tip to Ctrl + click my way through. Then you can keep your pretty doors on your cottages and other structures :). Strangely the kids were used to just destroying the doors to get into structures. Hmmmm…. When you launch the Minecraft client for the first time follow the instructions to set up for multiplayer. The guide says to use just “localhost” for the connection but you actually have to use localhost:<port> where port is either the default port that is defined for CanaryMod which is 25565 or whatever port you may have alternatively defined in the CanaryMod configuration file. Ok then, the guide refers to posting some text in the commandline. Note that there is a commandline within the Minecraft client itself, as well as in the terminal where you started your CanaryMod server. There will be occasions in the client where it seems like your keyboard strokes are not all going through. My oldest calls it “when it gets glitchy”. That’s a good label. 🙂 You will have to restart your Minecraft client in that case and sometimes also your CanaryMod server. If you don’t want to get distracted with the monsters and other creatures that come out to kill your character, make sure to set your gamemode right away to creative. That can be done both in the config file as well as by typing on the cmdline within Minecraft: /gamemode c . This will help you and your kids focus on programming and playing versus trying to survive. First start with making sure ScriptCraft is working: We started off with the exercises in the guide which had us creating blocks programmatically with a one line call: This created a box made of oak with an id of 5 and with the specified width, depth, and height. This was created in the air because it started from where the cross hairs are pointing. One of the kids immediately asked if we could spawn creatures like cows and horses: These calls were different than the ones we used to create blocks which prefaced with /js not /jsp. Doing some digging on this Walter added this mechanism so that regular players could make some calls intentionally exposed to them. Remember the “op” command? That enables you to make powerful calls using /js which you shouldn’t enable for all players. When it starts to rain or get dark in the game you can also show some quick command prompts to give your child back a clear and sunny day: I found it challenging getting my kids to pause long enough in their Minecraft gaming to do some simple exercises to get them engaged and excited about programming. This is where you spend some time understand what your kids are doing with Minecraft and start looking for examples of things to point out can be done so much faster with just a bit of code. The following are some suggestions for you to get them going: Here’s an aerial view of the result: This clears a large area and can take up to a minute to run so be patient. The kids thought this was wonderfully cool. If you have tall trees or mountains in that space you may end up with some floating remnants in the sky but you can reapply the code in the air as well by flying up to that section and repositioning your crosshairs. We could have also cut to the chase and set up the server as a flat world to begin with but that would have taken some of the fun out. Note that if you try numbers higher than 50 for those dimensions you may end up crashing your server. Ok – so finally you’ve got them interested and asking questions about what else you can do. Now is a good time to explore other calls you can make and even create your own mod. Here are some links to get you going. I’ll be covering our adventures in mod programming in the next post and would love to hear how yours goes too. Ping me on twitter @gracefr and follow us on @atlassiandev", "date": "2016-02-03"},
{"website": "Atlassian", "title": "Retiring IE11 support for Atlassian cloud, server, and Data Center products", "author": ["Emily Chan"], "link": "https://blog.developer.atlassian.com/retiring-ie11-support-for-atlassian-cloud-server-and-data-center-products/", "abstract": "In 2015 Microsoft released Edge as the browser to supersede Internet Explorer (IE). Since then IE has not received major updates, or added support for many modern web standards. Microsoft recently discouraged the use of Internet Explorer as a default browser , and we've also seen a decrease in IE11 usage across our cloud, server, and data center products over time. To allow us to continue to take advantage of modern web standards to deliver improved functionality and the best possible user experience across all of our products, we have decided to end support for IE11. End of support means we will not fix bugs that are specific to IE11, and will begin to introduce features that aren’t compatible with this browser. Your app should continue to be compatible with IE11 (i.e. no loss of functionality, critical IE11-related bugs should be fixed) until the following timeframes: We welcome any questions and feedback on our Developer community post . Thanks, The Atlassian team", "date": "2019-09-24"},
{"website": "Atlassian", "title": "Take a glance at your app in Jira mobile", "author": ["Joshua Carolan"], "link": "https://blog.developer.atlassian.com/take-a-glance-at-your-app-in-jira-mobile/", "abstract": "Apps are about to appear within issues in Jira Cloud for Android . Currently, Jira for web contains app content within an issue's details, showing which apps are connected to the issue and providing it with information. We call this feature a glance . When a Jira issue is opened on Android, related apps will be displayed on the screen. Users will quickly see which apps are providing information to the issue. The glance includes your app's icon, content and status. When the user clicks on the glance, a glance panel will slide in to reveal your app's content. Follow these 3 steps to test your app in Jira Cloud for Android: 1. Go here and enter your Jira Cloud test instance URL . We'll be in touch once we've enabled your instance so you can test your app. 2. Test how your app's content appears in Jira Cloud for Android, including its glance panel (if it has one). Things to note: If you spot a problem, let us know via jira-mobile@atlassian.com with the subject line: Atlas Camp – app test . 3. When you're good to go, let us know (simply reply to the email we sent you), and we'll add your app to the whitelist. Your app will then appear as a glance in Jira Cloud for Android – for all users around the globe! NOTE: Glances on iOS are not currently supported, but are in the pipeline.", "date": "2019-09-10"},
{"website": "Atlassian", "title": "Apps management in Jira Server just got improved", "author": ["Grazyna Kaszkur"], "link": "https://blog.developer.atlassian.com/apps-management-in-jira-server-just-got-improved/", "abstract": "Since Jira Server 8.2, we've fixed several issues that caused performance degradation in large Jira instances when installing or updating the  Marketplace apps. Until now, every update of an app could make Jira slow down or even freeze for some time. This was one of the most impactful bugs on jira.atlasian.com causing friction to Jira users. You can read more about this fix in detail here or look at the charts below, that show how it looked before and after. Jira 8.0 (before) Jira 8.2 (after the changes) Apart from the positive business impact (less friction while updating apps during the workday by Jira admins on large instances) with the changes made by us in the UPM plugin system app developers can optimize how your app is impacting Jira performance. How? Just please follow the instruction below. New atlassian-plugin event: PluginTransactionEndEvent ATLASSIAN-PLUGINS 5.2 Across Jira the following pattern was frequently used: A plugin operation like install/uninstall, enable/disable could trigger thousands of those events. There is a new event in atlassian-plugins which wraps the existing events into a \"plugin transaction\", so a user operation like disabling a plugin (with all its modules) would be wrapped in a single \"plugin transaction\", disabling a single module will also wrapped in a single transaction. New event in atlassian-plugins: So when disabling a plugin with 100 modules you should receive: So now you can base your logic related to plugin changes on the new event: The logic in the event handler can be aware of the type of events which were part of the \"plugin transaction\": The new event is used from Jira Core 8.2.0 . We are planning to apply this pattern in (bundled) plugins in 8.4.x . This event can be used in other products after upgrading atlassian-plugins (>=5.2.2). Soon we are planning to backport this change to the latest enterprise release 7.13.x. There is a contract between Jira and plugins on the PluginEnabledEvent , where Jira internal state should reflect the just-enabled plugin . Unintentionally, with the change described above (in Jira 8.2.0) we broke this contract and fixed it in Jira 8.3.1. Despite the old contract being respected once again, we still recommend plugin developers to switch to the new PluginTransactionEndEvent .", "date": "2019-09-18"},
{"website": "Atlassian", "title": "Ecosystem Roundup – 3LO support in Confluence Cloud, CDN support in Bitbucket Data Center and much more", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/ecosystem-roundup-3lo-support-in-confluence-cloud-cdn-support-in-bitbucket-data-center-and-much-more/", "abstract": "In this post, we round up all recent Ecosystem news. We are happy to announce that Confluence Cloud now supports 3LO (3-legged auth or OAuth 2.0). With this, developers can now allow external applications and services to access Confluence APIs on a user's behalf. For example, if a user has granted a Gmail app access to Confluence Cloud, via OAuth 2.0 (3LO), then that app can interact with Confluence content (for example; pages, blog posts, comments, etc.). This provides the user the power to have more fine-grained controls on what the app can access from Confluence. Please find more information here . Check out the Confluence Cloud 3LO announcement on the Developer Community. Bitbucket Data Center will soon have the ability to serve static assets from a CDN. This includes static resources that are served by plugins. Most Atlassian applications have moved to stateless delivery of JavaScript and CSS resources. Apps that don’t use the new APIs for web-resource transforms and conditions may cause static assets to be cached incorrectly. See Stateless web-resource transforms and conditions to make sure your app is using the new APIs for web-resource transforms and conditions. You can also use the following endpoint to list incompatible plugins: https://<base-url>/rest/webResources/1.0/deprecatedDescriptors As of Jira 8.4 , when you import your data from an external issue tracker to Jira, you can do so in the CSV or JSON file formats only. We have deprecated all product-specific importers, so you cannot use them anymore. Concentrating on CSV and JSON will give us enough bandwidth to improve these two most frequently used importers. Head over to the Developer Community for more information. As we were about to release Confluence 7.0, we thought it would be a good time to share what goes on behind the scenes as a complement to all the information already available to you at Preparing for Confluence 7.0 . So here are a set of routine updates and best practices our teams follow in order to maintain and nurture our code-base. Head on over to the Developer Community to take a look at all the details. Every year, at Atlassian Summit, Atlassian recognizes Marketplace Vendors for their accomplishments. This year we have 6 awards including a new award to recognize developers that go above and beyond in support of the community. Head over to the Developer Community to learn more about the awards and to nominate a developer contributor in one of the 6 awards. As always, if you are seeking help in building your apps there are three avenues available to you. If you seek help, mentoring, or support in building your app please head over to the Atlassian Developer Community where over 8000 developers and over 900 Atlassians are available to help out. Need Marketplace Vendor support, head over to the Atlassian Ecosystem Marketplace Vendor Support Desk . Ralph is a Developer Advocate for ecosystem developers.", "date": "2019-09-24"},
{"website": "Atlassian", "title": "Java debugging at scale: when rare events become commonplace", "author": ["Robbie Gates"], "link": "https://blog.developer.atlassian.com/java-debugging-at-scale-when-rare-events-become-commonplace/", "abstract": "As an individual contributor at Atlassian, one aspect of my job is resolving bugs in production. This can be a challenging and enjoyable form of problem solving. I recently helped understand and fix an intermittent problem with Jira Cloud. It was an interesting tale, not just because of the root cause we finally uncovered, but also because it displayed a few interesting aspects of this process to me: Suppose I toss a fair coin – there is a 1 in 2 chance that you'll see a head. If I toss it twice, what are the odds of two heads? It's 1 in 4 (= 2 x 2). Three heads from three tosses? 1 in 8 (= 2 x 2 x 2). Thirty heads from thirty tosses in a row? 1 in 1,073,741,824 – about one in a billion. This seems pretty unlikely, right? However, what if I do this about 10 times per millisecond? That is, about 10,000 times per second. That works out to about 10,000 x 60 x 60 x 24 = 864,000,000 times per day. In this case, you'd expect to see the thirty tosses all come up heads on about 80% of days, or about 5 or 6 times a week. In other words, events that seem rare as a once off, aren't rare at large enough scales. As mentioned in the introduction, this story is about an intermittent problem with Jira Cloud. It happened about once a week, give or take. Infrequently enough to be weird, but repeated enough to warrant investigation. The symptoms were very odd. It started with a stack overflow. Once one of the nodes in the Jira cluster experienced this stack overflow, that node got them in huge volumes – for example nearly 14,000 in a minute. The only way out was to kill the node. The stack, when we got it, had a repeating section that looked like I've compressed the package names here, the first and last frames are from classes in a com.atlassian package, and all the other frames are from open source or JDK code: sun.reflect , java.lang , org.springframework , and org.eclipse.gemini . The only Atlassian code is the first and last frames, and even then they're actually CGLIB generated proxies – an implementation detail of crossing OSGi boundaries between components (called bundles ) of the Jira monolith. If you inspect the code for UserBeanFactory.buildUserBeans , it doesn't call itself, and so we didn't have an explanation for the apparent recursion above. These OSGi proxies are used to bridge class loaders between bundles. When one bundle wants to call a method ( UserBeanFactory.buildUserBeans in this case) in another bundle, it calls a method on a proxy. This proxy locates the correct OSGi service in the other bundle, and forwards the call with a bit of class loader special sauce. Or, at least, that's what it is supposed to do. In this case, what it actually does is locate itself, and call itself, which results in an infinite regression and a stack overflow. Of course, it didn't reproduce when I ran Jira locally, even when I ran it integrated with running staging services and with request URLs that mimicked those seen in the production logging. Whenever I tried, it correctly looked up the right underlying OSGi service. After a bit of debugging the happy path through the code, I had a theory that the symptoms could arise if the correct service had been removed, based on the way the code handled fallbacks when the correct service was missing. OSGi is a dynamic system, which allows services to be registered and unregistered at runtime, which in turn allows for code to be upgraded at runtime. This is how we upgrade bundles in our server products, providing runtime upgrades for server plugins. In cloud, we don't upgrade bundles at runtime, rather we redeploy the monolith to upgrade code. So this bundle unregistration should never happen in Jira Cloud, but perhaps our server legacy was resurfacing in some error path? To investigate this, I added logging for any changes to OSGi services after startup. A fair bit of service wiring goes on as Jira starts up, but it should all stabilize before we start serving requests. In fact, we saw a little tail of additional registrations, either because the logging is turned on a bit early, or we are a bit lazy during startup. After some waiting, the problem happened again, and the logs contained: A type 4 is an UNREGISTERING event according to the constants in org.osgi.framework.ServiceEvent . However, we didn't know why this was being called – I had neglected to include a stack. A coworker jumped in to add logging for the stack trace for this case. After a bit more waiting, we were rewarded with: Interestingly in this case the stack overflow didn't occur, and the service isn't the one we saw before. However, it was an unregistration, and it came with a stack, which led me to a code path we have that implements request scoped beans. It iterates over a collection of destructionCallbacks to destroy request scope beans at the end of the request. Now, neither UserBeanFactory nor PermissionSchemeBeansFactory are request scoped, so why are they being destroyed? So we added logging again, this time to log calls to the code path responsible for registering the callbacks which were implicated in the stack above, that is, the code which added entries to the collection of destructionCallbacks . My reasoning was that somehow the wrong beans were getting into that collection, and this would flag them. We immediately saw in the logs the registration of a known request scoped bean dealing with URI context, so I was confident the logging was correctly flagging registrations. Eventually, we saw an unexpected UNREGISTERING event again. Excited, I went back to find out how it had managed to get into the destructionCallbacks – and drew a blank. No logging. That’s right – we never registered the bean for destruction, and yet we had a stack trace calling the destruction logic. This was quite surprising. At this point, I knew something very odd was going on. The logging for the adding of the callback was only seeing expected calls, and yet the destruction logic was unregistering unexpected beans. How could this happen? It led me to examine the original destruction stack again – very closely. Somehow, I reasoned, this could be called for the wrong bean. Something, somewhere, must have been mistaking destruction of the request scoped bean for destruction of the OSGi services. Frame by frame I walked the code, keeping forefront in my mind \"something must be getting the wires crossed\". Finally, I found what I was looking for in Spring Scanner – a spring extension we use to make it easier to write Atlassian plugins using annotations. The code in question was: Here, exporters is a Hashtable<Integer, OsgiServiceFactoryBean> – it maps beans (via their identityHashCode – more on that in a moment) to the service factory that created them. This code in Spring Scanner is called from a Spring DestructionAwareBeanPostProcessor implementation, specifically its postProcessBeforeDestruction . The way this works is that each Spring BeanFactory is notified that a bean is about to be destroyed. This allows the factory to take action if needed. Spring Scanner uses this to remove OSGi services when plugins are uninstalled in Server. So what is identityHashCode ? It returns the same result as Object ‘s implementation of hashCode for the provided object. Why on earth is this code using that? To understand identityHashCode , we need a little history. My understanding of the background is that early in its evolution, Java ran only on 32 bit machines. It didn't use a copying garbage collector, and so objects occupied the same address in memory for their lifetime. In this world, there's a nice cheap implementation of hashCode – you return the pointer to the object, as a 32 bit integer. This is a great hashCode . It's stable – it doesn't change for the lifetime of the object. It's also unique for the lifetime of the Object . Different objects have different hash codes. Note that this doesn't happen for most hash codes – generally they lose information. However, in this special world, that doesn't happen. This meant that you could use hashCode to remember Object identity without retaining the object . This gives a very convenient way to track objects you have seen without risking complications with garbage collection. In a world before weak references, this was very useful. So useful, in fact, that System.identityHashCode let you get this value, even if the class of the object provided its own hashCode implementation. However lots has changed since then. We now have copying garbage collectors, and so in fact you need to store the hash code on the object so that it is stable. We have better solutions (weak references) to garbage collection complications. However, most importantly for this story, we moved to 64 bit JVMs. When this was done, the API for identityHashCode was not changed. A 64 bit JVM can make more than 2^32 objects. Lots more. This means that identityHashCode can no longer be unique, no matter how you implement it . When I learnt this, it was called the pigeon hole principle . If you have more than N pigeons, and you need to put them in N pigeon holes, then at least two pigeons end up in the same pigeon hole. If we have more than 2^32 objects, and we try to fit them into 2^32 hash codes, then at least two objects end up with the same hash code. As an aside, if you dig into the JVM source, my understanding is that in fact only 31 bits get used, but the basic problem is the same. For many of the requests to Jira, we create a request scoped bean for handling URI context. This is the bean that I found when I added the last batch of logs. Every request that finishes destroys this bean. When this happens, we tell Spring Scanner that the bean is being destroyed. Spring Scanner checks if it is one of its OSGi beans by comparing the identityHashCode . If it matches, it destroys the OSGi service. But wait – what are the odds of that? You need what is effectively a random Integer (the hash of the request scoped bean) to match that of an OSGi service. Two 32 bit integers lining up at random? That's like flipping 32 coins in a row and seeing all heads. That's a four billion to one (against) chance. Jira processes a lot of requests. I did a quick search of the logs of our production Jira clusters on our Splunk log index, and found around 10 million events in the default 15 minute window. Or 40 million requests an hour. Or nearly a billion requests a day. Our one in four billion odds should happen about once every four days. Did I mention the bug was hitting about once a week? Ok – so the numbers are a bit loose here. There's a lot of OSGi services in fact – thousands. There are lots of requests, but not every single request creates request scoped bean. However, lots do, and so the orders of magnitude check out. The final proof of course was to fix the root cause. A colleague fixed Spring Scanner , we rolled that out to cloud Jira, and the symptoms stop. We also connected the root cause (incorrect deregistration of OSGi services) to other intermittent hard to track down bugs, and those were also rectified. By the way, it's not just Atlassian that has this kind of code. It's also in current Spring code : However, I think it's possibly traversed rarely – that is, only at startup and shutdown – for typical usages of Spring – and hence much less likely to cause problems there. As discussed above, this story took me down an interesting path of discovery and problem solving. I think one can learn a few things from this story, including some general observations about the nature of problem solving in a large and complex software system: Rare events happen at scale. Our scale is increasing over time, and we should expect to see rare events more frequently as we scale up. We’ve got a long way further to scale, but we are already seeing one in a billion events daily. Debugging some things will take iterations of logging in production. Jira is large and complex, and we won't always be able to reason about its behavior. Sometimes we'll need to add code to determine what is actually happening in production. Dead ends are also information. I was really surprised when the last logging came back empty handed. I was pretty sure we were on the right track, and I really expected to see some weird corner case accidentally request scoping the wrong thing. However, this just meant back up, and re-examine what you know. Robbie is a Senior Architect on the Jira Cloud team and has a background in academic mathematics and software engineering.", "date": "2019-09-04"},
{"website": "Atlassian", "title": "Jira 8.5/Jira Service Desk 4.5 Enterprise releases: EAP available now", "author": ["Alex Lewis"], "link": "https://blog.developer.atlassian.com/jira-8-5-jira-service-desk-4-5-enterprise-releases-eap-available-now/", "abstract": "The EAPs for both the Jira Software 8.5 and Jira Service Desk 4.5 Enterprise releases are here! While the 8.4/4.4 versions are still being finalized, there are no breaking changes added between these versions and the Enterprise release versions so we are able to provide early EAPs to help you ensure your apps are ready. Many of our largest customers are looking forward to Jira 8.5/JSD 4.5 Enterprise releases as their first enterprise-ready opportunity to adopt the Jira 8 platform. We're anticipating high adoption of these versions across our largest and most demanding customers, which may mean increased load across mission-critical apps. Read more in about what's changed since the last Enterprise Release in our Changelog and download the EAP here . If you have any questions or feedback, feel free to add them to the community post here . Best, The Jira Team", "date": "2019-09-04"},
{"website": "Atlassian", "title": "Q&A with a self-taught developer turned app entrepreneur", "author": ["Katrina Morales"], "link": "https://blog.developer.atlassian.com/developer-turned-atlassian-marketplace-app-entrepreneur/", "abstract": "Vitalii Zurian is a self-taught developer who started building apps for Atlassian Marketplace as a side project in 2014. He is now a full-time app entrepreneur who has 10 apps listed on the Atlassian Marketplace under the company name Lizard Brain. His apps have been used by hundreds of thousands of users and have generated over $1.2 million in lifetime revenue. Vit shares his full story in the video below. Here are some highlights from a recent Ask Me Anything session he hosted on the Atlassian Community: I started making apps as a side project with no intention of substituting income from my job. In fact, I initially listed my apps for free because I was scared of losing potential customers. While it might result in less customers at first, a paid app attracts high-quality users who believe in your features. Customers that are willing to purchase a license validate the need for your solution. Furthermore, charging for your app really motivates you to not disappoint your customers. This motivation, especially when I was just starting out, is what drove me to babysit my Atlassian apps for hours at night when I got home from work. When you receive feedback from a paying customer, it gives you an extra push to get the job done. I ultimately took it full-time not out of necessity but because I saw an opportunity. As my Atlassian Marketplace apps experienced success, I envisioned becoming an independent developer with a business that was more lucrative than my day job. I set a revenue target of 20% above my salary, or about $10K monthly recurring revenue, and upon hitting that target I took the leap and committed myself full-time to app development. I came up with the name Lizard Brain randomly, inspired by a book I was reading at the time, Seth Godin's Linchpin. In it he talks about the concept of the \"reptilian brain\" that we humans have. There is no deep reasoning behind the name honestly, I just liked how it sounded (and it is also kind of funny when German authorities have to pronounce it within a serious setting, e.g. when discussing a tax declaration or some other legal aspects). The best apps solve real problems that customers face during their typical work week. One of the methods that I use to identify these user pain points is by reading through the Atlassian Community forum. If I encounter a question without a resolution, I brainstorm ways that an app could fix the underlying problem. I also look through existing apps for inspiration, and when I encounter negative reviews, I imagine ways to create a new app with better functionality. Also, I'm always on the lookout for popular apps and services that don't currently have an integration with Atlassian products. We are an organization of two people and we have 10 apps listed on the Atlassian Marketplace. In my experience, customer support is closely related to app development, and I use customer feedback to fix and improve apps. Therefore, in many cases a support request turns into a feature request, which leads to the development of a feature and its subsequent release. With that in mind, I spend around 3/4 of my time maintaining existing apps and the rest on developing new ones. Naturally, the more apps you have the more time consuming it becomes to maintain them, so having my business partner – who's a very talented developer himself – really helps. We actually use Jira Service Desk for support, so shout out to the JSD Widget team as I really like the JSD Widget user experience! We are a cloud-first vendor so server to cloud migrations are very welcome! However, we have some server/data center listings too which comprise about 10-15% to our revenue. Server apps normally require a lot more customer support, therefore if you are a fresh developer I would recommend starting with a cloud app, as cloud apps are technology agnostic. You don’t have to know Java (though it's good if you do!) to make a cloud app. It could be done in JavaScript or PHP or Ruby or Python or even in bash for that matter! Focus on the needs of the customer. As developers, we often obsess over picking the right technology and trying to perfect our solution, but it's important to not overlook the most important variable in the equation — customer need. If you spend lots of time perfecting an app that is not needed or well received, then much of the time you invested has gone to waste. Let's say you’re building a Google calendar integration and you initially focus on putting together the code required for the integration, which simply creates a calendar time entry linking back to a Jira issue. In theory, you could spend an equal or greater amount of time polishing the app. There's potential for numerous new features: changing the title, adding assignees, using react or angular so there's no need for page reload, configurable reminders, etc. I suggest developing the essential features of a solution and then starting work on another app. When you receive customer feedback, you can implement new features as needed or requested. Don’t get discouraged by negative reviews. People with negative feedback tend to be much more outspoken than people with positive feedback. If you receive a negative review, don't take it personally and view it as an opportunity to improve your product. I really believe in communicating with my customers, being open to their suggestions, and implementing changes. Don't be afraid to reach out to the person behind the review and talk to them about the issue. You can work together towards a solution to their problem, and the result will often be a loyal, satisfied customer who changes their negative review to a positive one! Get help from Atlassian's developer community. I'm a self-taught developer and I was building apps on Atlassian as a one-man team until recently, so I'm very comfortable walking through problems in my own head. That being said, I've always had support from Atlassian's developer community through the online forum or in-person at Atlassian developer events (e.g. Atlas Camp , Developer Day , App Weeks ). If you hit a wall when building an app, chances are that somebody else has overcome a similar problem. There's a ton of really talented developers building apps on Atlassian's platform that can help you along the way. Katrina is the Developer Community Manager for Atlassian's ecosystem.", "date": "2019-08-27"},
{"website": "Atlassian", "title": "What Bitbucket learned from migrating its unit testing tool", "author": ["Maciej Adamczak"], "link": "https://blog.developer.atlassian.com/lessons-learned-bitbucket-unit-testing-migration-jest/", "abstract": "More than four years ago, Stash Bitbucket transitioned from running Javascript unit tests by Java to Karma. Today, I can announce that we switched our test runner to a modern tool called Jest ! Before Jest, our testing framework was a combination of different tools and helpers wired up together: Each of them serves a different purpose and is mandatory to write unit tests. The problem with that setup is that you need to maintain the custom integrations and this can cost you time and a lot of headaches. As you can guess, the complexity of this solution was evolving over time. There is no one to blame for that. A few years ago there was no other way to even think about running unit tests inside NodeJS only. Karma and PhantomJS were the only solutions if you wanted to think about unit tests for your Javascript code. Those days are gone. We have more mature tools and, most importantly, more flexibility in how we want to provide and to run our unit tests. Thanks to the jsdom project we can run the code that works in the browser without a browser. It wasn’t just hype over a new shiny Javascript tooling. We had requirements that our old setup was not meeting: Let's say you have code that depends on one of the NPM packages. If you are lucky enough the package is published as a bundle with the UMD. Then if you want to wire a unit test in Bitbucket you only need to update a karma.conf.js file and provide the proper mapping between the NPM package name and a file location inside the node_modules directory. If for some reason you need to include multiple entry points from the NPM package, you need to repeat this step manually for every entry point you are using. It's manual work that you can easily forget after you install a new NPM package or if someone else overlooked that. With Jest, you don't need to worry about that anymore. Jest will resolve the modules for you based on their names and configuration. With our Karma setup that was not possible. Atlaskit is shipping their components with only two formats, CJS and ESM, and soon they will ship only ESM. We need either AMD or UMD for our Karma runner. This meant that Bitbucket was not able to load and run the ESM module without transpiling it. This can be solved with the help of parable package . You could manually whitelist some of the node_modules directories and transpile them using Babel. Additionally, you need to either transpile all the internal modules into AMD or UMD format so the browser can load them and run during runtime. Atlaskit is written with a modern ESM code, so you will also need to update the karma.conf.js file and provide the mapping between all of the entry points and transpiled file paths. A few weeks ago I wanted to write a unit test for one of the components that is using Atlaskit. I started investigating the problem and thinking about a possible solution. At the end of this day, after spending a few hours manually providing the mapping I decided to hold on for a while and stop. It was a total waste of my entering time. I didn't want to provide a missing integration between Karma, parable, babel, Atlaskit and webpack or browserify because I will only increase the complexity even more. At this point, I was close to finalizing the Jest migration and decided: now or never. We need to start using Jest. Now! One of the beneficial features of Jest is the ability to debug tests during the runtime. You can run the tests in the watch mode with —watch flag. When the watch mode is enabled the test will rerun every time you change the test or if you change the code that your test depends upon on. Let's say you have a unit test T1 that depends on module A and B , the module B depends on C and D , and module D depends on E and also test T2 depends on module E . Every time you make a change in test T1 or T2 or any of the modules like, e.g. module E all the tests that are depending on it will be re-run. Thanks to that you can have instant feedback and check if the changes you introduced are not breaking the test. What is even more important is that you can debug the test run thanks to node debugger. You can hook up Jest with the IDE and debug your tests in the editor. How cool is that? Recently Google introduced a new project that is a standalone node debugging environment. It's built on top of Chrome DevTools UI that can be used to inspect and debug the code just like in regular DevTools. Personally, I found this really useful because you can have the same debugging experience as you do when you are debugging the code in the browser. After we migrated to Jest, our testing framework stack was simplified from many tools into only two: This is way simpler, isn't it? For now, we still run our unit tests with a QUnit, Sinon and Squire syntax but this is the temporary solution, and the plan for the near future is to use a codemod to migrate the syntax to Jest. I started working on the migration to Jest in May 2018. During the breaks between projects, I was working on fixing the current unit tests so we could run them in Jest. It was tedious work that took me almost a year. I created more than 20 pull requests before finally merging the last one that enabled Jest on our Continuous Integration and on a local development environment. There were plenty of different problems that had to be fixed, for example leaking data between the tests. Jest is using a sandbox mode and runs each spec in a separate context. Karma, on the other hand, runs all the tests in the same context. If your test depends on a mocked module or requires providing WRM dependency, it's easy to overlook that when you are working with Karma. The most challenging problem was: How can we migrate our tests to Jest in iterations without having two different test runners over the migration period? After all, we still had to deliver the new product features and ensure our code is not broken. The solution was to write a QUnit-like-jest bridge. Based on the QUnit documentation I provided exactly the same API and syntax but with Jest working under the hood. When the test was trying to run and execute QUnit.test('something') code, it will work with two different runners at the same time. The problem with two different incompatible test runners was solved! … mostly. We were able to get rid of the custom framework and remove plenty of code from our repository. Precisely, we removed more than 8k lines of code from our codebase. We still need to migrate the old syntax with a codemod. The migration to Jest unravelled one additional problem: our test coverage for Javascript is pretty low. Before we migrated to Jest, we had 73% aggregated coverage for our front-end code. This number includes the coverage collected for the test code which, in my opinion, is not a correct configuration. “In computer science, test coverage is a measure used to describe the degree to which the source code of a program is executed when a particular test suite runs.” https://en.wikipedia.org/wiki/Code_coverage Have you ever thought for a second what it means to provide a 100% code coverage for your source code? Does it mean that every line, every \"if\" statement, and every corner case is tested? What about the case where you have two files A and B that are not depending on each other and your tests are covering 100% of the tests for file A but you don't have a test for file B . Does the coverage still equal 100%? Or maybe it’s 50%? Those are the questions you should ask yourself when you measure the code coverage: I would say the second option is more accurate if you want to stick to the facts. If you don't have any tests for file B , it means it's not covered, right? Additionally, you shouldn't include a test’s runtime in the code coverage score. Those are not part of the application source code and don't give you much value when you think about the big picture of your project. (By the way, one of our Marketplace vendors wrote a great post on how to use JaCoCo as a code coverage tool for Scala .) Sometimes you need to face the ugly truth. And the fact was our coverage level for Javascript code was not acceptable. It was below 50% if you still wanted to include the code from tests itself. The real coverage we should talk about was below 30%. It means that every time I click the merge button in my pull requests, I'm only 1/3 confident that the code works as expected. That's a pretty low ratio, don't you agree? You might be thinking right now, \" Hey, Maciej, our CI is all green and shiny, so what do you expect from us? \" That's true; unit tests are not the only single source of truth. It doesn't mean we are doomed or that we need to call out the engineering health sprint and start writing Javascript unit tests. We have plenty of browser functional tests that are covering the gaps, but unfortunately functional tests are quite expensive and hard to maintain. Just a reminder: in the test pyramid, those are the ones located at the very top of it, while the base of the pyramid is built from the unit tests. It means providing the unit tests at the early stage of the development process can save you not only a lot of time but also a lot of $$$. Migrating to Jest was the right thing to do, even though it seemed daunting at first. By iterating the changes over time, we avoided the pain of a big-bang type migration. Having Jest in place enables us to write unit tests more efficiently. Now we can be more productive while using new modern tooling. Our level of code coverage for Bitbucket Server's front-end codebase is not awesome, but at least we can start improving that! Maciej is a Senior Developer on the Bitbucket Server team.", "date": "2019-08-26"},
{"website": "Atlassian", "title": "Resolve tickets in Jira Service Desk faster with our asset management API", "author": ["Amaresh Ray"], "link": "https://blog.developer.atlassian.com/resolve-tickets-in-jira-service-desk-faster-with-our-asset-management-api/", "abstract": "IT teams have a lot on their plate. They have to juggle multiple tasks constantly, from troubleshooting uptime issues to supporting users with hardware problems. Jira Service Desk offers IT teams an effective tool to deliver a great experience to their customers. In order to resolve tickets, agents often need more context. For example, when a laptop is constantly rebooting, agents want to know when it was purchased, the model number, OS version, patches applied to it, etc. In addition, they want to be able to pull up a list of previous requests tied to that device. With Jira Cloud's new asset management API, it's now possible to store and view asset information in Jira Service Desk Cloud, letting agents and end users link assets to tickets for additional context. This helps IT teams speed up issue resolution by cutting down the back and forth between agents and the end users to get the relevant information. This API is not limited to Jira Service Desk. Jira Software and Jira Core users will also be able to view and link assets to issues from an assets custom field. Using the asset management API, developers can push objects into the asset field in Jira Cloud. This can be done by calling: Assets which have been pushed to Jira can be retrieved by calling: These objects can then be linked to a Jira issue. The platform provides the ability to display additional information about the linked asset on an issue via an assets panel . In addition, admins can apply filters to restrict these custom fields by type. This is helpful if you want to restrict a field to list only assets that belong to a certain asset type (e.g. only show me assets of the type \"laptop\"). When defining an asset, you can also specify an assignee , which we map back to Jira Service Desk end users: This means end users can select assets that are assigned to them or are unassigned on the portal when raising a request: This API is available now, so you can start using it today to build a custom solution for your team. Looking for a solution that has already been built using this API? We have a handful of partners with apps and integrations already listed on Atlassian Marketplace , ranging from open source asset management tools to enterprise configuration management databases. Amaresh is a product manager on the Jira Service Desk Cloud team.", "date": "2019-08-20"},
{"website": "Atlassian", "title": "Site rename feature for Cloud customers – an update for app vendors & FAQs", "author": ["Victoria Li"], "link": "https://blog.developer.atlassian.com/site-rename-feature-for-cloud-customers-an-update-for-app-vendors-faqs/", "abstract": "Last year, we published a blog regarding the development of our new site rename feature, including information on the impact of this feature on your Cloud apps. The feature allows customers to rename their site from foo.atlassian.net to bar.atlassian.net . Since April, this feature has been available to customers via submitting a Support request. We action a site rename request based on the criteria that the ecosystem apps installed on a customer's site are compatible with our feature Over the past few months, we have been reaching out to vendors to verify the compatibility of their apps with our feature. This is an ongoing process and we have listed vendors who have confirmed with us on this public ticket here ( CLOUD-10809 – Ecosystem vendors that are compatible with the Cloud Site Rename feature ). If you want to verify your apps’ compatibility with us, want to test the feature on your site or have questions, please reach out to us via here. We are actively developing a self-serve option for customers and the site rename feature is expected to be available to customers via the Admin UI in the near future. Concurrently, we are working on enabling customers to have custom domains. The custom domains feature enables customers to configure their Atlassian products to be accessible via a custom domain e.g. the customer can change the site URL from foo.atlassian.net to jira.foo.com . We'll be engaging with vendors soon on the impact of this work. What is required to be compatible with site rename? To recap from our last blog, we have decided to leverage an existing lifecycle method , “installed”, to update the installation and send a new payload with the new base URL of the site. Everything else will be the same, and the payload will be signed with the existing shared secret. This change will have an impact on your app when: Your app can use the existing site name to reach Jira or Confluence APIs for some time period. These requests will be slower due to redirects so we do recommend apps update promptly to the new site name. Please also note that your Connect app must use an HTTP client library that can handle redirects.  However, if you have used the site name as some sort of key in your data storage, you will need to make changes to be compatible with our feature. I'm using Atlassian-Connect-Spring-Boot or Atlassian-Connect-Express, am I compatible with the feature? If you are using the storage mechanism provided by Spring Boot or Atlassian Connect Express and have not implemented your own storage layer, you will be compatible with the feature. All versions of spring-boot are compatible. As an example you can have a look at this piece of code from Atlassian Connect Spring Boot: LifecycleController.java As you can see whenever an ‘installed’ callback is received all the payload is used to update the record in database. On top of updating the record you need to make sure that: When a site gets renamed, does the tenant's client key change? No, the tenant's client key remains the same. Only the base URL is changed. Are you actively renaming customers? Yes, we've been actively renaming customers since April. Customers and Vendors can request for their site to be renamed via a Support request . Does the old site URL redirect to the new one? Yes it does. We retain a history of all site names used by the customer. Upon accessing an old site URL, the customer will be redirected to the new site URL. How long do you retain the site name history for? We are currently retaining site name entries from the customer's history to enable redirects and support customers transition from their old site name to their new one. However, in the long-term we will review the data around the usage of our feature and determine when we would start releasing these previously used site names. Is there a limit on how many times a customer can rename a site? The current limit is five renames per site. If you require more renames for testing purposes, please let us know .", "date": "2019-08-19"},
{"website": "Atlassian", "title": "Announcing atlassian-connect-spring-boot 2.0.0", "author": ["Einar Pehrson"], "link": "https://blog.developer.atlassian.com/announcing-atlassian-connect-spring-boot-2-0-0/", "abstract": "Atlassian Connect Spring Boot, the officially supported Atlassian Connect Java framework, provides a Spring Boot starter for building Atlassian Connect apps for Jira (Software, Service Desk, and Core) and Confluence. We’re happy to announce that version 2.0.0 of atlassian-connect-spring-boot is now available via Maven Central. This release primarily includes an overdue upgrade to Spring Boot 2.0. See below an extract from the project changelog . As an aside, you can use artifact-listener.org to receive notifications of every new release.", "date": "2019-08-22"},
{"website": "Atlassian", "title": "Change Notice – Padding update for Jira Cloud navigation UI", "author": ["Peter Van de Voorde"], "link": "https://blog.developer.atlassian.com/change-notice-padding-update-for-jira-cloud-navigation-ui/", "abstract": "We are increasing the spacing between Jira's navigation sidebar and vendor applications by 20px. This is a short-term fix to ensure that our sidebar does not interfere with how vendors want to build their embedded products. We are in the process of updating the padding of the navigation UI so that there is more flexibility in how 3rd party apps want to build their product interface. 20 pixels will be added to the sidebar so that there is increased spacing between Jira's navigation UI and the vendor app's respective UI. This is purely a cosmetic change and should not impact any functionality built by vendor applications. This change will impact all apps who are currently utilizing Jira's navigation UI. We will start rolling this out incrementally on 22 Aug 2019 and expect to be fully released by the end of August. Please keep in mind that this is a short-term solution. We are still thinking about how we might approach this from a long-term perspective. If you are a vendor that currently uses the sidebar, we'd love to work with you to better understand your needs and requirements. Please reach out to us directly at mtse[at]atlassian[dot]com.", "date": "2019-08-20"},
{"website": "Atlassian", "title": "Why Atlassian uses an internal PaaS to regulate AWS access", "author": ["Robin Fernandes"], "link": "https://blog.developer.atlassian.com/why-atlassian-uses-an-internal-paas-to-regulate-aws-access/", "abstract": "Atlassian has an internal Platform-as-a-Service that we call Micros . It is a set of tools, services, and environments that enable Atlassian engineers to deploy and operate services in AWS as quickly, easily, and safely as possible. The platform hosts over 1,000 services that range from experiments built during our ShipIt hackathons, to internal tooling supporting our company processes, to public-facing, critical components of our flagship products. The majority of Atlassian Cloud products are either partly or fully hosted on Micros. Despite its crucial responsibilities, Micros is a relatively simple platform. The inputs it takes to deploy a service are just a Docker image containing the service logic, and a YAML file – the Service Descriptor – that describes the backing resources that the service needs (databases, queues, caches, etc.) as well as various configuration settings, such as the service's autoscaling characteristics. The system takes care of expanding those inputs into production-ready services, ensuring operational functionality (e.g. log aggregation, monitoring, alerting) and best practices (e.g. cross Availability-Zone resiliency, consistent backup/restore/retention configuration, sensible security policies) are present out-of-the-box. We haven't invented much here: nearly everything Micros offers is achieved by using standard AWS features. With this in mind, it is common for engineers to question the need for such a platform: couldn't we simply open up plain, direct AWS access to all teams, so that they can use AWS's full and rapidly-expanding functionality? This is a great question which we'll explore below, focussing on the following points: AWS's breadth of Cloud infrastructure features is extensive to say the least, and only a subset is made available to Atlassian engineers via Micros. This discrepancy isn't due to Micros being unable to \"keep up\" with AWS. The limitation exists primarily because we believe in the value of reducing technical sprawl, especially at the bottom of our stacks. There are fantastic benefits that manifest when teams across Atlassian use infrastructure in a consistent manner. To name a few: These economies of scale are a consequence of using a consistent, controlled interface to provision and manage services in AWS, combined with sensible bounds on the vast array of AWS features available. Both would likely be degraded if direct access to AWS were the norm. Let's take a closer look at how Micros achieves some of the above benefits. Services on the PaaS can be represented in a simple and homogeneous way: highly available compute combined with whichever backing resources the service needs. However, if you look beneath the covers, you'll see that the platform has provisioned much more, thereby enforcing sensible defaults for autoscaling, alerting, security, backup policies and more. So, we've chosen a strategy which favours a controlled, consistent and hardened subset of AWS features, over free-form access to AWS accounts. This strategy has some trade-offs. For example: Let's look at some mitigations to these trade-offs. If teams would like to experiment with AWS features to validate they fit their use cases, or just to learn more about them, we generally point them to the Training Account. This account has some notable restrictions: it is not connected to the rest of our internal network, and it is purged on a weekly basis. Nonetheless, it's an ideal \"playground\" to experiment, validate assumption and build simple proofs of concept. The above isolated experimentation is valuable but can only go so far. Fortunately, there's a range of ways in which the PaaS can be extended. Many resources that Micros provides are integrated to the platform via a curated set of CloudFormation templates. Teams can branch our repository of templates and add their own CloudFormation templates , which can immediately be referenced by Service Descriptors and therefore be provisioned by services in development environments. This allows for the resource to be tried and tested, and for us to examine in detail what would be required to make the resource available to all teams in production. Acknowledging that not everything one may wish to provision alongside a service is best represented with a CloudFormation template, Micros also accepts extensions in the form of service brokers that implement the Open Service Broker API . In other words, teams can contribute services which may themselves become part of the provisioning flow for subsequent services, by deploying and managing new types of  assets defined in the Service Descriptor. Building and running such services is no small undertaking, and we take care to ensure this extension point isn't used as a vector to pump out PaaS features that don't have a high level of ongoing operational support. In practice, we have used this functionality primarily to decompose the core of the PaaS, and to scale the PaaS team. For example, this extension point enabled us to spin out an independent sub-team that owns all the platform's data stores and their associated tooling (including the components that provide their provisioning logic), and to give our Networking team greater autonomy in the implementation and ownership of the platform's integration with our company-wide public edge. Aside from expanding the range of resource types that are available to services, some teams need to extend the PaaS by adding to the components that run alongside their service on their compute nodes. To this end, the platform offers a concept of Sidecars – shareable binaries that service owners can add to the set of processes that are spun up when their service starts . These have been used to provide additional diagnostics functionality, local caching for performance and resilience, a standardised implementation of staff authentication, and more. While we value consistent infrastructure, we understand that sometimes the boundaries of the PaaS are at odds with other factors that put teams under roadmap pressure. Therefore, we sometimes bend the rules of the platform to unblock teams. All such cases start with a ticket raised on the Micros team's Service Desk, and often involve a face-to-face discussion so that we can align on the cost/benefit, risks and ramifications of the exception. Once implemented, most exceptions – especially those that could be risky if used without fully considering the implications  – are kept behind a feature toggle so that only specific teams or services can make use of them. Examples include sticky sessions (which we discourage by default to avoid resilience issues brought about by unnecessary statefulness), or the ability to target a specific Availability Zone to achieve affinity with a database for latency gains (thereby trading off on the resilience benefit of our being spread across 3 Availability Zones in production). We keep track of all exceptions, and periodically review them to ensure they don't stay in place longer than necessary. In many cases, the requests represent temporary relaxations of the platform rules for specific services, so that service owners can get stuff shipped. In other cases, the requests are an indication that the platform boundaries need to shift – and they therefore evolve into broader feature requests. There is no mandate at Atlassian that says services must run on Micros. In fact, there is a well established channel for teams to obtain their own, separate AWS accounts that they manage independently. This flexibility comes with additional responsibilities and considerations, which echo the list above describing where Micros helps. Here are the questions teams must consider before going off-platform: All services deployed to the PaaS automatically get a record in our internal service directory \"Microscope\", which presents essential information about services in a concise and discoverable manner. It's worth noting that while Micros adds some functionality and integrations around plain AWS, I avoid referring to it as an \"abstraction\", because it is intentionally leaky: Micros very deliberately exposes the details of the underlying AWS infrastructure that it provisions and manages. For example, services descriptor contains fields very similar to those you'd find in an equivalent CloudFormation template. Once resources are deployed, you can examine them directly via the AWS Console. You can get tokens to interact with them via the standard AWS CLI. From your code, you can use the standard AWS SDKs. We use standard AWS constructs to enforce security policies. Because engineers use most AWS features directly, many enhancements to existing resources (such as DynamoDB Transactions ) become available to you as soon as they land in AWS. By and large, everything works as documented in AWS docs. It is rare that we add layers of in-house invention between Atlassians and AWS. (An arguable trade-off is that this coupling with AWS would make a theoretical shift to another cloud provider more difficult. We believe that the concrete benefits we achieve now by avoiding heavy abstractions outweigh the hypothetical efforts we'd need for such a future migration.) The above helps explain why free-form, direct access to AWS is not Atlassian's current platform strategy, and that having an internal PaaS is valuable… even if it sometimes feels like it gets in the way! However, while the PaaS is valuable today in its current form, it cannot stand still. There are two main drivers that will continuously push our platform forwards: the evolving needs of Atlassian engineers, and those of our Cloud customers. The first driver, the needs of Atlassian engineers, means we will keep improving the developer experience we provide, and increasing the speed at which engineers can innovate & get their job done. This involves reducing the operational burden on developers, and polishing the dev loop. We're implementing a range of features on that front today, including improved Lambda support to reduce the amount of boilerplate code required for services that primarily react to AWS events, adding Kubernetes as a compute environment to speed up deployment & scale-out time, supporting more pre-configured combinations of backing resources that solve common use cases (such as DynamoDB tables pre-configured to stream to Elasticsearch), and externalising a range of common service-to-service communication concerns to a service mesh layer. The second driver, the needs of Cloud customers, means the platform has a key role to play in Atlassian's duty to continuously strengthen our Cloud products’ security, reliability and performance. These cross-cutting concerns cannot be delivered upon at the top of the stack alone. The platform will assist by delivering monitoring improvements to maintain our visibility into our systems as they grow ever more sophisticated, more consistent and centrally observable rules for service to service authorization, and better mechanisms for data classification across our fleet of services. Even if you only have a few services for now, it isn’t too early to start thinking about how a service platform can help your fleet evolve and scale. Here are some points to think about early on: Robin is a Dev Manager who has worked on various aspects of Atlassian Cloud over the past 8 years. Today, his focus is on the platform that accelerates Atlassian engineers and enables them to concentrate on the unique needs of their customers.", "date": "2019-08-14"},
{"website": "Atlassian", "title": "The REST APIs you’ve been waiting for – The new JIRA Service Desk and JIRA Software REST APIs", "author": ["Andrew Lui"], "link": "https://blog.developer.atlassian.com/new-jira-service-desk-and-jira-software-rest-apis/", "abstract": "We’re delighted to announce that JIRA Service Desk now has a public REST API. This complements the JIRA Software REST API that we released with JIRA Software last year. These new APIs make it easier to quickly build integrations and Atlassian Connect add-ons for JIRA Software and JIRA Service Desk. The JIRA Service Desk REST API is currently an experimental release, and will eventually become a stable release. The JIRA Software REST API is a stable release already. See our REST API policy for more details. Check out both REST APIs below: You’ll be happy to know that the JIRA Software REST API and JIRA Service Desk REST API have a range of resources for interacting with JIRA Software or JIRA Service Desk respectively. Although they don’t have everything, we think we’ve covered the main scenarios in this initial release. Here’s a few examples of what you can do with the new REST APIs: You can find all of the reference information you need in the REST API documentation linked above. Both REST APIs use the new documentation theme which makes them easier to scan, with expandable sections for resources. We’ve also written a guide to exploring the JIRA Service Desk domain model via the REST API , which should be helpful to all Service Desk developers, new and old. Happy integrating! If you need help with the new REST APIs, try posting a question on Atlassian Answers or log a ticket ( JIRA Software or JIRA Service Desk ). Note, if you are currently using the private JIRA Agile REST resources, please read the following: If you have an Atlassian Connect add-on — Be aware that the resources that are marked as private in the JIRA Software REST Scopes have been deprecated. These private resources will no longer be whitelisted on the 1st February 2016. You must migrate to the new public REST API before this date. Note, there is feature parity between the whitelisted private resources and the new public REST resources. If you have a P2 add-on — Be aware that the private REST resources that were whitelisted, have now been deprecated. You can continue to use these resources, but we recommend that you migrate to the new public REST API. Note, feature parity does not exist between these private resources and public REST API, and we cannot promise that there will be feature parity in future.", "date": "2016-01-15"},
{"website": "Atlassian", "title": "Confluence Server 7.0 Beta is Available Now", "author": ["Rachel Robins"], "link": "https://blog.developer.atlassian.com/confluence-server-7-0-beta-is-available-now/", "abstract": "The first beta for Confluence 7.0 is available now. This is a platform release, so there are breaking changes that may directly affect your app. New in 7.0 To find out what’s in this release, check out the Confluence 7.0 beta release notes . Get started Download the beta from our site to start testing your app for compatibility with the changes. Be sure to take a look at our Preparing for Confluence 7.0 guide to find out what you need to do to make your app compatible. We’ll update that page regularly, so keep an eye on it to see any changes. Cheers, The Atlassian Confluence team Rachel is a technical writer on the Confluence Server and Data Center team.", "date": "2019-08-16"},
{"website": "Atlassian", "title": "Atlas Camp 2019: Vienna – Agenda Unveiled", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/atlas-camp-2019-vienna-agenda-unveiled/", "abstract": "Atlas Camp , Atlassian's premier developer conference is coming to Vienna, Austria 🇦🇹 on 11-12 September 2019. For many of you that was all that was needed for you to buy your ticket however some of you need a little more convincing. I am really excited to announce the agenda for Atlas Camp 2019. We'll have 4 tracks of great content from both Atlassians and ecosystem developers. Two general sessions, forty 40-minute talks, twenty 20-minute talks, over 60 Atlassians on-site, hands-on labs, and more. Let's dig into the details. This year we'll present our traditional keynote talking about all the things Atlassian's ecosystem developers have accomplished, as well as talk about new features that are coming. In addition, we'll be holding a day 2 general session to go deeper on those new features. From the agenda: A Special Announcement Day 2 | 9:00 – 9:50 AM Join us on Day Two for a very special Atlas Camp exclusive. Join Principal Product Manager Joël Kalmanowicz and Engineering Lead Tim Pettersen for a sneak peek of something new we've been working on. After the session, Atlas Camp attendees will be invited to take part in a hands-on preview of some new features for our ecosystem developer platform. You'll definitely want to be in Vienna to experience this! We'll have the content that you're looking for in one of these familiar tracks: New this year is the Experience Design track. After a very successful Experience Design focused App Week last year we wanted to bring some of that content to Atlas Camp. We'll have Experience Design talks on Day 1 only. In addition to the amazing content already listed, we have a full day 2 of track content that we'll release during the Atlas Camp Keynote . You won't want to miss it. This year’s agenda is jam-packed with great content. Let's look at a few talks that the content committee is particularly excited about. Preparing for Data Residency and Custom Domains Nuwan Ginige | Principal Product Manager, Atlassian Day 1 | 13:00 – 13:40 | The Atlassian Platform Atlassian customers have long requested the ability to control where they host their content in Atlassian Cloud. They've also long desired the ability to configure their cloud products to be accessible via a custom domain. These features are coming soon to Jira and Confluence Cloud! What will this mean for Marketplace app developers? Join Nuwan Ginige, Principal Product Manager on the Cloud Platform team, as he walks through how the evolution of Atlassian's cloud platform has shaped the development of these capabilities. Learn how these changes will impact Marketplace apps, and how you can get involved in app vendor early access progress before general availability. Updates on the Data Center Apps Program Benjamin Magro | Product Manager, DC Apps, Atlassian Day 1 | 14:20 – 15:00 | The Atlassian Platform In this session, Benjamin Magro, Product Manager for Data Center team, will cover updates to the Data Center Apps program that will affect vendors in 2019. This will cover an introduction to our new performance testing framework, changes to the performance testing requirements, as well as additional questions that will form part of the architectural review being added later this year. Efficient Use of Atlassian Cloud REST APIs Raimonds Simanovskis | Founder and CEO, eazyBI Day 1 | 15:30 – 16:10 | Advanced App Development You've developed a data-intensive app which uses many Jira or Confluence Cloud REST APIs. It worked fast for small test instances, but now your first large customers with large data volumes report your app has become slow for users. You may need to optimize the way you use Atlassian REST APIs. In this session, Raimonds Simanovskis from eazyBI will discuss how to efficiently use Atlassian REST APIs to minimize the wait time for your users. He'll present examples of real-life performance improvement use cases, while discussing how to monitor and measure REST API requests, and make fewer requests; when and what to cache; how to use persistent connections; how to use parallel requests and parallel pagination, and handle pagination inconsistencies; how to request only data you need, and expand additional data; how to handle errors and retry requests; using webhooks instead of REST APIs; and how to identify when data has changed. Integration Testing on Steroids: Run Your Tests on the Real Things Jörg Brandstätt | Senior Developer, Resolution GmbH Day 2 | 11:20 – 12:00 | Advanced App Development At AtlasCamp 2018, Jon Mort and Mark Gibson from Adaptavist gave a presentation about how they brought Arquillian into the Atlassian SDK. In this talk, Jörg Brandstätt from Resolution will help you to put their learnings into practice and take your tests to the next level. He will also share how Resolution is using this approach to test some of the Top 30 Server & Data Center apps. The session covers how you can run your test code on remote Server and Data Center instances with different databases ad-hoc from within your IDE and during the build process, and provide detailed information about how to set up Maven to execute your tests within your Bitbucket build pipelines. Building Apps With Enterprise in Mind Pawel Wodkowski | Senior UX Designer, Atlassian Day 1 | 15:30 – 16:10 | Code and Beyond Designing for the enterprise comes with a unique set of challenges; ensuring readability and accessibility at scale, meeting the needs of multi-layered organizations, and building a trust when your software – used by dozens of thousands of employees – is considered mission-critical. At Atlassian, we’ve spent countless hours digging deep into our enterprise customer’s needs and we’ve gathered a vast repository of insights. In this talk, Pawel Wodkowski, a senior designer on Jira Server, will share all that we’ve learned from our research (while not being shy about busting some of those wild admin myths!). You’ll get a crash course in what it means to design for scale the Atlassian way. Not All Heroes Wear Capes: Skills and Tools Helpful in Becoming a Support Superhero Paweł Mazur | Product Manager, Spartez Software sp. z o.o. sp.k. Maria Heij | Support and Test Manager, Refined Day 2 | 14:20 – 15:00 | Code and Beyond “Don’t #@!% the customer” and “Play, as a team” are two Atlassian values that resonate with Support Teams. Support teams’ goal is always to help the customer – but sometimes being passionate is not good enough. They need a superhero’s toolbox to get the job done! In this session, Paweł Mazur, Product Manager at Spartez, and Maria Heij, Test & Support Manager at Refined, will show how you and your team can build an efficient and rewarding support experience for you and your customers. We’ll cover both the technical tools (like scripting, Docker, browser dev tools, and more!) as well as the team processes which you can use to turn your support team into a team of superheroes. Nailing Measurement: a Framework for Measuring Metrics that Matter Josephine Lee | Product Manager, Atlassian Day 1 | 14:20 – 15:00 | Experience Design When it comes to designing apps and new features, we just can’t get enough of metrics. In an age where we can collect data from almost anything, how can we cut through the noise and focus on the right metrics to measure the success and failures of the apps that we're building? Join Atlassian Product Manager Josephine Lee as she delves through what exactly makes a good metric. Throughout the talk, we'll walk through real Atlassian examples of good and bad metrics. By exploring a framework for measurement, we'll cover detailed features that showcase how best to measure and choose the right set of success, supportive, and counter metrics. You’ll walk away with tips and learnings from Atlassian's approach to measuring success, and learn how to use data and metrics to inspire action in your apps. It's was really hard picking just a few talks to spotlight this year. They're all fantastic! Go check out the full agenda and let us know in the Developer Community the session you're looking forward to the most. If you've ever been to a developer conference before you know the real value in Atlas Camp is the conversations and relationships built at the event. This year we will have plenty of opportunities to meet others. Breakfasts, happy hour, hallway track, sponsors, informal meeting spaces, and an information booth to help you find the right Atlassian to talk to. We're excited to see you in Vienna next month. But we can only see you if you register today . Ralph is a Developer Advocate for ecosystem developers.", "date": "2019-08-13"},
{"website": "Atlassian", "title": "Make work flow with the new Jira Cloud workflow APIs", "author": ["Krzysztof Kercz"], "link": "https://blog.developer.atlassian.com/make-work-flow-with-the-new-jira-cloud-workflow-apis/", "abstract": "Workflows are at the heart of Jira, governing everything that happens with issues. Over the last few months, we've been working hard to bring the full power of workflows to apps and integrations. This post provides an overview of all the new APIs that were recently added. (Note that all of this applies only to workflows for classic projects. Next-gen projects will receive their own set of dedicated APIs in due time.) For a long time, Jira Cloud enabled apps to provide custom workflow post-functions , but that was the only extensible part of workflows. We've now launched two new Connect modules for other types of workflow rules to let apps unleash the full power of Jira workflows. The following set of workflow extension modules are now available: But there is more. We've also created a new REST API for managing workflow rule configurations , to give apps full control over their extensions. You can now retrieve the configuration of all your rules added to workflows, and even update it at runtime, without any interaction on the user side required. The new resources are (click the links to go to the documentation): (By the way, if you're wondering what a \"workflow rule\" is: we are using it as an umbrella term for different types of workflow elements: conditions, validators and post-functions.) Getting a list of transitions available for a given issue or project is a common requirement, however, for a long time it was not easily achievable. This changes now with the new workflow search resource , which allows you to retrieve any Jira workflow, along with its transitions, statuses, and even status properties (so that you can check if an issue is editable in the given status). See the full documentation here: GET /rest/api/2/workflow/search (Get workflows paginated) . Even the best API for retrieving workflows would be useless if you couldn't connect workflows with associated projects and issues types . Hence, we prepared a new API to retrieve workflow schemes for a given set of projects. Read about it here: GET /rest/api/2/workflowscheme/project (Get workflow scheme project associations) . As an example, let's see how to get all transitions for a given issue . First, get workflow schemes for the issue's project by calling: This returns a list of workflow schemes associated with the provided project, so in this case just one entry, for example: Now, we need to check what type our issue is, and get the correct workflow. Let's say the type of our issue has ID 10000 . We can see in the issueTypeMappings property value of the workflow scheme object, that the workflow used by the issue type is named \"scrum workflow\". We can now retrieve this workflow along with its transitions by executing the following request: We believe the new workflow API is already fully viable and has enough features to cater for most use cases, but we are still looking forward to hearing your feedback. This is why we keep the new resources marked as \"experimental\". Let us know what you think on our community forum .", "date": "2019-07-24"},
{"website": "Atlassian", "title": "Ecosystem Roundup – Jira Cloud Deprecation notice and Confluence Cloud Change notice", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/ecosystem-roundup-jira-cloud-deprecation-notice-and-confluence-cloud-change-notice/", "abstract": "In this post, we round up all recent Ecosystem news. We are deprecating the ability to disable time tracking in Jira Cloud via REST APIs on 16 June 2019. Support for these APIs will be removed after 16 Jan 2020. We are making changes to the existing /wiki/rest/api/search REST API to not support user search starting 20 Jan 2019 . We have introduced a new user search REST API ( /wiki/rest/api/search/user ) that you can use today. As always, if you are seeking help in building your apps there are three avenues available to you. If you seek help, mentoring, or support in building your app please head over to the Atlassian Developer Community where over 7450 developers and over 850 Atlassians are available to help out. If you found a bug , want to raise a feature request, or report an app security incident with any of our APIs, SDKs, or app frameworks please open an issue over at the Atlassian Ecosystem Developer Service Desk . Need Marketplace Vendor support, head over to the Atlassian Ecosystem Marketplace Vendor Support Desk . Ralph is a Developer Advocate for ecosystem developers.", "date": "2019-07-25"},
{"website": "Atlassian", "title": "Change notice – Changes to installation of a local app on Atlassian Cloud products", "author": ["Anmol Agrawal"], "link": "https://blog.developer.atlassian.com/change-notice-changes-to-installation-of-a-local-app-on-atlassian-cloud-products/", "abstract": "Atlassian products will no longer allow installation of a local app in development mode at \"Manage apps\" page, which uses the same key as an installed Atlassian Marketplace app. When this is attempted the install will fail with the following message: This change has been put in place to prevent any user or admin confusion between locally installed apps and apps installed from Atlassian Marketplace. If you are an app developer, this may affect your development process. You will now need to use a separate instance to undertake testing with a local version of your app. Anmol is a Developer Advocate for ecosystem developers.", "date": "2019-08-09"},
{"website": "Atlassian", "title": "Add-ons can now act on behalf of users.", "author": ["Neil Mansilla"], "link": "https://blog.developer.atlassian.com/atlassian-connect-now-allows-add-ons-to-make-requests-on-behalf-of-a-user/", "abstract": "Atlassian Connect now supports the JWT Bearer token authorization grant type for OAuth 2.0 for JIRA and Confluence Cloud. This allows add-ons with the ACT_AS_USER Scope to access resources and perform actions in JIRA and Confluence on behalf of users. Suggested uses: To get started: Neil Mansilla is the Head of Developer Experience at Atlassian. He’s a life-long developer with a special place in his heart for APIs, and a penchant for ops. If chicken pot pie was wine, he’d be a sommelier. Follow Neil at @mansillaDEV.", "date": "2016-10-24"},
{"website": "Atlassian", "title": "The Atlassian Marketplace API: stable, feature-rich, and documented!", "author": ["Neil Mansilla"], "link": "https://blog.developer.atlassian.com/marketplace-api-now-available/", "abstract": "Today at AtlasCamp , we announced the latest version of our Atlassian Marketplace API. This REST API is stable, supported, provides the ability to access and manage multiple aspects of add-on listings and vendor businesses, and best of all – is fully documented . This REST API is designed to integrate Atlassian Marketplace directly into your business process and tooling. Let’s look at an example API usage. One of the simplest ways to use the API is to retrieve an add-on’s details. We can do so with the following cURL command: What about something more useful, for example, a request to get a list of all licenses for customers who have recently purchased your add-ons: It gets even better! The licenses resource from this example supports a number of request parameters to allow for advanced license searching. Keep reading to find out more about these parameters and how to use them. That’s just the start. Yes, this REST API is easy to use for a number of reasons. First of all, it follows the HAL specification to provide clear and consistent behavior. Most notably, the API’s responses include link maps ( _links ) to reference related resources and embedded representations ( _embedded ) to clearly denote when sets of properties belong to a different-yet-related object. Using the link maps, clients can easily navigate between resources to access all of the desired data. Additionally, resources which support a number of optional request parameters will be represented as link templates to provide a programmatic interface with which clients can determine how to use it. As with most REST APIs, you’re free to use your favorite REST client; my favorites are Chrome’s Postman and cURL . Alternatively, if you’re accessing the API in a JVM-based language, you can use the Marketplace Java client . And of course, the API is now finalized and stable — meaning that you can build on top of it with confidence. You can access the API directly and navigate through the links map. Just use a browser or your favorite REST client. Alternatively, you can view our extensive API documentation . The majority of our documentation is structured with detailed information about request and response formats. This part of our documentation is built with Swagger and deployed with RADAR , a new API documentation generator that we’ve built to present this and other Atlassian APIs. Additionally, we have open-sourced RADAR as part of joining the Open API Initiative . Lastly, you can follow the tutorials to learn more about how to use the Marketplace API. Version 1.0 of the Atlassian Marketplace API will remain stable and accessible. However, future improvements and bug fixes may only apply to the newer version. We hope that you try out the new API! Please provide us with any feedback that you have in the Marketplace team’s JIRA project . Neil Mansilla is the Head of Developer Experience at Atlassian. He’s a life-long developer with a special place in his heart for APIs, and a penchant for ops. If chicken pot pie was wine, he’d be a sommelier. Follow Neil at @mansillaDEV.", "date": "2016-05-24"},
{"website": "Atlassian", "title": "Looking Back at the Ecosystem Highlights of 2017", "author": ["Neil Mansilla"], "link": "https://blog.developer.atlassian.com/top-5-ecosystem-releases-of-2017/", "abstract": "Looking back, 2017 was an exciting and productive year on all sides of the Atlassian ecosystem. Thousands of new developers joined the Atlassian developer community, and hundreds of new apps have been published on the Marketplace. On the Atlassian platform side, there were a myriad of improvements to our developer experience, as we focused on improving site usability, providing regular content updates/comms, and even rolled out our new branding along the way. Here are some highlights of the significant initiatives from the past year. Last spring we began working on modernizing our developer portal at developer.atlassian.com. Over the years, the web site racked up significant technical debt, mostly due to the site being spread out over various systems, managed by a number of different teams. We set out to have one system to rule them all, and migrated all of these sites under one roof. This change allows us to more easily manage our documentation and other content. It also allows you to contribute to the docs through the “Improve this page” links. A majority of the content has been migrated from legacy systems to the new infrastructure, but we still have more work to do. The most trafficked content on the dev portal are the reference API docs. Last year, we set out to provide API docs that were more consistent (across products), thorough, and easier to navigate. On the surface, you may have noticed the changes in the IA (information architecture), design, as well as a new location (from docs.atlassian.com to being consolidated under developer.atlassian.com). Underneath the hood, we needed to make some investments to make it easier for our teams to update and publish docs. For example, the new reference API docs sections are generated from Swagger files that come directly from code. We’ve also introduced an easier way to define and manage Cloud platform apps. Just sign in to developer.atlassian.com and click on your avatar to create an app. In the app management console you’ll find an app’s client ID (API key) and secret, and you can select which APIs (and scopes) you’d like to enable. The Stride API is the first to use this new app management platform, and we hope to have others moving over as well. Under the hood, these changes help us to improve security and scalability, in addition to making it easier for you to build and manage your apps. During the migration, some content, like our Cloud documentation, received an update. For instance, we retired the separate Atlassian Connect documentation site (connect.atlassian.com) and relocated all of its content to the developer product documentation. This puts all of the documentation in one place. Take a look at these changes: Improving the usability of our developer and marketplace sites was a primary goal this year. We’ve iterated improvements with our designs, navigation, and developer tools to increase productivity for both developers and customers. Sometimes, the best way to learn how an API works is to just make a call. A number of our REST API docs now feature the Run in Postman button. Postman is a free REST API console tool, making it incredibly easy to make API calls — even complex POST and PUT calls, with headers, bodies, and authentication. Clicking this button automagically imports the API’s complete set of resources and methods into Postman. Our marketplace has a brand new homepage experience as well as design improvements to search, and other features including: In 2017, we shipped notable updates that have shaped our future brand, products, and communication. The new Atlassian Design Guidelines have reshaped how Atlassian products look and feel. ADG has been rolled out to our Cloud version of Jira, Confluence, and Bitbucket. Additionally, ADG has been rolled out to developers, in the form of AtlasKit , a UI library built according to the new design guidelines. We also launched a new brand. Atlassian designed a whole new set of product logos unified under one parent brand. We understood the need for a reliable community resource for developers to seek advice from their peers and Atlassian staff members. Our new developer community is the place for developers (and Atlassians) to connect, learn, and share their passion and expertise. Thousands of new developers joined the dev community last year (and a few hundred Atlassians), creating over 7,000 new posts. At the Summit US 2017 conference, we announced our new product Stride . We also launched an early access program (EAP) to the Stride API for developers to get a head start on building Stride apps. By the way, the Stride API is now available to all developers, so check out the Stride documentation to get your early start. The Jira Cloud Ecosystem team released their public roadmap . See what the team is working on, what’s recently been released, and what’s planned for the future. We also released the Atlassian platform for developers roadmap . This covers products and projects that support our Ecosystem developers, vendors, and partners to build apps, to list them in the Marketplace, and to manage their customers. The Ecosystem team also runs a number of events to support developers and Marketplace vendors. Here are the events from last year: Held at our office in Amsterdam, we hosted Connect Week, a week-long hackathon where Marketplace developers and vendors connect with Atlassian product and ecosystem engineers. The goal of Connect Week is to help developers build new apps faster, and get them on their way to being published in the Marketplace. During the Summit and AtlasCamp conferences, the Ecosystem team held sessions and workshops for developers to learn about key ecosystem releases and best practices. Another Connect Week, with more great app development. The Trello API made quite a splash at this event. The Ecosystem team led the tehnical speaker tracks, as well as held developer training sessions covering Atlassian Connect and REST APIs. We invited 30 developers from the Stride API EAP to work alongside with Stride engineers on their apps. At this App Week, Developers learned what having a Data Center app entails. This week was largely focused on app scalability (cluster awareness, caching, etc.) and testing strategies. All of the things we were able to accomplish in the past year were only made possible because of you, our amazing developer and vendor community. From our team to yours, thank you. We look forward to an even more amazing 2018. Neil Mansilla is the Head of Developer Experience at Atlassian. He’s a life-long developer with a special place in his heart for APIs, and a penchant for ops. If chicken pot pie was wine, he’d be a sommelier. Follow Neil at @mansillaDEV.", "date": "2018-01-29"},
{"website": "Atlassian", "title": "The New Developer Statuspage", "author": ["Neil Mansilla"], "link": "https://blog.developer.atlassian.com/new-developer-statuspage/", "abstract": "Atlassian’s Ecosystem team understands that its developer community and third-party vendors are huge partners in everything it does. As part of this relationship, the developer community and vendors depend on Atlassian’s platform, APIs, and storefront capabilities to build their own businesses. When Atlassian encounters incidents, not only is it impacted, but so are these crucial partners. Incidents happen. They’re an undeniable part of the software industry. However, when these incidents do occur, the least one can do is communicate with their developers about what’s going on. The Ecosystem team has launched its new developer Statuspage ! This page centralizes everything developers need to know about: API health, documentation availability, Marketplace uptime, and so forth. Many of Atlassian’s developers are also vendors on our Marketplace , and as a result this Statuspage combines everything that both developers and vendors might be interested in; it is a single Statuspage for all developer-related domains and functionality. End users need not worry about all of the intricacies of Atlassian’s developer ecosystem. They don’t need to know about the availability of Atlassian’s Maven repository or Marketplace’s sales APIs. However, end users do care about things such as whether or not they’re able to discover and purchase apps, and if there are known issues with the APIs backing their installed apps. Atlassian’s main Statuspage continues to be the source of truth for end users to monitor the health of all things Atlassian. Now included on this page are the developer-related components which matter most to end-users. You can direct your customers to this page if they want to know more about an Atlassian incident impacting your apps. Don’t worry about the the two Statuspages getting out of sync with each other; the developer Statuspage exposes shared components to Atlassian’s main Statuspage, guaranteeing that both will always reflect the same component health to both developers and end users. These shared components are accessible for use on your own Statuspage, too. To add them to your page, go to the components view in your Statuspage management console, click “Third-Party Components”, and select “Atlassian Developer”. Got feedback? Want to see something else added to our Statuspage? Let us know. Want a Statuspage for your own team? Give it a shot! If you sell paid Cloud apps on the Marketplace, you even qualify for a free account . Neil Mansilla is the Head of Developer Experience at Atlassian. He’s a life-long developer with a special place in his heart for APIs, and a penchant for ops. If chicken pot pie was wine, he’d be a sommelier. Follow Neil at @mansillaDEV.", "date": "2018-02-05"},
{"website": "Atlassian", "title": "Trello joins Codegeist", "author": ["Peter Van de Voorde"], "link": "https://blog.developer.atlassian.com/trello-joins-codegeist/", "abstract": "Codegeist ends August 4th, submit your add-on now to get your shot at $10,000! Trello has joined JIRA, Confluence, and other Atlassian products in Codegeist with its own special prize. $10,000 will go to the team or person who builds the best Trello Power-Up and adds it to the public listing. Check out http://www.codegeist.com today for more details. Get started building a Trello Power-Up with Trello’s Developer Site . There you’ll find documentation on building Power-Ups, example projects, and more info. Not sure what to build? Check out the Trello board of ideas for Power-Ups that users have asked for! You can submit your Trello Power-Up for review at https://developers.trello.com/submit-your-app", "date": "2017-07-10"},
{"website": "Atlassian", "title": "Codegeist 2016: Help Teams work better!", "author": ["Peter Van de Voorde"], "link": "https://blog.developer.atlassian.com/announcing-codegeist-2016/", "abstract": "We’re happy to announce Codegeist , Atlassian’s ninth add-on hackathon. This year, we’re putting special emphasis on Software, IT, and Business teams. Once you have an idea of what you want to make, ** sign up for the competition, build an add-on and have it approved and listed in the Atlassian Marketplace by August 26th, 2016**. To enter, check “Codegeist 2016” when submitting for approval of your public add-on. See the official rules for more details. Whether you’re an experienced add-on developer or trying something new, now is the best time to build your add-on and share it with the world. All participants can apply to get $75 worth of credits for Amazon Web Services. Additionally, they’ll gain free promotion of their add-on and this year’s Codegeist t-shirt. The winners gain access to a variety of prizes totaling more than $100,000. Absolutely, check out our developer documentation to learn how to build an add-on, including all of the APIs, webhooks, and UI extensions. Click this button to sign up and start building your awesome add-on: Sign up and start coding", "date": "2016-07-11"},
{"website": "Atlassian", "title": "The ultimate developers guide to Atlassian Summit", "author": ["Peter Van de Voorde"], "link": "https://blog.developer.atlassian.com/the-ultimate-developers-guide-to-atlassian-summit/", "abstract": "If you're a developer attending Atlassian Summit and you’d like to integrate and customize JIRA, Confluence, HipChat, or Bitbucket with add-ons – this guide is for you. And if you’re not attending, you should check it out! Ecosystem day is a full day of workshops, product updates and marketing training to take your add-on business to the next level. Marketplace vendors and Experts are welcome to join ecosystem day . This year we have an entire day of Atlassian Connect Training . In the morning we’ll focus on JIRA and Confluence. In the afternoon we’ll cover HipChat and Bitbucket. Its only $99 for each morning or afternoon session. Register for training Meet our Connect experts and learn how to use Atlassian Connect to bring your add-on, new idea, or integration into the cloud with our products. Ralph Whitbeck Bitbucket Patrick Streule HipChat Dallas Tester JIRA Matthew Jensen Confluence On Wednesday morning, you won’t want to miss the keynote from the founders of Atlassian. You’ll hear all the details of what we’ve been working on for the last year. Then come along to the Enhance track . On Thursday Connie Kwan will kick off the day showing off new features and what’s coming for Marketplace. Then hear how Twilio , Oscar Health Insurance and Camerican International use add-ons with their Atlassian products, so they work faster and smarter. You won’t want to miss a session of the Enhance track. Technical updates about Atlassian Connect Hear from add-on users The Summit Bash at San Francisco’s world-famous Exploratorium on Wednesday night is one not to miss. A chance to relax and socialize with Atlassians and your peers as you chat and explore in one of the coolest venues in the city. We can’t wait to see you at Summit 2015. Get started with Connect training .", "date": "2015-09-29"},
{"website": "Atlassian", "title": "Git with the Program and sign up for AtlasCamp", "author": ["Peter Van de Voorde"], "link": "https://blog.developer.atlassian.com/atlascamp-dev-track-git-ci/", "abstract": "AtlasCamp is right around the corner on June 9-11 in Prague, and will be like no other AtlasCamp. For the first time we have two tracks of content, starting with our usual track focused on how to customize, integrate with, and extend our products. This year we’ve also introduced a brand new track focused on sharing great developer content to help you build great software and services. Our main conference still has some room for new registrants but you’ll need to sign up soon as we’re getting close to capacity. The new track has educational sessions on Agile, CI and Git. We’re sharing our learnings building SaaS solutions and providing un-interrupted service to our customers at scale. You’ll learn about technologies like Docker, React, JSX, and Flux. Speaking of builds – learn how to manage the complexities of CI/CD from our own war stories doing this across our different products and services. Some highlighted talks you won’t want to miss: If you want to do some deep dive training, we have a few spots left in our workshops being delivered by our resident devops expert Steve Smith who’s teaching a Docker + Bamboo workshop and Git expert Nicola Paolucci who’s delivering a training on Advanced Git – “Master the art and practice of DVCS”. Beyond all these great sessions, we’ll have developer breakouts (mini tech talks), network time for you to chat with speakers, other attendees, and we’ll even have technical folks from AWS, Azure, and Google there to answer your cloud questions. I’ll be MC’ing this track and hope to see you there. Let me know if you’re coming and particularly if you are a Woman in Tech I’ve got a special networking event for you at AtlasCamp I’ll be happy to send you an invite. See ya in Prague!", "date": "2015-05-14"},
{"website": "Atlassian", "title": "Join Us for Our Next Dev Den Open Office Hours", "author": ["Peter Van de Voorde"], "link": "https://blog.developer.atlassian.com/winter-office-hours/", "abstract": "The gang is reassembling again for another Dev Den Open Office Hours at the end of this month where we’ll be talking all things Git again including discussion on last month’s Git security incident. We ran last month’s broadcast all in one room in Sydney as we were there for a team offsite. It was great to have the team in one place for the first time and brainstorm future content and projects. If you missed our last session we were able to sort out all the kinks of running a Google Hangout and posted both a recording and transcript. This month we’ll be “dialing in” from different locations so some of us may be slower at answering those burning questions as we perk up with some coffee. 🙂 Look forward to seeing you there! Transcript", "date": "2015-01-09"},
{"website": "Atlassian", "title": "Welcome to App Week, Portlandia Edition", "author": ["Katrina Morales"], "link": "https://blog.developer.atlassian.com/app-week-cloud-portlandia-edition/", "abstract": "We just wrapped another successful Atlassian App Week! During App Week Cloud | June 11-15 , App Week went back to its Cloud roots while traveling to Portland, Oregon for the first time. While the previous two App Weeks had brand new focus areas – App Week Data Center (November 2017) and App Week Experience Design (March 2018) – App Week Cloud was focused on customer announcements we are working on with SaaS partners and Marketplace vendors in preparation for Summit Europe, as well as migration of top Server apps to Cloud. In particular, our goals for this App Week centered around Cloud by helping select SaaS partners and Marketplace vendors with popular Server apps either build new Cloud apps, or improve the features or performance of existing Cloud apps in the Atlassian Marketplace. SaaS partners including Google , InVision , Box , New Relic , Cloudability , and BlueJeans , plus Marketplace vendors with popular apps such as Adaptavist , Wittified , EasyAgile , Refined Wiki , CodeBarrel, ALMWorks , and EazyBI traveled in from around the US, Australia, Mexico, and across Europe. These companies represent just a few of the 35 Marketplace vendors and SaaS partners who applied to, then attended App Week Cloud to build or improve existing Cloud apps for Atlassian products. What makes App Week such a hit with app developers is the access to our talented Atlassian engineers and product leads who can help them solve code issues in minutes versus weeks. Team members from Jira Software Cloud , Jira Service Desk Cloud , Confluence Cloud , Bitbucket Cloud , Stride , Trello , and Statuspage were all on deck to help our app developers build, as well as present the latest and greatest product updates, tips and tricks, and best practices. On the last day of the conference we encourage attendees to demo what they accomplished during the week, and show off new functionality they plan to ship for Atlassian products – some of which will be part of customer-facing announcements at Summit Europe 2018. Get ready for Barcelona! To better understand the value of App Week Cloud to the Marketplace vendors and SaaS partners who attend, we wanted to share stories from three of our newest companies building Atlassian apps who attended App Week for the first time last week: Hugo , Rollout.io , and Cloudability . \"App Week gave us the opportunity to 'join' the Atlassian team for a few days, working next to the designers, engineers, marketers and product gurus behind the apps that we work with everyday. We were able to build faster and deeper into the Atlassian ecosystem, providing an even better experience for our mutual customers.\" – Hugo's Co-Founder, Darren Chait Rollout built a Stride bot at App Week that opens a pull request in Bitbucket Cloud for the flag and relevant code to be removed. It also creates a Jira Software Cloud issue to track the flag removal request. This is our first feature flag app integrated across multiple products. At App Week, Team Rollout met top-notch technologists from Marketplace vendors and SaaS partners across the Atlassian ecosystem. App Week also provided a very collaborative environment to bounce ideas and consult with others working on Atlassian Marketplace apps. Last but not least, they found the talks very useful and insightful (not just for engineering), from an app onboarding session, an OAuth 2.0 code authorization grant talk, and updates to the Connect framework lecture. Cloudability came to their first App Week ready to address user feedback they had received from power users of their product: Atlassian Cloud Engineers based in Sydney. After using Cloudability to get better insights into Atlassian’s cloud infrastructure spend, Atlassian’s engineers sent the Cloudability team feedback on their product: “Wouldn’t it be great if instead of having to pull data out of Cloudability, put it in a spreadsheet, then paste data into separate Jira tickets assigned to different teams responsible for managing cloud spend, we could just use a Jira Software app integrated with Cloudability? Your team should talk to Atlassian’s SaaS partnerships team about that. We (and a lot of your other customers) would use that app!” As a result, Team Cloudability applied to and came to their first App Week Cloud. They received valuable feedback from Jira Software product, engineering and design Atlassians that will strengthen their features and app integrations going forward. The Atlassian SaaS partnerships team has been working with Cloudability for several months on getting this out, and App Week was what got the app to completion! Cloudability was also able to demo their new Cloudability app for Jira at their very fun App Week Cloud happy hour that Cloudability hosted at their Portland offices. They also enjoyed meeting like-minded developers from other companies building Atlassian apps located around the world, whose excitement was contagious and energized their team. Demand for cloud spend management tools is growing, so integrating Jira Software functionality directly within tools where DevOps and IaaS management teams already work is super useful. Just ask the Atlassian engineers who suggested it! Next up: Atlassians that work directly with our extended Ecosystem TEAM of external Marketplace vendors and SaaS partners are next looking forward to Atlas Camp, Atlassian’s annual developer conference happening Sept 6-7 in Barcelona. See you at Atlas Camp -Delyn Head of Ecosystem Marketing Katrina is the Developer Community Manager for Atlassian's ecosystem.", "date": "2018-06-22"},
{"website": "Atlassian", "title": "How we stopped vulnerable code from landing in production", "author": ["Hasnae Rehioui"], "link": "https://blog.developer.atlassian.com/how-we-stopped-vulnerable-code-from-landing-in-production/", "abstract": "Around a year ago, my team shipped one of the most highly voted features for Confluence Server—and no, it's not rename space key. We were looking into Team Calendars and wanted to show our customers that we are still very much invested in the product by finally making support for CalDAV a reality. I joined the project mid-way, but little did I know that the third party library the original team used came with a serious vulnerability. I attended BugBash Sydney , a two-day event that brought together Atlassian engineers and security researchers to try and break into our products and uncover potential code vulnerabilities or security bugs. In particular, I spent the first day watching some hackers study one of our open source repositories; they were identifying code patterns that display known XML vulnerabilities in our endpoints, and exploiting them by constructing malicious requests. After a memorable moment of \"OMG what is this sorcery!\" I found myself suddenly immersed in reading up about XML attacks and joining forces with the hackers to help them craft the best exploits, to the point where some of them thought I was a security researcher myself… \" What? No, actually, I work on the product you're trying to break into.\" We were looking at an open-source app that enriches Confluence with the Web Distributed Authoring and Versioning protocol. The plugin came with a reverse proxy class that does some XML processing: The pattern we were worried about was the TransformerFactory#newInstance() which makes it seem as if we could be subject to an XXE (XML external entity) attack ; however, after a more thorough study of the rest of the app code, we eventually came across a security patch our developers put in place a long time ago: So once we established that the webdav plugin is actually safe when it comes to XML processing, we started scrutinizing the same code except this time we were after symptoms of SSRF (server side request forgery) . Staring long enough at the code above we realized that it was indeed vulnerable to SSRF by injecting a malicious host header onto any of the requests targeting the reverse proxy. Well, after a whole afternoon improving on my ability to think like a hacker, I spent my commute back home reminiscing about all the features I contributed to during my tenure at Atlassian, tapping myself on the shoulder \"good job Viqueen, that was a secure one\" …all up until CalDAV… \"oh no, I think we messed up.\" Needless to say, it kept me up all night. Turns out we built our feature on top of a third party library that was carrying XML processing vulnerabilities through one of its dependencies. After regrouping with the security researcher on the second day of BugBash, I showed them the way to the doors of Team Calendars and ask them to make their way in. We managed to bring down a Confluence test instance by feeding it an exponential entity expansion request, commonly known as the billion laughs attack. I am quite a cheerful person, but I wasn't laughing then. Since entity expansion was possible, we thought to take it a step further and attempt to expose files from the host instance. It was not easy, but we managed to upload files from a Confluence host instance to an external ftp server through a rather elaborate scheme: In retrospect, I am sure there was an easier way to achieve that exploit, but to be honest I always dreamed to have a complex terminal setup like the following: We were able to report that the CalDAV feature was vulnerable to XXE and SSRF after which the researchers finally called it a day, leaving me with the tedious task of showcasing the issue to my fellow colleagues and patching Team Calendars. I also reported the vulnerability to the authors of the third party library and contributed a fix. Glad you asked. At Atlassian, we are very serious about learning from our mistakes in order to make sure they never happen again; so after resolving any incident we ran an Incident Postmortem , a session designed to determine and understand the root cause of what happened. In the case of Confluence Server, we do have tools such as SourceClear put in place to analyze and scan our code for security issues; however, those only catch \"known and already reported\" vulnerabilities. So we first proceeded by reporting and fixing the issue in order for it to appear in security scanners; we then introduced security champions across all of our sub teams to make sure we catch what our tools cannot and challenge all of our solutions from a security perspective. Ideally you're learning your code vulnerability lessons through articles like this, rather than the hard way. If you still have further questions or feedback, feel free to reach out to us on twitter @atlassiandev or get connected to Atlassian’s developer community . Pro tip! If you are building an app that requires XML processing, you should use Atlassian's secure xml library. It will protect you from various XML attacks such as Billion Laughs and XML external entity expansion . Pro tip! If you’re building an app that requires a connection to an external service, you should use Atlassian's whitelist plugin :", "date": "2019-07-23"},
{"website": "Atlassian", "title": "Recap: App Week Berlin", "author": ["Katrina Morales"], "link": "https://blog.developer.atlassian.com/recap-app-week-berlin/", "abstract": "App Weeks bring together our ecosystem of third party app developers and our product teams. It's a mostly unstructured week to work together and learn from each other. The primary goal is to help our developers ship the apps they are working on, whether it's creating a new one or making improvements to an existing one. We host them in different cities around the world. In February we hosted one in Santa Cruz, California . The focus for this one was Cloud for the Enterprise. We invited Marketplace vendors with key apps that are used by our large customers to come to this App Week so that they could focus on building/improving the cloud versions of those apps. App Weeks in Europe are always popular due to our large vendor presence there, but this was the most popular App Week yet. We unfortunately had to turn away about half the companies that applied to attend due to space constraints. We keep App Week capacity low (approximately 100 people) to preserve the intimacy of the event. All in all, we had 92 developers from 43 companies attend. Hosted at the Sofitel in the Kurfürstendamm district of Berlin, this was our poshest App Week yet. The space was grand and beautiful, with lots of natural light, and the food was constant and delicious. Monday was dedicated to presentations from Atlassians. The product teams presented their roadmaps and the opportunities for vendors to build upon. Other teams presented on best practices, such as making Jira apps scale and the Atlassian Design Principles. After learning about Atlassian's bug bounty program from the security talk, one of the developers reported a bug and got paid some money for it! After standups in the morning, Tuesday, Wednesday, and Thursday were mostly dedicated to unstructured work time. Vendors and Atlassians gathered in breakout rooms and worked through issues side by side. The flexibility in the schedule allowed the vendors to self-organize discussion groups about how they handle things like security and cloud infrastructure. Friday was dedicated to demos, when the vendors present what they worked on during the week. A few of them were aspirational (e.g. we learned what it's going to take to get our app to work), but many of them presented working live demos. Though it's not mandatory to demo, we had excellent participation with 35 demos in total. On Wednesday evening, Berlin-based vendor re:solution took us on a bus tour of the city, followed by a mixer at one of their WeWork spaces. We were really grateful to have them organize and sponsor such a fun and relaxing activity (with great views of the city)! At the end of each App Week we send out a survey. We're happy to report that the overall satisfaction for this event was 4.6 out of 5 stars! We're proud of that and we will continue do the things that worked well, as well as work on the things that we could improve upon according to the survey. We don't have specifics for the next App Week available yet, but we will be sure to post here when we do, so stay tuned! In the meantime, you can register for Atlas Camp 2019 . No application required. 🙂 Katrina is the Developer Community Manager for Atlassian's ecosystem.", "date": "2019-07-09"},
{"website": "Atlassian", "title": "Atlas Camp 2019 registration is live!", "author": ["Katrina Morales"], "link": "https://blog.developer.atlassian.com/atlas-camp-2019-registration-is-live/", "abstract": "Atlas Camp is Atlassian's annual developer conference. This year it will be in historic Vienna, Austria on 11 – 12 September, 2019 at the Messe Wien Exhibition Congress Center. It's a great way to learn new skills, meet your fellow developers, and meet Atlassians! The agenda will feature a keynote, 4 tracks of content, and more. Our content tracks are: Explore topics around Data Center/Server, testing, performance tuning, scaling, and building for the Enterprise. Transform your team and get tips on Agile, testing, source control, DevOps, marketing, and running a business as an Atlassian vendor. Elevate your app and learn how to maximize experience design to improve user onboarding, increase adoption, and which tools can jump-start your app experience. Get up to date on APIs, roadmaps, new services, and features that will set your app or service up for success from the Atlassian Product teams. We're accepting talk submissions from our community until 11 July . Apply to speak in one of these four tracks, and you'll get free admission if your talk is accepted! Sponsorship opportunities are also available. Join us! Passes are €100 off for a limited time. Katrina is the Developer Community Manager for Atlassian's ecosystem.", "date": "2019-06-24"},
{"website": "Atlassian", "title": "Atlas Camp Call for Speakers suggested topics", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/atlas-camp-call-for-speakers-suggested-topics/", "abstract": "Atlas Camp is Atlassian's developer conference where the Atlassian developer ecosystem and community come together to learn, inspire, and grow. This year's event will be in Vienna, Austria for the first time. Atlas Camp will be on 11 – 12 September 2019 at the Messe Wien Exhibition Congress Center . In case you haven't heard, you can submit a proposal to speak up until 8 July 2019 23:59 GMT+2. Selected speakers will get free admission to Atlas Camp. We are continuing to strive to make the content for Atlas Camp the best ever. Here's what we're doing: You're in luck, we've put together a list of suggested topics that we feel that would be amazing at Atlas Camp. We wanted to share this list to help you with topics to submit to speak on. Check out the Atlas Camp 2019 – Suggested Topics Trello board with suggested topics in each track. Use the Trello board to see what the track leads are looking for. These are the topics that they are hoping to see when they start reviewing the talk submissions. We're hoping to inspire you to create a talk that allows you to share your knowledge with the rest of the developer ecosystem. You'll notice that there are many talks that only Atlassian can give. For instance, most of the sessions in the Atlassian Platform track should be about features and APIs available from Atlassian. We are openly sharing these topics so that you can consider the program as a whole when creating your proposal. Please keep in mind that this topic list is NOT a schedule or even the final list of topics. We'll determine that in July based on all of your talk submissions. So now is the time to submit your talk for consideration to speak in Vienna in September. Awesome, thank you for taking the time to submit . In mid-July, the program committee will meet with the individual track leads to select the talks for each track. This process will go on for a couple of weeks. Towards the end of July, we'll send out acceptance emails to the selected speakers. If your talk is accepted we'll provide you with a free ticket to Atlas Camp. A week after acceptance emails go out we'll let the rest of the speakers know that their submission was not accepted. You'll then work directly with the appropriate track lead in creating your content, slides, and rehearse your talk. Great! Don't forget to get your Atlas Camp ticket before the Super Fan rate expires (save €100). If you're only able to attend and weren't selected to speak (or you are not going to submit) you should still review the Trello board . You can use the board to display the value you'd gain in attending Atlas Camp to your manager. Remember you can save €100 by buying your Super Fan ticket . Head on over to the Atlas Camp site to submit your talk or to get your ticket. We hope to see everyone in Vienna in September. Ralph is a Developer Advocate for ecosystem developers.", "date": "2019-06-25"},
{"website": "Atlassian", "title": "Jira Server is making declaring dependencies easier", "author": ["Magda Michalak"], "link": "https://blog.developer.atlassian.com/jira-server-is-making-declaring-dependencies-easier/", "abstract": "Jira Server platform has hundreds of dependencies. Each app uses a subset of those to perform its function. At the same time it is important for an app to declare a component in the same version as provided by Jira platform because failing to do so may break the functionality provided by the app. During the Jira platform releases many dependencies are updated and it can be difficult for app developers to keep up with the changes. This can result in an app being compiled against an older version of a dependency, which is no longer provided by the platform. So far, keeping the dependencies in sync has required discovering if a component version has changed in a new Jira version and then updating this component in an app. That has proved just too time-consuming and demanding. With the changes we've introduced updating platform dependencies in an app becomes the matter of updating the Jira version in the app pom file. The Maven build tool provides a way to import dependency versions from another pom via the <dependencyManagement> section. Having a proper dependency management defined in the Jira platform pom and importing it into an app allows you to significantly simplify version management. Now the jira-project pom can be used by the apps to import the platform dependencies and get rid of the hard-coded platform component versions in their build files. To do this an app can import the jira-project pom in its dependency management section: You can then drop the platform dependencies definitions from the app's <dependencyManagement> section and also remove all the hard-coded component versions from the pom files. If you are working on an app which has the hard-coded dependency versions for platform components consider migrating it to import the jira-project pom. However, if you have your own way of keeping things up and running, this is fine, because the new way is just an option you can use to make your life easier.", "date": "2019-06-20"},
{"website": "Atlassian", "title": "Ecosystem Roundup – new button for Next-gen and more", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/ecosystem-roundup-12/", "abstract": "In this post, we round up all recent Ecosystem news. Jira Software Cloud is providing the ability to add a button on a board and backlog page in next-gen projects. This is to reduce the gap between classic and next-gen projects. You can learn more about this feature here . There is a lot of activity on the developer community due to our awesome community helping each other out.  There is also a lot of great topics that get posted that you might miss.  Fear not, we did the leg work for you and surfaced the threads we think you shouldn’t miss. These server products have been released since our last Ecosystem Roundup. As always, if you are seeking help in building your apps there are three avenues available to you. If you seek help, mentoring, or support in building your app please head over to the Atlassian Developer Community where over 7150 developers and over 825 Atlassians are available to help out. If you found a bug , want to raise a feature request, or report an app security incident with any of our APIs, SDKs, or app frameworks please open an issue over at the Atlassian Ecosystem Developer Service Desk . Need Marketplace Vendor support, head over to the Atlassian Ecosystem Marketplace Vendor Support Desk . Ralph is a Developer Advocate for ecosystem developers.", "date": "2019-06-06"},
{"website": "Atlassian", "title": "Tip of the Week: Using different SSH keys for multiple Bitbucket accounts", "author": ["Peter Van de Voorde"], "link": "https://blog.developer.atlassian.com/different-ssh-keys-multiple-bitbucket-accounts/", "abstract": "To generate a new key pair simply run this command in the ~/.ssh/ folder: The -C option is a comment to help identify the key. The -f option specifies the file name. Repeat the above for each Bitbucket account you want to use. To add a public key to a Bitbucket account, you need to go to the Bitbucket Settings Screen. Select SSH Keys in the left side menu and click Add key . For more detailed information check out the Bitbucket documentation: In ~/.ssh/ create a file called config with contents based on this: Replace user1 or user2 with your Bitbucket usernames This is only useful if you use a passphrase to protect your key. Otherwise you can skip this. Depending on your operating system you'll need to find out how best to do this. Linux users can use GnomeKeyring . Mac users can use the following command to permanently add keys to the Mac SSH Agent: Windows users can take a look here for more info: Git On Windows You can check the keys on your keyring with: If you don’t have a local copy of your repo, you have to run the following command to clone a Bitbucket repository: If you already have a local copy, you’ll need to update the origin: Now go to the local Git repo that you want to configure and enter: Where user1 matches the values you used earlier in your ssh config. You can now git push as normal and the correct key will automatically be used. I hope you found this tip useful! Check out our earlier Tips of the Week or tweet your own tip suggestions to @pvdevoor .” This post is based on the following sources: This post was updated on 17 April 2018 to reflect the change of bitbucket.com to bitbucket.org.", "date": "2016-04-13"},
{"website": "Atlassian", "title": "Pull Request Merge Strategies: The Great Debate", "author": ["Nicola Paolucci"], "link": "https://blog.developer.atlassian.com/pull-request-merge-strategies-the-great-debate/", "abstract": "When a piece of work is complete, tested and ready to be merged back into your main line of development, your team has some policy choices to make. What are your merge strategy options? In this article I’ll explain the possibilities and then provide some notes on how we do it at Atlassian. Hopefully at the end you’ll have the tools to decide what works best for your team. What are some possible merge strategies? There is a plethora of choices: Explicit, non fast-forward merge Implicit via rebase or fast-forward merge Squash on merge I’ll use master as the mainline branch in this article, but you can replace it with develop , staging , next , etc. Go nuts. Oh: and stay till the end because I have some goodies (scrolling down right now is not allowed!). This option is the least surprising and most straight-forward. And sometimes default and straight-forward is the way to go. When moving complete feature branches to master using explicit merges, git creates a merge commit which records the event. In the resulting commit git stores the two parents involved in the merge. That commit will unify the changes between the two branches using a recursive 3-way merge (unless you specify a different merge strategy ) Technically, a merge commit is a regular commit which just happens to have two parent commits. Let me show you: if you select the sha-1 of a merge commit and inspect its contents using cat-file -p , you get: Some teams avoid explicit merges because that they create clutter in the linear history of the project. But this argument generally stems from an unfamiliarity with branching workflows. For example, the “noise” issue is easily solved by learning one or two tricks like using git log --first-parent and the like. Another way to move complete work from a branch to master is to use rebase or a fast-forward merge. One of the uses of rebase is precisely to replay commits–one by one–on top of a specific branch. Note that this operation rewrites all the ids ( sha-1 ) of those commits. This happens because when git computes the unique id of a commit it takes into account the parent commit. If the parent commit changes, the sha-1 of the replayed commit changes too. Used this way, one can indeed apply some commits to master without creating a merge commit. This procedure completely loses the context of where those commits come from, unfortunately. Using a fast-forward merge to move code to master has some similarities to the above. Have a look: A fast-forward merge can only happen if in master there are no more recent commits than the commits of the feature branch. In this case master ‘s HEAD can easily be moved to the latest commit of the feature branch. And the merge can complete without an explicit merge commit: it literally just fast-forwards the branch label to the new commit. Differently than rebase , a fast-forward merge will not change the commit ids ( sha-1 ), but it will still lose the context of those commits as part of an earlier feature branch. A third way to move changes is to squash all feature branch’s commits into a single commit before performing a fast-forward merge or rebase . This keeps the mainline branch history linear and clean. It isolates the entire feature in a single commit. But it loses insight and details on how the feature branch developed throughout. So… trade-offs. In this scenario you might be compelled to keep the original, unsquashed , feature branch around for historical reasons. If you use explicit merges this need does not arise because the explicit merge commit allows you to reconstruct what was in the feature branch and its entire evolution. Stash –our enterprise git repository manager–allows teams to choose their merge strategies for pull requests. A pull request is a light-weight code review facilitated by the great paradigm shift to feature based development . Tweaking a simple parameter you can get *”squash on merge”* in your project, as you can get --ff-only and several others–with --no-ff being the default. What’s the merge policy at Atlassian? At Atlassian we lean strongly towards using explicit merges. The reason is very simple: explicit merges provide great traceability and context on the features being merged. A local history clean-up rebase before sharing a feature branch for review is absolutely encouraged, but this does not change the policy at all. It augments it. For more on this see a piece I wrote a while ago on [“ merge vs rebase workflows “. I’ll stop here for now, I hope you found these explanations useful and if interested in these topics take a second to follow me @durdn and the awesome @atlassiandev team for more git rocking. Oh! and subscribe our RSS feed ! You might also enjoy our ebook, “Hello World! A new grad’s guide to coding as a team” – a collection of essays designed to help new programmers succeed in a team setting. Grab it for yourself, your team, or the new computer science graduate in your life. Even seasoned coders might learn a thing or two. Read it online now Click here to download for your Kindle", "date": "2014-12-10"},
{"website": "Atlassian", "title": "Update plain text macros in Confluence Cloud for new editor compatibility", "author": ["Klaus Ihlberg"], "link": "https://blog.developer.atlassian.com/update-plain-text-macros-in-confluence-cloud-for-new-editor-compatibility/", "abstract": "Confluence Cloud has been gradually rolling out a new editor in which we've focused on aligning the edit and view experiences so that pages look the same while editing as when published. This has required us to rethink how plain text macros work, and we've ultimately decided to implement them as live macros. This blog will outline the key differences in how plain text macros will work between the two editors, and take Confluence macro developers through the steps they'll need to take to update them. Timeline : This change has already rolled out behind a feature flag, but will not be turned on for users until the 12 Jun 2019 so as to give developers time to update their macros. We can enable this for individual sites, so please contact us if you or a customer would like for this to be turned on before that date. A plain text macro is a macro whose body consists of plain text only and which the macro then renders as something else. Some examples include LaTeX Math and Advanced Tables for Confluence . In the old editor, plain text macros display as a block in which users input text. This means that users can't see what their macro will actually look like until they publish their page, and that pages look significantly different when published to what they look like while they're being edited. Some macros feature a custom modal that users can access by choosing 'edit macro' (the pencil icon). A few macros have included previews of what will be rendered in their custom modals. As one of the main focuses of the new editor is to provide a consistent and predictable experience between viewing and editing a page, the old implementation of plain text macros needed to be rethought. Most other macros have been converted to live macros – that is, full fidelity previews of the published macros. Live macros look the same while editing as they do when published, but not all live macros are fully interactive while editing. In order to turn plain text macros into live macros, we've moved the text input into the Confluence macro browser. This means that when a user adds a plain text macro to their page in the new editor, they'll be prompted to input text within the macro browser, and the rendered content will then be displayed on the page. We've modified the macro browser for plain text macros to have the text input take over the part of the browser that would usually display a preview. For a user, this essentially works the same as, for example, the native Confluence Chart macro. Users specify parameters and macro content entirely within the macro browser, and the macro then renders on the page. More details at CONFCLOUD-65717", "date": "2019-05-28"},
{"website": "Atlassian", "title": "Using JaCoCo as a code coverage tool for Scala", "author": ["Miles Buckley"], "link": "https://blog.developer.atlassian.com/using-jacoco-a-code-coverage-tool-for-scala/", "abstract": "This blog post was contributed by Ihor Uksta, a software engineer at iDalko . As test management gets more complex, developers need a way to highlight specific aspects of code which may not be adequately reviewed and require additional testing. Code Coverage is a metric that measures what percentage of your code has been executed during unit and integration tests. JaCoCo is a great open-source toolkit for code coverage measurements. JaCoCo was originally written for Java and runs on the JVM, but because it's bytecode-based it works for Scala too. It also includes reports which can be extremely helpful when working with complex projects. In this post, we're going to look through some practical examples of code coverage analysis via JaCoCo by using sbt-jacoco as an sbt plugin. JaCoCo supports a number of different metrics for code coverage measurement: JaCoCo’s build runs all the tests in a system, and then after the build is terminated, it generates a report file. The reports are published in the directory /target/scala-{version}/jacoco . Each metric mentioned above can be represented in a report as a percentage of the covered code. Additionally, some types are also visually highlighted: JaCoCo can generate reports in the following formats: Adding JaCoCo to your Scala project appears to be very simple with an sbt-plugin called sbt-jacoco . First, you just need to add the plugin to your plugins.sbt : Then run a command: sbt jacoco This command runs all your unit tests and collects code coverage metrics into an HTML report. The report can be found in the /target/scala-{version}/jacoco/report/html/index.html . In the logs you can get a short overview of the analysis: In this case, I have zeros everywhere because the project is completely empty, so we need to add some logic there. Let's take a look at this quick and dirty example in Scala: And the executor will be a simple main function: We should cover this with unit tests. We need to add a scalatest library to our build.sbt: libraryDependencies += \"org.scalatest\" %% \"scalatest\" % \"3.0.1\" % \"test\" And unit test itself: After adding this code our report should look like: As we can see our Branches coverage reached 50% and 1 out of 2 missed. Let’s check the HTML view report. Based on the yellow diamond we know that only a part of the branches on the line have been executed. We need more tests: Try it one more time: Finally, our Branches are 100% covered. But we still did not reach this number with our types. It’s because our project has a Main object which is just an executor of BeerSeller. In this case, we can exclude it from JaCoCo measurements by adding a more specific config. Here we set a new name for the report, a required level of Branches metric, and add one more report format – CSV (and of course, exclude Main object from code coverage analyzing). Now the report looks perfect: JaCoCo and other code coverage measurement tools are great to have in your developer’s toolset, and it only takes a minute to install the plugin using sbt. Your reports will generate automatically while your unit tests are running, saving you time instead of waiting for your test results. The ability to see highly detailed visible reports is also a great way to make sure your code is better covered and fully tested. However, you should always remember that 100% code coverage does not necessarily reflect effective testing—it only reflects the amount of code executed during tests. To check out the example code used in this article, check out the GitHub repository here . Ihor Uksta is a software engineer at iDalko , a Platinum Atlassian Solution Partner. He is currently leading the development of Table Grid for Jira . Every day he pushes to reach new heights with the next incarnation of this successful product. When he is not improving the world with his magical strokes of Scala, he is discovering the better parts of Lviv with his bulldog Chester. Miles is on Atlassian's Ecosystem team and works with our developers, Marketplace vendors, and customers to ensure a world-class experience when customizing and extending Atlassian products.", "date": "2019-05-23"},
{"website": "Atlassian", "title": "Developing for Jira Service Desk Cloud next-gen projects", "author": ["Joshua Vernon"], "link": "https://blog.developer.atlassian.com/developing-for-jira-service-desk-cloud-next-gen-projects/", "abstract": "TL;DR Jira Service Desk Cloud introduced next-gen projects in Feb 2019 and developers may experience some differences when calling either the Jira Service Desk Cloud REST API or Jira platform REST API. App developers and REST API consumers may need to make modifications to support next-gen projects. App developers and REST API consumers may need to make modifications to support next-gen projects. Next-gen projects are available in Jira and Jira Service Desk, and aim to simplify the project configuration experience for administrators and end-users. This blog will focus on the technical impact to Connect App developers and REST API consumers developing for Jira Service Desk. If you need more information on the impact on developing for Jira please read this blog . If you are looking for Jira platform REST API changes refer to this blog . There are no changes to the Jira Service Desk REST API for classic projects. The Jira Service Desk REST API has however been modified slightly to accomodate Next-gen project and project scoped entities. Next-gen projects are being improved each and every day and will soon be as powerful and then more powerful than classic projects from user and developer perspectives. Read and give us feedback by watching or using the next-gen tag in the Atlassian Developer Community .", "date": "2019-04-08"},
{"website": "Atlassian", "title": "Tips & tricks for developing a serverless cloud app", "author": ["Miles Buckley"], "link": "https://blog.developer.atlassian.com/tips-tricks-for-developing-a-serverless-cloud-app/", "abstract": "This blog post was contributed by Sebastian Hesse, a Software Engineer at K15t, based on his presentation at Developer Day 2019 . As organizations search for flexible and scalable data solutions, the pull for going serverless has never been stronger. However, applying emerging technologies to your company's processes is easier said than done. Luckily, you're not alone in your pursuit of serverless cloud app development. With first-hand experience in implementing serverless app development as a software engineer at K15t (an Atlassian Platinum Solution Partner and Marketplace Vendor ), I'll reveal some of my best practices and discuss the roadblocks we were able to overcome while going serverless. Before we look into the best practices, we need to set up our development environment. There are two ways you can develop serverless functions: locally or directly in the cloud. Developing a function locally allows you to execute, test, and debug code on your local machine, while the alternative requires you to upload and execute your code in the cloud. In the instance of local development, you have the choice of using different frameworks, like the Serverless framework , the Serverless Application Model (SAM) , or LocalStack . In order to go live with your app, you'll still need to use one of the cloud providers like AWS, Azure, Google, or similar to upload and execute your code. For this article, we're going to focus on AWS Lambda. One key drawback we saw with the cloud execution is that debugging your code can be more challenging. To make a well-informed decision about your development approach, it is important to consider three things: Another important point is to keep track of all your Lambda functions. The best way to do this is to describe your infrastructure using one of the frameworks above to help keep track of the Lambda functions deployed in your environment. You may also utilize a CI/CD pipeline , which allows you to automate your deployment as much as possible. The first best practice when going serverless is to limit the scope of your functions. For example, if there are different tasks you want to accomplish like receiving a webhook, processing the webhook data, and then sending a notification, you could put them all into one Lambda function. However, this will reduce your app's scalability. I suggest keeping your functions simple and separate your concerns. Consider focusing on just one task your function will perform and pour your energy into delivering that functionality very well. This will lead to better scalability and reusability in your app. This also reduces the file size of your Lambda function, so you may also be able to reduce the cold-start time. Every millisecond matters here, because the bigger your artifact, the slower the startup time of your function, so decide carefully on which dependencies you’ll include in your code. This is especially important for Java programmers! Avoid using frameworks like Spring – it just slows down your function. If you want to use your Lambda functions as a REST API, look for services like API Gateway or AWS App Sync to connect with your Lambda functions. However, keep in mind that due to the cold-start problem, the response times of your Lambda functions might have some spikes. If you follow the advice from above to split up your Lambda functions, then communication between your functions becomes very important in order to exchange data and create a flow within your app. Depending on your needs, you can utilize: Both approaches have their drawbacks: synchronous communication results in a tight coupling of your Lambda functions and might be problematic if you need to exchange big chunks of data, whereas asynchronous communication always involves a different party and can get pricey. However, in order to be more flexible and improve the scalability of your app, asynchronous communication is the preferred choice. Many services like AWS and Jira have the capacity to automatically scale applications, leading many developers to overlook this step. However, you may find yourself working with numerous services in order to achieve the desired functionality for your serverless application. These services won't all have the same scalability. Protect your code from malfunctioning by setting up a queue, or buffer requests if necessary, to hedge this issue. One service that proved to be very useful for us was Amazon Kinesis, a streaming service capable of handling lots of data, which we use to buffer all incoming webhooks. In Lambda, your functions have 15 minutes to run before they time out. Given that this limit was recently increased from 5 minutes, this may not seem like a big concern. But depending on your app, instances can quickly add up. You can optimize this a bit with some more parallelization within your Lambda function, but to really circumvent these time limits, consider using other approaches like recursion (calling yourself from within a Lambda function to continue processing) or AWS Step Functions , which manage the execution workflow around the time limit. You may also need to consider the viability of outsourcing this to another service. Other important aspects to take into consideration are the limits of a Lambda function. For example, you have up to 3 gigabytes available for your memory allocation, but be aware that memory allocation and CPU power correlate: the more memory you allocate, the more CPU power you have. The same is true for network and I/O throughput which correlate to memory allocation as well. Furthermore, you only have about 512 megabytes available for temporary local storage. If you're in the middle of a project (or ideally in the planning stage) and feel as though these limitations will be too challenging to overcome, then serverless may not be the way to go for this particular set of code. If you're considering (or in the middle of) creating a serverless app, you can utilize these key takeaways from my team at K15t to create a proper scope and avoid potential snags in the dev launch cycle. Let me know what you think about my best practices or reach out to me directly at shesse@k15t.com with any questions! Watch Sebastian’s full presentation from Developer Day 2019 to learn even more. Sebastian is a software engineer at K15t , where he has taken on a leading role in designing and implementing a high-scale cloud architecture for Backbone Issue Sync for Jira . He’s eager to share his experience with others, and when he’s not developing for K15t’s apps, you can find him speaking at local AWS user group meetups. Miles is on Atlassian's Ecosystem team and works with our developers, Marketplace vendors, and customers to ensure a world-class experience when customizing and extending Atlassian products.", "date": "2019-05-21"},
{"website": "Atlassian", "title": "Recap: Developer Workshops at Community Events", "author": ["Katrina Morales"], "link": "https://blog.developer.atlassian.com/recap-developer-workshops-at-community-events/", "abstract": "Over the last several weeks, we worked with two different Atlassian Communities to pilot something new: hands-on workshops aimed at introducing users to the power of developing their own apps and Power-Ups. (ICYMI, AUGs are now Atlassian Community Events/Leaders .) For our first pilot event, we headed out to the Trello office in New York City where we hosted a group of Trello aficionados to get them started building their own Power-Up. The two-hour session was lead by developer advocate Bentley Cook and supported by principal engineer Matthew Cowan (who was one of the main drivers for Trello adding the concept of Power-Ups in the first place). The attendees had a good mix of backgrounds including email producers, product managers, and financial analysts from industries like education, media, hedge funds, textiles, and even comics. The workshop started off with some basic sample code Bentley provided on Glitch.com , a free-to-use cloud tool that allows you to build and run cloud apps. There was an initial sense of concern from the group when they saw what looked like a complicated developer environment—especially when very few of them had ever touched a line of code before. However, in the workshop we broke down each step of development, explained why and how we would be building a Power-Up, and went through the process to actually build and deploy it. The group gradually got more comfortable navigating and modifying the code, and only occasionally would raise their hand and jokingly exclaim \"uhhh, I think I need an adult\" if they ran into an issue. By the end of the two-hour workshop, the entire group had successfully completed a rating Power-Up utilizing multiple scopes for board-buttons, card-buttons, list-sorters, and even some light authorization work. You can check it out below! As the event came to a close, it was awesome to see the transformation of the group from being nervous to touch anything that looked too \"technical\" to actively asking questions about future Power-Ups they might want to build, where to find more documentation, and how to customize or do more cool actions in the Power-Ups they just built. (By the way, if this sounds interesting to you, head over to the Trello developer documentation to get started building your own Power-Up.) Many thanks to Community Leaders Sam Barrow and Jill Moloney for making event this happen! Our next pilot took us to Berlin, where developer advocate Peter Van de Voorde gave a three-hour workshop introducing new developers into the world of apps for Jira Cloud . Half the attendees were server app developers who wanted to get to know cloud development , and the other half were Atlassian admins who wanted to know how they could customize and extend our products to better fit their team's needs. After getting a Cloud developer instance spun up for everyone in the room, the first thing we did was get an API-Token and use curl to call the Jira Cloud REST API. (By the way, Peter has laid out all of the steps for this in a blog post for your perusal.) After a couple of hiccups, everybody got this running, and some people were able to try different REST API endpoints they found on developer.atlassian.com . Next up, we introduced them to the world of Connect (Atlassian's cloud development framework) , assisted in large part by Ralph Whitbeck's presentation at Developer Day . After going over the differences between cloud and server development at Atlassian (and figuring out some interesting problems with npm), the group started building an example Peekaboo app for Jira ( check out the code repo here ). By the end of the day, the entire workshop had their own fully functioning custom Jira Cloud app. For those who completed this quickly, we challenged them to build another cloud app by following the Jira Cloud Activity App Tutorial , and some did! Huge thanks to Community Leaders Joerg Mueller-Kindt, Hubert Kut, Khallai Taylor, and Huiyi Lin for making this event happen. Overall, these events brought our customers together, taught them something new, and inspired them to do more by developing with Atlassian—but our next project is figuring out how to scale this content event further. We will continue to look for ways to reach developers (or soon-to-be developers) in more parts of the globe, so stay tuned! If you have a great idea for a future developer workshop, or content you'd like to share with Atlassian's developer community, let us know by starting a community thread or submitting an article to the developer blog . Katrina is the Developer Community Manager for Atlassian's ecosystem.", "date": "2019-05-13"},
{"website": "Atlassian", "title": "Ecosystem Roundup: GDPR APIs rollout, JAC dashboards, Confluence 7 EAP", "author": ["Peter Van de Voorde"], "link": "https://blog.developer.atlassian.com/ecosystem-roundup-11/", "abstract": "In this post, we roundup all recent Ecosystem news. Check out the roadmaps for the rollout of our GDPR APIs for Bitbucket Cloud, Jira Cloud, and Confluence Cloud here . Our Jira Server and Data Center teams have released 2 dashboards on JAC that focus on current and future work planned for Jira Server and Data Center. Check them out here: Read the notice for more about this change. Did you know that our Confluence 7 EAP is now available for download? As this is a platform release it will contain breaking changes including Java 11 support, feature deprecations, and removal of deprecated code. Be sure to check out the latest version of the EAP here . And read more about it on the developer community . As always, if you are seeking help in building your apps there are three avenues available to you. If you seek help, mentoring, or support in building your app please head over to the Atlassian Developer Community where over 6500 developers and over 750 Atlassians are available to help out. If you found a bug , want to raise a feature request, or report an app security incident with any of our APIs, SDKs, or app frameworks please open an issue over at the Atlassian Ecosystem Developer Service Desk . Need Marketplace Vendor support, head over to the Atlassian Ecosystem Marketplace Vendor Support Desk .", "date": "2019-05-14"},
{"website": "Atlassian", "title": "When should I start using React Hooks?", "author": ["Samyak Singh"], "link": "https://blog.developer.atlassian.com/why-you-should-adopt-react-hooks/", "abstract": "tl;dr Should you start using React Hooks today? Only to write new components, not to refactor old ones. Why write components with Hooks? 1. Separate (and isolated) concerns. 2. Avoid juggling HOCs, render props, children as functions, and classes. 3. Avoid duplicate logic between lifecycle methods and components. React recently pushed v16.8 to introduce the long awaited Hooks API. It was Alpha released with v16.7 and is now stable for production. It introduces a new way to conceptualize and write components as functions. This post outlines 3 main reasons to adopt Hooks for your frontend components. In my attempt to refactor Confluence's Blog Tree component, I learned that it's not worth refactoring an existing component with Hooks, because: Designing a component with Hooks allows us to separate concerns from day 1. It helps us produce deterministic components (that are based on pure functions) by isolating side effects. Code examples below are simplified for clarity There are several reasons to adopt Hooks. These are the top 3 that I've witnessed: In Class components, data concerns tend to get coupled inside lifecycle methods. This creates components that are difficult to understand, refactor, and test reliably. In the Class example below (left), notice that the data-fetching, internal data-manipulation, and the DOM manipulation are all chucked inside componentDidMount. However, they live and get invoked separately in the Hooks implementation allowing us to cleanly isolate different aspects of the component. With the Hooks implementation (right), it is also easier to extract the isolated effects and their corresponding states to custom Hooks that can be reused in another component (e.g. PageTree). Do you remember React Mixins? Well, there's a reason we're trying to forget it . However, Mixins led to the creation of HOCs, render props, and children as functions. While these alternatives work well for different use cases of reusability, they introduce complex patterns in our React components. For example, the Class implementation below (left) nests two Context Consumers to create a false sense of hierarchy in the AdminBannerComponent. The ThemeContextConsumer has no data-dependency on the SessionContextConsumer. When more Context Consumers are added (e.g. ApolloConsumer, PageContext, etc.) the nesting becomes large, harder to understand, and less manageable. In the Confluence Frontend codebase, we constantly juggle between HOCs, children as functions, classes, and plain JS functions. Hooks allow us to replace several paradigms with just functions as seen on the right below. This simplifies our components by making them flatter (no unnecessary nesting) and more explicit (clear data source). Notice the difference in clarity between the two styles of component composition. Duplicate logic convolutes components. This happens a lot inside componentDidMount and componentDidUpdate, mainly because lifecycle methods are not flexible or reusable. With Hooks, the concept of \"lifecycle methods\" disappears. Now, functions (useEffect or custom Hooks) will be executed on every \"render cycle\", but can also be declaratively invoked only when certain values change. In the Class example below (left), loadPages is invoked when the component mounts and again when the component updates. The logic to load pages (and its corresponding performance events) are duplicated between componentDidMount and componentDidUpdate. Furthermore, a check to determine whether the currentPageId has changed is necessary to avoid unnecessarily loading pages. This is imperative and inflexible. In the Hooks counterpart (right), loadPages is executed more declaratively. It is invoked on render cycles when the currentPageId changes, but a manual check is not necessary. Similarly, subscribing and unsubscribing logic can now live together instead of being dispersed in componentDidMount and componentDidUpdate. useEffect allows clean up logic if a function is returned. Additionally, both invocations of useEffect below can be extracted into their own Hooks to be reused in other components that require loading pages and subscribing to newly added pages. Most of the info mentioned below can be found on React's Hooks documentation » Upgrade your react packages to v16.8.0 Hooks have been released for production in v16.8.0. All react packages need to be upgraded in order to consume Hooks according to the Hooks FAQ . How to use the state and effect Hooks The first two Hooks that will allow a functional component to incorporate state and perform async/impure actions are useState and useEffect . Learning how to use them helps understand when a Hook gets executed during a component's render cycle, and further allows for creation of custom Hooks. Rules of Hooks The Rules of Hooks outline and explain a couple of best practices while using Hooks. They also mention the Hooks ESLint plugin that can help enforce those rules. How to write custom Hooks The React documentation on custom Hooks does a good job identifying the need for a custom Hook, and then extracting the re-usable logic into a separate function. This is a nice place to start writing custom Hooks. Community maintained Hooks Hooks written and shared by the React community can prevent reinventing the wheel. Frequently used Hooks like useEventListener or useLocalStorage have already been written! I hope this blog post helps you understand the differences between a traditional and a Hook-based component, and is a starting point for adopting Hooks. If you have any feedback or questions, feel free to let us know on the Atlassian Developer Community , or on twitter @atlassiandev. Originally from Nepal, I graduated from Virginia Tech in 2016. I'm now a Frontend engineer for the Confluence Cloud team at Atlassian.", "date": "2019-05-08"},
{"website": "Atlassian", "title": "–force considered harmful; understanding git’s –force-with-lease", "author": ["Steve Smith"], "link": "https://blog.developer.atlassian.com/force-with-lease/", "abstract": "Git’s push --force is destructive because it unconditionally overwrites the remote repository with whatever you have locally, possibly overwriting any changes that a team member has pushed in the meantime. However there is a better way; the option –force-with-lease can help when you do need to do a forced push but still ensure you don’t overwrite other’s work. It’s well known that git’s push --force is strongly discouraged as it can destroy other commits already pushed to a shared repository. This isn’t always completely fatal (if the changes are in someone’s working tree then they can be merged), but at the very least it’s inconsiderate, at worst disastrous. This is because the --force option makes the head of the branch point at your personal history,  ignoring any changes that may have occurred in parallel with yours. One of the most common causes of force pushes is when we’re forced to rebase a branch. To illustrate this, let’s have a quick example. We have a project with a feature branch that both Alice and Bob are going to work on. They both clone this repository and start work. Alice initially completes her part of the feature, and push es this up to the main repository. This is all well and good. Bob also finishes his work, but before pushing it up he notices some changes had been merged into master. Wanting to keep a clean tree, he performs a rebase against the master branch. Of-course, when he goes to push this rebased branch it will be rejected. However not realising that Alice has already pushed her work, he performs a push --force . Unfortunately, this will erase all record of Alice’s changes in the central repository. The problem here is that when doing a force push Bob doesn’t know why his changes have been rejected, so he assumes that it’s due to the rebase, not due to Alice’s changes. This is why --force on shared branches is an absolute no-no; and with the central-repository workflow any branch can potentially be shared. But --force has a lesser-known sibling that partially protects against damaging forced updates; this is --force-with-lease . What --force-with-lease does is refuse to update a branch unless it is the state that we expect; i.e. nobody has updated the branch upstream. In practice this works by checking that the upstream ref is what we expect, because refs are hashes, and implicitly encode the chain of parents into their value. You can tell --force-with-lease exactly what to check for, but by default will check the current remote ref. What this means in practice is that when Alice updates her branch and pushes it up to the remote repository, the ref pointing head of the branch will be updated. Now, unless Bob does a pull from the remote, his local reference to the remote will be out of date. When he goes to push using --force-with-lease , git will check the local ref against the new remote and refuse to force the push. --force-with-lease effectively only allows you to force-push if no-one else has pushed changes up to the remote in the interim. It’s --force with the seatbelt on. A quick demonstration of it in action may help clarify this: Alice has made some changes to the branch and has pushed to the main repository. But here Bob rebases the branch against master: Having rebased, he attempts to push, but the server rejects it as it would overwrite Alice’s work: But Bob assumes that this is due to the rebase, and decides to push it anyway: However, if he had used --force-with-lease , he would have had a different result, as git would have checked that the remote branch had not in-fact been updated since Bob last fetched it: Of course, this being git there are some caveats. The standard one is that this only works if Alice has already pushed her changes up to the remote repository. This is not a serious problem, however as when she goes to pull the rebased branch she’ll be prompted to merge the changes in; if she wishes she can alternatively rebase her work onto it. A more subtle problem is that it is possible to trick git into thinking that a branch has not been modified when it has. The main way that this would happen under normal usage is when Bob uses git fetch rather than git pull to update his local copy. The fetch will pull the objects and refs from the remote, but without a matching merge does not update the working tree. This will make it look as if the working copy of the remote is up to date with the remote without actually including the new work, and trick --force-with-lease into overwriting the remote branch, as you can see in this example: The simplest answer to this issue is to simply say “Don’t fetch without a merge” (or more commonly just do pull, which does both), but if for some reason you wish to fetch before pushing with --force-with-lease there is a way to do this safely. As with so many things git, refs are just arbitrary pointers to objects, so we can just create our own. In this case we can create a “save-point” copy of the remote ref before we perform the fetch. We can then tell --force-with-lease to use this ref as the expected value rather than the updated remote ref. To do this we use git’s update-ref feature to create a new ref to save the remote state before any rebase or fetch operations. This effectively bookmarks the point at which we start the work we’re going to force push to the remote. In this we’re saving the state of the remote branch dev to a new ref called dev-pre-rebase : At this point we can do the rebase, fetch and then use the saved ref to protect the remote repository in case anyone has pushed up changes while we were working: As we can see --force-with-lease is a useful tool for the git user who sometimes needs to force-push. But it is far from a panacea for all the risks of --force , and it should not be used without first understanding how it works internally and its caveats. But in its most common use case where the developers just push and pull as normal, with the occasional rebase, it provides some much needed protection against damaging forced pushes. For this reason, I would hope that in a future version of git (but probably not until 3.0) it would become the default behaviour of --force , and that that the current behaviour would be relegated to an option that shows its actual behaviour such as --force-replace-remote . You might also enjoy our ebook, “Hello World! A new grad’s guide to coding as a team” – a collection of essays designed to help new programmers succeed in a team setting. Grab it for yourself, your team, or the new computer science graduate in your life. Even seasoned coders might learn a thing or two. Read it online now Click here to download for your Kindle", "date": "2015-04-29"},
{"website": "Atlassian", "title": "Recap: Developer Day 2019", "author": ["Katrina Morales"], "link": "https://blog.developer.atlassian.com/developer-day-recap/", "abstract": "This year at Summit we held our first ever Developer Day. 450 Marketplace vendors and Atlassian power users signed up to learn the fundamentals of building on Atlassian products, or learn best practices about security, performance, scale, etc. As our CTO, Sri Viswanath, mentioned during the keynote , we have held many events targeted towards the developers who make commercially available apps in our Marketplace. At Developer Day, we wanted to address not just the Marketplace vendors, but also the power users and admins of our products that spend time tailoring their instances to their companies’ unique needs. The breakout sessions in our Fundamentals track were geared towards this audience, giving them the skills to go beyond configuration and into making REST API calls in Jira, building macros in Confluence, etc. We kicked off the morning with a keynote from Sri and our Head of Engineering, Stephen Deasy. They announced that we have 25,000+ active developers on the Atlassian platform, and that 3LO (3-legged auth or OAuth 2) is ready for Jira Core , Jira Service Desk , Trello , and Bitbucket , with Confluence in beta. We shipped VSCode integrations for Bitbucket and Jira Software , so you can manage pull requests and issues right in your IDE. We also shipped multiple improvements to Trello's Power-Up framework, such as an alerting UI for end users, easier API access, and icon and color updates. Finally, Jira Software has been hard at work making it easier to import and display information from other SaaS products within Jira's UI, so you have to do less context-switching. From there, attendees had a jam-packed day with 18 different sessions to choose from, covering everything from JWT authentication to how we modernized Trello's web stack . Overall, our content was rated 4.2 out of 5. We will definitely incorporate the feedback from the sessions into our planning for next year, so thank you to those of you who took the time to rate them! We finished the day with some popcorn and refreshments and a fireside chat with Sri and co-founder and co-CEO, Mike Cannon-Brookes, moderated by Head of Product, Anu Bharadwaj. They talked about everything from the history of Atlassian's ecosystem to its future to their personal pet projects. Sri even did an encore of his Dr. Suess performance from Atlas Camp last year ! Next year's Summit will be back at Mandalay Bay, March 30 – April 2, 2020. We hope to see you there! In the meantime, check out all the Developer Day recap videos below. Katrina is the Developer Community Manager for Atlassian's ecosystem.", "date": "2019-04-29"},
{"website": "Atlassian", "title": "Recap: App Week Santa Cruz", "author": ["Katrina Morales"], "link": "https://blog.developer.atlassian.com/recap-app-week-santa-cruz/", "abstract": "App Week is a week-long event where Atlassians come to meet and work with our third-party app/integration developers. Each App Week has a different theme, but the main goal is to help developers ship their apps, or help improve their existing apps through the lens of usability, performance, security, etc. We (the Ecosystem Marketing and Developer Experience teams) host these events in different locations around the globe. Previous App Weeks have taken us to Amsterdam, Key Largo, Portland, Sydney, and more. This year in February, we gathered at the Chaminade Resort and Spa which overlooks the Pacific Ocean from the foothills of the Santa Cruz mountains. The product teams from Jira Software Cloud, Jira Service Desk Cloud, Statuspage, and Opsgenie came to work with app developers and product partners that are building apps or integrations for IT teams. 41 developers from 18 companies came to build apps and integrations for the abovementioned products. Monday kicked off with a few presentations from the product teams and ended with a reception happy hour on the patio. Most of the week, though, is spent unconference style. We believe when you get smart, motivated people in a room together, they need very little direction. This proved true when Friday rolled around and our attendees demoed what they'd been working on all week. Some highlights… The next App Week will be held in Berlin, June 3-7, 2019. The theme is Cloud for the Enterprise, so if you are building a cloud app that enterprise companies would benefit from, make sure you apply to come ! Space is limited and the deadline to apply is Friday, April 26. Katrina is the Developer Community Manager for Atlassian's ecosystem.", "date": "2019-04-22"},
{"website": "Atlassian", "title": "Applications open for App Week Santa Cruz 2019: IT Ops", "author": ["Katrina Morales"], "link": "https://blog.developer.atlassian.com/app-week-santa-cruz-2019-apply-now/", "abstract": "We're now accepting applications for the next App Week, which will happen February 11-15, 2019 in the beautiful Santa Cruz mountains in California. With the launch of Jira Ops and the acquisition of Opsgenie , Atlassian is looking to grow its footprint in the IT market. If you are currently building an app or integration that makes the lives of IT teams easier, we want to help. We'll have product team members from Jira Ops, Jira Service Desk (Cloud), Statuspage, and Opsgenie at App Week to help make your app or integration meet our mutual customers’ needs. Due to the limited space, this is an invitation-only event. Apply below to indicate your interest, and you will be notified if you have been accepted to attend. Katrina is the Developer Community Manager for Atlassian's ecosystem.", "date": "2018-12-20"},
{"website": "Atlassian", "title": "Announcing the new developer blog", "author": ["Miles Buckley"], "link": "https://blog.developer.atlassian.com/announcing-the-new-developer-blog/", "abstract": "The developer blog URL has changed! You will need to update inbound links and RSS feeds to the new URL: https://blog.developer.atlassian.com/feed/ We're excited to unveil a new look and better experience on the Atlassian Developer blog. Here’s what you can expect to see: We have condensed more than 100 tags into streamlined, clear, and navigable categories. We also added a search function so you can find the posts most relevant to you in seconds. This structure allows for more intuitive browsing. Our new look and feel will help organize information visually, too. We'll still be serving up the same product updates and helpful hints you're used to, but there's more! We'll be taking you through the process of creating apps and integrations with our products, selling them in our marketplace, plus inspiring you to look under the hood, customize, and create. We're also serving up career, business, and real-life tips for developers. Drop by the developer blog for productivity smarts, ideas on creativity and collaboration, entrepreneurial advice, and other tips to help you function well at work. You will now be able to browse content 64.5% faster. Our load time to full page interactivity has decreased from 10.2s to 6.2s! This blog is about you – the software developers and engineers that power this community. More of our blogs will feature your projects, your successes, and your stories. Want to get involved? Drop us a line . Miles is on Atlassian's Ecosystem team and works with our developers, Marketplace vendors, and customers to ensure a world-class experience when customizing and extending Atlassian products.", "date": "2019-03-27"},
{"website": "Atlassian", "title": "Important: GDPR changes in Jira Server", "author": ["Daniel Rauf"], "link": "https://blog.developer.atlassian.com/important-gdpr-changes-in-jira-server/", "abstract": "To improve the compliance of Jira Server with General Data Protection Regulation (GDPR), we will be introducing a set of changes in Jira over the next few months. Read on for all the details, dates, and the impact these changes will have on your apps. The most impactful changes are related to the 'right to be forgotten'. This will include the following new capabilities for admins in Jira: To let your app work with these new capabilities, we are going to create 4 new extension points that will inform your app about users being anonymized and the involved actions. All of the new extension points will be in the com.atlassian.jira.user.anonymize package. The following actions will help you better understand the whole anonymization flow and how the extension points fit into it: To learn more about these extension points, refer to Javadocs that will be included in the Jira 8.2 Early Access Program (EAP), or see Preparing for Jira 8.2 . We are releasing the initial shape of the SPI in the first Early Access Program (EAP) of Jira 8.2. Please be aware that all of the related classes are annotated as <code>@ExperimentalSpi</code> and it's likely that they will change. The current goal is to collect feedback from you to refine the SPI before making it final. These changes will have a major impact on apps for Jira Server, because changing the user keys for existing users might break things. Another major change is a new way of generating user keys for new users. The reason behind it is that we want to reduce the number of places where we store users' personal information. Currently, user keys are similar to usernames, and these often contain actual names of their users. Currently, the user key is the same as username, lowercased. For example, user Admin will get a user key admin . If there's a naming conflict, the user key will be set to a value like ID10100 . We're changing this approach so that all user keys for new users (no changes for existing users) have the following format: JIRAUSER10100 This change will be available to test in the first Early Access Program (EAP) of Jira 8.2, hidden behind a dark feature flag ( com.atlassian.jira.user.dbIdBasedKeyGenerationStrategy ). This should give you time and means to prepare and test your code against it. We will enable this new behavior by default in upcoming releases. The dark feature is there to give you a head-start, a way to check and prepare your apps. This change may break any code that makes assumptions similar to userkey == username . If your code differentiates between the user keys and usernames, you shouldn't need to make any changes. Realistically speaking, we suspect that it will uncover unexpected behaviors in your apps. In our case, we discovered two bugs in the production code, and had to modify a number of tests.", "date": "2019-04-17"},
{"website": "Atlassian", "title": "Register webhooks dynamically in Jira Cloud", "author": ["Simone Russo"], "link": "https://blog.developer.atlassian.com/register-webhooks-dynamically-in-jira-cloud/", "abstract": "I'm happy to announce a new Jira Cloud API that allows Connect apps to register webhooks dynamically, as opposed to having to declare them statically in their Connect descriptor. This API notifies apps when issues are created, updated, and deleted in Jira. These issues can be filtered based on a specified JQL statement. Note that only a subset of JQL is supported (see here ) for now. We’ve restricted to a subset of JQL because we want to make this as performant as possible. We aim at reducing the amount of polling that Connect applications make to Jira. Some apps are often running JQL searches in order to keep up to date with what's going on in Jira. This puts a lot of load on our infrastructure and makes Jira slower for users. Through this API, we’ve reduced the need for polling. We still recommend polling for the unsupported JQL.  Additionally, you can use polling to periodically recover from the occasionally missed webhook. For more information, check out the documentation , the REST API docs , or the example Connect app .", "date": "2019-04-16"},
{"website": "Atlassian", "title": "Java 11 official support is just around the corner", "author": ["Magda Michalak"], "link": "https://blog.developer.atlassian.com/java-11-official-support-just-around-the-corner/", "abstract": "tl;dr: In Jira 8.2 we are planning to officially announce the support for Java 11. It means that you can run Jira on OpenJDK 11 or Oracle JDK 11 and request our support should any issues arise. At the same time we'll continue supporting Java 8 for Jira 7.x and throughout Jira 8.x lifetime. In addition to OpenJDK 8 and OracleJDK 8, we will officially support running Jira on OpenJDK 11 and Oracle JDK 11. However, we still encourage the plugins to be developed using Java 8 to ensure Java 8 compatibility and tested on both Java 8 and Java 11. Also the installers will still have AdoptOpenJDK 8 bundled with them for the 8.2 release. Currently, we do support other OpenJDK's however we do not test our products with them. Apart from supporting Oracle JDK, we test and bundle most of our products with HotSpot distribution of AdoptOpenJDK. It also means that our Support team uses AdoptOpenJDK to replicate issues that have been raised using OpenJDK. If you use a different distribution of OpenJDK (e.g. Zulu) we'll still provide support for our products. However, if the reported bug is caused by a problem in Java distribution we'll ask you to reach out to the Java distributor for help. We are planning to release Jira 8.2 in late Q2 2019. The same goes for Jira Service Desk 4.2 that will also officially support Java 11. For more details see this blogpost . Yours, The Jira team", "date": "2019-03-19"},
{"website": "Atlassian", "title": "Applications open for App Week Berlin 2019: Cloud for the Enterprise", "author": ["Katrina Morales"], "link": "https://blog.developer.atlassian.com/apply-app-week-berlin-enterprise/", "abstract": "We're now accepting applications for the next App Week, which will happen June 3-7, 2019 in the heart of Berlin, Germany. As Atlassian's cloud growth moves into the enterprise, make sure you're ahead of the game with cloud apps that will meet their demands. We will have product teams from Jira Software Cloud, Jira Service Desk Cloud, Confluence Cloud, Bitbucket Cloud, and Trello at App Week to help you build a new cloud app, or to help you improve your existing cloud app with regards to security, scale, and performance. App Week really helped us to work focused on a new marketplace app. We build a near-production ready app within 4 days what would not have been possible without the help of Atlassians to guide it into the right directions. Due to the limited space, this is an invitation-only event. Apply below to indicate your interest, and you will be notified if you have been accepted to attend. The deadline to apply is Friday, April 26. Katrina is the Developer Community Manager for Atlassian's ecosystem.", "date": "2019-03-15"},
{"website": "Atlassian", "title": "Ecosystem Roundup – GDPR Weekly updates, Jira Cloud change notice, JSD Server Action required and more", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/ecosystem-roundup-10/", "abstract": "In this post, we roundup all recent Ecosystem news. We’ve started doing weekly updates on the work being done for API’s needed for the API Migration to accountID from username and user key which is being deprecated on 29 April 2019 . In addition, we’ve posted a couple of other guidelines that are relevant to GDPR: We are in the process of optimizing GET /rest/api/2/search in Jira Cloud so that it returns the highest achievable number of entries per page based on the fields requested. Read the notice for more about this change . In Jira Service Desk Server 4.1, we're making some look and feel changes to the help center and customer portal. This is the first round of improvements aimed at making the customer experience both simple and polished. In order to do this, we've had to make some changes to the frontend codebase, and this may impact third-party apps. Learn more about these changes. Confluence Cloud is refreshing the navigation . There are a few potential issues that could impact developers. Read more about the navigation refresh over on the Atlassian Community. Take a look at when and where we are planning events this year . Learn more and save the dates Jira expressions can be used to evaluate custom code in the context of Jira entities. It's a domain-specific language designed with Jira in mind, evaluated on the Jira Cloud side. Check out the new docs and learn more about Jira expressions. As always, if you are seeking help in building your apps there are three avenues available to you. If you seek help, mentoring, or support in building your app please head over to the Atlassian Developer Community where over 6500 developers and over 750 Atlassians are available to help out. If you found a bug , want to raise a feature request, or report an app security incident with any of our APIs, SDKs, or app frameworks please open an issue over at the Atlassian Ecosystem Developer Service Desk . Need Marketplace Vendor support, head over to the Atlassian Ecosystem Marketplace Vendor Support Desk . Ralph is a Developer Advocate for ecosystem developers.", "date": "2019-03-08"},
{"website": "Atlassian", "title": "Jira Cloud next-gen projects and the effects on the REST API", "author": ["Mythili Gopalakrishnan"], "link": "https://blog.developer.atlassian.com/jira-cloud-next-gen-projects-and-connect-apps/", "abstract": "In the spirit of openness, we want to take this opportunity to communicate some of the side-effects that Jira’s new next-gen projects (formerly known as agility projects or independent projects) have introduced to Connect apps. The next-gen projects simplify the project configuration experience and allow non-jira-admins to create projects and configure them. All project configuration entities in these next-gen projects like issue types, statuses, and custom fields – are scoped to the project. Schemes don’t exist in these projects. Project configuration entities that are created within a next-gen project are only available to that project. The launch of these next-gen projects has resulted in some change in behavior to the existing Cloud REST API v2.0 as listed below: Read operations return both global and project scoped entities, which results in a semantic change. There is no public REST API available to create project-scoped entities like issue types, statuses and custom fields: POST and PUT calls work as per contract when applied to classic project configuration objects. All write operations (create and update) will NOT work when operating on next-gen project entities. Ex: PUT https://x.atlassian.net/rest/api/2/issuetype/10235/properties/color If issue type 10235 is project-scoped this call will fail with a 400 or 500. Create and Delete board is not supported for next-gen projects Epic endpoints only work for classic boards. The endpoints may appear to work for next-gen projects, however data returned will be invalid. Board properties, Versions and Sprint properties should not be set for any next-gen projects as these may change in future releases. Additionally, if an app adds a web item into a project’s settings and provides a project settings configuration screen, this web item and project settings page will not appear in next-gen project’s settings. We have logged two tickets to track these issues: As a first step, we plan to fix the API for those apps that add a web item and project configuration settings, their settings appear under the next-gen project settings. Subsequently we plan to fix the GET calls to provide additional information for project scoped entities so apps can differentiate the global entities from those independent to a next-gen project. Finally we will prioritize project-scoped POST and PUT calls. Once we have addressed all the issues, we’ll follow up with another post on what changes Connect apps need to make to ensure the apps work in both classic and next-gen projects. In the meantime, if you have any questions or concerns, please comment on any of the two tickets listed above, or to engage in a deeper discussion, head over to this thread on the developer community forums.", "date": "2018-09-24"},
{"website": "Atlassian", "title": "The importance of distributed work for colocated teams", "author": ["Jannis Hegenwald"], "link": "https://blog.developer.atlassian.com/importance-of-distributed-work-for-colocated-teams/", "abstract": "Last week, I celebrated three months on Trello 🎉 and it's been awesome! The time has just flown by and I still feel the excitement of having “just joined” Trello. One of the biggest changes from my previous team is how Trello is embracing and practicing distributed work. Although I had worked in a distributed team before and feel comfortable in all kinds of distributed collaboration, I wasn’t quite sure how teamwork and particularly the design process would work this time, given the new team and all. But to my own surprise, the distributed work—some of which with people I’ve never met in person—is going way better than expected. More importantly, though, I’ve noticed that practicing distributed teamwork improves collaboration in colocated teams as well. But before we dive into that, some context: Unfortunately, this is often not the case. Here are a few common examples: By following some of the guidelines for distributed collaboration, however, we can mitigate some of these effects and create a more equal playing field. Let’s start with example 1 from above: Instead of putting the person who’s working from home on the big screen in the meeting room, each team member could dial into the meeting from their own computer, using the camera and Zoom’s gallery view ( Commandment #3 ). That way, everyone’s equally present in the meeting and the person working from home is not at a disadvantage. Additionally, since everyone gets the same amount of screen space, physical size doesn’t matter as much anymore. And since you can adjust the volume on your computer, your voice matters a little less as well. In our example above, we haven’t really done much to empower people who are naturally shyer or more reserved. This is where some of the practices of Trello’s Design Huddles can be helpful. For example: The reason I believe this is a big deal beyond just making distributed collaboration work is that these practices improve all collaboration, distributed or not. Here’s what I mean by that: In a way, we’re \"sub-consciously designing\" for the users with the deepest need, the teammates who are not able to participate equally. And in doing so, we're improving the collaboration for all teammates. For context, this approach is often referred to as designing for extreme users and a good way to design in general. In short, extreme users are users whose needs are amplified and therefore often easier to identify. Here are a few examples of products that were inspired by so-called extreme users: In a recent internal blog post about distributed work in large enterprises, Trello's head of design, Chris Kimbell wrote: “It gets harder when you consider what approach will ultimately be most beneficial to a large complicated enterprise with a broad range of configurations. Do the needs of the many outweigh the needs of the few? Or does a \"all for one, one for all\" approach make the most sense?” To me, the needs of the few point us in the direction of how we can improve things for the many: If you’re curious, I recommend trying it out. Here's a challenge: For four consecutive weeks host at least two distributed team meetings each week. Follow the guidance that Chris Kimbell, Courtney Drake, and Lauren Moon have laid out on the pages below and see how it affects your team's collaboration. If you’re taking the dare, I’d love to hear about your experience. If you’ve done something like this before I’d be just as interested in hearing your takeaways. If you have other thoughts about this or resources everyone should be aware of, send them over at @jannishegenwald or @trello or @atlassiandev . ✌️", "date": "2018-12-05"},
{"website": "Atlassian", "title": "Jira Cloud and Confluence Cloud profile pages are being replaced", "author": ["Blake Riosa"], "link": "https://blog.developer.atlassian.com/jira-confluence-user-profiles-being-replaced/", "abstract": "We've recently replaced Jira Cloud and Confluence Cloud user profile pages with a new, unified profile across Jira and Confluence. Customers were able to extend their old product profile pages temporarily; on 17 December 2018 Jira and Confluence user profile pages will be completely and permanently removed. The new profile pages will not facilitate replacements for the old Jira and Confluence profile page extension points. At this time, customers will not have the ability to install apps for the people directory or new user profile pages. If your app relies on the Jira and/or Confluence user profile page extension points please update your app listing in Atlassian Marketplace to reflect the deprecated functionality. Should your app solely rely on those extension points you may want to consider de-listing the app. For assistance de-listing your app please contact Marketplace Vendor Support . The affected extension points are: The new profile pages are part of the Atlassian people directory , which is part of the Atlassian Teamwork platform. These new user profile pages span across Atlassian products and allow users to centralize control of their profile information. This centralized control of profile information is an important part of our Atlassian Privacy Model and strategy to provide a better user experience for managing privacy settings.", "date": "2018-12-10"},
{"website": "Atlassian", "title": "Amazing App Week in Amsterdam", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/app-week-ams/", "abstract": "AAA, in the US, is an acronym that stands for American Automobile Association. However, in Europe, AAA stands for A mazing A pp week in A msterdam. On November 12 – 16, 2018, We brought 97 developers/vendors and 35 Atlassians to the AtlasCamp 2013 venue in Amsterdam, The Netherlands to work on Data Center and Cloud apps. This was our ninth week of this program (formerly called Connect Weeks). App Week is when our third-party app developers and Atlassians come together for one week and work hard developing their apps that help customers customize and extend Atlassian products for the unique needs of every team. \"The App Week proved, once again, we can really achieve outstanding results when working together with Atlassian.\" Content presentations As is traditional with App Week, we start off the first day with targeted content that is aimed to help developers throughout the week. This time we found an easy and cheap (free) way to record the presentations and we shared them with Marketplace Vendors ( Private Forum ) that couldn't attend. The content focused on Data Center and Server App development for the first half of the day and Cloud App development the second half. On Tuesday we ran another half day of content around GDPR, design, Atlassian Access, GraphQL (as promised at Atlas Camp), Onboarding, and Jira Performance. On Wednesday, we had one talk on the new Jira Cloud Expression API, which turned out to be very popular with developers. \"Intense and engaging content. Great and productive atmosphere\" Over the last couple of App Weeks, we've introduced scheduled 1:1 time (or Breakout sessions as we called them this time). Attendees could schedule a time with a team on a Confluence page for a 30-minute block of time on Tuesday, Wednesday, or Thursday. Vendors could sign up for time with 17 Atlassian teams. Meetings took place in boardrooms in the hotel. We also may have taken over some of the tables in the hotel bar/lobby. Vendors claimed this is some of the most valuable time in all of our events. “My first App Week was absolutely wonderful. Everyone at Atlassian was very welcoming and having direct access to them [sped] up our plugin development.” We worked hard for a week on our apps but we also played hard as well. On Monday we held a Happy Hour in the Amsterdam Office. Thanks to the Amsterdam Workplace Experience team for hosting us and to the Amsterdam Atlassian employees that came out to meet our vendors. On Wednesday, we walked downtown to play some Virtual Reality and have some drinks. Like I said before, we went back to the Koepelkerk Event Center which is part of the Renaissance Hotel in Amsterdam and was the venue for AtlasCamp 2013. It brought back good memories for some of our vendors. It's an old venue that has a lot of character and provided a great place to hold a semi-conference and hack-a-thon. Being part of the hotel we had amazing catering provided by the hotel. Great venue, location and food Food was delicious Part of the fun of App Week is seeing the demos. We ask that vendors demo what they accomplished during the week, and if they don't have something to demo, at least talk about what they learned during the week. This time we had 18 vendors give 23 demos on Friday. We’re close to announcing the next App Week. Make sure to check back on the Developer Blog or Atlassian Developer Twitter account to catch the announcement. Ralph is a Developer Advocate for ecosystem developers.", "date": "2018-12-18"},
{"website": "Atlassian", "title": "Ecosystem Roundup – Jira Cloud Platform API Roadmap updates, AUI 8.0, REST API update, and more", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/ecosystem-roundup-8/", "abstract": "The Jira Cloud Platform API Roadmap Trello board has been updated. Here are the initiatives that have been updated: Last year we learned a lot about how difficult it is in finding the latest information on new releases, EAPs, changes in cloud, etc. We've been working to change this and have made improvements in Developer Comms in the past year. However, there is still a lot of work to be done to make this the best it can be. We want to see if we're making things better and would love it if you can take our survey to let us know how we're doing. The survey should take you 5-10 minutes depending on how much detail you provide (The more detail you can provide the more helpful it is for us). T Wow, what a journey. 6 months ago, we started work on a major overhaul of AUI. On November 28, we finally released AUI version 8.0.0 ! In this version, we aimed to: Check out the Developer Community post for the full highlights In order to ensure no potentially sensitive user data is exposed several endpoints previously deemed safe for use by anonymous users now have restricted access. Now when you perform an anonymous GET request on the following REST API endpoints: A 401 will be returned if you request these endpoints anonymously. Summit is back and bigger than ever at Mandalay Bay South Convention Center in Las Vegas, NV. The world's next best innovations will be built by teams like yours. This is why our mission is to help unleash the potential in every team and why we hold Summit year after year. We encourage you to join the team for our annual US event where you'll be connected to the information and people you need. Here are just a few events you don't want to miss out on: Developer Day brings new and seasoned Atlassian developers together for a full day of learning and networking. Join us and see what you can build on the Atlassian platform! Partner Day brings together Solution Partners, Marketplace vendors, Corporate Resellers, Training Partners, and Affiliates from around the world to gain the insights needed for a competitive edge. Summit is an opportunity to get inspired, learn how to transform the way teams work, and witness Atlassian's latest innovations first-hand. For a limited time, save $300 on your pass when you register today! Learn more about sponsoring and inviting customers As always, if you are seeking help in building your apps there are three avenues available to you. If you seek help, mentoring, or support in building your app please head over to the Atlassian Developer Community where over 5800 developers and over 700 Atlassians are available to help out. If you found a bug , want to raise a feature request, or report an app security incident with any of our APIs, SDKs, or app frameworks please open an issue over at the Atlassian Ecosystem Developer Service Desk . Need Marketplace Vendor support, head over to the Atlassian Ecosystem Marketplace Vendor Support Desk . Ralph is a Developer Advocate for ecosystem developers.", "date": "2018-12-20"},
{"website": "Atlassian", "title": "A New Year will bring a new Confluence Cloud editor", "author": ["Shreshth Luthra"], "link": "https://blog.developer.atlassian.com/new-year-brings-new-confluence-cloud-editor/", "abstract": "Confluence Cloud is pivoting and opening up to a broader audience of business teams and knowledge workers. Part of how we'll accomplish this is by simplifying Confluence so it's accessible and easy-to-use by every team, not only product teams. Our first milestone, a reimagined content creation and editing experience is here and ready to be rollout out to our customers. As we roll this out, we'd make sure that our ecosystem vendors and partners are well prepared for the changes. Benefits of the new editor: Creating content is at the heart of how customers use Confluence, whether it’s taking notes in a meeting, creating a dashboard to help track a project, reviewing and providing feedback on a script, or brainstorming on a new design. We've received feedback that our existing editor is complex and difficult to use. We want to make this experience better, so we’re focusing on some of the biggest user feedback issues we’ve received. The goal of all of these changes is to give customers: Blogs and meeting notes in Confluence are already using the new editor. Come January, we will begin rolling these changes out across all page types on eligible customer instances, starting with the most simple customers first (based on activity and content that new editor support), with the aim to reach 90% of customers by April 2019. Our largest, most complex customer instances will be migrated last. Please keep in mind, timing of the rollout is subject to change and we will do our best to keep our ecosystem vendors and partners informed of the changes. In order to be transparent, we'd like to share the major milestones of our migration plan with our ecosystem vendors and partners. We will be sending customer admins communications, including emails and in-product notifications, about the new experience and migration process six weeks prior to their migration date. Main milestones to be aware of: In August 2018, we published a blog for live macros experience with new editor . We understand that you would still have lots of questions about the rollout plans and how would we enable migration of existing apps onto the new editor. In order to address these questions, we will post a blog for you in January 2019 with more details. In the meantime, if you have any questions, head over to the Developer Community with your questions/concerns. In the meantime, enjoy the end of year time with your friends and family. Happy holidays!!!", "date": "2018-12-21"},
{"website": "Atlassian", "title": "December Update on Upcoming Breaking Changes across Server Products", "author": ["Peter Scobie"], "link": "https://blog.developer.atlassian.com/december-update-upcoming-breaking-changes-across-server-products/", "abstract": "Building upon last month’s blog , we have an update on our plans to ship breaking changes across Server Products. While plans often change, we do our best to be open about our work so the Atlassian Ecosystem can plan ahead to ensure a smooth experience for our customers. We’ve consolidated these updates into a single, easy to digest, post and will provide monthly updates until the end of the year. We have some exciting new features lined up for the Bitbucket Server 6.0, with a planned release in late January, 2019. We also plan to release an Early Access Program (EAP) Bitbucket Server 6.0 release, including all breaking changes, in the first week or so of 2019. For one of the features in Bitbucket Server 6.0, we need to deprecate direct access to repositories on disk for apps. You can find all the necessary details in the API changelog . We’re also going to remove the notification handler plugin point , and the related notification API . When you need to send notifications just listen for the event you are interested in using a listener based on that specific event, and get the recipients via com.atlassian.bitbucket.watcher.WatcherService to send notifications. We’re going to add support for Java 11 in Bitbucket Server 6.0, as Java 8 is reaching end of life in January, 2019. After Java 8 reaches end of life, unless an Oracle Java 8 subscription is purchased, no further fixes, including security patches, will be available for it. As a core dependency for Bitbucket Server, such patches are critical, so we’re adding support for Java 11. We’ll also end support for PostgreSQL 9.2 , which has been deprecated since the Bitbucket Server 5.7 release (see the supported platforms page for details). We’re going to end support for all versions of Git prior to Git 2.11: Starting from Bitbucket Server 6.0 custom file handlers will no longer be used for rendering diffs, only for source views. Internal front-end events (prefixed with bitbucket.internal ) have never been supported as API but they will be unavailable from 6.0 onwards. The JavaScript TextView.api (which has been hidden behind an unsupported internal event since 4.0) will also no longer be available. This includes DiffView.registerGutter for adding extra information to the diff gutter. Instead we recommend using Code Insights to add extra information to the diff. Confluence 6.13 is now released, and brings with it support for AdoptOpenJDK 8, as an alternative to the paid Oracle JDK. Full Java 11 support is planned to be delivered with Confluence 7.0 sometime in 2019. We have also added the ability to permanently delete a user account and anonymise the personally identifiable information associated with it. If your app displays people’s names or usernames (think places like mentions, or the page byline), you should test how it behaves when a user has been deleted. Check out the release notes and preparing for Confluence 6.13 for more details. Confluence 6.13 will be our next Enterprise release line. We are continuing our work on the TinyMCE editor upgrade from 3.x to 4.x. This has been ongoing for some time now and hopefully everyone has checked their apps for compatibility. If not, check out how to test your add on with the upgraded TinyMCE4 editor . We expect that this will land in 6.14. Confluence Server 7.0 scope, in addition to full Java 11 support is likely to include, upgrading Guava and AUI. We will be removing some very old and deprecated code from 1.x through 4.x releases. We also have a number of planned changes in Confluence 7.0 regarding removal of unused features. Formal end of support announcements will be made closer to the release of Confluence 7.0 We are also working on a new Search experience to replace the existing Quick Search experience ( what is quick search? ). If you have an app which interacts with the search experience, check out preparing for Confluence 6.14 for more details. We are continuing our work on Jira Software 8.0. The release has now been moved to the end of January 2019. If you haven’t started following our Jira Software 8.0 EAP program , we highly recommend taking a look at the most recently released version. More information on the 8.0 Early Access Program can be found here . The Jira Software 8.0 release will include: The beta version of 8.0 has been released . This version is feature complete but as we discover and fix bugs, some minor changes might still occur. We expect to release a Release Candidate version roughly two weeks before the final release. This will be the same build we plan to release as the final version, and will give an opportunity to app vendors to test with the final release build before it is released to customers. Jira 8.0 will be compatible with Oracle JDK 8 and OpenJDK 11. This means that Java 11 features will not be supported in the source code (Java 8 compatibility mode). Hence, we will only announce limited Java 11 support without making Jira officially compatible with Java 11 until a future 8.x release. This will give app vendors time to update their products before we announce official compatibility with Java 11 . Most likely, this will take place sometime in Q1 2019. After we announce the official compatibility with Java 11, we will also plan to have OpenJDK 11 bundled with the Jira installer. We continue to work towards Jira Service Desk 4.0 and in service to that the new EAP is available now . We�ve removed com.atlassian.fugue from our automation API and SPI, and updated them to use Core Java Data types and Exceptions instead. You will need to update any automation-specific scripts, integrations or apps to use the new API and SPI. For more details and EAP access, please visit this page . The frontend platform team are responsible for maintaining and improving a number of projects that power the frontend of Atlassian Server products. The platform is frequently bundled in and provided by Server products. This means that, most of the time, any breaking changes announced by the frontend platform team will first need to be adopted by Server products before the community will need to adapt to them. However, it is worthwhile for us to get the signal to you sooner, in the spirit of Open Company, No Bullshit. The final version of AUI 8.0 has been released. Head to our community post for the announcement and links to further resources and answers to common questions. AUI 8.x will receive development and maintenance throughout FY19. AUI 7.x will receive occasional updates and maintenance throughout FY19. Found a bug? Raise a ticket in the AUI project on our Ecosystem Jira . A major version of the WRM, 4.0.0, was released in December ( released code , upgrade guide ), alongside the Java Platform version 5. WRM v4 includes the following backwards-incompatible changes: What these changes mean for app developers: Questions? Please raise a ticket in the Ecosystem Developer Service Desk", "date": "2019-01-07"},
{"website": "Atlassian", "title": "Confluence Server 6.14 beta released", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/confluence-server-6-14-released/", "abstract": "This week we released the first beta for Confluence Server 6.14. There are changes in this release that may directly affect 3rd-party apps. To find out what’s in this release, check out the Confluence 6.14.0 beta release notes . Download the beta from our site to start testing your app for compatibility with the changes. Be sure to take a look at our Preparing for Confluence 6.14 guide for details on changes that may be relevant for your app. We’ll update that page regularly, so keep an eye on it to see any changes. Ralph is a Developer Advocate for ecosystem developers.", "date": "2019-01-11"},
{"website": "Atlassian", "title": "30 Days until Stride and Hipchat Cloud are Discontinued", "author": ["Aileen Horgan"], "link": "https://blog.developer.atlassian.com/30-days-stride-hipchat-eol/", "abstract": "Updated February 5, 2019! We previously stated that on February 15th we would start archiving app listings. Instead we will be deleting app listings. We’ve updated the blog post to reflect this change. We're quickly approaching the final days of Stride and Hipchat Cloud – these products will be discontinued on February 15, 2019. All Stride and Hipchat Cloud APIs, developer support, and Marketplace support tickets will remain available through Feb 15, 2019; Stride and Hipchat Cloud services will shut down after this date. If you offer a Stride or Hipchat Cloud app through the Atlassian Marketplace you have until Feb 14, 2019 to set your listing to archived. On February 15, 2019, Atlassian will remove Stride and Hipchat Cloud filters on the homepage and begin deleting app listings with integrations to these products. Hipchat Server and Hipchat Data Center apps can remain listed through the product end of life. However, we will no longer accept any new Hipchat Server or Hipchat Data Center apps on the Atlassian Marketplace. We are strongly encouraging customers who want to retain their Stride and Hipchat data to begin exporting their data now, as well as exporting any associated app data. The last day to export data from Stride and Hipchat Cloud is March 15, 2019. Hipchat Server and Hipchat Data Center versions will be supported through the following dates: Hipchat Server and Hipchat Data Center APIs, developer support, and Marketplace support tickets will remain available through the dates listed above; Hipchat Server and Hipchat Data Center services will shut down after each date. Please visit the Stride development developer community board or Hipchat development developer community board if you have additional questions about how this impacts your app(s). Lastly, a heartfelt thank you to those of you who invested time, resources, and energy into building on top of these products. You've created delightful customer experiences and made our products better as a result. If you're interested in building your Stride or Hipchat app on top of the Slack platform, check out this talk from AtlasCamp 2018 for recommendations from Atlassian and Slack teams.", "date": "2019-01-15"},
{"website": "Atlassian", "title": "Jira 8.0 has a Release Candidate!", "author": ["Magda Michalak"], "link": "https://blog.developer.atlassian.com/jira80-rc/", "abstract": "Please join me in welcoming the long-awaited Jira 8.0 Release Candidate! This Release Candidate is the final step before the actual release of Jira 8.0, which is just round the corner and due around Feb 4th-8th. The Release Candidate mostly incorporates the feedback we've received after the beta. We've also performed several tests and upped the heap requirement for Jira 8.0 to 2GB. You can read more about it in Preparing for Jira 8.0 . If nothing unexpected comes up, we are going to use this Release Candidate's build for the final GA. Download the Release Candidate to see it in action. If you're downloading mavens from maven.atlassian.com , download maven 8.0.0-m0033. For the list of features expected in Jira 8.0, see the Release notes . Should you have any comments, voice them here . Best, The Jira Server team", "date": "2019-01-23"},
{"website": "Atlassian", "title": "Updated best practices for Server app design", "author": ["Emilee Spencer"], "link": "https://blog.developer.atlassian.com/atlassian-design-guidelines-announcement/", "abstract": "Whether you are building a new app or refactoring an existing app, consider our plan to use Atlaskit for new development across server and cloud. Currently, Atlassian maintains two different Atlassian Design Guidelines (ADG), one for server which directs developers to implement the frontend using Atlassian User Interface (AUI) and one for cloud which directs developers to implement the frontend using Atlaskit. This separation has presented certain challenges for both Atlassian and vendors building apps on multiple hosting types. We have since decided that server products will begin to use Atlaskit and that Atlaskit will be added as an implementation option for the server guideline. The intention of this shift is to slowly merge the two guidelines in the future into one design system. By converging upon one design system, Atlassian aims to provide consistent, cohesive design patterns that will benefit the teams using the system. If you are building or refactoring an app for server, you can now use either AUI or Atlaskit to implement the ADG. We will begin explicitly calling out how to use Atlaskit vs AUI for server products in the ADG for server. When the first part of this is published, we will share it with this community. In the long run, we expect cloud and server guidelines will begin to converge as we bring Atlaskit more and more into different server products. Once we have reached sufficient commonality between these, we will deprecate the deployment-specific guidelines and publish one canonical ADG guide that covers cloud and server – Atlaskit and AUI. After this point, new development for server should use Atlaskit. While plans are evolving, we expect the transition to include the following: For more information on how this change will impact your server app, please watch the recorded webinar on this topic or add a question to the community thread .", "date": "2019-01-23"},
{"website": "Atlassian", "title": "How to speed up your Jira and Confluence app migrations to accountID", "author": ["Alexandra Kassab"], "link": "https://blog.developer.atlassian.com/speed-up-your-jira-confluence-migrations-accountid/", "abstract": "We're halfway through the deprecation notice period for Jira Cloud and Confluence Cloud REST APIs to improve user privacy in accordance with the European General Data Protection Regulation (GDPR) . This means there are less than 3 months left (deprecation period ends 29 March 2019) to update your apps to use accountID instead of username and/or user key. If you haven't started or are currently working on updating your app(s) here are a few tips that may help. Test your changes by opting-in to the new APIs behaviors before the end of the deprecation period. To opt-in early, for your cloud app, you will need to update your app descriptor with apiMigrations set gdpr to true . See example: For Jira Cloud REST API calls, whether you're making the requests in your app or outside of an app, you will need to add the header , x-atlassian-force-account-id: true , to every request. For Confluence Cloud REST API calls, whether you're making the requests in your app or outside of an app, you will need to add the query parameter privacyMode=true to any request. To learn more about the API migration Opt-in see the documentation for Jira Cloud and Confluence Cloud . Please note that Atlassian may continue to send some personal data fields in certain operations, even if the opt-in mechanisms described above have been implemented. An example: Some user references in webhook event bodies may still contain usernames. We are continuing to work through these during the deprecation period. If you have concerns please reach out to us for further information. After the deprecation period, change logs will no longer include usernames. You can force Jira Cloud to return change logs with accountID instead of username by using the opt in mechanisms described above. After the deprecation period, JQL will no longer accept username or user key as a search clause. To help update your JQL queries, we've introduced a new operation which takes one or more JQL queries with username and/or user keys and converts them to an equivalent JQL query with accountID(s). Read more about the PD Cleaner here . The Connect app migration guide for Data Migration has this helpful high-level migration guide: In order to migrate existing data, apps can use the REST resource /rest/api/2/user/ on the Atlassian application instance (Jira, Confluence, or Bitbucket) to retrieve user details, and update data in their store with the Atlassian account ID of each user. For example, here is a high level migration guide: Recommended: Delete saved usernames and user keys. Only store accountIDs. We recommend that you delete saved usernames and user keys once you've completed your migration work and only store accountIDs. AccountIDs are safe to store identifiers, while usernames and/or user keys may resemble more direct forms of personally identifiable information. For your Jira Cloud apps, if you are storing usernames or user keys in your own systems, you can batch find the equivalent accountIDs for those users using a bulk user search . After the deprecation period, user search will no longer accept username or user key. If you need help during the migration of your app to use accountIDs please head over to the Developer Community and raise a thread asking for help. Want to learn more about changes related to privacy? We’ll be holding a Webinar on February 12th to help you during your migration. Register today . Alex is a Senior Product Manager on the Ecosystem team.", "date": "2019-01-24"},
{"website": "Atlassian", "title": "Ecosystem Roundup – Webinar on GDPR Readiness, Atlaskit package deprecation, legacy mode for time tracking in Jira Cloud being removed, and more", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/ecosystem-roundup-9/", "abstract": "In this post, we roundup all recent Ecosystem news. Do you run or manage Atlassian Cloud apps and integrations? If so, are you aware of the upcoming March 29, 2019 deadline to ensure that those apps and integrations comply with Atlassian's developer guidelines for GDPR? If these questions made your heart race, don't sweat, we've got answers for you. Head over to the Developer Community to learn how to register for our upcoming webinar on Tuesday, February 12th, 2019 at 1 PM PST and learn how to ensure your app complies with Atlassian’s developer guidelines for GDPR. Catch our recent blog post for tips on how to speed up your migrations to accountID . The Atlaskit team has deprecated the @atlaskit/util-shared-styes package. If you’re using this package in your app please follow @atlaskit/theme migration guide for migration to @atlaskit/theme package in your project. As we roll out the new Jira issue view, we’re working to simplify configuration to make it easier to get key tasks done. We’ve decided to remove Jira legacy mode in time tracking on February 11, 2019 . We’ll disable legacy mode for time tracking for sites that have it enabled on that date, and remove the ability to enable it for all sites. It’s important to note that we’re removing legacy mode for both the new and old issue views, as everyone will eventually move over to the new issue view. We don’t foresee a developer impact with this removal as the number of customers currently using it is very low. Please read the announcement for more information . As always, if you are seeking help in building your apps there are three avenues available to you. If you seek help, mentoring, or support in building your app please head over to the Atlassian Developer Community where over 5800 developers and over 700 Atlassians are available to help out. If you found a bug , want to raise a feature request, or report an app security incident with any of our APIs, SDKs, or app frameworks please open an issue over at the Atlassian Ecosystem Developer Service Desk . Need Marketplace Vendor support, head over to the Atlassian Ecosystem Marketplace Vendor Support Desk . Ralph is a Developer Advocate for ecosystem developers.", "date": "2019-01-25"},
{"website": "Atlassian", "title": "Macro updates vendors need to make for the new Confluence Cloud editor", "author": ["Klaus Ihlberg"], "link": "https://blog.developer.atlassian.com/macro-updates-for-the-new-confluence-cloud-editor/", "abstract": "Late last year we announced that A New Year will bring a new Confluence Cloud editor . This, as promised, is our follow up post in which we're going to cover additional details about how we're rolling this change out, and what it will mean for you. We're going to be rolling out the new editor to all eligible Confluence Cloud customers during the first quarter of this calendar year, starting in late Jan 'Eligible' means that in the last 90 days , no user or app on this instance has created content that we can't support or migrate. If a user or app has created content in the last 90 days that can't be migrated, that entire site will be temporarily excluded from the rollout. If a site has unsupported content older than 90 days, they will still get migrated, but those pages will continue to open in the old editor. We'll be rolling out gradually, starting with the simplest, most dormant customers, and ending with the most complex and active. For each cohort, there will be a 6 week sequence that starts with communications about the migration (to both admins and end users) and a trial of the new editor, and ends with the entire instance migrated to the new editor. If you're a vendor offering macros for the existing Confluence editor, you'll need to ensure that your macros also work with the new editor. To do this, please follow the instructions below that correspond to your macro's body type. All macros will continue to work in the old editor, both before and after you've made the following updates, although they may look and behave differently. We recommend testing updated macros in both editors to ensure that everything works as expected. These are block or inline dynamic content macros with the following in atlassian-connect.json In the new editor, these macros will now be WYSIWYG (what you see is what you get). This means that they will no longer use the macro placeholder image, but will look the same while editing as they do when published. These macros should continue to work in the new editor, but you may need to update their sizing to accommodate the new width options. After updating the macro, you'll also need to verify that the updates have flowed through to the editor. The width of block level macros can be either block , wide , or full-width . This is controlled by the editor, so make sure that you haven't set any width values in atlassian-connect.json or in the CSS, as this may cause the macro to work incorrectly. The height of the macro is controlled by the CSS macro height, which you can set and change dynamically as appropriate. After editing a WYSIWYG macro in a custom macro editor , you need to update its parameters in order to trigger an update in the editor. This can be done by storing a revision number in the parameters, or by updating any other parameter you might need to store within the macro. These are block or inline dynamic content macros with the following in atlassian-connect.json These macros may continue to work as they are, or may require significant modification, as we'll cover below. We'd announced the deprecation of nested bodied macros in our post Introducing live macros to support a new editing experience in Confluence Cloud . This will affect you differently based on whether your macro depends on nesting another bodied macro, or on the rich text functionality. An example of this is some of the macros in Confluence Server, which provide tabs by having a rich-text bodied container macro, with additional rich-text bodied container macros within it for each of the individual tabs. The ability to nest bodied macros is no longer supported in the new editor. We know that this will be disruptive to existing vendors who have Cloud macros that rely on this functionality, but the overall usage across our customers in Cloud instances is very small. To help deal with this, we won't be rolling out the editor to any instances that have created new pages with nested bodied macros in the last 90 days, until we have a way to migrate those macros . We recommend changing these macros to be WYSIWYG, and using the custom macro editor for any inputs or settings that can't be changed directly from the page. The new editor is part of https://atlaskit.atlassian.com/ Macros that don't rely on nesting other bodied rich-text macros but which only utilize the container will continue to work in the new editor, with the caveat that other bodied macros cannot be nested inside them. This means, for example, that you can no longer create an expand or page properties macro inside another bodied macro. Macros that don't have a body, like a Jira issue macro or task report macro can still be nested inside bodied macros. We are planning on converting the expand and page properties macros to native editor components so that, in the future, you will be able to nest them inside rich-text macros. This is not currently supported , but will be in the future. Watch this ticket to follow its progress and be notified when it’s complete. CONFCLOUD-65437 To convert your existing macros to WYSIWYG macros, you need to change the body type of your dynamic content macro in atlassian-connect.json from: to: Then, when someone requests a page that has the old bodied macro, we'll store the macro body in a content property. You can request the existing content of the old bodied macro using the REST API. The content property will be stored under the page where the macro is contained using the macro key as an identifier. You'll still need to decide what to do with the storage format returned by the APIs, and how to present that in the new WYSIWYG macro. There are APIs to convert the storage format to the view format, as well as to the Atlassian document format, which is the format used by the new atlaskit editor. https://developer.atlassian.com/cloud/confluence/rest/#api-contentbody-convert-to-post Testing You can test these changes by doing either of the following: We have answers! Head over to the Developer Community with your questions, concerns, or any other feedback, and we'll be happy to help you out.", "date": "2019-01-25"},
{"website": "Atlassian", "title": "Bitbucket Server 6.0 Early Access Program (EAP) release", "author": ["Ben Humphreys"], "link": "https://blog.developer.atlassian.com/bitbucket-server-6-eap/", "abstract": "3.21\" parameter can be omitted. The atlas-version command can be used to determine the installed version. If you have any questions or feedback about this preview of Bitbucket Server 6.0, ask them in the Bitbucket Server category in the Atlassian Developer Community forums , and our friendly team of developers will respond to them. Bitbucket Server 6.0 EAP contains several changes that may affect existing add-ons. The changes are summarised below. Interfaces, classes, and methods in the Bitbucket Server Java API that were previously marked as deprecated have been removed. Plugins that use any of these interfaces (which would have generated deprecation warnings when built against Bitbucket Server 5.x) generally won't build with Bitbucket Server 6.x Precompiled plugins that used any of the removed interfaces will fail to install or run in Bitbucket Server 6.x, typically with java.lang.NoSuchMethodError or java.lang.ClassNotFoundExceptions . Bitbucket Server 5.0 introduced a new Repository Hooks and Merge Checks API. In 6.0 the legacy API has been removed. For an overview of the new API please see the Repository Hooks and Merge Checks Guide . In Bitbucket Server 5.10 direct access to our repositories on disk for plugins was deprecated. In 6.0 the deprecated API that permitted this has been removed. For further information please refer to the Bitbucket Server API Changelog entry for 5.10. Please refer to the Bitbucket Server 5.16 Javadoc for finding replacements for other deprecated/removed API: Plugins written in Scala were able to depend on Bitbucket Server (in versions older than 6.0) to provide the Scala runtime library (org.scala-lang:scala-library) as such did not need to bundle it in the plugin. As of Bitbucket Server 6.0 Bitbucket Server does not provide/export this dependency. Bitbucket Server 6.0 adds support for running on a Java 11 JRE, in addition to retaining support for Java 8. Plugins should be built with a target version of Java 8 which implies a language level of Java 8 also. Plugins must be tested on both Java 8 and 11. Note: Java 11.0.2 is unsupported due to Java bug JDK-8217364 . Bitbucket Server 6.0 upgrades Atlassian User Interface (AUI) from 7.x to 8.0 For more information on upgrading to AUI 8 see the AUI 8 upgrade guide . Though they were never API, some plugins have depended on web-resources in the plugin com.atlassian.bitbucket.server.bitbucket-web . In 6.0, most of the dependencies will stop working. Please use an equivalent public API , if available. Some exceptions were made for heavily used resources, but these will eventually be replaced with supportable implementations of API and will also be removed in the future. The temporary exceptions are: Similar to the above, some internal AMD modules were depended on by plugins. In 6.0, third-party plugins that depend on modules starting with bitbucket/internal/ will stop working. Where an equivalent public API is available, use that. Otherwise, consider implementing your own version of the functionality. Some exceptions were made for heavily used modules, but these will eventually be replaced with supportable implementations of API and will also be removed in the future. The temporary exceptions are: Events (consumed using bitbucket/util/events) starting with bitbucket.internal have never been official API but will no longer be able to be subscribed to and should not be considered stable or part of any official API. As a result of the changes to internal JS events and other changes, the TextView API (which was only accessible via an internal event) will no longer be accessible or supported. The TextView API was for modifying the source or diff view. Where possible it is suggested to use Code Insights to display extra information on a diff. Key methods of the TextView API were addLineClass , addLineWidget , and registerGutter/setGutterMarker . Custom file-handlers are no longer supported for diff views (but continue to be supported for source views). Any custom file-handlers registered for diff-views will be ignored when resolving the appropriate handler. These changes should not impact the development or running of third-party plugins. Bitbucket, along with most Atlassian Server and Data Centre products, provide a standard set of libraries and plugins for which API is stable for a given major release. These are known as the platform . In Bitbucket Server 6.0 these have been updated, with many including major version updates. atlassian-concurrent-util : Note the package name has also changed from com.atlassian.util.concurrent to io.atlassian.util.concurrent . In order to ensure a smooth transition, the old package is still exported and will continue to be until Bitbucket Server 7.0 is released. fugue : Note the package name has also changed from com.atlassian.fugue to io.atlassian.fugue . In order to ensure a smooth transition the old package is still exported and will continue to be until Bitbucket Server 7.0 is released.", "date": "2019-01-29"},
{"website": "Atlassian", "title": "Jira 8.0 and Jira Service Desk 4.0 are out! Read all about this dream team!", "author": ["Magda Michalak"], "link": "https://blog.developer.atlassian.com/jira80-release/", "abstract": "We are proud to present our latest grand achievements: Jira 8.0 and Jira Service Desk 4.0. Packed with new features, updates and improvements, they are ready to be downloaded and put to test. Read all about the new software features in our release notes . For more technical insight, check the software upgrade notes . Also, make sure to check out the Service Desk features here and 4.0 upgrade notes . Download Jira 8.0 to see it in action. To download Jira Service Desk, go here . If you’re downloading mavens from maven.atlassian.com , download maven 8.0.0-m0035. Jira 8.0 will also include access to the Jira Software iOS and Android apps, currently in beta. Soon you will be able to stay on top of the latest developments with your team, capture an issue, reprioritize, or share an update-wherever you happen to be. The app is nearly ready to be released so look out for it on Google Play and the App Store. As always, we are looking forward to your feedback. Best, The Jira Server team", "date": "2019-02-11"},
{"website": "Atlassian", "title": "Are you ready for the GDPR API deprecation?", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/app-opt-in-api-migration/", "abstract": "We've recently introduced a new mechanism in Atlassian Connect that allows developers to opt-in to new APIs before the deprecation period ends on the outgoing APIs. This allows developers to test their apps and then signal that their apps are ready for the end of the deprecation period. By adding the apiMigrations field to your app-descriptor you can then subscribe to the active migrations that are currently available. Currently, the only active migration is the GDPR migration to improve user privacy . The deprecation period for this migration ends on 29 April 2019 . Before doing any work you should update and publish an update to your app descriptor to your Marketplace listing by setting the apiMigrations.gdpr property to false . We are actively monitoring this flag for published apps. Updating this flag to false tells us that you are actively working on migrating your app and that you are not yet ready for deprecation. Setting this flag to false will have no effect on the APIs your app uses. In your local development environment, set the apiMigrations.gdpr flag to true . This will subscribe you to the new APIs which will modify the Connect API behaviors . For the GDPR migration, this means removing legacy user references (username and user key) as valid input parameters or values in the user object response. For Jira REST APIs, you'll also need to force them to use GDPR-compliant functionality by including the x-atlassian-force-account-id: true header in all Jira REST API calls. For Confluence REST API calls you can force to use GDPR-compliant functionality by setting the query parameter privacyMode=true . During the GDPR migration, we will be posting weekly updates to the Developer Community with status on any potential blockers that arise for vendors during their migrations. If you run into a blocker that isn't listed in the weekly update then please reply to the latest update to make us aware of the issue. Once you have tested your app and can confirm that it will work with the new user privacy APIs then you're ready to publish your app to the Atlassian Marketplace. Set the apiMigrations.gdpr flag to true in your published app’s app descriptor and include the header or query parameter in all of your REST API calls as detailed in Step 2 in your published app. Once published your app is now using the new GDPR-compliant APIs and you've signaled that your app is ready for deprecation of the old APIs. Ralph is a Developer Advocate for ecosystem developers.", "date": "2019-02-26"},
{"website": "Atlassian", "title": "Update on GDPR Deadlines you don’t want to miss", "author": ["Alexandra Kassab"], "link": "https://blog.developer.atlassian.com/update-gdpr-deadlines/", "abstract": "Previously we've communicated that apps are required to complete their migration to Atlassian AccountID and remove legacy user references (username and user key) by March 29, 2019 . Due to the amount of remaining open issues raised by you , the Developer Community, we've decided to extend the deprecation notice period by 30 days. The new target deprecation date for Jira Cloud, Confluence Cloud, Bitbucket Cloud, and Connect REST APIs is April 29, 2019 . We are tracking use of the apiMigrations flag in the Connect App descriptor as a way to understand developer readiness to deprecate username and user key from our REST APIs. This flag serves as both a mechanism to test the new API behaviors (when set to gdpr:true ) and a communication tool to signal when you are blocked (when set to gdpr:false ). Today, we have 62 apps that have signaled that they are ready for the deprecation, 15 that have signaled they are blocked, and ~700 that have not indicated their status. In order for us to understand where you are with migration to accountID and use that information to effectively communicate with our product teams, we need you to signal your status using the \" opt In mechanism \" . Learn more about how to properly use this flag. Alex is a Senior Product Manager on the Ecosystem team.", "date": "2019-03-05"},
{"website": "Atlassian", "title": "Jira Server 8.0 Beta fresh from the oven", "author": ["Magda Michalak"], "link": "https://blog.developer.atlassian.com/jira80_beta/", "abstract": "The long-awaited beta for Jira Server 8.0 is out and ready for testing. It contains all the features we intend for Jira 8.0 and all the planned breaking changes that we were gradually releasing in EAP milestones. For a list of all the breaking changes, see Preparing for Jira 8.0 . Then check out the features in the Release notes , and then download the beta to see it all in action. If you're downloading mavens from maven.atlassian.com , download maven 8.0.0-m0030. As our special icing, we are also releasing a beta version of the upgrade documentation. Check it out here , and, should you have comments, add them in the community post . You can also tell us what you think about the beta release by posting a comment here . Best, The Jira Server team", "date": "2018-12-17"},
{"website": "Atlassian", "title": "Hacking the DockerHub into Bitbucket", "author": ["Nicola Paolucci"], "link": "https://blog.developer.atlassian.com/hacking-the-dockerhub-into-bitbucket/", "abstract": "Today, I want to tell you about the results of a hackathon, or as we call it a ShipIt project. It’s fun for me to share and I hope you’ll find it interesting. At Atlassian we have a big culture of innovation and experimentation. Every quarter, the company stops for 24 hours and employees can pick their own project to scratch their own itch: they form groups, sprint and spike working on new ideas. Sometimes we work on things completely off the wall, sometimes tiny improvements. Several key features included in our products (or even entirely new products) came out of these sprints of innovations. The ShipIt time is always very exciting for me. A few months back I was in San Francisco and I paired up with a couple of colleagues on a Docker related project, as you might know or not know, I have been very excited about the technology and I have been talking about it a lot. So here’s my idea: the Docker registry collects a lot of software packages and or ready-made images. We already have a cool integration with the Docker registry in the sense that you can set-up the Docker registry to rebuild your image anytime you push something to Bitbucket. It would be nice if we could show the state of the images and also the interaction with the Docker community on Bitbucket itself. So what I needed to develop a solution was: But enough teasing! Have a look at the video for the (then live) DEMO and an extended explanation of the Docker setup used. Full transcription", "date": "2015-01-14"},
{"website": "Atlassian", "title": "A better pull request", "author": ["Tim Pettersen"], "link": "https://blog.developer.atlassian.com/a-better-pull-request/", "abstract": "If you’re using Git , you’re probably using pull requests. They’ve been around in some form or other since the dawn of DVCS. Back before Bitbucket and GitHub built fancy web UIs, a pull request might’ve simply been an email from Alice asking you to pull some changes from her repo. If that sounded like a good idea, you could run a few commands to pull the changes into your master branch: Of course, randomly pulling Alice’s changes into master isn’t a great idea. master represents the code that you’re intending to ship to your customers, so you typically want to keep a close eye on what gets merged in. Rather than pulling into master , a better pattern is to pull them down into a separate branch and inspect the changes before merging them in: Using git diff ‘s “triple dot” syntax shows us the changes between the tip of alice/master and its merge base (or common ancestor) with our local master branch. This effectively shows us all of the changes that Alice wants us to pull. git diff master...alice/master is equivalent to mastergit diff A B At first glance, this seems like a reasonable way to review the changes involved in a pull request. In fact, at time of writing, this appears to be how most git hosting tools have implemented their pull request diffing algorithms. However there are a couple of problems with using the “triple dot” diff approach to generate a diff for a pull request. In a real project, the master branch is going to significantly diverge from any given feature branch. Other developers will be working on their own branches and merging them in to master . Once master has progressed, a simple git diff from the feature branch tip back to its merge base is no longer adequate to show the real differences between the two branches. You’re only seeing the difference between the branch tip, and some older version of master . The “triple-dot” git diff master...alice/master doesn’t take into account changes to master Why is not seeing these changes in the pull request diff a problem? Two reasons. The first problem is something you probably run into fairly regularly: merge conflicts . If you modify a file on your feature branch that has also been modified on master , git diff is still just going to show you the changes that have been made on your feature branch. git merge on the other hand will spit out an error and spew conflict markers all over your working copy, showing that your branches have irreconcilable differences. Or at least differences beyond the capabilities of git’s sophisticated merge strategies. No-one enjoys resolving merge conflicts, but they’re a fact of life for all version control systems. At least, version control systems that don’t support file-level locking, which has its own problems. But merge conflicts are far preferred to the second problem you can run into if you use a “triple dot” git diff for pull requests: a special type of logical conflict that will merge cleanly, but can introduce subtle bugs into your codebase. If developers modify different parts of the same file on different branches, you might be in for some trouble. In some cases, different changes that work independently and appear to merge happily without conflicts , can actually create a logic bug when combined. This can happen in a few different ways, but one common way is when two or more developers incidentally notice and fix the same bug on two different branches. Consider the following javascript for calculating the ticket price for an airfare: There’s a clear bug here – the author has neglected to include the customs fee in the calculation! Now imagine two different developers, Alice and Bob, each notice this bug and fix it independently on two different branches. Alice adds the customsFee before the immigrationFee : And Bob makes a similar fix, but on the line after the immigrationFee : Because different lines were modified on each branch, these two branches will both merge cleanly into master , one after the other. However, master will then have both lines . And, a serious bug that will cause customers to be double-charged the customs fee: (This is obviously a contrived example, but duplicated code or logic can cause pretty serious problems: goto fail; anyone?) Assuming you merged Alice’s pull request into master first, here’s what Bob’s pull request would look like if you used a “triple-dot” git diff from the branch tip to the common ancestor: Because you’re reviewing a diff against the ancestor, there is no warning of the impending doom that will occur when you hit the merge button. What you really want to see in a pull request is how master will change when you merge Bob’s branch in: This diff clearly shows the problem. A pull request reviewer will spot the duplicated line (hopefully) and let Bob know that the code needs some rework, thus preventing a serious bug from reaching master and eventually production. This is how we decided to implement pull request diffs in Bitbucket and Stash. When you view a pull request, you’re seeing what the resultant merge commit will actually look like. We do this by actually creating a merge commit behind the scenes, and showing you the difference between it and the tip of the target branch: git diff C D where D is a merge commit shows all differences between the two branches If you’re curious, I’ve pushed the same repository to a few different hosting providers so you can see the different diff algorithms in action: The “merge commit” diff used in Bitbucket and Stash shows the actual changes that will be applied when you merge. The catch is that it’s trickier to implement, and more expensive to execute. The first problem is that the merge commit D doesn’t actually exist yet, and creating a merge commit is a relatively expensive process. The second problem is that you can’t simply create D and be done with it. B and C , the parents of our merge commit, could change at any time. We call a change to one of these parents rescoping the pull request, because it effectively changes the diff that will be applied when the pull request is merged. If your pull request is targeting a busy branch like master , your pull request is likely being rescoped very frequently. Merge commits are created any time either branch changes. In fact, every time someone pushes to or merges a branch into master or your feature branch, Bitbucket or Stash is potentially going to need to calculate a new merge in order to show you an accurate diff. The other problem with performing merges to generate pull request diffs is that, every now and then, you’re going to have to handle a merge conflict. Since your git server is running non-interactively, there won’t be anyone around to resolve them. This makes things a bit more complicated, but actually turns out to be an advantage. In Bitbucket and Stash, we actually commit the conflict markers as part of the merge commit D , and then mark them up in the diff to show you how your pull request is conflicting: In Bitbucket and Stash diffs: green lines are added, red lines are removed, and orange lines are conflicting. This means we can not only detect ahead of time that your pull request is conflicting, we can also let reviewers discuss how the conflict should be resolved. Since conflicts always involve at least two parties, we feel that the pull request is the best place to determine an appropriate resolution. Despite the additional complexity and cost, I believe the approach we’ve taken in Stash and Bitbucket provides the most accurate and useful pull request diff. If you have questions or feedback, please let me know in the comments or on Twitter. If you like, you can follow me ( @kannonboy ) for occasional updates on Git, Bitbucket, and other neat stuff.", "date": "2015-01-22"},
{"website": "Atlassian", "title": "Understanding JWT for Atlassian Connect", "author": ["Travis Smith"], "link": "https://blog.developer.atlassian.com/understanding-jwt/", "abstract": "Atlassian Connect uses JSON Web Token (JWT) for authentication between the host product (e.g. JIRA, Confluence, or HipChat) and your add-on. To ensure the security of everyone’s data, Atlassian includes additional claims so a signed request cannot be intercepted and used to perform other actions. We offer frameworks that hide that complexity for node.js , Play! , and ASP.NET as well as atlassian-jwt for those working on other Java stacks. What happens when you don’t want to use our frameworks? That’s not a problem, you can use whatever framework you want – one of the joys of building add-ons with Atlassian Connect. You can implement the JWT authentication yourself — I’ll walk you through the added security features Atlassian uses with Connect. There are two ways one might interact with JWT signed requests. You need to know how to validate a signed request made to your add-on from the host application and you need to know how to properly generate the claims and sign a request you made out to the host product (e.g. REST API calls). For this post, we’re going to assume that you have a [library able to do the encoding and decoding of the JWT signature] understanding-jwt . What we’re focusing on is how to deal with the claims attached to that signature. The first bit to remember is that there’s a shared secret provided during the install callback that happens at the start of the Connect lifecycle . You will need this shared secret to do anything with the JWT signature. In addition, the install callback also contains a clientKey that’s used to distinguish between product hosts. Validating a request against your add-on is the simplest place to start. If you have a webhook endpoint registered in your atlassian-connect.json descriptor, then it’s possible for you to receive messages from the host product. Validating the source of the request makes sure a malicious actor can’t start sending requests and corrupting or exposing data they shouldn’t have access to. The first thing to do is decode the JWT signature using the shared secret key provided during the install callback. You should end up with a JSON representation of the claims provided in the signature. Something similar to `clientkey`. in the same format at `iat`. To generate the query hash, or qsh , we need HTTP method, resource URI, and query string for the request. An example would be POST https://app.my-add-on.com/hooks/issue_updated . To convert this into a query hash, we need first the HTTP method, in this case POST. Then we combine that with the resource URI, relative to the base of your add-on (or the context path of the product). In the example it would be /hooks/issue_updated . Then we need the query string in canonical format. The canonical format is basically escaping then sorting all the query string parameters so that the generated value will always match for a given resource. More examples of that later, but first let’s wrap all these parts together. We have the {HTTP method}&{resource URI}&{canonical query string} , which in this case is simply POST&/hooks/issue_updated& . If for some reason the resource URI would be blank, replace it with / . This makes sure requests for http://localhost are no different than requests for http://localhost/ . Once you have the string formed, hash it with SHA-256 and convert those bytes to a string using hex representation. And there’s your query hash claim. Now, when sending a request to an API, it might be more complicated. Signing of your requests to the host product has some slight differences. iss is your add-on key instead of the clientKey . If your requests have query string parameters, like say https://jira.atlassian.com/rest/api/2/search?startAt=2&maxResults=4&fields=summary,comment&expand=names does, then we need to walk through generating the canonical query string part of the query hash. The first part, the HTTP method, is simple and is GET in this case. The URI is /rest/api/2/search . The parts of the query string need broken up as well, so you have Given that list, we need it sorted and values percent escaped. If you have multiple parameters with the same name, the values are sorted then joined together with a comma. You’ll see fields went from summary,comment to summary%2Ccomment . It’s important the percent encoded characters are uppercase. Now take the results and join it all together with the other members like Hash that, stick it in qsh and send your claims off to your JWT encoder with the secret key for the client in question. Add a Authentication: JWT $token header to your request with the result of the JWT encoder and you have a signed request. A few items to keep in mind that might help out… It’s not that difficult to build your own JWT authentication — many add-ons do this now and so can you! Do you have an internal system that would benefit from integrating with your Atlassian tools, or a great add-on idea? Get started with Atlassian Connect today.", "date": "2015-01-26"},
{"website": "Atlassian", "title": "1:40 of code art: 2014 in Bitbucket seen through Gource", "author": ["John Garcia"], "link": "https://blog.developer.atlassian.com/a-year-in-bitbucket-seen-through-gource/", "abstract": "It’s been an incredibly busy year here at Bitbucket! We’ve served more active users, made more improvements, rolled out more features, and fixed more bugs than ever before. To commemorate the year that was, I’ve visualized our Git log using the Gource log visualization system to give you something to look at while your code is compiling . You can read about how I made the magic happen below the video. As a weekend project, I kicked around the Gource source-code visualization system, which walks through your log and generates a video display showing the results of your commit activity. After mapping each commit to a video frame and each file to a node, user avatars float around and zap each commit into its destination file, color-coded for creates, changes, and removes. With the help of my household’s Artist-in-Residence (aka my dear wife), we found a pattern that makes a very visually appealing show of a year of commits in a large software project. I then set this to an appropriate piece of music from YouTube’s royalty-free music library, and voila, code-art is made. For starters, I brew ed gource and cloned out a copy of the repo. Since I want to build this from the main development branch (Staging), simply running over to Bitbucket and cloning the repository sufficed; otherwise, I’d want to checkout a specific branch: Next, we get the user avatars into the repository for use in composing the video. This is done through a Perl script found at the [Gource Gravatar Example] which is run from the root folder of the repo: If you want, you can then enter <repo>/.git/avatar and clean up the images. Alternately, you can add in Identicons for users with no avatar by using modifications to the script found in the comments. I elected to use Charlie, our corporate mascot. The avatars are set using the following, including a user-scale argument that will ensure they’re visible once we have built a huge tree: We’re going to set framesize, hide some distracting elements, and add titles and logos. This makes for a more appealing, less cluttered presentation, with graphics that break up the flat black background as well as helping the viewer gain context: Bloom describes the etherial glow that the folder-nodes have, with intensity describing its brightness and multiplier describing its radius. I decided to go with a gentle, diffuse glow; multi-sampling describes a computationally expensive but very effective smoothing technique. Since we’re rendering this offline, we’ll throw that in, too: Bitbucket, like most large projects, has an astronomical number of objects. First and foremost, we need to speed this thing up. Let’s set the seconds per day and the number of seconds we’ll dwell at the end of a day’s commits to really small values: Next, we want to manage the animation properties to fit these timings. We’ll make sure the files appear appropriately quickly and slow down the ‘jumpy’ user avatars by decreasing their max speed and the time they’re allowed to coast (friction): Since Bitbucket has been around for a very long time, we’re going to limit this exercise to the last calendar year. This will keep our movie under two minutes, and also provide us with a finished product that commemorates a milestone. Also, as this is just one year of commit activity, we’ll allow the nodes to be permanent by turning off their idle-time expiration: Any Linux or Mac die-hard will tell you that a primary reason for being a Linux or Mac die-hard is this little thing called a pipe, which allows you to pipe the st an d ard out put (or stdout ) of one program into another. We’ll send this into ffmpeg , which will encode it directly into an mp4 file (your codecs may vary). By using the gource -o - option to direct the output to stdout, we can supply the pipe with a stream of bitmaps. I won’t get into the magical incantations required to make ffmpeg go – there be dragons. That said, this little bit of copy-pasta I got from the [Gource Videos] page worked great for me. You can adjust the -threads option for more processing speed: Naturally, all of this put together is a huge command line, but it makes for a nice, minute and thirty-five seconds of modern art: This was a bunch of fun to make, and I highly recommend that you try it out on your own repo. Because the process abstracts so much of the information in the log prior to two layers of visual transcoding, there’s no risk of any trade secrets being released through the process. As many commercial software packages are closed source, this offers an attractive way to expose your fans and evangelists to your development process and generate excitement in the developer community. Go try it yourself! Get started at the GSource Homepage at Google Code. Follow me on Twitter for updates about Atlassian software, robotics, Maker art, and software philosophy at @bitbucketeer .", "date": "2015-02-03"},
{"website": "Atlassian", "title": "Powerful new search and property storage APIs in Confluence 5.7", "author": ["Steve Lancashire"], "link": "https://blog.developer.atlassian.com/confluence-5-7-apis/", "abstract": "With the release of Confluence 5.7, developers get two new and exciting tools to help them build Confluence add-ons. The REST API has been extended with advanced search capabilities that we’re calling CQL (Confluence Query Language) providing great new ways to find the content you’re interested in. In addition, content properties now support indexing and querying using CQL – especially handy for Connect add-ons that need to store and query their data in Confluence. I’ll walk you through these new features with a simple use case. CQL is an approachable and natural language for querying content in Confluence. Announced at Atlascamp 2014 , it’s now available in Confluence 5.7. Let’s start with a simple example to see some of the fundamental concepts. Let’s say I wanted to find all blog posts on developer.atlassian.com created by me that mention CQL, visually we could represent this query as: — CQL gives us a language to represent this query. A simple CQL expression is made up of a field, an operator and a value, taking one of the circles in the diagram above we could find all blogposts, with: type = blogpost If you’ve used JQL, JIRA’s query language for querying issues, you’ll immediately recognise CQL’s similar syntax, but breaking this down: And by passing this as the cql query parameter to the REST API we can retrieve all blogposts from Confluence, for example: developer.atlassian.com/rest/api/content/search?cql=type=blogpost Simple expressions can be combined with boolean keywords, (AND, OR, NOT). Continuing our example above, to further narrow our search from the blue circle of all blogposts, to the intersection of the three we can combine two more expressions: creator.fullname ~ \"Steve Lancashire\" and text ~ CQL These combine to form our full query to fetch all blog posts created by me that contain the text CQL: type = blogpost and creator.fullname ~ \"Steve Lancashire\" and text ~ CQL If we then want to find the most recent blogs we can add an ORDER BY statement to the end of the query to get the blog posts listed in order of creation date, for instance: type = blogpost and creator.fullname ~ \"Steve Lancashire\" and text ~ CQL order by created DESC More details about the available fields operators and functions can be found in the advanced searching documentation . We know add-on developers are always trying to support as many versions of Confluence as possible from the one code base. To make it easier to achieve this, and take advantage of CQL when it’s available, we’ve added a capability, confluence-content-search-api , to the capability service. To determine if CQL search is available on a given instance of Confluence you can query the capabilities API, it is published under: /rest/capabilities Example response or from within a add-on use the CapabilityServie ```java CapabilityService capabilityService;... capabilityService.getHostApplication().hasCapability(\"confluence- content-search-api\"); The CQL search language forms part of the a public REST API, and as a result it conforms to the Atlassian API policy with guarantees about its stability across versions of Confluence. Content properties allow an add-on to store data, as key / value pairs, against a piece of content in Confluence, be it a page, blog, comment or attachment. So say we had an add-on that implemented a formal approval for a page in Confluence, allowing a reviewer to click an “Approve” button to sign off on a document. Prior to content properties there were other ways a P2 add-on could implement this, like creating an Active Objects table and storing the data in that, and then keeping it in sync when that page was moved or deleted. As Connect add-ons run outside the Confluence server, they do not have access to Confluence’s storage Java APIs. Prior to content properties, a Connect add-on would need to provide their own datastore, calling back to their server when rendering the add-on. Content properties provide an easy way to store data against content in Confluence using the REST API, facilitating performant “static” Connect add-ons and removing the need for a Connect add-on to have their own backend server. For instance: or a P2 add-on using the ContentPropertyService in java: You can learn more about indexing content properties and the range of data types supported by reading the documenation . We’ve seen how CQL can be used for querying for content in Confluence, and how content properties can be used to store and retrieve data against pieces of content in Confluence. Wouldn’t it be great if, having used content properties for storage, we could refer to them in CQL as filters? Fetching content that has a particular value for a content property. In Confluence 5.7 with the combination of CQL and content property indexing, add-on developers can query the properties they’ve added to content. To continue our example, we might want to show all blog posts that have been approved in the last week. By adding an index schema module to the content property, the property will be indexed and automatically exposed in CQL. We create a content property index schema in the Atlassian plugin descriptor: This extracts the approver value and the approval date into Confluence’s Lucene index. Using the REST API and CQL we can retrieve all blogposts that have been approved in the last week with the query: type = blogpost and content.property[approval-plugin-data].approval-date > now(-7d) So now you’ve seen a little of what CQL and content properties can do for your add-on, and how combining them can boost the power you get out of the Confluence Platform when building your add-on. Let us know in the comments how you’re using these new APIs and any enhancements you’d like to see to them. Stay tuned for more information on how to further extend CQL with concepts from your add-on.", "date": "2015-02-11"},
{"website": "Atlassian", "title": "Easy data storage with Atlassian Connect Express", "author": ["Travis Smith"], "link": "https://blog.developer.atlassian.com/settings-storage-in-ace/", "abstract": "You’ve been working on a Connect add-on using Atlassian Connect Express (ACE) . You’re to the point where you need to store some data in your add-on. You can use the add-on properties REST API in some cases. You can add an entire storage mechanism to handle this but ACE has a settings storage mechanism already. Get this — you can just piggyback on that! It’s not hard to do, and it’s perfect for things like global configuration for your add-on. If you don’t want to store any data in your add-on, you can use the add-on properties REST API. This has some limitations though: If those limitations are acceptable to you, you shouldn’t even bother with worrying about storing data in ACE. However, there are plenty of times these limitations don’t work for you. Assuming you’re using ACE 1.x, you’ll find a couple of methods on addon.settings you can use to retrieve and store data. If you need to store large amounts of data, this might not be the best way to handle it. But for many scenarios with limited storage requirements this should work brilliantly. Here is a simple example of loading some data and passing it off to a template for rendering. Storing the data with addon.settings.set(key, value, clientKey) is similar; it also returns a promise. In this example there is an example that would be called with an AJAX post from the /configure page above. Since this request isn’t from the product host, you should use the ACE token to authenticate with addon.checkValidToken() . This helps ensure security for this request. Now that you have the knowledge to use addon.settings in ACE, you have an array of options to choose from. Entity properties in JIRA and content properties in Confluence allow you to store data inside the product instance tied to content. Add-on properties allow you to store data tied to your add-on. Storage of configuration data should be no problem for your add-on!", "date": "2015-03-11"},
{"website": "Atlassian", "title": "Node Summit 2015 videos for Atlassian developers", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/node-summit-2015-videos-for-atlassian-developers/", "abstract": "Last month, Atlassian was a proud supporter of the Node Summit in San Francisco. We were there to talk to developers about using NodeJS to build add-on microservices for Atlassian Cloud products. Earlier this week, Node Summit released the videos from the 2015 Conference . Building an add-on with Node? Here are a few useful sessions! Gord Tanner of bitHound talks about how they developed a large scale distributed computing platform to analyze source code in Node.js. Find out what worked and didn’t work in architecting the back end services of bitHound. Watch (28:28) Brandon Cannaday and Taron Foxworth of Modulus talks about how simple it is to take your Node.js application and provide it to all the world to enjoy. Watch (10:19) CJ Silverio from npm discusses the npm registry and the signs of strain it was showing after 4 years of being a side project and suddenly finding itself in massive growth. By the end of 2014 it became a stable critical component to the Node.js experience. Look at the path to get there and learn about the challenges involved to refactor critical infrastructure without downtime. Watch (25:07) Peter Elger of nearForm discusses embracing and build microservices without fear of creating a monster! Watch (28:11) Bryan Cantrill of Joyent talks about the explosion of developer interest in containers which are a good match for efficiently developing and deploying microservices architectures. Bryan shares his experiences using Node.js containers to build distributed systems, including the difficulties, improvements and successes. Additionally, explore the challenges that remain. Watch (36:29) What sessions from Node Summit did you find useful? Let us know in the comments or tweet at us at @atlassiandev or directly to me at @RedWolves . Ralph is a Developer Advocate for ecosystem developers.", "date": "2015-03-20"},
{"website": "Atlassian", "title": "Tip of the week: Configure your Java version in Tomcat", "author": ["John Garcia"], "link": "https://blog.developer.atlassian.com/totw-configure-java-version/", "abstract": "During my tour of duty providing support for our Tomcat and Java based behind-the-firewall apps, a common concern was the effect of updating Java or the JVM in the host after installing the product, and how it may affect a running production system. As some operating systems will auto-update system libraries such as Java with little notice, this risks loss of service from mission-critical apps in the unlikely case of a Java breaking change. Another common scenario that comes up is a case where a specific Java version is required for the app, which may not match the default version installed in the host. In this case, we’ll want to specify a Java path to override any environment variable set at the host level to ensure consistent app behaviour. Finally, admins who wish to deploy multiple apps that require different versions of Java will be able to invoke them on the same host, each with the correct JVM. An oft overlooked part of the Apache Tomcat instructions is the mention of the setenv file, which is typically pre-packaged with a recommended configuration from the vendor (if you download Stash or our other products standalone, you’ll see it’s packaged in this way). Most folks will just unpack the tarball, set a system environemnt variable to declare their STASH_HOME, and be on their way. This works great for 90% of installs, but power users will need to get into this file for a variety of reasons, such as configuring Tomcat’s memory utilization or GC implementation. This file can typically be found at {app_install}/bin/setenv.sh or {app_install}binsetenv.bat in Windows. To configure the JAVA_HOME in *nix, open setenv.sh file in your favorite text editor and add the following as a new first line: Or in Windows, open setenv.bat and add the following at the top: Once complete, save the change to the file and start (or restart) the server! Let us know in the comments your use case for deterministically configuring your Java! Follow our @atlassiandev Twitter handle for updates about our fine line of developer tools. Follow me on Twitter for updates about Atlassian software, robotics, Maker art, and software philosophy at @bitbucketeer .", "date": "2015-03-30"},
{"website": "Atlassian", "title": "Zero to Bitbucket in three weeks flat", "author": ["Tim Pettersen"], "link": "https://blog.developer.atlassian.com/bithound-integrating-with-bitbucket/", "abstract": "We have all been there. When we started bitHound I knew we would inevitably be supporting multiple platforms and different environments for analyzing code. However, with several code hosting platforms out there and limited resources available we had to grab one platform to start and do so quickly. This decision was then followed by a pile of assumptions, shortcuts, leaking abstractions and general hackery that defines any software project. Eventually, the time came to clean things up, get back on track and for us to focus on adding true support for an additional code management platform, Bitbucket. Authentication and authorization is always a tricky part of any project, however, bitHound didn’t need to store much user information outside of the OAuth token from GitHub stored on the session. Adding support for Bitbucket was just a matter of creating real user records that allowed us to have both Bitbucket and GitHub tokens linked together as one user. This turned out to be quite a simple process since we use Passport and wired everything up for supporting multiple OAuth types quickly. Our UI had grown rather organically – adding pages, features and controls over the course of a year of development. Most of our views required us to get information from GitHub about repositories, users, commits, files, etc. Over time we had abstracted most of these calls into an API wrapper which we called and used the raw GitHub returned objects to render our pages. We started with a simple provider -> API -> Map combo that allowed us to extract out and modularize what our types were, map them to API’s and provide a standard interface: Where the provider call would map which API calls are needed to build up the new standard repo type: After some refactoring and getting all of our pages using the new provider it was just a matter of matching up the Bitbucket API calls to our entities. One of the best tools I had while working on this was the rest browser . Being able to quickly test and browse API’s to find the ones that matched what I needed was very useful. Once we had the list of API’s we were surprised that there were very few cases where we were missing something that GitHub had provided us. Between the Version 1 and Version 2 API endpoints, we had access to everything we needed. Once we had most of the functionalities and UI working, we needed to make sure our backend process could work with the repositories. Our back end services at the time would take the owner/repo and access token, and would use that information to clone/analyze and save the results. While GitHub allows us to clone via https with the access token, bitbucket does not allow this. Initially we were a little scared, because cloning with just the token was so easy. That said, once we got over our initial fear we found that it would actually be simple for us to handle this with Bitbucket. Bitbucket provides two API’s for adding SSH keys to allow cloning of private repositories. Deploy keys are keys you can add to a repository as an admin that allow read only access to the code. User level SSH keys allow an application the same access that the user has to all repos. In addition, we needed to be able to clone and analyze any repo a user had access to– regardless of admin rights. We ended up settling on adding a user SSH key. This is done automatically once a user attempts to process a private repo. The key is unique per user and can be updated on regular intervals. Using a custom SSH key with Git is really easy and just involves having an alternative script Git calls for ssh defined in the SSH_GIT environment variable in which our bithound_ssh.sh script is: Overall, working to build our integration with Bitbucket was a very smooth, positive experience. Atlassian’s Bitbucket development team was both informative and supportive. They have some really great tools and APIs available to developers and we are really excited to see what will happen now that the integration is complete. —", "date": "2015-04-14"},
{"website": "Atlassian", "title": "Tip of the Week: Enable Garbage Collection Logging", "author": ["John Garcia"], "link": "https://blog.developer.atlassian.com/tip-of-the-week-enable-gc-logging/", "abstract": "Forewarned is forearmed, or so the saying goes. This means in the event of an incident, it’s always best to have as much logging information available for diagnosis and remediation as is reasonable. Garbage collection is a crucial part of keeping my JVM happy, so I like to get garbage collection logging enabled before I need it rather than after. In the pattern used by Atlassian’s Java apps, we will enable logging using the setenv.sh file, in the JVM_SUPPORT_RECOMMENDED_ARGS variable. Use the following context: If you prefer to have actual system datestamps logged instead of millis offset from VM Launch, use -XX:+PrintGCDateStamps instead of -XX:+PrintGCTimeStamps . This will set up your system to log every garbage collection run into a text file that will be generated in your <product-install>/logs folder for easy retrieval. This file will be destroyed and a new one generated every time you start the JVM, so you’ll want to retrieve it beforehand if you need to restart the application. Garbage collection tuning is as much an art as a science. For more advice on how to tune garbage collection in Stash, check out this page: Garbage Collection Tuning . The guidelines found at that page are suitable for any tuning project. Watch out for news and info from our @AtlassianDev Twitter feed! Follow me at @bitbucketeer !", "date": "2015-04-20"},
{"website": "Atlassian", "title": "Maximize your next conference with these tips", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/maximize-your-next-conference-with-these-tips/", "abstract": "AtlasCamp is fast approaching, June 9-11 in Prague, Czech Republic. AtlasCamp is our developer Super Bowl, World Cup, Stanley Cup, or whatever sports metaphor that works best. I’m looking forward to meeting new people, learning new techniques and building new partnerships. I wanted to offer my experiences in how I make the most out of developer conferences. Here are a few of my suggestions for getting the greatest value out of a conference. At any conference your time will compete with many activities. How can you maximize your time while you’re there? Here are some things to consider before you arrive. Check the conference web site for answers. You may need to contact the organizers to determine these answers. The organizers may not have considered answers to these questions yet. To show that there is demand you should contact them early. The most important question to answer is the availability of video recordings. Without video recordings attending the sessions is the priority. With video recordings, attending the sessions is secondary. Video frees you to talk, hack, build relationships with others in the “hallway track.” A lounge or hack area will make the “hallway track” more enticing to others as well. With video, you can take the choice out of which session to attend. You can stay put in one track and catch the other track later on. Without video you’ll need to plan ahead and choose the session that is best. At AtlasCamp, we will have two tracks this year. Track 1 will center around add-on development and track 2 on developer tools. We will record each session and they will be available after the conference. Finally, we will have a network lounge and developer breakouts for attendees. The one session you should not miss is the keynote. All conferences have a keynote. It dictates the rest of the conference with high profile speakers or major announcements. Yet, the keynote is the single common session for the entire conference. You can use the keynote to break the ice while networking. For AtlasCamp, we’ll have Atlassian co-founder Mike Cannon-Brookes giving the keynote address. He’ll sure to have great information about Atlassian products that you’ll talk about later. There should be at least one networking event or after party to meet others. Make sure you go and bring lots of business cards and a pen, to jot a note about the person you just met. Then go out and talk to people. Join in their conversations, listen and ask questions. Learning how to network and connect with other people is a great skill to learn. Dale Carnegie’s book, How to Win Friends and Influence People defines those skills. He provides a few simple rules (that are covered more in-depth in the book) that apply: I familiarize myself with these rules anytime I’m going to meet people. It helps me condition my thinking into that of the other person. I want to leave the person with a positive experience of me and the company I represent. Conference sponsors are the life blood of any conference. Their monetary contributions are what allow organizers to do the extras. In exchange, they’re hoping for a moment of your time. They’ll even give you swag. Spend some time in the exhibition booth. Use the network skills above to talk with the sponsors. You may find a service or product that interests you. Have your business cards with you to hand out. Think of sponsors as eager networkers who jumped the line to talk with you. Remember that attending a conference is more then sitting in sessions. The real value is talking with others and building relationships. AtlasCamp 2015 is coming up fast and is the perfect event to get the most out of with these tips. Make sure to talk with me at AtlasCamp. You can find me @RedWolves on Twitter. Ralph is a Developer Advocate for ecosystem developers.", "date": "2015-04-21"},
{"website": "Atlassian", "title": "Webinar: Building add-ons for Atlassian Cloud with Connect", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/may-5-webinar-atlassian-connect/", "abstract": "Join us on May 5th at 10am PDT for a free webinar to learn more about developing for Atlassian Connect . Atlassian Connect is Atlassian’s next generation platform for building add-ons for Atlassian Cloud products. We know you love JIRA and can’t get enough Confluence, but did you ever have this feeling that you would work so much faster if JIRA did this, or Confluence showed that? With Atlassian Connect add-ons you can now add features, integrate with other systems, and scratch that itch. At the end of the webinar, you will understand the motivations and concepts behind Atlassian Connect, and know how to: Setup your local development environment for building add-ons Write a Connect add-on for JIRA and deploy it locally Deploy your add-on in a public cloud like Heroku or AWS Publish your add-on as a listing in the Atlassian Marketplace Use your add-on in a Cloud instance Ralph is a Developer Advocate for ecosystem developers.", "date": "2015-04-28"},
{"website": "Atlassian", "title": "JIRA SOAP API is waving goodbye", "author": ["Dave Meyer"], "link": "https://blog.developer.atlassian.com/jira-soap-api-announcement/", "abstract": "Remember February 2013? Pope Benedict XVI resigned, Beyoncé performed at the Super Bowl, and Atlassian announced that JIRA’s SOAP and XML-RPC APIs would be deprecated in JIRA 6.0. Fast-forward two years and we’re approaching the next major milestone in our journey: the SOAP and XML-RPC API modules will be removed from JIRA Cloud soon and in the 7.0 release of JIRA Server . First, the good news. If you haven’t migrated from SOAP to REST yet, we’ve prepared a method-by-method migration guide to jumpstart your development . Here are the key details: JIRA 7.0 will ship without the SOAP and XML-RPC APIs The JIRA REST API will have functional parity with SOAP JIRA’s rpc-soap and rpc-xmlrpc modules will be removed JIRA will not ship with the rpc-jira-plugin Our original reasoning from February 2013 remains true: Deprecating SOAP and XML-RPC means that developers have a clear and consistent message from us about how they should be interacting with JIRA remotely. They won’t build up code relying on SOAP and then be disappointed when new features don’t get added or bugs don’t get fixed. No false hope and no bullshit: REST is the future. We’ve invested in adding functionality to the JIRA REST API while making it more usable and performant for developers. Atlassian has also introduced the Atlassian Connect framework , which has even more building blocks for rich, functional add-ons for JIRA that are scalable and cross-platform. We know that many customers have written integrations with the SOAP API, or are using third-party add-ons that are built on top of the API. The JIRA Ecosystem team is committed to helping JIRA customers and add-on developers move to REST. We have prepared a SOAP to REST migration guide documenting the existing SOAP methods and our recommended REST equivalents. We strongly encourage you to collaborate with us if you need help, need to report a bug, or have a feature request for the JIRA Ecosystem team. Simply create an issue in the ACJIRA project on ecosystem.atlassian.net . All the new 7.0 REST APIs will become available with the first EAP release of JIRA 7.0 Server and in JIRA Cloud in the next couple months. We will post another update here once this ships.", "date": "2015-05-08"},
{"website": "Atlassian", "title": "Integrate all the things: connecting software development tools", "author": ["Ian Buchanan"], "link": "https://blog.developer.atlassian.com/connect-software-development-tools/", "abstract": "If you work for a big company, you know all the big ALM tools. Fortunately, the experience fills out a résumé nicely. Unfortunately, you have to deal with all of them at the same time. I’ve seen it happen for any number of reasons: outsourcing, acquisitions, restructuring. Whatever the history, the variety of tools can cause communication silos. According to Melvin Conway , those silos doom the software you produce to be siloed. If you thought the only alternative was a lot of custom development using a hodge-podge of languages and protocols, then you may have missed a new generation of middleware designed specifically for ALM integration. Although I’ve seen many problems with mixed tooling in big companies, I’ve also been witness to how easy it can be to solve the headaches with a new breed of ALM middleware. For each of these products, I’ve seen production usage involving 1000s of users with 1000s of objects. What’s common to these ALM middleware products is hub-and-spoke architecture. The hub performs the work of mapping between fields. For enumerated values like workflow status, there are submappings. The hub has filters that scope the data synchronization. Between mappings and filters, it is possible to replicate most data structures. Things can get complicated when one product has a flat list structure but the target has a tree structure. The hub has rules for deciding the direction of flow and how to detect and resolve conflicts. Unlike point-to-point integrations, the hub-and-spoke architecture is suitable for multiple scenarios: Migrate between different tools. Migrate between instances of the same tool, usually for consolidation. Federation of different tools, usually to cross organizational boundaries. Federation of instances of the same tool, usually to cross geographic boundaries. And combinations of the above. The spokes are adapters that convert each ALM product’s API into a consistent internal representation. While all connect to JIRA, each product has a different set of adapters. Despite the architectural similarities, each middleware vendor has a different business model. Pricing can be a function of one or more of the following: Users across the integrated ALM products. Connectors by ALM product. Number of instances of ALM products. Who you buy from. Some ALM vendors have become resellers of ALM middleware. All of the options provide a user interface for creating and maintaining the mappings and integration flows. This is a young market. The 2 following vendors seem to be the front-runners. The list is not meant to be exhaustive. If you are a Java developer using Eclipse, then you may already know about Eclipse Mylyn . Originally Mylyn was a way to improve the Eclipse user experience by hiding information in context of a task. In time, Mylyn became the de-facto standard for accessing tasks from ALM tools from inside Eclipse. The Tasktop company formed around commercializing Mylyn as a product, now known as Tasktop Dev . Building on the existing ecosystem of Mylyn connectors , Tasktop created a server-to-server middleware product, Tasktop Sync . In addition to leveraging the Eclipse de facto standard task interface, Tasktop Sync is an adapter for the Open Services for Lifecycle Collaboration specification. Most recently, Tasktop has released Tasktop Data . Tasktop Data aggregate ALM data from multiple systems for reporting. This is an excellent separation of concerns. It avoids the problem of overloading one “source of truth” system with synchronization from everything else just for the purpose of aggregate reports. Tasktop products are not currently SaaS. If, for example, you wanted to synchronize GitHub Issues and JIRA Cloud , then you’ll have to provision and maintain your own machine to run Tasktop Sync. Even if as a VM in the cloud, that may be more cost and effort than you want for synchronizing 2 cloud products. That caveat might not matter much since many big companies still rely on behind-the-firewall development tools. While OpsHub is a newer company, they beat Tasktop to market with server-to-server integration. OpsHub Integration Manager (OIM) was publicly available before Tasktop Sync. Although OpsHub makes no appeals to open source and open standards, that helps them keep laser focused on building out an impressive list of both commercial and open-source ALM systems. The from-scratch approach has also helped OIM have broader support for the variation of “things” in ALM. With Tasktop’s background in “task-focused” Mylyn, it has stayed closer to the kinds of workitems you find in JIRA. In addition to JIRA Issues, OIM has reached for things like tests, builds, and changesets. OpsHub has on-premise, customer cloud, and SaaS hosting options. When I was last informed of OpsHub’s pricing model, it seemed to scale down to team level better than Tasktop. Those aspects may make OpsHub a better choice for a single team looking to solve their own integration problems inside a big company. While ALM middleware does help reassemble information from the tool silos, the middleware is still subject to the CAP theorem . That means you should have realistic expectations about what “synchronization” means. Namely, don’t expect every change to propagate immediately. These ALM middleware products can be configured to synchronize on very short cycles. However, the integrated ALM products are typically the bottleneck. In my experience, 5 minute synchronization intervals are reasonable. Faster is possible with fine-tuning of filters and mappings to the bare essentials. If you can get away with lightweight linking instead sending all the data, so much the better. Sometimes just a textual reference is sufficient. Longer may be required when synchronization includes elaborate descriptions, images, and/or file attachments. While I have seen these products deliver value in many situations, there are still simple scenarios where custom coding is the best option. All of these products are built on APIs. You can use them too. But when you are comparing custom coding to these ALM middleware products, don’t forget how much these products have invested in the non-functional capabilities you’re going to need for large scale implementations.", "date": "2015-05-11"},
{"website": "Atlassian", "title": "Atlassian SDK Upgrade to Java 8", "author": ["Dallas Tester"], "link": "https://blog.developer.atlassian.com/sdk-upgrade-to-java-8/", "abstract": "We’ve upgraded to Java 8! Our SDK now requires that you run with JDK 1.8. Given the recent end-of-life of Java 7 , our products are on the road to deprecating JDK 1.7 as a supported environment. For you add-on devs out there, you’ll need to upgrade your environment to Java 8 and JDK 1.8 . If you’ve run your environment lately and ran into issues such as a blank screen, read on to update and get back into action! To make the transition easy, we’ve updated the Vagrant boxes to automatically include Java 8. If you’re new to add-on development, simply follow the instructions for environment setup , and you’ll be good to go. If you have previously installed the Vagrant box for local development, you’ll need to dismantle and rebuild it. Thankfully there are just a few steps to get you back up and running with the latest SDK! Please note that the vagrant destroy step is, well, destructive! You will lose any changes you’ve made to the virtual machine, so back up anything you hold dear! For an environment not using the Vagrant box, the steps are easy! Just go to the Oracle website and download the Java 8 SDK for your platform. You can install it from there and re-run your local environment. Simple as that! If you’re interested in keeping up-to-date on supported environments for Atlassian products, check out each product’s supported platforms page.", "date": "2015-05-13"},
{"website": "Atlassian", "title": "Last chance to register for AtlasCamp 2015!", "author": ["Nick Wade"], "link": "https://blog.developer.atlassian.com/atlascamp-2015-last-chance/", "abstract": "Once a year developers from all over the world get together for Atlassian's premier developer conference: AtlasCamp . You can learn how to customize and integrate JIRA, Confluence, Bitbucket, HipChat, Bamboo, and Stash, helping your team work even faster. And if you want to take your developer skills to the next level with Git, CI, CD, Docker and more, you should be at AtlasCamp . There’s only 14 days left until AtlasCamp, register now before we run out of spots: There’s many great reasons, but we boiled it down to four things you can take away. We look forward to hosting you at AtlasCamp in Prague on June 9-11. There’s only 14 days left. So don't wait. Register now.", "date": "2015-05-25"},
{"website": "Atlassian", "title": "Tip of the Week: Keep a test server handy", "author": ["John Garcia"], "link": "https://blog.developer.atlassian.com/keep-a-test-server-handy/", "abstract": "For Tip of the Week, we’re taking suggestions from the community. This week’s tip comes to us from plugin developer Daniel Wester of Wittified , who wants to remind us of how important it is to test changes to your Atlassian applications before deploying them to production. For any of the products, it’s important to have a staging/test instance available to test major config changes and/or addons. Be careful to consider database and email settings when designing your test environment. It’s best to avoid copying the application-home/dbconfig.xml file entirely; if this can’t be avoided, be sure to modify its contents before starting the test server. For JIRA, which uses mail handlers to fetch emails from outside servers, it’s important to disable this using the instructions at Restoring Data – Disable Email otherwise, a race condition can occur. Let us know in the comments below if there are any use cases you use that haven’t been mentioned here. Watch out for news and info from our @AtlassianDev Twitter feed! Follow me at @bitbucketeer !", "date": "2015-05-28"},
{"website": "Atlassian", "title": "Node Rockets integrated with HipChat flew over JSConf 2015", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/node-rockets-integrate-hipchat-jsconf2015/", "abstract": "On May 27th, JavaScript developers descended onto Amelia Island, Florida for JSConf 2015 . Atlassian again, sponsored this years Node Rockets hack day event. Sandwiched between two days of amazing talks from the top JavaScript developers is a day of hacking. Again this year the Node Rockets team put together a day of building and programming sponsored by Atlassian. Building a rocket involved wiring sensors to the Raspberry Pi. Custom coding the telemetry data with Node.js. Cutting and assembling the soda bottles to house the electronics. And a bottle that houses the pressurized water that makes the rocket fly. Putting a rocket together took most of the day with assembly and QA testing. As this was an Atlassian sponsored activity we wanted to add a little extra incentive. We offered Amazon gift cards to the team that had the best integration with HipChat using Atlassian Connect . Many teams put together a good effort and we’re happy with what the winners came up with. You can see more from that day as well as an interview with the winning team in the video below. The Atlassian’s team rocket launched their rocket five times that day. Ralph is a Developer Advocate for ecosystem developers.", "date": "2015-06-09"},
{"website": "Atlassian", "title": "Atlassian Connect for Bitbucket: A new way to extend your workflow in the cloud", "author": ["Ike DeLorenzo"], "link": "https://blog.developer.atlassian.com/atlassian-connect-for-bitbucket-a-new-way-to-extend-your-workflow-in-the-cloud/", "abstract": "This is a cross post from the Bitbucket blog More than 3 million developers and 450,000 teams use Bitbucket to manage and collaborate on source code. But code collaboration is only a fraction of what software teams do on a daily basis to ship software. Nowadays, shipping great software involves constant context switching using tools that don't integrate tightly. Even when integrations are made, toolchains have to be painstakingly maintained on an ongoing basis with reams of messy bespoke integration code that has to be regularly re-written. This is a challenge that development teams face every day. Most integration architectures available right now in the software industry only partially solve the problem. They only provide simple integration points that prevent teams from creating deeper integrations and a more unified workflow. To solve this problem, we've re-engineered Bitbucket into a platform that helps remove the interrupt-driven workflow and brings all the information to ship software in one place. Atlassian Connect for Bitbucket provides an integration architecture that embeds your add-ons right within the product UI creating a more unified workflow. Here is a demo of code search enabled by the add-on built by Sourcegraph: If you are still wondering why you should build an add-on using Atlassian Connect for Bitbucket, here are 3 reasons: Atlassian Connect is a next generation extensibility framework, providing much deeper integration than what is offered by standard REST APIs and webhooks. Add-ons are straightforward to implement. You can build an add-on in any language, on any stack, using any interface. We couldn’t launch an integration platform without the help of the community. Here are previews of add-ons currently available on Bitbucket: Cloud IDE: Codeanywhere, Codio Code quality: bitHound, Codacy Code search: Sourcegraph Code analytics: StiltSoft Deployment: Aerobatic, CloudCannon, Platform.sh Crash and Exception Handling: Rollbar Rapid Integration: Wittified If you are currently a user of Bitbucket and want to take a sneak peak, click on your avatar, select “Manage Account”, and simply install these new add-ons by selecting “ Find new add-ons ” from the left menu. It is now super-easy to extend Bitbucket so that you have the best workflow for your team. Click here for documentation to get started with Atlassian Connect for Bitbucket. We look forward to seeing what exciting and interesting add-ons you build using Atlassian Connect for Bitbucket. Imagine the possibilities – happy developing!", "date": "2015-06-10"},
{"website": "Atlassian", "title": "New to HipChat: Connect dialogs", "author": ["Peter Brownlow"], "link": "https://blog.developer.atlassian.com/hipchat-connect-dialogs/", "abstract": "We are proud to announce that the HipChat UI is now extensible for add-on developers. Powered by Atlassian Connect, you can now customise your HipChat experience by surfacing arbitrary HTML in modal dialogs. Dialogs can be opened from links sent in chat messages, which allows users to experience rich, interactive, context-aware information without leaving the HipChat web client. Great examples include opening calendar invite details or information on why your CI test suite just failed. Dialogs are available to all HipChat add-ons, which can be kept private for your team only, or distributed to thousands of others using the Atlassian Marketplace. The availability of dialogs is just the first step in the road towards extensibility within the HipChat client. Raise your first HipChat dialog in a matter of minutes by following our HipChat dialogs documentation . Here are two examples of internally developed dialogs. A super brief summary: post a link marked up with a data-target=\"dialog\" attribute and with its href pointing to your web service, then in the HTML that you serve at that URL load a specified JS file. When users of the web client click the link they will see it as a dialog. Read more in our HipChat dialogs documentation . If you’ve ever used dialogs in JIRA or Confluence then you already know a lot about how they work in HipChat. Right now, Connect dialogs are available only in the web client, and they will eventually make their way into other desktop clients. Users of clients that do not support dialogs will see your content in their web browsers, and when non-web clients begin supporting dialogs then users will simply see your dialogs inside these clients without you having to do any further work. Build something amazing with HipChat dialogs.", "date": "2015-06-23"},
{"website": "Atlassian", "title": "JIRA SOAP API removal update", "author": ["Dave Meyer"], "link": "https://blog.developer.atlassian.com/jira-soap-update/", "abstract": "This is a follow up to our previous reminder about the JIRA SOAP API. The JIRA SOAP API will be removed from JIRA Cloud on July 6. Once more, in big letters: After our next JIRA Cloud release on July 6, JIRA’s SOAP and XML-RPC APIs will no longer be available, and any integrations making requests to those APIs will not behave correctly. We have collaborated with several add-on developers, including Zephyr and ServiceRocket, to make sure that popular integrations continue to work seamlessly. However, if you have developed a custom integration that uses the JIRA SOAP API, we recommend you migrate it as soon as possible. Get started with the JIRA SOAP to REST Migration Guide . If you are running JIRA on your own server, the SOAP API will be removed with the 7.0 release of JIRA Server. We strongly encourage you to collaborate with us if you need help, need to report a bug, or have a feature request for the JIRA Ecosystem team simply create an issue in the ACJIRA project on ecosystem.atlassian.net .", "date": "2015-06-26"},
{"website": "Atlassian", "title": "Let’s build a Bitbucket add-on!", "author": ["Tim Pettersen"], "link": "https://blog.developer.atlassian.com/static-addon-for-bitbucket/", "abstract": "A few weeks ago, we introduced Atlassian Connect for Bitbucket, an add-on framework that lets you modify the Bitbucket UI. This post is a quick walkthrough on how to build a simple add-on that retrieves data from a repository, does some simple aggregation and displays it to the user. Bitbucket add-ons are stand-alone web applications that expose a simple JSON descriptor describing the end-points and Bitbucket modules that they implement. In this tutorial we’ll build a simple static add-on comprised of a little bit of HTML, JavaScript and CSS. Though static, it exhibits some powerful dynamic behaviour by interacting with Bitbucket’s REST API using the cross-domain JavaScript bridge . Here’s a screenshot of the add-on we’ll be building: The add-on creates a new link in the left-hand Bitbucket navigation menu (labelled File types in the screenshot above) and a new repository view that shows the types of files present in the repository. For a little bit of visual appeal, we’ll use a simple word cloud to represent how many files of each type are present. The font-size for each file type corresponds to the number of times it occurs in the repository. The video tutorial is thirty minutes long, but teaches you everything you need to know about building a static Bitbucket add-on from scratch. I recommend viewing it in full screen mode on YouTube.com in glorious 1080p: Alternatively, if you’d prefer to dive right in and start hacking on your own add-on, you can jump straight to: The add-on in this tutorial is a simple client-side add-on built on HTML, CSS and JavaScript. But the coolest thing about Atlassian Connect is that the API for building add-ons is simple HTTP! You’re free to build an add-on in PHP, Java, .Net, Rails, Python, Go, Haskell.. whatever language and framework takes your fancy. For some examples of other add-ons, check out the Find new add-ons section of your Bitbucket account settings. If you’ve enjoyed the tutorial, or have questions or comments regarding Atlassian Connect, please hit me up on Twitter! I’m @kannonboy .", "date": "2015-07-07"},
{"website": "Atlassian", "title": "JIRA 7 EAP release now available", "author": ["Dave Meyer"], "link": "https://blog.developer.atlassian.com/jira-eap-announcement-1/", "abstract": "The JIRA development team is proud to announce that the first Early Access Program (EAP) release of JIRA 7 is now available. JIRA 7 is the biggest JIRA release ever. There are a large number of API changes and library changes, so now is the time to begin testing your add-on. Download the JIRA 7 EAP today and check out the preparing for JIRA 7 development guide . EAP releases are early previews of JIRA during development. We release these previews so that Atlassian’s add-on developers can see new features as they are developed and test their add-ons for compatibility with new APIs. The number of EAP releases before the final build varies depending on the nature of the release. The first publicly available EAP release for JIRA 7 is EAP-05 . Developers should expect that we will only provide a few EAP releases before the final RC release. The JIRA 7 EAP is best explored hand-in-hand with the Preparing for JIRA 7 development guide . The development guide contains all API-breaking changes in JIRA 7, suggested migration paths, and important new features that may affect add-on developers. If you discover a bug or have feedback on JIRA 7, please let us know . Create an issue in the JIRA project on jira.atlassian.com with the component JIRA 7 EAP .", "date": "2015-07-28"},
{"website": "Atlassian", "title": "Monitoring Connect applications", "author": ["Shailesh Mangal"], "link": "https://blog.developer.atlassian.com/monitoring-connect-applications/", "abstract": "Uptime is perhaps one of the most important operational aspects of cloud-based services. High performance and functionality is only good when services can stay up 100% of the time. Frequent outages not only cause disruption in regular usage but also cause tangible damage to your brand. Gone are those days where 3 nines and 4 nines were considered good standards of high availability . Consumers these days expect 100% availability and depend heavily on it. Imagine Google Maps being down for an upgrade or not getting emails because Office 365 is rebooting its servers. While it's important to follow good design patterns and keep your services scalable, that's still only half the story. Equally important is a healthy runtime for hardware, applications servers, and database servers. In this post I share some of the vital types of monitoring and tools needed. In modern cloud infrastructure, hardware failures are inevitable. Core Infrastructure Monitoring (CIM) is about detecting early signs of hardware related bottlenecks along with capturing hardware failure signals, and acting upon them before they become a larger issue. CIM includes monitoring health of hardware. Knowing the health of machines, CPU usage, memory consumption, and network bandwidth gives us insights into the current state of your infrastructure, its handling of your overall load and its scaling at various loads throughout the day. There are several great tools you can use to get a good sense of hardware health. In most cases, tools provided by your hosting provider (e.g. Amazon AWS, Heroku) should be sufficient for this purpose. Application Level Monitoring is all about monitoring the state of various servers, such as database servers, application servers, analytics servers, and Hadoop clusters. Parameters that are monitored are generally specific to the application and tools being monitored. There are several great tools for monitoring applications. Some great tools to consider are Datadog and New Relic . Microservices are an integral part of modern cloud architecture and are the king ping of horizontal scalability of cloud services. Whether you are running a traditional monolithic system or a well designed, well-orchestrated honeycomb of micro services, there will always be different API endpoints, different contracts to abide by, and different SLA requirements to fulfill. Microservice monitoring is all about monitoring throughput and the performance of every service – making sure SLA is met at all times. This kind of monitoring typically requires instrumenting the apps, making instrumentation configurable, and connecting it with a collector that can gather stats and periodically send them to a permanent storage, an analyzer and an alert system. This needs to be carefully designed as it can generate a lot of data and may impact app performance. You can avoid performance issues by creating groups of services and add configurability to switch on and off collection of each groups' monitoring data. Also, keeping collection period (e.g. every minute, every couple min) configurable also helps. Storage engines e.g. GraphiteDB or InfluxDB and visualization tools e.g. Kibana or Grafana . Being able to monitor logs and deduce actionable insights or identify root causes following a problem is perhaps one of biggest challenges of multi-tenant deployments. There is a significant volume of logs generated for numerous clients. Having a single unique identifier (e.g. tenantId) should be the first level of log segregation. In addition, log statements should be grouped by requests. This becomes particularly important when a request makes hops between different services, each one generating some log message helpful in identifying the issue. Properly configured Classic ELK ( Elasticsearch , Logstash , Kibana ) stack. For more info on an ELK stack checkout this webinar . To build well performing Atlassian Connect applications that will delight users, you need to spend the time setting up detailed monitoring. Great monitoring needs to cover everything from hardware, applications, and micro services. And if you've built a multitenant application, using a properly configured ELK stack will help you more quickly diagnose issues.", "date": "2015-08-11"},
{"website": "Atlassian", "title": "Codegeist 2015: Enhance the tools you use", "author": ["Dallas Tester"], "link": "https://blog.developer.atlassian.com/codegeist-2015-launch/", "abstract": "We’re delighted to announce Atlassian’s 8th add-on hackathon. Do you have an idea to make JIRA, Confluence, HipChat, or Bitbucket even better? Build an add-on, launch it in the Atlassian Marketplace , and you could be the next winner of Codegeist. Whether you’re an experienced add-on developer or trying something new, now is the time to build your add-on and share it with the community. All participants will get $75 worth of AWS credits, free promotion of your add-on, and an exclusive Codegeist 2015 t-shirt. For the winners, we have prizes totaling more than $100,000! Think you can build an awesome add-on? Sign up now and list your add-on by October 19. If you’re new to developing add-ons, check out our quick start guides on the Atlassian Developer site to start building today.", "date": "2015-08-26"},
{"website": "Atlassian", "title": "Tip of The Week – Adapting the Issue Collector", "author": ["Peter Van de Voorde"], "link": "https://blog.developer.atlassian.com/totw-adapting-the-issue-collector/", "abstract": "After a short break we’re back with more Tip of The Week articles. This week’s article is about the JIRA issue collector and how to remove the default name and email field. Because of restrictions on JIRA Cloud this tip only works on JIRA Server. The issue collector is one of the many hidden gems in JIRA . It’s easy to use and can provide you with another tool to get feedback from your users into JIRA. The issue collector does have 2 fields that are not configurable: Name and Email. They’re shown on the collector form when the user isn’t logged in to JIRA. These fields are not mapped on custom fields, but are simply shown in the description field of the issue. So how can you remove these fields and replace them with your own custom ones? Just add the following to the description of a field on your issue collector in your project’s field configuration scheme: Now you can use your own custom fields for Name and Email and make them searchable in JIRA.", "date": "2015-09-09"},
{"website": "Atlassian", "title": "Integrating Docker Hub 2.0 into Bitbucket with Atlassian Connect", "author": ["Steve Smith"], "link": "https://blog.developer.atlassian.com/docker-bitbucket/", "abstract": "As part of the Docker Hub 2.0 launch we’re pleased to announce integration of Docker Hub into Atlassian Bitbucket. This brings your Docker workflow together with Bitbucket to save you time and allow you to see source code stats along side your Docker repo in one place. The Docker Hub shows the status of a corresponding Docker Hub image repository from within your Bitbucket repository. This allows you to keep your Dockerfiles in Bitbucket and see their build, star and pull status directly in the repository. Installing it is a 2-step process. By default the add-on will assume that your Bitbucket and Docker Hub accounts/repositories have the same names; e.g. if your Bitbucket repository is: the corresponding Hub repository is: You can override this default in .docker-repository.yml YAML file. If you add the entry repository the add-on will use this for the Docker account/repository. For example: The integration is fairly simple at the moment, but we intend to improve it over time, including adding support for private repositories and registries, manual build actions, and more build information. The add-on is built on top of Atlassian’s Atlassian Connect for Bitbucket framework. This system gives remotely hosted, third-party services access to the UI real-estate and internal events of Bitbucket, without having to become part of Bitbucket. It does this by leveraging modern web technologies such as webhooks , REST and cross-domain messaging . Because it is built on web-technologies this means it is by definition cross-platform. As a developer this makes me very happy, as it allows me to build Atlassian add-ons in whatever language or framework I wish. In this case the Docker Hub add-on is built on top of the Clojure language. There’ll be another blog post along shortly to explain some of the details about this, but for now the add-on is open-source and hosted on Bitbucket , so feel free to have a dig around it there if you’re interested. For hosting the add-on is built into a 12-Factor application in a Docker container, and deployed to our own hosting service called “Micros”, that runs on top of AWS. But Connect add-on can be hosted anywhere, and I’ll be talking a bit about how to do this with various cloud solutions in future posts.", "date": "2015-09-17"},
{"website": "Atlassian", "title": "Easy HipChat Addons In Go", "author": ["Nicola Paolucci"], "link": "https://blog.developer.atlassian.com/easy-hipchat-addons-in-golang/", "abstract": "If you are a Go programmer you know how easy it is to whip up an application that speaks HTTP. Go was born for the task. So it will come as no surprise that it’s possible to create an Atlassian Connect for HipChat add-on with less than two hundred lines of commented code. What will this code accomplish? A new custom command /test_hook , installable on any channel you are administrator of: This post will show you the code to do just that. Our only dependency is the tiny and awesome mux router to simplify creating HTTP Handlers and Golang HipChat . For the bare bones app I have in mind the data structure we need is just a Context to keep track of: The starting point of our add-on, the main , is simple: we initialise the Context parsing some command line parameters, we create the routes we’ll respond to and start listening on a port . The baseURL flag can be set manually from the command line but it is initialised from the environment variable BASE_URL , useful thing to have when deploying your application inside a container for example. The routes of our web application are defined in a method and defer to the respective methods of our Context : The HipChat API version 2 will invoke several callback URLs to customise and embed our application. To do this HipChat will scan the capabilities we have specified in atlassian-connect.json . Just a simple health check to prove that our application is up and running (needed by our internal PaaS): The technique json.NewEncoder(w).Encode(...) is a neat way to write a data structure to JSON in a HTTP Response. Next I want to parametrise the descriptor atlassian-connect.json with the proper baseURL so that it works both locally, with ngrok or deployed in staging and production, so we parse the JSON configuration file through the html/template library: The next section implements the needed web routes to respond to the HipChat API version 2 as defined in atlassian-connect.json . The /installable callback URL is invoked when the add-on is installed and the incoming request contains a couple of critical OAuth strings that we need to store so that we can submit authenticated HipChat API calls. I use a helper function DecodePostJSON to read the request body and store it into a Go map[string]interface{} . This callback is called to display the configuration pane of your add-on: The webhook capability allows to specify room command ( /test_hook in the example below) that will trigger a callback to the add-on. In this case we just ping back to the room a simple styled message: Due to us reading up payLoad in a generic interface{} we have to write a long winded cast to get the roomID back. See the entire source on Bitbucket . To run this tiny application locally do the following: That’s it for now. I hope you found this piece interesting and useful. Ping me questions here in the comments or at @durdn .", "date": "2015-09-24"},
{"website": "Atlassian", "title": "Tip of the Week – Use JIRA Agile boards to visualize anything", "author": ["Peter Van de Voorde"], "link": "https://blog.developer.atlassian.com/totw-vizualise-anything-with-jira/", "abstract": "This week’s article is about using JIRA Agile to visualize much more then just your development workflow. As developers we are familiar with using JIRA Agile to track our tasks, bugs, stories and epics in an easy and intuitive way. We might even use it to track the tasks of several teams across the company on one big board. But did you know that you could pretty much visualize anything you wanted using JIRA Agile? Once you understand the basics of the JIRA workflow and issue types you can do amazing things like: Here are some of the crazy things we at Atlassian have done using JIRA Agile: So what are your cool uses of JIRA Agile? Share them in the comments!", "date": "2015-10-02"},
{"website": "Atlassian", "title": "Confluence 5.9 EAP release now available", "author": ["Matthew Jensen"], "link": "https://blog.developer.atlassian.com/confluence-eap-now-available/", "abstract": "The Confluence development team is proud to announce that the first Early Access Program (EAP) release of Confluence 5.9 is now available. There are a large number of API changes and library changes, so now is the time to begin testing and planning new compatibility for your add-on. EAP releases are early previews of Confluence during development. We release these previews so that Atlassian’s add-on developers can see new features as they are developed and test their add-ons for compatibility with new APIs. The number of EAP releases before the final build varies depending on the nature of the release. The first publicly available EAP release for Confluence 5.9 is pluginsfour018 . Developers should expect that we will only provide a few EAP releases before the final release. To find out what’s in this release, check out the Confluence 5.9.1-pluginsfour018 Release Notes . Download the EAP release from our site and start testing your add-on for compatibility with changes to Confluence. Be sure to take a look at our Preparing for Confluence 5.9 guide for details of changes that may affect your add-on. We’ll update that page regularly, so we recommend you check in on it regularly to ensure you’re aware of any changes.", "date": "2015-10-06"},
{"website": "Atlassian", "title": "Run, Bucket, Run: a Bitbucket FileView add-on", "author": ["Tim Pettersen"], "link": "https://blog.developer.atlassian.com/bitbucket-fileview-addon/", "abstract": "Bitbucket recently released a new add-on module type: the FileView . FileViews allow you to define how files of a particular type are displayed on the Bitbucket source view page. In this post, I’ll show you how I built Run, Bucket, Run : a fun, if somewhat inefficient, way to view your source. FileViews are great for building all sorts of things. You can visualize a binary filetype that isn’t natively supported by Bitbucket, like 3D STL s or OBJ s, or office documents or Keynote files. Or you can provide an enhanced experience for viewing specialized types of text files, like linking modules from package.json to their source repository. Or, you can build something almost totally useless but mildly entertaining, like me . I love video games, and procedurally generated content is a concept that’s fascinated me for a while. So I built a simple platform game that generates levels from your source code. Each platform represents a single line of code (LOC), with the platform length corresponding to the line length, and the platform elevation corresponding to the indent of the line (the deeper the indent, the higher the platform). This means that long line translate to bigger platforms, and code styles that make heavy use of whitespace translate to more interesting levels. You can play Run, Bucket, Run for any source file you like by installing the add-on on Bitbucket . But since a Bitbucket add-on is really just a standalone web application, you can also play it in “quine mode” right here in this iframe: “ Quine mode” is when the game uses it’s own source code to generate the level. If you’re curious, getting the source of a the current HTML document is pretty simple: Run, Bucket, Run is a static HTML5 application comprised of two important files: game.html , which contains the game logic, and connect.json , which is a Bitbucket add-on descriptor that contains all the information needed to make it installable in Bitbucket. The game is built on the HTML5 Canvas API . One of my goals was to write the game with zero dependencies, so that to finish the game you must navigate the entire source of the game. In order to acheive this I built a simple physics engine and rendering pipeline from scratch. Take a look at the source and you’ll realize pretty quickly that this is both my first time working with Canvas and writing a platform game, but I’m pretty happy with the final experience. I won’t go into detail about the gameplay mechanics here, but I’ve left fairly verbose comments in the source if you’re curious. The more interesting part is how we wire it into Bitbucket as an Atlassian Connect add-on. As I mentioned earlier, a Bitbucket add-on is a basic web application that exposes a JSON descriptor documenting the various end-points and add-on modules that it supports. Here’s the descriptor for Run, Bucket, Run: If you like, you play the descriptor too (my high score is 0x0000088b ). There’s quite a lot going on there, so let’s break it down: This is some meta-data about the add-on, this will end up being used to list it in Bitbucket’s Find new add-ons screen. The baseUrl specifies where the add-on is hosted. I’m actually using another Bitbucket add-on – Aerobatic – to host Run, Bucket, Run. Aerobatic supports a really nice continuous deployment flow for static HTML5 web apps straight from Bitbucket repositories. If you have a Bitbucket repository containing a web site, check them out, their deployment flow is very slick. The modules section is the meat of the add-on. This is where we define the areas of the Bitbucket UI that our add-on will augment. There are a bunch of different supported modules , but in this case we’re just adding a single fileView . Here’s what the various properties mean: All Bitbucket add-ons must specify an oauthConsumer . In our case, it will be used to allow us to make requests on behalf of the user playing the game to retrieve file contents from their repositories. Scopes are the permissions that an add-on requires users grant it in order to operate. Run, Bucket, Run requires only the repository scope in order to retrieve source file content, but there are plenty of others available that allow add-ons to take other actions. Contexts describe how the add-on is installed. There are two possible values: account and personal . The account context means the add-on is enabled for all repositories owned by the user or team that installs the add-on, and is visible to all users that browse the repository. The personal context means the add-on is visible only to the user who installs it, but is enabled on all repositories that they browse to. I’ve made Run, Bucket, Run a personal add-on so you can “play” any source code that you come across, regardless of who owns the repository. Whew. That’s probably enough about connect.json for now. There’s just one other thing we need to do to make our add-on installable in Bitbucket. In order to securely sandbox third-party code, Bitbucket embeds add-ons in an iframe . But not any ordinary iframe. The Atlassian Connect framework provides a JavaScript bridge that allows the add-on to breakout of it’s sandbox and communicate with the Bitbucket host page via a cross-frame API. The API allows you to display dialogs and status messages, resize the iframe, access local storage and subscribe to events published by other frames. To set up the bridge, you simply need to include a special script file, all.js served from Bitbucket. The bridge also exposes a library named request that allows add-ons to send requests to the Bitbucket REST API, authenticated as the current user. The add-on is only able to access REST resources that correspond to the scopes declared in its descriptor and oauthConsumer . Run, Bucket, Run uses this to retrieve the file text content used to generate the level: In the code above, the srcRawUrl variable is initialized with a URL targeting Bitbucket’s source REST resource, which we have access to thanks to the repository scope declared in connect.json . To construct the URL, we also need to know the context repository, commit and file path that the user is viewing with Run, Bucket, Run. These are taken from the URL of our iframe, as Bitbucket substitutes the {repo} , {cset} and {path} tokens in the url property of our fileView module for the appropriate values from the current context, so: becomes: The ability to make cross-frame requests is especially convenient as it allows our add-on to be completely static and written entirely in simple client-side HTML, CSS & JavaScript. This in turn makes it responsive, scalable and keeps hosting costs negligible. The security conscious among you might be wondering why we would let an add-on make API requests on your behalf, even if they are scopes. In fact, I skipped over a small but important step in the flow. The first time an add-on attempts to make a cross-frame request, Bitbucket will actually prompt the user to see if they’re happy to allow the add-on to act on their behalf. This prompt is triggered automatically by invoking the request function shown above. Once the user has granted the add-on access, the request automatically proceeds. If they refuse, the add-on’s error handler is called. This is pretty neat as Bitbucket does all the heavy lifting of rendering the dialog and prompting the user, all the add-on developer has to do is call request ! Thanks for reading this long-ish post! If you’re curious about add-on development, head on over to the Bitbucket developer docs and try out the five minute tutorial. If you liked the music and SFX, you should send some love to my bro, Matt who put together all of the awesome audio used in Run, Bucket, Run. If you have any questions about Bitbucket or add-on development, drop me a line on twitter (I’m @kannonboy ).", "date": "2015-10-08"},
{"website": "Atlassian", "title": "Tip of the Week – Triggering a Bamboo build from Bitbucket", "author": ["Peter Van de Voorde"], "link": "https://blog.developer.atlassian.com/totw-triggering-a-bamboo-build-from-bitbucket/", "abstract": "First we need to create our Bitbucket repository as a Linked repository in Bamboo . Then we’ll need to create a build plan for the code in this repository. We must take special care to create the right trigger: Next up we need to go to our Bitbucket repository and create a Bamboo service . . That’s it, if you now push your changes to Bitbucket the Bamboo build will be triggered automatically. If you like this article don’t hesitate to share it!", "date": "2015-10-08"},
{"website": "Atlassian", "title": "Announcing HipChat Connect beta – Chat will never be the same again!", "author": ["tcrusson"], "link": "https://blog.developer.atlassian.com/announcing-hipchat-connect/", "abstract": "At AtlasCamp 2015 we announced we were working on Atlassian Connect for HipChat. During a recent hackathon week in San Francisco we gave early access to 25 developers from awesome companies – including New Relic , PagerDuty , StatusPage.io , Tempo , Wittified , Meekan , Notify , and Zendesk – and the result was quite impressive! The API is currently in private alpha and will enter public beta in a few short weeks. So get ready – and on that front – we’re running developer training at Atlassian Summit 2015. Register for training Join the beta See what you could quickly build with this demo from SupportKit : HipChat Connect is HipChat’s next generation API that lets you build add-ons extending the HipChat apps. It is based on the same Atlassian Connect framework you already use to build add-ons for JIRA, Confluence, and Bitbucket. You can build HipChat add-ons in any language, and run them on any platform: all you need is for your add-on to talk to HipChat over open Web protocols (HTTPS, REST, OAuth, JWT). Here are the components of the API: Your app can send a Card to HipChat by POSTing structured JSON data to a REST endpoint. HipChat will display it as a Card in a HipChat room, and will optimise the rendering of the Card for the device the user is viewing it on: Cards: notifications that should interrupt/educate the conversation You can add Actions to various places in the HipChat apps. Currently, supported locations are input and message actions. Actions can open a sidebar/modal dialog View, or redirect the user to another application. Message action: deploy, add to your todo list, etc. Input action: start a hangout, select a meme, etc. You can add a Glance to the HipChat main sidebar. Notifications come and go in Chat rooms, but Glances are sticky, and always show in the sidebar in the context of a room. You can use them to display time critical information that needs someone’s attention: there are 3 open incidents, your product was mentioned 30 times on Twitter in the past half an hour, etc. Clicking on a Glance opens a sidebar View. Glances: what’s the status of what my team’s working on? Does anything need my attention? Your app can extend the HipChat user interface by adding Views (HTML/JS/CSS) to specific locations, currently in the HipChat right sidebar or in a modal dialog. HipChat will create a container and load the content from your app. From a View, your UI can use the HipChat JavaScript API to interact with the HipChat App, for example to create/open a room or a 1-1 chat, or populate the input field. Sidebar View, e.g. list all blockers impacting the current sprint Dialog View, e.g. select a Giphy These are the basic building blocks of HipChat Connect. Now we’ll show you how you can use them to build powerful integrations. Do you have an app you'd like to integrate with HipChat using HipChat Connect? Here are a few examples of what you can do: The problem : if your application already integrates with HipChat, teams can stay up to date on important events thanks to notifications your app sends to HipChat rooms. But these notifications can sometimes become somewhat…spammy. The solution : You can now improve the signal-to-noise ratio using HipChat Connect.your app can add a HipChat Glance to let users know if something needs their intention, and a HipChat sidebar view to show more details. If something needs urgent attention, your application can still send a notification as a Card in the main chat view. Monitor the status of Cloud services with StatusPage.io The problem : users share a lot of links in HipChat rooms. In some cases, for example a public YouTube link, HipChat can automatically resolve the link and show more information, e.g. a thumbnail of the video. But if users share links from apps requiring authentication or deployed behind a firewall, then we can't do that. The solution : your app can implement a webhook that listens for messages in HipChat rooms matching regex patterns (URL, issue key). As a response, your app can post a HipChat Card to the room, providing more information about the resource the link points to. Rich context in conversations when talking about Zendesk tickets Going further, Cards can expose links/actions which open an up-to-date view of the linked resource in the HipChat sidebar. This is very handy for users when discussing time sensitive, critical events. Discuss new exceptions in your apps with Sentry The problem : HipChat is where users make decisions. But where do they go to take action? Today they have to keep switching back and forth between applications, and navigating to the right context to take action. The solution : your application can add HipChat Actions to Cards, which can open a View in the HipChat sidebar or modal dialog, or redirect the user to your application in the right context. The problem : if you were to create polls in HipChat today, you would do it via a bot or a slash command. They are both difficult to discover and not very intuitive to use. The solution : user interfaces… Doh! Conversational style with Hubot Extending the HipChat UI with Convergely he problem : let's say your app lets users escalate incidents, and most of your customers are also using New Relic, which sends notifications about incidents to HipChat. Today, to escalate an incident, users would have to go to your app and copy/paste the New Relic data. The solution : your application can contribute HipChat Actions to other application Cards, for example you could add a “escalate incident” Action to New Relic incident Cards. That is very powerful. Linking it all together from a New Relic alert You can read the API documentation here: HipChat developer docs , and start to familiarize yourself with the API components and how they fit together. The API is in private alpha for few more short weeks. But you can register your interest by sending an email to integrations@hipchat.com – make sure to tell us what you’d like to build! We will send you instructions as soon as the public beta comes out. We will also run developer training at Atlassian Summit 2015 so you can get started and build a HipChat add-on using most of the components of the API. It’s your chance to learn directly from the developers of the API. Register for training", "date": "2015-10-13"},
{"website": "Atlassian", "title": "Tip of the Week – Using a macro-enabled keyboard in Confluence.", "author": ["Peter Van de Voorde"], "link": "https://blog.developer.atlassian.com/totw-using-a-macro-enabled-keyboard-in-confluence/", "abstract": "I’ll let Stephen explain it: When we do a major page overhaul and we need to update 70+ pages with a new setup we have a little trick: We don’t use blueprints due to the nature of how we use Confluence, so I implemented some macro functions I could use with a macro enabled keyboard and edited all 70 pages in under 1 min. The interns love this since its less busy work for them. After requesting some more information about what kind of macros he uses he gave us the following answer: The macro keys are rather simple once you understand them. A macro is a small script that’ll run every time you hit the macro key. So I set up Chrome to open all the pages I wish to edit. From there I activate the macro key which does a CTRL+TAB to switch between tabs, a little bit of key magic which select/pastes/tabs/more pastes. Then CTRL+S to save and then the macro loops. I have it set to be toggled, but you can set it to time out after a certain amount of runs. Here is a small screengrab from Stephen: Thank you Stephen for this tip! Another use case might be using such a keyboard to quickly do routine maintenance tasks in Bamboo, JIRA, or Bitbucket Server. So how do you start using this kind of keyboard? Here are some first steps: For those of you who don’t feel like buying another keyboard or who would like to do this “Developer Style” there is of course our fully enabled REST API for Confluence. This allows you to write something similar to this solution in your favourite language. Take a look here for a tutorial : Confluence Gardener Tutorial . Cheers and till next week!", "date": "2015-10-16"},
{"website": "Atlassian", "title": "Bitbucket Server now available in the Atlassian Plugin SDK", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/bitbucket-server-amps-6-1-0/", "abstract": "The latest version of the Atlassian Plugin SDK now has support for Bitbucket Server . Get the SDK You can get the latest version by either downloading the latest installer , running atlas-update or using your favorite Package Management software, like Homebrew . Once you have the latest Atlassian Plugin SDK you can then use the atlas commands with bitbucket . For example… You can start a version of Bitbucket Server for development of add-ons with the following command: “` $ atlas-run-standalone --product bitbucket “` You can create a Bitbucket Server add-on skeleton by running the following command: “` $ atlas-create-bitbucket-plugin “` You can run the add-on and attach it to the server with a debugger attached at port 5005: “` $ atlas-debug “` Get the SDK Have questions or issues with the SDK the best place to ask is over at Atlassian Answers . Ralph is a Developer Advocate for ecosystem developers.", "date": "2015-10-20"},
{"website": "Atlassian", "title": "Release: Bitbucket Docker Hub 0.3.0; the private repo release", "author": ["Steve Smith"], "link": "https://blog.developer.atlassian.com/bitbucket-docker-0-3-0/", "abstract": "In case you missed it, last month we launched our Bitbucket Docker Hub integration as part of the Docker Hub 2.0 launch . We are now pleased to announce the next version of this add-on is now available. If you already have it installed you’ll get it automatically. If you haven’t already installed it see below for instructions on adding it to your account. Carry on reading for more information on this release. The Docker Hub shows the status of a corresponding Docker Hub image repository from within your Bitbucket repository. This allows you to keep your Dockerfiles in Bitbucket and see their build results and pull status directly in the repository. Installing it is a 2-step process. By default the add-on will assume that your Bitbucket and Docker Hub accounts/repositories have the same names; e.g. if your Bitbucket respository is: the corresponding Hub repository is: You can override this default in .docker-repository.yml YAML file. If you add the entry repository , the add-on will use this path for the Docker account/repository. For example: will then use the Docker Hub repository path of: Learn more by checking out the add-on source repository .", "date": "2015-10-29"},
{"website": "Atlassian", "title": "Tip of the Week – Using the JIRA Software REST API", "author": ["Peter Van de Voorde"], "link": "https://blog.developer.atlassian.com/totw-using-the-jira-software-rest-api/", "abstract": "I’ll show you the following basic JIRA Software scenario: Warning: To use the JIRA Software REST API for anything other than reading information you’ll need to be authenticated, even if you can browse the project anonymously! For more information about authenticating using the REST API take a look in our documentation . Let’s start by creating a project called ‘REST API Example Project’. As you can see we are piping the JSON to json.tool to get nicely formatted JSON. I explained this usage in another Tip of the Week: Making JSON readable on the command-line . We get the following JSON reply: Remember the project id, we’ll need it for our next steps. And here you can see our project in JIRA Software: Let’s add 3 issues for this example: And JIRA Software in combination with json.tool returns us this nice JSON: And here we can see those 3 issues in our product backlog in JIRA Software: Now we’ll create a first sprint: And the following JSON is returned: Remember the id, we’ll need to use this in the following steps. Our sprint has been nicely created on our Scrum board: Let’s add our 3 issues to this sprint: And here you can see the result: Let’s get this sprint started! We get all the available information back in JSON: And our sprint has now officially been started: Now we can move our issues through the different steps of our workflow. We’ll first need to know the available transitions, REST to the rescue! This gives us the following available transitions: So let’s move 2 issues into Done and 1 into In Progress . Which gives the following board: To round it off we’ll finish this sprint, even though one issue hasn’t been completed yet. This once again returns all the available information for this sprint: And as you can see our unfinished issue is back on top of the product backlog. This is only one of the many things you can do using the JIRA Software REST API. For those who need something to start here is a small script you can use to create a project and some issues: Set-up Test Project Script . Keep in mind that this is just an example script. Enjoy the REST API and please share this article if you found it helpful!", "date": "2015-12-07"},
{"website": "Atlassian", "title": "Let’s build a Bitbucket add-on in Clojure! – Part 2: Serving our Connect descriptor", "author": ["Steve Smith"], "link": "https://blog.developer.atlassian.com/clojure-connect-part-2/", "abstract": "In part 1 of this series we did the fundamental work of building a Twelve Factor HTTP-stack from the ground using Leiningen , Ring , Compojure , and Immutant . However, that was just the foundations, and now we’re ready to start adding the necessary tooling to produce a full Atlassian Connect for Bitbucket application. This will include templating and introduce how to specify and authenticate our Connect add-on via its descriptor . All Connect add-ons need to serve up a descriptor . This JSON file provides the add-on’s location and identifying information along with the permissions it requires, its API, and other metadata such as a name and description. As we want our add-on to insert run-time information into this descriptor before it goes out we’re going to need a templating tool. In the Clojure world templating tools generally fall into two groups: those that take Clojure s-expressions and output a given format (usually a tree-oriented one such as HTML or JSON), or those that operate on marked-up files/data. The latter is what most people think of when talking about templating, and it’s what makes the most sense for generating our descriptor, which is largely static. However Ring and Clojure HTTP clients can be configured to automatically convert between JSON and Clojure data-structures, which will come in useful later. There are quite a few markup-style templating engines for Clojure. Most of them aim to be compatible with defacto standards from other ecosystems, such as Handlebars (Javascript) or ERB (Ruby). In this case I’ve chosen to use Selmer , which is closely related to Django ‘s templating system, but you can use an alternative one if you prefer. The descriptor we need for this project looks like this (template injections are delimited by {{ / }} ): As you can see we need we’re going to inject two variables: base-url and oauth-key . The base-url is where the add-on will be running (e.g. your ngrok tunnel if running it locally). The oauth-key is the key that uniquely identifies who controls this add-on. This needs to be generated within Bitbucket via Manage account > OAuth > Add consumer ; see the Bitbucket Connect getting started guide or the later installments in this series for more details on using ngrok and OAuth to develop Connect add-ons. As mentioned previously, we’re building this as a Twelve Factor application. This means we want to define our runtime information in the environment and then extract them with the environ library we used earlier to configure the HTTP server. In production we would set the variables using the system environment via export or similar. However in development this can be a pain. Luckily environ also supplies some methods to supply these during development. The simplest is to add the file .lein-env with a dictionary of your variables; in our case it would look like: If you don’t want this checked into to git just add it to your .gitignore . For a more flexible system, environ supplies a Leiningen plugin; add the following to your project.clj :plugins list: This allows us to create an :env entry the same as the file above but in the Leiningen :dev profile which will set the environment. You can also add this to your profiles.clj ; see the profiles documentation for more information. Before we can render the descriptor we need to make it available to the runtime. In the JVM world this usually means adding it to the resources path. Create a new directory in the base of our project called resources , and another under that called views . Place the above descriptor template into a file called atlassian-connect.json.selmer . We tell Leiningen about this resources directory by adding the following entry to our project.clj : Leiningen will then place that directory on the JVM classpath. Now we can have Selmer render the content and Ring / Immutant serve it. Add the following functions to our handler.clj : This first function extracts the necessary variables from the environment and uses them to render the template we created (you’ll need to add [selmer.parser :as selmer] to your :require list). The second function wraps the resulting output in Ring response map. This is what we’ll pass back to the HTTP server to return. The last step is to add a route to retrieve the file. We’ll have both / and /atlassian-connect.json serve this up by default. To do this return to the defroutes section of handler.clj and replace the existing “Hello Connect” route with the following: Now we can start up our server with lein run and go to http://localhost:3000/ . If everything is working OK you should be redirected to http://localhost:3000/atlassian-connect.json and see the rendered version of our Connect descriptor. The code for this part of the tutorial series is available in this tag in the accompanying Bitbucket repository . There will also code appearing there for the later parts as I work on them if you want to skip ahead. Now that we have a descriptor we can start adding actual functionality to our add-on. You may notice that our descriptor above contains references to a lifecycle . This API allows us to Bitbucket to initiate a relationship between a user and our add-on by providing information such as user metadata and a unique identifying key. Next time we’ll look more closely at how to respond to these calls, and after that we’ll look into communicating with Bitbucket using a client-side Javascript channel or server-to-server calls.", "date": "2015-12-10"},
{"website": "Atlassian", "title": "How we built a HipChat Connect add-on in a week and a day", "author": ["Jake Furler"], "link": "https://blog.developer.atlassian.com/nps-for-hipchat/", "abstract": "Last month the HipChat Connect platform was announced at Summit. HipChat Connect brings the power of Atlassian Connect to HipChat, allowing you to create add-ons that run on the platform of your choice and add functionality and custom user interfaces to HipChat. During the most recent ShipIt at Atlassian, a small team of us created an add-on using HipChat Connect. The HipChat Connect platform was super simple to use, and we want to share how exactly we went about creating the add-on so that you can give it a go yourself! Net Promoter Score (NPS) is a measure of how customers feel about your product and what you can do about it. NPS for HipChat, our add-on, integrates with SurveyMonkey to bring NPS feedback from customers into your product team’s HipChat room. Once configured, the add-on allows you to view a list of all responses to your NPS survey in the HipChat sidebar. You can filter responses, view each response in detail, and bring them into the chat for discussion with your team. NPS for HipChat is listed publicly on the Atlassian Marketplace , so you can install it and try it out for yourself. We’ve also made it open source on Bitbucket . Check out this quick demo to see how it works: Atlassian Connect is language agnostic – you can build add-ons using whatever web development languages and frameworks you want. However, to make things a bit easier Atlassian provides a couple of frameworks for developing HipChat Connect add-ons – one for Python ( flask ) and one for Node.js ( Atlassian Connect Express ). We chose to use Atlassian Connect Express (ACE). ACE can take care of authentication of requests using JSON Web Token (JWT) , data persistence, and the install/uninstall callbacks for you. It's really simple, and it allows you to focus on developing the add-on itself. To get started, after installing ACE on your machine, you just need to run the following command to set up your project structure and environment: The first thing we want to look at is atlassian-connect.json, the descriptor file. This is the core of any Atlassian Connect add-on. It declares everything the add-on intends to do and points HipChat to the right places on our add-on server. In our descriptor , we’re declaring that we want to use three main extension points: a glance, a web panel, and a configure page. A web panel is used to display your own content in the sidebar using ordinary HTML, CSS, and JavaScript. Here’s how we define ours in the descriptor, and how it ends up being displayed: We’re providing HipChat with a unique key and a location, which tells HipChat to display the web panel in the sidebar. The url parameter provides HipChat with an endpoint on our add-on server from which it can fetch the content we want to display. Everything inside the orange box in the image above is contained within an iframe, served from the endpoint we defined. We can display whatever we want. To link to the web panel, we’re also defining a glance in our descriptor. A glance is a button that appears in the sidebar alongside all the other add-ons. Again, we’re providing HipChat with a unique key and an endpoint on our server where it can fetch dynamic data. In our case, the dynamic data we return is a sunny icon if the NPS is on track and a cloudy icon if not. The value of the target parameter is the key of the web panel we defined above. This tells HipChat to open the web panel when the glance gets clicked. Finally, we’re defining a configure page. This one’s pretty simple. Like the web panel, this will end up inside an iframe as well, displayed when we install the add-on. In the descriptor, we’re just providing an endpoint that we’re calling config from which our content will be served. Let’s look at how we’re implementing that endpoint as an example. Atlassian Connect Express is based on the Express framework, so if you've used that before, or if you've used ACE in any of our other products, some of this may look familiar to you. The first thing we want to do is define the config endpoint using express, and use ACE to authenticate any incoming requests. The addon object is provided by ACE and all we have to do is call authenticate() on it. ACE handles the installation callback for you. When we call authenticate, behind the scenes ACE is matching the JWT token of the incoming request against the information established during installation. We can use this to make sure any incoming request is coming from a known HipChat room. From there we can perform whatever backend logic we need to and pass data to a handlebars template using express by calling res.render . Here’s our entire endpoint implementation : We’re simply performing some SurveyMonkey-specific logic, and we’re also using ACE to fetch some stored client data. ACE handles data persistence for you. Once you’ve set up your database through a few lines in a config file, you can achieve simple key-value data persistence by calling addon.settings.set and addon.settings.get . In our case here, we’re storing the survey name and the goal NPS for every room. And that’s it! That’s our endpoint. Everything else is ordinary web development. The framework provides total flexibility where it's useful, but also simple methods for providing data where a consistent design is needed. We can post cards into a room’s chat through HipChat’s REST API . When a user clicks on the “discuss” button in our sidebar web panel, we post to the HipChat notification endpoint. We include a JSON object containing the information that we want to display, and HipChat will format it into a card design and post it into the room. You can also define the description of a card using basic HTML. We’re using this to display the description as a link, which we can tell HipChat to open in the sidebar using the data-target parameter. We can even tell HipChat to pass a JSON object of parameters when it hits that link using the data-target-options parameter. The description value becomes: So we’re able to put all of our detailed data from the sidebar into the link of a chat card without displaying it. And then when that link gets clicked, the data gets passed back into the sidebar to be displayed in detail. Atlassian Connect is deployment agnostic too. We chose to deploy on Heroku just because it was simple to use with Node.js and Redis. But you can deploy your add-on service however and wherever you want. We made a functional version of this add-on within 24 hours at ShipIt. We developed it into the current version we just showed you during a second week-long hackathon. It's not difficult or complicated – you can have a look at our source code on Bitbucket . So have a test of our add-on on Marketplace and have a go at making one yourself!", "date": "2015-12-11"},
{"website": "Atlassian", "title": "Going (way) beyond slash commands with Uber and HipChat", "author": ["rmanalang"], "link": "https://blog.developer.atlassian.com/going-way-beyond-slash-commands-with-uber-and-hipchat/", "abstract": "Ever wanted to go to lunch with some colleagues and end up in an endless string of messages trying to line everyone up to meet in the lobby, pick a spot and show up on time? Next, everyone claims that their car is dirty and they don't want to drive. \"It is not always this dirty I swear!\" Sure, we could easily build an integration with Uber that used slash commands, but do you really want to trouble your users with typing in commands? If you've used the Uber app before, you'll know that the experience of requesting an Uber ride is pretty special. With essentially a tap, you can have a car head your way within just a few minutes. It's magic made possible by the little devices we all carry around. We wanted to build an experience within HipChat that represents Uber and that magic. Slash commands or talking to a chat bot would not have provided that experience. With HipChat Connect, requesting a ride with Uber is intuitive and easy. Once a ride is requested, you're instantly notified in the room of your ride's status (as well as on your Uber mobile app). In addition to requesting a ride, your team can can join a ride that's been requested. This gives you the ability to self organize without having to do the curbside shuffle. When your team's ride is arriving, each rider will will be notified in the room: All of these features amount to a rich experience that's intuitive and useful. Here’s a more complete demo: Uber's API is very straightforward and easy to use. Everything you can do in the Uber app, you can pretty much do in their API – right down to tracking where your ride is in real-time. So, we set out to design the experience we wanted. We could have easily built a slash command based integration in a matter of hours: But, now that we have HipChat Connect , a slash command solution seems like an anti-pattern. Slash commands are difficult to discover and use; Imagine typing in your destination address without auto-complete. While many of our technical users adore them, the other 80% of us are often confused with how to interact with them. HipChat Connect allowed us to build a more immersive user experience. This new integration with Uber was made possible by HipChat Connect and Uber's API . If we had built this integration without HipChat Connect, we'd be stuck with a bunch of slash commands that are difficult to use and impossible to remember – all the magic of Uber would be gone. The command line experience is still useful for those who prefer that. We actually have an /uber command stubbed out in our add-on, but haven't implemented it – and we may never implement it. The fact of the matter is, most users won't use a slash command if there's a better option available. Underneath this new add-on is a bunch of really cool tech: If you're interested in seeing what it looks like underneath the hood, feel free to clone it and run it locally. We've open sourced this add-on and would love for you to contribute to it: HipChat Connect is still in beta, but you can use it and build on it today. The next time you need a ride for your team, try using Uber from HipChat. Add it to your room today: You can also find Uber for HipChat along with 100+ other awesome HipChat integrations and 2000+ add-ons for all Atlassian products in the Atlassian Marketplace . If you’d like to see more features, reach out to us on Twitter at @hipchat or send us a pull request .", "date": "2015-12-14"},
{"website": "Atlassian", "title": "Tip of the Week – The Atlassian AllStars tips and tricks for JIRA – Part One.", "author": ["Peter Van de Voorde"], "link": "https://blog.developer.atlassian.com/totw-atlassian-allstars-tips-part-one/", "abstract": "We call them the Atlassian AllStars. They are an online community of Atlassian users, experts, and fans. This community offers opportunities to learn more about our products, to take part in exclusive surveys and interviews, to leverage content and to connect to like-minded fans. If you think you have what it takes to become an Atlassian AllStar, apply here ! This week we will focus on tips and tricks for JIRA Admins and power users. We know that in JIRA, you can use the \"opsbar-sequence\" property to nicely order the transition buttons. Be sure to: Always order the buttons in their most likely to be used order. (The most likely, \"happy path\" transition button should be displayed first.) Instead of ordering with values like 1,2,3, use larger values, like 10,20,30. That way, if a new transition is ever needed, you can insert it into the list without having to reorder the existing transitions. I like to use the card colours in JIRA Software to highlight items that are about to come overdue. I will have them turn yellow when they are a day out and then red on the last day before they are due.   There are many ways to use the card colours to bring attention to items, this is just one way I have done it. Tools like Atlasboard are a great way for organizing all your teams data into a custom expandable wallboard! Implementing other great ideas from ShipIt can also help your teams. I really like the ability to use the JIRA REST API (I use the atlassian-provided jar ) to read and manipulate JIRA. You can use it in any automation pipeline you want. Even if you are not using JIRA Software boards for your main development methodology, having a Kanban-style board of issues of interest to you is really useful. Mine has 3 swimlanes. Assigned to me; created by me; watched by me. Using the dot(.) and gg keyboard shortcuts , which open JIRA’s Operations and Administration search dialogs, saves an enormous amount of time for JIRA admins and users. A big thank you to all contributors, you are awesome! Be sure to come back for part two next week! Share your own tips and tricks in the comments or on Twitter, be sure to mention me: @pvdevoor or @atlassiandev !", "date": "2016-01-07"},
{"website": "Atlassian", "title": "Let’s build a Bitbucket add-on in Clojure! – Part 3: Creating our API", "author": ["Steve Smith"], "link": "https://blog.developer.atlassian.com/clojure-connect-part-3/", "abstract": "In part 2 of this series we built upon the foundations we created in part 1 to generate a Connect descriptor . That descriptor specifies, among other things, the API that Bitbucket should call on key events in our repository and add-on lifecycle. In this installment we’re going to look at how to specify and serve this API, and how to convert JSON sent to us by Bitbucket into Clojure data-structures. If we take a closer look at our descriptor we can see we define the followinhcg API: We’ll cover the webpanel component later on; for now we’ll look at the REST endpoints. The most important of these is /installed , which is called by Bitbucket when a user adds the repository to an account. This provides the add-on with some key information the it needs to store for future use, including a shared secret to verify future calls and authorise calls to the Bitbucket API, a unique per-installation key, and information about the installing user. Obviously we need to store some of this information across restarts of our add-on, so we need some storage. In a full production environment we’d probably use a database of some sort (for example the Docker Hub add-on uses Amazon’s DynamoDB ); however for this example we can just abuse Clojure’s ability to dump and read runtime data in its EDN format. Let’s create a new storage.clj file and add some save/load operations: ( Note : This is a very simplistic implementation and should not be used in a real add-on. In particular it’s not multi-tenanted as it doesn’t separate storage per installation via the clientKey field. But it will do for simple illustration purposes.) Now we have something to do with the information sent to us on installation let’s handle the API call. Bitbucket sends the installation context as JSON in the body of a POST to the endpoint; we’d prefer that the information was converted into Clojure’s internal data structures. There are libraries to do this, but as it’s such a common operation Ring provides a handy wrapper that will do this for us. To enable this, just import wrap-json-body from ring.middleware.json and add it to our routing stack: This will check the Content-Type of incoming requests and parse any JSON body into Clojure. Now we can add the API endpoint to the routes; first add the following to handler.clj : This will save the installation context and return an empty OK status. Then we add the endpoint to the defroutes section: We should also process /uninstalled calls. While not a major issue in our toy implementation we generally would like to clear unneeded data out of our database, and it is generally good practice to not store user information unnecessarily. However we don’t want to allow just anybody to make an uninstall call; in fact, we’d like all our API to be authenticated as coming from Bitbucket from now on. Luckily that’s what the installation information above is for, in particular the sharedSecret entry in the context. Obviously we’d like this context to be loaded on future runs of our add-on, so let’s load it on startup in handler.clj : The method Atlassian Connect uses to pass authentication to an add-on is JWT , a web-standard for encoding authorisation claims. There are already Clojure JWT libraries available , but Atlassian’s Connect defines an extension to the standard to add additional verification of query parameters. However as part of writing the Docker Hub Bitbucket add-on I produced a library to implement this extension, along with some helper operations. This allows to define a simple Ring-style wrapper that will authenticate a request and either deny access or forward onto another handler: Now we can create a handler to do the uninstallation (which with our naive storage implementation is just a delete): Then we add a route for /uninstalled that calls the uninstaller wrapped in authentication: We’ll use this method for the rest of our API. The last REST endpoint we need to create is /webhook . The webhook is called for any key events in the repository, for example on a push event. We’re not going to actually do anything with this information for our plugin, but it’s useful to see what we receive from Bitbucket. So we’ll create a function to pretty-print the received data: And again we just define and authenticate the endpoint: The code for this part of the tutorial series is available in this tag in the accompanying Bitbucket repository . There will also code appearing there for the later parts as I work on them if you want to skip ahead. Now we’ve handled the Bitbucket to add-on aspects of the API we need to address the more complex issue of retrieving information from Bitbucket and creating user-visible content from it. In the next part we’ll look at how to do that using both the Bitbucket REST API and a pure-Javascript implementation. We’ll use both of these techniques to display some key information in an embedded component on the Bitbucket repository page. Tune in next-time for more Clojure goodness!", "date": "2016-01-08"},
{"website": "Atlassian", "title": "Bitbucket’s new npm integration: a tale of three XHRs", "author": ["Tim Pettersen"], "link": "https://blog.developer.atlassian.com/bitbucket-npm-integration/", "abstract": "$(document.body).on('afterCodeRender', function() { // Highlight CORS headers in curl output $(\".CodeMirror-line\").each(function(i, line) { var $line = $(line); if ($line.text().toLowerCase().indexOf('access-control-') === 0) { $line.css(\"background\", \"rgba(250,250,70,.5)\"); } }); }); I’m a huge fan of Node.js and npm, so I’ve built a little npm for Bitbucket add-on that adds module metadata, dependency information and download statistics to the npm modules hosted on Bitbucket. What makes the add-on special is that it’s built in a slightly peculiar way: it’s 100% static HTML & client-side JavaScript. However, it uses a variety of interesting XHR techniques (CORS, window.postMessage , and API proxying) to exhibit some pretty powerful dynamic behaviour. npm for Bitbucket provides two additional bits of UI to your Bitbucket experience. First, it adds a statistics panel on the overview page for any repository containing a package.json in the root directory: This shows the version of the npm module in your repository, the latest version that’s been published to the npm registry, and some download statistics. The statistics are just vanity metrics, but the Repo version and Published version are handy if you or your co-workers have the habit of invoking `npm publish but forgetting to git push` (or vice-versa), which I have been personally guilty of on occasion. Second, the add-on provides a new Package Info view mode to Bitbucket’s source screen: Package Info renders your package.json file as a table of dependencies with some additional bits of information pulled from the npm registry. The Version and Latest columns are useful for keeping your project up to date with the latest and greatest npm module releases, and the License column is useful for making sure your project conforms to your dependencies’ licensing requirements (e.g. making sure you’re not violating the GPL). I also decided to inline the Description field for each dependency to give casual observers a bit more information about each module. With 200K+ modules in the registry named things like gulp , grunt , yo , and q it’s not always immediately obvious what each one actually does. I’ve put together a short demo video here if you want to see it in action: The add-on is open source and free. If you use npm in your projects, you should give it a try ! It’s hosted on Aerobatic , a static website hosting service who happen to have excellent Bitbucket integration. The rest of this post details how the add-on works under the hood. Actually, “under the hood” is a bit misleading, as the add-on doesn’t have a traditional application backend. Instead, the add-on is basically a static website that makes use of several flavors of XMLHttpRequest to source data from Bitbucket and npm. A static web application imposes some interesting design constraints that I’ll get to in a minute, but it does provide some seriously attractive benefits: Of course, there are some big drawbacks to using the static application approach too: These limitations make it hard or impossible to build many applications in a static fashion. However since my npm add-on is basically stateless and I’m comfortable with JavaScript, the only real challenge I had to overcome was that pesky browser same-origin policy. The rest of this post is about how I’ve used (and possibly abused) the browser XMLHttpRequest API to retrieve data from Bitbucket and npm and synthesize it into the integration above. In order to display the version information and download statistics, we first need to determine the name of the npm module contained in the repository that the user is viewing. To get this data, we need to retrieve the package.json file from the Bitbucket repository. Bitbucket has a convenient REST API for retrieving raw file content. For example, you can retrieve the add-on’s own package.json file here: GET /1.0/repositories/tpettersen/bitbucket-npm/raw/HEAD/package.json To make the REST request we’re going to need a couple of things: Fortunately the Bitbucket Connect framework provides both of these things. The npm statistics bar is integrated into the Bitbucket UI as a “web panel”, which is basically a souped-up <iframe> that targets our add-on server. The excerpt from our add-on’s JSON descriptor for configuring the web panel looks like this: At render time, the {repo_uuid} and {repo_path} parameters in the url property are substituted with values based on the repository that the user is currently viewing. This allows our add-on to pluck the UUID and repository path from the iframe’s query string and use them to identify the repository in our REST request. Bitbucket Connect also provides a cross-frame JavaScript API that allows us to make authenticated requests to the Bitbucket API. To use it we need to include a special JS file from Bitbucket into our iframe: all.js provides a rich API with various modules for interacting with Bitbucket. We can use the request module that it provides (which wraps the popular xhr npm module and mimics its API) to retrieve the package.json file from Bitbucket: As an aside: the all.js functionality is namespaced under AP , which originally stood for Atlassian Plugins in an earlier version of the Connect framework, but is now kept around for backwards compatibility reasons. The correct nomenclature for an application built with Bitbucket Connect is a Bitbucket Add-on . Of course, letting the add-on make requests on behalf of any user who browses to that page would be a security problem. So the first time a user views the integration, they’ll see a little prompt asking for the permissions we’ve configured in our add-on’s OAuth consumer: The OAuth dialog prompt is actually handled by Bitbucket Connect framework. All we have to do is invoke request and Bitbucket handles the authentication dance for us. Neat! Assuming the user clicks “Grant access”, our success function should be called with the contents of the module’s package.json . Then it’s simply a matter of parsing the JSON and picking out the name property. Now that we know the npm module’s name, we can retrieve some interesting information about it from npm! npm has a really nice API for querying package download statistics. From the package name, we can construct a request to get the downloads for the last seven days: GET api.npmjs.org/downloads/point/last-week/git-guilt This gives us the (fairly modest) download counts for my git-guilt npm module: Typically, same-origin restrictions would prevent us from querying this API directly, except that the npm download API returns CORS headers in their responses! That beautiful “ access-control-allow-origin: * ” HTTP header means we can hit the API from client-side JavaScript served from any domain matching * . That is, from anywhere ! I decided to make the request using superagent , a nice HTTP request library that works in both Node.js and the browser. It make what looks like an ordinary AJAX request, but with an absolute URL targeting the npm API: Well that was easy! The next step is to retrieve some version metadata from the npm registry. Unfortunately, npm doesn’t have a formal API for retrieving package metadata. However, you can mimic the npm command line tool and just hit their CouchDB web app directly: GET registry.npmjs.org/git-guilt It returns a nice JSON representation containing all published versions of your package.json : Unfortunately, there’s one small problem: No CORS headers! This means that, despite the fact that the API is public and unauthenticated, we can’t hit it from client-side JavaScript. A quick aside: developers, if you maintain an anonymously accessible API, please consider adding CORS headers for the sake of us static application developers! 🙂 However, with a bit of googling I did stumble across an intriguing web app living at npm-registry-cors-proxy.herokuapp.com . Sure enough: Huzzah! The magical “ Access-Control-Allow-Origin: * ” HTTP header. Unfortunately I couldn’t figure out who owned the proxy ( tweet me if you know) so it seemed unwise to build a production application on top of it. However, it did give me an idea. Aerobatic , the hosting solution I’m using for npm for Bitbucket, allows you to register proxy routes as part of your static web app. These allow static app developers to hit APIs that either require authentication or don’t support CORS as if they were regular AJAX end points running on their own domains. Configuring it is simple, you just add a _virtualApp section to your package.json mapping local routes to remote API paths: This maps any AJAX requests made by our add-on to /registry/{package_name} through to the npm registry at https://registry.npmjs.org/{package_name} . So now we can retrieve the package metadata using superagent again to make a simple AJAX request: And there we have it! With the ability to query data from Bitbucket, the npm stats API and the npm registry itself, building the rest of the integration was just a simple matter of HTML and CSS. You can check out the full source code if you like, and as always, contributions are very welcome. With a variety of XHR based techniques we’ve built a static application that exhibits some powerful dynamic behaviour, costs next to nothing to host, scales to very large numbers of users, and is extremely secure by virtue of running entirely in the user’s browser! Static applications certainly aren’t always the answer, but next time you’re building a new app or feature, consider whether part of it could be built using static techniques. And please, add CORS headers to your public APIs! If you’d like to try out npm for Bitbucket , you can use this handy install button: <a class=”aui-button aui-button-primary install-button” If you have any feedback on npm for Bitbucket, questions about Bitbucket or add-on development, or just want to chat about static web apps, drop me a line on Twitter! I’m @kannonboy . If you’ve enjoyed this post, you might also enjoy my talk about integrating with npm from our Mad Science Node.js meetup in November.", "date": "2016-01-12"},
{"website": "Atlassian", "title": "Let’s build a Bitbucket add-on in Clojure! – Part 4: Talking to Bitbucket", "author": ["Steve Smith"], "link": "https://blog.developer.atlassian.com/clojure-connect-part-4/", "abstract": "In part 3 of this series we added REST and JSON capabilities to our add-on. However most Atlassian Connect add-ons will want to add some user-interface elements to the Bitbucket repository too, usually by working with data from the repository. To get this data, the add-on will need to talk to Bitbucket directly. In this installment, we’ll look at a couple of ways to do this, including how to authenticate using the handshake information we received in the previous blog post . The real power of Atlassian Connect is the ability to modify Bitbucket’s UI to expand its functionality. However for most add-ons this will entail communicating with Bitbucket via the API to interact with a user’s repository and other information. This is why in the previous installment when we implemented the calls that Bitbucket will make back to our add-on we left out the /connect-example endpoint which serves up the HTML component that will be embedded into Bitbucket. Before we can fill out this endpoint we need to be able to fetch some metadata about the repository from Bitbucket. The Bitbucket API can be accessed via two methods; server-side calls, or from the browser UI component itself via JavaScript. We’ll use both in our example, so let’s define our UI component template in resources/views/connect-example.selmer : This is pretty much a straight port of Tim Pettersen’s NodeJS version Connect example , but using Selmer markup and adding a Clojure logo. The JavaScript Bitbucket callbacks are implemented in /js/addon.js , which is ripped straight out of Tim’s project: This code in turn utilises the all-debug.js library loaded from Bitbucket that sets up the web messaging between the browser and the server-side. (At this point, some people are probably asking pointedly why I’m not using ClojureScript . Don’t worry, we’ll get to that in a later installment, and maybe some bonus ClojureScript+ core.async fun too.) But more interesting for the time being are the server-side calls, so let’s get those working, too. We’ve already created template markers for variables in the template above, but we need to retrieve the information. We’ll create a new namespace in bitbucket.clj for this. As we’re going to be making REST calls and authenticating we’ll pull the HTTP client as a dependency: The JavaScript calls to Bitbucket are automatically authenticated by the browser, but for the server-side call we’ll need to authenticate ourselves. The recommended method to acquire an OAuth key in Connect is to use the shared-secret we received during the add-on installation (via the /installed endpoint) to request an OAuth key from Bitbucket . This is done by making a JWT-authenticated call to Bitbucket: (Note that these tokens last 90 minutes so should be cached for a production setup. However as this is just a demo we can cut corners.) It is also possible to use the JWT token to authenticate with the API directly, however this is not fully documented at this point so we’ll use the officially supported OAuth method. We’ll also specify a function to fetch the current user’s display-name by requesting the full information and extracting it with a Clojure threading macro : Now that we have our calls and our template, we can pull it all together by defining a calling function and route in handler.clj : In the previous installments we’ve created our Connect project, configured the descriptor and created a REST API. With the addition of a valid UI component to serve-up in Bitbucket we’re ready to take it for a spin. In the next installment we’ll review some of the practicalities of doing that, from both the Clojure and Bitbucket Connect points of view. The code for this part of the tutorial series is available in the part-4 tag in the accompanying hello-connect Bitbucket repository . There will also be code appearing there for the later parts as I work on them if you want to skip ahead.", "date": "2016-01-14"},
{"website": "Atlassian", "title": "Tip of the Week – The Atlassian AllStars tips and tricks for JIRA – Part Two.", "author": ["Peter Van de Voorde"], "link": "https://blog.developer.atlassian.com/totw-atlassian-allstars-tips-part-two/", "abstract": "We call these users the Atlassian AllStars. They are an online community of Atlassian users, experts, and fans. This community offers opportunities to learn more about our products, to take part in exclusive surveys and interviews, to leverage content and to connect to like-minded fans. If you think you have what it takes to become an Atlassian AllStar, apply here ! This week we will focus on tips and tricks for JIRA Admins and power users. Create an email subscription to an issue filter where priority is critical and the issue is unassigned. Set it to email a support group every 15 minutes (or x minutes) until someone takes the issue. This has been great for us in identifying high priority issues. Don’t forget to set the \"Fire an event that can be processed by the listeners\" post function on each transition in your workflow. This will make sure that the correct notifications will be sent out. Requirements traceability between Confluence and JIRA has been useful for keeping our requirements in one place as a single source of truth. Using the Requirements Blueprint in Confluence we can create all our requirements (and documentation) and then generate the JIRA issues from the requirements table. This automatically creates the JIRA issues based on the information in the requirements, as well as creating a two way link between the Confluence page and the JIRA issues. Now when we need to make changes to requirements we just edit the page (one single source of truth) instead of using the JIRA comments. You can set up an organisation-wide simple \"to do\" project for personal use by individuals by using JIRA’s \"Reporter Browse\" permission to allow only the person creating their to-do items to see them. The JIRA REST API was a major help when doing the initial migration of tens of thousands of Bugzilla issues. Only a small number of API calls and a handful generated scripts were used with great efficiency. My quick tip for JIRA is the CHANGED option in JQL .  Effectively, this allows you to find out much more information about how a particular field has changed over time.  Let’s say we’re a Project Admin, and we need to know which issues have moved from \"In Test\" to \"In Progress\" this week (even if they’re back in testing now).  With the CHANGED function, this is simple: A big thank you to all contributors, you are awesome! Be sure to come back for the next part next week! Share your own tips and tricks in the comments or on Twitter, be sure to mention me: @pvdevoor or @atlassiandev !", "date": "2016-01-14"},
{"website": "Atlassian", "title": "Release: Bitbucket Docker Hub 0.3.1; the made-for-Mercurial release", "author": ["Steve Smith"], "link": "https://blog.developer.atlassian.com/bitbucket-docker-0-3-1/", "abstract": "In case you missed it, last year we launched our Bitbucket Docker Hub integration as part of the Docker Hub 2.0 launch . We are now pleased to announce the next version of this add-on is now available. If you already have it installed you’ll get it automatically. If you haven’t already installed it see below for instructions on adding it to your account. Carry on reading for more information on this release. This is a bug fix release that addresses the following issues: The Docker Hub shows the status of a corresponding Docker Hub image repository from within your Bitbucket repository. This allows you to keep your Dockerfiles in Bitbucket and see their build results and pull status directly in the repository. Installing it is a two-step process. By default the add-on will assume that your Bitbucket and Docker Hub accounts and repositories have the same names; e.g., if your Bitbucket respository is: the corresponding Hub repository is: You can override this default in the .docker-repository.yml YAML file. If you add the entry repository , the add-on will then use this path for the Docker account and repository. e.g.: will cause the add-on to access the Docker Hub repository path of: Learn more by checking out the add-on source repository .", "date": "2016-01-15"},
{"website": "Atlassian", "title": "These are the content properties you’re looking for: Confluence Connect integrations and CQL", "author": ["Kate West-Walker"], "link": "https://blog.developer.atlassian.com/confluence-connect-integrations-with-cql/", "abstract": "The Confluence platform team has been building more ways for your Atlassian Connect add-ons to integrate with and extend search in Confluence. We’ve added Connect support for CQL query aliases and the ability for your add-ons to integrate with search filtering. Initially announced at AtlasCamp 2015, these awesome new features allow you to get the most out of content properties. Content properties are a way to store key-value pairs against a piece of content in Confluence. They allow your Connect add-ons to store associated JSON directly in Confluence using the REST API, removing the need for a backend server for static add-ons. Where they get really powerful is when they’re used in conjunction with CQL , allowing you to filter your search results using the data you previously saved on the page. One of the best things about content properties is the CQL integration. Defining your own search indexes allows you to hook into Confluence search and grab content based on the information you’ve previously saved as a content property. For example, you might have saved a calculated ranking in every Confluence page, and you want to pull up pieces of content based on that. This is what that CQL query might have looked like before: Now, with aliases, that can be simplified down to the query below, just by adding a single alias field to the definition of your content Property. In the definition of your content Property in your atlassian-connect.json , use the alias field to define a simple name to refer to that property by. It should look something like this: For more context on how these work, head over to the Atlassian Connect documentation on aliases . To extend the support for filtering content, we need to integrate it into lots of different places in Confluence. We want to be able to filter content not just as developers querying the REST API, but also provide that capability to end users. With UI support , you can add powerful search filters based on your content properties to any existing macros that aggregate and display content, e.g. the content by label macro . For example, say you had saved a ‘category’ property for each piece of content on your Confluence instance, and wanted to display content filtered by this property: In addition to enhanced macros, your filters will also show up in the search screen, allowing users to refine their search queries. Defining the uiSupport object in your atlassian-connect.json provides the information needed for Confluence to include it in all the CQL UI filtering locations available, such as enhanced macros and the search screen. The defaultOperator field decides what will be used as a comparator in the UI, while the dataUri defines an endpoint in your add-on which returns a list of valid values for this content property. In addition to building these new features, we’ve also put together a developer add-on to improve the process of working with content properties while working on your Connect add-on. This allows you to quickly see the state of content properties set in the current page, without having to do a cURL call to the REST API each time you want to check them. You can grab the content property toolbar from Bitbucket . You can also get more information on these new features and how they fit into the Confluence Platform by checking out Ben Mackie and Matthew Jensen’s talk from AtlasCamp 2015. Head over to the content property documentation to get started!", "date": "2016-01-18"},
{"website": "Atlassian", "title": "Let’s build a Bitbucket add-on in Clojure! – Part 5: Deploying our add-on to Bitbucket", "author": ["Steve Smith"], "link": "https://blog.developer.atlassian.com/clojure-connect-part-5/", "abstract": "In part 4 of this series we added the Bitbucket UI to our add-on. Although there’s more tweaks we’ll do in later installments, for now we have enough to do an initial test against Bitbucket. In this post we’ll show how to do this running from our development machine, and how to build and run a standalone deployment image of our add-on. In the last installment we added the last major pieces to produce our working Clojure Bitbucket add-on. So now we need to take it for a spin. As mentioned in the installment about generating a descriptor, Bitbucket Connect requires that we generate an OAuth key to identify who is responsible for our add-on. This key in turn is tied the URL where out add-on is hosted, so the first thing we need to do is decide where our add-on will live. If you already have a development hosting environment you can use that. However if you’re just want a quick test from your development environment then ngrok is a good choice. ngrok acts as an ad-hoc tunnel a port on our machine and the wider internet. So let’s get that running: This will produce output that looks something like: The part we’re interested in is the generated URL: https://1a222264.ngrok.com . Now we have this we can generate our OAuth identifier in Bitbucket. Login to your account and go to your setting (in the drop-down in the top-right corner). Then go to OAuth , click on Add Consumer and fill out with the following settings: This will result in an entry in the OAuth Consumers list. Click on it and retrieve the Key (not the Secret ): Note : If your ngrok URL changes you’ll need to update this OAuth key. This can be down using the Edit entry in the key options. Now we have a URL and its key we can pass this to our add-on to use in the descriptor as described in part 2. As we’re building a 12 Factor application we set this using the environment. In previous installments we’ve used Leiningen to set the environment and run the application, but this time let’s try something a little different. Leiningen can be used to run applications in production, but the more common method is to pack our application and all qits dependencies into an uberjar , which can be distributed and run in the JVM. To do this we need to make a minor modification to our core module; Clojure is a dynamic language that can compile its code at runtime or ahead of time . However we JVM expects a main class as an entry-point to the application. To ensure this exists we need to tell Clojure and Leiningen to generate a full Java class for the core module. To do this we just add (:gen-class) to the namespace clause: Now we can ask Leiningen to construct our uberjar: The JAR we’re interested in is the -standalone one, which contains our application, all its dependencies, and Clojure itself. This can be run from the vanilla JVM, but before we do that we need to pass our configuration in via environment variables: (Obviously fill in the details above with your own values.) With the environment configured we can run the application, making it available on the internet via ngrok (which should still be running): We can test this by retrieving the descriptor with cURL : Now that we’ve got our add-on up, running and on the internet let’s get it into Bitbucket. Luckily Bitbucket makes it very easy to add custom add-ons to our personal accounts. All we need to do is tell it the URL; go to your account settings again and select Manage add-ons : Click on Install add-on from URL and enter the ngrok URL we generated earlier: This will then verify that you wish to grant permissions to the add-on. Confirm this and you will see our add-on in the list: Now you can go to any of your repositories in Bitbucket and you should see (after a brief loading time) the web panel we produced in the previous installment: Now that we have a running Connect add-on we can start on the real fun, which is improving it. In the next installment we’ll the Javascript we added in part 4 and introduce ClojureScript, an implementation of Clojure in Javascript. We’ll then use this to reimplement our client-side code in Clojure, and look into the project changes necessary to integrate this into our build-chain.", "date": "2016-01-21"},
{"website": "Atlassian", "title": "How Zephyr made their add-on lightning fast", "author": ["Shailesh Mangal"], "link": "https://blog.developer.atlassian.com/faster-add-ons-with-entity-properties/", "abstract": "A major part of building add-ons for JIRA is the ability to show [web fragments] as an integrated part of pages within JIRA. This allows your add-on, via Atlassian Connect , to seamlessly integrate with JIRA. Getting JIRA to know when the right time to show these web fragments turned out to be a more complex and impactful problem than we expected. You can conditionally load web fragments registered in your add-on’s descriptor with remote conditions . On page load, JIRA will ping the URL endpoint registered in the remote condition, looking for a JSON payload returned with shouldDisplay as true or false . This controls if the UI will display your web fragment on the page. It seemed like remote conditions could help us in dynamically controlling the display of our web fragments — but not quite. Every time JIRA loads a page with a web fragment, the remote condition is evaluated. We discovered that remote conditions impact the overall health and performance of our add-on because of how chatty they are. If our add-on is responding slowly, this in turns impacts the end user’s experience with JIRA. And if the add-on becomes unavailable, waiting for requests to time out on every page load is a painful experience for the end user. For each add-on with remote conditions an end user has installed, this problem is multiplied. For our add-on Zephyr For JIRA Cloud we have a few web fragments that show up on the ViewIssue page. We only want to show the fragment if the issue type is configured for it. To achieve this, we added a remote condition that checked the configuration stored in our add-on. JIRA invokes this remote condition with the current issue ID, which can be used to look up the issue details and discover the issue type. Then the remote condition can respond to the request with a true or false based upon the end user’s configuration for that issue type. JIRA then decides to show the fragments based upon that response. We soon realized that round trips are expensive. After we received the remote condition call from JIRA, we’d have to fetch the issue type from the issue and that caused even more latency. So, we started caching the issue responses. Caching solved the speed problem for previously viewed issues, but the latency persisted for new issues. As we added more customers, traffic kept increasing. We were adding roughly three calls over the network per issue rendered in JIRA, even the issue types that weren't of concern to us were getting impacted. Customers were complaining about our add-on dragging JIRA performance down. We reached out to the JIRA team who came out with conditions based on entity properties . We replaced our remote conditions with conditions based on entity properties. Conditions based on entity properties provide a much better alternative to remote conditions wherever applicable. Entity properties let add-ons store arbitrary data on JIRA entities and conditions can leverage this data within the product. This eliminates the need for a remote call and provides better performance to the end user. This also reduces the bandwidth and computational needs for hosting your add-on. In our case, we implemented the following steps: *Note: For issue type, entity properties you can only add to existing entities and not at the time of entity creation.* No remote call needed once entity properties implemented By switching to entity properties, we eliminated all remote condition calls and reduced traffic to Zephyr servers by almost 80% Remote conditions are powerful but they have a performance and scalability disadvantage. Entity properties along with conditions provide a compelling alternative. The transition to entity properties provided a significant savings in load and bandwidth. We recommend every add-on developer try to use entity properties before remote conditions.", "date": "2016-01-26"},
{"website": "Atlassian", "title": "Let’s build a Bitbucket add-on in Clojure! – Part 6: Finally, ClojureScript!", "author": ["Steve Smith"], "link": "https://blog.developer.atlassian.com/clojure-connect-part-6/", "abstract": "Back in part 4 of this series we introduced two methods of accessing Bitbucket from our Atlassian Connect add-on, including via the client-side (browser) JavaScript API . However as we’ve written all of our server-side code in Clojure so far, it’s a shame to have switch to another language. In this installment we’ll take a look at ClojureScript and how we can integrate it into our project. In part 4 we introduced accessing the Bitbucket API via the JavaScript API. This API uses the cross-domain messaging standard to open a communication channel between the parent (i.e. Bitbucket) page and the iFrame child page (i.e. embedded Connect add-on content). This API uses a special AP.require() call to have the parent page make asynchronous calls on behalf of the child iFrame. We provided some examples of using this API in part 4 of of this series: However as we’ve written our add-on in pure Clojure so far it seems a shame not go the whole way. Enter ClojureScript … ClojureScript is a version of Clojure that targets JavaScript and Google Closure . This allows developers to write a major subset of Clojure that will run in the browser. This brings a number of advantages such as immutable/persistent data structures , macros , and access to channel-based concurrency via core.async (the benefits of which we’ll get into in a later installment). So let’s port our JavaScript which interfaces with the Bitbucket API to ClojureScript… But the first thing we’ll want to do is restructure our project a little. By default Leiningen places all Clojure code into src/ . However we’ll now be working with two compilers (one for Clojure and another for ClojureScript), so it’s good practice to separate the two codebases, so we’ll move them into separate subdirectories: Now we have src/clojure and src/cljs ; we just need to tell Leiningen about this change in our project.clj : Now that we have a home for our ClojureScript module, let’s produce a straight port of Tim Pettersen’s example JavaScript : As you can see above, this looks like a mixture of Clojure and JavaScript. The first thing we do is import some of the Closure and Clojure utilities, and then define some functions to use these libraries to manipulate the DOM and set our values. Then we make two calls to the Bitbucket AP.require() API with callbacks to lookup the values. Which is to say our code functions almost exactly the same as the original. The only major difference is that we’re using Google’s Closure library to manipulate the DOM instead of jQuery. We put this code into src/cljs/hello_connect/core.cljs , and now we need to compile it. Naturally Leiningen has a plugin to help with this, so let’s add it to the :plugins section: We also need to add ClojureScript itself to the :dependencies section: (As always you can use lein ancient to keep these dependencies up to date.) Now we need to configure the compiler itself, which has some trade-offs and may require some explanation… The Google Closure compiler has a number of optimisation levels that have different effects on the code-base. The main two we’re interested in are :none , which is recommended for development, and :advanced which is recommended for production. However :none requires the developer to provide some dependencies manually which are not required in production (as :advanced does more packing and minimisation); this effectively means you must use slightly different HTML in dev, which is not a great idea. It is usually considered bad practice to make a lot of changes moving from dev to production. On the other hand the :advanced optimisations are really slow (11 second compilation time for our simple ClojureScript code). However like Clojure/Leiningen’s slow start-up time not being an issue in practice because we always leave the REPL running, in practice we don’t rebuild ClojureScript from scratch. While ClojureScript supports REPL development, another option is to use its incremental compilation. Basically all we need to do is run `lein cljsbuild auto` and not only will it compile the code but it will watch the source files and initiate a conditional compilation on change, which is much faster: And for more advanced development scenarios there is also Figwheel , which will also live-load ClojureScript into a running browser window. So we want to build our ClojureScript with :advanced optimisations, however there is one gotcha in doing that. :advanced is very aggressive and will mangle any function names, including functions that are external to our code; in this case the AP.require() / (.require js/AP) calls. There a couple of ways to fix this, but the least intrusive is to just declare any ‘protected’ symbols to the compiler. This is done by adding them to a file on the path, in our case under resources/AP-externs.js : This declares that the AP module and require() functions should not be mangled. Now we can give Leiningen the information it needs to compile our code in our project.clj : This compiles our code into a single file core.js if we issue `lein cljsbuild auto . We'd also like to include this code when building our uberjar`, so we add the output directory into the resources path so it gets merged with all our other HTML, CSS, etc: Of course, we also need to update our HTML to use our generated JavaScript. Open our template connect-example.selmer and change the line: to: The last thing we need to do is tell Leiningen that we would like our cljsbuild called as part of any builds it performs: And that’s it. We can now build and run our uberjar as we did in part 5, and we can see our ClojureScript running in the browsers. We’ve produced a lot of boilerplate code over these 6 parts of our tutorial. But boilerplate isn’t something that fits into the Clojure world; if something is repetitive then it should be automated away. In the next installments we’re going to have a look at some of the ways Clojure and its ecosystem enables this. The code for this part of the tutorial series is available in the part-6 tag in the accompanying hello-connect Bitbucket repository . There will also be code appearing there for the later parts as I work on them if you want to skip ahead.", "date": "2016-01-28"},
{"website": "Atlassian", "title": "Integrating with Jira or Confluence Cloud", "author": ["Robert Massaioli"], "link": "https://blog.developer.atlassian.com/integrating-with-atlassian/", "abstract": "Atlassian provides two standard frameworks to get up and running with Atlassian Cloud App development quickly. They are: However, the Atlassian App framework was explicitly designed to make it easy to integrate any web App with Atlassian Cloud, and this guide aims to show you how. You should be able to integrate your specific programming language (and web framework/library) with Atlassian Cloud. However, what do you need to do to integrate your web App with Jira and Confluence Cloud? This blog post is going to show you! In this tutorial, we are going to guide you through writing a “dynamic” Atlassian App because most web frameworks involve writing server-side code. Note : The definition of a static App is: An Atlassian App that only relies upon static assets that could be served by a CDN; like HTML, CSS, JavaScript and JSON files. Conversely, the definition of a dynamic Atlassian App is: Not a static app. It likely requires a server backend. It can also do work outside of the client’s browser. Static apps are awesome. They have many advantages: You can host your static resources with a static hosting provider, and your App runs in the user’s browser. One of the best features in the Atlassian App framework is the AP.request method. It lets a static app, running inside an App iframe, requests data or takes actions against the parent product, on behalf of the user that is currently viewing the app . You can read more about this in the AP.request documentation . Unfortunately, static apps can’t do everything. Here are some of the use cases that you might have that would cause you to want to write a dynamic Atlassian App: With a static app, you may be able to make REST calls to an external service that has a CORS-enabled API. However, there are many services out there on the web that you want to integrate with that don't provide such REST APIs. However, if you want to integrate your service with Atlassian Cloud via the REST APIs (without Webhooks or providing UI inside of Jira or Confluence) then Atlassian has introduced a new App feature that might be interesting to you: OAuth 2.0 authorization code grants (3LO) for apps . We don’t discuss this new feature further in this post. Many apps take data from the host product, operate on that data, and then return it to the user in a much more useable manner. Atlassian Connect apps that provide reports are an excellent example of typical behaviour. For example, instead of calculating a result from scratch in the user's browser often your App can be more performant by saving customer data in your own data structures and performing queries on that instead. These are just some of the factors that may push you towards deciding that your situation requires you to write a dynamic Atlassian app. You may find more reasons when you develop your own Atlassian Apps. With that in mind, you may have some questions: The following sections answer those questions. Short answer: Any programming language at all so long as you respond correctly to the Atlassian App interface. Any web service can behave like an Atlassian App. To prove that you can write an Atlassian App in any language look no further than the previous Atlassian Apps that I have written: The Atlassian Ecosystem team maintains a list of other languages and frameworks that are available for use. Please choose any framework or tool that best suits your needs when writing an Atlassian App. The question that naturally arises is: I want to write an Atlassian App in <web framework>. What code do I need to write so that it integrates seamlessly? The answer to that question is the subject matter of this post: You are required to handle the installed lifecycle flow so that you can save tenant-specific information. JWT authentication requires this information. [ Source code ] You need to validate incoming JWT tokens to ensure that iframe page load requests and webhook POSTs come from Atlassian Cloud and not somebody pretending to be Atlassian Cloud. All Atlassian Apps must load the all.js file in their iframes to set-up a bridge between your App and the host product. If you want to make an HTTP request back to the host product, then you need to sign the HTTP request with a JWT token that proves to Atlassian Cloud that you are an App it knows about and therefore should have access to customer data. In pictorial form, these steps look like this: If you implement a web framework so that it can handle these four stages, then you have effectively made your web framework behave like an Atlassian App. Writing code to accomplish the above four tasks is quite achievable. In this post, we punctuate descriptions with links to working code samples that implement the explained topic. Atlassian Cloud has thousands of customers (where a customer is a site like https://your-domain.atlassian.net ), and your App needs to be able to handle being installed in any (or all) of them. For example, 1000 Jira customers could choose to install your App, and you need to have a way to separate the data for each customer from each other cleanly. It is a privacy and security issue if one customer is ever able to see another customer’s data. For this post, we consider each unique “host product” (Jira or Confluence) that installs your App to be a “Tenant” of your app. When a customer installs your App into an Atlassian Cloud product, you receive an installed lifecycle event that contains a set of valuable information you need to save and be able to recall for future requests. The data structure that you use to store this information is your “Tenant model”. In the Atlassian Connect for Sprint Boot framework, we use the Spring Boot Persistence layer to enable you to store this data in a SQL or NoSQL database; the choice is yours. However, the data model that we store is the same. Please have a quick read through the Atlassian Host Tenant model definition . To learn more about this data structure you can read through the Lifecycle structure in the documentation . The most important detail of this Tenant model is that you should be able to look up the Tenant model data by clientKey . If you store this data in a relational database, then clientKey should be unique, and it should be an indexed column in your table. You can create your Tenant model in any way that you wish but you must have a tenant model, and you must be able to persist the data provided by your tenants. To authentication requests in the future, you need this information. In the previous section we talked about the installed event, but we did not talk about what triggers it or how to handle it. Let’s start with how the lifecycle event is triggered: In your App descriptor, you must declare that you require jwt authentication. If you don’t, then the lifecycle event will never be sent because we assume that you don’t need this information for future HTTP requests. [ Documentation ] When an Administrator installs your App your descriptor is read, the product provisions your modules and webhooks and, most importantly, your App is permitted to receive and post requests to that customer’s tenant. During that flow, Atlassian Cloud sends your App an HTTP POST that is the installed lifecycle event. Atlassian Cloud uses the url of your App that you declared in the lifecycle.installed field in your descriptor. [ Documentation ] When your App receives the lifecycle event, it must decide what to do with it. If it accepts the request, then it should respond with an HTTP 204. If it blocks the request, then it should return an HTTP error code, like an HTTP 403 Forbidden. If an App returns a success response in all cases, then it has been implemented incorrectly. However, how do you know if you should respond with an HTTP 204 or an HTTP error for any particular request? A better question might be: How could a third party attacker exploit this system to gain access to customer data? We want to block an attacker from getting access to customer data. Thus any installation attempt that could take an existing customers tenant and turn it into the attacker’s tenant should result in an error. In the Atlassian App framework, every time your App updates it receives a new installed event. Thus Apps must handle two general install scenarios: If the Atlassian host product thinks that it is installing your App on this Tenant for the first time, it does not set a JWT token on the installed HTTP Post request. Therefore, the presence of the JWT token is optional on the installed HTTP request. With every installed request you receive the lifecycle payload, which contains the clientKey field. Every Tenant that an App installs into has an Atlassian Cloud unique identifier called the clientKey . You then persist these clientKey s into your storage layer. You can see this if you use the Connect Inspector developer tool . Here is an example of an initial install (bottom) followed by a new install that is an update (top): In the initial install, you can see that there is no JWT token. Atlassian Cloud provides the clientKey and sharedSecret in the lifecycle body. In the update install, you can see that there is a signed JWT token present. Atlassian Cloud signs The JWT token with the sharedSecret from the last successful installed request. Also, the iss field set to the clientKey from the last successful installed request. Given that, we should be able to implement the following logic: Using the excellent code2flow app, we can convert this into a flowchart for you to implement: (P.S. code2flow has a Jira Server and Confluence Server App on the Atlassian Marketplace!) You can see that logic implemented in: With that information, you should be able to handle the installation of any Atlassian App in any programming language and any web framework. Your App is going to have incoming HTTP requests that will either ask to get data from your App (iframe loading) or try and give data to your App (Webhooks). You must validate that these HTTP requests originated from Atlassian Cloud and not some attacker. You can use the Tenant model data that you saved in the previous step to validate incoming HTTP requests from Atlassian Cloud products. You must perform JWT validation . The process, generally speaking, involves the following steps: The token issuer ( iss ) should match the client key of the host product that made the request. You should be able to extract it directly. You do not need to do any validation on this step. Using the client key as an identifier look up the matching tenant in your persistence storage. Perform this lookup so that you can get the sharedSecret for validation purposes. Hold on to the tenant. (If you can’t find a tenant for the clientKey then validation has already failed.) Now that you have the sharedSecret you can validate if the original token was legitimate. Important : This means that you need to be able to parse the JWT token and validate in separate steps. Make sure that your JWT library of choice can do that; most should be able to. It also means that you need to be able to validate the query string hash . That way you don't have to continually look up the tenant for the remainder of your request handling code. That is all that you need to do. If you encounter an error at any point in the time, then you can fire back the appropriate HTTP error code. If you want to see examples of this incoming HTTP request JWT validation, you can look at examples in: Incoming request validation is a common task so all of these frameworks have centralised this logic into common functions that can just be easily applied where needed. These examples should give you a good starting point for implementing similar login in your web framework of choice. The Atlassian App framework uses iframes to sandbox Apps when they provide user experiences inside the products. For example: When a page first loads with these iframes present, the iframes are not shown immediately. Instead, a spinner is shown in their place and does not disappear until the all.js file is loaded and run into the iframe. Including this snippet as the first element in the <head> ensures that all.js loads on the page before any other JavaScript executes: To learn more about AP and the all.js script, please read the documentation . Since this logic needs to run on every page in your Atlassian Cloud App, we highly recommend that you turn this into an HTML template/wrapper that you can include or use in every iframe. Now is probably a good time to note that, for frontend development, Atlassian has the Atlassian Design Guidelines . These Design Guidelines are implemented by Atlaskit and available for use in your App so that your App can be themed correctly and quickly. Atlaskit is delightfully easy to use and built on React. At this point, we can save tenants, verify incoming requests and create iframes that render inside an Atlassian host product. The missing part of this picture is our App server making requests to the host product. To solve that we need to prove to the host product that we are the App that installed into it; we need an authentication mechanism. JWT is the authentication mechanism required to talk from the App to the host product. The process of generating a valid JWT token is explained thoroughly in the documentation , but we can surmise some key points: The hardest part of generating the JWT token is providing the only part of the token that is not part of the vanilla JWT specification: the query string hash. The query string hash is used by Atlassian products to ensure that you can only use the JWT token to make an HTTP request to the exact URL that the JWT token was generated to access. We do this so that, if an attacker got hold of the JWT token, they could only make the request that the token was explicitly designed to make. Generating this token is tricky but the Atlassian documentation has an excellent guide on how to write a generator. There are also existing implementations that you contrast with your implementation. [Example implementation in Haskell] At the top level of your descriptor, you would have provided an App key. The App key must be used as the issuer (iss) when generating the JWT token. The shared secret for the host that you are trying to communicate with must be the shared secret that you use to sign the JWT token. Lucky for you, you have a Tenant model, so it should be easy to look up the shared secret for any given host product. With that in mind, you can look at these examples of signing outbound HTTP requests: However, each of these implementations is relatively simple, in essence, they: Congratulations, this is the final step required to integrate your web framework with the Atlassian App framework for Jira and Confluence. Making it through this entire guide is a huge achievement! Congratulations and well done! From here, I would encourage you, learned reader, to go and put this all into practice by listening to a Jira Issue, or Confluence page, update Webhook event in your App and then reacting to it by putting a comment on the issue or page respectively. Doing this forces you to exercise: This covers the majority of the steps that we ran through in this post and thus would be valuable to your new integration in a new web framework. We are looking to make changes in the future to simplify this integration story even further. We want it to be straightforward to integrate your web App with Atlassian Cloud. On that note, we can't wait to see what you build next, and we hope that this guide helps you build great Atlassian Cloud integrations. If you build one, then don't hesitate to let us know about it! We would love to see it.", "date": "2016-01-30"},
{"website": "Atlassian", "title": "How TestRail designed, built and deploys universal add-ons for JIRA Software Server & Cloud", "author": ["Dennis Gurock"], "link": "https://blog.developer.atlassian.com/universal-addons-for-jira-server-cloud/", "abstract": "More and more teams adopt and migrate to JIRA Software Cloud to benefit from the various advantages of Atlassian’s hosted platform, such as easier distributed access, great performance, less maintenance hassle, no upfront costs, and automated updates. JIRA Cloud also provides great opportunities for add-on developers, as it offers better control of the user experience, easier upgrades, faster deployments, choice of development platform as well as recurring revenue. At the same time there’s a huge existing JIRA Server customer base. If you are building a new tool or integration for JIRA, you ideally want to target both server and cloud customers to reach as many users as possible. So when we designed and planned our all-new JIRA test management integration for TestRail 5.0 , one of our design goals was therefor to offer the same rich integration options to JIRA Cloud and Server customers. To make things even more complicated, we also offer TestRail server and cloud editions to choose from. So how did we build the integration to work seamlessly with all possible integration combinations? We came up with a universal integration approach for both JIRA Server and Cloud. We noticed that the approach we designed works great for many kinds of tools, integrations and add-on ideas. So we decided to share our approach to help other developers overcome the same challenges we faced and I will explain the details of our approach in this article. Building a JIRA Cloud integration is pretty straightforward by using the Atlassian Connect platform. Especially for TestRail, but also for a lot of other application integrations and add-on ideas, Atlassian Connect is a great fit. Instead of trying to extend the internals of JIRA and rendering elements and pages with code that runs on the JIRA server, Atlassian Connect provides various clearly defined integration points to embed external pages and page elements in JIRA’s user interface (in addition to providing powerful APIs and web hooks to access and manipulate data stored in JIRA). So building a new Atlassian Connect add-on mainly consists of telling JIRA which page elements, full pages, and menu items you want to embed. And then building (or extending) an external application to render and provide these elements. When I say external application , this can either be a standalone application such as TestRail, which comes with its own user interface and provides various additional page elements to embed in JIRA. Or it can be an application that provides its full user interface through JIRA, ultimately hiding the fact from users that it’s a separate application at all. In both cases, elements integrated in JIRA will look and feel like native page elements rendered by JIRA, so users wouldn’t notice a difference to classic server plugins. In TestRail’s case, and this is true for many applications that provide a multi-tenant environment, customers can choose their own domain name for their application instance (for example, example.testrail.net ). Additionally, we also want to enable JIRA Cloud users to integrate with TestRail Server instances (and vice versa). So the Atlassian Connect add-on cannot just hard code the address of page elements to embed, but let the user configure the address of their TestRail instance to link to (see below). So in TestRail’s case, the Atlassian Connect add-on actually provides just a thin communication layer between JIRA and TestRail. It provides and stores all the configuration options on a per JIRA instance level and embeds the pages of the configured TestRail address. To implement this in a universal way, the Atlassian Connect add-on needs to render its own iframes and implement certain JavaScript functionality (e.g., to automatically resize the frame based on the content height as well as displaying dialogs). The result is a fast and universal integration that allows JIRA Cloud users to embed any TestRail cloud or server instance. When we looked at the various options to develop the same integration for JIRA Server (on-premise) instances, we first reviewed the typical way add-on developers would approach this: building a Plugins 2 add-on . It’s important to note that we only ever really considered a Plugins 2 add-on implementation for JIRA Server. Trying to use Atlassian Connect with JIRA Server has never been a good idea as it’s not officially supported. And Atlassian also announced that they don’t have plans to extend Atlassian Connect for local servers for now (which makes sense, as it’s just not a good fit). Developing a Plugins 2 add-on would involve rendering any page elements and implementing any custom logic via Java as part of the JIRA add-on. JIRA provides rich APIs and many add-on options to extend the application. In general, if Atlassian can use a certain feature or extend JIRA in a certain way, add-on developers can benefit from the same flexible customization options. For many add-ons, including our TestRail integration, displaying data also involves requesting the data from external sources first, usually via REST APIs. In TestRail’s case this would mean test results, details about test cases that are linked to JIRA issues, and the underlying metrics to render rich testing reports. Building a separate Plugins 2 add-on in addition to the Atlassian Connect add-on would create multiple issues. The biggest issue with this approach is the need to implement the full JIRA add-on features and interface twice. First as embeddable pages and elements that TestRail provides for the Atlassian Connect integration in JIRA Cloud. And then we would have to implement all the same functionality as part of Plugins 2 add-on for JIRA Server. We wanted to avoid implementing the entire integration twice. So instead of requesting and rendering all the data in the server add-on, why not embed the same page elements in JIRA Server that we embed with Atlassian Connect? What if we could reuse the same integration points, with small adjustments, instead of rebuilding everything? And that’s exactly what we did. We identified all relevant integration points and page elements and built a Plugins 2 add-on that uses iframes similar to Atlassian Connect to embed all relevant TestRail page elements. Atlassian Connect provides various libraries and conventions to make it easier to embed external pages and page elements so they look and behave similar to page elements rendered by JIRA. For example, iframes aren’t usually resized automatically based on the embedded content size. Likewise, accessing methods to show dialogs or messages isn’t directly possible from referenced pages due to browsers’ same-origin policy . In order to emulate Atlassian Connect’s behavior, we had to build a similar thin JavaScript layer to automatically resize iframes, show dialogs, or access other JavaScript methods of the host. Implementing (and securing) this via JavaScript messaging is very straightforward, and just a small price to pay compared to implementing the entire integration twice. Deploying both the cloud and server add-ons, as well as configuring the integration, is very different for both platforms. For JIRA Software Cloud our Atlassian Connect add-on, (which is basically a standalone application that sits between TestRail and JIRA) runs on our highly available AWS-based infrastructure to render the add-on configuration page for each JIRA instance. Whenever a JIRA Cloud instance embeds a page element from our add-on, the add-on looks up the add-on configuration for the particular JIRA instance and returns a simple page with another iframe to include the actual TestRail pages (and passes relevant configuration and security parameters). By using this approach, even TestRail Server instances behind a firewall can be easily embedded as only the user’s browser needs to be able to reach both JIRA and TestRail. For JIRA Server instances and our Plugins 2 add-on we developed and render pretty much the exact same configuration page as part of the add-on code. The configuration page also allows JIRA administrators to configure their TestRail web address so the add-on knows the page elements it needs to embed. In case of the Plugins 2 add-on, the add-on can directly render the iframes for the relevant page elements. For example, to embed test results for a JIRA issue on the issue page, the add-on registers the relevant web panel and injects an iframe with the external page reference. TestRail returns the same page elements regardless of which JIRA edition is embedding the pages, making it very easy to extend and improve embedded page elements in future updates. One big advantage of this approach is that we can update and improve the TestRail integration for JIRA without the need to deploy new add-on versions at all. Unless we need to add new integration points for the add-ons or change the configuration pages, we only have to update the embedded pages and page elements on TestRail’s side. If we have to actually build and deploy new add-on versions, we can simply deploy a new version of our Atlassian Connect add-on on our servers and upload a new build of the Plugins 2 add-on to the Atlassian Marketplace. After reviewing the general approach on building universal add-ons for both JIRA server and cloud, let’s review some quick tips and recommendations that make it easier to design and develop such add-ons. One important thing we figured out quickly is to start with a simple foundation for both add-ons and gradually add new features to both add-on editions in parallel. It’s much easier to start with a minimally viable add-on for both editions and extending the functionality as you go. By building both add-ons in parallel you can identify potential limitations of the platform earlier and can make design changes faster. Another important lesson we learned is that if you can avoid requiring a direct connection between JIRA and your particular integration, application, or external data source, it makes the integration across editions and servers much easier and more robust. For TestRail’s JIRA add-ons, no direct connection between TestRail and JIRA is required. This means that no firewall, NAT, or security rules can affect the functionality of our add-on. If you need to access JIRA’s REST API to store important configuration settings or update conditions, accessing the API via JavaScript in the context of the JIRA user can make things much easier. Last but not least, it’s easy to fall into the common trap of using your application’s existing stylesheets and code to render elements and data inside JIRA. Don’t do this — it will result in a sub-par user experience inside JIRA, as different sections and elements will look and behave differently. Instead, use the AUI library for both page elements embedded from your application as well as the page elements rendered directly by the add-on (i.e., the configuration pages) so your integration has the same look and feel as the rest of JIRA. Building and designing universal add-ons for JIRA can make it much easier and less time-consuming to target both JIRA Software Cloud and Server customers at the same time with a common code base. If you are interested in learning more about TestRail, our JIRA integration, and how it’s used by teams to boost their testing efforts, make sure to give TestRail a try .", "date": "2016-02-09"},
{"website": "Atlassian", "title": "Bitbucket Connect Add-on in Go", "author": ["Nicola Paolucci"], "link": "https://blog.developer.atlassian.com/bitbucket-connect-in-golang/", "abstract": "In the past months we’ve been showing how to extend and integrate with Bitbucket using different languages and technology stacks. For example check out Steve Smith’s series on using Clojure . Today is Go ‘s turn. The focus of this piece will be creating a minimal Go add-on for Bitbucket using Atlassian Connect . I love Go’s essential and pragmatic design. I’ll try to keep things essential also in the design of this very simple add-on. The screenshot below shows what we’ll build: a simple embedded panel that – once installed – will show in the overview page of any of our repositories. This can be the base of more complex and useful (!) developments. If you are not familiar with the architecture of a third party application that wants to integrate with Bitbucket, have a look at this diagram: Steve has brilliantly defined what it means to build a Connect add-on: a Connect add-on is a web application written in any language you like, running on any stack you choose, in any location you want. Once registered with the Atlassian application your web app can progressively enhance ours with new screens, features and functions that appear directly embedded as if a part of our cloud. It can also enhance our app with new behind-the-scenes logic, all via REST APIs and Web-hooks. All of Connect’s components consist of standard web protocols and conventions, and, apart from a minor extension to the JWT spec, are supported by most languages and libraries out of the box. This is part of the power of Connect, and what gives it a true cross-platform experience. With the objectives and the definitions out of the way let’s show some code. The source of the entire project is available on Bitbucket. The bulk of it is less than one hundred fifty lines with comments (with a couple of added helper functions for debugging) so we can easily just go through it and explain. Our only convenience dependency is the tiny and awesome mux router to simplify creating HTTP Handlers, but we could’ve done without it. The main of our application is short: we parse some optional command line parameters, inject some environment variables – which are either secret and should not be hard-coded or change per deployment environment -, initialize the application Context , and start listening on a port. We create all the routes of our add-on in a simple method, deferring each to a handler function attached to the Context : Bitbucket will invoke the above routes at various points during the life cycle of the add-on, we define the actual URLs in the atlassian-connect.json descriptor. In line with the common Go context pattern , we use a Context to keep relevant data available to the web application while we process requests: We need to define the baseUrl of our add-on as it will be used by the Connect framework as base to invoke into us. consumerKey is a unique key that Bitbucket has assigned to our add-on (you can generate it in your Bitbucket settings, see later for screenshot). We also need a structure to store meta-data and security information for each “installation” of our add-on on different user accounts, let’s call it TenantInfo : When I started I didn’t know the full format of the tenant information: I used a trick to generate that neat structure: I dumped the JSON received from the /installed callback – which is called by Bitbucket when the add-on is first installed – into the fabulous JSON-to-Go to auto-generate a suitable Go struct. What you see above is the result, barring compressing a couple of fields to interface{} to shorten the code snippet. Responding to the various requests coming from Bitbucket is straightforward, we just need to write some standard handler functions. Let’s go through them: The handler above serves a template of the atlassian-connect.json descriptor, interpolating the proper baseUrl and consumerKey using the standard html/template Go library. The installed handler is invoked when the add-on is first installed by a user, it contains important values like the OAuth keys needed to authenticate requests coming from Bitbucket and also for the add-on to authenticate itself with Bitbucket when it needs to access the REST API. Skeleton handler invoked when the add-on is uninstalled. As we’re not storing any data in this sample add-on, I leave it stubbed. The example handler serves the HTML content that will be displayed in the web panel inside Bitbucket’s project overview page. The page is served an interpolated template so that we can populate the HTML dynamically with parameters, for example the repository path we received when Bitbucket called us. Here’s how the template looks like: We mentioned before a couple of times the Atlassian Connect descriptor atlassian-connect.json so let’s review it together. It is the main configuration file our application feeds to Bitbucket to provide information about the add-on and certify that we are authorized to be embedded. We serve it as template because we want to inject into it a few parameters like our LocalBaseUrl and the authenticated ConsumerKey : As you can see we specify some basic information about our add-on, like what’s the base URL of the add-on (i.e. the URL where it’s hosted), which URL should Bitbucket invoke when the add-on is installed by a user ( /installed ) and so on. Running the add-on from the command line is straight forward, provided we remember to export a couple of environment variables. For more information and examples have a look at Atlassian Connect for Bitbucket . Also feel free to review the source of the project and let me know your thoughts at @durdn or my awesome team at @atlassiandev .", "date": "2016-02-11"},
{"website": "Atlassian", "title": "How to connect Bamboo and Bitbucket Server", "author": ["Peter Van de Voorde"], "link": "https://blog.developer.atlassian.com/totw-connecting-bamboo-and-bitbucket-server/", "abstract": "We’ll start with a clean install of Bamboo and Bitbucket Server. You can download them here: And simply follow the install instructions: For demo and testing purposes the embedded database install is perfect. Be aware that you might need to install certain prerequisites first! Once you’ve installed both the tools we need to connect them, for this we need to set-up an application link. Take a look at Integrating Bamboo with Bitbucket Server to see how you can set this up. Now you can import a test project into your Bitbucket Server. I choose the run-bucket-run project from my colleague Tim Pettersen which is hosted on Bitbucket Cloud. Use these instructions to clone the repo to your local instance: Tip of the Week – How to move a full Git repository . Once this has been done we can create a Bamboo build plan for our Bitbucket repo. Go to Bamboo start creating a new plan, here is my configuration: Due to the fact that Bitbucket Server and Bamboo are linked I can immediately select my repository. The next step is adding jobs and tasks to the build plan. For the purpose of this example I simply do a source code checkout: Before clicking on create be sure to activate your build plan! Immediately a first build will be triggered and displayed in Bamboo: This build plan will be triggered every time you commit something to the repository. You will also see the build result on each commit in Bitbucket Server. And be able click through to the detailed build results in Bamboo: And you will be able to see all commits in a build inside Bamboo. Clicking it will instantly bring you to the relevant commit or diff in BitBucket Server.: This is only the beginning! Let me show you what Plan Branches can do for you. Configuring plan branches is as easy as going to your build plan configuration, go to the branches tab and selecting Create plan branches for all new branches in the dropdown next to the label New Branches : Now Bamboo will create a plan for each new branch it will detect in your repository. This build plan will be a copy of your original build plan but will have it’s own logs, artifacts and triggers. Out of the box it will be triggered by a commit on the branch itself. So let’s create a new branch feature-totw to test this out. Here is how Bamboo looks like after we’ve created the new Branch: As you can see we can now select 2 branches (Master and feature-totw), this is what we see when feature-totw is selected: It’s a complete build plan overview for this new branch. If you have several branches at the same time you can even get a nice overview off how their builds are going in Bamboo: This is very cool if you are working with a big team and using feature branches, you can immediately see which features are breaking their builds. And this is not everything plan branches can do for you. If have been paying attention you would also have noticed the Merging title on the plan branches configuration tab. This option will give you the possibility to either update your branch with the latest commits from another branch ( Branch Updater ) or push your changes to another branch ( GateKeeper ). Both of these actions will happen after a successful build. These provide you with some extra tools to make working with feature branches so much easier. Simply use Branch Updater to keep your feature branch up-to-date with the latest changes on Master and you will never have to worry about suddenly breaking a build when you merge your feature into Master. Here is the result of Branch Updater in Bamboo: And here is what you see in Bitbucket Server: So this is all Bamboo, but what about Bitbucket Server? You already saw the build result that are available in Bitbucket Server and the automatic merges created by Bamboo. But what else is there? You can use successful builds as a requirement in Pull Requests! Simply go to your repository settings and select the Pull requests menu item. Here you can set the number of successful builds a pull request needs before it can be merged. And if you try to merge a pull request before you have to right number of successful builds you’ll see this: So this helps you again to be absolutely sure that anything you will merge into Master will not break your Master Build and will have no impact on your releases. Share your own tips and tricks in the comments or on Twitter, be sure to mention me: @pvdevoor or @atlassiandev !", "date": "2016-02-16"},
{"website": "Atlassian", "title": "6 Bitbucket secrets", "author": ["Tim Pettersen"], "link": "https://blog.developer.atlassian.com/6-secret-bitbucket-features/", "abstract": "At Atlassian, one of our design principles is to gracefully reveal depth . As we’ve iterated on our UX, certain Bitbucket power user features that strayed too far from the happy path have been hidden away behind a dropdown or keyboard shortcut. There they bide their time until an adventurous user stumbles upon it through a capricious key press or mouse click (or someone reads the docs). Here’s six of my favourite Bitbucket Cloud features, that you’ve possibly never heard of: Bitbucket ‘s omnibar is a quick actions bar, similar to Jetbrains’ ⇧⇧ or Sublime Text’s ⌘+P . From any page, you can launch the omnibar by pressing the . key. By default, it shows you a set of context-sensitive actions: Once you start typing, it matches on the names of repositories owned by you and your team: Plus the titles of your issues and pull requests (across all of your repositories): And even filenames within the current repository: If you happen to be a JIRA user as well, try hitting . the next time you’re viewing a JIRA issue. Using . to trigger the omnibar is just one of the many keyboard shortcuts available. You can hit ⇧+? on any page to see a context-sensitive list of shortcuts. Some of the ones we use daily on the Bitbucket team are: The modifiers for keys vary by browser and platform — the above are Chrome on OS X — so bring up the shortcut list ( ⇧+? ) in your own browser to check! You might’ve used Bitbucket’s Snippets to share code in the past, but you might not be aware that you can also share other types of files with them. Just drag any file onto the Snippet: This can include binary files like images, videos, or even compiled code! There is a limit of 10 MB per file. There’s also a handy command-line tool for uploading files from your shell. The repository downloads page lets you download an archive of your code at a particular commit with your choice of compression ( zip , tar.gz or tar.bz2 ): The URL that backs these links looks something like: bitbucket.org/atlassian/atlassian-connect-express/get/v1.0.4.zip Or more generally: bitbucket.org/<repo_owner>/<repo_name>/get/<revision>.<zip|tar.gz|tar.bz2> It turns out that you can replace <revision> with any non-ambiguous commit identifier. So you could download a commit by specifying its short SHA-1: bitbucket.org/.../get/badc0de.zip Or use ancestry references to find, for example, the second great-grandparent of the tip of your master branch: bitbucket.org/.../get/master^2~2.zip This can be quite handy for continuous integration and deployment scripts that need to download a snapshot of your repository at a particular commit. There are all sorts of cool ways to specify commits in Git , and the /get end-point works for Mercurial repositories too! Last June we launched the Bitbucket Connect framework, which allows you to augment the Bitbucket UI. You may have used some of the third party developer tools that have since launched Bitbucket integrations using the framework. However you might not be aware that you can use Bitbucket Connect to customize your own Bitbucket experience. For example, this hunk of JSON is actually a freestanding Bitbucket Connect add-on: Once installed, a Download as ZIP link is added to the Bitbucket commit page, using the /get/<revision>.zip end-point we looked at above: The add-on is hosted as a Snippet and can be installed using this button: <a class=”aui-button aui-button-primary install-button” href=” https://bitbucket.org/site/addons/authorize?descriptor_uri=https://bitbucket.org/!api/2.0/snippets/tpettersen/78Ajj/master/files/connect.json&redirect_uri=https://bitbucket.org/snippets/tpettersen/78Ajj “> Install Download as Zip You can use this technique to add links to your repositories, build visualizations for your code , or whole new features on top of Bitbucket. For more information on building Bitbucket add-ons, check out the developer docs or this 30 minute video tutorial . Or feel free to bounce ideas off me . If you’ve read this far, you’re likely one of those special types who enjoy living on the bleeding edge. If that sounds like you, head to Bitbucket Settings and click Manage features in the left hand menu to enable some pre-release features. This week, a slick new pull request experience awaits! If early adoption is your thing, make sure you check back from time to time as the Bitbucket team often put out teasers and betas well ahead of the official release. Thanks for reading! If you have your own Bitbucket secrets you’d like to share — or a nifty idea for a Bitbucket add-on — please leave a comment below or hit me up on Twitter! I’m @kannonboy .", "date": "2016-02-16"},
{"website": "Atlassian", "title": "Doing the Bitbucket OAuth dance with Python", "author": ["Ian Buchanan"], "link": "https://blog.developer.atlassian.com/bitbucket-oauth-with-python/", "abstract": "In the world of REST APIs, it is no longer necessary to rely on a service to provide an official SDK. While API wrappers may add some semantic sugar, it is often just as easy to use simple HTTP client libraries. In Python, Requests is often all you need to make use of the Bitbucket Cloud REST API . Just look at how simple this code is: Works great! Unless, you’ve enabled two-factor authentication (and I hope you have). When you have enabled two-factor authentication, then you are no longer allowed to access Bitbucket’s API with Basic Authentication . Instead, you have to use OAuth . With the additional security comes a little more code. Let’s start by adding an OAuth consumer in Bitbucket. First, find Bitbucket Settings under your account. Then, find OAuth under Access management . Toward the bottom of the page, you’ll find the button for Add consumer . On this page, enter a human-readable name and description. In time, you may have many OAuth consumer entries, so the name and description are mainly for you to remember what they are. Here’s a consumer I created. Bitbucket supports both OAuth 1.0 and 2.0. For this example, we’ll use 2.0 so we must provide a callback URL. We’ll provide https://localhost/ even though there isn’t any web application. Also for this example, we’ll enable the Account Read permission. If you are following along, you can come back to edit the consumer if you decide to grant your code more permissions. After we click the Save button, we can click on the name of the new consumer to reveal a key and secret. We will need these in the client code so we’ll leave the browser open to keep them handy. Fortunately, Requests already has an extension for OAuth support, called requests_oauthlib . I prefer to install libraries into Python virtual environments . In any case, we can install the library with pip install requests_oauthlib . For Bitbucket, the version of this library is important. As of writing, the latest version (v0.6.0) is required because it contains my patch to make it work with Bitbucket. Next, you’ll want to grab the following snippet : To run this example, you’ll need to replace the client_id string with the key you obtained from creating an OAuth consumer in Bitbucket. And the client_secret with the secret. When we run the code, we are prompted to go to a URL to approve. (If you use try to use the exact link shown above, it won’t match the client_id and state provided by any real consumer so it won’t work.) Once you click the approve button, your browser will redirect to localhost with a query parameter. Unless you are actually running a web application on your machine, you should just get a simple 404 error from your browser. That’s expected. Just copy the URL from the browser and paste it back to your command-line program. It might look something like this: That was the OAuth dance, so the code just spits out some JSON for a normal REST API request. I wish. If you were writing a command-line application, there are better grant types than the default one for requests_oauthlib . That flow is the first one in the OAuth 2.0 specification, known as Authorization Code Grant . It is the most popular implementation on servers, which means it is the one flow surely supported by every OAuth library or tool. To set a baseline with a new library or tool, I always start with this flow. However, popularity doesn’t mean the authorization code grant flow is good for command-line applications. Although we saw how it can work, all the copy paste stuff is error-prone and tedious. A better grant flow would be Resource Owner Password Credentials Grant . In requests_oauthlib , you’ll want to learn more about the legacy application flow and the LegacyApplicationClient . On the other hand, if you are writing a web application, you will replace those 2 awkward copy-paste steps need with real web interactions. The first, where users authorize, can just be a web redirect. The second, where users return with an authorization code, should redirect to a real URL within your application, not localhost . Also, this contrived example only made a single HTTP request. If our code is going to make multiple requests over time, we would need to account for the fact that OAuth tokens time-out and need to be refreshed. In requests_oauthlib , you’ll want to learn more about refreshing tokens . I hope you find that a useful starting point. If you have questions or need other help working with the Bitbucket API in Python, tweet me at @devpartisan or my team at @atlassiandev .", "date": "2016-02-18"},
{"website": "Atlassian", "title": "How to connect Bamboo and Bitbucket Cloud", "author": ["Peter Van de Voorde"], "link": "https://blog.developer.atlassian.com/totw-connecting-bamboo-and-bitbucket-cloud/", "abstract": "Start with a clean install of Atlassian Bamboo . Simply follow the Bamboo installation instructions . For demo and testing purposes the embedded database install is perfect. Be aware that you might need to install certain prerequisites first! And if you don’t yet have a Bitbucket Cloud account you can sign up here for a free account. If you don’t yet have a repository you can use these instructions to clone a repo: How to move a full Git repository . Or you can choose to fork a repository that’s available to you. Now connect Bamboo to Bitbucket Cloud. Take a look at Integrating Bamboo with Bitbucket Cloud to see how you can set this up. Here I’m setting up my Bitbucket Cloud repository in Bamboo: Go to Bamboo and start creating a new plan, here is my configuration: Due to the fact that Bitbucket Cloud and Bamboo are linked I can immediately select my repository. The next step is adding jobs and tasks to the build plan. For the purpose of this example I simply do a source code checkout: Before clicking on create be sure to activate your build plan! Immediately a first build will be triggered and displayed in Bamboo: At this time the build plan will be polling Bitbucket Cloud for changes every 180 seconds. If you want to let commits in Bitbucket Cloud trigger builds you’ll need to setup the Bamboo Hook for your repository. And you’ll have to change your build plan triggers . Now the build plan will be triggered every time you commit something to the repository. Be aware that this will only work when your Bamboo instance is reachable from the cloud! You will also see the build result on each commit in Bitbucket Cloud. And be able click through to the detailed build results in Bamboo: This is only the beginning! Let me show you what plan branches can do for you. Plan branches are used to represent a branch in your version control repository, with the plan branch using the same build configuration as your plan. Configuring plan branches is as easy as going to your build plan configuration, go to the branches tab and selecting Create plan branches for all new branches in the dropdown next to the label New Branches : Now Bamboo will create a plan for each new branch it will detect in your repository. This build plan will be a copy of your original build plan but will have it’s own logs, artifacts and triggers. Out of the box it will be triggered by the same triggers as your original build plan. So let’s create a new branch feature-totw to test this out. Here is how Bamboo looks like after we’ve created the new branch: You can now select 2 branches (Master and feature-totw), this is what you see when feature-totw is selected: It’s a complete build plan overview for this new branch. If you have several branches at the same time you can even get a nice overview off how their builds are going in Bamboo: This is very cool if you are working with a big team and using feature branches, you can immediately see which features are breaking their builds. And this is not everything plan branches can do for you. If have been paying attention you would also have noticed the Merging title on the plan branches configuration tab. This option will give you the possibility to either update your branch with the latest commits from another branch ( Branch Updater ) or push your changes to another branch ( GateKeeper ). Both of these actions will happen after a successful build. These provide you with some extra tools to make working with feature branches so much easier. Simply use Branch Updater to keep your feature branch up-to-date with the latest changes on Master and you will never have to worry about suddenly breaking a build when you merge your feature into Master. Here is the result of Branch Updater in Bamboo: And here is what you see in Bitbucket Cloud: So this is all Bamboo, but what about Bitbucket Cloud? You already saw the build result that are available in Bitbucket Cloud and the automatic merges created by Bamboo. But what else is there? Pull Requests will contain build information: So this helps you to be sure that anything you will merge into Master will not break your Master Build and will have no impact on your releases. Share your own tips and tricks in the comments or on Twitter, be sure to mention me: @pvdevoor or @atlassiandev !", "date": "2016-02-29"},
{"website": "Atlassian", "title": "Hosting a lean Atlassian Connect add-on for under $40 a month", "author": ["Daniel Wester"], "link": "https://blog.developer.atlassian.com/hosting-connect-addon-for-under-40-month/", "abstract": "At Wittified, all of our products rely on one of a couple add-on development frameworks provided by Atlassian. Many of them were built using Atlassian’s long-standing Plugins 2 (P2) framework. These are server based add-ons, because they run locally on the customer’s server. As an add-on vendor, that’s one of the initial benefits about P2 add-ons: wherever the customer elects to host their Atlassian tools, that’s where all their P2 add-ons are hosted as well. Seems like an obvious benefit. An easy choice for the developer, right? Well, not exactly. Since P2 add-ons run on the customer’s server, add-on developers usually have very little access to real time insights. This can make it challenging to quickly address small issues before they become larger problems. It can also mean a whole lot of environment-specific troubleshooting when there are problems. In other words, by avoiding an investment in hosting you may pay a whole lot more in terms of support. Enter Atlassian’s latest development framework: Atlassian Connect . Atlassian Connect resolves many of the challenges around real time insight by giving developers the ability to host their own add-on functionality. Rather than rely on each customer’s server, the add-on’s core logic is hosted centrally by the developer, and the communication with Atlassian’s tools (JIRA, Confluence, etc.) is handled through HTTP calls. It’s a fundamentally different way of building add-ons, and if it doesn’t sound exciting, it should. That’s because it can provide a whole lot more control over your code and it can make supporting your add-ons a heck of a lot more efficient. These big benefits require some investment. But is the investment really that big? Some developers may overlook Connect because of this hosting requirement. They may think “Can I really host my own add-on without breaking the bank?” or “Can I sell a cost effective add-on for Atlassian Cloud and still make a profit?” By the end of this post not only do I plan to convince you that you can, but that you should ! At first glance, Heroku, Open Shift, or other PaaS (Platform as a Service) solutions may seem like a no-brainer to quickly get started. A common benefit of PaaS services is that they can manage the full platform for you. They also offer a compelling guarantee: that they’ll keep the service up and running for you. But like most things, there’s another important (and often costly) trade-off. That guarantee can quickly turn into an expensive proposition once your add-on goes into production and you need to start scaling. You grow some customers, the platform grows with you. Great. But remember, the cost of that guarantee grows very quickly as well. In my opinion, often much faster than necessary! Looking closer, there can be other unforeseen costs as well. One of the ways PaaS services are able to get you up and running so fast is by requiring that you adopt their standards and conventions. This means that moving from one service to another can cause some serious conversion headaches beyond the additional cost of the next service. On the other side of the spectrum are IaaS (Infrastructure as a Service) solutions like Amazon, DigitalOcean, Rackspace, Linode, and others. If you’re willing to apply a little bit of effort to stand up your own server and maintain it, this is where you can save a bunch of money ! So for the purpose of this article, that’s exactly what we’re going to do. Before we continue though, a quick disclaimer. This write up describes just one way of doing things. It’s definitely not meant as an “end all, be all”, or a “one size fits all” by any means. As with any endeavor, always do your own research before implementing anything new (especially when it comes to security). For the purpose of this post, we’ve selected DigitalOcean as our service provider. (However, we could have just as easily used Linode, Amazon, or Rackspace. The instructions below would be pretty much the same.) We’re going to set up a very basic Atlassian Connect application using a Node.js application and the Atlassian Connect Express framework. If you’d like to follow along, first you’ll need to create a DigitalOcean account. Once you’ve created your account, you can upload an SSH key to streamline further password requirements. (We’re already using SSH keys when using Bitbucket, right?) Next we’ll want to create a basic artifact to upload. For the purpose of this post, you can grab a very basic Atlassian Connect Express (ACE) application over here . (It’s literally just a basic ACE application without any modifications.) In order to prepare it for installation somewhere, we have a basic deploy.sh in the root: When the deploy.sh is triggered with both an application name and a numerical ID (such as a build number) the output is a versioned tar.gz file, which can be sent off to a server (or two) and then unpackaged there. We’ll now need to create a basic server called a “droplet” (in DigitalOcean’s vocabulary). Click on the Create Droplet link at the top of the DigitalOcean control screen and you should see the following options: We’ll need to give the droplet a hostname. Since we’re focused on the Node.js application in this case, we’ll add a suffix of -app1. Select the $5 host option (1 cpu and 512MB memory) and pick any of the New York locations. For context, the Atlassian Cloud host applications are currently up in the US north east (as of the writing of this article at least) so that would theoretically be the closest spot. Next, select the distribution you would like. For this post, we’ll select Ubuntu 14.04 . (Note that the rest of this post is based on our selection of Ubuntu 14.04, so if you pick anything else you’ll just need to convert the commands and examples below.) Make sure to enable Private Networking. That way you won’t have to go outside the datacenter, unless you absolutely need to talk to your other servers. Note that we’re not going to create the database just yet, so we won’t enable backups at this stage. Finally, click Create Droplet and wait. (Depending on your variables, creating a new droplet can take 30 seconds or more.) Let’s go ahead and SSH into the server to start setting it up. One of the easiest ways to install Node.js on Ubuntu is by using the apt-get command. However, it’s important to note that you may get an older version. For this example, we’re going to do it that way, though I would recommend reading this article if you would like more control. In addition, we’re going to install the excellent PM2 Node.js library which will keep things running for us. Ok, now we can start configuring things. First, we’ll stop running as root. (Never, ever, ever run an application as root !) Let’s create a nodejs user that will run our application for us. This directory structure allows us to keep our released versions in /opt/apps/releases and establish an active symlink with /opt/apps/active . We’ll place all of our utility scripts in /opt/apps/bin – the first of whichis activate.sh . Next, we’ll need to make this executable: Now let’s start up the app! Go ahead and upload the tarball from earlier into /opt/apps/deploys . Switch to the nodejs user with: Then execute the following: You should see things being untarred and then end with: `[PM2][ERROR] Process my-app not found` Let’s now configure the environment variables that the application will need in order to run. In this case we’ll set the environment to be DEVELOPMENT . (If there are any other similar environment variables you’d like to set, this would be a good time to do so as well.) The public-key and private-key are your RSA keys generated specifically for this add-on by utilities such as JSEncrypt . Then start up the application with: You should be presented with something like this: You should also be able to access the application on port 3000. Before we finish with the application server though, there’s one more thing to do there. We need to make sure it always starts! Go ahead and execute the following as root: Now whenever the server dies, PM2 will restart. PM2 will then restart your application whenever it dies. Good stuff! We still need something very important. We’re creating an Atlassian Connect add-on and that requires HTTPS . In order to do that we need a SSL proxy. My personal preference is NGINX . Let’s head back over to DigitalOcean’s [droplet creation UI]( https://cloud.digitalocean.com/ ) and create another basic droplet for $5 a month (512MB, 1 CPU). Once that’s been created we’ll install the web server. Edit the /etc/nginx/sites-enabled/default and change it to the following, replacing ip-from-app-server with the ip address of the application server we created earlier: We’re now running on port 80, but we still need to add in SSL. Luckily there’s an open effort that’s growing in popularity called Let’s Encrypt that provides free certificates. This will be perfect for our example, though later on you may still want to get a traditional certificate. This page should provide you with the necessary instructions to follow. Once that’s in place, just restart NGINX and we’re almost done. Now we just need to button up our DNS. Luckily Amazon Route 53 is really cost effective and easy to use. (There are some other great options out there though, so feel free to shop around.) Depending on your Time To Live (TTL) requirements and how many users you have, for a basic application you’re probably looking at less than $5 (though your final cost may vary a bit). Not bad! For this example, let’s head on over to Amazon Route 53 and create an A record for the IP address of the webserver with your desired DNS entry. We’re now almost done with the application set up. We just need to store the data somewhere. Time to head back to our trusty DigitalOcean account to create another $5 droplet. This time let’s setup Postgres on it. Next, let’s create the database we want and give it the necessary permissions so that our Node.js application can use it. You’ll also need to add your application server’s IP address to /etc/postgresql/9.3/main/pg_hba.conf so that the application has a chance to authenticate. Then, head back to the application server and update your Node.js application to use the new credentials. Ok, so far we’re spending about $20 per month. Wait? That’s all? Almost. We’re on our feet, but we don’t have any real safety nets in place just yet. Thankfully, that’s a really easy one to resolve! First, we’ll need to shutdown both the application and the web server using the regular linux command: sudo shutdown -h now Then, we’ll head into DigitalOcean’s droplet interface. Within each of the droplets (database, web and application server) select the option to take a “snapshot” of each of them. This will provide you with an image of each. Once that’s done the servers will come back online again automatically. Next, let’s double down on our capacity. Within DigitalOcean, go ahead and create a few more droplets for app2, webserver2, and db2 – but for each of these we’re just going to use the snapshots that we just created, instead of just plain Ubuntu. Once those have been created, we’ll need to do the following: Go ahead and SSH into the webserver and edit the /etc/nginx/sites-enabled/default file. We’re going to add the IP address of the application server right underneath where it says: *server ip-from-app-server:3000;*. The result should look like the following, making sure ip-from-app-server and ip-from-app-server2 are replaced with the appropriate numbers: Then, restart the web server service on this new server by executing `service nginx restart`, and repeat this for the original web server that we previously created. This is a good point to take a new snapshot of the server as well. Next, head over to your DNS provider and just add the new IP to the same record. As a final safety net, we’ll set up the database in an active/standby configuration. This article from Digital Ocean provides a good tutorial. Next we’ll make sure that we can restrict access. For this we’ll use UFW so that we can set up a very basic firewall on each server. Note that you may want to look into additional security provisions to fit your individual needs before heading into production. To configure UFW , we’ll just need to SSH into each of our three different machine types and enable the next layer to access it, in addition to the SSH port (so we can access it): Now we’re good to go! We’ve defined specific IPs that can access specific ports and things are becoming more secure. For a deeper dive into UFW, check out this article . To keep going from here, a great next step would be to look into VPN solutions in order to secure your SSH access. If for some reason one of your web servers dies, Amazon Route 53 will automatically redirect traffic to the other web server. If one of the application servers dies, the NGINX layer will redirect the necessary traffic as well. And if application itself dies on a server, then PM2 will take care of restarting it. Pretty awesome stuff!! 🙂 What’s even better is that if you get a huge amount of load, you can easily add capacity. And do so at an extremely reasonable price. It’s by far the best of both worlds in my opinion! Now you have complete control over your application and environment variables, access to real time information, and you can track down user issues a heck of a lot faster within a centralized location. All for under $40 a month! Obviously there are more considerations to be mindful of around security, monitoring, and automated deployments. We’ll be happy to touch on those areas as well within a future post. In the meantime, we invite you to go check out Wittified’s add-ons portfolio on the Atlassian Marketplace which includes both Server as well as Connect-based offerings. As we mentioned above, every situation is different. Like anything else, there’s no single answer that covers every scenario when picking your path for developing a new Atlassian add-on. Hopefully our experience and perspective with Connect simply highlights it as a compelling opportunity for developers. One that’s worth a little up front effort in exchange for a whole bunch of long term benefit!", "date": "2016-03-01"},
{"website": "Atlassian", "title": "Set up Visual Studio Code to build HipChat Connect add-ons faster", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/totw-hipchat-schema-in-vs-code/", "abstract": "This tip comes to us through a pair programming session I did last week with Rich Manalang , HipChat Developer Relations. We both love the new Visual Studio Code editor as it has great debugging capabilities for Node.js. Rich showed me how to hook up the HipChat Atlassian Connect JSON Schema document into the editor. This made creating atlassian-connect.json descriptors fast and simple by utilizing IntelliSense. I explain it all in this short screencast: Here are the key things you’ll need from the video to enhance your Visual Studio Code editor to use IntelliSense for creating HipChat atlassian-connect.json descriptors: You can copy and paste this JSON code into your User Settings (don’t forget to add a comma to the previous line): Now whenever you’re in a file named atlassian-connect.json you can use the CTRL-SPACE shortcut to bring up IntelliSense. In the end, you’ll be able to start building your HipChat Connect add-on at ludicrous speed . Share your own tips and tricks in the comments or on Twitter, be sure to mention me: @RedWolves or @atlassiandev ! Ralph is a Developer Advocate for ecosystem developers.", "date": "2016-03-07"},
{"website": "Atlassian", "title": "Connecting Connect with Spring Boot", "author": ["Vincent Kok"], "link": "https://blog.developer.atlassian.com/connecting-connect-with-spring-boot/", "abstract": "Since we wrote this blog post we’ve built a fully supported and more awesome version of a Spring Boot framework. This post is still interesting but describes an early version of a framework, which is no longer supported. Please see Atlassian Connect Spring Boot . To write an Atlassian Connect add-on in Java, the Connect quick start guide tells you to use the Play framework. There should be room for another Java based framework. This is where Spring Boot kicks in. Spring Boot is a very opinionated framework to build microservices. It will help you to get going rapidly via the notion of starters. These starters allow you to bootstrap a service simply by adding a dependency. The Spring Boot Connect starter will transform your Spring Boot microservice into a fully functional Connect add-on by handling all Connect plumbing out of the box: For this tuturial you need a working Java development environment. This requires the following to be installed on your machine: Bootstrap a Spring Boot service by creating an empty Maven project and add the following snippet to your pom.xml . Simply add a dependency to the Spring Boot Connect starter to transform your microservice into a Connect add-on. Define a descriptor by adding a file called atlassian-connect.mustache in the resource folder. It supports substitution for all environment variables and the hostname. Custom substitution descriptors are supported as well. The descriptor above describes an endpoint available via the path /show. Let’s create a controller that does the job. The example below will call back to the Confluence host via Spring’s RestTemplate. It will get the content of the page as JSON and returns it as such. All authentication like verifying the incoming call and signing the outgoing REST call is taken care of. Find the source code and roadmap in the Bitbucket repository . Raise an issue for bugs and feature requests or even contribute by opening a pull request. Have questions about this tutorial? Please leave a comment.", "date": "2016-03-09"},
{"website": "Atlassian", "title": "Atlassian introduces new cloud development environment", "author": ["Robert Massaioli"], "link": "https://blog.developer.atlassian.com/cloud-ecosystem-dev-env/", "abstract": "The Atlassian Connect team is proud to announce the launch of the cloud development environment for JIRA and Confluence. We are really excited about this new environment and want to share with you a little more info about how to use it to build your add-ons. Until now, you had to use the Atlassian Plugin SDK to run a “local cloud” version of JIRA or Confluence. This isn’t a great experience, and running a “local cloud” version comes with a number of caveats: The caveats above mean that you are developing against an environment that is different from production, and every difference between development and production is a difference that could blossom into a bug — the fewer differences the better. In order to make the cloud development experience better, we have designed a new environment for you that includes free development instances exclusively for add-on developers. When you sign up for a developer instance, you will get: We have also added Development mode to the settings avaliable on the Manage add-ons page. Development mode enables the installation of Atlassian Connect descriptors that do not come from the Atlassian Marketplace. This means you can install your development add-ons directly in your instance without creating a Marketplace listing. You will continue to develop add-ons in your local development environment. The main difference is that you’ll now be creating a local proxy that enables you to install your add-on in your cloud instance. We are no longer supporting the use of the Atlassian Plugin SDK for local development of cloud add-ons. Since the vagrant image depends on the Atlassian Plugin SDK it too is no longer supported. This means that we will no longer be updating the Atlassian Plugin SDK with new features and bug fixes for cloud development. Don’t worry, the Atlassian Plugin SDK is still supported for server plugin development (P2 plugins). We hope that you are as excited about the new environment as we are and enjoy developing directly against cloud as much as we do. If you have any questions, feedback, or suggestions please don’t hesitate to contact us on the Atlassian Connect developers mailing list or on Atlassian Answers using the label atlassian-connect .", "date": "2016-04-28"},
{"website": "Atlassian", "title": "Protect our Git Repos, Stop Foxtrots Now!", "author": ["Sylvie Davies"], "link": "https://blog.developer.atlassian.com/stop-foxtrots-now/", "abstract": "Dancers gearing up to do the Foxtrot A foxtrot merge is a specific sequence of git commits. A particularly nefarious sequence. Out in the open, in lush open grasslands, the sequence looks like this: But foxtrots are rarely seen in the open. They hide up in the canopy, in-between the branches. I call them foxtrots because, when caught mid-pounce, they look like the foot sequence for the eponymous ballroom dance: Others have blogged about foxtrot merges, but they never name them directly. For example, Junio C. Hamano blogs about having Fun With –First-Parent , as well as Fun With Non-Fast-Forward . David Lowe of nestoria.com talks about Maintaining Consistent Linear History . And then there's a whole whack of people telling you to avoid git pull and to always use git pull –rebase instead. Why? Mostly to avoid merge commits in general, but also to avoid them darn foxtrot vermin. But are foxtrot merges really bad? Yes. They are clearly not as bad as the Portuguese Man o' War merge. But foxtrot merges are bad, and you do not want them creeping into your git repositories. Foxtrot merges are bad because they change origin/master's first-parent history. The parents of a merge commit are ordered. The first parent is HEAD. The second parent is the commit you reference with the git merge command. You can think of it like this: And if you are of the octopus persuasion : This means first-parent history is exactly like it sounds. It's the history you get when you omit all parents except the first one for each commit. For regular commits (non-merges) the first parent is the only parent, and for merges it was the commit you were on when you typed git merge. This notion of first-parent is built right into Git, and appears in many of the commands, e.g., git log –first-parent. The problem with foxtrot merges is they cause origin/master to merge as a second parent. Which would be fine except that Git doesn't care about parent-order when it evaluates whether a commit is eligible for fast-forward. And you really don't want that. You don't want foxtrot merges updating origin/master via fast-forward. It makes the first-parent history unstable. Look what happens when a foxtrot merge is pushed: You can calculate the first-parent history yourself by tracing the graph with your finger starting from origin/master and always going left at every fork. The problem is that initially the first-parent sequence of commits (starting from origin/master ) is this: B, A But after the foxtrot merge is pushed, the first-parent sequence becomes this: D, C, A Commit B has vanished from the origin-master 's first-parent history. No work is lost, and commit B is still part of origin/master of course. But first-parent turns out to have all sorts of implications. Did you know that the tilda notation (e.g., ~N) specifies the Nth -commit down the first-parent path from the given commit? Have you ever wanted to see each commit on your branch as a diff, but git log -p is clearly missing diffs, and git log -p -m has way too many diffs? Try git log -p -m –first-parent instead. Have you ever wanted to revert a merge? You need to supply the -m parent-number option to git revert to do that, and you really don't want to provide the wrong parent-number. Most people I work with treat the first-parent sequence as the real \"master\" branch. Either consciously or subconsciously, people see git log –first-parent origin/master as the sequence of the important things. As for any side branches merging in? Well, you know what they say: But foxtrot merges mess with this. Consider the example below, where a sequence of critical commits hits origin/master in parallel to your own slightly less important work: At this point you're finally ready to bring your work into master. You type git pull , or maybe you're on a topic branch and you type git merge master . What happens? A foxtrot merge happens. This wouldn't really be of any concern. Except when you type git push and your remote repo accepts it. Because now your history looks like this: Nothing. Leave them. Unless you're one of those antisocial people that rewrites master. Then go nuts. Actually, please don't . There are a few ways. My favorite approach involves 4 steps: This is my preferred approach because it keeps the foxtrots away, and it prints a cow whenever a foxtrot is blocked: There are other ways. You could disable direct pushes to master, and hope that pull-requests never merge with fast-forward. Or train your staff to always do git pull –rebase and to never type git merge master and once all your staff are trained, never hire anyone else. If you have direct access to the remote repository, you could setup a pre-receive hook. The following bash script should help you get started: Suppose you install the pre-receive hook, and it blocks your foxtrot. What do you do next? You have three possible remedies: But please don't do #3, because the final result is called a \"Portuguese man o' war merge,\" and those guys are even worse than foxtrot merges. At the end of the day a foxtrot merge is just like any other merge. Two (or more) commits come together to reconcile separate development histories. As far as your codebase is concerned, it makes no difference. Whether commit A merges into commit B or vice versa, the end result from a code perspective is identical. But when it comes to your repository's history, as well as using the git toolset effectively, foxtrot merges create havoc. By setting up policy to prevent them, you make your history easier to understand, and you reduce the range of git command options you need to memorize. Did you like the git graphs in this blog post? You can make similar git graphs for your own blog posts using the git-graph-drawing-tool: http://bit-booster.com/graph.html . You can also install the Bit-Booster Commit Graph add-on for Bitbucket Server to block foxtrots AND see nice git graphs.", "date": "2016-04-28"},
{"website": "Atlassian", "title": "npm for Bitbucket 2.0: Now with private packages!", "author": ["Steve Smith"], "link": "https://blog.developer.atlassian.com/bitbucket-npm-2-2/", "abstract": "(Update: The NPM add-on is now available in the Atlassian Marketplace. See the updated version of this post for more details.) We’re pleased to announce version 2.0 of Tim’s npm for Bitbucket integration . The major change for this release is the addition of support for private packages. Read on for more information, plus a little easter-egg. The headline feature for this release is the addition of support for private packages in the official npm repository . If you have private packages, you just need to add your authentication token from your .npmrc in your Bitbucket settings. To do this go to `Bitbucket Settings->Configure NPM Stats`: Once that is in place you can then visit any Bitbucket npm project with a private package and you will be able to see metadata for it: One thing to note is that we display slightly different data here, as private packages have different stats stored against them. Private packages now show maintainer and dependency information, instead of downloads. But don’t worry; public packages carry on working the same as before. As with the previous version, clicking on the metadata will bring you to a custom package.json file viewer that analyses the package data and displays a table of dependencies with current and latest versions: But to complement this we’ve also added something new… You may notice above in the viewer dropdown there is a third viewer, called Dependency visualisation . When viewing the package.json you can select this option to open up a basic visualization of your package dependencies: This visualization is also available by clicking on the dependencies count on the main metadata view in your Bitbucket repository. Version 2.0 of the add-on is currently going through the process of being updated in the Bitbucket marketplace, however if you want to get it early just follow these steps: That’s all; after that, and setting up your authentication token, your private (or public) packages should now show the updated metadata and data visualization. Have fun!", "date": "2016-05-23"},
{"website": "Atlassian", "title": "Developing a JIRA add-on like it’s 2016", "author": ["Andreas Knecht"], "link": "https://blog.developer.atlassian.com/jira-add-on-dev-2016-part-1/", "abstract": "We at Code Barrel are a bunch of former Atlassians hard at work developing rich new add-ons to help your team be as productive as possible! With this in mind we developed the NPS for JIRA add-on to enable your teams to get feedback directly from your customers, without ever having to leave JIRA or use a 3rd party tool. In this blog, we wanted to share a brand new process that we used to develop this add-on in the hope that it will help other add-on developers in the ecosystem. We didn’t come up with this approach all by ourselves – full credit where it’s due: we were part of the JIRA Core at Atlassian when we pioneered this approach. It was so good that we decided to steal it use it as well. This blog is part one of a two part blog series. In today’s blog we’ll look at the build configuration in depth. In part two we’ll cover development practices in detail. We’re going to go into a lot of technical detail in this blog. If you’re interested in a more product focused blog about how NPS for JIRA can help your team figure out what your customers really think, please let us know . Add-ons have been developed in more or less the same way for several years. They usually consist of: The problem with this approach is that it makes developing an add-on for both server and cloud ( Atlassian Connect ) difficult. Certain constructs such as webwork1 actions are not available in a cloud add-on. Also most of the JavaScript that’s been written traditionally was heavily dependent on JIRA’s bundled JavaScript – something that would not work very well in a connect add-on (since the UI is rendered on a different server and domain). To solve this problem we introduced a few guiding principles that need to be respected when developing an add-on: In the next sections we’ll take an in-depth look at how this approach can be realized. In theory, no specific tools are needed to to achieve our second goal of independent UI components. It is possible to write JavaScript components like this today. In practice, this is incredibly hard and requires far too much discipline from any dev team to be practical. This is why we need a few new practices and tools to make this possible. First we will look at how we structure our project. From there we will look at configuration and dependencies. Then finally we will show you our dev loop and how we integrate with our build and test automations. In order to ensure we separate out all our UI components from JIRA we need a new folder structure: This is a standard JIRA server add-on using the plugin SDK . The only thing that’s different is the npsclient folder at the top level. This folder is excluded from the jar built in a standard Maven build. Looking at this in depth we can see that it’s a standard npm package: This npm package could in fact also live outside of the plugin, but to keep things simple for now we include it directly in the add-on source repository. Looking at the folder above you’ll notice that a few more tools are in use: Looking at the dependencies in package.json we notice that we’re using quite a few more tools: In particular we’re using: The .babelrc configuration is quite simple: It instructs Babel that we are using React and ES6 and that we also want to use the new ES7 object spread operator. Babel can now down-compile this to ES5 (browser) compatible JavaScript code. Now that we’ve looked at all the different tools in play, how do we hook them together? It all starts with the package.json in the npsclient folder. It contains a number of scripts to package the app for inclusion in JIRA: The important script for development is the dev-jira script. To start developing the NPS add-on we simply launch JIRA in one console with the standard atlas-debug command. Then in a second console from the npsclient folder we launch npm run dev-jira . This starts Webpack with the dev configuration and watches files for changes, so that any edits in our source code trigger a re-package to make the packaged JavaScript available to JIRA. Lets take a look at the Webpack configuration in detail: There’s a number of things going on here: How do we hook all of this up with the atlassian-plugin.xml ? Simple: Webpack bundles our external JavaScript into the src/main/resources/client folder inside our Atlassian plugin where we can then simply declare it as a standard web-resource in the atlassian-plugin.xml . If we make a change in the npm module, the Webpack process we started earlier will automatically rebundle the *.pack.js file and since we’re running in dev mode, the Atlassian plugin SDK will automatically load the file from disk when we reload the browser. *In practice this means our dev loop is ‘change file’ → ‘refresh browser’*. The final jar file we release is built with Maven. Maven is needed to compile our Java source files and to bundle the plugin correctly with the maven-jira-plugin. This build now needs to also run the npm Webpack build to ensure we bundle the npm JavaScript resources in our plugin jar. To achieve this, simply add the following maven-exec-plugin definition to build → plugins in the Maven pom.xml file: This does a few things: We mentioned earlier that one of our guiding principles is to develop components that are standalone from JIRA. Webpack once again makes this easy with its dev server. By simply running the dev script defined in package.json via npm run dev we can run a standalone version of the npm module and test the client outside of JIRA. The Webpack dev server starts up incredibly fast and auto-reloads changes in the browser! Here’s an example of the config form React component running in the standalone Webpack dev server version: We’ve covered a lot so far. We’ve looked at what’s wrong with the current approach of developing add-ons, outlined guiding principles for how to fix these problems and covered how the structure, tools and build of our add-on works to help us achieve these principles. In part two of this blog series we’ll get into the meat of how things work: We’ll take an in-depth look at how some of the new components are implemented, how we test and internationalise them, and how we apply Redux to more complex parts of our app! Thanks for reading – we hope you found this useful and please let us know your thoughts or suggestions in the comments below!", "date": "2016-06-07"},
{"website": "Atlassian", "title": "Developing a JIRA add-on like it’s 2016: Part Two", "author": ["Andreas Knecht"], "link": "https://blog.developer.atlassian.com/jira-add-on-dev-2016-part-2/", "abstract": "Welcome back! In part one of this blog series we covered all the new tools and build details required for a better way to build a JIRA add-on. If you remember the goal is to write add-ons that can be converted from server → cloud with ease. This requires we follow two guiding principles: Today we’ll look at how some of these components are implemented on the client side using React and ES6. Building complex UIs solely on the client side has improved significantly in recent years. Gone are the days of spaghetti JavaScript code. It’s now simple to write clean re-usable components that are easy to test. We’ll take a look at a very complex part of the NPS for JIRA add-on – the reports page: There’s quite a lot going on, on this page: Lets examine the ReportsPage React component. One would think that this is quite complex, but it’s not: Even for someone not familiar with the code, it’s quite easy to see what’s going on here and how the reports page is broken down into a number of React sub-components: The syntax may seem a little unfamiliar to some, but thanks to the power of Babel we can use all the power of ES6 and React JSX: We mentioned earlier that our reports use Chart JS to draw the graphs. This can be a little tricky with React since React keeps very tight control over the DOM and how it gets manipulated. Here’s an example of how the NPS Doughnut component (a sub-component of the Scores Report above) works to draw the doughnut chart: Lets take a look at the render() function first: Once render() has been executed React will call componentDidMount() when the rendered content is inserted into the real DOM from the shadow DOM. In this method, we can now initialize all our Chart JS chart options and initialize the Chart using the ref to the canvas DOM element. The only other thing we need to do is to ensure we update the chart via componentDidUpdate() whenever this component receives new props that may change the chart (for example if a filter is changed in the ReportsHeader). That’s it! Traditionally JIRA add-ons have been tested with Web-driver tests and qunit tests in the browser. Both of these approaches tend to be quite slow since they require JIRA to be running and a browser to be setup. This is not so much of a problem while developing, but can be a pain to setup and slow to run during CI. For the NPS plugin, front-end code is tested using a lightweight JavaScript testing framework named Mocha JS . We also use Sinon JS for mocking. Here’s an example of a test to ensure that the overtime chart can handle various responses from the server: Due to the async nature of the REST call, we need to use Promises that get resolved by the component itself via the ajaxDataRenderResolve prop before we can run assertions. This is a bit of a smell and we’ll see later how Redux can help us to remove it. A few other interesting things to note: Running these tests is incredibly fast since we don’t execute them in the browser, but in jsdom – a node implementation of the DOM. This requires a bit of setup which is done in the test-setup module. Looking at our test package.json script from part one we can see how we require this test-setup module: This module simply contains setup for jsdom and we also configure our polyfills: You can now run tests from the command line with npm run test . There’s also a Mocha runner for IntelliJ. In part one of this blog series, you can also see the test-maven script we invoke from Maven which uses Mocha test reporters to produce a junit test result xml file that can be parsed by Bamboo. There’s a number of React i18n frameworks available. However most seemed too complex or not quite the right fit for our purpose. JIRA already does a lot of heavy lifting in terms of i18n for client-side resources through its web-resource transforms and the availability of AJS.I18n.getText('some.key') . So we decided on a simple solution. We provide global i18nStrings module in our web-pack configuration (see part one for the full config): This requires a module defined by a standard JIRA web-resource that has transforms applied. The contents of this module are: $$i18nPrefixes is a ‘special’ function that actually gets transformed by our own web-resource transformer into a JSON object containing all the i18nized key → values for the ‘survey.plugin’ prefix. It’s just some syntactic sugar to save typing all the individual keys. This approach would have to be re-implemented in a different way in an atlassian-connect add-on for cloud. Then finally we provide an i18n module to all our React components for import: The I18nHelper we import provides the same getText(key, args) API as AJS.I18n.getText() . Now in our React components we can simply call: Earlier when looking at unit tests we discovered a bit of a smell in one of our components – the fact that our NPS Dougnut chart was deeply aware of state and was making REST calls. This made testing less than ideal and our component overly complex. Ideally with React most components should simply be ‘dumb’ stateless functions rendering the passed in props and not aware of state. Redux, a predictable state container for JavaScript aims to solve this problem. Explaining Redux goes far beyond the scope for this blog and is unnecessary since Redux’s documentation is incredible! Lets take a look at how using Redux can make another complex part of the NPS for JIRA add-on much simpler by looking at the ConfigForm: There’s a few things that need to be managed here: This is what the NPSAdmin React component that renders this page looks like: It’s a dumb stateless function. That makes it very easy to unit test: we can just pass in different props. No need to deal with asynchronous tests and Promise callbacks like in our NPS Doughnut chart example. The interesting part as far as Redux is concerned is at the bottom. We wrap the NPSAdmin component using a call to connect() and pass in two mapping functions for mapping state to props and dispatching actions. The mapStateToProps function is invoked whenever state changes in our Redux store. mapDispatchToProps will dispatch the appropriate actions whenever the ConfigForm triggers its onSave or onDelete handlers. Lets take a brief look at what the save action looks like: This is now where we handle calls to the server and dispatch new actions depending on the result. Testing this is a lot easier since Redux provides a few helpers (redux-mock-store). We could just test our dumb NPSAdmin component by passing in different props. However another test is to ensure our NPSAdmin component reacts correctly to actions dispatched on a real Redux store: This is a much better test than what we saw previously with the NPS Doughnut component. Once again we covered a lot in this part. We looked at the anatomy of a few different React components and how being able to use ES6 creates very clean, easy to maintain code. We also showed how to test and internationalize these components. Finally we were able to make a complex part of the app much easier to maintain and unit test with the help of Redux! We hope you enjoyed this blog series on how to write JIRA add-ons using a new development process that should lend itself to producing re-usable UI components for both server and cloud, using some of the latest tools available in web development! We’re sure there’s many suggestions and improvements for this approach. Please let us know in the comments below!", "date": "2016-06-09"},
{"website": "Atlassian", "title": "Hyperlinking logs to source code", "author": ["Ian Buchanan"], "link": "https://blog.developer.atlassian.com/hyperlinking-logs-to-source-code/", "abstract": "A lot of news came out of AtlasCamp 2016 . I was especially proud of the story about TestFairy’s end-to-end integration . I helped TestFairy build the connections between JIRA, HipChat, Bitbucket, Bamboo, and Bitbucket Pipelines to their service for mobile beta testing. In their press release, TestFairy CTO, Gil Megidish explained: Based on requests we’ve received from our large developer and testers community, we’re happy to release the solution that closes the loop between source code and crashes, by creating a link between the two. When showing a crash, TestFairy will automagically link the video recording of the user session and the stack trace to your Bitbucket repository, at the right branch and revision. Many of our customers who already tested the solution came back and said their work became much more efficient. I hope to see other integrations follow TestFairy’s lead and hyperlink to source code in Bitbucket . Fortunately, it is relatively simple to implement with some forethought. The hyperlink itself requires 7 parameters. When you assemble these parameters, you get a URL like this (some spacing added for readability): The trick is that different parameters can come from different places. The easiest source is what I labeled static. In other words, it’s a value that could be hard-coded into an integration, without causing long-term maintenance pain. This URL pattern only works for Bitbucket Cloud; hence, the base URL can be harded-coded to the production URL for Bitbucket Cloud. The file viewer is an optional parameter so it could be left out entirely. The safest file viewer would be the default, since any other choice would require every user who clicks the link already has that file viewer. The build server should know the “ provenance ” of a build and deployment. It should be able to forward the source context to the log server. Let’s look at some more concrete ways that might happen. For TestFairy, we used specific parameters on the deployment hook to remember the source context from the build environment. Consider the following lines of Bash: Although this example used specific variable names from Bitbucket Pipelines, the same information is available in other build servers like Bamboo and Jenkins. The same information can be forwarded in different ways. For Java JARs, it could be written into the META-INF/MANIFEST.MF . Or maybe it could be provided via a naming standard for the JAR itself, like: repo_owner-repo_slug-commit.jar If not provided by a build server, an approximation could be provided by a user for a given log. A user can easily find the owner and repository name. The commit SHA might change frequently, so instead the user might provide the branch where source is most closely related. With common Git workflows, that is likely to be master . While it might be easier to obtain these details from a user, the downside is there may be subsequent commits on the target branch, making the pointer into the file either invalid or wrong. The details of filepath and fileline will usually be found in a stack trace. The trick here is to parse the file path from the stack trace and to know that file’s relative path in the source repository. For Java projects, Maven directory conventions will inject a few extra directories. That’s why the TestFairy approach sends that information explicitly. For other languages, the mapping to source code is often easier because mapping to the source path follows a simpler convention. I hope you’ll find ways to weave in hyperlinks back to Bitbucket. If you have questions or need other help working with the Bitbucket endpoints, tweet me at @devpartisan or my team at @atlassiandev .", "date": "2016-06-13"},
{"website": "Atlassian", "title": "GorillaStack’s serverless HipChat Connect boilerplate", "author": ["Elliott Spira"], "link": "https://blog.developer.atlassian.com/gorillastack-serverless-hipchat-connect/", "abstract": "Have you ever wanted to try out a full serverless architecture ? Or write a HipChat plugin ? The new open source boilerplate from GorillaStack , announced at this year’s AtlasCamp , enables developers to easily deploy the back-ends for their plugins to Amazon Web Services’ serverless products in Lambda, API Gateway, DynamoDB, and S3. The HipChat team, together with Amazon Web Services , engaged GorillaStack to develop an open-source boilerplate repository to help their plugin developers. GorillaStack is a SaaS solution that automates optimizations in the AWS cloud and is a go-to tool for DevOps engineers who want to ease the heavy lifting of cloud cost management. GorillaStack has displayed its commitment to open-source contributions in the past with a particular focus on serverless solutions so were the obvious partner for this contribution. There are four key benefits to a serverless deployment: Firstly, you pay for what you use. With AWS Lambda , you pay per invocation and for the amount of time your code runs and amount of memory required. No more paying for idle time. Secondly, it’s easy to jump in and get going. There’s no need to spend time building environments or building CI-CD pipelines for blue-green deployments. Just get your code running and the rest will take care of itself. Thirdly, you can iterate faster. It’s super easy to deploy different stages and versions: dev, staging, beta, prod, v1, v2. Cutting down on deployment time gives your more time to focus on the development work that truly matters. And fourthly, it has scale built-in. There’s no need to worry about performance testing or worrying about whether your infrastructure will be able to handle high volumes. If you want to try out a fully serverless architecture or just write a HipChat plugin, head to the GorillaStack repository on bitbucket and let us know what you think!", "date": "2016-06-22"},
{"website": "Atlassian", "title": "The (Written) Unwritten Guide to Pull Requests", "author": ["Blake Riosa"], "link": "https://blog.developer.atlassian.com/written-unwritten-guide-pull-requests/", "abstract": "To help your software development team empty those \"in review\" columns quickly, here are some guidelines for your pull requests from a variety of different developers at Atlassian. I’m sure the size of that \"in review\" column is familiar with many-a-team. First, let’s admit it: reviewing pull requests is really hard. As a reviewer, it’s your responsibility to make sure that the code is correct and of high quality before it gets merged into master. You are expected to do that by looking at a diff and list of files changed. You have to understand what the pull request is trying to achieve, what approach is being taken, what’s going on, and how all these files fit together – enough that you could potentially suggest an improvement. You may have to be on the lookout for typos or style problems. That’s a LOT of stuff a reviewer needs to do, especially in a large pull request! How can we make our pull requests easier for our reviewers? Let’s put on our product management hat. If pull requests are a product, then reviewers are our customers. We want our customers to ‘buy’ our pull requests by approving them so we can ship quickly and empty that review column. If we are to manage this product well, one thing we need to do is understand our customers and market. It’s pretty simple, really. Since most of us pull request authors have likely been reviewers already, simply put yourself in the shoes of the reviewer and ask, \"How could this be easier for me?\" Making smaller pull requests is the #1 way to speed up your review time. Because they are so small, it’s easier for the reviewers to understand the context and reason with the logic. Now, you may be thinking: But Blake, my issue just exploded in complexity after I started working on it. Trust me, I’ve been there. It’s really easy to throw yourself into finding the solution to the problem you’re working on and lose focus on the bigger picture. Unfortunately, in my experience, solving the issue usually represents a surprisingly low portion of the time spent between ticket creation and release to customers. Review, quality assurance, and the release process all take time. Spending a bit more time breaking down the problem while you’re actually problem-solving is worth it, especially when your team has endless review columns. I have no way to tell whether a pull request is going to be big until I start on an issue, at which point it’s usually too late. It’s easy to make big pull requests. It’s difficult to make small, logical ones that are quick to review, push, and achieve velocity with. On my team, we are experimenting with small, time-boxed spikes on issues we pick up to see if we should break them down any more before pushing any code. We’ll see how that goes, but in the meantime, it’s definitely worth time breaking down your tickets or pull requests before you commit them in one massive push. Writing a useful description in the \"details\" section of your pull request can be almost as effective as making a smaller pull request! If I do make a large pull request, I’ll make sure to spend a lot of time making a really helpful description. The most helpful descriptions guide the reviewer through the code as much as possible, highlighting related files and grouping them into concepts or problems that are being solved. This saves the reviewer a lot of time because they don’t have to read every file to try and group them up and identify related concepts. After that, it’s a lot easier to reason about and review your approach. The pull request author is the best person to do this since they made these files in the first place, and have all the details fresh in their minds. Similarly, a useful summary title (instead of just the issue key) makes it clear to the reviewer what’s being solved at a high level, which takes off one extra step for the reviewer. At the end of the day, both of these things give the reviewer more context around the change that’s happening. \"addressed PR feedback\" I don’t expect everyone to have every line of their Git commit messages down to a strict 72-character limit, ( although the first 50 characters are useful as a summary ), but a good commit message can help improve a code reviewer’s experience. First, it can make Bitbucket’s auto-generated pull requests more useful, especially for smaller pull requests. Good commit messages can also provide a nice bullet-point-like summary of the code changes as well, and it helps reviewers who read the commits in addition to the diff. I’m a bit guilty of lower-quality commit messages. That said, commits are very much at the code-level, and should be about the code changes. A pull request, on the other hand, though it is code-focused, requires a higher-level, architectural understanding of the change. A high-level summary and understanding of the problem is very useful for people looking back through a repo’s history in addition to reading the details of the individual code changes. For this reason, I’m very much a proponent of putting a JIRA issue key in every commit message, so no matter where a user finds a commit message from, there’ll always be a trail back to the pull request. This also helps soften the blow of lower-quality commit messages. Even if you have completely on-point commit messages, I still believe in writing a good description in the pull request over only using the auto-generated commit log. As I said before, commits are very much at the code-level while code review requires a higher-level understanding of the change, and that’s hard to achieve with a commit message log alone. A colleague gave me really nice summary of how to think while writing commit messages: Commit message should talk about WHAT changed, and WHY. Not HOW – how is the diff, and you don’t need to repeat it. Have you simply re-indented lines in one file? Is a particular file the \"main bulk\" of your change? Is a file related to, or coupled with, another in the same pull request? Consider leaving a comment inline, at the top of the file, to let the reviewer know. These help the reviewer navigate your pull request. Even better, it’s possible to create a pull request with no reviewers allowing you to review it yourself and write comments pointing out the interesting bits before anyone else sees the code. It’s worth noting that pull request comments should not be used to explain your code. If you find yourself explaining your own code in a pull request comment, consider making it an actual, in-code comment instead. These comments are only for helping the reviewer navigate your pull request. Add some screenshots for your front-end changes! Screenshots simply make the job for the reviewer much easier. It’s easier to reason about front-end changes when you can visually see what’s been changed in the pull request. If you’re feeling extra generous, add a GIF or video of you using the feature (potentially in multiple browsers, if that’s important) to add some extra validation for your UI changes. Also, consider adding your designers to pull requests for front-end changes. They can often spot visual quirks, as well as copy mistakes, earlier in the process thanks to the screenshots! These are just some of ways to write pull requests to improve the pull request experience for reviewers around your company. If you start thinking of a pull request as a product, with the author as the seller, and reviewers as customers, then that helps us understand our customer in order to \"sell\" our pull request more effectively and get faster approvals. Though this is how I think about pull requests, I want to emphasize that these are just guidelines, not hard and fast rules. Please feel free to comment below with your own techniques for keeping your PR column under control and share your pull request experiences – I’d love to hear how your team handles code review! Hopefully, together we can create better pull request and code review experiences for everybody.", "date": "2016-07-25"},
{"website": "Atlassian", "title": "npm for Bitbucket 2.0: Now with private packages!", "author": ["Steve Smith"], "link": "https://blog.developer.atlassian.com/bitbucket-npm-2/", "abstract": "Note: This is a repost of our previous announcement . However if you don’t have it already, the Bitbucket NPM add-on is now available in the Atlassian Marketplace . We’re pleased to announce version 2.0 of Tim’s npm for Bitbucket integration . The major change for this release is the addition of support for private packages. Read on for more information, plus a little easter-egg. The headline feature for this release is the addition of support for private packages in the official npm repository . If you have private packages, you just need to add your authentication token from your .npmrc in your Bitbucket settings. To do this go to `Bitbucket Settings->Configure NPM Stats`: Once that is in place you can then visit any Bitbucket npm project with a private package and you will be able to see metadata for it: One thing to note is that we display slightly different data here, as private packages have different stats stored against them. Private packages now show maintainer and dependency information, instead of downloads. But don’t worry; public packages carry on working the same as before. As with the previous version, clicking on the metadata will bring you to a custom package.json file viewer that analyses the package data and displays a table of dependencies with current and latest versions: But to complement this we’ve also added something new… You may notice above in the viewer dropdown there is a third viewer, called Dependency visualisation . When viewing the package.json you can select this option to open up a basic visualization of your package dependencies: This visualization is also available by clicking on the dependencies count on the main metadata view in your Bitbucket repository. Version 2.0 of the add-on is [available in Atlassian Marketplace]. You can install it via the Marketplace link above or through [your Bitbucket account]. If you have private packages, just navigate to the “Configure NPM Stats” section of your account settings and enter your authentication token. After that your private (or public) packages should now show the updated metadata and data visualization. Have fun!", "date": "2016-08-18"},
{"website": "Atlassian", "title": "Custom Security for Add-ons: Building “Upwork Job Post for JIRA", "author": ["Sameer Shah and Lance Caraccioli"], "link": "https://blog.developer.atlassian.com/custom-security-for-addons/", "abstract": "Upwork ‘s mission is to make it easy to get work done and hire freelance help – including development help. Many Upwork customers and freelancers use JIRA Software for development planning. This post shares our lessons learned from building an integration between Upwork and JIRA, including using our own security framework. Due to the complexity of Upwork’s marketplace architecture and the need to ensure proper security and authentication, the team at Upwork needed to create a customized authentication mechanism in Java. This approach allowed us to ensure the smoothest customer experience while maintaining security throughout the job posting flow. To see what we built check out Upwork Job Post for JIRA . We used Atlassian Connect , JIRA Software for Cloud APIs, and Upwork Developers OAuth APIs to create the add-on. For building on Atlassian Connect, you need to host your own proxy service which serves the JSON descriptor file as well as HTML. This HTML gets rendered in an iframe in JIRA Software Cloud. Authentication between your proxy service and JIRA Software occurs using JSON Web Tokens . At Upwork we have an in-house microservice architecture called Agora where different microservices communicate with each other to form the backend for the entire Upwork site. The Agora framework is a wrapper around the dropwizard framework and provides an easy way to deploy and scale services on Amazon Web Services. Because of that, it was an easier choice to create an Agora proxy microservice for hosting the JIRA Software add-on. Dropwizard provides support for the mustache framework to render HTML and an asset servlet to serve different static assets. Finally, MongoDB was selected as the database of choice to store the client credentials, user tokens, and the association between the JIRA Software issues and Upwork jobs. We went with MongoDB because it was the most straightforward choice given the existing infrastructure. Atlassian provides a Node.js framework called Atlassian Connect Express which takes care of the whole JWT authentication mechanism for you. However, since we decided to use our own Agora framework, which is written in Java, we had to replicate the authentication mechanism in Java as well. When an add-on is installed for the first time in JIRA, JIRA sends some basic information like client ID, shared secret, etc. The proxy service stores this information in Mongo after verifying that it’s coming from the JIRA cloud server. After the client’s shared secret is stored in the database, every subsequent request from the JIRA Service, made through the Connect framework, contains the JWT token either as a param or as an authorization header. This token is verified using the authorization filter and this filter is called only if the resource API method is annotated with an annotation called @JWTAuthorizationRequired. Also JWT tokens contain other useful information like the client ID and user which gets added in the request context so that APIs can use that info if they need it. Here is how the JWT token can be extracted: For JWT token verification, we used the Nimbus Jose JWT library. Atlassian Connect provides a good example to verify and extract JWT tokens. Finally, in some cases when the request is not made through the Atlassian Connect framework, JWT tokens won’t get added to the request. For these cases we need to create the session token and add it to the request. Session tokens could be sent as the query param or authorization header. The following code shows how to create a session JWT token that is valid for some time. We kept the validity for only 30 minutes. This expiry time is embedded inside the session token. Apart from that much more useful information can be embedded inside the JWT session token as a claims object. I embedded information like issueId, projectId, userKey etc. which can be used to identify the information about the user and JIRA on the server side when the session token is passed in AJAX. This is how the JwtClaims class and the ClientInfo class above are POJO objects , containing information about the user, JIRA and client instance of JIRA. Once the session token is generated we need to pass it in each and every AJAX request made by the add-on so that we can validate the request. We decided to use the layout template to embed the session token. This layout template is part of each and every other template and hence the session token was available in each and every AJAX request as the Authorization header. The following code snippet shows how a session token can be added in each and every AJAX request. Now once you have the session token at the server, you can extract useful information like user and client information to associate them in the database with various other objects – like an Upwork job post in this case. Upwork APIs are exposed through OAuth. You can request the OAuth key and the secret for using the APIs at the following link . Once the key and secret are obtained, we use the Upwork Java OAuth APIs to make requests. Here’s the API link we used to create a job post on Upwork. https://developers.upwork.com/?lang=java#jobs_post-job When a user logs in and views the issue details, the Atlassian JIRA framework makes a request to the proxy service to get the details about the add-on. A Jwt token is automatically added to the request which contains other useful information like userKey, clientKey and validation information for the request. Once Proxy service receives the request for the webPanel HTML, it tries to find whether there are any access tokens associated with the userKey and clientKey in the database. If not, the “Connect to Upwork” button is presented to the user to share the access tokens with the proxy service. The add-on uses OAuth 1.0 (Three Legged flow) to obtain authorization on behalf of Upwork users. Only the request token is exposed on the client side. The final leg of the OAuth flow is completed by the add-on’s server side implementations where it receives the OAuth access token. Following is the flow on how the tokens can be associated with the user. Once authorization has been granted, the access token is then associated with the JIRA user by storing that association in MongoDB. Now whenever a request for webPanel HTML is received, the user is presented with “Post this Job To Upwork” button. When the user clicks on the button a dialog box is created which will ask the user to put all the relevant data related to that job. Once a user fills out all the details in the job post dialog box and submits, an AJAX request is made to proxy service to post job to Upwork and associate the job details to issueKey and clientKey so that next time the user is shown the job posted to corresponding issue. When the request to webPanel HTML is made and Proxy Service finds that there is an associated job reference to issueKey and clientKey, it retrieves that job reference from the database. Now the HTML that is returned back for the webPanel is “View Job” link. This link is created using the jobPublicUrl that you see in the UpworkJobReference collection above. Interested in trying the Upwork Job Post for JIRA ? You can add it to your JIRA environment on the Atlassian Marketplace.", "date": "2016-08-25"},
{"website": "Atlassian", "title": "Better know a feature: the fightin’ postInstallPage", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/better-know-a-feature-the-fightin-postinstallpage/", "abstract": "One of my favorite shows, that is sadly not on the air anymore, was The Colbert Report with Steven Colbert. He would do a regular segment called “Better know a District ” where Colbert would interview a United States Congressman from one of the 435 congressional districts. It was a great segment as we would learn a lot about another district that we would most likely have never heard of. These segments either would hurt the congressman politically as it showed how out of touch with reality they were or it showed how down to earth they were and represented their district with humility, as only one could while being interviewed by Steven Colbert. In the clip below, Colbert interviews Congresswoman Marcia Fudge of Ohio’s 11th District (Cleveland, OH), or as Colbert calls it, the fightin’ 11th: In Developer Relations, I get asked many questions from integrators and I’ll occasionally stumble across little features of Atlassian Connect while researching an answer. Today, I bring to you a new blog segment called “ Better know a Feature “. In this first segment, let’s touch on a little known feature called postInstallPage , the FIGHTIN’ POSTINSTALLPAGE . In JIRA and Confluence, when an admin installs your add-on into their Cloud instance you may want to walk them through how to use your add-on. One way to do that is to use a Page module called postInstallPage . When you include the postInstallPage in your descriptor a Getting Started button will link to the page specified in both the add-on’s entry in Manage Add-ons and from the dialog that the user is shown when they successfully install the add-on. In your atlassian-connect.json descriptor file, you declare a postInstallPage module: Unlike generalPages or adminPages you can only have one postInstallPage . Which means you can’t assign an array of page objects, postInstallPage expects only one object. After your add-on has been installed the user will get a dialog box with a Getting Started button: Additionally, a Getting Started button will appear on the Manage Add-on screen for your add-on. And each of those buttons will take you to a page where you can style and add content to introduce new users to your add-on. It’s a great way to make your add-on user friendly from the start. Feel free to install this add-on yourself. To install: You can also check out the source code at https://bitbucket.org/rwhitbeck/better-know-a-feature-postinstallpage . Know of an interesting feature of Connect? Let me know in the comments below or follow me on Twitter, @RedWolves . For more tips like this follow us @atlassiandev . Ralph is a Developer Advocate for ecosystem developers.", "date": "2016-10-05"},
{"website": "Atlassian", "title": "How we built Bitbucket Data Center to scale", "author": ["Richard Friend"], "link": "https://blog.developer.atlassian.com/how-we-built-bitbucket-data-center-to-scale/", "abstract": "This post is the first part of a series on how we built Bitbucket Data Center to scale. Check out the entire series here Today is Bitbucket Data Center’s second birthday! It really was two years ago that Stash Data Center 3.5 (as it was then called) became the first collaborative Git solution on the market built for massive scale. On the day it was born, Bitbucket Data Center had just 7 customers (who had worked with us throughout the development and pre-release phases), and a small handful of add-ons whose vendors had made sure their products had already earned the Data Center compatible badge on day one. Since those humble beginnings, Bitbucket Data Center has changed its name and experienced enormous growth in adoption, functionality, and deployment flexibility. Some highlights we’re particularly proud of include But the number one feature provided by Bitbucket Data Center since the beginning — and still the primary reason why many customers adopt it — is performance at scale . Large enterprises with thousands of active users and build agents hitting their central Bitbucket instance can’t serve all their load with a single machine. Instead, sysadmins must use the scale features of Bitbucket Data Center to handle heavy loads without sacrificing performance for their users. To celebrate Bitbucket Data Center’s latest milestone we’ll describe some of the work we’ve been doing – behind the scenes – to make Bitbucket Data Center the first massively scalable Git solution and still the leading and most performant product available today. The scaling challenge When it comes to scale, the most demanding load many Bitbucket instances deal with is managing Git hosting requests (simultaneous user-initiated commands, like git clone , fetch , push , and so on). This is because when you run a Git command that must communicate with a remote repository on Bitbucket, your Git client opens one or more connections to your Bitbucket instance (depending on whether you are using HTTP or SSH). When each of these connection reaches the backend server, after authentication and other processing, the connection spawns a Git process and streams its standard input, output, and error output back to the client. These Git processes on the server are CPU and memory intensive, especially when they generate packfiles to transfer repository contents over the network. By comparison, most other kinds of operation you can perform against a Bitbucket instance (like browsing around, interacting with pull requests, and so on) are generally much lighter and faster. These graphs of the CPU and memory usage of a typical git clone operation on a server might help to illustrate just how resource intensive Git hosting operations can be. The blue lines shows the resource consumption of the Git process alongside the red lines representing that of Bitbucket. Bitbucket does a bit of work when the connection comes in and then hands it off to Git, using very little CPU itself. The first thing Git does is create a packfile, consuming about 100% of a CPU core for a while, then it does some compression which consumes even more (Git is multi-threaded). Its memory consumption also climbs during these phases, often by a few hundred MBytes or more. After the packfile has been generated, streaming it back to the client uses hardly any CPU in Git or Bitbucket, but the memory allocated by Git isn’t released until the request has been fully served and the process exits. The resource consumption of just one git clone may not seem so bad, but when you have hundreds or thousands of users doing these operations concurrently, the CPU and memory usage add up quickly. Continuous integration systems like Bamboo and Jenkins are also famous (or, perhaps, infamous) for making very many Git hosting requests against a Bitbucket instance, to clone and fetch from (and sometimes also push to) repositories for builds and tests. Load from build agents tends to come in bursts: builds configured with many parallel or cascading stages can generate massive “storms” of Git hosting operations all at once. Many continuous integration systems (like Elastic Bamboo ) can also spin up large numbers of elastic build agents in a cloud environment like Amazon EC2. This can provide vast amounts of CPU resources in a short time to get through the build queue, but can overwhelm a Bitbucket instance that’s not provisioned with enough resources to handle the peaks. Horizontal scaling If you have a standalone Bitbucket Server instance, the only approach available to you to handle the sharp spikes in load (for instance, from CI systems like Bamboo or Jenkins) is to buy a bigger machine, with more cores and more memory. Scaling up your instance “vertically” like this certainly does work, up to a point. But there’s a limit on how far you can scale with one machine, and putting “all your eggs in one basket” by buying one very large (and often very expensive) high end machine isn’t always a good idea. With Bitbucket Data Center, though, you have another option: you can also scale your instance out “horizontally” by adding machines to form a cluster of many nodes, behind a load balancer. Bitbucket Data Center co-ordinates all the machines in the cluster seamlessly, making it appear externally as one single instance of Bitbucket (only larger). It’s very easy to add new nodes to the cluster at any time, even while it’s running. Every node you add scales up the amount of CPU and memory resources available for both Bitbucket and Git. The more CPU and memory resources you have, the greater the capacity of your instance to handle load before performance starts to degrade. The Elastic Experiment Executor (E 3 ) At Atlassian we run a busy production Bitbucket Data Center instance of our own that hosts nearly all of the repositories used by our dev teams. Atlassian’s build engineering team is always looking for ways to improve performance and regularly ask about scenarios that would have a drastic impact on performance. _”What if we added another 300 build agents to one of our Bamboo instances? Will Bitbucket be able to cope? How many more nodes would we need?”_ Naturally, the answers depend on many variables. Until recently our approach to capacity planning was to initially deploy a few (usually two) reasonably large nodes. Once we noticed performance degradation during our busiest times we would ask IT to provision another node. This approach works (to a point), but is far from ideal. Because, while we do proactively monitor the load and response times in our production instance, it can take some noticeable performance degradation (and some unhappy users complaints) before any action is taken. In addition, while we wait for IT to provision some new hardware, our coworkers could be left twiddling their thumbs waiting for Bitbucket to respond and their builds to run. We eventually needed something more scientific than the “try it and see” approach, so we wrote a performance testing tool that let us spin up any configuration of Bitbucket, throw a repeatable workload at it of any desired size and shape, and observe its throughput, response times, and other vital statistics while it runs. This tool eventually became known as the Elastic Experiment Executor (or E 3 for short). Not only does E 3 let us measure the performance of different hardware configurations, it also allows us to take different versions Bitbucket, or instances configured with different options, and compare them side-by-side. After only a few hours the tool gives us accurate performance data that used to take days or even weeks for someone to crunch out manually. With E 3 , developers also have an easy way to run controlled performance experiments on their local changes before they merge them to master, so our teams has better data, and much more confidence about that data, that each new version of Bitbucket will perform as well or better than its predecessor before we roll it out to our production environments. For an example of the kind of performance data we can get using E 3 , here’s a chart of the overall throughput of eight different Bitbucket instances (with cluster sizes ranging from 1 node to 8 nodes) under ten different load levels. This chart shows Bitbucket Data Center’s actual sustained tests per second (TPS) performance in 80 separate test stages, each one representing a different cluster size and load level. A “test” here means any operation (such as a git clone , loading a page, or hitting a REST endpoint) successfully executed against the Bitbucket instance under test. The operations are taken from a typical mix of interactions including Git hosting operations, browsing, interacting with pull requests, and so on. E 3 also produces charts that show the breakdown of individual operations in the workload mix visually, and how much each one contributed to the overall “TPS” value. To put a Bitbucket instance under enough stress to reach its capacity limit, you generally need to generate load from a cluster of many client machines, each running many threads, with each in turn doing many operations back to back: and this is what E 3 provides automatically. (This particular test used 10 high end machines running up to 400 client threads, which is the kind of workload that 20,000 – 40,000 typical users and build agents would generate in a busy period.) Charts like this don’t only show how Bitbucket Data Center’s capacity scales horizontally out to 8 nodes on this particular infrastructure. They also give us an easy way to estimate the maximum capacity that a clustered instance with a given number of nodes can sustain before response times start to suffer. And suffer they do, as the following E 3 chart from the same experiment shows. Experiments like these also give us confidence that the Bitbucket application itself and other infrastructure (database, file server, network, OS, and so on) it’s deployed on don’t have hidden bottlenecks that might limit the capacity and scalability of a very large cluster. You might wonder where E 3 gets the wherewithal to provision eight real Bitbucket Data Center clusters and eight client clusters, consisting of more than 100 machines, together with databases, load balancers, and all the rest. Atlassian’s performance engineering team does operate a “modular performance lab” with racks of high end, bare metal machines which we have sometimes used for such testing. But when we really need large numbers of machines at short notice for tests like this, we usually go straight to Amazon EC2. EC2 lets us quickly spin up topologies of machines with different sizes and shapes (i.e, balances of CPU, memory, and I/O) for specific performance tests, far faster than we could ever do with physical machines. E 3 has the capability to provision any or all of the resources it needs, both the instances under test and the client workers, in EC2 using Amazon CloudFormation templates with any desired combination of parameters. E 3 is easy for developers to use: All we need to do is define our “experiment” in a JSON formatted file. Then with one command, E 3 can spin up all the machines it needs in Amazon, run all threads of the experiment in parallel, download the results and statistics (gathered with collectd ) off the machines, analyze them, and finally tear all the instances down again when their work is done. We’ve made the complete source code to E 3 , as used by Atlassian’s development team, publicly available here: https://bitbucket.org/atlassian/elastic-experiment-executor/ The experiment file we used for the above charts can be found in E 3 ‘s standard experiment library, called horizontal-scaling.json . For any of the E 3 -based performance charts in this series, you can go and check out the corresponding experiment from this repository and not only see our exact methodology, but (if you have enough machines of your own or are willing to spin some up in AWS) can even run the same experiment for yourself and reproduce similar results for your own infrastructure.", "date": "2016-12-01"},
{"website": "Atlassian", "title": "How we built Bitbucket Data Center to scale (part 2)", "author": ["Michael Studman"], "link": "https://blog.developer.atlassian.com/bitbucket-adaptive-throttling/", "abstract": "This post is the second part of a series on how we built Bitbucket Data Center to scale. Check out the entire series here Throttling Git In our previous post we saw that when an under-provisioned Bitbucket instance starts hitting capacity constraints, performance degrades for all users. This usually manifests as longer page loading times and Git hosting response times as Bitbucket and many git processes vie for scarce CPU and memory. All versions of Bitbucket since the very first release have applied a simple and effective strategy for dealing with this problem: Limit the number of Git hosting operations that are allowed to run concurrently. If too many users (or, more likely, build agents) are trying to perform too many Git hosting operations all at once, only some of them are processed immediately. The rest have to wait in a queue until the overall number of running Git hosting operations falls back below the limit again. The part of Bitbucket that enforces this limit is called the _Throttle Service_. The way the Throttle Service works is to make each hosting operation acquire a ticket from a fixed pool of available tickets before it is allowed to proceed. The ticket pool acts like a car park with a fixed number of parking slots. When a car arrives the driver can enter and park only if the car park is not already full. If it is full they must queue outside until they are at the head of the queue and there is at least one vacancy. If they queue for too long they may give up and go home. Throttling in Bitbucket works in much the same way: there are a fixed number of tickets and each Git operation must try to acquire one before proceeding. Operations that find there are no tickets left will queue for one (i.e., until the next Git operations finishes) or if delayed for too long will give up causing the Git client to receive a failure message. Operations aren’t allowed to build up in the queue indefinitely. If an instance comes under so much load that Git operations are stuck in the queue for more than 5 minutes, they are rejected with a message telling the user to try again later. With throttling, Bitbucket not only prevents a few aggressive build agents from degrading performance for everyone, it can even improve the overall throughput for hosting operations too. This may seem counter-intuitive, but just think what can happen if you (say) double the concurrency level of a workload: If this pushes the system into “thrashing” due to CPU and/or memory exhaustion, it can easily lead to each job in the pipeline taking more (sometimes much more) than double the time to run, leading to lower throughput overall. The only tricky bit was deciding how many tickets Bitbucket should allow in the pool. If it allows too few, users pushing to or pulling from repositories might have to wait longer (or have their requests rejected) even though the server’s resources aren’t under pressure. If it allows too many, a system could experience resource exhaustion, and performance could suffer for everyone. The “optimal” number for an instance in all situations can actually depend on a lot of factors, like the CPU speed, I/O performance, network speed, and even the average size of repositories in the instance. The decision we took was to let the limit be configured by the administrator, with the default calculated by a simple formula: 1.5 x the number of CPU cores available in the machine. This scales the available ticket count with the size of the machine, so larger machines have more available tickets and smaller machines, fewer. This approach to Git operations has served Bitbucket quite well and the default has been a reasonable choice for many instances. But for some with ample hardware resources and a hosting load consisting of many small, light Git operations, the “one size fits all” approach of a single fixed limit can sometimes lead to underutilization of the server hardware. This diagram from a Datadog console for one of our production Bitbucket clusters shows such a situation. In the bottom graph dark blue represents the number of held tickets or Git operation in progress, aqua represents the fixed total number of tickets, and lime represents the number of queued requests. Here too many lightweight Git operations have arrived too quickly and instantly exceed the fixed ticket count meanwhile the CPU remains underutilized. This server could be processing more operations but fixed throttling is causing them to be queued instead. Users and CI servers are being held up but the server is only just warm. So finding the optimal ticket limit for all instances in all situations was sometimes harder than we’d like. In Bitbucket 4.11, we set out to change all that… Introducing Adaptive throttling In 4.11 we introduced a new feature called adaptive throttling which we believe goes a good way towards achieving this goal. Adaptive throttling in Bitbucket 4.11 works a bit like cruise control in a car. Just as cruise control can achieve a steady speed by applying the accelerator or brakes when it detects that the car’s speed is higher or lower than the desired level, Bitbucket 4.11 also monitors load in your system continuously, and “opens up the throttle” (i.e., allows more Git hosting operations through) when it detects that the system is lightly loaded, and “hits the brakes” (by allowing fewer Git hosting operations) when it detects that the system is under stress. Compared to fixed throttling, this strategy gives you the best of both worlds: When your machine has capacity to spare, Bitbucket allows as many concurrent Git hosting operations as you can throw at it. But if it starts to strain under the load, Bitbucket dynamically detects this and winds back the ticket limit, protecting the overall responsiveness of the whole system. The metric that Bitbucket uses to measure the “stress” of the system is the overall CPU utilization, as reported by the operating system. Bitbucket monitors this statistic continuously (smoothed to filter out noise), as well as the actual number of Git hosting tickets in use at the time. Bitbucket aims to achieve a constant CPU target (75% by default) if there is enough workload to sustain it. An estimate of how many more Git hosting operations the instance could sustain and still remain under the target is calculated by applying a simple linear extrapolation formula. (This of course has to assume that all Git hosting operations consume about the same amount of CPU on average, an assumption which actually turns out to hold pretty well in practice.) This formula gives a value that Bitbucket then applies as the new total ticket count. Bitbucket keeps dynamically recalculating the new ticket count all the time, allowing it to respond to spikes or other sudden changes in system load in just a few seconds. The following diagrams illustrate how the fixed and adaptive throttling algorithms respond to different kinds of load spike. If a flurry of “light” Git hosting operations hits the instance (as in the left hand side of this chart), Bitbucket can queue requests even though the machine still has plenty of CPU resources to spare. If on the other hand a spike of “heavy” (i.e., CPU intensive) Git hosting operations hits the instance (as in the right hand side), Bitbucket doesn’t queue the requests enough, and they hog all the available CPU at the expense of other operations. This can impact page load times and general responsiveness of the instance. Flurries of “light” Git hosting operations are allowed to proceed without queuing so long as the CPU utilization stays under the 75% target. But for “heavy” Git hosting operations, adaptive throttling detects the CPU load exceeding the target and “slams on the brakes” by reducing the Total hosting ticket limit. This causes queuing of Git operations, protecting the system from extended high CPU load. The 75% CPU target is configurable, and represents a balance between throughput and responsiveness. A higher target can squeeze more throughput out of the instance, but potentially at the expense of response times. A lower target keeps the system very light and responsive, but can lead to more queuing of Git hosting operations. Since Bitbucket now adapts the ticket limits depending on circumstances, it also needs safeguards so that extreme circumstances don’t result in extreme over-reactions. For instance stubbornly high CPU should not completely starve Bitbucket Server of Git operations no matter how poorly the server is coping. (The high CPU could easily be from some process unrelated to Bitbucket so reducing the ticket limit all the way down to zero probably wouldn’t have the desired effect anyway.) Nor should a burst of initially-idle Git operations be able to fool adaptive throttling into waving through even more operations only to have a CPU and memory crunch come down the line once they all wake up. To this end adaptive throttling allows you to specify the minimum and maximum range for the ticket limit. The minimum (a default of 1 x CPU cores) establishes a base service level for concurrent Git operations. The maximum (a default of 12 x CPU cores) acts as a safety net. Also note that while adaptive throttling chooses CPU load as its “north star” in terms of how it should adapt the ticket limit, it also takes machine memory into consideration. On startup Bitbucket Server looks at the total system memory and does a simple calculation of how many Git processes could fit in memory resident with Bitbucket and its search service. For some low-memory systems this may reduce the maximum operation limit from its default or configured value but is generally necessary to protect against ticket limits that could not be achieved, realistically, from the available hardware. Monitoring in production So how does Adaptive throttling perform in a real instance? Within Atlassian, we monitor both the machine level stats (CPU, memory, IOPS, etc.) and many JMX statistics on all our production Bitbucket instances with New Relic and Datadog . The JMX statistics in particular give us excellent insight into what’s going on “under the hood” in our instances. The JMX MBean exposed by Bitbucket for the throttling algorithm is called com.atlassian.bitbucket:name=HostingTickets . The Total attribute is the maximum number of Git hosting operations that Bitbucket calculates each node can cope with, the Used attribute is how many are in flight, and the QueuedRequests attribute indicates how many requests are waiting for a hosting ticket. The following 24 hour window from our New Relic console shows the spikiness of the Git hosting workload on one of our production instances in a typical workday. The proportional control algorithm in adaptive throttling has done a pretty good job at estimating the Total number hosting operations a Bitbucket node can cope with (line in green), both in busy periods and quiet ones. Thanks to this kind of monitoring, we have been able to tune our CPU utilization target and bring down the number of QueuedRequests (line in blue) on this instance, without harming responsiveness. This instance, by the way, is running on 8x CPU cluster nodes with 24 GBytes of memory each. Under the previous fixed throttling algorithm, the default Total hosting ticket count would have been 12. With Adaptive throttling, we now can (and do) allow up to 8 times this number. The throughput of Git hosting operations is much higher as a result, and request queuing on this instance is now extremely rare, happening only very briefly if it gets hit by a particularly bad “build storm” from one of our fellow dev teams with a lot of builds (ahem, Confluence…). This graph from our Datadog console show a much shorter window of ten minutes (from a different period) on the same instance. By zooming in we can see more clearly how adaptive throttling responds to sustained periods of high CPU. In the bottom of the two graphs the dark blue represents Used tickets, aqua represents Total tickets, and lime represents QueuedRequests . We can see that at around the 00:21 mark there is a modest rise in the number of Git operations followed by a sudden spike in CPU where it hovers around the 100% mark for 2 to 3 minutes. In response adaptive throttling drives the Total ticket count from around 70 down to its configured minimum of 8. This causes a spike in queued requests but is enough breathing space for the CPU-intensive Git operations to complete, allow CPU load to fall, let Total tickets rise again and allow queued requests to begin. JMX is an extremely powerful tool to monitor the capacity and general health of a Bitbucket instance, and there are many tools out there that can monitor the JMX statistics exposed by Bitbucket. Apart from com.atlassian.bitbucket:name=HostingTickets , some of the key MBeans in Bitbucket that we monitor all the time are: Some tools, like Datadog, can be set up to send alerts to a HipChat room if a threshold is crossed. The Bitbucket dev team uses these as an “early warning sign” of possible trouble.", "date": "2016-12-02"},
{"website": "Atlassian", "title": "How we built Bitbucket Data Center to scale (part 3)", "author": ["Michael Heemskerk"], "link": "https://blog.developer.atlassian.com/bitbucket-caches/", "abstract": "This post is the third part of a series on how we built Bitbucket Data Center to scale. Check out the entire series here SCM cache As we saw in parts one and two of this series, build agents are notorious for generating “storms” of git clone and fetch operations: One push can sometimes trigger many builds, each consisting of many parallel stages, so the same repository can end up being cloned very many times in quick succession. The repetitiveness and relatively high cost of these clones makes them perfect candidates for caching. Ever since version 1.2, this is exactly what we’ve done. Whenever Bitbucket receives a git hosting request, we generally spawn a git process and hand it off to that. But that’s not all we do. We also “listen in” on the connection, decoding all the exchanges between the git server and client at the packfile transfer protocol level. If Bitbucket determines that the request is a clone and potentially cachable, then while streaming the generated pack back to the client we also save a byte-for-byte copy of it to a file on the cluster node’s fast local storage. The next time an identical clone request is received for the same repository, we can simply stream the same pack, byte-for-byte, back to the client from the cached file, without even having to start a git process. The part of Bitbucket that does all this is called the _Source Code Management cache_, or SCM cache for short. The SCM cache saves a lot of CPU and memory in a typical Bitbucket instance, allowing it to handle a much greater load of clone operations on the same hardware. Under typical workloads, the SCM cache is highly effective. On our own production instance, the hit rate is generally as high as 80%. When a push is received: SCM cache in a Nutshell The introduction of the SCM cache has another interesting benefit, even when it misses. On a cache miss, the cache serves as a buffer between the server git process and the client, effectively decoupling the two. The server git process output can write the packfile to the cache as quickly as it likes, without having to wait for the client to read all the data over the network. Meanwhile, the client request is being served out of the cache, and can take as long as it likes without holding up the git process on the server. This decoupling allows the server process to finish and release the memory to the operating system much more quickly, greatly reducing memory consumption overall. SCM cache is especially beneficial in a clustered Data Center instance where all the repository data is stored on a shared NFS volume. The caches are stored on a fast local disk on the cluster node. A cache hit replaces the network I/O required for generating the pack data with local I/O for reading from the cache, thereby reducing the demand on the NFS server and network bandwidth. The graphs below show how the introduction of the cache affects the CPU and memory usage on the server for clones. All scenarios (use the selector to choose one to display) clone a 250 MByte repository over a 15 Mbit DSL connection. The limited bandwidth means that the git process on the server frequently has to wait for the remote client to read data before it can continue. Clone scenario No Cache SCM cache (miss) SCM cache (hit) In the “no cache” scenario, the git process on the server stays alive until the client has received all of the requested data (just under 140 s) and consumes CPU and memory during this whole time. In the “cache miss” scenario, the git process on the server streams its output directly to the cache and can finish much more quickly (after about 20 s). It then takes another 120 s for the pack to be sent to the remote client, but during this time there is no git process on the server consuming CPU and memory. The total git CPU time is about the same in the “cache miss” and “no cache” scenario, but in the “cache miss” scenario it is compressed in a much shorter time window (20 s instead of 140 s). Looking at memory consumption, the “cache miss” scenario is much better; the git process still consumes up to 700 MBytes, but releases the memory much more quickly, freeing up resources for other processes. The “cache hit” scenario is the absolute winner: it consumes minimal CPU and memory because git does not have to compute the pack at all; it can simply be streamed from the cache. In recent versions (4.10 and 4.11) we’ve made further improvements to the cache eviction mechanism to ensure that “hot” repositories remain in the cache longer. When the disk cache starts getting full, caches that have not been accessed for a long time are the first to be cleared. This further increases the cache hit ratio and effectiveness of the cache. Loose coupling While we’ve focused a lot of attention on Git hosting load, that doesn’t mean we’ve ignored performance and scale across the rest of Bitbucket. We’ve also spent a ton of effort making sure Bitbucket Data Center scales not only git operations, but all kinds of operations horizontally across large clusters. First of all, a bit of history on the development of Bitbucket Data Center. Since the beginning, Bitbucket Data Center has used Hazelcast internally to distribute many of its subsystems across the cluster. Using Hazelcast let us build “cluster awareness” relatively easily into every subsystem that needed it, and tune all our various clustered data structures for the best balance of consistency, availability, and tolerance of network conditions. As we’ve deployed Bitbucket Data Center to greater and greater scale, though, we’ve increasingly found it preferable to minimize the dependence of cluster nodes on each other wherever possible. The clustered data structures provided by Hazelcast are convenient for sharing data across multiple cluster nodes, but they can sometimes come at a cost: “tight coupling” between nodes and occasional delays where one node has to wait for another to respond. In Bitbucket, contention between cluster nodes on the same data is actually quite rare, so it makes a lot of sense for us to use more “loose coupling” between nodes and “optimistic” concurrency control strategies wherever possible. Loose coupling not only reduces the network traffic between nodes and improves performance, but is generally good for the health especially of large clusters, as any delays on one node (for example, due to network issues, OS issues, or GC pauses in the JVM) can’t affect request or other processing on other nodes in the cluster. An example of where we’ve evolved towards looser coupling is in our Hibernate second level (L2) cache implementation. This cache accelerates all Bitbucket’s accesses to database records and queries, and must be kept both transaction aware and consistent across the cluster. Our Hibernate L2 cache implementation is provided by Hazelcast, and can be configured in two modes: DISTRIBUTED and LOCAL. DISTRIBUTED mode pools the memory of all cluster nodes to provide a very large cache, but does mean that some cache accesses may need to communicate with another cluster node to get or put the data they need. LOCAL mode allows each node to manage its own memory and cache records and queries independently, with node-to-node communication only needed for invalidating entries to maintain consistency. We spent quite some time optimizing and stress testing both DISTRIBUTED and LOCAL mode cache providers (which incidentally we also contributed back to Hazelcast), and for us LOCAL mode was the clear winner. Now, all our Hibernate L2 caches are configured in LOCAL mode by default. The effect on communication between cluster nodes when we rolled this change out on one of our internal instances (shown by the red arrow) was dramatic: LOCAL mode caches also give significantly better overall throughput and response times, especially in large clusters. This controlled E 3 experiment (called loose-coupling.json ) shows the improvement with a typical workload mix against a couple of different cluster sizes configured in LOCAL and DISTRIBUTED modes: The Hibernate L2 cache is one example, but there have been a host of other areas where we’ve optimized Bitbucket for looser coupling across all our subsystems in recent releases. Our background job scheduler (which used to be implemented using Quartz , but now uses Atlassian Caesium ), key parts of Bitbucket’s pull request, comment drift and SCM cache algorithms, and a range of other caches across Bitbucket core and applications have all been optimized for loose coupling in a similar way. Conclusion The result: for both Git hosting load and general interactive load with tens of thousands of users, we’ve put in a ton of effort to make the latest release of Bitbucket Data Center (4.11 at the time of writing) the most performant and scalable Bitbucket yet. And with the E 3 tool now public anyone in principle can run the same performance tests that the Bitbucket development team uses, and see the performance improvements for themselves.", "date": "2016-12-05"},
{"website": "Atlassian", "title": "Submit today to speak at AtlasCamp in Barcelona", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/submit-today-to-speak-at-atlascamp-in-barcelona/", "abstract": "AtlasCamp is Atlassian’s premier developer conference, bringing developers from all over the world. This year we’re featuring the first Atlassian Summit in Europe and AtlasCamp together. Here’s your chance to join us May 2-5, 2017 in Barcelona, Spain. Share your knowledge, experience, and awesome success stories. Our call for speakers is currently open and will be closing soon on January 31, 2017. We want to see you presenting on our biggest developer stage. This year we have four tracks we’re offering attendees: Build Amazing Add-ons Developer Best Practices Advanced Add-on Techniques Growing your Add-on Business We’ve compiled a list of suggested topics for each track. Topics we know our attendees will want to hear about. Use these ideas to get the juices flowing, and to help you create an amazing talk proposal that you know we’ll love. Get up-to-speed on building add-ons with Atlassian Connect and Plugins 2. Learn how to build amazing add-ons that integrate or extend Atlassian products. Sample topics include beginner/intermediate and product specific topics: Fundamentals of building an Atlassian Connect add-on New ideas on how to make add-ons and services work better together Unique use cases of other services integrating with Atlassian APIs Learn best practices from other developers to make you a better developer. What’s the right way to secure your app? How can you make your app faster? What’s the best way to architect your app to scale to handle high traffic? Sample topics include: Cloud development Security Performance Developing for scale New for 2017 – Dive deep with the greatest minds in the Atlassian Ecosystem. Learn from the experiences of other ecosystem developers when developing or operating their add-ons. Sample topics include: Problems you ran into developing your add-on and how you solved it How are you handling webhooks and securing them against denial of service attacks What is your approach to security? New for 2017 – Guides and success stories to grow your Atlassian add-on business. Learn what it takes to optimize your marketplace listings and make your add-on stand out. Hear from customers about how add-ons change their business. Sample topics include: Optimizing your Marketplace listing Marketing to DevOps teams Growth Hacking Developer Stories: How other Marketplace vendors grew their Atlassian business Head on over to our call for speakers page and submit a talk today. Submit your talk Ralph is a Developer Advocate for ecosystem developers.", "date": "2017-01-23"},
{"website": "Atlassian", "title": "Fast CDN-based repository clones on Bitbucket Cloud", "author": ["Erik van Zijst"], "link": "https://blog.developer.atlassian.com/bitbucket-cdn-cloning/", "abstract": "Git and Mercurial were both started in April of 2005 by Linux kernel developers. Both are fully distributed, similar in technical design, and both have grown large communities over the last 12 years. A few years later dedicated code hosting sites began to spring up around these new source control systems. Bitbucket Cloud was one of those early sites, launched in 2008 as a Mercurial-only service. A lot has happened since then. Today Bitbucket Cloud is one of the largest, and busiest, DVCS services, seamlessly working with both Mercurial and Git. Our aim is for Bitbucket to abstract away most of the underlying SCM specifics in favor of a unified interface. That said, under the hood we try to take advantage of both Git and Mercurial’s individual strengths. To this end we recently did some work to improve our Mercurial integration. When cloning a repository, both Mercurial and Git collect all the repo data the client needs and then generate a custom, compressed file that the client can apply locally. This is a very expensive process, especially for larger repos and most of our server-side resources are spent serving clone and pull operations. Luckily as of 3.6 (2015), Mercurial offers the ability for the server to host a pre-bundled snapshot, or clonebundle, of the entire repository externally. When cloning, clients then automatically download the clonebundle first, immediately and transparently followed by a pull for any recent changes not in the bundle. On the client you can tell when a clonebundle is being applied: Here you can see that the clonebundle did the heavy lifting, providing the client with 89375 change sets, while Bitbucket only had to provide an additional 9 through the subsequent pull, making what would normally have been a very costly, CPU-bound server-side operation effectively free. The additional benefit is that since clonebundles are static files, they can be served much quicker than when they were generated on the fly, leading to substantially faster clone times in some cases. The latency of the additional HTTP request to fetch the bundle tends to outweigh the benefit of the faster download on very small repos and so you may not see a clonebundle for every repository. Besides using Mercurial 3.6 or higher, there is nothing you need to do to take advantage of clonebundles. The past few years also saw an improvement in compression efficiency through a modification in Mercurial’s revlog file format. A revlog file is a series of revisions. When a new revision arrives, its content is compared against the previous revision in the file and a zlib-compressed delta/diff is generated and appended to the revlog. Similar to MPEG video’s “key frames”, the length of the “delta chain” is kept in check by occasionally storing a full revision. Unfortunately a revlog linearizes the DAG, interleaving revisions from parallel branches, and so a revision’s physical parent in the revlog may not be its actual logical parent. Diffing revisions of diverging branches can lead to larger deltas in the revlog, affecting the overall compression ratio. In version 1.9 (2010) this was addressed with the introduction of “GeneralDelta”, which adds a logical parent pointer that allows deltas to be against their true logical parent, leading to smaller repos. New repositories created with 3.7 (2016) or higher automatically use this new format, however old existing repos would need to be converted first. Over the holiday season we have done this on our end and so every repository on Bitbucket has become a little more efficient. Mozilla’s Gregory Szorc (incidentally also responsible for clonebundles) offers a ton more interesting detail on his blog . When data is pushed or pulled, the bundle protocol is used. The original protocol however had limited extensibility and predated later features like bookmarks, phases and obsolescence, which lead to multi stage, non-atomic pushes that in rare cases could lead to race conditions. The newer bundle2 protocol has addressed these and other issues. It also supports GeneralDelta, reducing not only the chattiness of the original protocol, but offering better compression too. Until now, Bitbucket exclusively supported the original bundle1 protocol, but with the migration to GeneralDelta, we have also enabled bundle2. Clients running 3.6 or higher will now automatically use it. Older bundle1 clients remain supported too.", "date": "2017-02-02"},
{"website": "Atlassian", "title": "JIRA 7.4.0 EAP Now Available", "author": ["Wazza Thompson"], "link": "https://blog.developer.atlassian.com/jira-7-4-eap/", "abstract": "The JIRA development team is proud to announce that the JIRA 7.4 Early Access Program (EAP) release downloads are now available. We’ve prepared separate EAP downloads for JIRA Software and JIRA Core, which include important functional changes such as additional project administrator permissions and a Spring 4.2 upgrade. Download the JIRA 7.4 EAP today , and check out the preparing for JIRA 7.4 development guide to start testing your add-ons for compatibility. EAP releases are early previews of JIRA during development. We release these previews so that Atlassian’s add-on developers can see new features as they are developed and test their add-ons for compatibility with new APIs. The number of EAP releases before the final build varies depending on the nature of the release. The first publicly available EAP release for JIRA 7.4 is EAP01. Developers should expect that we will only provide a few EAP releases before the final RC release. The JIRA 7.4 EAP is best explored hand-in-hand with the Preparing for JIRA 7.4 development guide . The development guide contains all changes in JIRA 7.4, and important new features that may affect add-on developers. If you discover a bug or have feedback on JIRA 7.4, please let us know. Create an issue in the JIRA project on jira.atlassian.com with the component JIRA 7.4 EAP.", "date": "2017-05-08"},
{"website": "Atlassian", "title": "Four great improvements to the JIRA Cloud API that you’ve been asking for", "author": ["Eve Stankiewicz"], "link": "https://blog.developer.atlassian.com/whats-new-in-jira-cloud/", "abstract": "The very first European Summit followed by an exciting AtlasCamp is in the books. It was a busy time, but we’re taking steps to make sure we’re regularly communicating our progress to people building add-ons and integrations for JIRA Cloud. You might have seen us engaging more regularly in discussions on the developer community. We revealed a lot of improvements during the Build a Better JIRA Add-on session at AtlasCamp. In addition to communicating new features, we also give you visibility into our development priorities. Recently, the JIRA Cloud team opened up their roadmap of plans and projects for developers, so that you can stay up to date with what we’re working on. Check out the JIRA Cloud Platform API Roadmap board , which covers improvements to Atlassian Connect for JIRA Cloud, JIRA Cloud APIs, and other services that add-ons and integrations for JIRA Cloud rely on. If you want to provide feedback or ask about a specific project, feel free to comment on the project’s card or on the linked ACJIRA issue. I’ll take this opportunity to follow up on those announcements and provide a few new ones. The ability to use the product in a language that is most comfortable for the user is one of the keys to improve users’ experience and increase their engagement. That’s why we’re making investments in improving the quality of already available languages as well as working on official support for more languages in JIRA Cloud. And we also want to give you the opportunity to provide a seamless journey for non-English speaking users of your JIRA add-ons. We’re happy to announce that support for internationalization in Atlassian Connect is now available for you to implement in your add-ons, as explained in the new section in the Atlassian Connect docs. You can already get started adopting it, while we keep rolling out the feature progressively to all Atlassian Cloud sites from now until the end of June. We know that many Atlassian Connect developers were suffering because webhooks for different types of issue events weren’t granular enough, or weren’t being sent at all. This was a big gap for many add-ons and integrations. For instance, the JRACLOUD-8505 request to add a new \"Issue Linked\" event for listeners to respond to , with over 500 votes made it to the top webhooks related suggestion on jira.atlassian.com. Similarly, issue attachments were another highly desired improvement by JIRA Cloud add-on developers. Without those webhooks, it wasn’t possible for JIRA Cloud add-ons to react when attachments and links were added or removed from issues unless there was a comment added at the same time. As a workaround, add-ons that relied on links-based automation had to frequently pull down all issues just to check for updates in issue links. Unfortunately, this solution required more work from add-on developers and resulted in worse performance for JIRA users. We’re pleased to announce that webhooks for both issue link and attachment events are now available for JIRA Cloud. With this improvement, you can much more easily set up custom integrations using IFTTT, Zapier and other similar services, or have your add-on immediately notified with new issue links and attachments. Check out the JIRA Cloud developer documentation for more information on using issue links and attachment webhooks. Another extension, which has been available for JIRA Cloud developers since the end of April, is support for keyboard shortcuts for add-ons. Many JIRA users tell us that keyboard shortcuts are an invaluable productivity booster for them. Now add-ons for JIRA Cloud can declare keyboard shortcuts that will allow users to quickly navigate to a page, open a dialog module or invoke a web item action of the add-on. Refer to the JIRA Cloud developer documentation to learn more about implementing keyboard shortcuts.", "date": "2017-06-07"},
{"website": "Atlassian", "title": "Aliasing authors in Git", "author": ["Ian Buchanan"], "link": "https://blog.developer.atlassian.com/aliasing-authors-in-git/", "abstract": "So you’ve got a Git repository where the same person has used multiple emails. Maybe some commits were created with a mistake in the Git configuration. Maybe you used a home email address by mistake. Maybe you are merging with a new company so your old email address isn’t valid anymore. However it happened, you want to fix it the Git way. If you don’t, then you are losing the portability of the Git repository. In other words, you want the mapping of emails to work just as well on the repository manager, like Bitbucket, as you do with local tools like git log . The Git solution is the .mailmap file . The .mailmap file is a way to tell Git about user aliases. You might find that Git repository managers, like Bitbucket Cloud and GitHub, allow you to add multiple email addresses to your account. While that may change the web view of which user made commits, it doesn’t change the information stored within Git. So if you ever move your repo to another repo manager, you have to setup all those aliases again. Moreover, your local history will still look like commits came from multiple people. Why not rewrite Git history ? This can create a number of problems. Rewriting history, even for just authors, changes the commit hash. This invalidates any signed commits. It also requires everyone with a clone to need to rebase their branches, and maybe reclone. Atlassian generally recommends not changing Git history for commits that have been shared with other people. The advantage of .mailmap is that it’s a built-in capability for Git. Even if not a perfect solution, it avoids the problems of rewriting history, while providing a standard way to signal that different names or email addresses are really the same committer. To start, let’s check our repository for duplicates with git shortlog . The shortlog command is a special version of git log intended for creating release announcements. This is an easy way to see who’s been working on what. In this case, we’ll use -s to summarize by author, providing a count of commits, and we’ll use -e to show the email of each author. We’ll only need to make a fix if we see duplicate names or emails. This is the result for one of Atlassian’s repositories: We can see a number of variations on people’s names and email addresses. So, it’s worth creating the .mailmap . Let’s use the existing information to create an initial mail mapping file. We’ll simply pipe the previous command through a command to strip the leading summary numbers. git shortlog -se | perl -ple \"s/^s*d+t//\"  > .mailmap Which yields a similar list in a file, but without the number of commits per author: We’ll modify this generated .mailmap file until all the users and emails are aliased the way we want. We can remove the [Atlassian] text from peoples’ names, by just removing it. So this becomes: Effectively, we’re telling Git, “Whenever you see the email in angle brackets on the right, use the name you see on the left.” When the email is the same, we can combine entries to the one we want. So this becomes: Krystian Brazulewicz <kbrazulewicz@example.com> Where the name is correct, we can map one email to another: So this becomes: Neal Riley <nriley@example.com> <nriley@salesengineering.office.example.com> Effectively, we’re telling Git, “Whenever you see the email in angle brackets on the right, use the name and email you see on the left.” Sometimes we need the email alias and a name change: Neal RIley <demo@demonstcomputer.office.example.com> Neal Riley <nriley@example.com> So this becomes: Neal Riley <nriley@example.com> Neal RIley <demo@demonstcomputer.office.example.com> And sometimes we need multiple emails and name changes: Neal RIley <demo@Demonstration-Computer.local> Neal RIley <demo@demonstcomputer.office.example.com> Neal Riley <nriley@example.com> Neal Riley <nriley@salesengineering.office.example.com> So this becomes: Neal Riley <nriley@example.com> Neal RIley <demo@Demonstration-Computer.local> Neal Riley <nriley@example.com> Neal RIley <demo@demonstcomputer.office.example.com> Neal Riley <nriley@example.com> <nriley@salesengineering.office.example.com> After making multiple kinds of changes, our full .mailmap file looks like this: Adam Saint-Prix <asaintprix@example.com> Alex Yakovlev <alyakovlev@example.com> Brent Plump <bplump@example.com> Dan Radigan <dradigan@example.com> dan radigan <dan@radigan.com> David Jenkins <djenkins@example.com> <djenkins@cli-4902.office.example.com> David Jenkins <djenkins@example.com> davidglennjenkins <djenkins@example.com> David Jenkins <djenkins@example.com> EC2 Default User <ec2-user@ip-172-31-43-152.ec2.internal> Krystian Brazulewicz <kbrazulewicz@example.com> Marcin Oleś <moles@example.com> Martin Suntinger <msuntinger@example.com> Matt Shelton <mshelton@example.com> Neal Riley <nriley@example.com> Neal RIley <demo@Demonstration-Computer.local> Neal Riley <nriley@example.com> Neal RIley <demo@demonstcomputer.office.example.com> Neal Riley <nriley@example.com> <nriley@salesengineering.office.example.com> Norman Ma <nma@example.com> Pawel Skierczynski <pskierczynski@example.com> Peter Koczan <pkoczan@example.com> Piotr Święcicki <pswiecicki@example.com> Tim Wong <twong@example.com> Now when we use git shortlog -se : All of this works similar to the .gitignore file. Namely, we can create this file, edit it, and see the results locally, before we commit or push anything. However, once we’re satisfied, we should add, commit, and push the file so that everyone will have consistent behavior. While this is the Git way to fix the problem, it comes with some wrinkles. While shortlog uses this file automatically, other subcommands don’t. For example, if you want to use the full log, you’ll have to use the --use-mailmap argument: git log --use-mailmap If you found this quick tip useful, you might find more interesting tips on Getting Git Right .", "date": "2017-06-27"},
{"website": "Atlassian", "title": "Hands on developer training at Summit", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/summit-us-developer-training/", "abstract": "Got new developers to onboard? Want a refresher course in building cloud add-ons with Atlassian Connect? Read on! At Atlassian Summit in San Jose, we’ll be offering a half-day, hands on training on September 12, 1:30pm to 5pm , on Atlassian Connect. Atlassian Connect is the framework that allows you to build add-ons for our cloud products. You’ll learn what you can do with Atlassian Connect, additional frameworks that make developing add-ons easier, and how you can monetize your add-ons in the Atlassian Marketplace. This training covers: An overview of what Atlassian Connect can do Deep dive into setting up an add-on with the installation lifecycle Learn how to extend the UI of Atlassian products Learn how to call authenticated REST APIs with JWT Build a sample add-on And right now it’s only $99! That price goes up to $199 after 11:59pm PDT on July 15. So don’t miss out! Note: You must be registered for Summit to attend this training. It is not being offered a la carte. Ralph is a Developer Advocate for ecosystem developers.", "date": "2017-07-13"},
{"website": "Atlassian", "title": "Announcing JQL for users beta for JIRA Cloud", "author": ["Simone Russo"], "link": "https://blog.developer.atlassian.com/user-search-language/", "abstract": "In JIRA Cloud, support for user search is limited, as the JIRA Cloud REST API only allows you to search for users by username. We’ve heard regular feedback from developers about the different situations where they need to search for users, but are unable to. JQL would be the ideal solution for this. It is one of the best-loved and most powerful features in JIRA, due to its flexibility and the familiar SQL-like syntax. However, currently JQL can only be used to search for issues. Today, we’d like announce that we’re addressing these problems with the introduction of a new API that brings the power of JQL to user search. You can use the same JQL-like structures and syntax, but the API will return user results. We have implemented a new REST resource that supports searching for users that are assignees or reporters of specific issues or any issues in a project. A few examples: We’re highly interested in adding additional capabilities based on what will be most useful to your add-ons and integrations. These are some of the possibilities for future development that we’ve thought of so far: This feature is in beta and will be turned on upon request. If you’d like to know more and have this enabled on your JIRA instance, feel free to use this google form . We are at the initial stages of developing this API. Let us know what you think. Is this useful? Have you been looking for better ways to search for users in your add-ons? Share your thoughts on our ecosystem community !", "date": "2017-07-31"},
{"website": "Atlassian", "title": "Security Warning (Git,Mercurial,SVN)", "author": ["Matthew Hart"], "link": "https://blog.developer.atlassian.com/security-warning-git-mercurial-svn/", "abstract": "Git, Mercurial and SVN recently released fixes for vulnerabilities in their client-side applications that could lead to remote code execution on the victims machine: Git: CVE-2017-1000117 Mercurial: CVE-2017-1000115 and CVE-2017-1000116 SVN: CVE-2017-9800 If you are using Git, Mercurial and/or SVN, it is strongly recommended that you upgrade to the latest versions to ensure that you are protected from these vulnerabilities. As some of our products use Git, Mercurial and/or SVN, our security team has investigated the impact of these vulnerabilities to our products. Products outside of those mentioned below are not impacted and no further action is required. SourceTree for macOS and Windows that are configured to use Git and/or Mercurial are impacted by these vulnerabilities. We have released two new versions of SourceTree to protect our customers against these vulnerabilities — Windows customers can now update to version 2.1.10 (or higher), and macOS customers can now update to version 2.6.1 (or higher). If you would like more information about the SourceTree vulnerability, please read our security advisory . Bamboo instances that are configured to use Git and/or Mercurial are impacted by these vulnerabilities. Bamboo does not come with Git and/or Mercurial installed by default, however if you have configured your Bamboo Instance to use Git and/or Mercurial, it is strongly recommended that you upgrade to the latest versions to ensure that you are protected from these vulnerabilities. Bamboo Instances are not impacted by the SVN vulnerability. In addition, Bamboo Agents that have vulnerable versions of Git, Mercurial and/or SVN installed are potentially vulnerable to malicious Bamboo build plans and should also have their versions updated. FishEye/Crucible instances that are configured to use Git and/or Mercurial are impacted by these vulnerabilities. FishEye/Crucible does not come with Git and/or Mercurial installed by default, however if you have configured your FishEye/Crucible Instance to use Git and/or Mercurial, it is strongly recommended that you upgrade to the latest versions to ensure that you are protected from these vulnerabilities. FishEye/Crucible is not impacted by the SVN vulnerability. Bitbucket Server is not affected by the Git, Mercurial or SVN issues mentioned on this page as Bitbucket Server does not invoke any of these applications in a malicious way, however it is possible that a third-party plugin is vulnerable and so it is recommended that all installed versions of Git, Mercurial and/or SVN should be updated to non-vulnerable versions.", "date": "2017-08-11"},
{"website": "Atlassian", "title": "Jira Server 7.6 EAP brings important API changes to priorities", "author": ["Tomek Bartyzel"], "link": "https://blog.developer.atlassian.com/jira-7-6-eap/", "abstract": "We've just released the first sample of Jira Server 7.6 as part of the Early Access Program (EAP) to let you know about the changes coming to Jira. It's too early for complete features yet (getting there!), but we're bringing some important changes to priorities, as a first step towards making Priorities per Project a thing. If you develop apps (formerly add-ons) for Jira, or if you're using the priorities' API in any way, make sure you read on here: Preparing for Jira 7.6 . We've prepared separate EAP downloads for Jira Core, Jira Software, and Jira Service Desk, so pick your favorite and download the Jira 7.6 EAP today. EAP releases are early previews of Jira during development. We release these previews so that Atlassian’s apps developers can see new features as they are developed and test their apps for compatibility with new APIs. The Jira 7.6 EAP is best explored hand-in-hand with the Preparing for Jira 7.6 development guide . The development guide contains all changes in Jira 7.6, and important new features that may affect apps developers. If you discover a bug or have feedback on Jira 7.6, please let us know. Create an issue in the Jira project on jira.atlassian.com with the component Jira 7.6 EAP.", "date": "2017-10-04"},
{"website": "Atlassian", "title": "Breaking changes to the Jira Service Desk experimental Java API", "author": ["Matthew McMahon"], "link": "https://blog.developer.atlassian.com/jsd-experimental-api-update/", "abstract": "Jira Service Desk 3.9.0 for Server is almost ready for release. In this release, some methods of our API will be moving out of the experimental state. We’ve recently introduced breaking changes to some of the experimental APIs. We’ve prepared an Early Access Program (EAP) release for developers to start testing against. These changes involve changing the IDs to return integer values rather than longs. An integer value aligns with Jira’s values stored in the database. It also keeps us from having to do a lot of unnecessary type conversion. In preparation for the final release, we’ve also removed all deprecated components. This change cleans up the experimental API and prepares it for it’s recommended state. We have documented all these changes, in-depth on the preparing for Jira 7.6 notes page. Once released, check out the JSD Javadocs for more information on the API. Please download and use the Jira Service Desk 3.9.0 EAP and start testing and validating today.", "date": "2017-10-05"},
{"website": "Atlassian", "title": "Confluence Server 6.5 beta released", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/confluence-server-6-5-beta-released/", "abstract": "This week, we released the first beta for Confluence Server 6.5. There are changes in this release that may directly affect 3rd-party add-ons. To find out what’s in this release, check out the Confluence 6.5.0-beta release notes . Download the beta from our site to start testing your add-on for compatibility with the changes. Be sure to take a look at our Preparing for Confluence 6.5 guide for details on changes that may be relevant for your add-on. We’ll update that page regularly, so keep an eye on it to see any changes. Ralph is a Developer Advocate for ecosystem developers.", "date": "2017-10-20"},
{"website": "Atlassian", "title": "What’s new in the Jira Cloud Ecosystem", "author": ["Eve Stankiewicz"], "link": "https://blog.developer.atlassian.com/whats-new-in-jira-cloud-ecosystem/", "abstract": "We’ve been busy these last few months, we welcomed Stride to our product portfolio and we launched our new brand. In the Jira Cloud team, we’re excited about all these changes, and we introduced a couple of our own. Read on if you want to learn about the new capabilities we introduced. Normally, apps that need information on a large number of Jira users – for example to generate user-related reports – had to fetch the details of each user separately. In order to make it easier for an app and reduce the number of network roundtrips, we created an endpoint for requesting user details in bulk. Now you just need to specify an array of usernames or user keys in the request body, in order to get full representation of multiple users’ details in a single REST query. Refer to the GET /user/bulk API documentation for implementation details. Note that the endpoint is currently in an experimental state, and changes to the API are still possible. One of the characteristics of Jira custom fields is the ability to define the default value, which will be pre-selected when the field is displayed to the user. However, issue fields provided by Connect apps did not have the option to define the default value. This meant that the end users always had to browse through the whole selection of options and choose the right one. Now you can save your users the unnecessary steps and populate the issue fields with default options for any given project. This functionality is available for single select and multi select issue field types. In order to define the default value for a specific issue field, use the defaultValue attribute in the issue field options, as in the example: A few weeks back we released JQL for user-based search , which gives you the opportunity to look at Jira’s data not only as a collection of issues, but also from the perspective of users who work with them. Since the original beta access announcement, we have made the endpoint public (while keeping it experimental) and implemented more options to narrow down the search results. In addition to looking for assignees and reporters of issues in given projects, it is now possible to search for users with specific entity properties value. Refer to the GET /user/search/query API documentation to give it a try and let us know in the related Trello card what other capabilities would make it even more valuable. Up until recently, issue changelog entries referenced issue fields only by the name of the field. That was problematic for apps and REST API clients, especially when field names got renamed or translated. Now, permanent IDs are provided for issue fields in changelog responses, so you can easily identify which fields have changed. In a Jira issue lifecycle, there are situations when the issue cannot be edited anymore. For example when the project has been closed, the issue has been closed or archived. But even though the issue itself should not be modified, users might still want to log or update time tracking information. It is now possible to use the /rest/api/2/issue/{issueIdOrKey}/worklog REST API endpoint with overrideEditableFlag set to true, to add or update worklogs in issues that are in a non-editable state. Note that the flag is set to false by default, and only Connect app users with admin scope permission are allowed to use it. If you attended this year’s AtlasCamp in Barcelona, you may recall Max Mancini, Head of Ecosystem, announcing the adoption of Swagger as our standard for API documentation. Jira Service Desk was the first one, followed by Confluence , to publish their REST API reference in Swagger format on the recently recreated Developer Portal . It is now high time for Jira Cloud to conform and provide the improved, consistent and highly interactive documentation experience to all Atlassian Ecosystem developers. We’re working at full speed on migrating the existing JIRA Cloud REST API reference into a Swagger file and make it available on the Developer Portal. As part of our open culture, we want to help you make informed decisions that impact your businesses. And, therefore, we share our Jira Cloud Ecosystem roadmap with you. It is a publicly visible Trello board, where you can find out what we’ve shipped recently, what we are currently working on, and which can give you an idea of what we’re prioritising for the near and long-term future. You may notice that some initiatives are linked to individual issues in the ACJIRA project , while others are more general and indicate what investments we want to make to support and grow the Atlassian Ecosystem. Insights from Jira app developers are an incredibly important part of our prioritisation, and we refer to ACJIRA project on ecosystem.atlassian.net as one of the most essential source of your feedback. Jira Cloud Ecosystem team reviews all incoming improvement requests on a regular basis to get a clear picture of the gaps and opportunities for apps development in Jira Cloud. We’d like to encourage you to demonstrate interest in individual features by voting on selected ACJIRA issues. As a voter or watcher of the issue, you will also get first hand information that the improvement was delivered to Jira Cloud.", "date": "2017-10-31"},
{"website": "Atlassian", "title": "Announcing the Atlassian platform for developers roadmap", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/announcing-atlassian-platform-developers-roadmap/", "abstract": "Being an open company is core to our culture at Atlassian . It’s the value that expresses how we work together, build our products, and partner with others. We’re committed to delivering a great platform and experience for our ecosystem of developers, vendors, and partners. We appreciate that you build tools and entire businesses that rely on the performance and features of the Atlassian platform. In the spirit of open company and teamwork with our developer, partner, and vendor community, we are happy to announce the release of the Atlassian platform for developers roadmap . Our roadmap shows what we’ve shipped, what we are currently working on, and what we plan to work on. It also shows future initiatives under consideration. As with any product roadmap, this is subject to change . We will keep this updated so that you have the latest insight into our thinking. So be sure to subscribe to the board to get all the latest changes directly in your inbox. Questions and comments on the roadmap can be asked in our Developer Community , your single stop for all ecosystem talk. You can also vote on individual roadmap cards to let us know which product efforts will have the greatest benefit for you. Our product team will be monitoring votes, questions, and comments. The Atlassian platform for developers covers the capabilities that help our Ecosystem developers, vendors, and partners build apps, list them in the Marketplace, and manage their customers. The four major categories are: Ralph is a Developer Advocate for ecosystem developers.", "date": "2017-12-06"},
{"website": "Atlassian", "title": "Ecosystem Roundup – Get our APIs in Postman, Stride API EAP Changelog and more", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/ecosystem-roundup-2/", "abstract": "In this post, we roundup all recent Ecosystem news. A recent feature has been shipped to developer.atlassian.com . In the reference API documentation we now include a Run in Postman button . This will create a collection of the products APIs in Postman that makes it easy to test API requests. This is currently available in the reference docs for If you’ve been invited to develop with the EAP of the Stride API then head over to the Developer Community to check out the most recent Stride API Change log and known issues . We recently saw the release of a few server patch releases: We recently announced the availability of the Atlassian platform for developers roadmap . This is in addition to the Jira Cloud Platform API Public roadmap . As always, if you are seeking help in building your apps there are three avenues available to you. If you seek help, mentoring, or support in building your app please head over to the Atlassian Developer Community where over 2600 developers and over 350 Atlassians are available to help out. If you found a bug, want to raise a feature request, or report an app security incident with any of our APIs, SDKs, or app frameworks please open an issue over at the Atlassian Ecosystem Developer Service Desk . Need help with your Marketplace listing, head over to the Atlassian Ecosystem Marketplace Vendor Support Desk . Ralph is a Developer Advocate for ecosystem developers.", "date": "2017-12-11"},
{"website": "Atlassian", "title": "Confluence Server 6.7 EAP released and important info about upcoming look and feel changes", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/confluence-server-6-7-eap/", "abstract": "We have some news for you regarding Confluence Server 6.7 and some visual changes we’re making. We’re bringing a refreshed look and feel to many of our Server and Data Center products, and Confluence is the first cab off the rank. This work is based on the new Atlassian design and will include things like the updated colour palette, typography and icons. We’re not currently working on any significant changes to navigation, or features like Home, that you may have seen in our Cloud products. We’ll be implementing many of these design changes via an upgrade to AUI 7.x. The AUI Upgrade Guide has details of all the recent changes. In case you missed it, there have also been some changes to AUI licensing, see The license for AUI 7 is changing in our developer community for more info. We’ve aimed to minimise the impact to add-ons as much as possible, however as we release Confluence milestones, you should check your add-ons, and may want to update any non-AUI or non-product UI components to match the design guidelines. Head over to Preparing for Confluence 6.7 for more information, and some sketches of the new look. The first Confluence Server 6.7 EAP milestone is available now! EAP releases provide a snapshot of our work-in-progress and give you an opportunity to test and fix your add-ons before the final release. We’ll be releasing regular milestones and betas during the development of Confluence 6.7, so head to Preparing for Confluence Server 6.7 to find out what’s changed. We’ll update that page each week with any notable changes. Get the latest EAP If you have any problems with an EAP release, please raise an issue to let us know. The earlier we know about problems, the more time we’ll have to fix them before the final release. Enjoy! Ralph is a Developer Advocate for ecosystem developers.", "date": "2017-12-12"},
{"website": "Atlassian", "title": "Alpha program for the new Jira Cloud issue view", "author": ["Giles Brunning"], "link": "https://blog.developer.atlassian.com/alpha-program-for-the-new-jira-issue-view/", "abstract": "One of Jira Cloud’s key components—the issue details view—is getting a redesign, grouping key actions and information in a logical way and making it easier for users to scan and update issues. We’ve already rolled the new issue view out for a small percentage of users on boards and backlogs in Jira Software and Jira Core projects, and it’s also available when viewing Jira issues from Bitbucket . The rollout has so far been limited to those who don’t have apps that appear in the view issue page. The new issue view will eventually be the basis for every issue view in Jira Cloud, including the full page and issue navigator views, meaning there’s an exciting opportunity for apps that extend the issue view. Deploying your app once and having it appear in the same place on every issue view, new-app notifications, and adding apps right from the issue view all mean you can reach a wider audience and offer an even better user experience. We’ll share more information on timing and other details closer to the start of the alpha program. Right now we’re looking for ecosystem partners to join our alpha program, where you’ll get early access to see how your apps look in the new issue view. The sign-up period will run until mid-January, when we expect to start enabling access for the alpha program. To take part, add your details via the link below and we’ll be in touch with more details—API documentation, guidelines, and plenty of examples—closer to the start of the alpha program. I’m interested! Sign me up for the alpha. We share updates with users on our documentation page on the new Jira Cloud issue view . We recommend you visit the page to see what’s changed, and watch it to be notified when we ship new features to users. Below is a high-level overview of what each of these changes mean. The major changes that will affect marketplace apps are the introduction of quick-add buttons and glances . While your existing apps will still display with these new patterns, you’ll likely need to update them to provide a high quality user experience. The quick-add buttons—under the issue summary—give users one consistent place to add content to issues. When a user clicks the quick-add button for an installed app, a panel provided by your app will be displayed below. This quick-add button can be used to display a dialog initially to help customers input some details before loading the web panel below. The purpose of this is to only show apps when there is content available to display, reducing the clutter. Advantages of the quick-add pattern: Ecosystem apps are shown alongside primary Jira features, like attachments and subtasks, so users can quickly and easily build out content about the issue. The structured set of quick-add buttons will adapt to user behavior over time, allowing their favorite apps to be more accessible in their day-to-day work. The Add more option lets users discover and install marketplace apps right from a Jira issue, driving more potential customers to your business. Users can easily see when new apps are installed, increasing the visibility for everyone on the team. The quick-add buttons will replace the current atl.jira.view.issue.left.context location. (Changes are backwards compatible. Apps that use this location will still work in the new Jira Cloud issue view, but may need to be updated to provide a high quality user experience.) Glances appear in the context area of the issue details. This is the right column for the board or full page issue view, but appears just below the quick-add buttons in the backlog or mobile view due to their single-column layout. Glances have two states: summary and detail. The summary can contain small pieces of information, like the number of support tickets linked to the issue and a status. The detail view—shown when the user clicks on the glance—takes over the right column and displays a panel provided by your app with details for the items included in the glance. Advantages of the glances pattern: Marketplace apps can specify whether they want the glance to open the detail view or a modal dialog. Users can focus on the content of the issue whilst also referring to information provided by your app. Initial load times are improved and critical information is displayed up-front, helping users focus. Glances will replace the current atl.jira.view.issue.right.context location. (Changes are backwards compatible. Apps that use this location will still work in the new Jira Cloud issue view, but may need to be updated to provide a high quality user experience.) There will be minor changes to other UI issue view extension locations as they’re implemented in the new design. We’ll continue to support operations web items and the tab panel for the activity section of the issue view. If you have questions about the new Jira issue view or the alpha program, head over to the Jira Cloud section of our community forum and ask away.", "date": "2017-12-13"},
{"website": "Atlassian", "title": "Introducing Cloudtoken: Console Access to Federated Authentication Tokens", "author": ["Shane Anderson"], "link": "https://blog.developer.atlassian.com/introducing-cloudtoken/", "abstract": "Today, we are thrilled to announce that Atlassian is releasing Cloudtoken as Open Source. Cloudtoken is a federated authentication tool for AWS IAM Roles. The new tool allows users to benefit from the security IAM Roles provide, mitigates the one-hour credential expiration and improves the development lifecycle by simulating an EC2 environment available locally. The source is available on Bitbucket and the tool is available as Python packages, installable via pip. Please see the README for more information. But now for a little history… At Atlassian we’ve been using AWS for a while now. Like many organizations, early in our journey we provisioned IAM Users for any staff that required access to AWS. This worked initially and allowed us to quickly get our first accounts and services provisioned but, as we increased our AWS usage, we quickly encountered scalability issues with user management. Specifically there was no automated process to add staff to groups or revoke access if staff moved to another team or left the company. After looking into the issue we decided to implement Federated Authentication and move to using IAM Roles for staff access. Staff were then able to authenticate with our internal Identity Provider , which has knowledge of our internal departmental groupings and is aware when staff members move teams or leave the company. After a staff member is authenticated, they are presented with a list of IAM Roles they have permission to assume. IAM Roles also come with the added security benefit that credentials expire after one hour, which significantly limits the risk posed by leaked credentials. This solution ticked all the boxes and definitely seemed the way forward. Then we hit a roadblock. When using IAM Users it was easy for staff to download their AWS credentials and configure them in the awscli tool or any other AWS application they had on their workstations, but unfortunately there is no easy or standard way to obtain credentials when using IAM Roles. After looking at the existing tools available, we decided that none provided the features or flexibility we required. So, we decided to develop a new tool that would fit our needs. The command line utility Cloudtoken is written in Python, it enables the use of ephemeral AWS credentials provided by IAM Roles on your local workstation. It features a pluggable architecture that allows for custom authentication sources such as ADFS , Shibboleth and SimpleSAMLPHP and custom handling of credentials. Additionally Cloudtoken can be started in daemon mode, which replicates the instance metadata endpoint ( http://169.254.169.254 ) that AWS makes available in the EC2 environment. When run in daemon mode, Cloudtoken will automatically refresh the credentials every 45 minutes to prevent the credentials from expiring. This allows ephemeral credentials to be used for long running tasks, such as API calls made over a period greater than one hour (such as iterating over a large list of objects in an S3 bucket). Having the instance metadata endpoint available in our local development environment allows us to develop applications with a greater degree of confidence that they will behave consistently across environments. Here’s an example of Cloudtoken running in daemon mode: An example of querying the instance metadata endpoint: The Cloud Engineering team at Atlassian would love for Cloudtoken to become the communities tool of choice for command line authentication with cloud providers. We hope the community will assist us in this by contributing code and plugins via the Bitbucket repository and we can’t wait to see what everyone comes up with. The repository is also where people can raise bugs, feature requests and read the documentation.", "date": "2017-12-14"},
{"website": "Atlassian", "title": "Atlassian Connect content integrated with product docs", "author": ["Becky Todd"], "link": "https://blog.developer.atlassian.com/atlassian-connect-content-integrated-with-product-docs/", "abstract": "The Atlassian Connect documentation site has been retired and all of its content has been separated by topic and relocated to product documentation. Connect content is integrated alongside product content in the following places: In addition, we’ve improved how we generate the modules and JavaScript documentation so they always stay up-to-date with each product release. These are now located in the Reference section. Since many of the pages had combinations of Jira, Confluence, and Marketplace content, we’ve added a page to help you find what you’re looking for. You’ll see the page below if you click on a link pointing to the old documentation site: Last year, we changed the way we deliver developer documentation . Since then, we’ve been steadily rolling out changes to our documentation and migrating content onto the new platform. We’ll continue to improve our content going forward, so stay tuned for some big changes! As always, we welcome your feedback and questions . Don’t hesitate to let us know if you have any questions or comments.", "date": "2017-12-15"},
{"website": "Atlassian", "title": "Jira Server 7.7 EAP has been released!", "author": ["Tomek Bartyzel"], "link": "https://blog.developer.atlassian.com/jira-7-7-eap/", "abstract": "We’ve just released the first sample of Jira Server 7.7 as part of the Early Access Program (EAP) to let you know about the changes coming to Jira. With this release, we’re bringing the following improvements: Experimental REST API to let you manage priority schemes outside of the user interface Fixed problems with images pasted into issue’s description New languages: Italian and Finnish Improved code quality: We’ve started converting parts of the Jira Server code to Asynchronous Module Definition (AMD), maintaining the backward compatibility You can read more about these changes in the release notes and the development guide for Jira Server EAP. We’ve prepared separate EAP Server downloads for Jira Core, Jira Software, and Jira Service Desk, so just pick your favorite here: Download the Jira 7.7 EAP . EAP releases are early previews of Jira during development. We release these previews so that Atlassian’s apps developers can see new features as they are developed and test their apps for compatibility with new APIs. The Jira Server 7.7 EAP is best explored hand-in-hand with the Preparing for Jira Server 7.7 development guide . The development guide contains all changes in Jira Server 7.7, and important new features that may affect apps developers. If you discover a bug or have feedback on Jira Server 7.7, please let us know. Create an issue in the Jira project on jira.atlassian.com with the component Jira 7.7 EAP .", "date": "2017-12-21"},
{"website": "Atlassian", "title": "Ecosystem Roundup – Bitbucket Server design changes, Survey results, Holiday presents and more", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/ecosystem-roundup-3/", "abstract": "In this post, we roundup all recent Ecosystem news. We're bringing a refreshed look and feel to many of our Server and Data Center products, and Bitbucket is following closely in Confluence’s footsteps . This work is based on the new Atlassian design and will include things like the updated color palette, typography and icons. We are not currently working on any significant changes to navigation that you may have seen in our Cloud products. Find out more about the changes on the Developer Community . Last month we ran a survey to learn how we could do better at Developer Communications. We had 35 responses and we learned a lot about what you’re looking for. Here’s the top 3 things we heard: Thank you to everyone that took the time to fill out the survey. Our developer community is going strong (we now have over 2700 members) and has some amazing content, created by all of you. As a surprise, we want to give you an early holiday present , a way to show off your contributions and expertise in our community. Badges! We recently saw the release of a few server releases: In addition to product releases there was a release to AMPS: As always, if you are seeking help in building your apps there are three avenues available to you. If you seek help, mentoring, or support in building your app please head over to the Atlassian Developer Community where over 2600 developers and over 350 Atlassians are available to help out. If you found a bug, want to raise a feature request, or report an app security incident with any of our APIs, SDKs, or app frameworks please open an issue over at the Atlassian Ecosystem Developer Service Desk . Need help with your Marketplace listing, head over to the Atlassian Ecosystem Marketplace Vendor Support Desk . Many of us at Atlassian will be out next week. Let us know what you’re doing next week . Ralph is a Developer Advocate for ecosystem developers.", "date": "2017-12-22"},
{"website": "Atlassian", "title": "Announcing Enterprise releases for Server and Data Center versions of Jira Software and Confluence", "author": ["Aileen Horgan"], "link": "https://blog.developer.atlassian.com/announcing-enterprise-releases/", "abstract": "We're excited to share an update to our release process based on feedback from our customers, partners, and developer community. We've heard that for some, keeping up with Atlassian's Server and Data Center product release cadence is a challenge. This is true not only for companies who deploy our products at scale, but also for our Marketplace vendors who build apps that custom fit our Server and Data Center products to the needs of every team. While it's important for us as Atlassian to maintain speed in our innovation, our Server and Data Center customers also value stability. Our customers rely on us to be transparent about release dates and feature information so that they can make the best upgrade decisions for their own business. In turn, our Marketplace vendors need clear guidance on which releases they should focus their support efforts. Beginning in 2018, Atlassian will be introducing Enterprise releases for Server and Data Center versions of Jira Software and Confluence. Enterprise releases will help our Server and Data Center customers get the most value from our product releases, while giving them peace of mind when choosing the release version to which to upgrade. Enterprise releases are not separate from our standard releases. Enterprise releases are simply a designated version for those Server and Data Center customers who prefer to upgrade less frequently, and for whom the process of planning to upgrade their product can take months. This release designation better fits their needs and existing upgrade patterns. At least one feature release per year will be designated an Enterprise release. This release will receive additional backported bug fixes to ensure Server and Data Center customers can remain on the release for longer if they prefer. The first Enterprise releases will be Jira Software 7.6 and Confluence 6.6 . Enterprise releases will be designated when the corresponding feature release is available. There are multiple criteria that are considered when designating a release as an Enterprise release, such as time since last release, features, overall stability/performance for large deployments, etc. For example, we will designate a release as an Enterprise release when version 7.x.0 is generally available, but the Enterprise release (7.x.3) will be made available shortly thereafter. You will be able to see if a release is an Enterprise release or not in the release notes. Our Marketplace vendors will have more clear direction on which Data Center and Server versions they should focus support efforts for their apps, as Enterprise releases will be suitable for customers who prefer to upgrade less frequently. This will be particularly helpful and important as vendors build apps for Data Center, as part of our previously-announced Data Center app readiness program. If you have questions about these changes, you can ask in the developer community or contact us at http://go.atlassian.com/marketplace-support . Enterprise releases are not separate from normal releases. It is just a designation for one release per year that is recommended for those Server and Data Center customers who prefer to upgrade less frequently, and for whom the process of planning to upgrade their product can take months. You can also think of this as \"knighting\" a release to elevate it to Enterprise release status. Enterprise releases will also include at least 3 bug fixes within a feature release. Enterprise releases are an included benefit with active Software Maintenance for all Server and Data Center customers. If a Server customer's software maintenance expires, they will have to renew to continue receiving access to technical support, new Enterprise releases, and bug fixes. There are currently no plans to charge a separate cost for Enterprise releases in the future. No: we believe that customers will gain value from each release we make available to customers for our Server and Data Center products. However, we understand that customers often skip some feature release versions and do not follow the practice of upgrading to every single release. When customers consider which release version to which they should upgrade, we are offering the Enterprise release designation as an additional data point to consider, especially if a customer has tended to stay on a single version of an Atlassian product for many months (or even years) in the past. We can strongly recommend that a customer evaluate an Enterprise release and consider upgrading to it, but it is not required. This release will receive additional backported bug fixes to ensure Server and Data Center customers can remain on the release for longer if they prefer. Support for Enterprise releases is not a requirement at this time for participation in the Data Center app readiness program. However, app vendors should prioritize their support efforts on Enterprise releases, since these are the most likely candidates to which customers will upgrade their Server and Data Center products. Customers will continue to be supported on Enterprise releases according to our EOL policy. We will also make regular bug fix releases available for that feature release for 24 months. We will publish a change log from the last Enterprise release. Additionally, we will provide more detailed upgrade documentation for upgrading Data Center from the prior Enterprise release, such as database and schema changes, so enterprise customers can upgrade with more confidence.", "date": "2018-01-05"},
{"website": "Atlassian", "title": "New module and REST APIs for Jira Cloud apps", "author": ["Eve Stankiewicz"], "link": "https://blog.developer.atlassian.com/module-and-api-for-jira-cloud/", "abstract": "At Atlassian we often say customer feedback is a gift. Thanks to your guiding insight, the Jira Cloud Ecosystem team has spent the last few months on delivering features that were highly requested, voted and discussed by app developers. Read on to learn what new features are now available for Jira Cloud apps. The new Jira Cloud user interface focuses on keeping Jira users working inside of a single project, and navigating between the different aspects of that project, like boards, backlogs, and issues. We want to help app developers provide the same great navigation experience without taking the user out of the context of the project. For that purpose we’ve released a dedicated project page module, which offers rich configuration capabilities with control over content layout that were not available in web panels. It is also more straightforward to implement and maintain, compared to the older former solution using a web item with a web panel. Refer to the project page module documentation for details and learn how to add a link to Jira project sidebar . You can also follow this guide , which demonstrates usage of the project page module in an example app. Entity properties have been hugely successful from day one, becoming the most essential feature of the Jira Cloud API. With the growing popularity and usage of entity properties, we heard from app developers consistently that setting or updating property values could be improved. Especially for issue properties, often apps need to update the same property on tens or hundreds of issues. Previously, this would mean the app needs to make tens or hundreds of requests to Jira Cloud � one for each issue. In order to make this process easier and faster we built an API for bulk setting and deleting issue entity properties, which allows the app to update one property (identified by the property key) on a number of issues in a single call. Our test results demonstrate that it takes some 200ms to update 100 issues this way, comparing to 2 seconds when updating them independently! Details on how to use this API are available in the documentation . Jira Cloud REST API offers a range of endpoints for working with issue screens. It turned out that using those APIs was quite cumbersome, because, while each endpoint requires relevant Screen ID in the request, there was no way to obtain those Screen IDs within the Jira Cloud REST API. We’re filling this gap by providing a method to get all screens , which returns a list of all screens with their names and IDs. Now, managing fields and tabs on issue screens will be much smoother. Sending of comment objects in issue webhooks has been deprecated for over a year now, and replaced with comment webhooks. You made us aware that comment webhooks are missing some features that are available for issue webhooks, and your apps cannot use them in a similar way. Lack of issue context in the comment webhook was the most critical, as the webhook clients could not associate the comment with the issue. We solved this problem by adding a limited issue representation to comment webhook payload. On top of that, we enabled filtering of comment webhooks with JQL so that only relevant webhook events are fired. With these two enhancements in place, we are soon going to remove comment objects from issue webhooks, and consequently stop sending issue webhooks for comment events. This change will start taking effect from January 22nd , and will be fully rolled out to all Jira Cloud instances by the end of January 2018. As many of you already noticed, the Jira Cloud REST API reference documentation is now fully integrated into the developer.atlassian.com site. It provides the readers not only with smooth navigation experience and mobile view, but also creates a collection of the API endpoints in Postman with just a click of a button. This is one step to making it easier for you to test API requests. Judging by the number of votes and comments on the above feature requests, we hope that they made many app developers happy and will help you build great apps for Jira. As usual, you can check the Jira Cloud Platform API roadmap to get the latest insight into our thinking about future plans.", "date": "2018-01-17"},
{"website": "Atlassian", "title": "Ecosystem Roundup – App Week, Confluence Server, and Tooling updates", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/ecosystem-roundup-4/", "abstract": "In this post, we roundup all recent Ecosystem news. Want to make your apps a better experience for your customers? Apply to come to App Week in Miami, March 19th – 23rd, 2018, and learn how to empower your users through design and user research. There is still time to apply to this invitation only event. A new Confluence 6.8 EAP milestone is available now! EAP releases provide a snapshot of our work-in-progress and give you an opportunity to test and fix your add-ons before the final release. We have some exciting changes in store this time around, including an update on our editor upgrade work , and a brand new read-only feature, to help admins when performing routine maintenance. Both of these may have an impact on your add-ons, so be sure to take a look. As usual, we'll be releasing weekly milestones and betas during the development of Confluence 6.8, so head to Preparing for Confluence 6.8 to find out what's changed. Get the latest EAP As we reported earlier this year , Confluence Server will designate one release a year as a Enterprise Release. Confluence Server 6.6 Enterprise Release is now available. See the change log for all the latest. As always, if you are seeking help in building your apps there are three avenues available to you. If you seek help, mentoring, or support in building your app please head over to the Atlassian Developer Community where over 3000 developers and over 375 Atlassians are available to help out. If you found a bug, want to raise a feature request, or report an app security incident with any of our APIs, SDKs, or app frameworks please open an issue over at the Atlassian Ecosystem Developer Service Desk . Need help with your Marketplace listing, head over to the Atlassian Ecosystem Marketplace Vendor Support Desk . Ralph is a Developer Advocate for ecosystem developers.", "date": "2018-02-07"},
{"website": "Atlassian", "title": "Jira Server 7.8 EAP has been released!", "author": ["Tomek Bartyzel"], "link": "https://blog.developer.atlassian.com/jira-7-8-eap/", "abstract": "We’ve just released a sample of Jira Server 7.8 as part of the Early Access Program (EAP) to let you know about the changes coming to Jira. The most important change in this release is an improved quick search that lets you quickly search through all your issues and projects. The quick search will display the most relevant results while you’re typing the search term, and refresh them whenever you change your mind. If you’ve already found what you were looking for, you can treat quick search as a handy work diary that will show you all issues you’ve been working on recently. Tell me more Stated in the Preparing for Jira 7.8 development guide is this suggestion for plugin developers to consider. What might be useful for plugin developers is monitoring how your users are searching, and limiting the number of concurrent searches with regards to Quick search. The quick search doesn't affect performance in a significant way unless many users start searching at the same time. If you notice any slowdown, start monitoring the searches to find a limit that works best for your instance. We always try to speak your language, and the Dutch language pack is a new addition in this area. We’re no longer shipping the Oracle JDBC driver with Jira. If you’re using Oracle as the database, you’ll need to download the driver from the Oracle website and place it in the Jira installation directory after the upgrade. You can read more about these changes in the release notes and the development guide for Jira Server EAP. We’ve prepared separate EAP Server downloads for Jira Core, Jira Software, and Jira Service Desk, so just pick your favorite here: Download the Jira Server 7.8 EAP . EAP releases are early previews of Jira during development. We release these previews so that Atlassian’s apps developers can see new features as they are developed and test their apps for compatibility with new APIs. The Jira Server 7.8 EAP is best explored hand-in-hand with the Preparing for Jira Server 7.8 development guide . The development guide contains all changes in Jira Server 7.8, and important new features that may affect apps developers. If you discover a bug or have feedback on Jira Server 7.8, please let us know. Create an issue in the Jira project on jira.atlassian.com with the component Jira 7.8 EAP .", "date": "2018-02-15"},
{"website": "Atlassian", "title": "Moving away from per-installation shared secrets for apps", "author": ["Norman Atashbar"], "link": "https://blog.developer.atlassian.com/single-shared-secret-intro/", "abstract": "Over the last year, we’ve been working on a new Atlassian platform for developers . You can see this new platform if you are currently working on building a Stride app . This new Atlassian platform for developers will make the developer experience consistent when developing apps across our products. We’re currently working to bring Jira and Confluence to use the new Atlassian platform for developers and to do this we need to make some changes. As a starting point, we will be using credentials managed in the developer site for the new and existing installations of these apps. When this project is done, currently estimated to take three months, instead of a using one set of credentials per installation there will be only a single set of credentials for each Marketplace listed app. This work will not have any user-visible effect and will not impact your app development workflow. However, your app(s) that use JWT authentication method will receive a call made to the ‘installed’ lifecycle endpoint during the migration process, providing a new shared secret, signed with the existing shared secret. It will need to update the shared secret accordingly, similar to the upgrade process. An app correctly implementing the spec for signing of the lifecycle callbacks should support this, but just to make sure, we have tested our client libraries and the major apps listed in Marketplace before committing to these changes. We will put safety measures in place and will migrate apps gradually. However, we will get in touch with you in case we detect problems requiring fixes on your side. Feel free to reach to us via our service desk if you have questions or need help.", "date": "2018-03-12"},
{"website": "Atlassian", "title": "Registration for Summit Europe, Partner Day, and Atlas Camp 2018", "author": ["Emilee Spencer"], "link": "https://blog.developer.atlassian.com/registration-open-for-summit-atlascamp-partnerday/", "abstract": "Hi everyone! The ecosystem team wanted to give you a heads up about the registration process for our three events later this year. We’re also still accepting submissions to give a talk at both Summit and Atlas Camp, and sponsorships are still available for Atlas Camp. 3 – 5 September 2018 Summit Europe is our annual user conference and biggest global event of the year.  It’s an opportunity to get inspired, learn how to transform the way teams work, and witness Atlassian’s latest innovations first-hand. Summit will be held at the Fira Barcelona Gran Via which is the same venue as last year. In order to give priority to customers who want to attend, we will be letting customers register first and will later allow Marketplace vendors to register depending on availability. Here is how registration will work for customers: Here is how registration will work for Marketplace vendors who are Summit sponsors: If you are a sponsor, you will be able to claim your included passes as well as purchase a limited number of additional passes (per your sponsorship package) and then register via promo codes that will be issued to you. Request your allotted additional passes by filling out the form that will be emailed to you directly by the events team. Registration for sponsors using those distributed promo codes will open on Tuesday, 15 May. Here is how registration will work for Marketplace vendors who are NOT Summit sponsors: If you are not a Summit sponsor, you can indicate your interest in attending Summit by filling out a ticket here . (One ticket per company) On or after Tuesday, 15 May we will allow some non-sponsor partners to register for Summit and will reach out to those people individually. Once approved, only specific email addresses will be allowed to register, so be sure to submit the email address tied to the person who will actually attend when you fill out the ticket. 3 September 2018 Partner Day brings together Solution Partners, Marketplace vendors, Corporate Resellers, Training Partners, and Affiliates from around the world to gain the insights needed for a competitive edge. Partner Day will be held at the Fira Barcelona Gran Via which is the same venue as last year. Registration will open on Tuesday, 15 May and will be available on a first come, first served basis for partners only . Check the website for updates. 6 – 7 September 2018 Atlas Camp is our annual developer conference where our developer community gets together to enhance their skills, learn how to extend the power of Atlassian, and discover the future of product roadmaps. Atlas Camp will be held at the Hotel Porta Fira in Barcelona which is a different venue than last year. Registration will open on Monday, 19 March and will be available on a first come, first served basis for developers who build apps with our APIs . Check the website for updates. Hope to see you at one of these great events this fall!", "date": "2018-03-15"},
{"website": "Atlassian", "title": "Confluence 6.9 EAP released – read-only mode is coming!", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/confluence-server-6-9-eap/", "abstract": "The latest Confluence 6.9 EAP milestone is available now! EAP releases provide a snapshot of our work-in-progress and give you an opportunity to test and fix your add-ons before the final release. In this release we're finalising work on our brand new read-only feature, to help admins when performing routine maintenance. This feature may have an impact on your add-ons, so please head to How to make your add-on compatible with read-only mode in our developer documentation to find out what you need to do to make sure your add-on is compatible. April 18, 2018 : Important update regarding the read-only feature. Hey App vendors, thanks so much for your engagement and support in making your Apps compatible with read-only mode whilst we continue to develop it. With the 6.9 Beta shortly upon us, we have had to make the decision to bump the feature from this release as it is not quite production ready. The good news is that in the coming couple of weeks we expect to be completed from the Atlassian side and that will give you a little extra time to get your App compatible for when the feature is released in 6.10. Keep an eye on the EAP updates as we have 1-2 more deliverables to make it a simpler for plug-ins to be compatible with read-only mode, whilst not breaking your backward compatibility with earlier Confluence versions. Thanks again for you ongoing support. Adam – Confluence PM As usual, we'll be releasing weekly milestones and betas during the development of Confluence 6.9, so head to Preparing for Confluence 6.9 to find out what's changed. If you have any problems with an EAP release, please raise an issue to let us know. The earlier we know about problems, the more time we'll have to fix them before the final release. Ralph is a Developer Advocate for ecosystem developers.", "date": "2018-03-26"},
{"website": "Atlassian", "title": "Jira Server 7.9 EAP has been released!", "author": ["Tomek Bartyzel"], "link": "https://blog.developer.atlassian.com/jira-7-9-eap/", "abstract": "We’ve just released a sample of Jira Server 7.9 as part of the Early Access Program (EAP) to let you know about the changes coming to Jira. With this release, we’re bringing the following improvements: New supported database: Microsoft SQL Server 2016. Faster Kanban boards: To improve performance and reduce the clutter, the ‘Done’ column of your Kanban board now only shows recently modified issues. Extended wildcard search: You can now use a wildcard when searching through all version fields. Delimiters for CSV: When exporting your issues to CSV, you can choose one of the four most commonly used delimiters: comma , semicolon , vertical bar , and caret . Disabling empty JQL queries: You can control how an empty JQL query behaves—returning either all issues (like it is now) or no results at all. This will help you avoid performance issues with empty filters created by mistake. New events in the audit log: boards (created, deleted), sprints (deleted). You can read more about these changes in the release notes and the development guide for Jira Server EAP. We’ve prepared separate EAP Server downloads for Jira Core, Jira Software, and Jira Service Desk, so just pick your favorite here: Download the Jira 7.9 EAP . EAP releases are early previews of Jira during development. We release these previews so that Atlassian’s apps developers can see new features as they are developed and test their apps for compatibility with new APIs. The Jira Server 7.9 EAP is best explored hand-in-hand with the Preparing for Jira Server 7.9 development guide . The development guide contains all changes in Jira Server 7.9, and important new features that may affect apps developers. If you discover a bug or have feedback on Jira Server 7.9, please let us know. Create an issue in the Jira project on jira.atlassian.com with the component Jira 7.9 EAP .", "date": "2018-03-27"},
{"website": "Atlassian", "title": "Coming soon: Updated look and feel for Jira Server", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/coming-soon-updated-look-and-feel-jira-server/", "abstract": "We're bringing a refreshed look and feel to many of our Server and Data Center products, and Jira is following the footsteps of Confluence and Bitbucket . This work is based on the new Atlassian design direction , and will include things like the updated color palette, typography and icons. We are not currently working on any significant changes to navigation that you may have seen in our Cloud products. In the first milestone, we are working on the frequently used screens in Jira, which include: Issue view and Issue Navigator Create (edit) issue Global Issue Navigator Dashboard Projects' list Login page We're also updating the branding to reflect the current Atlassian family identity. In the second milestone, we are planning to update screens specific to Jira Software, primarily Scrum and Kanban boards and backlogs. We want to minimize the impact on apps, so for any EAP releases containing changes from the first and second milestone, we will keep you informed of any known issues as soon as possible. You should follow the upcoming Jira EAP releases , and update any non-AUI components in your apps to match the design guidelines. Many of these design changes will be implemented via an upgrade to AUI. The AUI upgrade guide has details of all the recent changes. In case you missed it, there have also been some changes to AUI licensing, see The license for AUI 7 is changing in our Developer Community for more information. We are working closely with the Jira Service Desk team to understand the scope of changes for customers using Service Desk projects. We want to ensure that these customers receive the best experience when the above changes are introduced. To achieve this goal, we've identified further work, and are scoping it specifically for Jira Service Desk. These improvements will be released independently by the Jira Service Desk team. We're excited to bring the fresh and modern look to Jira. Here's a sneak peek of what we're working on. Please note that these designs are indicative and things may change in the next EAP or public release. Issue details – part of milestone 1 Board view – part of milestone 2 Backlog view – part of milestone 2 If you have any questions or comments please ask in our Developer Community post . Ralph is a Developer Advocate for ecosystem developers.", "date": "2018-03-27"},
{"website": "Atlassian", "title": "Simple verification procedure to prepare your app for single shared secret migration", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/single-shared-secret-simple-verification-procedure/", "abstract": "We recently announced that we are moving away from per-installation shared secrets for Jira Cloud and Confluence Cloud apps. In that announcement we detailed how we’re migrating over to a single shared secret and that your apps will receive a second installed lifecycle endpoint call to make the update. There have been some discussions on the developer community on how to ensure apps are ready for the migration and Norman Atashbar provided a simple verification procedure you can do on your app to ensure that it is ready for the migration call to the installed lifecycle endpoint. The sync process is very similar to an install-uninstall-install sequence that causes Connect to generate a new shared secret and sign it with the old shared secret for the second 'installed' lifecycle callback. By ignoring the uninstall event an app developer can simulate the shared secret sync operation that will happen during migration. Here is what you can do to verify your app will behave properly during migration: If you had any issues, please feel free to reach us via this thread or by creating a new service desk ticket . We have an internal process to dry run the migration on a test instance. Ralph is a Developer Advocate for ecosystem developers.", "date": "2018-04-03"},
{"website": "Atlassian", "title": "Calling all developers––You’re invited to Atlas Camp 2018", "author": ["Emilee Spencer"], "link": "https://blog.developer.atlassian.com/atlas-camp-invite/", "abstract": "Atlas Camp is our developer event which will take place in Barcelona, Spain from the 6th – 7th of September. This is a great opportunity to meet other developers and get the latest information first hand. Registration for Atlas Camp is now open and for a limited time only, you can save up to €100 on your ticket with our Early Bird pricing . At Atlas Camp, you will: Walk away from Atlas Camp buzzing with knowledge and get up to date on APIs, roadmaps, features and new services that will set you up for success. Choose from one of the following tracks and learn about the latest and greatest innovations. Atlas Camp is a great opportunity to share ideas with like-minded individuals. Meet teams from around the globe at the largest gathering of Atlassian experts, enthusiasts, and developers. We’re sending our experts to Atlas Camp to discuss app development, DataCenter/Server, testing, and performance tuning and scaling. You can walk away from Atlas Camp buzzing with the knowledge to implement new ideas and tactics at your own company. Become a better developer through sharing from the Atlassian Ecosystem – exchange stories and learn from other fellow developers. Share your journey with like-minded people and make connections that last a lifetime. Interested in becoming an Atlas Camp speaker? Please submit your session before June 1st .", "date": "2018-04-23"},
{"website": "Atlassian", "title": "Announcing new APIs for Portfolio for Jira Server", "author": ["Peter Van de Voorde"], "link": "https://blog.developer.atlassian.com/portfolio-for-jira-api-announcement/", "abstract": "The Portfolio for Jira team is proud to announce not just one but two APIs for our Server version. Our new REST and a Java API together solve two of our most voted and watched issues: JPOSERVER-438 and JPOSERVER-1814 . (236 votes) You now have access to all the teams, people, and their skills inside of Portfolio for JIRA. All through the Portfolio Team Management REST APIs , the Portfolio Live Plans REST APIs or the Portfolio Java API . Here are some ideas we came up with, but we’re sure you can think of some more: Thus, making users feel confident about their road maps and forecasts. Thus, enhancing the reports and providing everybody a better understanding of the challenges they are facing. Check out the documentation here: Integrating with Portfolio for Jira Let us know what other APIs may be helpful, and we'll consider adding them in. Ask questions and leave feedback on the developer community", "date": "2018-04-24"},
{"website": "Atlassian", "title": "Jira Server 7.10 EAP has been released!", "author": ["Tomek Bartyzel"], "link": "https://blog.developer.atlassian.com/jira-7-10-eap/", "abstract": "We’ve just released a sample of Jira Server 7.10 as part of the Early Access Program (EAP) to let you know about the changes coming to Jira. With this release, we’re bringing the following improvements: You can read more about these changes in the development guide for Jira Server EAP. For any questions you have, reach out to our developer community . We’ve prepared separate EAP Server downloads for Jira Core, Jira Software, and Jira Service Desk, so just pick your favorite here: Download the Jira 7.10 EAP . EAP releases are early previews of Jira during development. We release these previews so that apps developers can see new features as they are developed and test their apps for compatibility with new APIs. The Jira Server 7.10 EAP is best explored hand-in-hand with the Preparing for Jira Server 7.10 development guide . The development guide contains all changes in Jira Server 7.10, and important new features that may affect app developers. If you discover a bug or have feedback on Jira Server 7.10, please leave your comments in this suggestion .", "date": "2018-04-30"},
{"website": "Atlassian", "title": "Building momentum through the Stride API", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/building-momentum-through-the-stride-api/", "abstract": "We opened the Stride API to developers in February and we’re really impressed with all the amazing apps developers are building for Stride . The Stride API makes it easy for teams to build custom apps specifically for their teams. Additionally, we are seeing more and more apps being submitted for approval in the marketplace. We currently have 24 apps available and more on the way. Our users are installing and using these apps. We’ve seen active installs growth of apps at an average rate of 227% month-over-month. The Stride API makes it easy to do end-user onboarding of an app with the built-in configuration state . Obie used this feature to ensure the app was configured correctly. “A neat endpoint we use unique to Stride is the built-in configuration status for apps. We make it mandatory for users to complete onboarding/configuration before using Obie. While Obie is awaiting configuration, there is a status indicator beside his app icon in the side toolbar so that the installer is aware that configuration is required before proceeding to use the app fully.” – John Kyeremeh Developers at Message.io also agree about this feature , “Another power feature that carries over from Hipchat is configurable pages. Atlassian is a product for product builders and it makes sense to blend application developer in one centralized experience.” Of course Obie and message.io also love the ability to build bots in Stride and being able to build bots that are first class users. “Our favorite addition for Stride is the introduction of Bot users. Unlike in Hipchat, bot users are 1st party members of conversations and teams. This was the most requested feature from Bot developers we work with. 🤖 🎉 “ – message.io The ability to extend Stride’s chat UI is one of the more popular features. Drafted uses the ability to create a UI in a Sidebar to allow users to interact directly with Drafted right in chat. Users can see open job positions and submit referrals through a modal dialog without ever leaving the chat window. Drafted brings its interface right into chat through the Glance and Sidebar. Drafted users are able to send referrals on open job positions right in chat with the use of a dialog Over on the Developer Community there are more and more developers showing up building for Stride . Developers can ask questions and get help on their app. A developer just submitted their app for Marketplace approval and got a lot of help through the forums. Developers are even sharing tools and apps back to the community to help others. A developer built an app to help see the format of a message in its Atlassian Document Format (ADF) and this allowed him to debug his messages easily. Then they shared it with the rest of the Stride developer community ❤️ The app has two integration points: It allows you to display the ADF version of a message via a message action and it can help you set arbitrary ADF messages as your message via an input action. We use it for debugging ADF documents we generate and parse. You can install it by adding the following descriptor URL as a Custom App: https://connect.atlassian.io/addon/descriptor/0cxmR22ZOwmXs724v052D3ofrZT1ye8i/latest The developers at Yasoon who built the Outlook Calendars for Stride app shared a guide repository on building a Stride bot using Microsoft’s Bot Framework. You can check out the repository which has the code as well as a readme to guide you through it. What do you think? Think you can build an app that you can use on your team? Want to integrate your product/service with Strides users? Here’s how you can get started: Happy building! Ralph is a Developer Advocate for ecosystem developers.", "date": "2018-05-01"},
{"website": "Atlassian", "title": "Rolling out support for apps in the new Jira Cloud issue view", "author": ["Giles Brunning"], "link": "https://blog.developer.atlassian.com/releasing-support-for-ecosystem-apps-in-the-new-jira-cloud-issue-view/", "abstract": "Back in early December 2017, we announced our Alpha Program for the new Jira Cloud issue view and started taking in sign-ups for ecosystem apps. Then in early February 2018, we announced on the developer community forums that the alpha program for marketplace apps in the new Jira cloud issue view was live , with links to the documentation and design guidelines . The alpha program let us get the new issue view and ecosystem changes into your hands as early as possible. It also allowed us to collect feedback from different use-cases and make any adjustments as we continued to build towards our production release. To everyone who participated in the Alpha program, we want to say thank you! Your feedback and early engagement is going to help make this a smooth transition for our customers, and allow us to ship a much better experience moving forward. Now we’re ready to announce the next step in this journey. We’ll start enabling ecosystem support for customers who have the new Jira issue view enabled starting May 10th, 2018 . The new issue view is already rolling out to customers, but until now we’ve tried our best to limit that rollout to Jira customers who didn’t have marketplace apps installed. As of today, the new issue view is only for the board, backlog, and full-page issue view locations for Jira Software and Jira Core. Starting on May 10th, we’ll start to enable ecosystem support for Jira customers who already have the new issue view as our starting point. That’s because some of these customers may have installed apps but aren’t currently seeing them. We want to address this first, before we start rolling out further. Each user has the option to turn the new issue view off if they have any problems using it. We’ve completely re-imagined the issue extension experience with marketplace as a first-class citizen. We’re investing in helping users try, discover, and use more marketplace apps. The new design scales to support more apps installed on the issue, with apps loaded in the background improving performance and usability. To support the new issue view design, we’ve introduced two new modules for ecosystem developers: Issue Content and Issue Glance . These two modules are replacing the atl.jira.view.issue.left.context and atl.jira.view.issue.right.context locations. If you aren’t yet using the new modules, we provide backwards compatibility to display your app. But we highly recommend you start making use of the Issue Content and Issue Glance modules, to provide the best experience for your users. If you’re using the issue operations or issue activity extension points, these won’t be affected and will be rendered as they are today but in the new issue detail view. The consistent appearance of new issue view means you can integrate once and see your app appear in every issue view location, whether it be on the board, backlog, or in search results. Based on your feedback from App Week and other channels, we’ve made the following improvements over the past few weeks: Update your descriptors to make use of the new modules and take advantage of the improvements we’ve made in the new design. You can get an overview of the changes in the design guidelines for the new issue view . Note: Some Jira users will still be using the old issue view. If you already use the atl.jira.view.issue.left.context and atl.jira.view.issue.right.context locations in your descriptor, you’ll need to maintain these and add the new modules (don’t remove the old locations) until the new issue view has entirely replaced the old. We’ll handle rendering the new modules—Issue Content and Issue Glance—or the old webPanels depending on which issue view the user is looking at. We’ll let you know when these locations are officially deprecated, at which point you can remove the old modules and locations in your descriptor. If you have questions about these ecosystem updates or the new Jira issue view, head to the Jira Cloud section of the developer community and ask away. Please tag any posts with “ new-issue-view ” for quicker response times.", "date": "2018-05-02"},
{"website": "Atlassian", "title": "Another Jira Server 7.10 EAP brings project archiving!", "author": ["Tomek Bartyzel"], "link": "https://blog.developer.atlassian.com/jira-7-10-eap2/", "abstract": "We’ve just released a new version of the Jira Server 7.10 EAP (Early Access Program) to let you know about the upcoming changes to Jira. Project archiving: Archive inactive or completed projects (or any other for that matter!) to improve performance and reduce the clutter in your Jira instance. You can archive a project in an instant and easily restore it whenever it’s needed again. Project archiving is available for Jira Software Data Center and Jira Service Desk Data Center. New REST and Java APIs: We’re also releasing new REST and Java APIs that let you archive, restore, and manage archived projects. You can read more about these changes in the development guide for Jira Server EAP. For any questions you have, reach out to our developer community . We’ve prepared separate EAP Server downloads for Jira Core, Jira Software, and Jira Service Desk, so just pick your favorite here: Download the Jira 7.10 EAP . EAP releases are early previews of Jira during development. We release these previews so that apps developers can see new features as they are developed and test their apps for compatibility with new APIs. The Jira Server 7.10 EAP is best explored hand-in-hand with the Preparing for Jira Server 7.10 development guide . The development guide contains all changes in Jira Server 7.10, and important new features that may affect app developers. If you discover a bug or have feedback on Jira Server 7.10, please leave your comments in this suggestion .", "date": "2018-05-14"},
{"website": "Atlassian", "title": "Now accepting applications for App Week Cloud: Portland", "author": ["Tara Blaker"], "link": "https://blog.developer.atlassian.com/apply-for-app-week-cloud-portland/", "abstract": "Want to make your apps a better experience for your customers? We're currently accepting applications to Atlassian App Week in Portland, Oregon, on June 11th – June 15th, 2018. As an evolution of Connect Week, this App Week is an opportunity to work directly with Atlassian designers, developers, and architects to build new apps using the latest cloud APIs and features and to bring your new and current apps to the next level. We will have Atlassians from Jira Cloud, Confluence Cloud, Bitbucket Cloud, Stride, Trello, Statuspage and many other teams to help your team make your app run faster, look great, and be better for your customers. Due to the limited space, this is an invitation-only event. If you think your app would benefit from an intensive week of hands on work to get your ideas ready for Summit Europe 2018 and Atlas Camp, then make sure you apply to come.", "date": "2018-05-21"},
{"website": "Atlassian", "title": "Onboard customers to your app, part 1: Overview", "author": ["Kelly Snow"], "link": "https://blog.developer.atlassian.com/onboarding-customers-part1/", "abstract": "Customers should be able to install and configure apps quickly and easily so they can fairly evaluate how well they solve a business problem. Some customers never get through the installation and configuration steps and quit in frustration. We want to connect customers to solutions as easily as possible. This blog series will help you think about an onboarding flow that avoids friction and helps you determine which patterns and practices to design into your app. First, I describe the overall flow and basic principles to think about. Next, I’ll focus on the admin experience. Last, I’ll look at the end users experience. This journey starts with an app trial installation and ends with end-users confidently using your app. We’ll go over each step and related patterns. Admin UI within Atlassian products, available to admin users. Universal Plugin Manager (UPM) is the Marketplace embedded within Atlassian products that handles app installations. Product UI, available to end users Assume users (admins and end users) know nothing about your app. Assume someone asked their admin to install the app and didn’t communicate this to their organization. In this scenario, it’s important the admin understands what your app does, especially if configuration is involved. They need to successfully install and set up the app for users. Once you get through that step, end users may not know your app was installed, especially if it has a small presence in the product UI. We want users to flow seamlessly from one step to the next. As you think about onboarding, consider the minimum steps a user needs to complete a task successfully. Sometimes that means adding a step or bit of information to explain complexity. Other times it means removing distractions. Keep the user within the Atlassian product to complete a task. This is important for configuration in particular – if the admin can’t figure out how to get the feature installed and configured, and has to seek documentation elsewhere, it increases the chance they will give up and uninstall. You can read a manual about how to use your camera… Or you can learn through experience. It’s often easier to show your users how to do something in a picture, rather than describe the process. When we are coaching our users through the setup process, remember that showing is better than telling. Learn how to apply these principles to your app onboarding journey end users, in Part 2: Onboard admin users .", "date": "2018-06-13"},
{"website": "Atlassian", "title": "Onboard customers to your app, part 2: Admins", "author": ["Kelly Snow"], "link": "https://blog.developer.atlassian.com/onboarding-customers-part2/", "abstract": "Now that we’ve covered the overall journey required for your app to get up and running successfully in a product, we will take a closer look at what the admin user needs to know and how to help them. The first opportunity to control the user experience of installing an app occurs with the post-install modal. You can trigger a modal at the end of the installation process by adding a get started URL to the descriptor. Marketplace detects that URL and presents a “get started” or a “Configure” button on the modal. We recommend a Get Started button that takes you to a Get started landing page that you control. If you have configuration steps, be sure to indicate that on the page and link to configuration. When your app is installed, you can trigger an email to the installer. This is helpful to your admin user as another signal the app installed correctly. It is also an opportunity to introduce you, the vendor. You should really think about a Get Started page if you don’t have one already. It should be the first thing your admin sees after finishing the installation step. When they click on that “Get Started” button, it provides them context for the app and should direct them on what to do next. This is a channel that you control to communicate with product admins, and should be applied whether you have configuration or not. Key content considerations: As you identify content and design, here are a few tips to keep in mind. Use this template to get started or roll your own! This static html page provides key content areas to consider, along with basic formatting and elements like an image and Button with clear call to action. There are several patterns to aid your admin or end user in getting your feature operational. The first time users come to a page, you may have an empty page. This is an opportunity to focus the user’s attention on what to do. Provide a clear CTA and an option to learn more about the feature. If your product is simple, this is a good technique. For complex apps, use real examples to populate a new data table. Real examples can be reused or reverse engineered. For complex cases in particular, be mindful of conveying new or abstract concepts. Help people build an understanding of the relationships between various items and objects through visual or descriptive. Explain why to help build that mental model. For form components, provide default values so the user doesn’t have to fill it out. Do this when you believe users will fill it out the same way 80% of the time. When you can’t define a good default value, provide tip text in and around form fields to help users understand the type of information you want submitted. Provide support within the context of the page at key moments a user might want guidance. Take a look at your support tickets to determine a strategy for help information. Where do users struggle? What requires explanation that won’t fit into the UI? Help admins to drive awareness to end users with templates, prompts, tools. These resources can be part of your Get Started page as part of a multi-step process. This content is part of a welcome email. Other ideas include a widget that allows admins to email/share directly from your page. Your user may want to revisit the Get started or Configuration pages more than once, so consider placing them in the admin navigation. Let’s look at two approaches to organizing links. In this example, the links are ordered sequentially in how you would engage with the app. This approach helps reinforce the order the admin should go through the app. If order is important, this is a good option. In this example, links are ordered by frequency of use. An infrequently accessed link is last. Get started is typically something users will go to when they are first learning about your product, but after that, won’t have much need. Learn how to apply these principles to your app onboarding journey end users, Part 3: Onboard end users .", "date": "2018-06-14"},
{"website": "Atlassian", "title": "Onboard customers to your app, part 3: End users", "author": ["Kelly Snow"], "link": "https://blog.developer.atlassian.com/onboarding-customers-part3/", "abstract": "In part 1 of this blog series covered the overall onboarding journey of your users. In part 2 , we took a close look at the admin journey. Now we can focus on the end-user and building awareness of your app, then teaching users to use your app competently. This flow shows key steps in the journey vendors control and best practices at each step from the perspective of the end user. Users are focused on achieving their own goal. Remember users are trying to get their job done, they're not poking around Jira for things that have changed overnight. They might have requested the app be installed, they might not have. Don't rely on admins to communicate directly to users without a prompt. Think large orgs. At this stage the admin has installed the app, and the end user has no idea anything has changed. First awareness is about notifying the user that the app has been installed, and get them excited about using it. For the end user, the welcome email is a notification that something in the product has changed and will require different content from the admin email. The welcome email to end users should pitch the value of the app to the user, make them want to use it. The email should have a call to action, pointing the way to getting started. This is similar to the get started page in the admin experience, but with a different purpose. The admin get started page is focused more around configuration and getting the app ready for end users. If your app requires some configuration by the end user, it may also be necessary to include getting started tutorials and educational content. This page contains content the user should only have to refer to once, it should be lightweight, clear and concise. Don’t forget to include help and support links. Some features are quite subtle and users may never know they exist. A great way to help the user learn about changes and new features is to pop a dialog that introduces the feature. This should only show to the user the first time. After dismissing it, it should not show again. Server apps will have more flexibility when it comes to using the following patterns. Spotlight: This is one of our new ADG3 patterns, spotlight , which is used to ease users into using the app from within the Atlassian product. The spotlight improves engagement by introducing users to new features and functionality at relevant moments. Limitation in cloud apps Only cloud apps can use the spotlight pattern within a whole page iframe. Navigation integration points don’t allow this pattern, unfortunately. Here is a spotlight variation you can use within a smaller iframe. Why purple? Do this If you have more than one step, make sure you allow the user to skip forward. Always allow the user to cancel or close the wizard. Don’t do this Don’t trigger a wizard or onboarding modal beyond the first encounter with each user. Don’t use a wizard or onboarding modal if your app is simple and it’s clear where to go. An empty state appears when there are no items to be shown. Rather than showing a blank screen, which could come across as 'something is broken'. When designing a landing page or empty state, what is the primary action you expect users to take? Lead the user through information with obvious first and next actions. Support and documentation links should be available, but secondary. Now the user has completed their first tasks using your app, they're getting a feel for how it works in the product. It's important to continue supporting your users. During first use, you don't have to show users every feature at once. Be opportunistic with educating users, show them new things on a need to know basis. This also gives you an opportunity to enhance their perspective of the app. Examples, defaults, tool tips, UI copy. These are not so much on boarding best practices, but rather design best practices in general. Tip text for date input Defaults, help and support Remember your users might not live and breath your app as you do, they might use it a couple of times a week, maybe less. Give them ways to recall what is expected of them. This post completes a three part series about designing for discovery and onboarding in cloud and server Atlassian products. Please check out the full series! Part 1: Onboarding journey Part 2: Onboard admin users Part 3: Onboard end users Resources ADG3 guidelines ADG2 guidelines Jira integration points", "date": "2018-06-19"},
{"website": "Atlassian", "title": "Closing the Confluence Ecosystem Jira Project on ecosystem.atlassian.net", "author": ["Shreshth Luthra"], "link": "https://blog.developer.atlassian.com/closing-ce-project-eac/", "abstract": "In Oct 2016, we announced a new Ecosystem Support Portal to better service your needs. As a follow up, we are deprecating the old Confluence Ecosystem Support Jira Project on June 25th, 2018. We have moved the existing Confluence Ecosystem Support Jira Project tickets to CONFCLOUD on jira.atlassian.com , so you can search for and watch your old tickets there.", "date": "2018-06-21"},
{"website": "Atlassian", "title": "Jira Server 7.11 EAP has been released!", "author": ["Tomek Bartyzel"], "link": "https://blog.developer.atlassian.com/jira-server-eap-7-11/", "abstract": "We've just released the first EAP of Jira Server 7.11 as part of the Early Access Program (EAP) to let you know about the changes coming to Jira. With this release, we're bringing the following improvements: You can read more about this release and how to download it in our development guide Preparing for Jira 7.11 . If you have any questions or would like to give us your feedback, please comment on the Developer Community .", "date": "2018-06-26"},
{"website": "Atlassian", "title": "Ecosystem Roundup – An update from Scott, Cloud REST API changes, and more", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/ecosystem-roundup-5/", "abstract": "In this post, we roundup all recent Ecosystem news. Atlassian co-founder, Scott Farquhar, provides an update on the internal Ecosystem organization within Atlassian. Throughout 2018 and 2019, Atlassian will undertake a number of changes to our products and APIs in order to improve user privacy in accordance with the European General Data Protection Regulation (GDPR). Please see these notices: On July 15, 2018, Atlassian will be bringing the standard discount provided to Partners on Marketplace apps into better alignment with the discounts provided on Atlassian products. Discounts vary by Partner Type and may also vary by the type of transaction (new & upgrade vs. renewal transactions). With this improved alignment, we will also be enforcing the renewal discount rules applied to Atlassian products which includes in order to qualify for a discount, the original product or prior renewal must have been purchased through the same Partner. For more information see the Developer Community topic . We recently became aware that an individual not associated with Atlassian sent an e-mail survey to many of our Marketplace vendors. Get more information on the Developer Community . We recently saw the release of a few server releases: As always, if you are seeking help in building your apps there are three avenues available to you. If you seek help, mentoring, or support in building your app please head over to the Atlassian Developer Community where over 4000 developers and over 500 Atlassians are available to help out. If you found a bug, want to raise a feature request, or report an app security incident with any of our APIs, SDKs, or app frameworks please open an issue over at the Atlassian Ecosystem Developer Service Desk . Need help with your Marketplace listing, head over to the Atlassian Ecosystem Marketplace Vendor Support Desk . Ralph is a Developer Advocate for ecosystem developers.", "date": "2018-06-27"},
{"website": "Atlassian", "title": "Gear up for building for Stride with the Developer Toolkit", "author": ["Trevor Thompson"], "link": "https://blog.developer.atlassian.com/building-for-stride-with-developer-toolkit/", "abstract": "If you’ve ever been a web developer, you know how invaluable developer tools are to any web browser and any modern web developer’s workflow. Whether it’s Chrome, Firefox, or Internet Explorer, all of these web browsers now come with developer tools right in the browser to make it easy to build, debug, and test everything that’s happening in the browser. This is useful for tweaking visual elements, inspecting how the HTML was rendered, and even testing compatibility across browsers. So today we announce the same for Stride: Stride’s Developer Toolkit. For any given message sent in stride, you’ll notice two new options under the message actions; Open in message inspector and Open in message builder. With a combination of these two tools, we think it will be easier than ever to build for Stride. The first tool in the Stride Developer Toolkit is the message inspector. With this, you’ll be able to inspect any message and view key contextual information regarding the message and the environment you’re in. With this view, debugging is easier than ever. Now anytime you’re looking for a Message ID or trying to figure out who the sender was, it’s only a couple of clicks away from the message itself. From here, you can also go to the next developer tool, message builder. Stride’s use of the powerful Atlassian Document Format will enable you to build detailed and informative cards to send to Stride. When building and designing these, it always helps to have relevant samples of cards and what they’re capable of doing. With the Stride message builder, all messages are now a playground for you to inspect other cards, modify them in a live view editor, and polish the perfect card for your app. We think this will supercharge the developer experience for both designers and developers building for Stride-and our developers agree! “As partners and platform developers, I think the message builder really enables closer, more iterative work between design and engineering—helping us to be more aligned and agile.” Alex Sopinka Co-Founder / CTO Obie.ai As of Stride version 1.21, you’ll now be able to enable Developer Toolkit under the “Advanced” tab. Get started today, and let us know your thoughts on how it helps you build easier for Stride!", "date": "2018-07-11"},
{"website": "Atlassian", "title": "Atlas Camp is back and the agenda is better than ever", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/atlas-camp-is-back-packed-agenda/", "abstract": "Atlas Camp , Atlassian’s premier developer conference is coming to Barcelona, Spain 🇪🇸 on 6-7 September 2018. For many of you that was all that was needed for you to buy your ticket however some of you need a little more convincing. I am really excited to announce the agenda for Atlas Camp 2018. We’ll have 3 tracks of great content from both Atlassian’s and Ecosystem developers. 27 40 minute talks, 15 20 minute talks, 9 40 minute open discussion topics, DEVHELP Live (1-on-1’s with Atlassians) and more. Let’s dig in to the details. We’ll have content that you’re looking for in one of three tracks: Atlassian Platform for Developers – get up to date on APIs, roadmaps, new services, and features that will set your app or service up for success from our own product teams. Advanced App Development – Collaborate and hear advanced topics around development, Data Center/Server, testing, performance tuning and scaling, and how to use the Marketplace API to inform you business. Beyond the Code – Discover new ways to transform your team. Explore topics areound Agile, testing, source control, DevOps, design, and marketing. But I hear what you’re saying, “ Damn it Ralph, you’ve already told me the track descriptions, I want to know more. “ Ok, let’s go deeper 😃 In this track , we’ll have 9 40 minute sessions and 5 20 minute lightning talks. We’ll cover what’s new in products such as: Jira Cloud Jira Server Jira Software Server Jira Service Desk Confluence Cloud Bitbucket Cloud Trello Additionally, we’ll share information on the Atlassian Document Format now available in Stride and coming soon to other products. We’ll explore AMPS for plugin developers who may be new to the Ecosystem in the past few years. We’ll get a look at the exciting new features of the Atlassian Platform for Developers . We’ll explore what we’re doing to improve the cross-product app experience . And we’ll look at the new world of privacy in an Atlassian Cloud world . Seriously, I’m excited for this already 🎉. But wait, there’s more! 😮 In this track , we aimed to give you the advanced content and take-aways that you’ve always wanted from Atlas Camp 👌🏻. We heard from you loud and clear that you wanted to hear more about testing your app. So we got some testing sessions that share best practices from both Atlassian’s and experienced Ecosystem developers. End-to-End App Testing Rule your Tests with Arquillian Testing DC/Multi-Product Atlassian Server Apps Made Easy Want to know how Atlassian builds and operates their apps? We have two great sessions that dive deeper into our integrations and products that are nothing more than a Atlassian Connect app . Want to know how Ecosystem developers are building and operating their apps? We have sessions for that as well. Learn how to leverage the Marketplace API to gather key metrics on your app. Learn how to handle a security incident from a developer that went through one. Learn how to develop an app using AWS Lamda and go serverless. Want to go deeper into some services or frameworks? We’ve got those sessions too. Go deeper into the Connect Frontend Framework to build faster apps . Go deeper to make your Jira Server app fast, performant and high-qualilty . Finally, we’ll go deeper on API Authorization . I know, right? But there are still 3 more sessions still to be revealed in this track. It’s only going to get better! In this track , we wanted to explore the non-code related aspects of building apps for the Atlassian Ecosystem. Topics like how to choose your path before beginning , design , user experience , user onboarding , writing docs , design feedback , builds and deployments , analytics , privacy , marketing , and running a company . You’ll hear from both Atlassians and successful Ecosystem vendors. Additionally, we’ll explore other topics like planning to scale your app and microservice architecture with the strangler pattern . Finally we’ll get a sense on what’s on the Atlassian Technology Radar . This agenda is so packed I can’t imagine what more we could add… Again, I hear you saying “ Damn it Ralph, what’s the Firepit Discussion Room and why does it say NDA required? “ Great question, one of the things we hear at App Weeks from developers is that App Week feels like the Atlas Camp of the past when it was smaller and featured great discussions. We wanted to try to bring a little bit of the old camp back. Here’s what we’re doing. We have a small room that can hold about 30-40 people and in there we’ll have chairs configured in a circle (imagine sitting around the camp fire ⛺️🔥). Instead of a presentation we’ll have a theme to discuss. We’ll be holding 9 themed discussion sessions to have discussions. The descriptions of the themes will be posted to the Atlas Camp site later but the themes are: Building, Deploying, and running Cloud Apps Building, Deploying, and running Server/DC Apps DataCenter Readiness Program How to work with Atlassian (support, feature requests, dealing with changes) Operating your Business Privacy and GDPR Security Testing and Monitoring What I learned at Atlas Camp There will be Atlassians in the room to help keep the conversation going. We worked with our external program committe (Daniel Wester and Dan Hardiker) to come up with the themes for discussion. They also suggested that these conversations be covered under the Marketplace Vendor Confidentiality Agreement. This is so that developers and vendors can feel free to speak freely without worrying about what might be still considered confidential. If you don’t have a Marketplace Vendor account you can still participate but we will ask you to sign a non-disclosure agreement before entering. We’re pretty excited about this room and hope that it brings back a small piece of Camp from yesterday. This year we’re integrating our 1-on-1 area with our DEVHELP Support desk . We’ll be sending attendees information on how to sign up for a 1-on-1 appointment with an Atlassian. We’ll ask for some details about the conversation you want to have and ensure we have all the information. Then we’ll schedule you a time during the conference that works for you. We’ll use the issue created to remind you of your appointment and contact you with any follow up information after the appointment. Arrive in Barcelona and forgot to make an appointment? No worries come up to the DEVHELP Live table and talk to a Support Engineer who will help you schedule an appointment. We’re hoping that by integrating with the DEVHELP Support desk you’ll get a better experience with your appointment before, during, and after Atlas Camp. So what are you waiting for register today and don’t miss out. Ralph is a Developer Advocate for ecosystem developers.", "date": "2018-07-12"},
{"website": "Atlassian", "title": "Change notice for Jira Cloud – Get my permissions resource will require a permissions query parameter", "author": ["Krzysztof Kercz"], "link": "https://blog.developer.atlassian.com/change-notice-get-my-permissions-requires-permissions-query-parameter/", "abstract": "In order to guarantee stability and performance of Jira Cloud, we are constantly reviewing Jira REST APIs to make sure their design enables and encourages good practices. Starting 1 February 2019, we will require that you provide the permissions query parameter in all requests to the Get my permissions resource. Get my permissions checks permissions for a user. The list of permissions is extendable and can grow without limit. This can make requests slow, as the operation checks all permissions by default. In addition, this method is usually only used to examine a particular set of permissions, so checking all permissions is redundant. To solve this performance issue, we’re introducing a new query parameter called permissions . This parameter is a comma-separated list of permission keys. Only permissions on this list will be returned in the response. The parameter will become required and requests without it will start being rejected after 1 February 2019. You will still be able to request all permissions by listing them in the permissions query parameter. You can get all permissions defined in a Jira instance using the Get all permissions endpoint. The request below checks all permissions. The request below checks only permissions defined in the permissions query parameter. If you need help about this change please ask in the Jira Cloud Development forum in the Developer Community . The official change notice can be found in the Jira Cloud docs .", "date": "2018-07-25"},
{"website": "Atlassian", "title": "Important Stride and Hipchat update: Atlassian and Slack announce a new partnership", "author": ["Aileen Horgan"], "link": "https://blog.developer.atlassian.com/atlassian-slack-partnership/", "abstract": "Today, we’re announcing a new strategic partnership between Atlassian and Slack. Millions of people around the world use Slack to connect their teams, unify their systems, and drive their business forward. As a result of this partnership, Atlassian is discontinuing all real-time communications products, and we are working with Slack to provide a migration path for Stride and Hipchat customers. We will support Stride, Hipchat Cloud, Hipchat Server, and Hipchat Data Center until the following dates: We realize that this news is unexpected, and that you may need time to understand how this will affect your business. Here is some additional information related to this change: Please read Atlassian’s announcement for complete details about the partnership, then visit either the Stride development developer community board or the Hipchat development developer community board if you have additional questions about how this impacts your app(s).", "date": "2018-07-26"},
{"website": "Atlassian", "title": "Upcoming breaking changes across Server Products", "author": ["Peter Scobie"], "link": "https://blog.developer.atlassian.com/upcoming-breaking-changes-across-server-products/", "abstract": "In the spirit of openness, we wanted to share our plans around upcoming breaking changes across many Server Products. We’re open about our plans so the Atlassian Ecosystem can plan ahead to ensure a smooth experience for our customers. We’ve consolidated these updates into a single, easy to digest, post and plan to provide monthly updates from now until the end of the year. The AUI team is currently re-architecting and breaking up the AUI front-end library in to smaller pieces. The team’s aim is to allow developers to use only what they need from AUI, so that they can improve their product’s and plugin’s runtime performance. The code shipped in AUI’s default web-resources and batch files will change. Some deprecated utility methods in AUI will be removed. Individual HTML markup patterns and component APIs will not change . The AUI team posted their direction for AUI 8 in the developer community . The community post dives deeper in to the planned changes and outlines an upgrade path. The AUI team is tracking to ship this release via NPM in early- to mid-August. Atlassian Server products will ship the library update in their upcoming major versions. Found a bug? Raise a ticket in the AUI project on our Ecosystem Jira . We’re planning to ship Bitbucket Server 6.0 late November, 2018. We have some exciting new features lined up for the release. For one of these features it is necessary for us to deprecate direct access to repositories on disk for apps. You can find the necessary details in the API changelog . Apart from this, we plan to add support for Java 11. Java 8 is reaching end of life in January 2019. After this time no further fixes, including security patches will be available for it. As a core dependency for Bitbucket Server such patches are critical. In Confluence 6.10 we shipped read-only mode for Confluence Data Center. Read-Only Mode for Data Center customers helps admins perform routine maintenance, recover from unexpected problems, or prepare to migrate content to a new site. It is important that you check that your app is compatible, and to mark it compatible to help inform system administrators, Check out how to make your add-on compatible with read-only mode for all the details. In 6.10, we also introduced a new way to handle resource intensive tasks in a sandbox, starting with document conversions. The sandboxes aren’t available for add-ons at present, but this is something we might consider doing in the future. Check out preparing for Confluence 6.11 for more details. We are currently planning to release support for Java 11 in an upcoming feature release, circa October 2018. This will allow the use of the JDK 11, however without support to use any new features or functionality at this time. We are also continuing our work on the TinyMCE editor upgrade from 3.x to 4.x. This has been ongoing for some time now, and hopefully everyone has checked their apps for compatibility. If not, check out how to test your add on with the upgraded TinyMCE4 editor . Confluence Server 7.0 scope and timing is currently under review. We’re still refining what goodies will be included but are strongly contemplating full Java 11 support, upgrading Guava, removing deprecated JS Globals, converting all plugin JavaScript to AMD modules (and no longer use globals in those modules) and remove deprecated SOAP/XML-RPC methods. In order to remove the deprecated methods, we anticipate releasing new and equivalent REST API’s. Our customers are moving to IPv6 and Atlassian Server products are now compatible in these new environments. We’ve recently shipped IPv6 support for Bitbucket Server( v5.8 ), Confluence Server ( v6.9 ), Jira Service Desk Server ( v3.14 ), Jira Server ( v7.11 ). Portfolio for Jira Server version 2.15 is compatible with Jira Server 7.11 and is therefore IPv6 compliant. We are working on making Bamboo IPv6 compliant and will be shipping that in this quarter. We plan to make FeCru compliant in 2019 and have no plans to add support for Hipchat Data Center. There is more information and implementation advice available in the product release notes. We’re planning to ship Jira Software 8.0 in October 2018. We’re still refining the scope of the release but our current aim is for it to include improvements to the search sub-system via an upgrade of Lucene, front-end improvements such as jQuery and Skate library updates and deprecation of global variables in favor of AMD modules (for more details, see this post ), Agile and Kanban boards performance improvements, and an upgrade to use new platform components and libraries which are compatible with Java 11. The JSD team plans to ship Jira Service Desk 4.0 release, in step with Jira Software 8.0 in late October 2018. As a major release update it’s important for our customers to be informed so they can absorb the changes and benefit from it. Jira Service Desk is built on the Jira platform, it stands to benefit from and incorporate changes being brought in by Jira. In particular JSD will incorporate sub-system improvements related to Lucene. There would also be updates to the Fugue code. We’re in the process of upgrading our Server products to support Java 11. As mentioned above this is in progress for Jira Software, Bitbucket Server and Confluence Server. Bamboo, FeCru, Portfolio for Jira and Jira Service Desk and Crowd have yet to roadmap this work but it is our intention that all maintained Server products will provide support for Java 11. We are also considering supporting OpenJDK for Java 8 and 11, please share with us your feedback and if you would like us to support it. If you have any questions or concerns about any of the changes mentioned here please raise a ticket in the Ecosystem Developer Service Desk .", "date": "2018-08-02"},
{"website": "Atlassian", "title": "Coming soon: Jira 8.0", "author": ["Klaus Ihlberg"], "link": "https://blog.developer.atlassian.com/jira-8-0-announcement/", "abstract": "As we’ve developed Jira Server and Data Center in recent years, we’ve encountered a number of opportunities to invest more deeply in the product. We’ve decided to start taking action on these opportunities, and need your help to bring these updates to customers. These changes will come as part of our next platform release, also known as Jira 8.0. We are committed to helping you prepare for the next platform release as you continue building your business on top of Jira. In the coming months, we will share as many specifics and details as we can about the scope of the changes to Jira, so you can effectively plan how you’d like to respond to these platform changes. Between now and the release of 8.0, we will release several Early Access Program (EAP) milestones. These will contain the breaking changes of the final platform release, to give vendors an opportunity to preview and develop against the changes of the platform release. When the first EAP is introduced, we will also be creating a community discussion topic under the Jira Server category specifically to address the changes introduced with 8.0, and will have members of the Jira Product Management and Engineering team on hand to help answer any questions. Please note that as development of the release is underway, the specifics of breaking changes will evolve. This is our best estimate of breaking changes at this point in time. Our EAP programs will contain the full scope of breaking changes in Jira for 8.0, along with supporting documentation with more detail and specifics. We will be upgrading Lucene from version 3.3 to a 7.x version. We will also be optimizing the index structure, which might be a breaking change for some apps. A potential outcome of this upgrade could be changes to the Custom Field SPI contract. The scope of the impact of the Lucene upgrade is still being determined, but we will be able to summarize the API changes in the EAP milestones for Jira 8.0. We will be working on parts of the Jira frontend to make it faster and more responsive. As part of this effort, we will be removing global variables on some pages. We’re planning to trim down ‘superbatch’ and disable it completely on some pages. We are also planning to upgrade jQuery, and may update other frontend libraries, such as SkateJS. As with the other upgrades in 8.0, the precise scope of the pages impacted is still being determined, but will be clarified with the coming EAPs. Some libraries will be upgraded to more recent versions, including: Spring Guava Commons-lang to commons-lang3 Other libraries may be upgraded as well, though this is the scope of the upgrades today. We will be sharing more information regarding upgrades to plugin system libraries with the coming EAPs. As work for Jira 8.0 proceeds, the Jira engineering team may discover work needed for other, smaller, breaking changes that have not been planned yet. We will do our best to inform the Ecosystem community as soon as possible when we learn about these changes and include them in the EAPs. We're planning to deliver several EAP milestones, followed by an EAP Beta, and finally the EAP Release Candidate. We expect to deliver the first milestone in the coming days, and then continue to release the following milestones every 2 weeks until we release the EAP Beta. We'll do our best to include most of the planned breaking changes as early as possible, to give you enough time to schedule changes to your app before the official Jira 8.0 release. After releasing all the breaking changes, we'll release an EAP Beta, and stop releasing milestones. There will be no breaking changes after this point. Finally, we’ll release the EAP Release Candidate, which will essentially be the same as the Jira 8.0 release. After that, you can expect the official 8.0 release. Once released, we’ll let you know about the first EAP milestone on this blog, and on the dev community . You can also monitor the EAP page for dev documentation and other important details. The changes above will also be reflected in Jira Service Desk, and customers will be able to benefit from the underlying Jira platform enhancements. We know that a platform release can provide many unknowns. Here are some questions that we wanted to address with the information we currently have. When will Jira 8.0 arrive? As mentioned above, we cannot provide an exact date, but our current target to release Jira 8.0 is late in 2018. The release will not arrive until after Summit Europe later in 2018. The first early access programs (EAPs) will arrive in the coming days. We strongly recommend vendors investigate the EAP builds and documentation to get a sense of the scope of the impact of 8.0. Why would a customer upgrade to Jira 8.0? Will there be new end user features? Jira 8.0 will include other end-user features beyond the breaking technical changes above. However, we won’t be publicly announcing the end-user features until we release Jira 8.0. The additional end-user features will not impact the vendors or developers building on top of Jira. Will there be breaking changes to Jira’s APIs? We do not yet know the scope of breaking changes to either the REST API or Java API. We are not planning major changes to either API, but we may update them with slight breaking changes, dependent on the progress of the updates above. We may also introduce new endpoints for app vendors and developers to use in Jira. Are there any restrictions on who has access to the EAP builds of Jira 8.0? No, there will not be any restrictions. The EAP is open to everyone. Will there be additional resources to help us prepare? Yes! In coordination with the EAP programs, we will provide more detail about breaking changes and the technical scope of Jira 8.0. We will also have technical documentation to provide detail on how to adopt the changes to Jira. We’ll be publishing all the documentation on the EAP page . Best, The Jira team", "date": "2018-08-03"},
{"website": "Atlassian", "title": "Jira Server 7.12 EAP has been released!", "author": ["Tomek Bartyzel"], "link": "https://blog.developer.atlassian.com/jira-7-12-eap/", "abstract": "We’ve just released a sample of Jira Server 7.12 as part of the Early Access Program (EAP) to let you know about the changes coming to Jira. With this release, we’re bringing the following improvements: AUI upgrade : We’re upgrading Atlassian User Interface (AUI) from version 7.8.0 to 7.9.6. The new version brings changes to multiple drop-down menus. Days in column : We’re adding an option to disable the Days in column time indicator displayed on issue cards. For large Jira instances, it will result in significant performance improvements. Sharing edit rights for filters/dashboards : You can now share edit rights for filters and dashboards, allowing chosen users to make any changes they need. This feature also brings changes to REST APIs. Custom fields optimizer : A new feature in Jira Data Center that lets you scan all custom fields, highlight the misconfigured ones, and improve them automatically by changing their configuration. You can read more about these changes in the development guide for Jira Server EAP. For any questions you have, reach out to our developer community . We’ve prepared separate EAP Server downloads for Jira Core, Jira Software, and Jira Service Desk, so just pick your favorite here: Download the Jira 7.12 EAP . EAP releases are early previews of Jira during development. We release these previews so that apps developers can see new features as they are developed and test their apps for compatibility with new APIs. The Jira Server 7.12 EAP is best explored hand-in-hand with the Preparing for Jira Server 7.12 development guide . The development guide contains all changes in Jira Server 7.12, and important new features that may affect app developers.", "date": "2018-08-13"},
{"website": "Atlassian", "title": "First milestone for Jira 8.0 is up for grabs", "author": ["Magda Michalak"], "link": "https://blog.developer.atlassian.com/jira-8-0-eap/", "abstract": "Since we announced the commitment to release Jira 8.0 , we’ve worked hard on the changes that we can release as the first milestone. Now, this milestone is available and ready to be downloaded . Between now and the release of 8.0, we will release Early Access Program (EAP) milestones once every 2 weeks. These will contain the breaking changes of the final platform release, to give vendors an opportunity to preview and develop against the changes of the platform release. We have created a community discussion topic under the Jira Server category specifically to address the changes introduced with 8.0. The topic will have members of the Jira Product Management and Engineering team on hand to help answer any questions. We have started upgrading Lucene from version 3.3 to a 7.3 version. For details regarding this change, see Lucene upgrade . This milestone does not contain Service Desk. For details of the milestone, see Important to know in Preparing for Jira 8.0 . Jira 8.0 will contain a number of breaking changes that we will introduce step by step across the upcoming milestones. If you want to keep track of the changes and monitor the EAPs in which they are introduces, refer to Preparing for Jira 8.0 . We know that a platform release can provide many unknowns. Here are some questions that we wanted to address with the information we currently have. When will Jira 8.0 arrive? While we are unable to provide an exact date, our current target to release Jira 8.0 is late in 2018. The release will not arrive until after Summit Europe later in 2018. The first early access program (EAP) milestone is available as of 14 August 2018, and we’ll be releasing milestones once every 2 weeks over the coming months until we release the EAP beta. Learn more about the EAP release cadence . We strongly recommend vendors investigate the EAP builds and documentation to get a sense of the scope of the impact of 8.0. Why would a customer upgrade to Jira 8.0? Will there be new end-user features? Jira 8.0 will include other end-user features beyond the breaking technical changes above. However, we won’t be publicly announcing the end-user features until we release Jira 8.0. The additional end-user features will not impact the vendors or developers building on top of Jira. Will there be breaking changes to Jira’s APIs? We do not yet know the scope of breaking changes to either the REST API or Java API. We are not planning major changes to either API, but we may update them with slight breaking changes, dependent on the progress of the updates above. We may also introduce new endpoints for app vendors and developers to use in Jira. Are there any restrictions on who has access to the EAP builds of Jira 8.0? No, there will not be any restrictions. EAPs are open to everyone. Will there be additional resources to help us prepare? Yes! In coordination with the EAP programs, we will provide more detail about breaking changes and the technical scope of Jira 8.0. We will also have technical documentation to provide detail on how to adopt the changes to Jira. We’ll be publishing all the documentation on the EAP page .", "date": "2018-08-14"},
{"website": "Atlassian", "title": "Now accepting applications for App Week Amsterdam: Cloud & Data Center", "author": ["Tara Blaker"], "link": "https://blog.developer.atlassian.com/app-week-amsterdam-2018/", "abstract": "Want to make your apps a better experience for you and your customers? We're currently accepting applications to Atlassian App Week in Amsterdam, November 12-16. Atlassian is experiencing significant growth in two areas—Cloud and Data Center. To ensure your success in the Marketplace, we're focused on helping you build apps for these high growth opportunities to meet rising customer demand, as well as the needs of diversified teams and new industries. This App Week is an opportunity to work directly with Atlassian developers, product managers, architects, designers, and marketers to build and test new apps using the latest Data Center and Cloud APIs, and to ensure your current apps meet our new performance standards. We'll have Atlassians from Jira Software, Jira Service Desk, Confluence, and other teams to help your apps run faster, look great, and perform better for  your customers. Due to the limited space, this is an invitation-only event. If you think your app would benefit from an intensive week of hands on work, then make sure you apply to come.", "date": "2018-08-27"},
{"website": "Atlassian", "title": "Introducing live macros to support a new editing experience in Confluence Cloud", "author": ["Klaus Ihlberg"], "link": "https://blog.developer.atlassian.com/introducing-live-macros-to-support-a-new-editing-experience-in-confluence-cloud/", "abstract": "We’re in the midst of rolling out a new editing experience to Confluence Cloud in order to tackle some key customer pain points, and this involves significant changes to Confluence macros. This page will outline the key differences and take Confluence macro developers through the steps to update their macros. One of the big improvements we're working on as part of this rollout is making the editing experience consistent with the viewing experience, so that pages look the same when editing them as they do once published. Macros are a huge part of this. In the current editor, most of the macros display as grey boxes when a page is being edited, and only render correctly in the published version. These grey boxes don't give any indication of the size, layout, or information contained in the macro. In the new editor, these grey boxes have been replaced with live macros, which look the same regardless of whether you're viewing or editing the page. That means you no longer have to worry about what the page will look like when you press Publish. We’ve heard your feedback, so with this new rollout live macros will be available for third party developers as well. To make your macro live in the new editor, your macro needs to have “outputType”: “block” and “bodyType”: “none” in the atlassian-connect.json. This then replaces the old placeholder image with an overlay displaying the macro as it would when the published page is being viewed. Macro width is controlled by the editor. There are three different widths for macros: Block, wide, and full width. Don’t set any width in atlassian-connect.json or in css as it may cause the macro to work incorrectly. The height of the macro is controlled by the CSS macro height. You can set and change it dynamically as appropriate to your use case. After editing a WYSIWYG macro in a custom macro editor, you need to update its parameters, as this is what triggers an update in the editor. This can be done by storing a revision number in the parameters, or by updating any other parameter you might need to store within the macro. Because we want a page to look the same while editing as it does once it's published, we've decided to deprecate nested bodied macros. Bodied macros are those such as the \"tabs\" macro which is popular on server, for which a user defines each individual tab inside the macro, and then on publish, the macro renders as a collection of tabs. For connect macros, these are any macros that have defined “bodyType”: “rich-text”. You will still be able to define these macros, but you won't be able to nest other macros that have “bodyType”: “rich-text” inside them, as this makes the page look vastly different when editing to what it looks like when published. Note: We are in the process of redefining the usage of some of Confluence’s native rich text macros like \"Expand\" and \"Page Properties\" so that it still will be possible to use other bodied macros inside them. This deprecation is planned for the end of 2018. Until then, nested bodied macros will continue to work in the old editor, but not in the new editor. After that date, any nested bodied macros will still display as normal when viewing a published page. However, if that page is edited, they will display as 'unsupported content' that can't be interacted with (except to be deleted). If you need help about this change please ask in the Confluence Cloud Development forum in the Developer Community .", "date": "2018-08-30"},
{"website": "Atlassian", "title": "August Update on Upcoming Breaking Changes across Server Products", "author": ["Peter Scobie"], "link": "https://blog.developer.atlassian.com/august-update-breaking-changes-across-server-products/", "abstract": "Further to last month’s blog , we have an update on our plans to ship breaking changes across Server Products. While plans often change, we’re open about our work so the Atlassian Ecosystem can plan ahead to ensure a smooth experience for our customers. We’ve consolidated these updates into a single, easy to digest, post and will provide monthly updates until the end of the year. The AUI team posted their direction for AUI 8 in the developer community last month . The community post dives deeper in to the planned changes and outlines an upgrade path. The AUI 8 release is on hold as the AUI team work with Atlassian Server products on acceptance testing with their upcoming major versions. Found a bug? Raise a ticket in the AUI project on our Ecosystem Jira . We’re planning to ship Bitbucket Server 6.0 late November, 2018. We have some exciting new features lined up for the release. For one of these features it is necessary for us to deprecate direct access to repositories on disk for apps. You can find the necessary details in the API changelog . Apart from this, we plan to add support for Java 11. Java 8 is reaching end of life in January 2019. After this time no further fixes, including security patches will be available for it. As a core dependency for Bitbucket Server such patches are critical. Also, in Bitbucket Server 6.0, we will end support for all versions before Git 2.11: In 6.10, we also introduced a new way to handle resource intensive tasks in a sandbox, starting with document conversions. The sandboxes aren’t available for add-ons at present, but this is something we might consider doing in the future. Check out preparing for Confluence 6.11 for more details. We are currently planning to release support for Java 11 in an upcoming feature release, circa October 2018. This will enable customers to run on Java 11, but with Java 8 language compatibility mode (which means features that are new to Java 11 will not work). We are also continuing our work on the TinyMCE editor upgrade from 3.x to 4.x. This has been ongoing for some time now, and hopefully everyone has checked their apps for compatibility. If not, check out how to test your add on with the upgraded TinyMCE4 editor . Confluence Server 7.0 scope and timing is currently under review. This will include full Java 11 support. We’re still refining what other goodies will be included but are strongly contemplating upgrading Guava, removing deprecated JS Globals, converting all plugin JavaScript to AMD modules (and no longer use globals in those modules) and remove deprecated SOAP/XML-RPC methods. In order to remove the deprecated methods, we anticipate releasing new and equivalent REST API’s. Our customers are moving to IPv6 and Atlassian Server products are now compatible in these new environments. We’ve recently shipped IPv6 support for Bitbucket Server ( v5.8 ), Confluence Server ( v6.9 ), Jira Service Desk Server ( v3.14 ), Jira Server( v7.11 ). Portfolio for Jira Server version 2.15 is compatible with Jira Server 7.11 and is, therefore, IPv6 compliant. We are working on making Bamboo IPv6 compliant and will be shipping that in September. We plan to make FeCru compliant in 2019 and have no plans at the moment to add support for Hipchat Data Center. There is more information and implementation advice available in the product release notes. We’re planning to ship Jira Software 8.0 in late 2018. This release will include improvements to the search sub-system via an upgrade of Lucene, frontend improvements such as jQuery library updates and deprecation of global variables in favor of AMD modules (for more details, see this post ), Agile and Kanban boards performance improvements, and an upgrade to use new platform components and libraries which are compatible with Java 11. It will also contain several other end-user features, and they will be unveiled closer to the release. Between now and the release of 8.0, we will release Early Access Program (EAP) milestones once every 2 weeks. More information on the 8.0 Early Access Program can be found here . Jira 8.0 will be working with Oracle JDK 8 and OpenJDK 11. It means that Java 11 features will not be supported in the source code (Java 8 compatibility mode). We will announce this limited support without making Jira officially compatible with Java 11 until a future 8.x release. This will give app vendors time to update their products before we announce official compatibility with Java 11 in an 8.x release of Jira. This will take place most likely in January or February 2019. Once we announce official compatibility with Java 11, we are also planning to bundle OpenJDK 11 with the Jira installer. You may have heard about our Jira Performance Testing (JPT) tool which has been in development. We are planning to open a public beta of JPT to help you test your instances and apps at scale. Stay tuned to the Developer Community as we will publish and update there about how to get access. In Jira Service Desk 4.0, we'll be updating our APIs to use Core Java Data types and Exceptions . We're introducing this change to make it easier to develop on Jira Service Desk. You can read the full deprecation notice . In accordance with the Java API Policy for Jira , we'll be permanently removing com.atlassian.fugue with the release of Jira Service Desk 4.0. You will need to update any scripts, integrations or apps that make requests to endpoints returning com.atlassian.fugue to use Core Java Data types and Exceptions instead. We’re in the process of upgrading our Server products to support Java 11. Jira Software, Bitbucket Server, Confluence Server are continuing their work as mentioned above with Portfolio for Jira and Jira Service Desk joining recently. Bamboo, FeCru and Crowd have yet to roadmap this work, but it is our intention that all maintained Server products will provide support for Java 11. Please check here for more details. If you have any questions or concerns about any of the changes mentioned here please raise a ticket in the Ecosystem Developer Service Desk .", "date": "2018-08-30"},
{"website": "Atlassian", "title": "Tune in to the Atlas Camp Keynote September 6th", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/atlas-camp-live-stream/", "abstract": "If you aren’t able to join us for Atlas Camp in Barcelona, you can still tune in online. Come back here on September 6th at 8:30 AM GMT to watch the keynote live. Can’t make that time? Come back later and watch the recording. In addition, keep an eye on the Atlas Camp site as we’ll be posting the recordings of the break out sessions as soon as they are available. Ralph is a Developer Advocate for ecosystem developers.", "date": "2018-09-01"},
{"website": "Atlassian", "title": "Atlassian announces Jira Ops and intent to acquire OpsGenie", "author": ["Aileen Horgan"], "link": "https://blog.developer.atlassian.com/jira-ops-incident-management-platform/", "abstract": "Today Atlassian announced two significant moves we’ve made to help companies resolve incidents faster and incur fewer incidents over time. First, we’ve entered into an agreement to acquire OpsGenie, a leader in incident alerting. OpsGenie helps over 3000 customers including The Washington Post, Air Canada, and Overstock manage their on-call schedule and notify the right people as soon as an incident occurs. Second, we’re launching a new product called Jira Ops to serve as a central incident command center, giving response teams one place to coordinate their response during a major incident. Click here to see the full announcement . We will continue to work with existing app vendors and partners who have published apps in this space. We value your partnership and together we'll pursue our goal to serve IT teams globally. This will mean additional opportunities to build apps that fit into the modern IT puzzle, along with consulting, integration, and staff augmentation opportunities. We will continue to work with new partners who want to list their incident-alerting apps on our marketplace. We have a long history of collaborating with partners where one or more of our products overlaps with their offerings. We're committed to giving our customers a variety of options, so that they can choose the solutions that best fit the needs of their modern IT teams. Jira Ops is now available through our early access program. Get early access to Jira Ops now by setting up and account at https://www.atlassian.com/ops . We’ll have more information about Jira Ops available once we announce pricing and make it available for sale (aiming for a paid 1.0 version in early 2019). We’ll also be hosting a live AMA on the Atlassian Community on September 5th from 10am to 11am PDT. Matt Ryall, Jira Ops Product Manager, will be taking questions on all-things Jira Ops and incident management. You can head over to start asking and up-voting questions now, then tune in for the live session on September 5th at 10am PDT. We hope to chat with you more there! Until the acquisition is fully closed, Atlassian and OpsGenie will continue to operate as separate and independent companies. Once the acquisition is final we will share more information with Atlassian Partners on plans to integrate OpsGenie into our business along with timelines and specifics for running evaluations, quoting and transacting, Enablement Academy training, product benefits for Partners and much more. We know this is exciting news, so please be patient while we close out the acquisition. We look forward to working with the OpsGenie team to offer you the best incident response solution on the market and unleash more of your customers’ potential. Together, OpsGenie, Jira Ops, and Statuspage are a compelling incident management solution for Atlassian partners to take to market upon deal close. These tools may unlock new opportunities within your customers’ IT teams and will help your customers respond and resolve incidents faster.", "date": "2018-09-04"},
{"website": "Atlassian", "title": "Say hello to EAP milestone 02 for Jira 8.0!", "author": ["Magda Michalak"], "link": "https://blog.developer.atlassian.com/eap02/", "abstract": "Following on our bi-weekly release commitments, we’re proud to present the next EAP milestone (m02) for Jira 8.0. Again, we worked hard on significant changes and updates. Some of these include Jira platform changes, moment.js upgrade, and further API changes to Lucene. With this EAP milestone, you can also run Jira on OpenJDK 11. For all the details, see Preparing for Jira 8.0 . To download this EAP milestone, go here . Did you like the milestone or want to suggest an improvement? Leave us a comment on the developer community . If you want to review the full set of changes for Jira 8.0, refer to the Jira 8.0 announcement . Best, The Jira team", "date": "2018-09-04"},
{"website": "Atlassian", "title": "Integrating with Portfolio for Jira’s Public Java API", "author": ["Tim Cerexhe"], "link": "https://blog.developer.atlassian.com/integrating-with-jpos-public-java-api/", "abstract": "While writing a plugin for Jira Server can be understandably daunting at first, writing a plugin for a plugin like Portfolio may just feel like a step too far.  But once you know a few tricks, you’ll find out soon enough that there’s really not much to it. In this walkthrough, we’re going to show you how you can write a Jira plugin that integrates with the recently announced Portfolio for Jira Java API . With this demo, it will take less than 15 minutes — to get from nothing to working. Before you begin, make sure you have a copy of the latest Portfolio Server OBR from Marketplace , and you’ve locally installed the latest Atlassian SDK . We’re going to create a new plugin for Jira by running this command in a terminal: Put something sensible for group and artifact id, or use these values: Let’s fix up our dependencies in the root pom.xml file. We want to upgrade a few things, so let’s update the  section at the bottom to look like this: Notice that we’ve upgraded Jira and Spring Scanner, and have added a JPOS target. Now let’s import the additional dependencies we want to use in our plugin: To complete the spring scanner upgrade: Since our plugin needs Portfolio , and Portfolio needs Jira and Jira Software , let’s add the necessary applications inside the maven-jira-plugin <configuration> block: To make startup a bit more stable, we also added some magic sauce to the <configuration> block: Next, you’ll need to create or overwrite the file src/main/resources/META-INF/spring/plugin-context.xml with: The only difference should be the /2 after all the atlassian-scanner URLs. Now, we can create pages and menus per usual. For example, let’s create a page template at src/main/resources/html/success.vm : You can then register your menu and web page in src/main/resources/atlassian-plugin.xml : You can also configure some default translations in src/main/resources/api-demo.properties : Now, we can write a page view that calls the Portfolio API and renders the result to a page. Create a new Java file at src/main/java/com/atlassian/jpos/demo/DemoView.java with: You can now build and run your plugin: (or use atlas-debug during active development). This will build and package your plugin, start up a version of Jira with Jira Software, and install your plugin. Unfortunately, Portfolio is not available as a public application, so your plugin will fail to start (OSGi can’t find the PlanAPI !). However, Jira with Jira Software should start up fine. We can fix this by logging in (username: admin , password: admin ), going to the Add-ons section under the Administration menu, and manually uploading the Portfolio plugin on the Manage add-ons tab. ℹ️ TIP: You should only have to manually add the Portfolio plugin after running atlas-mvn clean . Now, you’ll need to re-enable your plugin. You’ll see two greyed-out copies of api-demo: one is the plugin , and the other is a testing module . You can go ahead and enable them both — though for this walkthrough, you’ll only need to enable the copy with com.atlassian.jpos.demo.api-demo add-on key. This will start up Portfolio. Navigate to the Jira dashboard, or refresh the page, and the JPOS API Demo menu should now appear in the header. Click JPOS API Demo to access your new Public API page, which should show the result of calling the Portfolio API. Success! Now go forth and plan ahead! You can find all the code on bitbucket . Reach out to us in the User or Developer Communities if you have any questions, or even to share any new integrations with Portfolio that you may know of. ❤️ The Portfolio for Jira Server Team", "date": "2018-09-05"},
{"website": "Atlassian", "title": "Writing great docs for your app, part one", "author": ["Becky Todd"], "link": "https://blog.developer.atlassian.com/writing-great-docs-for-your-app/", "abstract": "Knowing how to document your app can be challenging. If you’re new to crafting documentation, you may be asking questions like: A large part of my job is helping people answer these questions, and in this blog I’d like to focus on one thing that you can do to help create useful and consistent content. I’d be a very slow writer if I had to create all new documents from scratch (if you want proof, just ask my colleagues how long it took me to write this blog post). The truth is that writing documentation is hard and being confronted with a blank page is one of the hardest parts. It can be a monumental task to figure out what to say, even when you know what you want to write about. Fortunately, using templates is a shortcut that can take some of the pressure off of writing. There are many reasons why it is a good idea to use templates, but I’d like to call attention to two especially important benefits: Below are three templates from our Writing toolkit for developer documentation, which you could adapt to meet your needs. A concept page explains complicated concepts simply by providing an overview of a particular topic, including relevant contextual information. These pages focus on the why not the how, and answer questions like these: Concept pages are a great place to include broader-level screen captures, images, and diagrams, especially for complex topics. You may also include other media, such as videos or short animations. A task page provides instructions for completing a piece of work by providing a simple, straightforward description of how to do a particular task. These pages focus on the how not the why, and answer questions like these: Task pages should not repeat content from concept pages or resemble guided walkthroughs in tutorials. Instead, the main purpose of a task page is to help the reader get something done. Create a task page when you need to document: A tutorial helps the reader learn a new skill or technique by walking them through an example, step-by-step. Tutorials briefly explain how concepts work together without being a repeat of a conceptual page or as straightforward as a task page. Below are a couple examples of use cases that a tutorial could address: Often, tutorials are written to help newer users complete first tasks or learn new concepts, but tutorials are also appropriate for all skill levels. It is helpful to state the intended skill level in the introduction so that readers can identify if the tutorial is right for their needs. Below are some tips for creating a great tutorial: We’ve spent a lot of time designing the templates above, and I’ll let you in on a little secret: Sometimes it is okay to deviate from a template. But you should do so with care, especially if you’ve never written a particular type of document before. Here are a couple of scenarios that may happen when you’re new to writing with templates: For the first scenario, it may be as simple as breaking the page up into parts, but you may also find that the documentation process has revealed some useful product feedback. I’ve experienced this a number of times over the years, and while it isn’t easy or fun to do, it has helped lead to better product outcomes. The second scenario is easier. Maybe you need to use a new template, or maybe you can trim a few sections from the template to support the content. Just be sure that you’ve covered the necessary information adequately. For example, it is okay to have a task page with a single set of instructions if that’s all there is to say. Stay tuned for other posts in this series with more helpful tips on writing documentation for your users.", "date": "2018-09-07"},
{"website": "Atlassian", "title": "OpenJDK comes to Jira Server 7.13 EAP", "author": ["Tomek Bartyzel"], "link": "https://blog.developer.atlassian.com/jira-7-13-eap/", "abstract": "We’ve just released a sample of Jira Server 7.13 as part of the Early Access Program (EAP) to inform you about the changes coming to Jira. In this release, we’ve focused on adding the support for OpenJDK 8, which you can already test in this EAP. OpenJDK is an option that we provide, however we’ll continue to bundle Jira with Oracle JDK. You can read more about these changes in the development guide for Jira Server EAP. For any questions you have, reach out to our developer community . We’ve prepared separate EAP Server downloads for Jira Core, Jira Software, and Jira Service Desk, so just pick your favorite here: Download the Jira 7.13 EAP . EAP releases are early previews of Jira during development. We release these previews so that apps developers can see new features as they are developed and test their apps for compatibility with new APIs. The Jira Server 7.13 EAP is best explored hand-in-hand with the Preparing for Jira Server 7.13 development guide . The development guide contains all changes in Jira Server 7.13, and important new features that may affect app developers.", "date": "2018-09-13"},
{"website": "Atlassian", "title": "EAP milestone 03 for Jira 8.0 is here!", "author": ["Tomek Bartyzel"], "link": "https://blog.developer.atlassian.com/jira80_eap03/", "abstract": "Following on our bi-weekly release commitments, we’re proud to present the next EAP milestone (m03) for Jira 8.0. Here are the changes in this milestone: For all the details, see Preparing for Jira 8.0 . We’ve prepared separate EAP Server downloads for Jira Core and Jira Software, so just pick your favorite here: Download the Jira 8.0 EAP . Did you like the milestone or want to suggest an improvement? Leave us a comment on the developer community . If you want to review the full set of changes for Jira 8.0, refer to the Jira 8.0 announcement . Best, The Jira team", "date": "2018-09-13"},
{"website": "Atlassian", "title": "EAP 02 for Jira 7.13 is ready!", "author": ["Magda Michalak"], "link": "https://blog.developer.atlassian.com/eap02-713/", "abstract": "Here's a chance to try out the upcoming Jira 7.13 that will be the long-awaited Enterprise release. In this milestone, we’ve upgraded Apache Tomcat to version 8.5.32. Feel free to scan our notes in Preparing for Jira 7.13 before you download this EAP. You can download the EAP here . If you use the maven.atlassian.com site, get milestone 7.13.0-m0003. Best, The Jira team", "date": "2018-09-27"},
{"website": "Atlassian", "title": "September Update on Upcoming Breaking Changes across Server Products", "author": ["Peter Scobie"], "link": "https://blog.developer.atlassian.com/september-update-breaking-changes-across-server/", "abstract": "Further to last month’s blog , we have an update on our plans to ship breaking changes across Server Products. While plans often change, we’re open about our work so the Atlassian Ecosystem can plan ahead to ensure a smooth experience for our customers. We’ve consolidated these updates into a single, easy to digest, post and will provide monthly updates until the end of the year. The AUI team posted their direction for AUI 8 in the developer community in July . The community post dives deeper in to the planned changes and outlines an upgrade path. The AUI 8 release is on hold as the AUI team work with Atlassian Server products on acceptance testing with their upcoming major versions. Found a bug? Raise a ticket in the AUI project on our Ecosystem Jira . We’re planning to ship Bitbucket Server 6.0 late November, 2018. We have some exciting new features lined up for the release. For one of these features it is necessary for us to deprecate direct access to repositories on disk for apps. You can find the necessary details in the API changelog . Apart from this, we plan to add support for Java 11. Java 8 is reaching end of life in January 2019. After this time no further fixes, including security patches will be available for it. As a core dependency for Bitbucket Server such patches are critical. Also, in Bitbucket Server 6.0, we will end support for all versions before Git 2.11: In 6.10 (Data Center), we introduced a new way to handle resource intensive tasks in a sandbox (now called ‘external process pool’). This started with document conversions and coming soon in 6.12 we will also be handling PDF exports in the same way. The external process pool isn’t available for add-ons at present, but this is something we might consider doing in the future. In the upcoming release of Confluence 6.12, we also have a few notable changes (check out preparing for Confluence 6.12 for more details.) We are currently planning to release support for Java 11 in an upcoming feature release, circa October 2018. This will enable customers to run on Java 11, but with Java 8 language compatibility mode (which means features that are new to Java 11 will not work). We are also continuing our work on the TinyMCE editor upgrade from 3.x to 4.x. This has been ongoing for some time now and hopefully everyone has checked their apps for compatibility. If not, check out how to test your add on with the upgraded TinyMCE4 editor . It will not be coming in 6.12, however we are oh so close. Confluence Server 7.0 scope and timing is currently under review. This will include full Java 11 support. We’re still refining what other goodies will be included but are strongly contemplating upgrading Guava, removing deprecated JS Globals, converting all plugin JavaScript to AMD modules (and no longer use globals in those modules) and remove deprecated SOAP/XML-RPC methods. In order to remove the deprecated methods, we anticipate releasing new and equivalent REST API’s. Our customers are moving to IPv6 and Atlassian Server products are now compatible in these new environments. We’ve recently shipped IPv6 support for Bitbucket Server ( v5.8 ), Confluence Server ( v6.9 ), Jira Service Desk Server ( v3.14 ), Jira Server( v7.11 ). Portfolio for Jira Server version 2.15 is compatible with Jira Server 7.11 and is, therefore, IPv6 compliant. We are working on making Bamboo IPv6 compliant and will be shipping that in September. We plan to make FeCru compliant in 2019 and have no plans at the moment to add support for Hipchat Data Center. There is more information and implementation advice available in the product release notes. We’re planning to ship Jira Software 8.0 in late 2018. This release will include: Between now and the release of 8.0, we will release Early Access Program (EAP) milestones once every 2 weeks. More information on the 8.0 Early Access Program can be found here . Jira 8.0 will be working with Oracle JDK 8 and OpenJDK 11. This means that Java 11 features will not be supported in the source code (Java 8 compatibility mode). Hence, we will only announce limited Java 11 support without making Jira officially compatible with Java 11 until a future 8.x release. This will give app vendors time to update their products before we announce official compatibility with Java 11. Most likely, this will take place in January or February 2019. After we announce the official compatibility with Java 11, we will also strive to have OpenJDK 11 bundled with the Jira installer. We're excited to announce that our Jira Performance Tests (JPT) beta tooling is open for early access! The Jira Performance Tests help you test your apps at scale. For more details and how to get started, check out the development community blog post . In Jira Service Desk 4.0, we'll be updating our APIs to use Core Java Data types and Exceptions . We're introducing this change to make it easier to develop on Jira Service Desk. You can read the full deprecation notice . In accordance with the Java API Policy for Jira , we'll be permanently removing com.atlassian.fugue with the release of Jira Service Desk 4.0. You will need to update any scripts, integrations or apps that make requests to endpoints returning com.atlassian.fugue to use Core Java Data types and Exceptions instead. We’re in the process of upgrading our Server products to support Java 11. Jira Software, Bitbucket Server, Confluence Server are continuing their work as mentioned above with Portfolio for Jira and Jira Service Desk joining recently. Bamboo, FeCru and Crowd have yet to roadmap this work, but it is our intention that all maintained Server products will provide support for Java 11. Please check here for more details. If you have any questions or concerns about any of the changes mentioned here please raise a ticket in the Ecosystem Developer Service Desk .", "date": "2018-09-28"},
{"website": "Atlassian", "title": "REST API changes toward improving user privacy – deprecation notice and migration guides", "author": ["Neil Mansilla"], "link": "https://blog.developer.atlassian.com/api-deprecation-notice-user-privacy-improvements/", "abstract": "Earlier this year, we announced that Atlassian would be implementing a number of changes to our products and APIs in order to improve user privacy in accordance with the European General Data Protection Regulation (GDPR). This included the rolling out of changes to Atlassian Cloud product APIs to consolidate how personal data about Atlassian product users is accessed by API consumers. Today, we’re publishing details on those changes for both Confluence Cloud and Jira Cloud REST APIs. Migration guides for both Confluence Cloud and Jira Cloud show you how to update your apps and integrations to adopt the privacy-related changes. They describe how the user privacy improvements affect the REST APIs, provide a migration path to the new REST APIs, and list all of the REST API changes. Confluence Cloud migration guide Jira Cloud migration guide Today also marks the start of the deprecation period for these API changes. During the next six months, the APIs will continue to support, for example, fields like userkey and username alongside the new Atlassian user ID; however, when the deprecation period ends toward the end of March 2019, the APIs will only support the Atlassian user ID in requests and responses. We’ve created pinned topics in the Developer Community where you can post questions and read about how other developers are working through the changes. Check out the threads for Confluence Cloud and Jira Cloud . The API change details and migration guide for Bitbucket Cloud will be posted soon. Look out for a future blog post and Developer Community thread in the next several weeks. Neil Mansilla is the Head of Developer Experience at Atlassian. He’s a life-long developer with a special place in his heart for APIs, and a penchant for ops. If chicken pot pie was wine, he’d be a sommelier. Follow Neil at @mansillaDEV.", "date": "2018-10-01"},
{"website": "Atlassian", "title": "EAP milestone 04 for Jira Server 8.0 has just arrived!", "author": ["Magda Michalak"], "link": "https://blog.developer.atlassian.com/jira80_eap04/", "abstract": "…and is ready for action! You can test further Lucene improvements, upgraded Apache Tomcat, and jQuery version 2.2.4, to name just a few breaking changes. You can also enjoy the new look and feel of the Scrum/Kanban boards and backlogs. Make sure you read our Preparing for Jira 8.0 notes because the Apache Tomcat upgrade might require an action on your side. You can download this EAP here . If you use the maven.atlassian.com site, get milestone 8.0.0-m0012. The good news is, we are nearly done! You can still expect some changes related to the Java upgrade in the next EAP milestone, but that’s mostly it! Did you like the milestone or want to suggest an improvement? Leave us a comment on the developer community . If you want to review the full set of changes for Jira 8.0, refer to the Jira 8.0 announcement . Best, The Jira team", "date": "2018-10-02"},
{"website": "Atlassian", "title": "Change notice – required encoding of some character for Jira Cloud REST APIs", "author": ["Giles Brunning"], "link": "https://blog.developer.atlassian.com/changes-to-jira-cloud-rest-apis/", "abstract": "As part of our continued focus on the security of our Cloud platform, and a recent Tomcat update we’ve made, we’re introducing changes that’ll require the encoding of some characters used in REST API calls. We’ll make these changes on the 1st of April next year (2019) , giving developers and consumers of Jira Cloud APIs six months to make any necessary changes. These changes are not related to other GDPR-related API updates we’re currently making. These changes require the following characters to be encoded: Using these characters in a browser will auto-encode them, but URLs including the characters will not be supported when left unencoded and used programmatically in scripts or API calls. To find out more about this change please check out the official change notice .", "date": "2018-10-10"},
{"website": "Atlassian", "title": "Here comes EAP milestone 05 for Jira Server", "author": ["Magda Michalak"], "link": "https://blog.developer.atlassian.com/jira80_eap05/", "abstract": "Go ahead and download and test this EAP . If you're using the maven.atlassian.com site, download milestone 8.0.0-m0014. Your comments, as always, are welcome. We're planning to bundle another batch of breaking changes in EAP milestone 06. This milestone will contain more Java 11-related changes (further upgrades to platform libraries and components), small changes to Lucene (change in Service Provider), and upgrade of AUI to version 8. Great changes require great effort, especially that we want Jira 8.0 release to be a success. Right now, the plan is to have 2 more EAP milestones before the beta, which is due at the end of October. Then, a release candidate will follow. Jira 8.0 is planned to be out in December 2018. However, that might change depending on the feedback we are constantly gathering. We are currently reviewing the plan and will keep you posted. Your feedback is always welcome. We are doing our best to address it as soon as we receive it. If you've got a comment, feel free to post it here . Best, The Jira team", "date": "2018-10-11"},
{"website": "Atlassian", "title": "Atlassian Verified changing to Top Vendor", "author": ["Emilee Spencer"], "link": "https://blog.developer.atlassian.com/verified-renamed-to-top-vendor/", "abstract": "If you’ve been on Marketplace today, you may have noticed something different – our Atlassian Verified logos recently got a makeover! The Atlassian Verified program, which has been a part of the Atlassian Ecosystem since 2014, was developed in order to symbolize the high level of quality apps from the vendors. Now as Atlassian continues to scale and grow the ecosystem, we have decided to rename the program to Top Vendor to better align with the program requirements and customer expectations. It’s pretty simple actually – we’re bringing Atlassian Verified into the Top Vendor program to unify the programs around one brand. The Top Vendor program will have 3 tiers all with different benefits. Platinum and Gold Top Vendor status are based on sales while the requirements for Top Vendor (formally Verified) are based on a variety of factors. Marketplace vendors will still need to apply to be apart of the program, but vendors who are already enrolled in the Verified program will automatically gain Top Vendor status. Talk to us on the thread on the developer community . We’ll be happy to cover any questions you have 😀", "date": "2018-10-15"},
{"website": "Atlassian", "title": "Hello world! EAP milestone 06 is ready!", "author": ["Magda Michalak"], "link": "https://blog.developer.atlassian.com/jira80_eap06/", "abstract": "The Jira team has pulled it off again and prepared the EAP milestone 06 for Jira 8.0 for you to download. Just click this link and get ready to test. If you use the maven.atlassian.com site, just grab version 8.0.0-m0015. Today, we're pleased to present a Search Provider change to the Lucene library and the upgrade of AUI to version 8. The good news is, we do not anticipate further breaking changes related to Lucene. We’ve listed all you need to know about this EAP milestone in our Preparing for Jira 8.0 write-up. Currently, we're still planning to release EAP milestone 07 with last breaking changes related to Java 11 and Jira platform upgrade. We're happy to answer all incoming comments. Just posted them here and we'll be on them shortly. Best, The Jira team", "date": "2018-10-17"},
{"website": "Atlassian", "title": "Core concepts behind Jira Cloud next-gen projects and ongoing changes to our existing APIs", "author": ["Javier Ferro Garcia"], "link": "https://blog.developer.atlassian.com/next-gen-projects-jira-cloud/", "abstract": "The introduction of next-gen projects at the beginning of 2018 changed some Jira Cloud REST API assumptions and core concepts. As a result some apps experienced unexpected behaviour, few apps stopped working in next-gen projects. We should have communicated this change in behavior ahead of time and moving forward we will do a better job of keeping the developer community up to date on changes that would affect their apps. App developers and cloud REST API consumers may need to modify their code to support this new model. This blogpost explains core concepts behind next-gen projects and ongoing changes to our existing APIs to allow app developers to fully support next-gen projects. The goal of next-gen projects is to simplify Jira project configuration experience for administrators and end-users. Next-gen projects simplify the project configuration experience and allow non-jira-admins to create projects and configure them. Project configuration concepts and their application have been simplified without compromising Jira’s granular configuration capabilities. You can find more detailed information from previous blogposts from Hannah and Mythili . This blog will focus on the technical impact fo Connect App Developers and REST endpoint consumers. As mentioned before, there are new concepts in the next-gen model that did not exist in the classic model. Not only that, there were few assumptions that are not valid anymore in next-gen: We introduced the concept of project scoped entities. All configuration entities are bound to a single project and are not sharable with other projects (better to say “not yet”™) which is quite different to the classic model. Project scoped entities cannot use global entities. Example: An Issue Type created for a classic project cannot be used in a next-gen project. Because of project scoped entities, we cannot rely on the name uniqueness of these entities. In fact, it will be pretty common to have multiple projects with different issue types named \"Bug\". Next-gen projects no longer have schemes like Issue type screen schemes, Workflow scheme, Issue type schemes, Field configuration scheme, and Screen schemes. Notificationscheme, Permissionscheme, Project Roles, Screens, Workflows, Fields, Issue Types and Status. Projects is a special case as we need to know when a project is next-gen or not. Classic and next-gen projects have different mental models and constraints. Existing Apps may have to modify their code in order to be able to deal with both worlds. Most of the Apps will require minor or no changes. Some Apps will require major re-design to accomodate to the new model and prepare for a better project configuration experience. Mutation operations These changes are already available for project, issue type, notification and permission scheme. We will apply these API changes from v2 onwards. We will keep enriching existing REST endpoints using App usage as priority You should expect more blogposts to give you a clearer picture of next-gen Jira and the Jira Cloud Platform API Public roadmap . As part of first iteration, we are enriching the following endpoints: projects, issue types, project roles and permission schemes. GET /rest/api/2/project GET /rest/api/2/project/{projectIdOrKey} Project details for the specific project will be enriched with a field named style which could be classic or next-gen. As we already explained, we are enriching endpoint responses to provide additional scope information for those entities that could be project scoped. The operation for retrieving all issue types will contain additional information that the developer will need to consume to differentiate between scoped issue types vs global ones. GET /rest/api/2/project/{projectIdOrKey}?expand=issueTypes returns the project details expanded with the issue types related to that project. Follows the same pattern as Issue Types. Follows the same pattern as Issue Types. Yes, you don't need to retrieve the whole list. You can use specific endpoints to get the small subset of roles related to a project. At this moment, we haven’t enriched the status endpoint to return all statuses with the scope information; however, there is an API that gives you that information. Check the documentation for further details: In progress . We are committed to providing constant developer communications on the latest updates and changes. With that said, we are currently planning to take the same approach for the remaining scope entities. I hope we provided you with a good overview of the current changes and the impact this has on your apps. Please keep a look out for follow up blog post with more information about the upcoming changes. So, when should you check your existing code to see if it's compatible with these changes? We will keep working in making next-gen projects awesome and more powerful than classic from the user and developer perspective. Come along on this journey with us and watch the next-gen tag in the Atlassian Developer Community .", "date": "2018-10-25"},
{"website": "Atlassian", "title": "Cloud instances can now be renamed, learn how this might impact your Cloud app.", "author": ["Norman Atashbar"], "link": "https://blog.developer.atlassian.com/site-rename/", "abstract": "We have assembled a team here in Atlassian to make the necessary changes to enable our cloud customers to rename their site name. Currently site renames are performed via export and import of sites, which has previously caused issues for Connect apps due to duplication of records. In order to prevent those issues, we had to exclude Connect apps during the export process. As a result, a site would lose all the Connect apps during export and they have to be installed again once imported. When the site rename functionality becomes available, there will be no longer any need to perform the export and import, as a result Connect apps will not be impacted. Well, not quite, keep reading. In order to provide this functionality in a timely manner, we have decided to leverage an existing lifecycle method , \"installed\", to update the installation and send a new payload with the new base URL of the site. Everything else will be the same, and the payload will be signed with the existing shared secret. This change will have an impact on your app when: your app cannot process the new installed payload and update the base URL in its database. your app uses the base URL as a primary key to identify a specific Jira or Confluence site. We recommend using the client key for this. your app stores full urls (including base URL) (a Jira issue URL for example) Please note that your app will not break if they necessary changes aren’t made, but it will be slower due to redirects being necessary. Please also note that your Connect app must use an HTTP client library that can handle redirects. Feel free to ask any question you have in the developer community thread .", "date": "2018-10-26"},
{"website": "Atlassian", "title": "October Update on Upcoming Breaking Changes across Server Products", "author": ["Peter Scobie"], "link": "https://blog.developer.atlassian.com/october-server-breaking-changes-update/", "abstract": "Further to last month’s blog , we have an update on our plans to ship breaking changes across Server Products. While plans often change, we’re open about our work so the Atlassian Ecosystem can plan ahead to ensure a smooth experience for our customers. We’ve consolidated these updates into a single, easy to digest, post and will provide monthly updates until the end of the year. The work on AUI 8 has been resumed, with beta and final releases available in November. A test preview of the upgrade guide is available . Consult the changelog for a consolidated list of differences in this release. Feedback for and discussion of the release is happening on our developer community post for the AUI 8 direction . Found a bug? Raise a ticket in the AUI project on our Ecosystem Jira . We’re planning to ship Bitbucket Server 6.0 late January, 2019. We have some exciting new features lined up for the release. For one of these features it is necessary for us to deprecate direct access to repositories on disk for apps. You can find the necessary details in the API changelog . We are also going to remove the notification handler plugin point and related notification API . When you need to send notifications, rather than using this deprecated API just listen to the event you are interested using a listener based on specific event and get the recipients via com.atlassian.bitbucket.watcher.WatcherService to send notifications. Apart from this, we plan to add support for Java 11. Java 8 is reaching end of life in January 2019. After this time no further fixes, including security patches will be available for it. As a core dependency for Bitbucket Server such patches are critical. Also, in Bitbucket Server 6.0 we’ll end support for all versions before Git 2.11: In recent releases we’ve added a new way to handle resource intensive tasks in a sandbox (now called ‘external process pool’). This includes document conversions PDF exports. The external process pool isn’t available for add-ons at present, but this is something we might consider doing in the future. We’ve also added new SVG icons in the editor insert menu, we recommend you update yours to match 😉 We have slightly changed our plans around Java support. Instead of supporting Java 11 initially, we will be introducing support for AdoptOpenJDK 8 in an upcoming feature release, check out Preparing for Confluence 6.13 for more details. This will ensure that customers have an alternative to the paid Oracle JDK from 2019. Confluence 6.13 will also be the next designated Enterprise release . Full Java 11 support is planned to be delivered with Confluence 7.0 sometime in 2019. We are continuing our work on the TinyMCE editor upgrade from 3.x to 4.x. This has been ongoing for some time now and hopefully everyone has checked their apps for compatibility. If not, check out how to test your add on with the upgraded TinyMCE4 editor . It will not be coming in 6.13, however we are still oh so close. Confluence Server 7.0 scope and timing is currently under review. We’re still refining what other goodies will be included but are strongly contemplating upgrading Guava and removing deprecated JS Globals. We will be removing some very old and deprecated code from 1.x through 4.x releases. We will unlikely be removing the already deprecated SOAP/XML-RPC methods at this time. We are also working on a new Search experience to replace the existing Quick Search experience ( what is quick search? ). If you have an app which modified interacts with the search experience then keep an eye on the upcoming EAP page for updates. Our customers are moving to IPv6 and Atlassian Server products are now compatible in these new environments. Our good news this month is that we’ve shipped IPv6 support for Bamboo in the new v6.7 release. This adds to the suite of already compliant products – Bitbucket Server ( v5.8 ), Confluence Server ( v6.9 ), Jira Service Desk Server ( v3.14 ), Jira Server ( v7.11 ). Portfolio for Jira Server version 2.15 is compatible with Jira Server 7.11 and is, therefore, IPv6 compliant. There is more information and implementation advice available in the product release notes. We’re are are continuing our work on Jira Software 8.0 and plan to deliver the release by the end of the year. If you haven’t started following our Jira Software 8.0 EAP program , we highly recommend taking a look at the most recently released version. The latest EAP version contains all breaking changes that will be included in the release. The Jira Software 8.0 release will include: Early Access Program (EAP) Between now and the release of 8.0, we will continue to release Early Access Program (EAP) milestones once every 2 weeks. More information on the 8.0 Early Access Program can be found here . Java 11 Jira 8.0 will be working with Oracle JDK 8 and OpenJDK 11. This means that Java 11 features will not be supported in the source code (Java 8 compatibility mode). Hence, we will only announce limited Java 11 support without making Jira officially compatible with Java 11 until a future 8.x release. This will give app vendors time to update their products before we announce official compatibility with Java 11. Most likely, this will take place in January or February 2019. After we announce the official compatibility with Java 11, we will also strive to have OpenJDK 11 bundled with the Jira installer. Jira Performance Testing tool We're excited to announce that our Jira Performance Tests (JPT) beta tooling is open for early access! The Jira Performance Tests help you test your apps at scale. For more details and how to get started, check out the development community blog post . In Jira Service Desk 4.0, we'll be updating our APIs to use Core Java Data types and Exceptions . We're introducing this change to make it easier to develop on Jira Service Desk. You can read the full deprecation notice here . In accordance with the Java API Policy for Jira , we'll be permanently removing com.atlassian.fugue with the release of Jira Service Desk 4.0. You will need to update any scripts, integrations or apps that make requests to endpoints returning com.atlassian.fugue to use Core Java Data types and Exceptions instead. Please raise a ticket in the Ecosystem Developer Service Desk .", "date": "2018-10-31"},
{"website": "Atlassian", "title": "Bamboo 5.10.0 EAP release now available", "author": ["Esther Asenjo"], "link": "https://blog.developer.atlassian.com/bamboo-5-10-eap-release-now-available/", "abstract": "The Bamboo development team is proud to announce that the first Early Access Program (EAP) release of Bamboo 5.10.0 is now available. There are a large number of API changes and library changes, so now is the time to begin testing and planning new compatibility for your add-on. EAP releases are early previews of Bamboo during development. We release these previews so that Atlassian’s add-on developers can see new features as they are developed and test their add-ons for compatibility with new APIs. The number of EAP releases before the final build varies depending on the nature of the release. The first publicly available EAP release for Bamboo 5.10.0 is 5.10.0-beta01 . Developers should expect that we will only provide a few EAP releases before the final release. To find out what’s in this release, check out the Bamboo 5.10.0-beta01 Release Notes . Download the EAP release from our site and start testing your add-on for compatibility with changes to Confluence. Be sure to take a look at our Preparing for Bamboo 5.10.0 guide for details of changes that may affect your add-on. We’ll update that page regularly, so we recommend you check in on it regularly to ensure you’re aware of any changes.", "date": "2015-10-21"},
{"website": "Atlassian", "title": "Six cool features of the Git 2.x series", "author": ["Nicola Paolucci"], "link": "https://blog.developer.atlassian.com/cool-features-git-2-x/", "abstract": "It’s been a while since I have reviewed Git release notes but that does not mean I haven’t been eagerly reading more recent ones and incorporating new gems in my routines. To celebrate my birthday (yay!) and the recent release of Bitbucket Server I have collected a round of my favorite features for the Git 2.x series (up to 2.6 ). Let me know if you end up using any of these. In Git 2.6 the rebase command received welcome attention. One of the more interesting new flags is this one: You can now specify if uncommitted changes should be stashed temporarily when you start a rebase or if the operation should fail in that case. You can make this behavior permanent by setting a new configuration option rebase.autostash . If you set it to true Git will stash your uncommitted changes before the start of the rebase and it will re-apply the changes at the end of the operation. If you turn on this feature be ready to handle possible conflicts when the stash content is popped back. If you turned on the autostash flag you will be able to temporarily disable it with the new --no-autostash command line option, courtesy of upcoming release 2.7 . A new worktree command has been introduced in Git 2.5. It allows you to maintain multiple checked out work trees of the same root repository. It’s fantastic. Before this command existed one had a few ways to switch context and work on different streams at the same time: With worktree one can create a sub-folder of the root project with a specific checked out branch – or ref for that matter – and keep working on it until done. The command saves you from having to create separate – out of band – clones. So imagine you’re happily working on the develop branch but you are tasked to work for a few days on the rewrite branch that has massive changes. You can create a worktree for this extended work and leave your other ongoing effort where it is. The syntax of the command is: So in our example the command would become: The command creates a sub-folder in the root of your project named – surprise – rewrite-folder which contains the checked out branch rewrite . We can push and pull from that sub-folder as if we are at the root of our Git project and when we are done with that work we can simply delete the folder rewrite-folder . The administrative files stored in .git/worktrees will be eventually pruned, but if you want to be thorough you can easily prune that data with: This tip came to my attention because Git contributor Luke Diamand, interviewed in the most recent Git Rev News , mentioned that his favorite Git feature nowadays is git commit --fixup . Here’s the setting to understand how to use it. You are working on a feature or a bug fix; You haven’t shared that code with anyone else yet and you realise that one of the previous non- HEAD commits should be changed – maybe to remove code which should not be there, maybe to fix a bug without creating an extra – unclean – commit. Here’s where commit --fixup comes in: Say I have a commit with id 026b6b5 that reads: I make some changes to my work tree and I just mark this as a “ fixup ” of that feature commit: This command will create a commit for you that looks like this: The final step before I push this branch is to interactively rebase it using --autosquash . Git will pick up all the annotated commits, like the fixup! one above and weave the fix into the right place: The command above will open an editor on the rebase interactive sheet with the correct action already pre-filled for you. You can omit the --autosquash option if you have set the global configuration variable rebase.autosquash=true in your .gitconfig . The command show-branch – in the words of the Git documentation – “ shows the commit ancestry graph starting from the commits named with <rev>s or <globs>s semi-visually “. Adding the --topics flag will show only commits that are NOT on the branch given. This is useful to quickly hide commits that already appear in the main branch. A refinement has been added on 2.5 that makes the command: Compare the given commit against all the local branches. For example in the project containing this blog the result is: Sometimes you want to look for commits that do not have a specific string in their messages. This is where the new flag --invert-grep comes in. For example find all commits that do not contain a certain issue id: It will return all commits that do not contain PRJ-25 in the commit messages. Nifty. You can define the default behavior of a naked git push by setting the push.default configuration parameter. In the cases where the push command is updating more than one ref – like when pushing multiple branches or tags at the same time – you can now specify a new option --atomic that guarantees the updates either all succeed or all fail. There you have it. The drop is done. Did you find anything interesting? Are you using other esoteric flags in your daily development work? I’d like to hear about it, I am always eager to learn new tricks and flags so tweet at me at @durdn or @atlassiandev or leave a comment here.", "date": "2015-10-26"},
{"website": "Atlassian", "title": "Profiling Node.js apps", "author": ["Travis Smith"], "link": "https://blog.developer.atlassian.com/profiling-node-apps/", "abstract": "At Atlassian, we have add-ons we develop and use to enable more powerful workflows within our products. Using Atlassian Connect , we can write those add-ons in any language. We often write those add-ons using Node.js. But once in a while we run into a performance problem with an add-on. I had to learn how to profile a Node.js app, so we could live happier lives with less waiting. If you poke at node via node --help , it doesn’t appear if there are lot of options. Since Node.js is just a front end for V8 , you can find the options hidden away behind node --v8-options . I wouldn’t jump to typing out that command in your terminal just yet. There are a couple hundred options that you could pass to the V8 engine. A little searching turns up V8 supports node --prof . If you run your Node.js application with the --prof flag (e.g. node --prof app.js ), node will generate a -v8.log file, e.g. isolate-0x101804600-v8.log , of stack samples. I believe Node.js added this in v0.12.x, so if you’re stuck on an older version you might be out of luck. Peeking in the log, it appears to be a CSV file with entries of execution details. This generates data every few moments, taking a “tick” by peeking at the stack to see where your program is spending time. Raw data is a bit hard to use, but there’s a nice profiling visualization tool to collate the data for you in the V8 repository. This profiling visualization tool processes the file in your browser and provides a plot and ordered call information. Select the file your node instance generated and hit “Start”. The profile information will give you a block of text which might start off like (path edited so it fits on the screen): Showing me a few things of interest at the top of usage list of the profiler. UglifyJS is the first bit of userland code showing up. But UglifyJS doesn’t help us find a problem with normal requests, so you can change range of ticks (effectively time, but each tick should align with the next iteration in the event loop) in the page to something that would exclude the app startup and thus one time costs, such as UglifyJS. There is a plot generated with this tool; but I’ll admit I haven’t figured out how to interept that data. If someone knows of some good instructions on interepting that data, feel free to share it in the comments. So between using node --prof and the visualization tool you should be able to see bottlenecks in your application! Using the --prof flag isn’t perfect, if you need more details you would need to instrustment your code instread of sampling it to better results.", "date": "2015-10-29"},
{"website": "Atlassian", "title": "Changes to comments", "author": ["Travis Smith"], "link": "https://blog.developer.atlassian.com/changes-to-comments/", "abstract": "Earlier this year, we moved comments on confluence.atlassian.com to Atlassian Answers , and now we’re moving comments from documentation pages on developer.atlassian.com to match. We will provide links and integrations with Atlassian Answers to give you a place to ask questions and get support. Disqus comments on blog posts, such as this post, will remain. A preview of what you can expect to see next week: We believe that this will help increase the response rate on comments that appear on developer.atlassian.com. Often comments go without response, and the teams with the right knowledge to respond don’t engage — but Atlassian Answers has a positive rate for responses to questions. Teams in Atlassian and around our ecosystem already answer questions on Atlassian Answers. Comments that revolve around code can have a wider context than a single page, and Atlassian Answers provides a platform that makes that more obvious. Access and visibility to great questions and responses automatically happens, great content isn’t lost as documentation changes. We plan on rolling out these changes to developer.atlassian.com on Wednesday, December 9, 2015.", "date": "2015-12-03"},
{"website": "Atlassian", "title": "Atlassian Orchestration with Docker: multi-host support for the win!", "author": ["Nicola Paolucci"], "link": "https://blog.developer.atlassian.com/atlassian-docker-orchestration/", "abstract": "After the fantastic DockerCon Europe and the recent releases of Docker 1.9.1 , Compose 0.5.2 and Swarm 1.0.1 I finally have all the missing bits to automatically deploy a suite of Atlassian products to a swarm cluster without supervision: This is the dream I have – and we probably all have as an industry: to describe our software components, describe how they are linked together and let the infrastructure automatically arrange itself to match our needs. It’s here! It has been cooking for a while and depending on the technology stack maybe it is already there for you. Nonetheless the Docker suite of tools have reached that moment for me. And it’s glorious. Let me show you an example of the possibilities. I start with a meaningful goal : Deploy Bitbucket Server and PostgreSQL to a 3-node cluster created with Docker Machine and managed via Docker Swarm . This is the end result I have in mind, where my setup does not mention any hard-coded IP address: As a prerequisite I need an account on an IaaS provider, this time around I choose Digital Ocean but any other of the Docker Machine drivers will do. I create an authenticated API_TOKEN and this allows me to create nodes at will using “ docker-machine “. The new multi-host capabilities of Compose and Swarm require a more complete discovery service than the basic Docker Hub Swarm tokens, so in this piece I will use Consul , a discovery server and key/value store from HashiCorp. This specifies the “ ams2 ” region, passes my token and names this machine “ consul “. Now we can create a cluster of 3 machines, with slightly different requirements. Take note we chose a specific Debian 8.2 image debian-8-x64 , the default Ubuntu image that Docker Machine chooses on Digital Ocean won’t work because it has an older kernel that does not work with Docker overlay networks. We also pass cluster-store and cluster-advertise to the Docker engine on this new machine with information on how the swarm can store keys and values of the infrastructure we are building. Those are stored on the consul instance we readied before. We require the machine to have 2GB of RAM and tag this machine with label java so that we can deploy our application based on labels. We tag this machine with label db so that we can deploy our application based on labels. Next on the list is to write the multi-host configuration in a docker-compose.yml , which will take care of starting both our Java application and our database in the proper order. It will also create a transparent overlay network between the cluster nodes involved. The interesting points of the setup are: This is the complete docker-compose.yml : License data-only was built from a Dockerfile written like this: And the only file it stored in reality is a single bitbucket.properties file with this: To start everything we can now invoke docker-compose , making sure we turn on the multi-host networking and specify we want to use an overlay network: The result is our application deployed to the cluster: Note that the Java application “Bitbucket Server” was deployed to the instance with 2GB of RAM labelled java as planned, and the PostgreSQL onto node2 which was labelled db . Beautiful. While creating the setup above I ran into a whole set of issues, partially due to the novelty of the tools and partially due to my hastiness. To understand what happened I even went looking into the source code . Turns out that to get the full blown multi-host support in compose and swarm, you need at least a 3.15+ Linux kernel (as explained here ), and the default Digital Ocean Ubuntu image had an older one: To make things work I had to add --digitalocean-image \"debian-8-x64\" to my docker-machine create command. I tried all Ubuntu images and they all failed, including 15.04 ,so I had to use an image for Debian 8.2, that had the proper kernel version and didn’t crash. The source of the above configurations can be found on Bitbucket. This for me was the first magical step into having an entire suite of Atlassian tools deployed and run automatically onto a Docker Swarm. Stay tuned for the next chapter in the series. If you found this interesting and want more follow me at @durdn or my awesome team at @atlassiandev .", "date": "2015-12-16"},
{"website": "Atlassian", "title": "Introducing Atlassian Tech TV", "author": ["Chris Mountford"], "link": "https://blog.developer.atlassian.com/introducing-atlassian-tech-tv/", "abstract": "For my tenth year working at Atlassian, I’ve changed from doing 100% hands-on software development to focus on writing, speaking and, most recently, video production. It’s a freaky change for me – shooting and editing video is quite unlike software development but I love change. This project gives me an opportunity to showcase the knowledge and insights of the talented Atlassians I work with every day. It’s also an opportunity to bring my engineering methods and tools into the video production workflow, but that’s a topic for another time. Atlassian Tech TV is a new project that takes you inside Atlassian where you get to hear how we make software directly from members of Atlassian’s software teams. We kick off with a series of one-on-one interviews as I chat with some of our most talented people from roles across engineering, design, QA, and product management. We publish to our dedicated Atlassian Tech TV YouTube channel weekly, so subscribe now to catch episodes as soon as they’re available. To get the flavor of the content of these interviews, check out the trailer: From each interview I take at least one five minute topical selection and cut a preview video. People are busy and can’t always commit time to watch the whole interview. But it’s impossible to actually reach any depth or subtlety on innovative topics in only five minutes, and after a shoot I end up with more than 30 minutes of great discussion, so I also make the full interviews available, broken into two digestible parts. We aim to bring a diverse range of people from different roles and levels of experience, each with a unique perspective and valuable insights on how Atlassian makes products. I want our subscribers to get something useful out of every episode, something to take back to their teams and hopefully, every now and then, a new way of looking at things that brings a breakthrough change of thinking. In my first interview, I sit down with Nick Pellow , development manager for Bitbucket Server (previously known as Stash), to discuss topics such as the technique of dogfooding and the importance of root cause analysis for bugs. We also learn a little about how Nick got into software. As a teenager he was earmarked by his father for solving the infamous blinking twelve problem on the family VCR years before expanding his technical passion into computer science at university. Check out what Nick Pellow has to say about eating one’s own dog food here: Where Nick’s experience led us into topics around people leadership, my interview with software architect Robbie Gates features an interesting discussion on technical leadership. Robbie and I discuss cloud-native architectures and we also delve into some high level technical topics such as functional programming (FP), even touching lightly on its mathematical basis in category theory, a branch of algebra that is Robbie specialized in during his previous life in academia. Here’s the five minute preview episode of my interview with Robbie: Both Nick’s and Robbie’s interviews have been very popular, sparking conversations online and generating a growing amount of positive feedback. People are hungry for the inside story of how Atlassian makes software. Things are off to a great start with six videos published so far but more than anything I’m excited for what’s to come. I have about ten more interviews in my edit queue and top of that list is my interview with front-end developer Lucy Bain . Lucy moved from the USA to a place she’d never visited and professed to know almost nothing about: Sydney, Australia. Taking a job at Atlassian, she joined the Bitbucket Server product team. In previous roles she worked as a full-stack developer with a specific interest in Ruby. Lucy shares her experience in agile methods and takes a very practical approach to the benefits of practices like pair programming. Check out Lucy’s blog on Pair Programming from earlier this year. Other episodes coming down the pipe include my chat with principal engineer Jed Wesley-Smith, who, in addition to providing architectural guidance on back-end implementation details, is a great supporter of our developer communities as he volunteers to run many of the meetup events we host at our offices, making sure the beer is cold. For insights from my conversations with Atlassians, subscribe to Atlassian Tech TV , and if there’s an Atlassian you’d especially like me to interview, or if you have feedback about the shows, hit me up on twitter: @chromosundrift", "date": "2015-12-17"},
{"website": "Atlassian", "title": "Neat new features in Git 2.7", "author": ["Tim Pettersen"], "link": "https://blog.developer.atlassian.com/git-2-7-release/", "abstract": "A quick two months after 2.6, Git 2.7 has been released with a plethora of features, fixes and performance improvements. Here’s some of the new stuff we found interesting on the Bitbucket team. The awesome git worktree command, introduced in Git 2.5 , lets you check out and work on multiple repository branches in separate directories, simultaneously. For example, if you need to make a quick hotfix and don’t want to mess with your working copy you can check out a new branch in a new directory with: Git 2.7 adds the git worktree list subcommand to display your repository’s worktrees (and their associated branches): The git bisect command’s support for worktrees has also been improved. The refs that bisect uses to track “good” and “bad” commits have been moved from .git/refs/bisect to .git/refs/worktrees/$worktree_name/refs/bisect so you can run bisects concurrently across multiple worktrees. For completeness, as of Git 2.7, you can also now clone a worktree on disk. This creates a new, independent Git repository (not another worktree). Fun-fact: Git worktrees aren’t just for branches! When researching the new Git functionality for this post I compared the Git v2.6.0 and v2.7.0 tags in parallel by checking them out in separate worktrees and building them: If you’re a fan of git rebase , you might be familiar with the --autostash option. It automatically stashes any local changes made to your working copy before rebasing, and reapplies them after the rebase is completed. This is handy as it allows you to rebase from a dirty worktree. There’s also a handy config flag named rebase.autostash to make this behaviour the default, which you can enable globally with: rebase.autostash has actually been available since Git 1.8.4 , but 2.7 introduces the ability to cancel this flag with the --no-autostash option. I think this is mainly for completeness, as using it only seems to give you the dirty worktree warning: I might be missing something though, let me know if you have a proper use case! Speaking of config flags, Git 2.7 also introduces stash.showPatch . The default behaviour of git stash show is to display a summary of your stashed files. Passing the -p flag puts git stash show into “patch mode”, which displays the full diff: stash.showPatch makes this behaviour the default. You can enable it with: If you enable stash.showPatch but then decide you want to view just the file summary, you can get the old behaviour back by passing the --stat option instead. As an aside: --no-patch is a valid option, but doesn’t negate stash.showPatch as you’d expect. git filter-branch is a versatile tool for rewriting git history. Since every commit object has a reference to its parents (and transitively to all of its ancestors), rewriting a particular commit means rewriting all of its successors as well. This means that even trivial history rewriting operations can take some time if you pick an older commit. Git 2.7 introduces a nifty new progress bar that estimates how much time a filter-branch command will take to complete: As a bonus, filter-branch commands that don’t modify the index or tree, now skip reading the index entirely, leading to dramatically better performance. The commit-filter command in the GIF above simply rewrites the Git author associated with each commit name from my real name (“Tim Pettersen”), to my handle (“Kannonboy”), and doesn’t touch the tree. It took only ~38s to rewrite the first 1000 commits of Bitbucket Server under Git 2.7.0, versus ~64s under Git 2.6.0: an impressive ~40% improvement. The tests introduced with the performance improvements show even more dramatic savings of ~60% . .gitignore files let you exclude certain files that reside within your worktree being staged in your repository. You can negate these patterns by prepending a ! to “unignore” a particular file. For example: Will ignore all json files except cat.json . However, in Git 2.6, you couldn’t apply a negation to a file residing in a directory that had been ignored. As of Git 2.7, the second example above also works. You can now apply a ! to “unignore” files in directories that would otherwise be ignored. These are just a small sample of the Git goodness that landed in 2.7. For the full scoop, check out Junio C Hamano’s release notes , or peruse the Git repository yourself with: If you’ve enjoyed this post or have questions about Git or Bitbucket, drop me a line on Twitter: I’m @kannonboy .", "date": "2016-01-05"},
{"website": "Atlassian", "title": "Kubetoken, time-limited tokens for Kubernetes cluster access", "author": ["dcheney"], "link": "https://blog.developer.atlassian.com/kubetoken/", "abstract": "At Atlassian we believe strongly in empowering our developers to own the services they write and run. Ever Atlassian has the permission, and the responsibility, to push their service to production. To do this we make heavy use of Amazon’s IAM service , which has been glued into our corporate LDAP directory. A core tenet of this pattern is individuals don’t have direct access to root passwords or other critical credentials. Rather, permissions to various resources are assigned to IAM roles and, depending on LDAP group membership you have access to all the resources you need from day one by virtue of joining a team in LDAP. No days wasted waiting for overworked admins to action a ticket to add your ssh key to a jump box. Atlassian developers are well accustomed to the notion of acquiring a set of permissions by assuming an IAM role for a set period of time. We wanted to continue to provide this experience as we rolled out Kubernetes to Atlassian. Kubernetes provides a rich set of authentication modes including pre shared tokens, static password files, and X.509 certificates. Pre shared tokens are problematic in our environment as issuing them would require manual intervention from an operator and revoking them currently requires restarting the API server. Neither of these are things we wanted in our environment. The same is true for static password authentication, with the added complexity that we would have to acquire the clear text password of the developer beforehand (which is strongly discouraged in our environment), and has the same revocation drawbacks as the previous method. As we’re operating in the public cloud, encrypting all the traffic between client, kubelets, etcd, and api servers, is prudent. This meant for each kubernetes cluster we stand up, we have a certificate chain in place to secure and validate the communications between the kubernetes components. This proved to be the key to implementing our authentication mechanism using X.509 certificates. Introducing kubetoken, Atlassian’s solution for issuing time-limited SSL certificates for use with Kubernetes clusters. kubetoken consists of two components; a signing daemon and a command line client. Both components use Active Directory flavoured LDAP to validate group membership. Gaining access to a Kubernetes cluster proceeds thusly: We’ve chosen, somewhat arbitrarily, to issue certificates with a 6 hour validity, after that time they will no longer be accepted by the api server. This automatic expiry was a key reason for choosing X.509 certificates. Using X.509 certificates proved to be the ideal solution for us as it avoided the requirement, and latency, of distributing tokens to all apiservers. Our apiservers run in an autoscaling group behind an ELB, so may appear without notice complicating promulgation of a set of valid tokens. The user experience of a request sporadically failing because their token hadn’t been synced to the apiserver the request happend to land on would be less than stellar. X.509 certificates also gave us a reliable expiry mechanism that does not require restarting the apiserver. Expiring tokens by co-ordinating their removal from all apiservers would be complicated, and the risk of this removal process failing thereby leaving credentials valid beyond their expiry window was a strong concern. Using time bounded certificates avoids these expiry headaches. If you’d like to try kubetoken, the source is available on GitHub . Installation instructions are available in the README .", "date": "2017-07-05"},
{"website": "Atlassian", "title": "Introducing BDDA, the infrastructure workflow we use for Kubernetes", "author": ["Nick Young"], "link": "https://blog.developer.atlassian.com/kubernetes-workflow/", "abstract": "In Atlassian’s Kubernetes platform team, we’re working to bring the existing Kubernetes clusters under one team’s management; to arrive at some standard tooling, build processes, and workflow to build the Kubernetes clusters that are running internal workloads. This is the result. We’ve spent a lot of time trying to find a workflow and dev process that would allow us to use DevOps concepts at the same time as we make sure that we do everything as close to the right way as possible. The core part of this is a workflow that we call BDDA (pronounced like ‘Buddha’) – for “Build-Diff,Deploy-Apply”. As we started working on consolidating the existing Kubernetes clusters that had started springing up inside Atlassian, we decided that our environments had three goals: To explain further: We want to move as far as possible towards having our infrastructure be cattle, not pets. We weren’t sure the extent to which we would be able to make the compute underlying a Kubernetes cluster immutable, but the aim was to head as far down that road as we could. The high-level goal we used to help with this was that it needed to be easy to tear down a cluster and rebuild it from scratch. We believe that the declarative configuration model for config management is a better match for the core Kubernetes concepts – the reconciliation loop at the heart of Kubernetes is built around the idea of declaring the desired state of things and then waiting for the system to bring the world into line with the desired state. We also believe that declarative configuration tools tend to gravitate to solving problems by including the solution in the upstream tools, as opposed to a set of steps inside a private repo somewhere. That said, we do use imperative tools where it makes sense. We want to eliminate manual ‘Is this working?’ steps and lessen the chances of breaking changes making it to production. The best way to do that is to use the advances in CI/CD to run automated testing as part of our builds, using a dedicated tool. We’re using Atlassian’s Bamboo tool, which also has the side effect of giving us a bunch of compliance ticks for free. (How this is handled internally is a whole different story, that should be told by someone who knows it better.) The key insight we had was that, for infrastructure, a ‘build’ can correspond to the process of generating a diff between what is in the config repo and what is running on prod, storing that diff in the repo, then checking that diff does what’s expected in some way, and Deployment is applying the config and removing the diff for that environment. There are two big advantages to this approach: There are, of course, disadvantages too: As outlined in our other post , we break up our infrastructure into three layers: We would prefer to use Helm for the Kubernetes layer, and have a workflow all ready, using helm-diff , but it is currently blocked on Helm issue #1918 – turns out you can use the Tiller from inside the cluster to bypass all the RBAC controls we’ve implemented. So, for now, we’ve done this using Ansible’s templating, and using kube-diff to generate the diffs. As soon as that issue is resolved, however, we will switch to Helm. The configuration for each of these layers is stored in a separate repo, but builds for each of the layers involve building the entire layer cake – to check for upstream effects of changes. This diagram will hopefully help explain: The workflow works like this: So far, we’ve found this workflow to be great for our internal dev speed and completely compatible with our compliance regime, while meeting the original three goals: ease of re-provisioning, declarative config, and incorporates automated testing. I’d be very interested to hear thoughts from other people doing similar (or different) things. That said, if you would like to do something like this yourself, my recommendations would be:", "date": "2017-07-06"},
{"website": "Atlassian", "title": "Docker clusters from the ground up: front-end reverse proxy (Part 1)", "author": ["Nicola Paolucci"], "link": "https://blog.developer.atlassian.com/docker-cluster-reverse-proxy-1/", "abstract": "The Swarm is here. It is here to help us developers focus more about the relations between the components of our architecture and less about the physical machines and their IP addresses. If you need a refresher I spoke about and wrote an introduction to orchestration and the Docker tools : have a listen or read and come back because we are bringing this knowledge to the next level today. When working with self-organizing infrastructure you still need some key pieces of architecture to be stable, manually assigned. For example you need to have public DNS entries and at least one fixed IP address for your users to reach. While your orchestration framework spins up and down, moves and balances your services, a reverse proxy needs to constantly keep tabs on available services and route traffic accordingly. This is the focus of this post: let’s setup a fixed entry point to a sample application and make sure a self-updating reverse proxy can route traffic to all back-end instances. Here’s the final result I have in mind: To start my exploration I setup the four node cluster below – plus a Consul server: For the commands used to set it up see the appendix . For this example cluster I’ve decided to use HAProxy and Interlock : To test the cluster configuration I wrote a few lines of Go , a little web-serverino which just spits out a logo and its own IP address (source code here ): This tiny concoction results in a neat and useless: Let’s write a Docker compose YAML ( haproxy-demo.yml ) file to tie our HAProxy/Interlock pair to our frontend node and our web back-ends to non-overlapping back-end nodes: The docker compose configuration for the interlock container is a yaml-ified version of the HAProxy Interlock instructions . We can control the cluster by switching our docker command to it with: Now we can start the applications on the cluster with: Note the --x-networking flag which turns on the overlay networking for the cluster and allows all nodes to see eachother in a private VXLAN. Once we need to scale the web backend to other nodes we can let Compose take care of it by typing: If everything is working you can see front-end and back-end up in the HAProxy administration interface. You can set the password via an environment variable (for this demo I left the default). to /etc/hosts automatically: Then you will be able to access the front-end at: This concludes the first part of the series. If you want to be notified when the next installment is out follow me at @durdn or my awesome team at @atlassiandev . After all of the above this is how the cluster looks like: We can control the cluster by switching our docker command to it with:", "date": "2016-01-20"},
{"website": "Atlassian", "title": "Node.js releases security updates for all major release lines, July 2017", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/node-security-vulnerability-alert/", "abstract": "We have recently become aware of a Node.js security vulnerability that can be used to cause a denial of service attack. This vulnerability affects all major release lines of Node.js. If you are running Node in production it is strongly recommended that you upgrade to the latest version of Node for the major version you are on. If you are hosting a Marketplace add-on with Node.js or built with Atlassian Connect Express we strongly urge you to upgrade your production environment to the latest version in your release line as soon as possible. Node provides a summary of the vulnerability: Node.js was susceptible to hash flooding remote DoS attacks as the HashTable seed was constant across a given released version of Node.js. You can find out more about the vulnerability over on nodejs.org . Node provides downloads to the latest version for each major release line: We recommend that users of all these release lines upgrade as soon as possible. Note: The 0.10.x and 0.12.x release lines are also vulnerable to the Constant Hashtable Seeds vulnerability. We recommend that users of these release lines upgrade to one of the supported LTS release lines. Ralph is a Developer Advocate for ecosystem developers.", "date": "2017-07-14"},
{"website": "Atlassian", "title": "Build better command-line programs with docopt", "author": ["Ian Buchanan"], "link": "https://blog.developer.atlassian.com/better-command-line-programs-with-docopt/", "abstract": "Just before Thanksgiving last year, my colleague Tim Pettersen wrote an article about building command-line tools with Node.js . While I was reviewing the article, I asked Tim why he likes commander for parsing command-line arguments. As with many programming choices, it came down to what he had used before. I’m sure most programmers have been down this road before, especially for command-line parsing. What works best is something you know. That’s usually fine for me. But, in the world of command-line interfaces, I think there is now just one right way . It’s docopt . Writing command-line tools is something I’ve done many times. I’ve written them in C, Java, C#, Ruby, and Python. My latest is a Python program that manipulates Bitbucket Snippets . When I started about a year ago, there was an itch I just couldn’t scratch. I looked at the standard argparse library in Python. The code seemed easy enough. It automatically generates command-line help, which helps keep documentation up-to-date with the code. But here I was learning yet-another-argument-parser-library. I wanted something that would keep me consistent between languages, not just within Python. For web applications, I have been following the growing trend of API Description Languages , such as Swagger , RAML , I/O Docs , and API Blueprints . For some, these tools are reminiscent of heavyweight technologies like SOAP and WSDL. For me, these solve the important need to keep consistency between code and documentation. They are especially helpful because web APIs can be consumed by any language. And so it was that I found myself Googling for “command-line interface description language”. Fortunately for me, I wasn’t the first with the idea. At PyCon UK 2012 , Vladimir Keleshev had presented his Python implementation of docopt. Now, there are parser implementations in every language I had ever used, and for every one I might in the future. A docopt description is simple to write. In addition to Vladimir’s video, there are ample documentation and examples to learn from. There’s even a nifty docopt web tool to try out descriptions in a browser. Here’s the description language for my Snippet CLI tool: This description is exactly what the program emits with an implicit --help or -h , or whenever the provided command-line arguments don’t match the syntax. Even though it serves as documentation, it is also a full description of the grammar. Binding the description language to my code is also simple. In Python, I can just place the above as comments in my main.py . (In other languages, it is often just a string variable.) Python lets me access the code comments with the special doc string symbol __doc__ , which I pass it into docopt() like this: When my program runs, arguments will be populated with a dictionary of parsed values. Here’s a JSON representation for the arguments create --title=Example --public example.py : From here, my program is responsible for dispatching into the right code. This will vary by language, but you can see how I dispatched in my main.py method for Snippet . While my example shows how easy it is for a simple case, I have noticed some much more complex examples. One nice thing is the subcommands can be distributed in separate files. In the Python docopt repository, there is a sample using the Git command structure . In a real-world example, the developers at ActiveState wrote a tool with 60 subcommands . I realize that some developers might object to docopt because their favorite command-line framework does much more than just parse arguments. For example, even the standard Python argparse will automatically dispatch to Python methods. (For what it’s worth, Keleshev has written an additional Python library to perform dispatching .) More extensive libraries like Python’s clint help format text tables, write color output, and collect user input. But I hope for the best of both worlds. Ideally, docopt might be a plugin to a broader CLI framework. If you found this useful, or have your own tips about command-line tools, tweet me at @devpartisan or my team at @atlassiandev .", "date": "2016-01-27"},
{"website": "Atlassian", "title": "Breach Detection At Scale With PROJECT SPACECRAB", "author": ["Dan Bourke"], "link": "https://blog.developer.atlassian.com/project-spacecrab-breach-detection/", "abstract": "Every blog post needs a good truism, so here’s ours: Attackers have all the advantages and defenders have few asymmetries to exploit. That’s maybe two truisms, actually. Call it a bonus. Today we're releasing PROJECT SPACECRAB , an open source toolset that tips the balance in the right direction by enabling deployments of several thousand AWS honey tokens across your network and on every endpoint you own. Good attackers are a combination of smart, dedicated and lazy. They’ve got some objectives they want to meet and they’ll take any path they can to get there. One well-trodden path to those objectives is stolen credentials. We know from experience running our cloud detection capability that the first thing most attackers do after they establish initial access, is search for passwords and other credentials. Then they try to use those credentials to access other services and escalate their privileges. Passwords are extremely valuable to attackers (and you!), but in a world where much of the internet runs on Amazon's infrastructure, AWS access keys have become the keys to the kingdom. Those keys give attackers the ability to call Amazon’s API from the privacy of their own dodgy offshore VPS and manipulate whatever resources might be running in the attached account. While AWS has extremely powerful policies and permissions language to prevent leaked keys from being abused in this way, a single misconfigured policy – or API that’s not strictly forbidden – can lead to lateral movement, further information disclosure, or a sudden increase in your AWS bill as you start an enormous cryptocurrency mining farm for some reason. Because AWS keys are very valuable and able to be secured pretty comprehensively, they’re an excellent candidate for honey tokens. These tokens, which are credentials without the access to actually perform any actions, but set off alarms whenever they’re used, are a great way to use attacker’s methods against them. They function as tripwires or early warning alarms for breaches in your network or supply chain. As mentioned, AWS keys make great honey tokens because they’re very attractive to attackers, as they could contain unlimited cosmic power. Finding an AWS key is like finding an unscratched instant lottery ticket. It might be nothing, or it might be the reason you can quit your job tomorrow and move to somewhere where there are beaches and no extradition treaties. From a defender’s point of view, AWS keys are cheap to generate, can be locked down to have minimal (zero) permissions, and are easy(ish) to audit and alert on. To continue our lottery ticket metaphor, it’s easy to set them up so that when someone scratches the ticket, it has three of the same symbol but the prize is “jail time, 5 years” and it somehow calls the cops. It’s … listen, the fifth hard problem in computer science is “metaphors are difficult”, let’s move on. AWS keys are also found “naturally” in lots of different locations, so attackers’ delicate trap sensibilities won’t be triggered by finding them in your chat logs, in environment variables in ephemeral EC2 instances, or in a file called “AWS secret access tokens for production account root user.txt” on your CISO’s desktop. So if you’ve got a few places you want to sprinkle some honey tokens to catch inquisitive attackers, it’s easy for you to generate some tokens with locked down permissions and set up alerts in Cloudwatch that will tell you when someone tries to use them. In fact, the team at Thinkst has made that trivial with their free canary token service . This approach is fine for small honey token deployments, but it is difficult and tedious to manage if you want to scale to, for instance, several thousand honey tokens deployed all across your network and on every endpoint you own. That's where PROJECT SPACECRAB (just SPACECRAB in informal contexts) comes in! Just log in to a brand new AWS account, run the bootstrap CLI, wait a while, and you're ready to distribute thousands of honey tokens, automatically and securely, all over your network. Better yet, the detection logic is already set up and extensible to whatever alerting infrastructure you already use. SPACECRAB is a Cloudformation stack. That doesn’t sound very impressive, but it’s a Cloudformation stack that deploys a comprehensive set of tools, policies, datastores and alerting functions, and deploys flawlessly about 80% of the time. (If your deployment is part of the 20% that doesn’t work the first time, burn the stack down and deploy it again and it should be fine!) SPACECRAB’s three main components are: For instance: If you’re storing credentials in a third party service, and want to know if said third party has a breach, you can create a honeytoken with the annotations “stored in $THIRDPARTY, owned by $SECURITYTEAM”. You then store it in your third party service with a juicy title like “AWS root user keys” and wait. A while later, someone breaches their infrastructure and, eventually, pillages your account. They see “AWS root user keys” and, because it’s the sensible thing to do, try to use those keys to list resources in your account. They will get a series of permission denied messages, and, disappointed, go loot something else. But you will get a series of phonecalls from the nice PagerDuty robot telling you that someone has used the AWS keys stored in $THIRDPARTY and that you should initiate your incident response process. Ideally you will also get phonecalls from $THIRDPARTY telling you to initiate your incident response process. But in the unlikely event that this does not happen in a timely fashion, you’ve given yourself a significant head start (the alert also contains the user-agent and source IP the attacker was using, which you can feed into your investigations, which might help). Later, when you move to a new third party service, you can generate new tokens and sprinkle them there. In this way, you can set up a defensive perimeter in your supply chain. Now, this example is something you can probably manage on your own because there’s really only one location you’re storing credentials. But if you want to store unique credentials on every EC2 instance you ever spin up – so you get notified not just what service was involved, but the specific server – you’re going to need a more robust solution. You’re going to need SPACECRAB. ⚡️ ☁️ AWS magic ☁️ ⚡️ Fine, yeah, that’s not a good explanation. Here’s a pretty picture: Basically, SPACECRAB generates new user tokens with a deny-all policy. The only action these tokens can perform is the ‘ sts get-caller-identity ‘ action, which you literally cannot stop a token from doing. SPACECRAB then stores metadata about these tokens, including the location you’ll put them, in a database. The account you’re using to store and alert on tokens is configured with a comprehensive Cloudtrail logging policy. Any event that happens in that account is logged and sent to an S3 bucket , then a lambda function for analysis. Every logged event is checked to see if the principal is one of your honey tokens (against the records in the database ). If it is one of your honey tokens, the event is sent to an AWS SNS topic . That SNS topic triggers more lambdas, one for each of the alerting methods you’ve configured. Each lambda takes the event data from SNS, formats it appropriately, and triggers its configured alert action. Currently available alerting actions include the PagerDuty Events API v2 , and Amazon’s SES (Simple Email Service) , but you can write your own lambda functions to fire whatever alerts you need. The extremely tl;dr version of how to use SPACECRAB is “clone the repo , run manager.py, answer the questions it asks you”. There is a more detailed version in the repository itself, covering some steps you might want to do before and after, but that should cover it. Let us know what you think!", "date": "2017-10-19"},
{"website": "Atlassian", "title": "Bumpversion is automation for semantic versioning", "author": ["Ian Buchanan"], "link": "https://blog.developer.atlassian.com/bumpversion-is-automation-for-semantic-versioning/", "abstract": "Semantic versioning (or just semver) is a specification for communicating about changes in code from one version to the next. It targets the problem of dependency hell . Dependency hell is where you are when version lock and/or version promiscuity prevent you from easily and safely moving your project forward. In short, semver specifies the first number should indicate breaking API changes, the 2nd should indicate added functionality, and the 3rd should indicate backwards-compatible bugfixes. With such a strong focus on API changes, semver is most commonly used for shared libraries. The rules of semver already help dependency management tools like Maven , NuGet , and pip understand when libraries are out of date and when they can be safely updated. Despite how many developers benefit implicitly from semver, many are unaware that semver can be easy to maintain with the right tools. One tool that I like is bumpversion . Although it’s written in Python, it can be used in any build process to “bump versions”. On the command-line, you can use bumpversion like this: That would update the version number 0.5.1 to 0.6.0 in the file src/VERSION . However, if you have already wired your build to use only 1 file, then you’ve already made semver easy. Don’t add one more tool just on my say so. The more compelling usage is with a .bumpversion.cfg file in the repo. It looks something like this: This file plays the same role as the previous src/VERSION file, in that it records what the current version is. But it also stores options for bumpversion that will propagate the version to other files. In the above example, there are version numbers that need replacing in both setup.py and README.rst . I can update both with the simple command: This time I don’t have to specify the current version, or the files that need updating. When it runs, bumpversion will modify both files, and the current_version variable. Obviously, now these 3 file changes have to be committed back to Git. Bumpversion can do that with the --commit switch or the commit = True option in the .bumpversion.cfg file. Also, it is convenient to track a release back to the code at that point. Again, bumpversion can do that, using the --tag switch or the tag = True configuration option. In either case, don’t forget to push when you’re done. I usually wait for the full build process to complete, including the upload to the appropriate artifact repository, before I push the changes. That way I can roll back the changes if things go wrong. Thanks to my colleage Nicola Paolucci’s treasure trove of git aliases , I can issue the following commands to roll-back the bump: Or, you can save some time when trying out bumpversion by using the --dry-run option. That will show you the changes it’s going to make without writing them to disk. I’ve only covered the most basic options here, but there are options to account for alternate versioning schemes, or additions to standard semver, like alpha/beta builds or release candidates. Some parts are still manual, like when you decide to run bumpversion and which kind of release bump to make. Nevertheless, I still find it a big help to make sure multiple files are updated consistently, and in accordance with the rules of semver. I hope you find it useful too. If you do or have your own tips about semantic versioning, tweet me at @devpartisan or my team at @atlassiandev .", "date": "2016-02-10"},
{"website": "Atlassian", "title": "Journey to continuous integration", "author": ["Chris Mountford"], "link": "https://blog.developer.atlassian.com/journey-to-ci/", "abstract": "When I first heard the term Continuous Integration (CI), my heart leaped like a gazelle. Poetic yet descriptive, entrancing and seductive. I knew instantly its true meaning and knew my software team had to get on board and do Continuous Integration today. — Nobody Ever. Let’s be honest, the words “Continuous Integration” are a terrible piece of terminology. I respect Martin Fowler who coined the term so I’m going to give him a pass on this one, but when many people first hear “Continuous Integration” their face shows a tortured combination between utter boredom and a sense of creeping doom, maybe with a dash of “what’s that smell?” If you’re already familiar with the jargon, imagine what it’s like for a beginner. When you say “Continuous Integration”, they hear “endless tax forms” or “infinite bureaucracy”. The tragedy is, of course, that while the words are a scourge, the thing itself is wonderful. Bad names aside, what is CI all about? CI is the software equivalent of a factory run on robots where deliveries of neatly wrapped source code are made to the cargo bay, fresh off the developers keyboards. Tireless automatons take it from there and well-tested, working software rolls off the production line at the other end. What could be more beautiful? In software this virtual factory with the robots is called a build . At Atlassian we have build systems, dedicated build engineers, and our product, Bamboo , a Continuous Integration and Deployment tool. CI is all about the builds. In any non-trivial software project, multiple developers are working at once on different parts of a system. Naturally developers should test their work before they push code to the repository. This testing, however, is not exhaustive and the developer’s laptop may not count as a clean, standardized deployment environment. Needless to say there are some problems that only show up once all the work of different people are put together on a shared installation and subjected to more exhaustive testing. This testing should be automated as much as possible. Even before rigorous testing, problems can sometimes be discovered at the integration point. Even a basic sanity check of combining all the components under active development with the specific target deployment platform can reveal a problem. On occasion we’ve discovered that our own software is unable to start up on an integration environment even though it works with no drama on a developer’s laptop. So even if you do not have automated tests, you can still gain some value from CI. And when you add automated tests, one by one, their value will be far greater than without CI. While I’ve joked about the terminology, it’s useful to understand the basis for it. The meaning of integration is bringing the parts together on one or more deployment platforms, like making an installable app or deploying a cloud service to an environment. Some of the typical environments we use for our projects include development, test, QA, dog food, staging, and production. Not all projects will need all stages in this deployment pipeline. Integration includes all the compiling, packaging, perhaps code generation, pre-processing, macro expansion, static defect detection, minification, obfuscation, automated unit testing, perhaps code signing, etc. The exact list of things that happen in a software build is different on every project. While “integration” may not be everybody’s word for building deployable artifacts, everybody uses the word “testing”. Do we all mean the same thing by that word? When we test, we check to see that things perform the way they should. Nothing earth shattering about testing, but automated testing , now that’s another matter. It’s testing, but done without any human intervention. So that’s integration and automated testing, But what’s with this continuous stuff? Typically, continuous integration means that all the software build processes that must occur to turn code as it exists in the source repository into a working software product all happen as often as possible — ideally on every commit, every time somebody makes a small change to the code. Let me tell you how the JIRA team adopted continuous integration and why. Back in 2005, the JIRA team did not do continuous integration. We had a monstrous hairball of a nightly build script. It was both a horror and a delight. It did everything. The latest version of the code was checked out from source control, compiled, and packaged into multiple distinct editions with different functionality. Each edition was tested every night while the developers slept like babies. Come morning there was a detailed report of what went wrong. Nothing should have gone wrong, since developers test and scrutinize their work, but to no-one’s surprise, sometimes things went wrong. OK, OK, things went wrong constantly — at least they did once the team grew to a dozen or so. Perhaps by rights the developers should not have been sleeping like babies but sitting bolt upright, unable to sleep for the bug crimes they knew they’d committed that day. Before going continuous , the JIRA build accumulated such a broad and deep range of automated tests that to execute on three JIRA editions all tests against each permutation of database, edition, JVM version, application server and distribution variant that we supported at the time, the total time required exceeded 24 hours. It could no longer be even called nightly. We joked with hollow dread at renaming it to a weekly build . When the team moved to a continuous integration process, that same process was initiated on every checkin of code by a developer. At least, that was the theory. The true ethos of continuous integration is that the builds are happening in response to new code being checked in. The time delay between checkin and the completion of a build should be minimized to ensure that the developer who may have introduced a problem can return to the code and fix it or roll back his or her changes without getting too deep into a new task. It’s all about fast feedback. If we had opted to go the other way and strode foolishly towards a weekly build, we would have had ever longer time delays between creating a bug and discovering or fixing it. We would also have increased the disruption to our team mates and ultimately reduce the feature throughput of the development team. Building every single commit is often talked about as an ideal, but when you have several factors growing quickly, this ideal becomes exponentially more difficult and expensive. When the number of developers and the complexity in the software both grow, the time required to compile, build and test will grow to exceed the typical time between commits. This is no big deal on smaller code bases with fewer committers as commits happen to land together only occasionally. But when it’s a problem, there are three possible consequences: In the JIRA case we had some of each. Any of these can be OK, depending on the project and the extent. Builds break. Of course they shouldn’t, but nobody on a large team project can run every conceivable test exhaustively on every check-in. If you disagree, consider sustained load tests that necessitate hours of running time in order to produce a valid result (known as soak time). There will be some kind of test that is run on your code after you have pushed your changes to a central repository. Depending on the details of your branching strategy, responding to a broken build can be an urgent matter. If it can affect other people’s work, you must respond and make sure you don’t block them. Having a culture of responding to and fixing broken builds promptly, or, as Bamboo Developer Esther Asenjo likes to put it, a Culture of Green , is not just a matter of professional pride. It’s a systematic engineering response that enables and promotes quality at high-speed. Check out Esther’s talk on this from AtlasCamp . Allowing builds to remain red for hours or even days can be corrosive to a team’s work. The benefit of using red and green, though it can be culturally exclusive and challenging for people with color blindness, at least draws an unambiguous distinction between what is working and what is broken. It’s binary. If you don’t have a binary rule, what is your rule? You must get this straight. Ultimately everyone must know the difference between success and failure at every level of detail. This is what your builds should be. You shouldn’t get into the game of “shades of gray” between red and green because you rob your team of the definition and therefore the hope of achieving clear success. This is the real meaning of Continuous Integration. It’s not the software equivalent of “infinite bureaucracy”. It’s much more like the idea of “ultimate clarity” or “irrefutable success”. Hmmm… maybe I’m on to something. Maybe I should start coining some terms of my own! Learn more about continuous integration and continuous delivery pipelines .", "date": "2016-02-12"},
{"website": "Atlassian", "title": "Video calling Santa Claus", "author": ["Saúl Ibarra Corretgé"], "link": "https://blog.developer.atlassian.com/video-calling-santa/", "abstract": "As part of the Video Engineering team, my work contributes to the video experience in Stride and HipChat . The video conferencing system we work on is the Open Source Jitsi project, which was acquired by Atlassian over 2 years ago. Being Open Source means others can use it too, and build all sorts of projects. People usually build things akin to meetings, but every once in a while we come across a project which is out of the ordinary. Today I’d like to share a couple of those with you: Los Reyes Magos TV and Papa Noel Online . These projects allow children in different regions in Spain to have a video conference with Santa Claus or one of The 3 Wise Men . Isn’t that fantastic? It gets better: this is a non-profit initiative, part of a larger project, which is funded with the support of different charities and several town councils. Last Christmas the project had over 4000 calls , each averaging 5 minutes! About a month ago I had the pleasure of meeting Javi Arranz , CEO of Eternity Online and the mastermind behind this project. He is deeply involved in several charitable efforts in Spain and he wouldn’t stop praising our work on Jitsi, claiming he couldn’t have done it without it . The project has grown a lot since last year, with more town councils and organizations joining in, and there is a prospect of the project being launched in Latin America too! We are incredibly proud and humbled that Open Source technology produced here at Atlassian is used in such wonderful and noble projects.", "date": "2017-12-19"},
{"website": "Atlassian", "title": "Reflecting on a Season of Giving", "author": ["Max Mancini"], "link": "https://blog.developer.atlassian.com/reflecting-on-season-of-giving/", "abstract": "As I reflect on the successes of the past year during this holiday season, I think it is important to highlight those that are committed to helping make the world a better place, one community at a time. Many of you may be aware of Atlassian's commitment to the Pledge 1% movement , but may be less aware of that many Atlassian cloud partners and Marketplace vendors have also made the pledge to donate 1% of their equity, time, product and profit back to their own communities. In a world where myopic focus on growth and investor return can drive bad behaviors in the corporate world, I am proud to work for a company that is committed to building a better future for everyone in ethical and responsible ways. I'm even more proud to help drive the success of companies who share this passion to improve the lives of not only our customers, but also extend their giving to their own local communities where equal access to opportunity is not always available. For those of you who are not familiar with Pledge 1%, “Pledge 1% is a global movement to create a new normal in which giving back is integrated into the DNA of companies of all sizes. Pledge 1% encourages and challenges individuals and companies to Pledge 1% of equity, profit, product, and/or employee time for their communities.\" I encourage those of you who are not already participating in this movement to join and share your stories. More than 3 out every of 4 of our most successful Marketplace vendors are Pledge 1% members. I would like to thank some of those great companies who have not only helped to make Atlassian successful, but are also supporting nonprofits where they work and live. I hope that everyone has a wonderful holiday and a prosperous and peaceful New Year. Warmest Regards, Max Mancini and the Atlassian Ecosystem Team", "date": "2017-12-22"},
{"website": "Atlassian", "title": "Scaling Commerce at Atlassian", "author": ["mbernhardi"], "link": "https://blog.developer.atlassian.com/scaling-commerce-at-atlassian/", "abstract": "We're building our team in Mountain View, California to create Atlassian's newest commerce platform. Read on to see how we're scaling to support hundreds of thousands of customers worldwide and how you can join the team that's defining the future of Atlassian's growth. Like many high growth software companies, we built our own commerce system because our business was small and our volumes at the time meant we could focus on a high-touch process with our customers. However, over the years we’ve been playing catch up as our business has grown – with more products, new enterprise features, and growing the number of customers. What started as a simple commerce system has since expanded to managing our product and pricing catalogs, customer and entitlement management, contracts to service provisioning, quotes, invoices, payments, and much more. Needless to say, our commerce platform won’t be able to keep up at the scale that we need it to in order to deliver a seamless experience to our customers. At Atlassian, the customer’s buying experience is always our top priority and what underpins this experience is simplicity. Our customers should be able to try and buy our products easily and that’s only possible if we have a commerce platform that is simple and intuitive to use, yet agile enough to adapt to our growing business product and service offerings. At the same time, we need to support our partners, and make sure that there are resources for customers who want more help. Our customers don't want to be bogged down by complex quote-to-cash processes. They want to buy, try and go. But on the back end, we need to be efficient, reliable and consistent for the business to scale effectively. We have some of the best engineering talent in the market and we believe we can continue to support our Flywheel Sales Model – even in an increasingly larger and more complex B2B market. \"Our high-velocity distribution model is designed to drive exceptional customer scale by making affordable products available via our convenient, low-friction online channel. We focus on product quality, automated distribution and customer service in lieu of a costly traditional sales infrastructure. We rely on word-of-mouth and low-touch demand generation to drive trial, adoption and expansion of our products within customers.\" We are growing fast – the rocket that took off 16 years ago, hasn’t changed it’s near vertical trajectory. We have outgrown our home-grown commerce platform and need to be smart about what we build to deliver competitive advantage and propel our new, agile, scalable B2B commerce platform into orbit. We’re transitioning to a more scalable architecture of functionally rich and specialised systems and services that include In the same way we are expanding our eCommerce platform globally, we are also expanding our commerce team footprint to Mountain View, California. We’re looking for Salesforce engineers, front-end engineers, business systems analysts, product managers, and integration engineers. If you’re looking for an opportunity to… …then come and join us! As part of Atlassian, you’ll be supported by comprehensive perks and benefits, but we’ll get into all of that later. If you’re interested in joining, check out our open jobs on the team in Mountain View, California here .", "date": "2018-01-24"},
{"website": "Atlassian", "title": "Introducing Escalator, the Atlassian-developed auto-scaling tool for", "author": ["Corey Johnston"], "link": "https://blog.developer.atlassian.com/introducing-escalator/", "abstract": "Last week, Atlassian released Escalator, our Kubernetes autoscaler optimized for batch workloads, into the open source community. It is available from our Github repository . At Atlassian we have a long history of using containers to run our internal platform and services. This work started around 2013/2014, when we built our original PaaS based on Docker. From the containers in our microservices stacks, to jobs in a CI/CD workflow which build them, Docker provided a convenient way to package all of the dependencies together to ensure workloads can run everywhere. Whether it be on your laptop, or a dev, staging or production environment, containers have allowed us to make our services more reliable, deterministic, and require much less effort to run. But not long into our container journey, it quickly became clear that there was a big piece of the puzzle missing: orchestration. There were four key missing pieces: When designing our compute PaaS, we saw that Kubernetes would help provide us the platform to solve these challenges. Kubernetes could orchestrate our containers given its rich deployment capabilities, its declarative configuration and reconciliation loop architecture, and its enormous community support. And so we built our Kubernetes platform tooling. We battle-hardened it for production, we built a dedicated Kubernetes team to help manage it, and we started migrating our existing container-based workloads to it. The dedicated team served (and continues to serve) a critical role: It enables our internal teams to leverage Kubernetes and not worry about how to run the platform itself – which keeps the internal teams focused on their most important task: developing great services. One of the first workloads we migrated was our Build Engineering infrastructure, which Atlassian teams use for continuous integration and delivery to build and deploy products. Previously, this had been running on a cloud provider’s proprietary container management system, so the workload had already been “Dockerised” into containers. Our challenge was to take this big hairy workload, which in its peak consumes several thousand cores, and lift-and-shift it to Kubernetes. Initially with our Kubernetes platform, we were pleasantly surprised by how quickly we were able to port these batch workloads to Kubernetes pods. However, when the number of concurrent jobs ramped-up, we started to notice a few bumps in the road. Namely, the cluster wasn’t scaling-up or down fast enough. Scaling up : When the cluster hit capacity, users would often have to wait minutes before additional Kubernetes workers were booted and able to service the load. This is not a great situation to have, because many builds couldn’t tolerate extended delays and would fail. The problem was that Kubernetes’ de-facto cluster-autoscaler service was missing a key feature: the ability to preemptively scale-up compute nodes before the cluster reaches capacity. Or more simply, the ability to maintain a fixed percentage of buffer capacity as insulation for spikes. Scaling down : We also experienced the reverse problem: when the loads had subsided, the default autoscaler wasn’t scaling-down quickly enough. While this isn’t a huge problem when your node count is in the single digits, this changes dramatically when it reaches the hundreds and beyond. Then you’re suddenly burning a lot of money on idle compute that is left running when you no longer need it. With the two scaling motivations in mind, our team set about examining the options. Do we extend the de-facto Kubernetes cluster-autoscaler? Was there another autoscaler we could leverage? Should we just develop our own autoscaler to optimize our jobs-based workload? For a variety of reasons, we chose the later and set out on a new project, called Escalator. We envisioned a horizontal autoscaler optimized for batch workloads with two initial goals: provide preemptive scale-up with a buffer capacity feature to prevent users from experiencing the ‘cluster full’ situation, and support aggressive scale-down of machines when they were no longer required. We’d also want to build in something for our Ops team: Prometheus metrics to allow us to dig beneath the covers to see how well it was doing. A few weeks into the project, we started to see the first fruits of this development: Escalator was supplementing Kubernetes’ standard cluster-autoscaler, such that it would ‘taint’ unused nodes. In this way, the cluster-autoscaler could more rapidly drain and remove the nodes from our autoscaling groups. We then worked to develop pre-emptive scale-up functionality, and provisioned it with configuration directives to allow users to specify a buffer capacity. The next milestone was to extend Escalator and build all the functionality for horizontal scaling, so it could work autonomously and entirely replace Kubernetes’ standard autoscaler for our jobs-based workloads. After several months of work, the result was exactly what we envisioned to build, and what we’ve released into the open source community. Gone are our three minute waits for EC2 instances to boot and join the cluster. Instead pods transition from scheduled to running in seconds. This is made possible due to the pre-emptive scale-up functionality that we built in. It is provisioned with configuration directives, so users can specify a percentage slack capacity that suits them. Gone too is all the wasted money we were spending every hour on unused, idle worker nodes. The cluster now scales-down very quickly, so we only pay for the number of machines we actually need. Escalator has enabled us to save a lot of money – ranging from hundreds to thousands of dollars a day – based on the workloads we’re running. We also built in compatibility for using Prometheus metrics, which Ops teams will love. Today we’re using Escalator for both job-based internal workloads and for our Bitbucket Pipelines customers (who are also running on our Kubernetes Platform). We’re also exploring how we can manage more service-based workloads for other teams and products down the road. But given the tremendous benefits its delivered in our environment, we’ve released Escalator to the Kubernetes community as open source, so others can take advantage of its features too. The source can be found on our Github account and we’d love to see what contributions you have for it. Be sure to checkout the roadmap to get an idea of where Escalator is heading, and please submit PRs and feature requests if you’d like to help shape its future.", "date": "2018-05-15"},
{"website": "Atlassian", "title": "Contributing to Git LFS", "author": ["Steve Streeting"], "link": "https://blog.developer.atlassian.com/contributing-to-git-lfs/", "abstract": "Need to store large media files in Git? We’re making major contributions to the Git LFS open source project to help make this happen! Want to know how this came about? What follows is a true story… Git’s great. When it comes to keeping a handle on your source code, there’s nothing quite as flexible, and developers are adopting it in droves. But there are a lot of teams whose needs haven’t been particularly well met by Git in the past, whose projects consist of not just code, but media files or other large assets. People like game developers and web design studios are common examples, and in many cases Git’s inability to elegantly handle this issue has meant they’ve had to remain on older source control systems. I’m well aware of this problem myself, because before I came to work at Atlassian via creating SourceTree , I ran an open source graphics engine called Ogre3d for 10 years. I worked with a lot of teams with large textures, models, and so on in their repositories using pre-DVCS tools. In the years after, when I started making Git & Mercurial tools, I kept in contact with many of these people and witnessed frequent problems adopting Git because of large files clogging up repositories, slowing everything down, and making clone sizes intolerable. A few teams used Mercurial’s large- files extension, but many of them still wished they had the option of using Git as well. Atlassian has focused on serving the needs of professional software teams for the last 13 years and we’ve heard from many of our customers that they’ve struggled with the transition from legacy source code management systems to Git because of the lack of a good solution for the large files problem. Software teams really like using Git but this one problem is a real spanner in the works for their adoption plans. What could we do? So, in late 2014, we started seriously looking at this issue. We tried everything that was out there already, and reluctantly concluded that the best way to solve this properly for the long term was to create a new tool. Creating a brand new tool wasn’t our first preference, but we felt that existing tools were either too complicated to configure for a team environment (once all the features you really needed like pruning were factored in) or were not fully developed enough and used technology we didn’t think would scale, so extending them wasn’t attractive. We chose to write this new tool in Go , a modern language that was both good at producing stand-alone, fast, native binaries for all platforms, but was also fairly easy for most developers to learn; important because we intended to make it open source. We initially called it Git LOB , and after working on it for a few months we attended Git Merge this May to announce it. What neither Atlassian nor GitHub realised is that we’d both been working on the same problem! Atlassians and GitHubbers met in the bar the night before our talks, to discover that we’d both: Crazy right? Was it really all a coincidence? Turns out that yeah, it absolutely was, we’d both done this completely independently. I guess great minds really do think alike 🙂 We decided it made no sense to fragment the community when Git LOB and Git LFS were clearly so similar in their approach. It wasn’t a complete overlap, there were things that Git LOB did that Git LFS didn’t and vice versa – the best solution would be to have the best of both. So we open sourced our version as a reference, then switched our efforts to contribute to Git LFS instead, starting with porting across features we’d already developed that were useful for Git LFS. We at Atlassian plan to continue collaborating on the Git LFS project for the foreseeable future. I’ve really enjoyed working with the community around Git LFS, it’s turning out to be a really productive team effort. I’ve contributed 36 pull requests so far, making me the biggest non-GitHub contributor to the project right now: If you’re using Git LFS, my name crops up quite a lot in the new features for v0.6 🙂 Many of these features were ported across from Git LOB, but the best thing is, I actually managed to make them better as I did the port, since you can always find improvements when you look at a problem the second time. While it was a hard decision for me personally to stop working on our own solution, a few months on I’m really happy with the outcome and it was absolutely the right thing to do for the community. I firmly believe we’ll create something even more awesome by working with the open source community, and that I’ll be able to contribute positively to that effort. We at Atlassian feel it makes perfect sense for us to work to a common standard just like we do with Git itself and concentrate on creating great solutions around it, especially those that fit the needs of professional teams. I’m working on a number of features for future Git LFS versions, including support for pruning, extensions to the SSH support, and more. I’ll also be just generally around the community commenting on stuff and trying to help out. I’ll be at GitHub Universe too, talking about our collaboration as part of the “Using Git LFS” panel on 1st October. You’ll also be hearing much more about Git LFS support in Atlassian products, which of course includes Bitbucket and SourceTree . Watch this space! In any case, thanks for reading and I hope my little story of random coincidences and community collaboration was interesting. 🙂", "date": "2015-10-01"},
{"website": "Atlassian", "title": "Building helpful CLI tools with Go and Kingpin", "author": ["Nicholas Whyte"], "link": "https://blog.developer.atlassian.com/building-helpful-golang-cli-tools/", "abstract": "Building a well documented command line interface is hard. Allowing users to discover functionality and get help without typing --help or looking at the docs is difficult to do. How many times have you found yourself reaching for --help because you couldn’t quite remember the command name you needed? Here at Atlassian, my team maintains a CLI tool to interface with our internal service. It’s written in Go and takes advantage of all the functionality that the CLI library Kingpin has to offer. We wanted to improve the experience for the users of our service. We decided providing Bash completion for the tool would allow faster usage of the tool. Unfortunately Kingpin did not support shell completion, however we implemented the functionality, and had our pull request approved and merged upstream. In this blog post I’ll cover how to make a friendly CLI that gives shell completion hints. I’ll also assume you’re familiar with creating Go projects. If you’re just getting started with Go, check out the Golang Getting Started Create a new go package for your CLI. Let’s start by populating your main.go with the following: This sets up a new kingpin CLI app which parses the command line arguments passed at runtime. Try it out: Let’s add some sub commands. We extend our main() and add a helper factory addSubCommand : Whilst we don’t have a full example yet, it would be nice to try out Bash completion right now. In order to enable Bash completion in our shell, we need to generate the completion script. You can do this with ./my-app --completion-script-bash and ./my-app --completion-script-zsh . It’s worth keeping in mind that the name of your binary must be the same as the name passed to kingpin.New() . Ideally, when packaging your binary tool, you will also include this script and install it in the appropriate location . For now we’ll just locally source it in our current session: Now try it out: If everything went well, you should be able to see the available subcommands. Whilst being able to hint subcommands is very useful, it would be even better if we could hint possible options for flags. We’ll add a new command nc which will use command line flags. We will update the main function, add a new struct NetcatCommand , and a factory for it configureNetcatCommand : If we build and run now we should be able to see hints for flags when in the netcat context. To show flag suggestions, we must first type “–“, then press tab. Finally, we’ll add some more flags to NetcatCommand . We make these modifications to configureNetcatCommand and also add a helper function listHosts() : Try it out: So now you’ve successfully made a user friendly to use CLI tool. You can check out the full friendly Go CLI code example . It was closely based on the provided Kingpin completion example that we included in our pull request. You can find the actual example here .", "date": "2016-03-02"},
{"website": "Atlassian", "title": "Atlassian and open source – developer tools engineering", "author": ["Mike Melnicki"], "link": "https://blog.developer.atlassian.com/os-dev-tools-engineering/", "abstract": "This post is part of a recurring series from the Heads of Engineering at Atlassian. This week, we’re featuring Mike Melnicki. He’s talking to us about how the developer tools engineering team interacts with the world of open source. We’re in the middle of a major shift in how software is developed across the globe. The primary driver of this shift is the need for more software at a faster rate and by more organizations than ever before. These shifts not only are changing how teams build software but also the tools they use to do so. We in the developer tools engineering team strive to bring modern software development to all teams. We are spread across Sydney, San Francisco, Austin, and Gdańsk and we collectively work on Bitbucket, Bamboo, FishEye, Crucible, SourceTree, and Clover. One of the amazing things about the dev tools engineering team is that we get to use our own products every day through the normal course of our work. As users of our own products, we’re very passionate about making our tools as good as they can be because they help us unlock our maximum potential as a team. Our team benefits from the open source community immensely. Git and Mercurial are probably the most obvious projects I should mention because they are the heart of our developer tool products. Almost all of our cloud infrastructure is built upon open source projects such as PostgresSQL, Redis, Memcached, OpenSSL, Nginx, HAProxy, RabbitMQ, Tomcat, and many others. As Linus Torvalds noted: I think, fundamentally, open source does tend to be more stable software. It’s the right way to do things. Our team has a commitment to the mission of open source software because we rely on it so heavily. As a result we contribute bug fixes and improvements back to many open source projects that we use internally and encourage our developers to be active in open source projects that we use. In recent memory, those that come to mind are: Truth be told though, we’ve benefited from open source software much more than we’ve contributed back and while the list above may seem impressive, most of our commits have been small bug fixes and/or minor improvements. Looking forward however, we are encouraging more open source participation by our developers and hope to increase Atlassian’s on-going participation in the world of open source. We recognize that as an growing organization, open source is crucial to our business and to the world at large. In addition to contributing to existing projects, the developer tools engineering team is also responsible for creating and open sourcing several projects such as: We’ve also released a laundry list of open source plugins and tutorials for our products with our open source projects hosted on Bitbucket . We are always looking for ways to contribute more to open source projects and intend to increase this trend in the future. Probably the biggest open source effort that we have happening right now is around our contribution to the Git Large File storage project. Hosting large files in Git repositories is a problem that has prevented some teams for being able to adopt Git and we’re working on trying to solve it. Obviously we always feel like we can participate much more in the open source community given how much we benefit from it. In the next year we’ll be encouraging more contributions from our developers back to key projects that we rely on. As we continue to break new ground with DVCS, CI/CD, and other innovations we’ll be looking to open source some of the components and supporting tools that we build for our internal use. Atlassian is known as a company that helps other teams level up when it comes to modern software development practices and as such we do a lot of blogging, coaching, and evangelism for the benefit of other organizations, to help them ship great software. While not directly related to open source in the traditional sense our technical blog posts on https://developer.atlassian.com/blog/ , our extremely popular Git tutorial site and our free desktop client for Git and Mercurial are other ways that we like to give back to the community. Atlassian supports and believes in the open source mission. Since we benefit so much from open source we make Atlassian software free to use for any open source project. It’s another way of giving back to the community and hopefully improve the quality of those open source projects that we use as well. Mike is life-long developer and currently the Head of engineering for developer tools at Atlassian where he leads engineering and helps define and execute the product strategy. Prior to that he has lead engineering teams and worked as a developer in various large & small companies in the San Francisco area. He is interested in developer tools, mobile technology, cloud services and large-scale distributed systems. Mike is also a frustrated musician\tand spends much of his free time playing loud, fuzz and reverb heavy guitar riffs.", "date": "2015-09-23"},
{"website": "Atlassian", "title": "Tip of The Week – Use Liquid Prompt to enhance your Bash or zsh", "author": ["Peter Van de Voorde"], "link": "https://blog.developer.atlassian.com/totw-use-liquid-prompt-to-enhance-your-bash-or-zsh/", "abstract": "This week’s article is about using Liquid Prompt to enhance your Bash or zsh shell. Thanks to Nicola Paulucci for the idea. Liquid Prompt gives you a nicely displayed prompt with useful information when you need it. It shows you what you need when you need it. You will notice what changes when it changes, saving time and frustration. You can even use it with your favorite shell – Bash or zsh. Some of it’s features include: You can find additional features and installation instructions here . Have fun using it!", "date": "2015-09-22"},
{"website": "Atlassian", "title": "Choosing the right open source license", "author": ["Virginia Badenhope"], "link": "https://blog.developer.atlassian.com/choosing-the-right-os-license/", "abstract": "As Mandy mentioned in our last open source blog post, we’ve been working hard to get our open source house in order . A core aspect of the tidying effort was settling on a uniform and consistent license framework for all our projects. Previously, licenses were applied to our projects on an ad hoc basis, and we were not always explicit about how we would accept contributions and under what terms. Image by: opensource.com We realized that if we wanted to grow our open source program, we'd need to have a more consistent approach to licensing, not only to ease the burden of administering projects but to provide more clarity from a legal and IP perspective so that organizations can feel more comfortable participating. As an attorney who looks after Atlassian's own consumption of open source code, I appreciate it when projects anticipate the typical concerns that a commercial legal department might have: what is the permitted scope of use, are there copyleft provisions to be concerned about, and how hard would it be to comply with the license terms. To address these concerns, we wanted a common, well-understood, permissive license with the lowest barriers to adoption, including in commercial applications. The obvious choices that meet these criteria are the MIT, BSD, and Apache licenses. Any of these would have met our goals – all are non-copyleft licenses with minimal compliance requirements – but we chose the Apache license for its technical drafting; it contains formal license grant language that explicitly includes both copyright and patent and uses the corresponding statutory terminology accordingly. In contrast, the MIT and BSD licenses rely on colloquial language and implied license grants, though their intent may be clear. Barring other considerations (and on an exception basis), we will apply the Apache License to software that Atlassian releases to the open source community. Another decision we had to make was the terms under which we would accept contributions. Again, we wanted to follow well-established precedent that would not impede participation, and again we turned to the example set by the Apache Foundation (and followed by countless other active and successful projects). Our \"individual\" and \"corporate\" contribution agreements mirror the Apache contribution agreements almost exactly and allow contributors to maintain ownership in the IP of their contributions while granting Atlassian the rights to control and maintain our projects. We considered simply accepting contributions under the same license that covers our projects (generally the Apache License, as discussed above), but opted for the additional assurances included in the contribution agreements (such as the representations that the contribution is an original work of the contributor and that the contributor has the legal right to make the contribution). These provisions are significant to maintaining the overall IP \"hygiene\" of our projects. We've deliberately chosen what we think is a plain vanilla legal framework for our open source program. We're not looking to break new legal ground or court any controversy. We just want you to engage and participate in our open source community, and we hope we have lowered any legal barriers you may face in doing so.", "date": "2015-09-11"},
{"website": "Atlassian", "title": "Why java.util.Optional is broken", "author": ["Jed Wesley-Smith"], "link": "https://blog.developer.atlassian.com/optional-broken/", "abstract": "As of Java 8, there is a new java.util.Optional<A> class that can be used to represent the optional presence of a value in the type. Traditionally, Java programs have returned null as a way to represent that a value isn’t present. For instance, Map.get(K key) returns null if the map does not contain a key – but, null can also sometimes be a valid value stored against the key, causing ambiguities such as those documented in the Map.get javadoc. If we have a method that returns type A , we must inspect the documentation to find out whether or not it may be null. By instead returning Optional<A> , there is a clear indication that the method may not return any value at all. Furthermore, Optionals force us to deal with the case where a value isn’t returned*. Option types are relatively common these days, and their usage has helped significantly reduce a common class of problems caused by incorrectly using nulls for optionality. When the Java 8 library team was designing Optional there was some opposition to the idea that it should contain some useful methods (essentially Optional.map and Optional.flatMap ) on the somewhat spurious grounds that they didn’t want their Optional to be a Monad . When I pointed out that it is anyway whether they liked it or not , they relented and put them back. Unfortunately, when they put back map they did so in a way that is fundamentally broken. To understand why it is broken, we need to step back a bit and look at what these two methods map and flatMap do. If I have a thing, when I map a function across it, I should get back a thing with the same structure as the thing I started with. So for instance, if I map a function across a List , I should get back a List of the same size as I started with, and with all the elements in the corresponding order. This means I can guarantee some things, like for instance if I reverse the order of the list and then map the function, I will get the same result as mapping the function and then reversing the list. I can refactor without fear . As a consequence of this property of not being able to change the structure, we can derive other properties of things that can be mapped (or things that have a map method). The most interesting and obvious is that we can say that mapping two functions one after the other should be the same as mapping one function that is composed from the two. Let’s look at some code: This is a very important and useful principle, and it turns out that there is a sound theoretical basis for this result. Things that can be mapped over are known as Functors (or more precisely as covariant functors, but just functor is commonly used). The principles are known as the Functor Identity and Composition laws . Unfortunately, java.util.Optional does not observe this principle. The first hint is in the opening paragraph of the class javadoc: If we look back at Map.get we see that it is legitimate for a Map to contain null as a value for a key. This is distinct from there not being a mapping at all. Java (as part of its design) allows null to inhabit any reference type – but Optional specifically prohibits null being contained in it! At face value this seems like an entirely legitimate thing to do. We are trying to replace usage of null as a signifier of optionality after all, right? Well, yes, but null is still a valid inhabiter of our type and may have other meaning apart from optionality, regardless of our strong opinions on this being an exceptionally bad idea or not. It has a specific consequence for the map method and the composition law. If we compose two functions and the first function returns a null , we will pass this null into the second function. As mentioned before, regardless of our opinion on the utility of nulls, this is what happens. If we however map that first function on an Optional , it must become empty/none, as a non-empty Optional cannot contain a null . The second function, which may have special logic to handle the null input, does not get called at all! We specifically have different behaviour depending which way we go, and we can no longer refactor without fear. We must know whether our functions will return null or not, or we risk introducing bugs simply by mechanical restructuring of our code. The answer is of course that we shouldn’t be using map , we should be using flatMap instead. That is the method specifically designed for changing the shape of the box, and it is trivial to turn a null-as-optional returning function into a function that actually returns an Optional – although there isn’t a helper for that in the standard library (sad). The final irony is that by attempting to discourage nulls, the authors of this class have actually encouraged its use. I’m sure there are a few who will be tempted to simply return null from their functions in order to “avoid creating an unnecessary and expensive Optional reference”, rather than using the correct types and combinators. So, please use it with care, or use our more correct alternative , the next version of which (due out soon) has full integration with Java 8 and removes all other dependencies. * Well, many Option types have some dangerous methods (like Optional.get) that do things like throw exceptions if there isn’t a value. You might also enjoy our ebook, “Hello World! A new grad’s guide to coding as a team” – a collection of essays designed to help new programmers succeed in a team setting. Grab it for yourself, your team, or the new computer science graduate in your life. Even seasoned coders might learn a thing or two. Read it online now Click here to download for your Kindle", "date": "2015-08-31"},
{"website": "Atlassian", "title": "Getting Our Open Source House in Order", "author": ["Mandy Black"], "link": "https://blog.developer.atlassian.com/getting-os-house-in-order/", "abstract": "I’m a person who likes things to be tidy and organized. In my ideal world, all the objects in my life have a purpose and a place where they live. Then seemingly overnight it happens – it’s been a while since I’ve done a good clean out, things are starting to become unorganized and cluttered – and it’s driving me a bit crazy. Time for spring (or summer, winter, fall) cleaning! feather-duster.jpg licensed under Creative Commons CC0 Can you relate? Sometimes the same things can happen at work. My latest spring cleaning project has been with Atlassian. We’re taking a good look at our open source work. What do we currently have going on in the world of open source? Are things organized? Does everything we’ve put out still have a purpose and a home? One of our first steps in this process has been an audit of existing projects. Here are some of the things we discovered, along with our actions to get things in order. We have a good amount of internal support around developing and contributing to Open Source Software. We don’t want to simply consume open source; we want to help develop new projects and contribute to existing projects. This is a part of the company culture that we want to continue to grow. If you are a developer at Atlassian, do you need permission to contribute to open source projects? If you are working on a project at Atlassian, do you know the process to open source the work? We have launched an internal Confluence space to help find the answers to these questions and more. The vast majority of open source projects released by Atlassian are hosted on Bitbucket . We plan to continue down this path. At the same time, we do not have a plan to migrate projects that are hosted elsewhere. Here are some stats. Inactive open source projects hosted on the Atlassian Bitbucket or GitHub repositories should be archived over time. Archiving a project is not statement about the quality of work that went into the project. A project may be archived for any number of reasons. For example, team resources to maintain the projects are no longer available, the project has become part of another project, or the project has simply reached the end of its life cycle. We are creating an archival strategy to relocate repositories that have little to no current activity. Archiving will not be a one time housekeeping activity. This is something that we should be doing on a regular basis. Looking at the public repositories on Bitbucket, here are a few stats to illustrate this point: If your house is anything like mine, you can’t organize it alone. Without the buy in of all the family members, your efforts will be quickly defeated. That’s the same with this spring cleaning project that we’ve been tackling at Atlassian. We’ve been asking for input from all Atlassians as we clarify our internal processes and develop strategies around archiving. It is exciting to see what our teams are already doing in the world of open source, and we look forward to putting tools and strategies in place to help us continue to move forward. If you have questions, recommendations, bright ideas, etc. for us, we welcome your thoughts here in the comments. In exchange for your feedback, we promise to keep everyone up to date on our progress.", "date": "2015-08-28"},
{"website": "Atlassian", "title": "Creating beautiful presentations using Highlight", "author": ["Dallas Tester"], "link": "https://blog.developer.atlassian.com/beautiful-presentations-with-highlight/", "abstract": "When I’m preparing content for a presentation deck, I often find myself struggling to get code snippets from Sublime Text to Keynote. The code loses highlighting and indentation. After having this frustrating experience, I looked for and found a nice little command line tool called Highlight by Andre Simon that helps with the struggle. Highlight is simple, cross-platform, and incredibly flexible. For me, I work on the command line a lot so having a tool that I can access immediately is great. The nice thing is that highlight has a GUI tool that contains a subset of the command line tool and a realtime preview. You can learn how to build the GUI in the installation instructions . The plug-ins and themes provide flexibility to support specific file types and to tailor your output for a given presentation deck. Highlight ships with quite a few themes that you can find in the share directory after installation. The first thing you’ll need to do is install highlight . My preferred method for OS X is to use brew , which makes it super simple: For Windows, you can simply download the Windows installer and follow the instructions. Easy! My primary workflow is sending a code file into highlight and piping the output to pbcopy to get it into the clipboard. From there, I can easily paste it into Keynote and do any necessary resizing to make it readable on screen. This example is the simplest example of the command line options. Note that the rtf output format is required for pasting into Keynote. This preserves the formatting and coloring. Aside from setting the output, Highlight has a lot of more available options, which you can find those in the manual . I would like to…highlight…a few of my favorites: My usual command line looks something like this: That line tells highlight to style the output according to the “earendel” theme, use the Courier font, and size the font to 50 points. The output looks like this once pasted inside of Keynote: Awesome output for such a simple command line tool! Highlight is straight-forward to install, easy to use, and ships with a great set of themes. Despite its simplicity, highlight is powerful and makes creating beautiful technical presentations a breeze. Try it out and see what you think!", "date": "2015-08-18"},
{"website": "Atlassian", "title": "Smaller Java images with Alpine Linux", "author": ["Nicola Paolucci"], "link": "https://blog.developer.atlassian.com/minimal-java-docker-containers/", "abstract": "Sometimes I need to be hit in the head with an axe to find a solution to a problem that has been bugging me forever. A minimal Java container has been on my wish list since I found out about Docker and I’ve been writing about running Java in Docker for sometime already. Official Java images have historically been mastodontic – cue picture above – I just tried “ docker pull java ” and I got an image of 816.4MB . A colleague of mine few days ago mentioned Alpine Linux , a minimalistic Linux distribution based on musl libc and BusyBox that comes with a nice package manager. And the base image is … 5Mb ?! Where have I been hiding? Why didn’t I know about this?! Anyway here’s my chance to make things right. The objective: to have a minimal Java container for my (and your) applications. Let’s dream together. If you are still using JDK7 and you don’t have a strong requirement to have the Oracle version, the easiest and leaner image I found is very simple to setup with this Dockerfile : Which outputs: The result is a Java 7 runtime environment, ready for your Java 7 applications in only 123MB instead than 800+MB . NICE! For many applications teams prefer or require the Oracle JDK. In this case we can’t use Alpine package manager (yet), we have to wrangle the installation ourselves from the official Oracle packages. Do you want to see how that’s done? This is the list of steps: The whole process is well laid out amongst others in a clean Dockerfile by anapsix which I list here for completeness: The result of building this image or pulling from anapsix/alpine-java is a fully functional Oracle Java 8 image weighing only 173Mb . Impressive! The whole point of the exercise above was for me to run a leaner container with Stash – our enterprise Git server – trying to shave space off from our official image . The task was a success – if not a smashing one. The final Stash image I produced weighs 368MB which adds up to a ~30% reduction over the official image. Here how I had to tweak the Dockerfile : I love the layering ability of Docker images but for base images upon which I’ll build my stacks often I’d like them to consist of a single layer. It’s a mental thing more than anything so excuse my weirdness if you can. Many times the extra layers in your base images will not be re-used. For those situations it can be helpful to strip an image of all its layers and flatten it. The technique to accomplish that is the following: That’s it for my discoveries today. Pings, likes, comments, love or hate gladly received here in the comments, at @durdn or at my awesome team @atlassiandev . ( Credit for the epic alot picture goes to Hyperbole and a half ). You might also enjoy our ebook, “Hello World! A new grad’s guide to coding as a team” – a collection of essays designed to help new programmers succeed in a team setting. Grab it for yourself, your team, or the new computer science graduate in your life. Even seasoned coders might learn a thing or two. Read it online now Click here to download for your Kindle", "date": "2015-08-10"},
{"website": "Atlassian", "title": "Collating repositories or grafting earlier history with Git", "author": ["Nicola Paolucci"], "link": "https://blog.developer.atlassian.com/grafting-earlier-history-with-git/", "abstract": "Let’s add another arrow to our already full quiver of version control tools and techniques. Do you know that the Linux kernel you clone normally contains only a part of its entire history? If you need access to its uninterrupted evolution since the first commit you have to “graft” a few separate repositories together chronologically. In this post I’d like to show you how it works and why would you want to do that with your projects. There are a few reasons why you might need to collate histories from different repositories. Let me name a few: (You can solidify all the above scenarios later with filter-branch to make the changes permanent). Git has a local – per-repository – mechanism to change and specify explicitly the parent of existing commits: These are called Grafts. In a repository they live in file “ .git/info/grafts ” (check the Git repository layout manpage for details ). This feature has been available for a long time in Git: it has the drawback that you have to always setup Grafts locally for each repository. To overcome this problem – and more – a new command is available since version 1.6.5 : git replace , which as the name implies is capable to replace any object with any other object. This command has the added benefit to track these swaps via refs which you can be push and pull between repositories. From the Git Wiki : When Linus started using Git for maintaining his kernel tree there didn’t exist any tools to convert the old kernel history. Later, when the old kernel history was imported into Git from the bkcvs gateway, grafts was created as a method for making it possible to tie the two different repositories together. To re-assemble the complete kernel history you need these three repositories: The syntax of the Grafts file in “ .git/info/grafts ” is simple: each line lists a commit and it’s fake parent using the SHA-1 identifiers. So to re-assemble the full history of the Linux kernel add the following grafts to .git/info/grafts : With these grafts, you can get a complete and continuous history of the kernel since 0.01. More info on the process here and here . So now that you have a bit of background let me guide you through the process using a sample repository, so that you can replicate it yourself: The resulting id is the last commit of the legacy branch. Now let’s retrieve the first commit of the restarted project in the restarted branch: This id is the first commit in branch restarted . Now we want to “graft” the last commit in legacy to the restarted branch replacing the first commit there: To verify it worked you can check that folder .git/refs/replace contains the correct graft: And in fact git log now shows the entire collated history: Because this replacement is stored in a ref , we can push it and share it with the team! Simply fantastic. Credits for helping me put together the instructions go to this fantastic SO post . If you’re into more Git materials, before I let you go let me suggest a couple further readings: That’s it for now, I hope you found this technique interesting or useful for your projects! In any case if you liked this why not follow me at @durdn or my awesome team at @atlassiandev ? (Clip icon credit Thomas Helbig from the Noun Project).", "date": "2015-08-05"},
{"website": "Atlassian", "title": "Is your team ready for DevOps?", "author": ["Ian Buchanan"], "link": "https://blog.developer.atlassian.com/is-your-team-ready-for-devops/", "abstract": "Seven years ago at Agile 2008, Patrick Debois and Andrew Shafer were the only people interested in Agile Infrastructure. This year at Agile 2015, DevOps is a full-fledged track . Just like conference tracks, many teams concurrently pursue the perceived benefits of Agile and DevOps . Unfortunately, they often do so without building a shared understanding of the journey ahead. The Agile Fluency model has already been useful to help align team and business expectations. What can we apply those ideas to help the DevOps cultural movement avoid the same missteps? By Barb Ballard on FreeImages.com licensed under FreeImages.com Content License . At OOP 2015 in Munich , I attended a talk by Steve Holyer about the Agile Fluency model . Agile luminaries Diana Larsen and James Shore proposed the model and summarize: Agile methods are solidly in the mainstream, but that popularity hasn’t been without its problems. Organizational leaders are complaining that they’re not getting the benefits from Agile they expected. … [This model proposes that] Fluency evolves through four distinct stages, each with its own benefits, costs of adoption, and key metrics. Steve’s talk helped me understand the gap between expectation and reality. Namely, people who acclaim Agile tend to lump all the benefits together. In reality, there are distinct stages, each with different kinds of investments and benefits. The journey toward DevOps has a similar gap. We hear the success stories from Flickr , Netflix , and Etsy . We relate to the starting point and aspire to where they are today; yet, we often miss the intermediate stages. I propose the Agile Fluency model can be mapped into DevOps concepts: To the extent that DevOps is an extension of Agile , the 1st stage is the same for both. Your team needs to know what the organization values and plan in those terms. Working on the right thing is a benefit to everyone. What members of the team recognize as purpose, managers perceive as greater visibility. Orientation to business value means managers have enough information to redirect people and teams when business priorities change. Team goals require trust in management and other team members. That’s why this stage requires a team be ready to change the culture of itself. Scrum and Kanban both provide good guidance and structure for making this kind of change. It should take 2-6 months for a team to become fluent in aligning to and tracking in terms of business value. If you go longer, maybe you need to try something else. Maybe Scrum isn’t a fit for your team, try Kanban. Maybe going it alone isn’t enough for your team, try an Agile coach. Or maybe the coach you have isn’t focusing on the right things, try another. There are many Agile practices that can help. I’m quite fond of Impact Mapping , User Stories , and Specification by Example . Each of those contribute turning big business goals into smaller pieces of work that a team can deliver in small chunks. In the Atlassian tool set, I reach for Confluence and JIRA to help. Confluence makes setting those business goals a collaborative process then keeps those goals ever-present as the team’s target. Once oriented to business goals, JIRA Agile helps track progress. One of the differences for DevOps in this stage is applying these practices to operations. Compared to development, operations often has more unplanned work such as responding to system outages. We have fewer industry examples about applying the same Agile planning practices to non-functional operating characteristics like reliability, performance, serviceability, and security. Just do a Google search on “ Why DevOps ” and you’ll find all kinds of generalized justifications. However, DevOps is not a business goal in and of itself. It needs to fit the unique goals and direction of your business. Shipping your product faster is not the same as Etsy deploying 50 times a day . Shipping your service with better quality is not the same as Netflix maintaining sub-second mean-time-to-recovery . In a previous role, I was on a brand new team. We had all written software, worked on Agile teams, and earned ScrumMaster Certifications. Despite all that experience, we still faced the frustration of relearning technical practices together as a team. Each one of us knew how to “do continuous integration” but we needed to learn what it meant to each other. The 2nd stage of Agile Fluency addresses the mismatch between individual knowledge and learning-as-a-team. While the 1st stage requires a team culture change, the 2nd stage investment is a team learning technical practices together. You may need to attend workshops together. You may need a different Agile coach with technical chops. You may need to establish a “technical owner”, as a parallel to product owner, to arbitrate decisions and prioritize automation options. With my team full of senior, agile developers we struggled with lowered productivity for a few months before reaching fluency as a team. I have known teams with no experience with agile practices to take as long as a couple years of practice before reaching fluency. Trying new things is risky. Avoid making it worse by starting too many new technical practices at once. This stage also yields a different kind of benefit. Automation focuses on the delivery of value. Many Agile teams already focus capturing value frequently, by having 2 week sprints and “potentially shippable code” at the end. Frequency leads to revealing obstructions early. Yet, delivering value is more than just frequency. Delivery of value means putting control into the hands of the business so it can release software at the rate the market will bear. Management notices teams at this stage because they have low defects and high productivity. Extreme Programming has long provided a bundle of Agile technical practices that apply to this stage: coding standards , unit tests built with test-driven development , automated acceptance tests , pair programming , serial and continuous integration , collective code ownership , and refactoring . I find it difficult to keep practices and tools separate in this stage. Except for pair programming and collective code ownership, automated tools closely match the Extreme Programming practices. I use IDEs and static analysis tools to enforce coding standards and perform refactoring. I use language-specific frameworks to make it easier to create unit tests and acceptance tests. Of course, both unit and acceptance tests are automation themselves. I use Bamboo for continuous integration to run all those automated tasks, as often as every commit. Good tools elevate the practice, making things more visible and aligned with business value. This is where software development turns in upon itself. Where economical, team practices become encoded as automation. This kind of automation is largely what developers bring to DevOps. For some teams, those Extreme Programming practices might be enough to deliver value. For open-source and shrink-wrapped software products, done means packaged and tested. Other software becomes valuable once it’s deployed to production. That’s why Scott Ambler has advocated for thinking about operations in Agile. His Enterprise Unified Process advocated for Production and Retirement phases and for a discipline of Operations and Support. Yet it seems only recently that DevOps has drawn mainstream attention to the role operations plays in delivering value. Jez Humble and David Farley advocated for Continuous Delivery as a holistic view of delivering value. Their book extends automated testing and continuous integration into an automated deployment pipeline . In the Atlassian tool set, Bamboo supports this idea by having specialized deployment projects . These model a deployment pipeline with environments, like Development, QA, Staging, and Production. Bamboo can track the deployment of an application into each of these environments. Although the building blocks of automation are often the same as continuous integration, tracking the pipeline requires a richer information model. Each environment serves an audience who needs to know how the automation works and when it is performed. In other words, Bamboo is approachable for not only developers, but also testers and operations. Before my experience on the “all star” Agile team, I had gone through many Agile transitions. At the outset, I always found it difficult to understand what the business wants, let alone measure it. As a backlog filled out, my team would turn toward leveraging automation to enable faster course corrections. Even at that point, measurement of business value always felt out of reach. This inward focus endured as long as the team was the constraint in overall delivery of value. When my team worked its way out of being the critical constraint, strange things would follow. We started waiting for the business to make decisions. Or for operations to provide us data about what we’d deployed. In 2011, Forrester coined this bookend experience, “ water-scrum-fall “. While software teams go Agile, business and operations often continue to operate with big-batch planning and execution. Bringing business, development, and operations together goes beyond team culture and technical practices. This is a change in organizational structure. This stage has many of the benefits attributed to DevOps: better product decisions, higher value deliveries, and more reliability. Orienting to business value means you know what the business wants. It takes even tighter collaboration with the business to know how to measure the value of those things. Automating as a team means you know how to build “sensors” and aggregate data. It takes collaboration with operations to get to the right data. Fortunately, monitoring is a competency of most operations teams. At Atlassian, our product teams measure Net Promoter Score and Monthly Active Users . It has been a long journey to survey and monitor not only our Cloud offerings but also on-premise products. Atlassian management uses these metrics to make business decisions. The objectivity and comparability of these metrics has replaced the politics with mutual trust. Product teams can negotiate rapidly to shift plans and align with what is best for the company as a whole. Data drives decisions at the speed product teams need them. Of course, this kind of change typically takes a long time. The Agile Fluency model estimates 1-5 years. The cost is often measured in social capital expended on incorporating business and operations expertise into the team. The team will need to avoid the negative implications of turf wars . If you want the rewards of this level, start cross-team collaboration early, even as you are working on earlier stages. With organizational change, there may still be some culture change and new practices. Lean Startup provides many good ideas for running experiments and measuring results. Lean Software Development also provides good guidance to help teams take a disciplined approach to optimizing value delivered through measurement. The tools for measurement are evolving. The scale of some organizations demands big data. However, many organizations don’t need the most expensive and cutting-edge big data tools. Instead, they need to focus on different data gathering mechanisms. The key idea is logging. Everyone already has logs from running applications in production. So much log data accumulates that steps have to be taken to dump the overload of information. What makes logs so important is the time-centric structure. This time-centric nature is also reflected in business data warehouses, with time as a primary dimension. The dimension of time synchronizes business, development, and operations. As they say at Etsy , “If it moves, track it.” The trick is finding the tools that can aggregate over the right time scale for your organization. If you are standing at or before the 1st stage, it can be hard to imagine “optimize as a whole”. The Phoenix Project concludes with Parts Unlimited at the apex of this journey. Getting to this stage requires the biggest kind of change: culture change for the whole organization. There are few non-fictional guiding examples in the industry. The organizations that achieve this stage invest in inventing new practices, instead of following existing ones. The idea of optimizing as a whole means cross-team decisions don’t require top-down authority. Teams that can measure business value well provide objective insight to the whole organization. That means teams can use data from other teams to optimize their role in the system. Agile Fluency recommends: For most organizations, four-star fluency is probably best left as an aspiration for the future, at least until three-star fluency is within reach. As individuals, I hope people set this as a target and reach for excellence. However, organizations need to balance risk and reward. The rewards of this stage are hard enough to describe and, with so few examples to draw from, the risks are even more difficult to enumerate. Although I count Atlassian as one of the companies with competency at this stage, I’ll leave it to those who have mastered optimizing for the whole to offer advice. Is what you hope to gain recognized as valuable by the business? If not, seek first to understand. Then try again in the language of your organization. Is your organization willing to make the investment? If not, set a more modest target of a lower stage. Then try again with a more focused option. Success will win the ability to try for the higher stage later. Ready now? Try Bamboo . The deep integration with JIRA makes it ideal for orienting to business value. With out-of-the-box features for automated testing and deployment projects, you can start to involve QA and operations early. The rich information model and ease of use make it ideal for keeping the whole team involved. Once you get started, remember the power of conviction. Forget that you can fail. Build the bridge as you walk on it.", "date": "2015-07-31"},
{"website": "Atlassian", "title": "Static Go binaries with Docker on OSX", "author": ["Nicola Paolucci"], "link": "https://blog.developer.atlassian.com/osx-static-golang-binaries-with-docker/", "abstract": "Recently I’ve been writing a service in Go to enhance the projects dashboard on Bitbucket – if you haven’t heard we launched Atlassian Connect for Bitbucket as a way for anyone to build add-ons for three millions of Bitbucket users out there. Like many other Gophers I’ve been happily deploying my Go services using Docker . The process is smooth and pleasurable if not for one thing: the size of the official default Go image . After all is said and done my application – which by itself would comprise of around ~6MB of static binary in size, becomes a whopping 642MB when using the default Go Docker image. Our internal Docker registry handles that with no problems but it seems such a waste of space. Recently I found this clear article detailing a Go, Docker workflow with clear instructions and snippets showing how to statically compile an application and shrink it to 1% of the size. The technique is elegant and simple enough but because my development system is OSX that approach needs to be modified with an extra layer of complexity. I need to manage cross-compilation of my Go project across OS architectures (OSX, Linux). I did some research and attempts at using gonative , but ended up going the Docker route to solve everything. While working through the problem I remembered an older article from Xebia that did something smart: perform the build and link step inside Docker containers and store the (now compatible) binary in a scratch image . The scratch image is the smallest possible Docker image and it’s generally used to build base images or to contain single binaries. So that’s what I set out to replicate with the new insight from the former article. I ended up with a streamlined process which automates everything smoothly: Here a breakdown of the steps in detail. The Makefile will be capable of doing several things: The interesting bit here is that the same Makefile will be used both to create the build container and as configuration inside the container for the compilation command (if you want you’re free to split the two logical uses in separate Makefiles but I found it delightfully efficient to keep only one). Here’s how the Makefile looks like: The golang Docker image expects the Go code to be stored in “ ./go/src/... ” The build flags specify you want a static binary. The builddocker step does the following: Run the Makefile with the simple: The Makefile uses two separate Dockerfiles as already mentioned. Let’s have a look at the Dockerfile.build : This simple Dockefile allows us to build the static Go binary calling make . If you want to kick off the build manually you can simply type: This will generate the cross-compiled binary executable as ./main inside the container. The last step is to create a minimal Docker container and put our binary into it. For this we we can use the very tiny tianon/true or the scratch image mentioned before. This is the magical step that allows to shrink the application image hundredfold. The Dockerfile.static for this step is pretty straight forward: Run it like this: As explained in the Docker workflow mentioned before , the certificates are needed if we want the application to run smoothly in a cross architecture setting. The ADD and COPY lines here are for adding the configuration files and the web application folders that contain standard CSS, HTML and JavaScript files. After the build command the application can be started as you would expect with: The end result is beautiful, a Docker image weighting 8.6MB including all the static assets. I know it’s a small thing but it makes me feel so accomplished. Find an example of the setup in this small Git repository . Liked this piece and want more Go content? Check out my recent article on Learning Go with flash cards . In any case if you found this interesting at all and want more why not follow me at @durdn or my awesome team at @atlassiandev ?", "date": "2015-07-27"},
{"website": "Atlassian", "title": "Git power routines [video course]", "author": ["Nicola Paolucci"], "link": "https://blog.developer.atlassian.com/git-power-routines/", "abstract": "Follow along or just sit back and enjoy a live, hands on tutorial on the power routines of experienced git users. We’ll explore with real world examples how to amend commits, do an interactive rebase – and why would you want to do one in the first place, how to solve conflicts without any merge tools, the power of less known merge strategies, how to do interactive commits, and much more. How to perform an interactive rebase to remove a binary file stuck in the repository In this 5th part of the course we’ll show a few concepts useful when solving merge and rebase conflicts with an interactive example on how to solve one. We’ll explain --ours , --theirs , conflict markers and what’s the process needed to solve a conflict using Git. We cover the basics of what a merge is in Git and we show the use of a couple of less known merge strategies like the “ours” merge strategy and the “octopus” strategy. In this part we show how to perform and interactive add using Git. That is splitting the contents of some change across to more semantically meaningful commits. How to use git stash to solve common workflow situations, swap context with ease and smoothness and look cool to your colleagues. I hope you enjoyed this material and if you have any questions feel free to ask here or ping me at @durdn or my entire team at @atlassiandev . Part of the footage of the session was recorded at AtlasCamp .", "date": "2015-07-22"},
{"website": "Atlassian", "title": "React is an exciting future", "author": ["Travis Smith"], "link": "https://blog.developer.atlassian.com/react-is-an-exciting-future/", "abstract": "If software is eating the world , then JavaScript is eating software, and React is eating JavaScript. In this food chain, it seems React has a big part of our future — which is why I have been spending some time with it. Some interesting things have surfaced… At Atlassian, we use React internally and externally . Perhaps we’ve gone a bit overboard with React before, but with all new things it takes time to learn the limitations. We aren’t alone in trying to learn React, I’ve seen people create fun stuff to learn, such as Tic-Tac-Toe . There’s a slew of books coming out to help people learn React, like SurviveJS – Webpack and React , Master React , and Easy React . Having shipped products using different languages and platforms, I can empathize with thinking Ruby has terrible tools . I’m starting to believe that React won’t be following the same path, even now using ES6 and ES7 features is possible with a bit of setup. Making life easier. With Redux & Webpack , it’s possible to have hot reloading via react-hot-loader and even rollbacks in your dev cycle. That’s some pretty impressive tooling that I’m excited to start using. I like the suggestion that mobile is “blowing up” because TV sucks . The web is easy (for limited definitions of easy) but limited. This explosion of mobile and fracture of differing platforms to support for client apps is a pain developers feel today. Without change, it will only get worse. We are seeing technologies reflect that pain. The transition from writing web to client apps can be easy with React Native . And tooling exists to support this, Nuclide is an IDE written to help work with React. Facebook based Nuclide on Electron so that they could reuse their expertise with web technologies to deliver a desktop client app. Convergence exists even in the tools we use to help us with convergence. This convergence of platforms is real and growing — Microsoft is part of this transition, with universal apps for Windows 10, Windows Phone, and Xbox along with adaptor to make it possible to support software written for other platforms. React has that feeling of an exciting possible future for us as developers, along with growing interest. React has good tooling (hot reloading, Nuclide, easy adoption of ES6/ES7 features). React is deployable to web, mobile, and desktop allowing developers to reuse libraries between platforms. React sure seems to have good reasons to have buzz around it. Even if React isn’t the future, it has learnings we can take on to the next phase of how we write software. It’s exciting watching this unfold.", "date": "2015-07-17"},
{"website": "Atlassian", "title": "Video conferencing with last-n", "author": ["Boris Grozev"], "link": "https://blog.developer.atlassian.com/video-conferencing/", "abstract": "Video conferencing systems are complex. They have evolved over the decades, along with hardware and software, to be much more accessible and affordable. But they are still complex and will remain to be so. In this post we will introduce our approach to one of the important problems – that of scaling the number of participants – and show how our solution also helps us to provide a better user experience. For those who don’t know us yet, we're the Jitsi team, and we're new to Atlassian. We specialize in real-time communication and now, as a part of Atlassian, we'll be adding our expertise in multi-user video conferencing to make HipChat better than ever. One-to-one video calls are ubiquitous – they're everywhere. After the IETF figured out proper NAT traversal , setting up a 1:1 call became simple. You can make a 1:1 call from your cellphone, tablet, or from any laptop. Add more people to the mix, now that's when it gets trickier. You can accomplish three-way calls without any server infrastructure, by using a full mesh topology (i.e., connecting everyone to everyone else). Beyond three people, this is where it gets complicated. You can safely say that full-mesh just doesn’t scale. To extend beyond four participants you need a server component. The participants and the server connect in a star topology, with the server at its center. People use two main groups of server components: These are server components which process the multimedia coming from clients. Usually they mix audio, create a composite video, and send a single “mix” stream to each client. The media processing makes them very expensive in terms of CPU, but allows for somewhat simpler client software. SFUs act like routers, just forwarding packets without media processing. This allows for very CPU efficient implementations, but the network resources that they need are generally higher. In the most simplistic case an SFU forwards everything, without any selectivity. This means that the number of streams it forwards (and thus the amount of bandwidth needed) grows as the square of the number of participants: three participants, 3×2=6 streams; ten participants, 10×9=90 streams. This scales up to around 10-15 participants and then breaks apart. With no selectivity, an SFU is just an FU. Our server component is called Jitsi Videobridge . It’s an SFU and it’s quite CPU efficient. Most CPU cycles are used for encryption/decryption of RTP (media) packets, so the CPU usage is proportional to the bitrate (see Jitsi Videobridge Performance Evaluation for more details). So if we solve the bandwidth problem for a big conference, we also solve the server CPU problem. One of the ways we use selectivity to solve the scaling problem is with what we call last-n . The idea is that we select a subset of n participants, whose video to show, and we stop the video from others. We dynamically and automatically adjust the set of participants that we show according to who speaks – effectively we only show video for the last n people to have spoken. How does that help with the scaling problem? The number of streams that we forward (and so the amount of traffic we generate) grows linearly: What does this mean? By using the same server resources you can have either: Conferences with more participants. More conferences. And what about the clients, the UI, and UX? Less network resources – fewer streams are received. Less CPU resources – fewer streams are decoded. Less clutter – fewer video elements in the UI. Requires no interaction – when someone speaks, we show their video automatically. In most conferences only a few of the participants actively talk. With last-n we take advantage of this, but still allow everyone to participate. One difficult thing with last-n is figuring out who to show. This is referred to as Dominant Speaker Identification and it’s a tricky problem. For us, it's additionally complicated, because we need to do it on the SFU – without decoding the audio streams. You can find out more about last-n in a paper which we recently published: Last N: Relevance-Based Selectivity for Forwarding Video in Multimedia Conferences If you want to see last-n in action, you can head over to our testing installation here: http://beta.meet.jit.si/ Note that we have set n=5 there, so you will need at least seven people before you see the effects of last-n . Also note that this is just a testing environment and might not be very stable. So, that's a basic breakdown of what the Jitsi Team has been up to. We're very excited to be a part of Atlassian and we're looking forward to all of the progress our two teams can create together.", "date": "2015-07-15"},
{"website": "Atlassian", "title": "Easy custom Elastic Bamboo agents with Packer", "author": ["Steve Smith"], "link": "https://blog.developer.atlassian.com/bamboo-packer/", "abstract": "Bamboo provides the powerful ability to dynamically scale your build farm by launching swarms of build agents on Amazon’s infrastructure. These AWS images are fully customisable, but the process is a bit involved. This post introduces a simpler method of doing this using Packer and Ansible. Read on for the details (and a ready-to-use example repository) … One of the most powerful features of Elastic Bamboo is the ability to provide custom AWS images for your agents to run on. This allows you to provide additional software with your build and deployment tools already available to your build agents. However, while the process is well documented , it is not necessarily something to be taken on lightly, especially if you’re not familiar with the arcane commands of the AWS suite. Not to mention in these days of configuration management hand-wrangling servers, and especially disposable VMs, is far from best-practice. It’s much better to automate these problems in a repeatable manner. One option is to encode the necessary commands into a script, but these rapidly become unmanageable; this is why tools such as Puppet and Ansible came about in the first place after-all. Luckily there is a corresponding tool for generation of virtual-machine and container images from a common configuration base, called Packer . Packer is not an alternative to existing configuration management tools but complements them, and can use them to perform the actual configuration of the images, allowing you to reuse your existing setups. So with that all said, lets create a new Elastic Bamboo AWS agent image using Packer, and some existing Bamboo agent setup scripts I’ve created previously . In particular, the standard Elastic images provided with Bamboo use an older version of Docker and don’t provide Compose , so let’s fix that up…. As usual, the source for all everything here is available on Bitbucket, in this case in this repository . Packer uses JSON for its configuration format; for now we’ll now just create a file called bamboo-image.json ; this will contain our initial configuration. I’ll introduce the individual sections of the configuration and then pull it all together further down. As we’re working with AWS we’ll need to provide an ID and key; obviously we don’t want to put those into our configuration as we’ll be checking it into Git. To avoid this Packer allows you to provide additional variables at run-time via the environment, command-line or separate file. The version in my repository uses an external file, but for simplicity we’ll assume you’ve put the information into the environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY . We then pull these into the configuration using a variables block like this: With that in place we can start creating our image. Packer provides a number of image builders for AWS images, including one that can provision into a chroot . However, for simplicity the EBS-based builder is the best for our case. It’s possible to produce a Bamboo AWS image from scratch, but as we already have a working image provided it’s easier when starting out to base our new version on that. To do that we need to lookup the ID of the existing Bamboo AMI; to do that go into your Bamboo server administration page and select Image Configurations . This will list all the pre-supplied images available. We’ll use the Ubuntu one, which has the ID ami-1c247d74 , which we put into the builders section, along with some other relevant information: While this is sufficient, it’s usually a good idea to supply some additional tags , which allow auditing and accounting of your images, so let’s add that too: Putting that together, the Packer configuration now looks like this: This is actually a complete Packer configuration; if you want to test it you can save it to the file bamboo-image.json and run the following commands: Note : this will use-up some runtime on the AWS cloud and may incur some fees, although they should be minimal. At the end of the Packer run you should see a like like: This is the ID of the new image that was created. The benefit of producing our own images comes from being able to provide custom software to our build agents. But the image we created above just copies the existing AMI, so is functionally equivalent. To modify the image before we save it we need to provide a provisioners section to our configuration. Provisioners are how Packer configures the image prior to dumping it to an AMI, and is where it integrates with existing configuration management tools. Packer supplies integrations with all the current provisioning tools such as Chef, Puppet and Ansible (and as it supports plain shell-scripts, potentially any other tool can be used with a little work). We’ll be using two of those here, shell and ansible . We want to use Ansible for our provisioning, as I already have a bunch of playbooks and roles available for managing Docker and Bamboo agents. The Packer Ansible provisioner relies on Ansible being available on the image before being run; this is a bit different from the usual Ansible system of connecting to the host via SSH. However, as we also use Ansible for our continuous deployment workflow we’ll like it to be on our images by default anyway. But this creates a chicken-and-egg situation, as we can’t deploy Ansible with Ansible; we need to get it on there first. This is where the all-purpose shell provisioner comes in. shell will run any arbitrary script on the image while it is being built, so we install Ansible with that. This script looks like: We just save this to a file and then get Packer to use it via the provisioners section: With Ansible on our image we can now get down to the real work of updating Docker and adding Compose. Going into the details of Ansible playbooks and roles is beyond the scope of this post, but the full versions Ansible files are available in the repository . The main files of interest are bamboo-docker-update.yml (the parent playbook) and roles/docker/tasks/main.yml (the role that configures Docker). These are YAML files which look like: This merely invokes the appropriate Ansible roles: The role uses the official Docker Ubuntu repository to upgrade to the latest, then downloads Compose and places it under /usr/local/bin/ : With the Ansible configuration in place we can now add it to our provisioners section: Putting all this together, the final Packer configuration looks like this: This is in the file bamboo-docker-update.json . This can be invoked with: Packer will go off for a while creating and modifying our image, then in the final step dump the running VM to an AMI and tell us the ID; the last line of the Pack run looks something like this: That (dummy, in this case) ID is what we need to do the next step; telling Bamboo about this AMI. Adding new AMIs to Bamboo is pretty straight-forward. Inside your Bamboo administration screen select Image Configuration ; at the bottom of the list of existing images is a form that allows you to enter the information. Just fill this in, including the AMI ID that Packer gave us: Once this is added, it will be available to be used by Bamboo plans. However, we probably want to prefer to use this image over the others, as it has a newer Docker and Compose installed. To do this, we need to update its capabilities . Capabilities are tags against Bamboo images that specify what they have available; usually installed software. To flag our unique AMI, click on Capabilities next to our newly created image; down the bottom is a form to add new ones. For the Docker version we’ll add a new one called docker.version with a value of 1.7 : To flag that we have Compose installed, we’ll add a new executable capability called Docker Compose : That’s it, now we can start using our new Elastic Bamboo image by depending on it’s unique capabilities in our Bamboo jobs… To ensure our jobs run on our new image we just make its capabilities a requirement in our Bamboo build plan. To do this, edit the job configuration and select the Requirements tab. Then add our new capabilities: From now on whenever this job is run Bamboo will automatically start one of this instances if necessary and run our job on it. If you want more information on Packer then the best place to start is at their home page . For more information on Bamboo and Docker check out our past posts on in our Bamboo and Docker categories. To keep up to date on what we’re up to follow the Atlassian Dev Twitter feed, or my Twitter: @tarkasteve .", "date": "2015-07-14"},
{"website": "Atlassian", "title": "Creating REST APIs for HipChat commands", "author": ["Travis Smith"], "link": "https://blog.developer.atlassian.com/extract-your-python-apis-for-hipchat/", "abstract": "I exist in a world that uses Python to power some pretty awesome sites and internal tools. From time to time there are REST APIs that I keep on hitting, it would be nice to turn one into a custom slash command for HipChat . I’ll walk you through how I did that for an API. You can find the code for pypples , a small API just does geocoding for a given address – backed by Google. It’s running on Heroku now. The project is pretty simple… pypples/__init__.py starts up a Flask app and pypples/api/__init__.py sets up some defaults for Flask-RESTful , and lets pypples/api/resources.py create a couple REST endpoints ( /geo , and /hipchat/geo ). I extracted the actual work the API executed into pypples/api/address_to_geocode.py , so the two endpoints in resources.py could share that code. Not much effort to this API deployed to Heroku using the Heroku Toolbelt : And if you’re new to Python & Heroku, check out Heroku’s Getting Started with Python article. So now I have two endpoints at https://pypples.herokuapp.com/geo and https://pypples.herokuapp.com/hipchat/geo . Because slash commands in HipChat post data to the endpoint, the second endpoint ( /hipchat/geo ) accepts HTTP POSTs. Now I want to hook this up to HipChat, it’s pretty simple. Go to your rooms , login, and find a room that you want to add functionality to. Once there, find the “Integrations” option in the left navigation. Find and click on “Build your own integration” and then name your integration. The name isn’t functionally important, it just helps you find the configuration later. Hit next once you name your integration and scroll on down to create a new slash command. We need the command and a URL to post data to. After this effort, you should be able to hit your slash command in your HipChat room! So feel free to use pypples as a template for building APIs you might share between normal REST consumers and HipChat. Good luck doing the same!", "date": "2015-07-10"},
{"website": "Atlassian", "title": "Testing Clojure macros with metadata", "author": ["Steve Smith"], "link": "https://blog.developer.atlassian.com/testing-clojure-macros/", "abstract": "This is a bit of a technical aside about Clojure macros and core.async . In the below sections I’ll show how you can: Read on to get the details… Let’s say we have a simple function that adds two number and then calls a function with the result: To get the results of our function we need to pass in another function that uses it: A little convoluted perhaps, but this is actually a fairly common pattern in asynchronous programming, especially in the JavaScript world. If you prefer a slightly more practical example, here’s a function that does the same thing but in a different thread, possibly utilising an additional CPU: The problem with this style of programming is that once inside a callback tree it is very hard to get out of it. The simplest thing is the pass the result of the callback to yet another callback, rapidly resulting in callback hell , with deeply nested anonymous functions, or a huge number of small, single-use functions. What we’d often like is the ability to return the result passed to a callback up to the ‘top level’ where we originally called adder . But when doing so we need to take into account that the callback may be called concurrently from the ‘main’ thread of control, so we need to wait for it to complete before retrieving a value. There are a number of ways of doing this, but luckily Clojure has a library to help with such asynchronous programming, called core.async . This provides, among other things, one-way, thread-safe channels. We can use these as a method to both wait-for and retrieve the value passed to the callback. The resulting code looks like this: Let’s unpack this a bit… The first thing we do is create a new channel with a length of 1 using (async/chan 1) (assuming core.async has been imported as async ). The reason we give it a length of one is to ensure that it doesn’t block when we put data onto it, decoupling it from the caller. The channel effectively acts as an intermediate store for the return value; if the length was zero then the push operation would block until the receiver fetched the value, which is not what we want. We then call adder with our result callback. This callback puts the result onto our channel using the >!! function. Finally, back in our main stream of control we retrieve this value from the channel. As the channel is blocking, we can ensure that we wait until the return value is ready. The let then returns the retrieved value. Obviously this is a somewhat convoluted example, but hopefully the point gets through; core.async and channels can be used to retrieve values from an asynchronous, possibly concurrent, callback system and make it act more like synchronous, imperative code. But, this isn’t actually much better is it? All we’ve done is replace callback hell with channel hell ; boilerplate code to support channel creation and manipulation. However, the important word there is ‘boilerplate’; in the Lisp world any use of boilerplate code is a sign you should be considering replacing it with a macro . If you’re not familiar with macros, the short version is that they are Clojure code that run at compile time and produce new Clojure code that is passed to the compiler. This is an immensely powerful idea and has long been one of Lisp’s biggest strengths. If you’re interested learning more about them the best reference in my mind is not a Clojure one but Practical Common Lisp by Peter Seibel. Common Lisp is slightly different to Clojure, but the core concepts are the same. But back to our boilerplate code. If we’re going to use this pattern a lot then it is worth our while to turn it into a macro. Being good little developers, naturally we’re going to follow TDD and write our testing up front. Luckily that’s pretty easy; for starters we already have an example of code we need to produce above: We also have the function macroexpand-1 which will run a macro and return the result. We can then compare these pieces of code to see if they’re equal (as Clojure code is also Clojure data). So lets write our test: Note: This assumes that you’re tests are in the same namespace as the tested code. If not you may need to fully-qualify the variable names. Now that we have our test we can write our macro. We’re going to keep the implementation simple and limit it to a single target function (but see below for pointers to a more advanced version). What we want is something like this: That will then expand to our test code above. So the steps to produce the macro are: As you can see this just uses the Clojure syntax-quote (aka ‘backtick’ ) operator to replicate the example code. The only special thing it does is inject the result function into the target adder code (aka body ) using concat inside an unquoted area. defmacro then returns this generated code to the compiler. And if we run our test we should see them passing. So far so simple. Except… If you have a background in Lisp, especially Common Lisp , alarm bells will be going off now, because our generated code uses named variables. While Clojure does take some steps to make macros hygienic (namely by namespacing variables in syntax-quotes) it is still to recommended to avoid statically named variables in macros, as we don’t know what context they’re going to be used in. Luckily Clojure (like most Lisps) provides a function called gensym to generate randomised variable names (aka ‘symbols’) that we use instead. Clojure even provides a handy shortcut for this; if you append a # to any variable inside a syntax-quote it will be converted into a randomised symbol. e.g: Unfortunately this name->symbol conversion is only valid inside the same syntax-quote operation though, which means we can’t use it in our macro as we have multiple quotes. So let’s rewrite our macro to be hygienic: As you can see, we generate two symbols, csym and rsym , then use unquote ( ~ ) to inject them anywhere we would have used rchan or result previously. Simple enough; let’s run the tests again… It’s failed. This is because we’ve replaced our static symbols with gensym s, so naturally they no longer match. And here we (finally) get to the point of this post, which is: how do you test a macro that has gensym s? After-all, the gensym s are pseudo-random and not reliably predictable across runs. (There’s another related question, which is “Should you test macro output directly, or just test the resulting code?” Hopefully the process of developing the macro above should answer this; comparing macro output to a ‘known-good’ reference implementation gives us meaningful test failures we can inspect, and also acts as later documentation for how the macro is supposed to work.) So what’s the answer? Well, Clojure has some properties that can help us here, namely homoiconicity and metadata . Clojure is homoiconic , which means that a Clojure program is represented by Clojure data structures. This is why macros are so straight forward: Clojure code that is input into a macro is manipulable in the same way as data from a socket or file, and the resulting data is treated as Clojure code and passed to the compiler. The other property we’re going to use is metadata . Clojure data-structures can also contain additional annotation information that can describe special properties of the data. The Clojure uses this internally to e.g. to attach documentation to functions or mark them as private. However this metadata can be attached to any data structure, including the code produced by a macro (which as noted above is just another form of data). So we can use this to pass information back out from the macro to assist any tests. So let’s rewrite our macro to use metadata: As you can see, the only difference is that we now use the with-meta function to attach a map to the generated code. This contains two entries that tell any testing code what the gensym lookups returned, allowing us to use those in testing. We can retrieve the metadata with meta : Now we can update our test to use this: And our tests now pass 🙂 If this all seems a lot of work for what is in-fact a very simple macro, a more involved example may be in order. As noted earlier on, this sort of callback-injection and extraction macro is potentially useful in the ClojureScript world, as JavaScript uses callbacks extensively. However, wrapping each call in a macro is a bit tedious; if we want to chain a bunch of calls together it would be much nicer to have something more like Clojure’s native ->> (“thread-after”) operator. Ideally we’d like a macro that’s named something unpronounceable like -|| (although you can call it “pipe-after” if you like) that takes this: and converts it to something like this: The actual implementation of this is a bit tricky and involves right folding the input to nest the output. In this case having a reference implementation to test against is very useful. If you’re interested see below for a link to the final result. Code for the simple macro example above is available in my Atlassian Bitbucket repository . If you want to look at the more involved -|| macro have a look in my personal Bitbucket repo .", "date": "2016-03-03"},
{"website": "Atlassian", "title": "Programming with Algebra", "author": ["bmckenna"], "link": "https://blog.developer.atlassian.com/programming-with-algebra/", "abstract": "One of the teams here at Atlassian has been working on an eventsrc backend for their application. The API for the storage backend looks like: In English, the methods are for: The team needed a way of taking two storage backends and getting a new storage backend which: The implementation was a bit tricky and relied on Tee from scalaz-stream . You might want to take a look at the code (specifically, take a quick look at the merge function) before reading on. Coincidentally, the team I work on also needed this feature a few months after it was implemented. My job was to take their work and put it back into the eventsrc project so that we could reuse it. I’ve used scalaz-stream for a few years but I’m always confused by how Tee and Wye work. The original project had some useful integration tests for the dual storage, which I was happy to trust, but I prefer to write property-based unit tests for algebraic properties as much as I can. I spent a while looking at the signature: If you ignore the specifics, you might be able to see it looks a bit like: Which is the same signature as the append operation on scalaz.Semigroup . The operation has the following law: Which is known as the associativity law. Could we turn our “combine” method into a Semigroup? That is, did the method uphold this law? To answer this, we need to think about what it means for instances of our storage API to be equal. Since we have 3 methods on the API, an instance is equal to another if it gives the same results as the other for each of the methods. It is easy to see that this would be true for the “put” and “latest” methods. With “put” we just use the first storage and we can see that it’s associative: And with “latest” we choose the maximum, which is also associative: The final method, “get” was not so clear. We don’t have to think too hard about it though, we can just use property-based tests to find out. Our property should look something like: Meaning that to test the “get” method, we need 3 event storage instances and a key. For a property-based test, we need a way to generate the inputs. To generate an event storage instance, we just need to generate a list of events: Now we can translate the property stated above into ScalaCheck: And executing our tests gives: Which means we generated 100 lists of events and tested that the merging inside the “get” operation (with the tricky scalaz-stream Tee work) is associative! I could just write the operation as a Semigroup and get a lot of useful helper functions from that fact. Easy. But there’s a lot of operations which are associative. The implementation of “get” could have just returned a stream from a single storage backend, not both. It could have always returned an empty stream. Both of these implementations would have passed the property tests, among many others. So recognising the Semigroup will allow us to reuse some functions now and in the future, but it only assures us a little bit of correctness. We need more tests. I spent a while looking at the signature, again: We can have any two values of the same type and get out a value of the same type. What do we get if we put the same value in both sides? The “get” method merges two streams together, ignoring events with duplicate keys. If we give the operation the same stream, the second one should be completely ignored – leaving us with the original stream. In algebraic terms: Seems like a cool property but it was one I hadn’t seen before. I didn’t know what the name was. The easiest way to find it was via Twitter: Is there a name for the following property? forall a. a • a = a — Brian McKenna (@puffnfresh) February 12, 2016 Within a few seconds I got the answer: this property is also known as idempotence ! Rúnar (coincidentally one of the authors of scalaz-stream) then tweeted a link to mathematical bands , a.k.a. idempotent semigroups . I was unfamiliar with the binary form of idempotence so this was really cool to learn! It looked like the event storage semigroup was an idempotent one. A simple test would tell us: And ScalaCheck tells us it’s not! But what we see is that ScalaCheck expected to see event #1 followed by event #-1873729368 – the problem here is that the merging function assumes that all of our streams are in ascending order. It was something we had never written down before, but all of our instances should make sure they give back results in ascending order. We rely on this – so this property made us turn an assumption into a requirement! After adding the requirement as a comment on the API, it was time to change the generator: And now: We get an error because we’re generating events in a single stream with duplicate keys. Urgh, that shouldn’t happen either. Need to be more specific with our comments and generators: That was it! This idempotency property doesn’t give a huge amount of reusable functions (it’s more of a performance improvement ) but it does rule out “get” from always returning an empty stream. I think the biggest advantage we got from this is a refined requirement on the API. Since we’re now saying that the API has requirements of being sorted and distinct, we should explicitly test that the requirements are upheld after being put through the Semigroup operation: And finally, to make sure we’re totally correct, we should test that our scalaz-stream Tee-based combining function is the same as a very inefficient, but simple, list-based approach: Now we can be confident that “get” is correct. We can go through and do the simple “put” and “latest” cases. We end up with the following: That’s 1000 tests, taking just a couple of seconds to run! We know that eventsrc’s stream combining is rock solid without knowing anything about the scalaz-stream Tee algorithm and without having any integration tests. There’s still a couple of algebraic properties we should look into. Either for reusability, extra correctness guarantees, optimisations, reasoning tools or even just as exercises! If you are interested in doing this, I will help you out as much as you need. Take a look at the outcome of our work on Bitbucket . Talk algebraic programming with me at @puffnfresh and follow us on @atlassiandev .", "date": "2016-03-04"},
{"website": "Atlassian", "title": "Why you should use Clojure for your next microservice", "author": ["Leonardo Borges"], "link": "https://blog.developer.atlassian.com/why-clojure/", "abstract": "There are a few reasons teams choose to implement some piece of functionality as a microservice. To me the most important ones are: This post focuses on the third point above. This is a very important property of a microservices oriented architecture. Microservices are meant to be small. How small is usually up to the company/team culture but I’ve heard of microservices no larger than 100 lines of code and others of a few thousand (if this is Java, I’d still consider it small). Since microservices are meant to be small, developers are encouraged to use whatever technology they think is best. If it turns out to have been a bad choice that’s Ok – we learned that and will take the new knowledge into account for future projects. In some cases the team might even decide to re-write the microservice in a language that is better suited since it’s most likely only a few hundred lines of code. This is all fine and dandy. However If you have total freedom in the choice of technology why would you choose Clojure over [insert your favourite programming language here] ? I’m so glad you asked! “Lisp, made with secret alien technology.” — lisperati.com Clojure is a modern Lisp for the Java Virtual Machine. Targeting the JVM has many advantages: So, the JVM is great, and we are already using Java. Why switch? Well, Java comes with its own set of trade-offs. It lacks many features other platforms take for granted – such as lambdas, though that’s been rectified in Java 8. Its type system is overly restrictive and its type inference is less than ideal. Java also doesn’t have data structure literals , making it even more verbose. Additionally the usual development feedback cycle in Java is too long. It involves multiple JVM restarts a day in order to run your applications and/or tests – even if all you did is change a single line (HotSwap does mitigate some of this). Experiments become hard given the amount of structure required just to get started. Finally, the lack of an interactive development environment is a deal breaker. “Lisp isn’t a language, it’s a building material.” — Alan Kay Being a Lisp means being extremely flexible, taking an opinionated data driven approach to programming and providing utmost malleability through the power of macros. It’s important to note here that Lisp macros are very different from, say, C macros. C macros are a simple way to replace names found in the source code with the macros defined and processed by the C pre-processor. Clojure macros on the other hand are a way to manipulate Clojure code itself – yes you can transform the AST ! – including the ability to control when and if things get evaluated at all. \"Within a couple weeks of learning Lisp I found programming in any other language unbearably constraining.\" — Paul Graham, Road to Lisp All this power would have gone to waste if we would have to go through the traditional development cycle of: change the source -> re-compile/re-start the runtime -> run the code to see the results/run tests -> rinse, repeat. One of the main reasons for which I became a software developer is the thrill of creation. Writing some code and immediately seeing the result of your labour is an amazing experience! I believe we should do whatever we can to reduce the time it takes to see the results of something we just created. Clojure – and indeed, most Lisps – ship with an advanced REPL (Read-Eval-Print-Loop) environment that is tightly integrated with many editors such as Emacs, Vim, IntelliJ and Eclipse. But what does it mean for day to day development? My normal Clojure workflow involves starting a REPL session in Emacs – though the same applies to IntelliJ, Vim and Eclipse – and generally leave it running for the duration of whatever task I’m working on. It’s not uncommon to have a REPL session running for days or even weeks. This completely eliminates the drawback of the JVM startup time as I can change code in my editor and immediately re-evaluate it in the running REPL using a simple keyboard shortcut. This makes developing in Clojure a lot more experiment-driven. The cost of exploration is minimal and as such you end up growing your application through the REPL. This applies to tests as well, the same low cost approach works for running them. After all they’re just functions. “Choose immutability and see where it takes you” — Rich Hickey, Clojure creator One of the places where Clojure sets itself apart from other Lisps is in its opinionated approach to functional programming. Just like in many other languages, Clojure supports higher-order functions. This means functions are first-class citizens and can be both used as arguments as well as return values from other functions. However in Clojure data is immutable by default. If you create a map, a list, a vector, what have you, it is immutable by definition and cannot be changed once created. This has a number of benefits: Additionally Clojure ships with many concurrency utilities such as reference types and software transactional memory (STM) and core.async , a CSP (think go-like channels) implementation as a library. Clojure gives us a powerful development environment but if we’re developing web applications most likely we are still using JavaScript which is very limited as a language. Clojurescript is the Clojure compiler that targets the browser, bringing all the power of Clojure to the frontend web development landscape. (but sure, you can run it on Node.js too ) Many companies have adopted Clojurescript in production, including us here at Atlassian . One of the main benefits we get from it is the ability to share our core algorithms between the server and the browser with minimal effort. I maintain that Clojurescript is probably the most mature alternative to JavaScript currently available. This is a very hand wavy post and it’s honestly hard to describe in words all the benefits I’ve listed so far so I recommend you watch this video by Bruce Hauman where he programs the Flappy Bird game interactively with Clojurescript – it’s a great example of the sort of workflow I described in this post. If you’re interested in learning more about Clojure I can also highly recommend the Sydney Clojure User Group . We’re a good bunch and welcome attendees of any experience levels. If you’re not in Sydney you can find your local user group in the official user groups listing in the Clojure website. Reach out @leonardo_borges to talk Clojure and FP and follow us on [@atlassiandev]( http://twitter.com/atlassiandev “Atlassian Dev Twitter Account”).", "date": "2016-03-10"},
{"website": "Atlassian", "title": "Critical security releases for Node.js and io.js", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/node-critical-security-release/", "abstract": "Late last week, there were new releases for both Node.js and io.js that addressed a recent critical security concern. A bug in the V8 JavaScript engine was found that could cause a denial of service attack. Vendors with add-ons running Node.js or io.js in production may want to upgrade your servers. The critical security releases for Node.js and io.js include: UPDATE: Only a few hours after this post was published was there another release of Node.js. Version 0.12.7 upgrades OpenSSL which fixes another security vulnerability. More info on the vulnerability can be found on the OpenSSL site . Again, if you’re running Node.js in production you’ll want to make sure you have the latest version. A brief description of the vulnerability from the Node & JavaScript Medium blog . Kris Reeves and Trevor Norris pinpointed a bug in V8 in the way it decodes UTF strings. This impacts Node at the Buffer to UTF8 String conversion and can cause a process to crash. The security concern comes from the fact that a lot of data from outside of an application is delivered to Node via this mechanism which means that users can potentially deliver specially crafted input data that can cause an application to crash when it goes through this path. We know that most networking and file system operations are impacted as would be many user-land uses of Buffer to UTF8 String conversion. We know that HTTP(S) header parsing is not vulnerable because Node does not convert this data as UTF8. This is a small consolation because it restricts the way HTTP(S) can be exploited but there is more to HTTP(S) than header parsing obviously. We also have no information yet on how the various TLS terminators and forward-proxies in use may potentially mitigate against the form of data required for this exploit. For testing you can switch versions of Node.js/io.js using a CLI tool called nvm . Installing: Switch to a version already installed: Subscribe to @atlassiandev and our blog feed to stay up-to-date with the latest information. Ralph is a Developer Advocate for ecosystem developers.", "date": "2015-07-09"},
{"website": "Atlassian", "title": "Code Approval Policies Explained", "author": ["Nicola Paolucci"], "link": "https://blog.developer.atlassian.com/code-approval-policies-explained/", "abstract": "Professional teams that produce quality software and maintainable systems often employ a lightweight process to safeguard the introduction of new or updated code to their stable branches. A code approval policy is an agreement within a single team or an entire organization to follow a set of rules regarding how and when code is accepted into the main lines of a project, on the way to reach “production” and final distribution. In short the way a team approves code for a new feature or bug fix making sure only what is good and high quality gets merged. For teams that use Git , a common way to adopt such a policy is to enforce merge checks at code review or pull request time. Check our documentation for the definition of pull request . There’s a lot of freedom in how teams can define the conditions required before a code change can be merged. And the options are not mutually exclusive, on the contrary, it might help to mix and match. Let’s review a non-exhaustive list. The easiest policy is to enforce that a few people look at the new feature or bug fix before it’s merged. For example, many teams decide that a [pull request] can only be merged if at least two developers have reviewed and approved the code. Your team may want to set an upper limit on the number of reviewers to prevent slowing down the progress too much but it’s often useful to invite more reviewers than the minimum approval limit so that the progress on the review is not stalled by busy team members. This might come without saying, but after a reviewer has provided feedback and suggested amends, code should not be merged until those suggestions are either implemented as requested or at least discussed and a consensus is reached. Failing to do so defeats the purpose of having the review in place. Bitbucket Server offers tooling to this effect. Another approval policy is to enforce that a certain number of builds are green – all the tests pass – before the code can be merged. The requirement for multiple successful builds can stem from the fact that a repository contains different components which have to be built separately, or from the fact that a “build” can sometimes be interpreted as a “successful deployment to environment x”. In other cases it’s a mechanism to protect the team from flaky tests. Over time some projects grow flaky tests – i.e. tests that fail randomly once in a while and are surprisingly hard to troubleshoot or refactor. Because of this some teams require X number of successful builds before merging a piece of completed work. Inside Atlassian, the Bitbucket Server enforces all three checks mentioned above. For a developer to be able to merge a pull request, he needs at least 2 approvals – from any member of the team – and the build needs to be green. Teams that have services sensitive to performance degradation can put in place policies related to that as well. For example, a policy might dictate that piece of code cannot be merged onto a stable release unless the overall performance target benchmarks are met. Teams might also put in place policies that are related to seniority review and systems thinking. They can nominate a gatekeeper, an Architect to oversee major changes to the code to make sure that systems stay maintainable, scalable and performing. In addition to that, domain experts might be involved with reviewing code that is within their expertise. If a developer is making a change to a load balancing algorithm on the back end, he may want to check his work with the original author of that code and with a Software Reliability Engineer to be sure he is taking all potential problems into account. Approval policies can differ per branch: it might be enough to have two reviewers approve a pull request to move the code to a development environment but a team may also want green performance tests before merging the feature onto the pre-production system. In deciding and enforcing a code approval policy this is always something to remember. The more laborious the process, the more roadblocks the team puts in place to ensure proper review, the slower the development progress will be, at least in the short term. In the long term, one would argue that the high effort on high standards will result in less maintenance costs for the team. At Atlassian, there is consensus in all development teams that code approvals are important and should be enforced for all. Teams are empowered and free to adopt the model that they think suits them best. You will recognise some similarities between the policies listed above and how we shaped our products to support an effective code approval policy. Bitbucket Server allows teams to setup pull request checks with a few useful conditions: Because Bitbucket Server is also extendable via add-ons it’s relatively easy for a team to create custom flows that resemble the policies defined inside their own organisation. How does your team do code approval? Let us know at @atlassiandev or in the comments below!", "date": "2016-03-15"},
{"website": "Atlassian", "title": "Tip of the week: securing your Bamboo remote agents", "author": ["John Garcia"], "link": "https://blog.developer.atlassian.com/totw-secure-bamboo-agents/", "abstract": "This week’s tip comes from the Bamboo documentation, with some extra explanation provided by Rafael from the Bamboo support team. We always recommend taking the steps to secure publicly-accessible hosts to prevent unauthorized access or packet capture. The best way to secure the Bamboo remote agents and their transmission channels is the use of secure shell (otherwise known as SSH) for data encryption when using insecure networks. Read on about Securing your remote agents in our documentation, then be sure to check out the extra notes found in the Bamboo Knowledge Base . Let us know what you think in the comments! Watch out for news and info from our @AtlassianDev Twitter feed! Follow me at @bitbucketeer !", "date": "2015-07-09"},
{"website": "Atlassian", "title": "Building your development skills with code katas", "author": ["Travis Smith"], "link": "https://blog.developer.atlassian.com/code-katas/", "abstract": "For years I studied martial arts. I did not spend my time studying the same martial art but branched out and studied Jujutsu , Kempo , Capoeira , Kung fu , and Taekwondo . I started off with Taekwondo, and my learning involved repeating forms (or katas more generally) until they became second nature. I became frustrated by repeating katas again and again — which pushed me to branch out to other martial arts. It wasn’t until later in my practice that I began to understand how repeating katas made me better. While I haven’t actively practiced martial arts in years now, I have begun a new habit of doing code katas to refine my development craft. Two to three times a week, I carve out 30 minutes for focused practice. Code katas are simple exercises in programming used to hone your skills. Often developers will repeat a given exercise each time, such as Roy Osherove’s string calculator kata . Repeating this exercise provides the opportunity to hone your development skills with practice. When starting with the Taekwondo forms, I struggled. Repeating the same activities over and over without obvious purpose was boring. It wasn’t until I started doing more Jujutsu that I began to understand. With Taekwondo, you would repeat the same form again and again. The idea was to move the movements from active memory to muscle memory. You shouldn’t have to think about the act of blocking and attacking, it happens without thought. In my practice of Jujutsu, katas were more like free-form sparring. You might focus on a particular block, hold, or throw, but your partner was there to mix it up so the input action was not always exactly the same. This made it click with me, and I understood the purpose. Starting off with code katas is similar. At first, you might struggle to complete the exercise in a reasonable time box. But once you figure out how to do that, repeating the exercise as is does not build important skills. At this point you have to think about what you want to practice. When I first started with code katas, I focused on my tooling the most. How can I run my unit tests the quickest? How do I get feedback from my activities faster? I would learn key combinations in my IDE for common actions. I would create snippets for unit tests or other boilerplate I would write. Whatever it takes to allow me to finish faster using my tools better. After I felt comfortable with my tools, I focused elsewhere. It would begin by something as simple as thinking “today I’m going to use library x” and spend the time to learn the library as a part of my exercise. The best part of doing it this way is that I already know the problem, so I can focus on the library I want to learn. Each time I would add a constraint to my practice, allowing me to expand in a different way and learn something new. I’m not alone in believing this is critical for katas to be useful . Without focus, you might get better at solving the specific problem in the kata but programming is about solving a range of problems. I feel that doing code katas have benefited me as a developer. It’s not hard to get started. There’s code katas in Coding Dojo’s Kata Catalogue — pick whatever interests you. Carve out regular but limited time and get cranking. Remember, with some of these katas at first you may not even be completing the exercise. That is okay. It’s a process — it takes time to get beyond the problem of the exercise and on to focusing on a specific skill. I’ve used katas to help learn new languages and libraries. Next, I want to find a way to do a full-stack web-app kata. To this point all the katas I have practiced have focused on code and tooling. The plentiful web frameworks that are out there could use a kata. I imagine something like TodoMVC , but TodoMVC does not represent the type of web apps I commonly write. I am on the lookout for a great, full-stack kata to take my practice to a new level.", "date": "2016-03-16"},
{"website": "Atlassian", "title": "Happy Gitiversary Junio", "author": ["Steve Smith"], "link": "https://blog.developer.atlassian.com/gitiversary-junio/", "abstract": "As Linus Torvalds has already pointed out , the 8th of July 2015 is the 10-year anniversary of Junio C. Hamano taking on maintenance of Git . And here at Atlassian we appreciate the excellent work he’s done over that time. For the last 10 years Junio has been nothing but an exemplary project maintainer, taking Git from a niche tool for the Linux kernel to being the industry-transforming platform it is now. Happy Gitiversary Junio, and here’s to many more years of maintainership from you! Update : As Junio himself has pointed out , didn’t actually take over the maintainership of Git until the 27th . However here at Atlassian we consider Linus’ excellent choice of successor to be worthy of celebration in itself and we’re happy to have two celebrations. 😉", "date": "2015-07-09"},
{"website": "Atlassian", "title": "Analysis: A new Nexus 5 edition is all about the Fi", "author": ["Steve Smith"], "link": "https://blog.developer.atlassian.com/nexus-fi-opinion/", "abstract": "The usual rumour mills are all fired up over the possibility of new Nexus 5 & 6 editions appearing this year, mostly over who would be making them. Most of the justifications given for a new Nexus 5 in particular seems to focus on either the new Android M hardware features such as fingerprint recognition and ARMv8. However, there is one hardware feature that I haven’t seen discussed anywhere, and it’s the one reason why I feel Google absolutely must release a new Nexus 5 model this year. That reason is simply Project Fi . Google has been trying to disrupt the US mobile industry since 2010, when it made the original Nexus One available unlocked on its own store. This went against US model of releasing phones only with a carrier’s plan; this practice famously frustrated Apple fans , who for years could only get the original iPhone on AT&T. Google’s previous attempts at disruptions seems to have had little effect however. Project Fi is still invitation-only and there are few details on its implementation, but appears to be Google’s latest attempt to maneuver around the mobile operators. It does this by treating them as a network of networks, supplemented with WiFi hotspots. This removes the user from the tyranny of having to choose a single carrier by allowing the phone to choose the best option at any given point. Google acts as the broker between the networks (whether that will put users under the tyranny of Google remains to be seen). The problem at the moment however is that Project Fi is only accessible to one phone model; the Nexus 6 . This is because Fi requires dedicated hardware, and the Nexus 6 is only phone that has it. It’s not clear what this hardware actually does, but it’s almost certainly related to managing the interaction between the networks (it may also handle encryption, although the Qualcomm Snapdragon 808 has a native encryption module). The problem is that automatic hand-off between networks of different types (as opposed to hand-off between different cellular towers on the same network) is hard to do. The reason is that it must do so seamlessly, possibly while the user is on an active call. This can be tricky to do even when the network is under a common control, which is why seamless WiFi roaming requires a custom hardware, usually in the form of Zero Handoff base-stations. On disparate networks that are unaware of each-other it’s going to be up to the client to somehow mediate the flow of packets to provide a smooth experience. So Fi requires custom hardware, and only one phone has it. And unfortunately it’s not a particularly popular phone; the Nexus 6 sales were disappointing , especially compared to the wildly popular Nexus 5 . Google could rely on 3rd-party manufacturers to include support, but they’re not going do so until Fi is popular, and Fi will remain niche until there’s widespread support: catch-22. So it’s up to the Nexus line to drive the adoption and encourage the 3rd-parties to follow along. But to a degree the Nexus line has always been about providing flagship features first, so this is nothing new. So to put my reputation where my mouth is, here’s some predictions. Come back in January to see how I did: Let’s see how this goes, shall we 🙂 I’m going out on a limb here a bit, but I think my reasoning is sound. But I’d be interested in hearing other views, so feel free to jump in on the comments below or ping me on Twitter @tarkasteve", "date": "2015-07-08"},
{"website": "Atlassian", "title": "Tip of the week: stow those logs", "author": ["John Garcia"], "link": "https://blog.developer.atlassian.com/totw-stow-those-logs/", "abstract": "A new tip of the week comes to us from Todd , who wants to remind us that valuable log files can be obliterated during some events. Be sure to keep them safe! Todd says it best: Sometimes incidents get resolved without us really understanding the root cause, and sometimes this is because one or more recovery steps (say rolling back to a snapshot) obliterates a necessary JIRA log file that we could have used in root-cause analysis. Consider porting your JIRA log into a tool like Splunk. It has helped me avoid that sinking feeling when you realize your logs are gone (a few times!). An answer to this is found at Atlassian Answers , including step-by-step instructions, but we’re interested in hearing your best strategy in the comments. See you next week! Watch out for news and info from our @AtlassianDev Twitter feed! Follow me at @bitbucketeer !", "date": "2015-06-30"},
{"website": "Atlassian", "title": "Insights on Women in Tech", "author": ["Chris Mountford"], "link": "https://blog.developer.atlassian.com/women-in-tech/", "abstract": "In a recent interview with Confluence developer Denise Unterwurzacher for Atlassian Tech TV , in addition to discussing the role of technical support in Atlassian and her move from that role to software development, we also tackled the topic of women in tech. In all honesty, I was nervous even asking her about it because while it’s an important topic, I didn’t want to put her on the spot and suggest that just because she is a woman in tech that she would want to dig into the subject on camera. As it turned out, Denise was not only keen to share her experiences, but she also provided some fascinating insights and a couple of stories that completely surprised me. As the technology industry faces up to its lopsided stats and seeks to address its terrible lack of diversity with initiatives and awareness programs, there is a lot that individuals can do — men, women, and people who identify with less traditional binary sex and gender definitions. As with many companies, Atlassian has initiatives in place to attract and retain qualified women in engineering positions to address the imbalanced ratio of men and women in tech. What can I do as a white, heterosexual, cisgender man? Aren’t I the problem? No. I’m not the problem, it’s attitudes and behaviors that are the problem, and like anyone else, I need to work and learn to overcome unconscious bias. As a part of this work, it’s important to find out more about other people’s perspectives. My interview with Denise uncovered some interesting new angles I hadn’t considered. Some women are happy being “one of the boys” and Denise confesses she is one of these women. But, Denise explains, many other women need to feel less like an outsider in order to enter or stay in a technology career. It’s not necessary or helpful for men to feel threatened by this fact. There are a range of perspectives and experiences. Something I hadn’t considered was the role those women play who are comfortable being in the minority, the women who might be more likely to find themselves as the only woman on a team. I was used to being the only girl in the entire team, just one of the boys , which is fine. That works for some people and it doesn’t work for others. I was actually talking with another girl at Atlassian recently and she mentioned that even though she’s happy being one of the boys, maybe it actually makes other women less comfortable to even attempt to join technology. So she sees it as part of her responsibility to be welcoming to the other women as well. I think it’s very important and it’s something that a lot of women forget about or just aren’t really aware of. Recalling her earlier years in system administrator roles at other companies, Denise tells a surprising story she’s seen play out, saying that there are times when “women in tech can be their own worst enemies.” There’s often one woman and she’s used to being the one woman in the team and if you come in as another woman, I’ve actually had a couple of women be very unwelcoming and very hostile. “I am the one of the boys and you’re not invited!” It’s worth emphasizing that Denise’s experience won’t match that of every woman and may not even be typical but it sure gives us some insight into the details that help us appreciate diversity. While it’s understandable that the industry narrative challenges men to practice more inclusive behaviors as a part of making the next waves of women feel welcomed into technology roles, Denise also encourages women to adopt the same attitude, highlighting the importance of the way women already in tech behave. I’ve noticed a tendency towards denial that there is a problem, both in my own knee-jerk reactions and in hasty responses by other well-meaning men when faced by the fact as Denise puts it that many women are always “made to feel hyper-aware of their difference.” Not being conscious of what we do that contributes to people feeling alienated may make us think that there is no change we should make in ourselves. There are two problems with this. The first is that because unconscious bias is unconscious, we should not expect to detect its presence without awareness training. The second problem is that too frequently we can’t avoid being caught up in self-righteousness, fueled by the pride of our self-image as a reasonable, educated and civilized human. We hijack the agenda with unwarranted outrage when what’s needed is empathy and focus on opportunities to help each other. Men I’ve talked to can become too easily stuck on the unpalatable idea that they might be contributing to the problem. To help prevent being derailed by paralyzing indignation, it’s helpful to stop seeing it as women vs men battle lines. Not only men suffer from unconscious bias. Everybody has bias and everybody has privilege — it’s very difficult to be without all of those things. You just have to be aware of it. When it does come up if someone makes you aware of something that you’ve done, you just have to step outside yourself for a moment and understand where it comes from. Being a man in tech includes certain privileges such as being invisibly free from that experience of being an outsider — the notable exception being women in tech events which have become more prevalent in recent years. Denise confirms my experience that women generally welcome participation and help from men in these events, but obviously, at some point, if there are too many men in attendance, it ceases to be a women in tech event. Some people question whether women in tech events are the right path. Clearly they’re a response to the need for change. Denise reports that some of her friends suggested that this is a kind of segregation that makes a bigger problem but Denise does not agree. I think that’s not true. There is a problem already. There is a problem with the representation of women in tech. That’s not a secret. It hasn’t come about because of women in technology events. Women in technology events don’t make it worse, and not having them — ignoring it — is not going to make it better. This problem isn’t going to go away on its own. Unconscious bias is something I assume to be at work in me as it is in everyone. Do I encourage my son in his interest in robotics more than I encourage my two daughters to develop technology skills like coding and electronics? Taking control of technology and being tech literate is useful even for someone not interested in a technology career. I’m not sure what being “in tech” will even mean by the time my kids finish school but if we get Denise’s way, my daughters wouldn’t be thinking about being “women in tech” any more than I do today about “what it’s like being a man in tech”. I really hope that in ten years, twenty years, I don’t know how long, but I desperately hope that some day people look back and wonder why on earth we had women in tech events because it just seems absurd. But we’re not there yet. Subscribe to Atlassian Tech TV to make sure you catch upcoming episodes and while Denise doesn’t tweet you should watch the first part of my interview with Denise to hear her thoughts on technical support culture and paths to careers in technology and you can follow me, Chris Mountford on Twitter: @chromosundrift", "date": "2016-03-30"},
{"website": "Atlassian", "title": "Taming the noise of whitespace with the right dot files", "author": ["Ian Buchanan"], "link": "https://blog.developer.atlassian.com/taming-whitespace-noise/", "abstract": "While I prefer Sublime Text, my colleague, Nicola , prefers Vim. While we both have Macs, I sometimes work on my Windows desktop. Sharing work across these environments sometimes creates a whitespace conflict. Nicola’s Vim put tabs to indent in shell script, instead of spaces. Or my Sublime Text on Windows put extra control characters at the end every line of Python. And, unless you are programming with whitespace , then you probably know how painful this can be. Fortunately, there are a couple tools you can use to avoid the most common whitespace problems. From emacswiki licensed under Creative Commons ShareAlike . Unfortunately, tabs and spaces don’t get along. In some cases, the debate has been settled by an authority. For example, Python says spaces . In other cases, the debate has been settled by the team. For example, Apache says spaces too . It’s not my aim to open debate. The problem is that, once settled by people, tools don’t always respect the convention. That’s why EditorConfig is wonderful. In the product’s own words: EditorConfig helps developers define and maintain consistent coding styles between different editors and IDEs. The EditorConfig project consists of a file format for defining coding styles and a collection of text editor plugins that enable editors to read the file format and adhere to defined styles. EditorConfig files are easily readable and they work nicely with version control systems. Here is what might be in the .editorconfig file for a Python project: It uses the common INI-style . Section names in brackets are filepath globs . In the example above, we start matching all files. We want UTF-8 and Linux-style endings. We want a newline at the end of every file. We want whitespace removed from the end of lines. The next section matches typical Python files. In addition to the settings for all files, we want the tab key to create 4 spaces. Once the .editorconfig is checked into version control, anyone who uses a compatible editor will automatically comply with whitespace conventions. Since EditorConfig is not new, there are many compatible editors. For Nicola, there is a Vim Plugin . For me, there is a Sublime Text Plugin . The notable exception is Eclipse but, where there’s a will, there’s a way . While mixing spaces and tabs can be annoying, mixing newline styles can break things. I’m looking at you Bash. Fortunately, Git solves this problem quite well with core.autocrlf . Windows users even have a sensible default that turns it on. Trouble still emerges when people don’t use defaults. If you work on a team where people can use Windows and non-Windows, then you don’t need to rely on defaults. You can specify the line endings with .gitattributes . In the simplest case where Git can just manage all line endings, your .gitattributes can be: Or, you can get specific per file extension with a .gitignore like this: Having been bitten by line endings before, I like to create .gitattributes at the start of a new repo. Most people won’t know they need one until after a mix of newlines has gotten into the code. Once you have a proper .gitattributes file, you can apply the settings retroactively to normalize all the files. From the Git documentation about line endings : If you are working on an open source project, I hope the value of these files is obvious. You want to encourage participation without having to scold collaborators for not following your whitespace conventions. If you are working on proprietary source code, you might think there is no need. Please consider the future developers on this project. Will they know how you used whitespace? Better to give some hints in the form of these files.", "date": "2015-06-25"},
{"website": "Atlassian", "title": "More on Git 2.8: push, grep, rebase, config neat things", "author": ["Nicola Paolucci"], "link": "https://blog.developer.atlassian.com/whats-new-in-git-2-8/", "abstract": "Hear Hear, the new Git 2.8.0 release is out! In the past couple of weeks, while the release candidate cycle was ongoing, I went through the commits and release notes trying the new things and scribbling down the interesting bits. To save you time here is a subjective selection of things to try out. Enjoy! This is a nice addition for both consistency and speed of typing. You could already use git branch -d to delete a branch locally, and now you can shorten remote branch deletion to git push -d as well. Example: A couple of relevant things have been integrated into grep ‘s functionality: git pull --rebase can now be run interactively as git pull --rebase=interactive . This is an interesting addition to the pull rebase workflow for when you want to quickly squash commits or edit the commit messages at the time of the pull. Usual rebase warnings apply . Git config can now show where a value was set whether in a file, loaded from standard input, a blob, or directly from the command line. For example I may ask: “Where did I set my st (status) alias?” git config can now tell me: allows you to refer to a commit that is reachable from <branch> but does not match the given <pattern> . The above notes are just a selection, the release contains much more! For other features included go to the source and see the full release notes . Also if you happen to be in New York next week come join the Git community at Git Merge ! Finally, if you want to keep in touch with me and hear of the latest things on Git and other tech endeavours, Follow @durdn", "date": "2016-03-31"},
{"website": "Atlassian", "title": "Tip of the Week: Using Bamboo for Puppet Deploys", "author": ["John Garcia"], "link": "https://blog.developer.atlassian.com/using-bamboo-for-puppet-deploys/", "abstract": "This week’s Tip of the Week is a clever idea for automating Puppet deploys using Stash and Bamboo. Many thanks to Peter Van de Voorde of RealDolmen for this idea. Peter writes: We store all our puppet scripts and config in Stash. To make sure that everything we push to Stash is correct we execute a homemade pre-receive hook that will do a puppet-lint and puppet-validate on all puppet scripts that are in every push to Stash. Now that we are sure of the ‘correctness’ of our puppet scripts we can use them from our Bamboo Build Plans. We can now use them as part of the set-up phase of a plan to set-up the configuration of any server or environment we want by simply checking them out of Stash and executing them using an Agent. We would also like to create a Build Plan in Bamboo that actually executes a number of puppet scripts on a list of servers (for example to update the Java Version on all these servers).", "date": "2015-06-22"},
{"website": "Atlassian", "title": "Deploy a Java Swarm with Docker Machine", "author": ["Nicola Paolucci"], "link": "https://blog.developer.atlassian.com/deploy-java-swarm-with-docker-machine/", "abstract": "In this video you’ll see me use the pre-release version of Docker Machine ( 0.3.0 ) to deploy a 3 node swarm on Digital Ocean and use label constraints to deploy our Java based enterprise Git server Stash and PostgreSQL . Follow me @durdn and @atlassiandev or come say hi next week at DockerCon , I’ll be there with a fun group of Atlassians to nerd out on Docker and the revolution at hand!", "date": "2015-06-18"},
{"website": "Atlassian", "title": "Learning Go with flashcards and spaced repetition", "author": ["Nicola Paolucci"], "link": "https://blog.developer.atlassian.com/golang-flashcards-and-spaced-repetition/", "abstract": "This year I have been choosing Go for all my coding projects. Go is brilliantly fast, simple to pick up, it has a powerful concurrency model based on message passing, and no forced – always on – object orientation . My impressions are similar to the ones many have previously articulated well – for example see “ Go is unapologetically flawed… ” or the hilarious “ Four Days of Go “. Add to those a fair bit of gesticulation and enthusiastic jumping around and you get what I think. So far in Go I wrote an image collection scanner that uses perceptual hashes to find similarities and duplicates, a few web back-ends for React applications, the prototype of a task tracking tool based on the autofocus methodology, and a bot that monitors real-time Google Analytics data to notify my team on HipChat rooms for relevant events. (I’ll mention in passing that writing Bots for HipChat is disarmingly simple and rewarding. The bot above was written during one of our internal Hackathons using a tiny go package called hipchat-go ). As my knowledge of the platform grows, I am adopting more and more concurrent programming patterns that suit the language so well. Like with anything, growing a solid knowledge of a language requires time and the ability to absorb new APIs and libraries so that they become second nature. Here’s what I’ve been doing on this front to deepen my command of the language. A couple of years back Derek Sivers wrote about “ Memorizing a programming language using spaced repetition software “; that idea really got me intrigued. The point is to use flash cards software to learn a programming language. You craft your own cards organically as you learn new things about the language and you end up with a completely customised path to proficiency. So I setup up Anki and have been following the routine that follows. If during a coding session I stumble on a small chunk of API, library, or best practice that I don’t know yet I look up the answer – many times Stack Overflow has a perfect match for my search – and I summarise it into a simple Flash card. This way I can test my knowledge about it later and slowly move the information into my long-term memory. Here is a sampling of the cards I have added over time, about things that are easy to do but that I used to nevertheless have to look up every time. No, keys in maps can only be marshalled/un-marshalled if they are strings. map[string]string works. Define a custom type: Implement 3 methods: Finally use like this: I can share my flash card deck export if there is any interest but I assure you that if decide to take on this technique you should build your own cards organically over time, the value you will get from spaced repetition will be tremendously enhanced. Thanks for reading this far and if you like these sorts of random tangent dives follow me @durdn and @atlassiandev on Twitter.", "date": "2015-06-16"},
{"website": "Atlassian", "title": "HipChat and the Adobe Creative SDK: A perfect pairing", "author": ["rmanalang"], "link": "https://blog.developer.atlassian.com/hipchat-and-the-adobe-creative-sdk-a-perfect-pairing/", "abstract": "I’ve built many add-ons on top of Atlassian products over the last five years. In that time, the most satisfying add-ons I’ve built were also the simplest. Since HipChat Connect launched in beta last November, I’ve been experimenting with new ways to bring tighter integrations with apps like Uber , Facebook and Twitter, into HipChat. Today, I’m sharing how I built my most powerful, yet simplest add-on using the Adobe Creative SDK . One of HipChat Connect’s most interesting capabilities is the ability to action on a message . If you’ve used HipChat before, you’re probably familiar with the fact that you can upload and share images (or links to images) with your colleagues. This is a useful feature if you’re part of a design or product team and often share mockups. It’s also generally useful when you just want to share a photo or meme you found on the web. Let’s be honest, we all share a lot of memes in HipChat. The idea I’ve long wanted to implement in HipChat is the ability to annotate these images then re-share them with the team. This is useful when you want to convey an idea with your designer or just want to point out a bug with your product team. So, I searched the web for an open source library that allowed you to annotate images… and there are lots of them (including one we use within JIRA Capture). However, what I ran into instead was Adobe’s Creative SDK The Adobe Creative SDK exposes a set of APIs to give their users access to creative tools and Adobe’s Creative Cloud platform directly from their own app. It's the engine behind some of the best creative apps on the market and at the heart of Adobe's own mobile offerings. I decided to use Adobe’s Image Editing UI component to build an image editor within HipChat. This Image Editor is not your dad’s paint.exe – it’s a modern image editor that runs entirely in a web browser (or mobile app). While having an Instagram like add-on might not be the most useful thing for your teams, I thought I’d take a crack at implementing it within HipChat anyway. It really is quite amazing what you can build when you have an amazing SDK or API in hand. I’ve spent most of my professional life building products on top of other people’s APIs and SDKs (as many of you have). However, building compelling products usually aren’t that trivial. Only once in a while are we reminded that simplicity in design and architecture and clear intents are what lead to purposeful and useful products. In this case, the combination of HipChat’s message action API and Adobe’s Creative SDK was a perfect pairing. Let’s start with the HipChat message action capability : The snippet above is the message action capability declared in the HipChat Connect descriptor. This describes the action and where that action should be enabled. In this case, we use conditions to make sure that the “Edit this image” action only shows up on messages with actual images in them. That’s really it on the add-on capabilities side of things. The rest of the code needed is inside the add-on dialog itself. Adobe’s Creative SDK for the web is pretty simple to implement Most of it can really be done within a few dozen lines of client-side code. Here’s the JavaScript code that runs in the add-on dialog itself: Adobe requires you to register your app and use the token they create for you. This token is used when you initialize the editor but isn’t used for anything else other than tracking the usage of the app. The rest of the code needed is the code that bridges the action, launching and dismissing the editor, and passing the edited message back to HipChat’s chat input component. Here’s the magic… this add-on is a static add-on. It doesn’t require an add-on server and can be hosted on any static web host. So, the best part of this was the fact that I didn’t have to set up hosting, worry about a database, and figure out how to keep this service working. Instead, I decided to push my code to Bitbucket then use a Bitbucket add-on called Aerobatic . Aerobatic is a simple static web host that deploys your code to Amazon’s Cloudfront (CDN) and it does this every time you push your code to Bitbucket. It even supports automated builds using NPM so you can use webpack and the like to package your scripts. This makes deploying your add-on simple a git push to Bitbucket. If you need to host any static sites, I highly recommend Aerobatic. It’s a great service coupled with Bitbucket. It even allows you to proxy API calls so you can make Ajax calls without cross-domain restrictions. I’ve only touched the surface with Adobe’s Creative SDK. Next, I’d like to implement Adobe’s Creative Cloud Asset Browser . This will allow you to access all your assets from the Creative Cloud so you can easily share them with your team. Stay tuned for this. In the meantime, feel free to try out the Image Editor . I’m always on the lookout for the easiest and most useful add-ons to build. I’m not sure I’ll find one to top this any time soon, but I encourage you all to do it. Here’s a challenge, maybe one of you can build this same add-on on top of Bitbucket, JIRA, and Confluence. Any takers?", "date": "2016-04-07"},
{"website": "Atlassian", "title": "Git LFS 1.2: Clone Faster", "author": ["Steve Streeting"], "link": "https://blog.developer.atlassian.com/git-lfs-12-clone-faster/", "abstract": "As we announced a little while ago , Atlassian is collaborating with GitHub and many others in the community to contribute to the ongoing development of Git LFS . Together, we’re trying to solve the problems that face teams who need to store large files in their Git repositories, on projects like game development, media production and graphic design. Our own Bitbucket Server and SourceTree provide professional teams with the ultimate Git solutions, including support for Git LFS. Last week, Git LFS reached a new milestone with the release of version 1.2 . Looking at it now I think we’re all surprised how many things we squeezed in! I thought I’d take a moment in this blog post to highlight one of the features of this release, but if you’re using Git LFS I recommend taking a look at the full release notes (in which I’m ‘ @sinbad ‘ 🙂 ) . A new feature you definitely want to take advantage of if you have a very large repository, and especially if you’re on Windows, is the specialized LFS clone command: The git lfs clone command operates exactly like git clone and takes all the same arguments, but has one important difference: it’s a lot faster! Depending on the number of files you have it can be more than 10x faster in fact. The difference is all about how exactly LFS content is detected and downloaded when you clone a repository. In Git LFS, large file content is stored separately from the Git repository, and only small pointers are committed in their place. In order to give you a valid working copy to edit, some process has to turn those pointers into real file content, downloading it if necessary. This task is usually done by something called the smudge filter , which is called whenever you check something out into your working copy. So under normal circumstances when you use git clone , firstly git downloads the repository objects, which in LFS terms are just pointer data. Once that’s done, the default branch is checked out to create your starting working copy, and the smudge filter is called for any LFS files. This then downloads any content it doesn’t have on demand. So far, so good. The problem is, Git calls the smudge filter separately for every LFS file it encounters, and unfortunately this doesn’t perform very well, for several reasons: We tried out a number of approaches, and the eventual solution we settled on for 1.2 was very much a group effort ; it even resulted in a patch to Git core which makes the feature even faster when used with the recently released Git 2.8. The new git lfs clone command first calls git clone , but it disables the LFS smudge filter using Git’s -c option; like this: Depending on the version of Git you’re using, this command will be slightly different to cope with the way Git behaviour has changed. Git 2.8+ is the most optimal case. This means that when it comes to the checkout part of the clone operation and files which are tracked in Git LFS are created in the working copy, the smudge filter is not invoked and they are initially written to disk exactly as Git sees them; just pointer files that look something like this: Once the clone is complete, git lfs clone moves on to the second step, which is to perform the same tasks as git lfs pull to download and populate your working copy with real LFS content. This is in itself a compound operation which does the following: In Git LFS the concept of fetching is the same as core Git; downloading the content you need but without changing the working copy. Because this is done in bulk when using git lfs clone , fewer calls are made to the LFS API and multiple files are downloaded in parallel, making this much faster than the per-file smudge approach. Normally the fetch operation will only download LFS content you need for your current checkout. However if you have configured Git LFS to fetch ‘recent’ content, this will also download other content to make it faster to switch to recent branches or commits subsequently. Run git lfs help fetch and see the “Recent changes” section for more information. In future we’ll look at whether we can gain similar performance benefits in scenarios other than clone, where reliance on the smudge filter could slow things down, such as pulling Git changes and switching branches. Clone is by far the most expensive case though so it made sense to tackle this one first. We’ll be updating SourceTree to take advantage of this faster clone route soon too, so you won’t have to do anything special to benefit from it. I hope you’ve found this interesting! I highly recommend you check out the new release of Git LFS , and we on the team look forward to bringing you even more useful features in future.", "date": "2016-04-19"},
{"website": "Atlassian", "title": "Clean up the leak!", "author": ["Peter Koczan"], "link": "https://blog.developer.atlassian.com/clean-up-the-leak/", "abstract": "Even with the most sophisticated and thorough QA process, bugs will make it to your production systems. Even the most experienced developers will be at times confident about a certain release, only to then face the reality of a production environment, where things just don’t work out the same way as they did during testing. This does not rate the quality of your code, it’s a part of life – your customers and users can always surprise you. What’s important is how you react to such incidents. You may have very few bugs in your releases, but if the mitigation of these incidents is not satisfactory, then the user experience – which grades your work at the end of the day – will dip rapidly. This post does not aim to provide a swiss-knife (or swiss army knife) for every scenario, but addresses a class of problems that is far more frequent than it should be. I mean the infamous and regular problem known to Java application administrators as the out of memory error. It is not news to any experienced application administrator or to a developer, that one needs to identify what is causing extensive memory usage. It may be a bug in the software, it may be a problem on how a certain functionality is used – but regardless of the cause, you will want to see the evidence, the footprints left during and after the leak . Neither are the methods new or complicated; you will capture and analyze garbage collection logs and ideally a heap dump, to not only see how the problem happened, but also what exactly may have caused it. The complication comes, when you only face such an issue on a production environment and you are yet to realize that the required diagnostic parameters are not in place – you do not have data to analyze. Specifically, if you did not enable garbage collection logging and automatic heap dump generation, all you see are the user complaints and possibly some vague log messages. Enabling these parameters will require a restart of the application, which then will result in even more unhappy users – far from ideal. So how about we avoid this additional restart and enable the required parameters runtime? You didn’t think it was possible? Neither did I, but then I was introduced to a simple little utility called jinfo that is bundled with the JDK! Putting it as simple as possible; there are a number of JVM arguments that can be changed runtime . Luckily enough, these are the ones related to garbage collection logging and heap dump generation. Use the following command to list all the parameters that you can alter: How does it work? For the long version, visit the related documentation , but in short you just use the following format (this example will set the PrintGCDetails option to true): Saving yourself a restart in business critical environments is desirable, but make sure not to rely on the jinfo tool as a part of automated processes, since Oracle marked this utility as not supported and it may or may not be a part of future JDK releases see the related tech note for more information . The settings applied/altered by jinfo are not persistent , neither will be visible on the output of ps . The effective values of manageable parameters can be queried using the jinfo tool itself. Put this handy little utility to your toolbelt and remember to use it, when you’re in trouble, save yourself (and your users) a restart – modify the diagnostic JVM arguments whenever it is needed! Need to share this information? Use our knowledge base article: How to change JVM arguments at runtime to avoid application restart", "date": "2016-04-29"},
{"website": "Atlassian", "title": "Tip of the Week: To fail to plan is to plan to fail", "author": ["John Garcia"], "link": "https://blog.developer.atlassian.com/totw-to-fail-to-plan-is-to-plan-to-fail/", "abstract": "This week’s tip also comes to us from Wittified’s Daniel Wester , who shares his strategies for trouble-free Bamboo test plan creation. A build plan should be repeatable in order to help other developers (and yourself) to debug any build issues. – Daniel Wester Before setting your work plan in Bamboo , spend some time considering what you’d like it to do specifically. This means talking to the team to discover or set shared expectations and ensure repeatability of the plan. At Atlassian, we use Confluence frequently to discuss and diagram our build plans. if you can script the build plan on the command line it will let you easily run it locally – Daniel Wester Builds that work from the command line speed prototyping and testing by allowing the developer to close the loop through local testing. This speeds up the debugging process as well. While there are certain “magical” features in Bamboo, if you can give your developers functioning test scripts you can prepare them to spot their own bugs before Bamboo does. Let us know what you think in the comments! Watch out for news and info from our @AtlassianDev Twitter feed! Follow me at @bitbucketeer !", "date": "2015-06-15"},
{"website": "Atlassian", "title": "Atlassian joins Open API Initiative, open sources RADAR doc generator", "author": ["Erik van Zijst"], "link": "https://blog.developer.atlassian.com/open-api-initiative/", "abstract": "Atlassian was founded by developers building tools to cater to other developers with whom we’ve always had a close relationship. Very early on we opened our products up with a plugin system allowing others to build upon and extend them, which over time has grown into a large, thriving developer ecosystem that we’re very proud of. Opening a product to external developers requires well-designed, stable APIs that are thoroughly documented. The first plugins were JAR files directly injected into our Java products. To help developers debug, we opened up the source code and to this day all product licenses come with the full source code. External integrations have been supported through XML-RPC, SOAP and eventually REST. As cloud services gained momentum we built Atlassian Connect , an entirely web-based add-on system for cloud services, open to external developers. APIs, and in particular REST, form the backbone of Atlassian’s developer ecosystem which in turn adds substantial value to our products. REST gained popularity in part because of its simplicity, debuggability and ease of use, but its lack of a commonly accepted interface description standard has sometimes lead to inconsistent APIs and hampered code generation efforts for rapid client development across languages. Over time, efforts like the Open API Initiative have emerged to address these limitations by defining a schema standard by which to describe REST APIs. As a company we’re fostering an ecosystem whose success is related to the quality and usability of its REST APIs. This means we and our external developers have a lot to gain from an open, successful and widely-accepted definition language for REST APIs. We’ve committed ourselves to actively contributing to the standard by becoming an OAI and Linux Foundation member organization, alongside industry leaders like Google, Microsoft, PayPal and others. We’re really excited to have Atlassian on board. For a technology standard to be successful and see broad adoption, it needs to address the industry’s issues and involving its key players I think is a condition for achieving this. Atlassian’s focus on interoperability and long history of successfully opening up its products through APIs is a huge boon for us and will contribute to an even stronger specification. Tony Tam, founder of Swagger, the foundation for the Open API Specification One of the things the Open API Specification facilitates is the ability to have API documentation automatically generated. As we add Open API support to our products, we’ll use it to replace our existing, hand-crafted API documentation. For this we’ve built a custom site generator, RADAR, for Open API specs to host our API documentation. Built on React, it offers searching, browsing and viewing REST documentation for any product and any version of that product. RADAR is a straight implementation of the current version of the Open API specification, not tied to any of our own products. Because of this, RADAR can be used by any Open API provider. Hosted by the Linux Foundation, the Open API members do great work both promoting and actively developing vendor-neutral open source tooling around the specification. With RADAR we’re following this model and so we are making it available to the community under the Apache 2 license. Like so many others in our industry, Atlassian relies on a tremendous amount of open source software. In return, it has traditionally made many of its products freely available to our community and develops core parts of its products like its plugin infrastructure as open source. We’re thrilled when the industry acknowledges the important role of open source in ways like this, and takes responsibility by contributing back. As such, we’re very happy to work with Atlassian as a new member organization of The Linux Foundation. Mike Woster, chief operating officer, The Linux Foundation We are committed to continuing its development and are keen to welcome external contributors. The project is hosted at: https://bitbucket.org/atlassian/radar", "date": "2016-05-24"},
{"website": "Atlassian", "title": "Tip of the Week: Set up global settings for Maven", "author": ["John Garcia"], "link": "https://blog.developer.atlassian.com/totw-custom-settings-for-maven/", "abstract": "This week, we’re highlighting a tip from Jim Bethancourt of Triple Point Technologies about configuring Maven in your Bamboo remote agents. If you use multiple agents on the same machine, this neat trick will ensure the agents have a consistent configuration each time out. Do you have multiple agents on a single machine and use a custom settings.xml in your Maven builds? Make things easier on yourself! – Jim Bethancourt When Maven starts, it looks for a generic set of settings in either %HOME%.m2 (Windows) or $HOME/.m2 (Linux). To make sure multiple agents and build plans start with the same Maven settings, make sure you have configured this file appropriately for your test environment. More info about the shape and features of this file can be found here: Maven Settings . Let us know what you think in the comments! Watch out for news and info from our @AtlassianDev Twitter feed! Follow me at @bitbucketeer !", "date": "2015-06-02"},
{"website": "Atlassian", "title": "We are discontinuing the support for Atlassian IDE Connectors", "author": ["Bartek Gatz"], "link": "https://blog.developer.atlassian.com/discontinuing-ide-connectors-support/", "abstract": "Four years in software industry is like two lifespans in human years. The evolution of Atlassian products is so fast that looking back at the product releases from four years ago is like watching the black and white photos of our grand parents when they were young. Four years ago Jens Schumacher, the Group Product Manager of Development Tools at Atlassian, wrote a blog post where he explained our motivation behind starting the open source Atlassian IDE Connector project. We had a goal of delivering a faster and more convenient way to interact with Atlassian applications. Since we started that effort, a lot has changed. The web and our products have evolved, and as a consequence, IDE Connectors are duplicating functionality that is already available today in Atlassian products. Over the last several years, we have performed a full redesign of all Atlassian products’ UI to match the Atlassian Design Guidelines . We made the flows more consistent, and we have tied many of our products much closer together. At the same time we have invested a huge effort into product APIs, and have created a whole new add-on platform to support all those cases where we could not provide functionality within the products directly, or users desired integration with external systems. There is still a lot of work ahead of us to make our products and respective platforms stronger. For that reason we have decided to end the support of IDE connectors . The product will remain available as open source , and we will continue to host it on Bitbucket. Atlassian will not release any new versions of IDE connectors, the current release is the last. We have also discontinued the customer support and Atlassian Answers effort related to IDE Connectors. Specifically: These changes are effective immediately. You will still be able to download these versions of the Connectors, but Atlassian will not maintain support for them going forward. The good news is that Atlassian IDE Connectors are open source , and we have just made it a whole lot easier for you to contribute by hosting the Atlassian IDE Connectors on BitBucket. Please feel free to fork  the VisualStudio , Eclipse or IDEA repository and contribute to the project. We are also making the documentation for the connectors open source. You can access them here: http://atlassian-docs.bitbucket.org/ Please feel free to get in touch with us. You can email me directly under: bgatz at atlassian dot com.", "date": "2015-06-01"},
{"website": "Atlassian", "title": "Fresh Stashberry Pi for Pi Day", "author": ["John Garcia"], "link": "https://blog.developer.atlassian.com/stashberry-pi/", "abstract": "In the US offices of Atlassian, we’re celebrating Pi Day tomorrow, 3/14/15, by installing Atlassian Stash onto a Raspberry Pi 2 . Like many engineers with a background in embedded systems, I am gobsmacked to hear that the new Raspberry Pi 2 provides a full gig of RAM and a proper quad-core processor for USD 35, and I could hardly wait to try it out with a commercial Java app! With a slim five watt power requirement and solid state construction, it’s an incredible value. If you are ever called to send DVCS on a rocketship to the moon, you will be hard pressed to find a more suitable candidate. It turns out this is actually a straightforward procedure, as the Pi’s Raspbian operating system comes with genuine Oracle Java JDK 8 . This common Java core is used throughout all of our BTF products, so within reasonable parameters you would expect to be able to do something similar to this with everything we publish from JIRA to Bamboo. There are really just two major caveats; the binary installers won’t work because they install IA64 or x86 compatible Java instead of the required armhf, and as the system has just 1GiB of ram, we need to be careful about the way we allocate our memory. As the install is pretty simple, I’ll refer to the docs where appropriate and merely call out specific changes. Don’t worry about a keyboard, video, or mouse. For this procedure, you can just plug it into your router with Ethernet and power it up. I want to be sure to point out that Atlassian does not officially support this install scenario, nor recommend it for use in a production scenario. Here’s some performance test results to show that it’s not a total waste of time. If you have a spare Pi 2 laying around and need a household DVCS server, this is a great way to go about it. For those keeping score at home, we used 50 users ramped over ten seconds with about 40 reqs each. Follow me on Twitter for updates about Atlassian software, robotics, Maker art, and software philosophy at @bitbucketeer .", "date": "2015-03-14"},
{"website": "Atlassian", "title": "DevDen Open Office Hours Transcript 27 Jan", "author": ["John Garcia"], "link": "https://blog.developer.atlassian.com/dev-den-open-office-hours-27-jan/", "abstract": "In this installment of our video podcast DevDen Open Office Hours, we discuss our migration to a new blogging platform and its implications at merge time, the Better Pull Request blog, some advice for handling conflicts for newbies to Git, and other exciting topics. Follow the DevDen on Google+! Grace Francisco: Hello and welcome to our next episode of the Dev Den Hangout. I’m Grace Francisco, one of your hosts for today. I am the head of developer advocacy at Atlassian. I have with me a team of experts today. I have Ian Buchanan out of Munich. Ian Buchanan: Hello. Grace: There is Ian, and Nicola Paolucci in Amsterdam. Nicola Paolucci: Hello there. Grace: Then Steve Smith, also in Amsterdam. Steve Smith: Hello. Grace: Then Tim Pettersen out of San Francisco. Tim Pettersen: Hey, folks. Grace: Today, we’ve got a whole bunch of great topics to cover, starting with Nicola. Nicola, tell us about the new blogging process that we’re following. Nicola: I find this very interesting to talk about and tell, and share this really cool thing we’re doing. Which is, very simply, we have restructured and adopted a different model, different process to publish our blog. We actually have transitioned all our technical content, and the intention of writing technical content in the future, to a new blog at developer.atlassian.com/blog that has a completely new technology stack and is much more developer-friendly than our previous process. Before we have, and we still do have a quite sizable and very performing, very advanced WordPress deployment for our corporate blog, which is blogs.atlassian.com . We wanted something a bit more tailored at developers, and where we can share our own hacks and our own findings in the tech sphere. We created our own basically, and what we picked was… we use a static site generator, which is a fancy thing to do nowadays. It is a very cool way where you can have a super performing blog system that shares only static files. The one that we chose after a review as being based on nodeJS , and it’s called lineman . Obviously, to store actually the sources of our articles, we use Bitbucket . We’re big fans of simple static text files for our articles, so we use markdown. The only thing that we needed to track some dynamic content and interaction with our readers is a way to track comments. For those, we use Disqus . More than the tech stack, what they wanted to study is our process. Obviously we use Git and pull requests to manage and publish our blog. The way we work is this. Any branch that gets merged to master goes directly to production. Each branch might contain one article. The next stage of our blog, so the next post or the next few posts that’s going to go live are branch off develop which is, let’s say, work in progress branch. When I want to write a new article, I’ll create a new branch off develop. I name it with a special prefix which is blog so blog/ then the topic or the article name. There, I write my own markdown files. When I’m ready to share this work in progress with my colleagues, I open pull request. We have a review that can happen very nicely on Bitbucket. We have a live preview of what’s happening on our staging site. We talked more about this a bit later. After the process works out and we fixed what we needed to fix and everybody is happy about it, we generally want to add at least two approvals for each article. Then, we go through also a final review at the end. After that, then we merge it to master and then we go live. That’s very roughly, very quickly the process. One interesting bit that came up in developing this solution was how we show on staging more than just one article at a time. Initially, we were like everything that was merged to develop was going to staging. Then, sometimes you want to change the data thing so you need to rework and move off branches, one article that’s going to go live. We actually came up with a very clever way to do it. Tim wrote an article about it. Maybe, Tim, you want to explain a little bit this advance technique that we use for our staging server? I think people will find it very interesting. Tim: Yeah. For sure, Nicola. As Nicola was saying, we have this amazing process now for staging our blogs and getting into production. The only problem is, because we built up this awesome process, all of the other developers at Atlassian are very keen to start working as well because it beats the hell out of WordPress, which is what we’re using before. One of the problems of having this increased uptake in office is that it means that multiple people developing their own blogs and trying to get them onto the staging server at the same time. This meant that we’ve had different individual feature branches as Nicola mentioned. Each blog is being developed on its own branch trying to be staged at the same time and we’re basically stomping each other’s changes. In fact, I can probably give you a little demo to illustrate that. I’m just going to try and share my screen here. Can you see my terminal there? Grace: Yup. Tim: Cool. Fantastic. What I’m going to do is run Lineman, which is the little static site generator that Nicola was talking about. We can actually see a local copy of our blog actually. That probably means I’m probably going to need to share more than just my terminal so give me one moment. I’ll share the entire screen. There we go. The lineman’s pretty quick to run up and we’re just running a little local HTTP server, which is going to be serving our static content. And we’re currently running from a branch called “A Better Pull Request,” which is a blog that was published last Thursday. If I go to my browser and hit http://localhost:8000 you can see our developer side. And if I go to the latest blog posts, you can see the latest blog post is “A Better Pull Request,” because we’re running from this branch, which was actually made to develop last Thursday, but it’ll do well enough for these demonstrative purposes. But if I change to a different branch…whoops. There are a few too many branches. Let’s check out “Getting to Know io.js,” which was another blog that was published last Friday and then refresh our page. You’ll see that the latest blog post is “Getting to Know io.js.” But that other blog post that we were just looking at, “A Better Pull Request,” has disappeared. If we flick back to “A Better Pull Request” again…I’m going to use this cool little command, git checkout - If you haven’t seen this before, it’s pretty cool. It’ll go back to the last branch that you were working on. So it’s a nice way to quickly switch contexts if you need to be working on a different branch. But if I go back and refresh, you’ll see that “A Better Pull Request” is back at the top of the list. This is effectively what was happening on our staging server, because basically we’d have multiple people developing the same branch, developing their articles, pushing them to the staging server. Then our continuous integration and continuous deployment job would be deploying that latest branch on top of it. What we did is we built a little tool that solves that problem. What we did is we considered the different solutions. There’s a few obvious ones. The first is that we could have one staging server per branch. The problem is with that is it could get relatively expensive, because if you have 10 different branches on the go at the same time, you’re going to end up with 10 different staging servers. The second solution that I came up with is I would just continually re-base my blog branch and keep pushing that to the server so my article is always deployed and stomping over the other authors. That, obviously, is not a great idea in a team context [laughs], and directly violates one of our main values, which is “Play as a team.” The third solution we came up with was “Perhaps there’s a better way we could solve this using Git.” Now, Git has a bunch of different merge strategies. One of them is a very special merge strategy called the Octopus Merge that allows you to merge more than two merge heads together. So we had this theory that we could potentially merge all of the different articles that were ready to be published or ready to be reviewed into the same hedge and then just deploy that merge result to the server instead of just deploying a single branch head. I might just share my screen again and I can give you a little example of what that looks like. Sorry, one moment. You can see my terminal again? Grace: Yes. Tim: I’ve got a little empty repository. It’s just got one commit in it. And it’s got this little Octopus-building script. Basically what this is going to do is generate eight different branches, each with three different commits on it. So if I do build-octopus.sh , you can see that’s generated a whole bunch of different commits. We now have eight leg branches, obviously, because it’s an Octopus Merge. The way that we do an Octopus Merge is simply running git merge and then typing in the names of the branches that we want to merge. So, it’s just like a regular merge. You just give it more than the additional head. Bear with me for one second. And the other thing I’m going to do is use the merge strategy. We’re going to create a new merge commit. This will be the body of the octopus, if you will. That’s created a merge commit with eight different parents, and just to illustrate that I’ll run git log , and you can see we have this quite attractive-looking Octopus thing going on with our merge commit at the top, and then every single branch being merged into that. This is effectively what we wanted to do with our blog site, because we wanted to have of the different commits that people had been independently developing and then merge them all together. The only thing, there were a couple of special constraints that meant we couldn’t use an Octopus Merge per se. The first kind of constraint on us was that this merge, because it’s going to be running on a Bamboo server, so basically running non-interactively, it can never, ever fail with conflicts. Because if it fails with conflicts, that means that it’s not going to be able to deploy, because it will have this conflicted working copy, and that’s not something that’s appropriate to deploy to a staging server. That basically meant that our octopus merge would have to be run in such a way that it would never conflict. The second thing that we needed to do is to make sure that the changes that end up in the deployment are only static changes, so actual blog content. Because that repository contains all of the actual site itself, so, all the logic for building that kind of attractive front page — if any of that stuff changes, we don’t want to automatically merge that, because there’s a chance to get logical conflicts. We only actually want to deploy individual article content. The third thing that we wanted to do is allow you to not stage your content if you didn’t feel like it was ready for review. So, we didn’t want to just be taking every single branch in the repository, bundling it all together, and then pushing it up to the server, because, potentially, there’s some sensitive content, or, potentially, you’re not ready for your things to be reviewed just yet. What that meant is we built, instead of just using up the first merge and putting that into our Bamboo script, we decided to build a new little tool called git merge-distinct . git merge-distinct is, basically, a wrapper around an octopus merge. I can probably just give you another quick demonstration on how that works. We just changed the screen once again. Do you see my terminal again? Grace: Yep. Tim: Cool. OK. I’m going to pop back to that command line that we were looking at before. I’m going to make my terminal a little bit bigger here. OK. I’m going to git checkout develop . Just a quick look at branches we’ve got here. We’ve got develop and then we’ve got two outstanding blogs, those two we were looking at before — a-better-pull-request and getting-to-know-iojs . What I’m going to do is run git merge-distinct . The first thing I’m going to do is, say that we only want to include branches that have changed paths underneath app/posts . What this is going to do is it’s only going to allow branches to be merged into this deployment artifact that have content that’s changed under app/posts . If the author of that branch just changed some content outside of that, so, possibly, some dynamic content, which is used as part of the core functionality of the site. Then that would be included in the actual branch that’s going to be deployed. The second thing I’m going to do is specify an additional qualifier specifying which branches I want to be merged. In this case, it’s going to be anything names based under blog/* . Basically, this means that only these two blog/ articles are going to be deployed. This other branch, series/continuous-integration , which, because it doesn’t start with “blog”, is going to be consider private, and not something which you we want to deploy. When we run this command — fingers crossed — excellent! It comes out with the output Merged 3 parents . Basically, it’s merged develop , which is the branch we were on originally, and then these to blogs, which is exactly what we were hoping for. Now, because our main server is running in the background, if we just pop back over to our local host and hit “Refresh”, you see we’ve got both “Getting to know io.js” and “A better pull request” deployed at the same time, which was the intent of the exercise. Nicola: Way to go. Tim: Cool. [laughter] Grace: Great. So, for the folks who were on the line right now — we’ve got quite a number of viewers — feel free to use that Q&A to ask us questions at any time. On to Ian Buchanan, who’s at OOP 2015, in Munich. Tell us, Ian, how are things going out there. Ian: Sorry for that, couldn’t find the mute button. Yes. Well, there’s [laughs] enough snow here to make me sympathetic to those in New England. We’re seeing some snow here too. Probably not as much. We’re looking for to having Sven talking about Atlassian coding culture tomorrow, and I’ll be talking about the business value of git on Thursday. So, if you’re in the area, pop by and see us. I spent the Monday tutorial building a domain-specific modeling language. That was really fun. This morning, I’ve listened to a talk about agile fluency and just saw a talk about sociocracy, both very interesting things. Coming up are some additional talks about automated testing, continuous integration, continuous deployment, and all things agile. In any case, if you’re interested in any of those topics, I’ll be blogging about some of those quite soon. At the booth, I’ve been having some fun discussions about how Bamboo is better than Jenkins, or how Bamboo can be used with Android, both some things I’ve been looking into lately. Anyway, if you’re listening from Munich, drop by the booth and have a chat. That’s pretty much what’s going on here. Grace: What’s the most popular question you’re getting right now, over there? Ian: It’s really a class of questions about Bamboo. I think that’s mostly because other folks at the booth feel like Bamboo’s very technical and they’re, “OK, all those questions go to Ian.” Grace: [laughs] Ian: But among them are things like what Bamboo is, how it works, and where it fits into the develop lifecycle. Mostly, I think, if people get interested in it, they know what continuous integration is but are just really looking for product differences. Grace: Super. Well, make sure to tackle Ian with your toughest questions at the OOP Conference, if you are there. [laughs] Thanks, Ian. Steve, you’re going to be coming out to San Francisco soon for DeveloperWeek. Tell us about that. Steve: Yeah. Hi, everyone. I’m going to be at DeveloperWeek in about two weeks’ time. What I’m going to be giving is a tutorial/workshop on Docker. The background to this came about when Nicola, who you maybe will guess is sitting next to me, has been giving all the talks about Docker for developers. But I’ve noticed this when talking to developers, there’s still a lot of confusion about exactly what Docker is and how you’d use it and why you would even want to use it, especially if you’re a non-operations person. So what I’m in the process of putting together, and I’m actually testing these out with Nicola and some of the developers here in Amsterdam today, was a bit of a tutorial about the background Docker, about how it fits together, some of the end-of-line technologies about it, and then an example project that will use Docker to do some testing. The project I’m going to be testing has some complexities around testing, which is to do with we have to use real Postgres, some real Elasticsearch, to do some real-time testing. What we do is we use all the technologies and the concepts we learn in the Docker background session to build up a set of Docker containers that we can use to test our tricky little application, and then we go through some more stages of simplifying it and using some of the higher-level tools like Fig to simplify it and eventually integrate it into a continuous deployment pipeline using Bamboo. It’s an hour session. There’s quite a lot had been packed in there. Hopefully, we’ll get through all of it. However, I will be hanging around the entire week of Developer Week. Hopefully, I’ll be able to have a lot of conversations with anyone who’s really interested in learning a little bit more about this. This is exciting stuff. It’s very new area and it’s changing rapidly. I’m trying to, hopefully, introduce some people to some of the core concepts to help them sort of reason through the rapids early changing landscape of the Docker and container ecosystem. There’s also going to be Codeship with this. Code is already out there. I think, Grace, if you could probably paste the link to it which should be on our confluence page. There’s code that you could sort of follow along with or if you just want to follow along afterwards or use the slide as a workbook, then hopefully that will be of use as well. That’s going to be on the ninth of February, I believe, on Monday. If you’re at Developer Week, come along. I’ll come along to session. Just come and see me and we’ll have a chat. Grace: Super. Thank you, Steve. Nicola just posted a link for that. Thank you, Nicola. All right. Moving right along. Tim, you had an interesting blog post on Bitbucket pull request last week. You want to talk about that a bit? Tim: Yeah, for sure. Absolutely. This is a kind of interesting post because it’s a feature that’s actually been in both Bitbucket and Stash since their conception so we’re kind of talking about a few years. We realized recently that people may not realize that it’s actually a feature. What I’m talking about here is how Bitbucket and Stash do their pull request diffs. I might just open the article so I can show you a couple of images from it. You just share my screen. When you think about a diff in terms of Git, quite often you’re just thinking about the changes that have occurred on a particular branch. Say you’re doing a pull request the old way so back before you had Git, Bitbucket, GitHub, GitLab or Stash. You just pull the branch down from a server and you wanted to see what had changed. Now, you might think that using this git diff triple dots syntax to see the changes on that branch is the reasonable way to do it. This does actually show you the changes that occurred on that particular branch. In terms of a pull request, which is actually not just a diff. It’s actually someone’s intent to merge some changes from one branch into another. This doesn’t quite show the full picture. The way we decided to implement pull request diffs in Stash and Bitbucket is to actually show you the difference between the two branch heads. The reason we do that is because if you just look at the changes that have occurred on the branch that you’re merging into your target branch, it’s actually going to be missing any changes that have occurred on that master branch since then. You might be workingndering why this is a problem. I was actually explaining this to my wife when we’re walking the dog yesterday. I was trying to think of a nice approachable way to explain why this diffing algorithm can be problematic or why you need to actually look at the changes on the master branch as well. The analogy I came up with is imagine you have two different developers working on a cook book. Say you’ve got three recipes in that cook book. One of them is a recipe for some pizza dough. Then, that’s already been sensibly completed. Then, these other two, well, chefs are writing recipes that will be contributed elsewhere in the book. Say maybe one is writing recipes for calzone and then the other one is writing a recipe for some pasta. While they’re working on their individual recipes and they’re working on individual branches because they’re using branch base development, one of the chefs realizes that the pizza dough recipe is missing salt in it and that’s something obviously you have to have in your pizza dough. They go and update the pizza dough recipe and say, “Look. You need two tablespoons of Himalayan rock salt.” Then, they commit that on their feature branch which contains this other recipe. At the same time, this other developer…Sorry. This other chef — I’ll use the term chef — is working on their own recipe. They also realize that that recipe is missing salt. On their separate feature branch, they make a similar change where they add two tablespoons of sea salt to the recipe. On each of their individual branches, the recipe is now complete. The recipe for pizza dough is now complete at least. They’ve made slightly different changes. One of them said, “We need Himalayan rock salt.” Then, another one said, “Hey, we need sea salt.” Now, the problem comes when you merge these branches together. Because when they complete their own individual recipe and merge that back into master, it’s going to add two tablespoons of salt to this pizza dough recipe. The second chef also merges their own recipe into master. It’s going to add an additional two tablespoons of salt to this pizza dough recipe. You’re going to end up with four tablespoons of salt and a super salty pizza recipe. That’s why it’s important to share the changes on master as part of the diff. The reason being, that when one of the chefs create…Well, when the second chef creates a pull request from their recipe branch back into master, they’re going to see in the diff that there’s already two tablespoons of salt there. Now, it does seem kind of a maybe drawn out in that analogy. I highly recommend reading the article if you’re curious about it. I’ll just post that into the blog, sorry, into the chat window. There is another reason why we show this type of diff as well. The way that we calculate this diff is actually by creating a fake merge commit in the background. There isn’t any simple command you can run in git to see this kind of diff that shows you all of the changes on both branches. What we actually need to do is every time you update your feature branch or you update your master branch, both Stash and Bitbucket create a merge commit behind the scenes. Then, they show you the difference between master and that merge commit, which is basically illustrating all of the changes that are going to happen on your master branch when you hit that merge button in the pull request. It gives you kind of the most accurate diff you can have because it’s really showing you how your branch merge is going to affect master and affect the code that you’re intending to ship your customers. That’s really where we feel it’s the best diff you can have. The fact that we create this merge commit also gives us another advantage because it means we can detect ahead of time if your branch is going to conflict because obviously that merge commit is going to conflict. This is really handy because it means that the way we handle this is we actually commit all of those conflict markers that get generated by the Git merge command into that merge commit. That means that when we show you the diff to the merge commit, we can actually show you if your pull request is going to conflict. That means that you can discuss with the other developers on the pull request how you’re going to resolve these merge conflicts. Since merge conflicts by its very nature is obviously going to involve more than one developer, we think that the pull request is the best place to discuss how you can resolve these merge conflicts. That’s basically it. As I mentioned before, it’s a feature that’s been there since the beginning of time. Massive kudos to the Bitbucket and Stash teams for coming up with this merge algorithm. All I really did was kind of show people how we built something kind of awesome. [chuckles] Grace: Your blog post created quite a bit quite of discussion on here. Nicola, you made some observation spots about some of those comments and discussions. Nicola: On a few technical forums and one of these post sparked quite a heated debate. It’s one of those debates that have occurring themes. It was very interesting to chime in, chat and discuss with other teams and other developers how they’re dealing with this sort of thing. A few of the comments that I saw and what is interesting to share with anyone that were recurring. Then maybe we can add our own opinion too and then also say what we think about this. For example, one comment I saw quite a few times throughout this conversations, for example, was, “This is all very nice. In fact, we don’t really notice this problem very much because we recommend to our developers always rebase their feature branches before they open a pull request. Then, this problem generally doesn’t happen.” This make sense. I think it’s a very fair assessment if your team is relatively small or maybe your code base doesn’t work very fast. It’s absolutely true. If you’re just working and you know what the other guys are working on, there’s only a handful of branches that working on it at a time, that works perfectly. It might even work perfectly as the team grows, when you scale this off to bigger projects where there might be five branches every couple of hours popping up and needs to be merge as the tests are complete. You reach a threshold where you merge even enough to see the real state of your branch whenever you would merge it. It’s a matter of scale. Bitbucket and Stash do, they give you a very complete view of what’s going to happen if you merge the work of your team’s size. You can work around it and a simpler model. It’s just slightly more advanced, more complete solution that you get. That was one of my comments. The other comment that I saw quite a bit. There is this ongoing debate about how to handle the merge strategies, pull request merge strategies. That is, “Yes, but we don’t like these merges. We actually like it to rebase the branches and then squash all the feature branches until we only have one and we’ll merge that one. That keeps the history of the project linear, very elegant and clean.” This debate actually has been ongoing for years. There’s pros and cons to different approaches. We personally prefer to have a complete view that preserves the context of a feature branch. We’re trying to encourage our developers to do explicit merges most of the times. I can understand in a way that elegance of having a very clean linear history. Some big teams do that too. I actually wrote an article about this a while ago. It’s called “Pull Request Merge Strategies” If you go to the website, you’ll be able to find it. I don’t know if you guys have any other comments on the debate that was sparked on… [crosstalk] Tim: …It’s fascinating to see that level of sophistication of the way that people merge things. There were three or four different kinds of…”Hey, we used merge. We used to do this. We always rebase before we actually pull request. Then, sometimes, we rebase after we upgraded the pull request before merging.” It’s really interesting to see the plethora of different merge strategies that people fly out there. I noticed that there’s a question that just popped up in the little Q&A section. Which version of Stash supports the better pull request? Actually it’s been this since the very first version of pull request with since Stash which, I think, was Stash 2.0 from memory. If you’re using pull request from Stash then you already have this diff, you don’t need to upgrade. Grace: There’s another question as well, Tim. Basically, should you only use the octopus strategy if you want to do a merge where you know that you won’t have any tools? Tim: That’s right. I should have mentioned before. If you want to give it a try, the little Git merge distinct tool that I posted before, it’s actually available via NPM. If you have got node installed locally in NPM, you can install it and test it out locally. In terms of whether you should use it when you’ve got conflicts or not, really the git merge-distinct adds a sort of layer on top of the octopus merge that allows you to find which branches you want to merge. You can still use an octopus merge strategy locally even if you expect conflicts and then resolve them locally. It has no problems with that. It’s just if you’re running it in a continuous integration environment, that’s where you got to want to avoid merge conflicts. The way that we do that at Atlassian with our developer blog is ensuring that only branches with distinct paths have been changed. That’s built into git merge-distinct as well. In fact, that’s something I actually mentioned before. That’s where it actually gets the name. If it detects the same path has been modified on multiple branches, then it’ll ignore later branches. That means that, basically, the only changes to one particular file on one particular branch will be merged into that bundle. Look, I just posted a link to the blog if you want to check it out and give it a try. I think that really with git merge-distinct it’s only really useful for deploying changes to types of content that can’t really logically conflict. It works well for our blog because it’s static content. It’s, basically, a bunch of marked down files that get rendered into HTML, and they don’t sort of interlink. We wouldn’t use this for something like a Java project where you have different class files calling into each other. Because if you’d change the contract of one of those class files and then one branch and you’d change it again on another one or you change something that depend on it on another branch, whether you merge those things together, then you’re going to have compilation errors. You have to be a little bit careful about when you might use the octopus merge strategy for deployment. Typically, static content is a good way to do that. Nicola: I see there’s another question which I think is very interesting. Maybe we can talk about it. From Sam, it says exactly how to resolve merge conflicts and if there’s any problems that we could recommend on how to do it and how does that work. Maybe we can elaborate a little bit of it and then give some pointers. Merge conflict is relatively common operation and common issues when you’re working with multiple branches and people modifying code at the same time, same lines at the same time. The only thing is you get the conflict is whenever you update changes, you collect and copy changes from other branches like you update the master or you’re trying to improve those changes in your own branches if you’re rebasing your own local branch or if you’re trying to merge your feature branch with master. Then, there’s a conflict. What Git does, it basically annotates so that the very basic way that git does is it stops the merge operation or the rebase operation saying, “Hey, there’s a conflict that I’m not able to resolve by myself.” Git is very helpful. It will annotate the files. If you type git status , it will list the files that have changes both in the incoming branch that you’re trying to merge or in your local one. If you open those files, you’ll see those relatively unfamiliar signs that says, “Hey, this is how that specific line looks in your version and that’s how it looks in the other branch.” One way to sort those merge conflicts is to do it manually, which I think to get familiar with how git works is a good place to start and you shouldn’t be scared by those esoteric signs. All those means when you see >> it means OK. That is the version that’s incoming. Then, you’ll see those specific lines look for the incoming changes. The << annotative set of lines are how those specific lines look in your local branch. What you could do is you can just remove those place holders and make the code. Those lines look exactly as you think they should look. If you need to discuss how things would be resolved, you can always contact a colleague. In general, for say, 90 percent of the conflicts, it’s generally easy to understand how things should be merged. Then, the next step which might be confusing for people is you need to tell git that you have resolved the conflict on that file. You do that by adding that file to the index which is to type git add and the file name on which you will have just resolved the conflict. By typing git add and the file name that was conflicting you’re notifying git that you have resolved that specific conflict. You do that operation for all the files that were conflicting. After that, you can just go on and type git commit . The merge operation or the rebase operation that was ongoing will resume. In case of rebase, you have to type git rebase continue so that it resumes. In the case of the merge, you can just type git commit and then the merge will happen. That’s the most manual way to do it. We have an internal discussion about it just at our company. A while ago, people were asking, “How do you guys solve conflict?” The problem with Sam is a problem that a lot of us have. What’s the most efficient way to solve conflict? We collected a few tips and actually we plan to write about it fairly soon on our blogs so stay tuned for that. Very briefly, a few things that came up and might be useful is you can use…If you use IntelliJ IDEA, for example, and your platform is Java, that has a very solid merging capability that works. Some people use an open source tool called Meld. Some people use to just to look at the diffs using SourceTree, which is our native client for Mac and for Windows. Some people do it manually. One thing you can do is go online, look for tutorial on solving Git merges and you’ll get some of the flows or try to use a visual client. As you see, Tim has posted a link to our beloved SourceTree. I don’t know if any of you guys…Steve, you have anything to add. I just wanted to give a brief overview about this because a lot of people have troubles with solving conflicts. Grace: Super! Thanks, Nicola. Ian, I’m going to switch over to you in Munich here for a minute. You recently had a nice lesson around git and the new blogging processes. You want to share some of that with us? Ian: In a way, it’s like this reverse process of the octopus merge. The situation that I had is I started with a branch where I created a lot of related blogs just to kind of get a feel for what might be a series. I try to cut them down into smaller post. I was very disciplined about commits, so I made small changes to one file at a time. By the end, I just wanted to pluck out the series of commits that related to the one that is the most mature, ready to go out the door. How should I do that? In the Getting Git Right tour, we talk about rebase as we’re playing commits on another branch or cherry-pick. Tim and I kind of bounced a couple of ideas around. I tried interactive rebase. One of the drawbacks there is that interactive rebasing didn’t just replay the commits that I was picking onto the target branch. It was actually recreating the branch that I was working on with all the big commits and all of my content. I had to back that out. I use the main blog and rearrange the pointer, something else that we pointed out in the Getting Git Right tour. This is a good reinforcement. Eventually just went back to cherry-picking. I just took kind of a blog post that was out there about how git rebase is defined in terms of git cherry-picking. Plus, some of the additional things that you would have to do with manipulating an intermediate branch just to kind of get the same effect as the rebase. Then, that helped me understand where I was kind of losing the plot about rebase. Turn out all the work in the end with cherry-pick and I was particularly pleased with how easily I was able to do the cherry-picking. That’s about it right there. Grace: Super. Thanks, Ian. We’ve got a few more questions that have come in. Let’s see here. What is the best form of deployment from staging to production? Do you have staging as you first stream and then have production as an upstream? I found to be running to this issue lately. Anyone want to take that one? Tim: In fact the way that we do it in the blog and some other teams at Atlassian where particularly for SaaS projects where we have one staging server and then our production server beyond that is that staging is always a subset of the commits that are on production. We usually do is name our staging branch develop and name our production branch master very similar to git flow. Basically, we promote changes through staging to master. The only thing about this particular way of working is that, basically, any changes that you have on develop should be intended to go through the master. We try to make sure that our features are complete before we merge them in. It’s very rare that we sort of unmerge a branch from develop. Instead, we typically try to roll forward, so if we kind of merge a feature into develop and then there’s issues with it, we’ll create another feature branch and get that fixed on staging before we promote to master and production. The actual mechanism that we use for that is typically where we can continuously deploying from both the develop branch and the master branch. Basically, as soon as you merge something to develop, that will get deployed to your staging server. Then, as soon as you merge develop into master, that will get deployed through your production server automatically using Bamboo. I don’t know if you guys have anything to add to that. Steve: My background on my previous team was on the internal systems team, business platforms team. We do a lot of continuous deployment. One thing we definitely try to do was basically roll out onto staging. Then, once we went through certain human-based QA on staging, whatever was on staging got promoted up to master, so there was no real separate streams of development for staging and for production. What we would do, we’d basically…When we’re ready to go out and what we’ll finally move towards is basically every single branch merged, every single branch, every single feature branch result in a release. It’s called release by feature. It gives you very fine grained releases but you have to be able to do continuous deployment at high availability to be able to pull this off. Every single thing you roll out is single cohesive change not a bunch of unrelated changes. What we do is roll with those out to staging. Once we’re happy with them on staging, they will get promoted directly up to master. There’ll be no separate staging to master branch. When I say master, it starts in production. We basically run the binaries for a complete set of automated tests. Roll them straight out onto staging. Staging would go through human testing. Then, whatever is on staging if it was satisfactory would get moved up to production. Tim: Nice. I just saw there’s a follow up question from Ben who asked the original question saying, “We commonly use cherry-picking to redo changes to a new branch when the original branch was made up by an incorrect branch.” Look. that’s totally fine. Another way to do that is to rebase your branch…Well, I mean, if you branch off master and you want to branch off develop instead, you could rebase your changes with that which effectively a cherry-pick. The other thing you could do is cherry-pick a range of commits. I’m not sure if you’re cherry-picking individually or a range of branching. Cherry-pick actually supports the same range of commits back as the get Git-log or Git-diff commands, so you could actually say git cherry-pick the base of the branch dot dot the tip of the branch. Then, it will actually cherry-pick all those commits over to a new branch. Hope that helps. [chuckles] Nicola: Yes. I saw that. I was about to answer exactly the same question. What I was thinking is that there’s no danger in copying commits around as long as you are the source is not used. In reality, if you reapply the exactly the same code changed twice, what you’ll get is a NOOP. That’s relatively low risk. What is wrong about using cherry-pick is, because you’re randomly reapplying patches here and there, you’ll lose track of where those changes come from. That’s the reason why we try to isolate and then keep work in feature branches, so that it’s easy to track where do changes come from. There are specific scenarios like the ones you mentioned that aren’t simply acceptable. Grace: Nicola, actually while we have you on here, do you want to talk a little bit about your Docker Hub hack? Nicola: Sure. It’s one fun hack I worked on a while ago. Every few months we do a hackathon here. We call it ShipIt Days where the company stops for 24 hours and groups are formed and we work on stuff that’s cool and could help us solve our own problems and improve timing and things at Atlassian. My idea was, “Hey, I’m very excited about Docker stuff. Can I do something relatively quick in 24 hours that can have an interesting impact or interesting use using Docker?” I wanted to do something with Docker using Docker. [chuckles] What I did, maybe I can share the pages I talked. There’s an article about this on the blog. It’s called Hacking the Docker Hub into Bitbucket. Docker has a registry where you can prepackage your applications to run in containers, it’s registry.hub.docker.com . There you can see the major data and the amounts of downloads of a certain container, certain package application has. I thought you can build containers from Bitbucket, but wouldn’t it be nice if it can show actually that information in Bitbucket itself? How many people have downloaded that container if it is packageable as a Docker image? What I did is I asked friends in the Bitcuket team to open an iframe there and I developed a small static website based on nginx that would go to the Docker Hub and collect all those statistics and inject it inside Bitcuket. That’s rolled out into production. I managed to do that all in 24 hours. It’s only limited as for exactly that. Actually, you cannot go to Bitbucket and see it live. I put a video of it demoing it and showing that it’s actually real and you can do a live update of this platform just by using and starting containers. I encourage you to have a look at the video and I posted a link. I’ll post the link into the chat if you guys are interested. I thought it was an interesting hack to show. Because we’re very interested at Atlassian about new developments about Docker, new trends of Cloud-based deployments. Personally, I’m very excited about it. Grace: Awesome. Thank you, Nicola, for sharing that with us. That wraps up our content for this session of the Dev Den Hangout. Thank you everyone for joining us. This is being recorded so we’ll posting this recording for everyone so you can take a look at it again. We’ll have a transcript available for it in two weeks. Thanks everyone. Thanks guys. Nicola: Thank you. Tim: Bye guys. Nicola: Bye.", "date": "2015-03-12"},
{"website": "Atlassian", "title": "Introducing the Ecosystem Support Portal", "author": ["Peter Van de Voorde"], "link": "https://blog.developer.atlassian.com/ecosystem-service-desk-announcement/", "abstract": "We'll be introducing an Ecosystem Developer Technical Support Desk to address technical support for building integrations on our platform and Marketplace Service Desk for Marketplace vendor inquiries and issues. Our goal is to streamline your interaction with our Ecosystem teams and achieve a better level of support. Over time, we'll be removing various channels such as our several Google groups where support requests have been collected so that issues can be routed through our new service desks to the appropriate internal teams. For the next few weeks we welcome you to share your feedback on this new process through these service desks. Visit our Developer Technical Support Desk Or our Marketplace Vendor Support Desk Got feedback? Let us know in our brand new Service Desk ! Cheers, Fatuma Kayembe & Peter Van de Voorde", "date": "2016-10-07"},
{"website": "Atlassian", "title": "Docker triad: composing the future", "author": ["Nicola Paolucci"], "link": "https://blog.developer.atlassian.com/docker-triad-compose-the-future/", "abstract": "Today I have an overview of the new developments in the Docker ecosystem. I’ll explain briefly what Docker is and how it is evolving from a tool to package applications and easily distribute them to a set of tools to orchestrate and manage loosely or tightly coupled cloud solutions. If you prefer pretty images and my “radio voice” (cough) you can click play here, otherwise keep reading below. If you’re not familiar with the Docker explosion in recent months or in the last couple of years, let me give you a brief overview about what Docker is. The scope of the project is ever-expanding but it can be summarized as being more or less four concepts: The above is one way to define what Docker is. There’s another way which is contrasting it with virtual machines. A very popular choice in modern cloud based development and deployment is the practice of packaging an application and install it inside a virtual machine. It’s an efficient model but it has drawbacks. The main drawback being that you’re always booting an entire virtualized operating system whenever you distribute an application. Together with your application – which might be just a few megabytes – you’re shipping a virtual Ethernet driver, an entire operating system with all the binaries and libraries, statically and dynamically linked C libraries that an operating system needs. Maybe that’s overkill, wouldn’t you say? The Docker engine is a process that sits on top of a single host operating system. It manages and runs these specially packaged containers in isolation. The container itself sees only its own process running. It has access to a special layered “copy on write” file system which isolates it from the rest of the underlying storage. The application inside the container thinks it has access to an entire machine. Containers start up at an incredibly fast speed because they don’t need to boot up an entire operating system. You can package many more containers in a single host than you can package virtual machines. Nowadays people are embracing Docker to go towards a micro-services architecture . For example a modern common web application has multiple components: it might have a database, front end code, serve static assets; it might need access to fast key/value stores and to relational databases. All these components need to be managed and coordinated and linked together in a cohesive unit. The Docker ecosystem is moving towards support for these workflows. Just a few days ago Docker maintainers announced beta versions of three new pieces of this puzzle. The first one is Docker machine , a tool to provision environments both locally and on cloud providers with very simple and streamlined commands. I published recently a short video screen-cast on how to use docker-machine . The second is Docker Swarm which is a cluster management solution. Once you have Docker installed on different hosts, you can control them as a unit and you can use constraints to automatically balance and deploy applications to your Docker Swarm. The third part is Docker Compose , formerly known as Fig, used to describe how the components of your applications should be linked together. If you want to see a few brief examples and configuration for all of the above watch the video below. Here’s some timed links to skip to the sections you care about: With these three pieces of the puzzle Docker becomes much more than just an application format. You now have the tool to install Docker on many hosts. The tool to declaratively deploy your applications on a cluster and describe how your applications should be linked together. If you put these together you get a very nice and compelling abstraction over the cloud infrastructure, which is fantastic. It’s early days for these new tools. The Docker system is moving really fast but the promise really big. I’m very excited by these new developments. I hope you are too and have found this useful. Thanks for reading and let us know what you think at @atlassiandev or at @durdn .", "date": "2015-03-12"},
{"website": "Atlassian", "title": "On-demand activation of Docker containers with systemd", "author": ["Steve Smith"], "link": "https://blog.developer.atlassian.com/docker-systemd-socket-activation/", "abstract": "One of the features of systemd is its ability to defer the start-up of networked applications until their services are actually requested, a process referred to as socket activation . This isn’t really a new an idea; systemd borrowed the idea from launchd which has been in OS X since Tiger ‘s release in 2005, and the venerable Unix inetd has implemented a simpler version of it since the 1980s. Socket activation has a number of advantages over scripted or event-based start-up systems; in particular it decouples the application start-up order from the service description, and can even resolve circular application dependencies. Docker containers (and other types such as systemd’s nspawn or LXC ) are almost exclusively network processes, so it would be useful to extend socket activation to them. Socket activation works by having the systemd daemon open listening sockets on behalf of the application and only starting it when when a connection comes in. It then hands the socket to the newly started application, which then takes responsibility for it. But one of its limitations is that it requires the activated application to be aware that it may be socket-activated; the process of accepting an existing socket, while simple , is different from creating a listening socket from scratch. Consequently a lot of widely used applications (e.g. nginx ) don’t support it. The trend of containerisation exacerbates this situation by adding another layer requiring activation support. However socket activation of containers would solve the problem for any containerised application by deferring it to the container. If you Google for “docker container systemd socket activation” you will find quite a few discussions of how to achieve this, most of them concluding it is not possible without explicit support from Docker. While Docker support would be the optimal solution this is not the whole story. The systemd developers have known that it may take some time to get activation support everywhere, and in version 209 they introduced systemd-socket-proxyd , a small TCP and Unix domain socket proxy. This does understand activation, and will sit between the network and our container, transparently forwarding packets between the two. So with a few crafted units (systemd’s configuration system) we can create a socket-activation framework for Docker containers. Caveat : The current release of Ubuntu has systemd 208, which does not have systemd-socket-proxyd. To try the example below you need 209 or higher; the pre-release of Debian Jessie works, as should any recent RedHat based distribution, e.g. Fedora. As is so often the case, an illustration may simplify things: What is going on here: While there some tricks required, conceptually this is quite simple. So let’s put it into practice with systemd and an nginx Docker container. The first thing we’re going to want to do is create the target container. In this case we’re going to create an empty nginx container using the official nginx image : This is the point you would perform any configuration of the container. Note that we use create and not run ; we don’t want the container to start-up yet. We also name the container appropriately so we can start it later. The only trick here is that we need to bind the container to a port other than the real-target port (8080 instead of 80 in this case). This is because that port will be owned by the socket/proxy, and you can’t bind two processes to the same socket and interface. Now we have a container, we can construct our activation pipeline. The first piece we need is the initial listening socket. This is called a socket unit and the behavior of the socket is highly configurable, but for our case the setup is quite simple: The [Socket] section describes a simple TCP socket listening on port 80. The [Install] section tells systemd when to start the socket, in this case along with any other sockets configured on the system. We place this unit into a file called /etc/systemd/system/nginx-proxy.socket . This is the only part of the chain that is active after boot, so we need to tell systemd to start it: When systemd receives a connection on the socket it will automatically look for a service with the same name and start it. So we need to create the service-file /etc/systemd/system/nginx-proxy.service : The [Unit] section describes the dependencies of the service. In this case we are telling systemd that before starting the proxy we need the actual container to be up. We’ll configure that service below. The [Service] section is responsible for starting the proxy process; there’s a lot of powerful things that can be configured here, such as what to do if the process fails, but for our purposes a simple start is OK. Note that we forward to 8080, where the container will be listening, and that we don’t tell it to listen on port 80; it just uses the socket it’s handed by systemd. Also notice the absence of an [Install] section; this service is not started by default, but when the socket activates it. As mentioned above, the proxy triggers the start-up of the container using Require / After of the service configure in the file /etc/systemd/system/nginx-docker.service which looks like: The basic concept is the same as the proxy service above; the ExecStart line tells systemd how to start the container; systemd prefers processes that don’t run in the background, so we add the -a flag which runs it in the foreground and has the added advantage of forwarding the nginx logs to systemd’s journald logger. The ExecStop tell systemd how to stop the container should someone issue systemctl stop nginx-docker . The tricky bit here though is the ExecStartPost line; this is run by systemd immediately after the main process is started; in this case we sleep for 1 second before continuing. This is necessary because while docker can start containers very quickly, the process inside the container may take slightly longer to initialise. systemd is also very fast, so what can happen is that the proxy may forward the initial connection before nginx is ready to receive it. So we add a small delay to give Docker/nginx a chance to start; it’s hacky but it works (but see below ). And that’s it; on boot systemd will start a socket, and on the first connection it a cascade of dependencies will result in an nginx Docker container responding to requests via a proxy. Easy. Anyone who’s spent some time trying to optimise system start-up times comes to hate arbitrary sleep s in code or configuration. They either needlessly delay start-up on fast systems, or cause random hard-to-debug failures. The sleep in the ExecStartPost above grates on my nerves, so I’d like to remove it. What we’d really like to do is check if the port is up, and only if it is not do we sleep. We would also like to fail if the port takes too long to come up. To do this I’m going to use netcat and a wrapper script: This script takes a host and port, checks if it is responding, and will retry it every 10th of a second for up to 1 minute before failing. We install this under /usr/local/bin/waitport (and make it executable). Now our nginx-docker.service file will look like And that’s it; the waitport script can be tuned to whatever parameters make sense in your systems, and will usually return immediately if the container is quick to start.", "date": "2015-03-04"},
{"website": "Atlassian", "title": "Docker machine screencast: deploy Atlassian everywhere!", "author": ["Nicola Paolucci"], "link": "https://blog.developer.atlassian.com/docker-machine-screencast/", "abstract": "Even if it’s still in early stages of development, Docker Machine is a very powerful tool, one of the three very intriguing new pieces of the Docker ecosystem – the other ones being compose and swarm . What does Docker machine do? It allows you to create and manage Docker hosts on local machines or cloud providers. Since we recently released an official docker image for our enterprise Git repository manager Stash – a first step towards dockerizing all our products – what better opportunity than to show the basics of docker machine and at the same time show how easy to deploy Stash? Because the written word is sometimes dry and my voice is much more enthusiastic than my text I thought this time I’d whip up a quick demo video to show you the capability of Docker machine and show you how easy it is to use it to deploy your applications. Enjoy and let us know what you think at @atlassiandev or at @durdn .", "date": "2015-02-25"},
{"website": "Atlassian", "title": "RSS to HipChat integration in under an hour", "author": ["Travis Smith"], "link": "https://blog.developer.atlassian.com/rss-to-hipchat/", "abstract": "I got a request from an internal team to get notified when a new post on this blog goes live. It seemed like something one should be able to do without too much effort. So I set aside about an hour to figure something out and now I want to share what I came up with to solve this problem. This same method could be used to integrate almost any services easily, quickly, and without investments in infrastructure. I don’t want to manage an application or infrastructure, so I’m going to use a couple of third party services here. The data flow is first from an RSS feed into Zapier , then use a web hook to push that data into webscript.io which transforms the data into a message payload that is then pushed into HipChat. Zapier has a HipChat integration – but it requires an admin token for the HipChat account. I’m interested in reducing the scope of access this integration has down to one or a few rooms only. I also want to have the flexibility to transform the data that using webscript.io can offer First off I know I need to get data into HipChat, thankfully we have build your own integrations with HipChat . Log into hipchat.com , and go to Rooms > Integrations > Find New . From there, click on \"Build Your Own\". I’ll start with naming my integration, in this case RSS Bot works fine. The next page gives me the details I need, most notably a handily named URL I’ll need later, Send messages to this room by posting to this URL . The external piece I need is a place to transform data and push it into HipChat. A webscript.io endpoint can be made to do just about anything using Lua so I’m going to do that. Sign up for a free account Click Create a new script… Choose a subdomain and path for the endpoint. I’ll select rss-to-hipchat.webscript.io/new-rss . Now that’s for the transformation I want to do. To get each new feed item over to my webscript.io endpoint, Zapier comes into play. Zapier trigger actions based upon RSS updates. Sign up for a free account with Zapier Click Make a New Zap button to start an integration For the trigger, select “RSS” then “New Item in Feed” For the action, select “Web Hook” then “POST” — at this point one could just integrate directly with HipChat but that requires a admin token rather than a room token and we want to scope the access of this integration to just a single room Click Continue to move forward to the configuration screens Put the RSS feed into the URL field For the web hook section, we start by putting in the URL for the webscript endpoint; e.g. rss-to-hipchat.webscript.io/new-rss Select json as the payload type Configure the payload by adding 3 fields url to Link title to Title date to Pub Date Configuring Zapier is now done, but before testing the webscript endpoint needs to be written. Going back to webscript.io and filling out the endpoint, by putting in the Lua code, converts posted payloads to messages that appear in a given HipChat room. So it will look something like this: With the endpoint built, httpie makes it easy to test. Executing this command posts a JSON payload to the endpoint on webscript and it should post a message to the configured HipChat room. If you get a 500 error, webscript provides logs below the script entry you can expand to help debug what’s going wrong. Make sure the URL of the http.request is updated, including the correct room id and auth token. Assuming that works, and the message is visible in the HipChat room, the final step is to use the tester in Zapier to make sure everything is hooked up right front-to-back. With these tools, I can quite easily take any data source and transform it however I want and send it into HipChat. I can also notify multiple rooms in one go just by adding additional http.request calls to the webscript endpoint. All of this without building or deploying any infrastructure – just using existing services hooked together.", "date": "2015-02-19"},
{"website": "Atlassian", "title": "Where we retire checkup.atlassian.com", "author": ["Nick Wade"], "link": "https://blog.developer.atlassian.com/retiring-checkup/", "abstract": "For some time we’ve provided an API compatiblity-checking service to plugin developers – checkup.atlassian.com . We built this service some years ago ostensibly so you could more easily check if any product Java API you are using is or isn’t a public supported API. We’re retiring this service shortly. While there have been a handful of developers and vendors using the service in recent months, it’s usage levels don’t make sense for us to continue support. As of March 31st 2015, Checkup will be shut down and all requests will be redirected to this post, and then later to our API documentation . Checkup was created to allow developers to validate their add-ons were API compatible between versions of products. Checkup provided some useful API checks, but every plugin developer still needed to test on multiple versions since API compatibility isn’t the only thing that matters. We feel that Checkup provides some value but not enough based on it’s current usage (about 100 tests per month) compared to its support needs. Finally, we’re a bit ashamed to admit that only a couple of Atlassian products have been providing new versions to Checkup in the last year. And that’s reducing to one product soon anyway (if we do nothing). This also limits its effectiveness in a subtle, forgotten in-the-corner way. This is not the kind of experience we’re accustomed to, or aspire to delivering. For now, if you want to continue to check on the support for given Java APIs, check out the docs repository. It includes links to the Javadocs for all our products and libraries. We will explore other ways to provide automated API checking to add-on and plugin developers in the future, although there’s nothing much we can say about that right now. While Checkup is still running, you can provide feedback about Checkup if there’s something you’d like to add. So long and thanks for all the APIs, Checkup!", "date": "2015-02-12"},
{"website": "Atlassian", "title": "Rebuilding HipChat with React.js", "author": ["rmanalang"], "link": "https://blog.developer.atlassian.com/rebuilding-hipchat-with-react/", "abstract": "When HipChat joined Atlassian, it had four clients: web, Adobe Air (Windows, OS X, and Linux), iOS, and an Android app. One of the first goals for the HipChat team was to replace the Air client with a native desktop client for OS X, Windows, and Linux. This kept our small team (at the time) very busy. Because of this focus on delivering first-class app experiences, our web client didn’t receive the benefit of updates we were making elsewhere. That sucks, and we’re fixing it. I’ve long tried to make a case for improving the web client and possibly even rewriting it. It’s not an easy sell or decision to rewrite anything, but the HipChat web client sorely needed to be improved all around. There should be few reasons to ask our users to download a native desktop client if we deliver a web client that performs on par or better. Bob Bergman , Clifton Hensley , and I did the research to come up with a reasonable rewrite plan, what technologies to use, etc. Most product or dev managers are more apt to dismiss a plan to rewrite an app if there’s one that works and can be maintained. Well, we got lucky. The original goal was simple — we needed to apply the Atlassian Design Guidelines (aka, the ADG) to the web client. Applying the ADG to HipChat would unify the look and feel with other Atlassian applications. That goal by itself might have been straightforward with the old web client, though it likely would have been riddled with bugs (since there were zero tests) and would require lots of jQuery soup programming; not a fun task for anyone. Because of the risk of making the existing web client actually worse instead of better, we were granted the runway to rewrite. We looked at a variety of new and popular JS frameworks: Angular , Ember , rolling our own with other smaller libraries… and in the end we looked at React. The rest is history. At first it was difficult to understand the benefits of React since it didn’t sell itself well. It was billed as a view library, not a framework. Ember and Angular’s popularity a year ago was impossible to ignore. I’ve built several Angular apps in the past including several Atlassian add-ons like the REST API Browser , Bob built HipChat video using Angular, and Clifton had some experience with Ember. We all knew about the big benefits you get with using those frameworks (e.g., 2-way data binding, MVC, testing, etc.). All of this made it harder to look at React objectively. We took a few days to prototype out a new HipChat client using each technology. All of them had some benefit, but when we got to React it became perfectly clear why we would want to use it for the next HipChat web client: Component based – This means that we can build reusable components that would allow us to share code with our native clients. Declarative – As with other component architectures, React is declarative. But it’s missing the bloat that comes with other component based libraries and is ready to be used today (I’m staring at you Web Components ). Virtual DOM – This is probably why most developers are so attracted to React. React manages its own DOM in memory. The most expensive operation most web apps suffer is mutating the DOM. React’s approach is to maintain a virtual representation of the DOM which allows it to calculate differences in the DOM so that it only mutates the part of the DOM that actually needs to be updated. This is a huge benefit! Small, it’s just a library… not a framework – Having worked with JS frameworks for several years, we all know that frameworks are often bloated and force you to include things you don’t need. That maybe ok in the server-side world, but a significant disadvantage with web apps. Simple – As engineers, we all try to follow the KISS principle as much as possible. But often times, the tools we use make it impossible. React is truly simple. The public API can be memorized in a day and once you’ve built your first component, it’s easy to build the next one with confidence that it’ll just work. Focuses on unidirectional flow – 2-way data bindings was an awesome idea when it came out. Back in the Backbone days, a lot of us were accustomed to writing lots of boilerplate code to update data throughout our apps. 2-way data bindings simplified all of that. However, it did come with drawbacks – mainly, that you had no idea how your data was getting updated. It was magic. React’s approach supports 2-way data binding , but discourages it. Flux, React’s application architecture, focuses on a unidirectional data flow and favors data immutability as it flows. The benefit of this is that you know exactly where your data is mutating, making it easier to test and maintain your app. Testability – React components simplify testing greatly. As a proof of it’s simplicity, our new web client has more tests than any of our other clients. In the end, React’s biggest benefits are summed up with: React’s declarative nature allows for predictable behavior that inspires confidence in the apps we build. Pete Hunt, the creator of React, has an excellent introduction in this JSConf video: * taken from Tom Ochino’s React.js Conf keynote Flux is Facebook’s pattern for React applications that focuses on unidirectional data flow. The basic idea with Flux is that everything happens in one direction. Data flows in as a result of actions. Actions trigger stores (data models) to be updated, which then triggers change events to fire, causing React views to update if needed. The cycle repeats itself as data changes throughout the app. When Flux was announced, it was just a pattern. Facebook didn’t release a library. We adopted the pattern for our new web client. However, because we built our Flux library from scratch, we made some trade offs with how things work. For example, Facebook’s Flux abides by the strict notion that the dispatcher has the following traits: However, our dispatcher deviates slightly in that it can be treated as a general pub-sub event emitter. For example, one thing our dispatcher allows us to do is dispatch events during a callback. The problem with this is that our dispatcher allows us to dispatch any event outside of the Flux flow, making it too easy to fall out of the Flux pattern. This is something we hope to fix soon by tightening our dispatcher’s responsibilities. With that said, the rest of our new web client pretty much uses the standard Flux components and follows the action to dispatcher to store pattern. Even with this slight deviation, the new web client’s codebase has been very approachable. As a testament to that, we’ve had code contributions from various parts of Atlassian from developers who haven’t used React/Flux before. New developers who join our team have been able to commit and deploy their first feature to production usually by their second day on the job. One of our side goals with the new web client is to build out components that can be reused by our other clients — and possibly other Atlassian products. With React, it’s possible to build out one component that can work in different clients, web or native. For example, our native desktop clients already use a web view to render the chat panel. We’re currently in the process of applying our React based chat panel to our native desktop clients. This means all of the complex logic of managing message rendering, history fetching, user states (e.g., someone is typing messages, etc.), scrolling animations and state management will all be handled by React — greatly simplifying our native client code. In the future, we’d love to see React components be shared across our other Atlassian apps. For example, what if we can take HipChat’s @mention or emoticon auto complete component and apply it to JIRA, Confluence, Stash, and Bitbucket? At React.js Conf 2015, Tom Occhino , manager of the React team at Facebook, introduced React Native at React.js Conf. Soon, we’ll be able to use React to build truly native components! We’re just getting started with building and using reusable components with HipChat. I can see a future where all of our new apps (web and native) are a composition of new and existing components provided by different Atlassian applications. Lots of possibilities to look forward to! The new HipChat web client is not entirely complete yet. We have a few features that are still missing, but we’re getting closer to feature parity with our other clients day-by-day. Moving forward, we plan to use the new web client as our reference client. The dev speed we’ve gained with React+Flux (and its friends: gulp , webpack , lodash , karma ) proves that we can release new client features faster and with more confidence on this platform than on any native client. We’ve been using the new web client inside Atlassian for several months now and have been slowly releasing to new and existing users. The rest of the world will have access to it soon, but if you’re interested in giving it a try, go for it…", "date": "2015-02-10"},
{"website": "Atlassian", "title": "Ten tips for wonderful bash productivity", "author": ["Nicola Paolucci"], "link": "https://blog.developer.atlassian.com/ten-tips-for-wonderful-bash-productivity/", "abstract": "I am always tweaking and tricking my bash environment. I hit the same issues again and again and I always have to look up the solution, time after time. This happens until I get annoyed enough to sit down – okay, generally I am already sitting down but you get the point – and create a custom function, put it in my .bashrc and deploy it to any machine I log on to. In the hope my struggle for ultimate terminal efficiency is of help to other fellow command liners have a look at some tips and functions I use frequently. It would be cool if this would turn into a two-way dialog and you also suggest your own bash shortcuts at @durdn , @atlassiandev or in the comments below! Without further ado, here’s what I’ve got today. I have to look up this every time. Here’s now to add a line at the top of a text file using sed : Easy and well known, here’s how to append ( >> ) several lines to a file. It uses the “here document” syntax which allows you to embed a document into your source by specifying which word delimits the end of the file. Most commonly used word is EOF (aka “End Of File”): Everything in between the first EOF and the last gets appended to the file. If you use Eclipse, IntelliJ or any other IDE you probably have powerful refactoring capabilities at your fingertips. But I bet sometimes you work on a tech stack that does not come with so advanced capabilities. How do I do a global search and replace on a directory tree on the command line again? Please, don’t make me use Perl, what about find and sed ? Thank you Stack Overflow : After a few times I have added a custom function to my .bashrc like the following: And you use it like this: I used to loved the scratch facility of Emacs and I missed the ability to quickly create a temp file to type persistent notes on my vim setup. So I came up with these two quick bash functions which use openssl to generate a random alphabetic string as filename: By typing sc or scratch on my terminal a new gvim or macvim windows pops up with random temporary file stored in my Dropbox folder ready for my “insightful” notes (aka random trash). Output a page to stdout ( updated !) following redirects and ignoring security exceptions: Download a file following redirects and ignoring security exceptions: I know, I know… there’s no need to note those flag down! It’s enough to read the very easy and short curl documentation! (humor engine engaged at full power). If you don’t have bashmarks in your .bashrc yet, what are you waiting for? They are awesome. They allow you to save and jump back to directories you use often. I have a minimal setup for them in my configuration like the following, but the link above has a more robust solution you can load up in your .bashrc : I use this daily. Really. I get some output and I only want the second column of it, or the third or whichever and typing out the full command is wordy: Why not create a simple function that you can use anytime? This makes slicing columns so easy, for example you want to remove the first column? Easy: I am a fan of xargs , I think it’s almost as good as sliced bread. But sometimes you need to massage the list you get back from it, maybe skipping a few values? For example when you want to remove stale docker images and the first line is a heading you don’t need: Here’s how to use it: in bash it’s incredibly easy to create your own command suite, names paced however you please. Have a look at some things I put in mine: With the above I can copy my ssh key to any site just by typing dur key user@host . Peruse the custom functions in my .bashrc or come up with your own. Do you have other neat tricks or short functions that help you in your daily terminal hackings? Let me know in the comments below or at @durdn on twitter. I’m always on the look for new ideas.", "date": "2015-02-05"},
{"website": "Atlassian", "title": "Realtime updates from PostgreSQL to Elasticsearch", "author": ["Steve Smith"], "link": "https://blog.developer.atlassian.com/realtime-requests-psql-elasticsearch/", "abstract": "The following is a repost of an article from my personal blog that describes how to perform event-driven updates from a PostgreSQL instance to Elasticsearch. In February I will be giving a tutorial at DeveloperWeek on development and testing with Docker , and this relies heavily on the code described in this post as an example project. So for consistency I am reproducing the post here. The final version of the code that will be used in the presentation is also available ; I recommend downloading it if you are attending the sesssion. Recently I've been evaluating Elasticsearch , and more specifically how to get data into Elasticsearch indices from source-of-truth databases. Elasticsearch is sometimes lumped in with the general NoSQL movement, but it's more usually used as secondary denormalised search system to accompany a more traditional normalised datastore, e.g. an SQL database. The trick with this pattern is getting the data out of the master store and into the search store in an appropriate timeframe. While there is already a mechanism for updates from SQL databases in the form of the JDBC river (' rivers ' being the Elasticsearch external data-feed mechanism); this operates by polling the database intermittently to retrieve any new or updated data. This is fine, and sufficient for most applications (e.g. an online storefront). However some of the systems I work on are less tolerant of delay (and as a rule I prefer event-based systems to polling), so I was curious to see if it's possible to implement event-driven updates from Postgresql that would propogate to the search cluster immediately. tl;dr: It is, but requires some non-standard components; the steps required are described below, and a proof-of-concept test implementation exists . Also, this mechanism is not Elasticsearch specific, so could be applied to other secondary datastores (e.g. an Infinispan cache grid). The basic idea behind this is pretty simple; we can use SQL triggers and PostgreSQL's notify extension to tell a dedicated gateway server that a change has occurred. notify / listen updates occur asynchronously, so this doesn't block the Postgres trigger procedure. The gateway then reads the changed data and injects it into the Elasticsearch cluster. The first problem with this concept is that I’m working on a JVM platform, and the PostgreSQL Java driver doesn't actually support asynchronous updates via notify. It instead requires you to poll the server for any new notifications, effectively negating the benefits of using notify. In fact, the driver doesn't support a lot of newer Postgres features such as multi-dimensional arrays. However while searching for possible workarounds for this I came across an alternative Java driver that attempts to fix the deficiencies in the current one, including adding true asynchronous notifications. The second issue with this concept is that notifies are not queued; so if the gateway server is down for any period of time updates will be lost. One possible workaround is to maintain a modified column on the tables and read any newer entries on gateway startup. This is fine for simple data-models, but for more hierarchical data this rapidly becomes a maintenance pain (as child tables may need to trigger an update from the parent tables). The workaround for this is to implement an intermediate staging table that stores references to updated data; on each update the gateways reads from it and then deletes the reference; on startup it is read for any unretrieved references that occurred during downtime. So the final workflow looks like: As a test of the principles I've implemented a Clojure -based proof-of-concept project that will propogate changes between a PostgreSQL server and an ElasticSearch cluster in <500ms; these results are for a PostgreSQL server and Elasticsearch node running inside a Vagrant / Virtualbox VM on a standard rotating disk, so I'd expect to see better results in a tuned production environment. If you're interested in trying this yourself the gateway, Vagrant config and test code is all available in Bitbucket .", "date": "2015-02-02"},
{"website": "Atlassian", "title": "Even more continuous integration via dependency management tools", "author": ["Ian Buchanan"], "link": "https://blog.developer.atlassian.com/even-more-continuous-integration-via-dependency-management-tools/", "abstract": "When centralized version control systems were state-of-the-art, it made sense for agile thought-leaders to promote storing project dependencies in a code repository as pre-requisite to continuous integration . The goal was to version every configuration element, including external libraries, and to make sure every developer can easily obtain everything necessary to build. While those goals remain relevant, it is also important to keep current with downstream changes from third-party libraries. Since the early days of continuous integration, new dependency management tools have become popular to keep up with changes in third-party libraries, making integration even more continuous. If you are still committing libraries to your version control system, it is time to make dependency management tools an integral part of your continuous integration practice. When writing about continuous integration in 2006, Martin Fowler admits, “The current open source repository of choice is Subversion.” With that bias, he recommended: Although many teams use repositories a common mistake I see is that they don’t put everything in the repository. If people use one they’ll put code in there, but everything you need to do a build should be in there including: test scripts, properties files, database schema, install scripts, and third party libraries. When all files, including third-party libraries, are in the source code repository, the procedure for obtaining the specific files and versions of third-party libraries is as simple as checking out the code base. Unfortunately, this leaves some aspects of integrating with those third-party libraries unanswered. For the purposes of continuous integration, the most important unanswered question is, “Are we using the latest version of this library?” The Agile Alliance asserts one goal of continuous integration is to “minimize the duration and effort required by each integration episode”. To elaborate, Extreme Programming explains: Continuous integration avoids or detects compatibility problems early. Integration is a “pay me now or pay me more later” kind of activity. That is, if you integrate throughout the project in small amounts you will not find your self trying to integrate the system for weeks at the project’s end while the deadline slips by. Always work in the context of the latest version of the system. Neither calls out external dependencies; however, software projects are more and more built on external libraries, frameworks, and platforms. Avoiding “integration episodes” and “compatibility problems” is just as important for these externalities and should be addressed as frequently as internal source code changes. Nearly a decade after Fowler’s advice, there are dependency management tools for all major programming languages (for a list of appropriate dependency management tools by language, see Nicola Paolucci’s blog on Git and project dependencies ). Despite the availability of dependency management tools, many developers still believe there is only a simple dichotomy of “inside your project, or installed on your build server.” Instead of manually managing multiple versions of libraries in either place, dependency management tools provide an explicit, repeatable, and reliable procedure for obtaining any given version. In short, dependency management tools move the responsibility of managing third-party libraries from the code repository to the automated build . Typically dependency management tools use a single file to declare all library dependencies, making it much easier to see all libraries and their versions at once. That means it is sufficient to check-in references to the libraries, without storing the files themselves. This is convenient for newer distributed version control systems (DVCS) because some do not handle large binary files efficiently . It also has the surprising side-effect of making the build process more transparent – developers can read which versions of external libraries are needed to run a build, instead of needing to inspect file names or internal properties. Even without reading the set of declared dependencies, dependency management tools typically automate answering, “Is this the latest version?” Thusly, dependency management tools enable more rapid integration of changes from third-party sources. The biggest drawback of dependency management tools may be that there are so many to choose from. Unlike version control and continuous integration tooling, dependency management tools are specific to programming language. Compounding the differences by language, each tool works slightly differently, each with different quirks. Despite these flaws, it is worth taking the time to learn them and integrate them into version control and continuous integration flows. These dependency management tools and the practice of declaring dependencies instead of storing them provide an important advantage of rapidly revealing integration problems from third-party libraries. The practices and tooling are still young. As recently as 2013, Prezi engineers Ryan Lane and Peter Neumark wrote: Dependency hell is an unsolved problem in Computer Science right up there with P ≠ NP. Many teams who have struggled with dependency hell react by doing third-party integration less frequently. With each evaluation of the cost of updating against the benefit of new capabilities in the library, the cost has increased. Just as with source code, the longer integration with the newer version has been deferred, the greater the pain of updating. Hence, each decision to defer third-party integration leads to the ultimate state of never updating third-party libraries at all. At the other extreme, there is automatically updating all libraries with every build. For the current state of dependency management tools, this can introduce so much instability that builds become unreliable. The problem is not so much frequency but isolation of change. It can be confusing for a developer trying to understand why his check-in caused a build break if the cause might be a change in a third-party library. The problem with both extremes is being blind to change, either ignoring change itself or the impact. Max Lincoln explains: If you don’t have a good dependency report that shows what you’re using and what upgrades are available then you are uninformed. You could try to manually assemble a report, but that is impractical on large projects (most Java projects), or projects with lots of small, frequently released libraries (most Ruby projects). The solution is to use dependency management tools in the build process to generate dependency reports and signal when updates to third-party libraries are available. Even for dependency management tools with some auto-updating capabilities, the process of introducing those updates to the code base should be consistent with any other code changes. Dependency management tools do not yet (and may never) prevent the anti-patterns that lead to dependency hell, such as too many dependencies, long dependency chains, conflicting dependencies, and circular dependencies. For DVCS tools that often means the changes, even if only just bumping version number references, are reviewed by members of the team on a pull request, who ensure a passing build before the change is merged into master. This formality keeps team members aware of when dependencies are changing and prevents breaking the build. Since dependency management tools are already language specific, it is likely those will be around for a long time to come. However, the back-end storage of the libraries themselves may benefit from a more general approach to binary repositories. Tools like Nexus and Artifactory can be the back end for multiple dependency management tools as well as package management tools. With a recent resurgence of functional programming, there is some fresh thinking about binary repositories and dependency management tools that may accelerate adoption even further. For example, Mark Hibberd recently described his not-yet-released, open-source project annex in a video from StrangeLoop . Annex would capture richer declarations of dependency, smarter updating of dependencies, and earlier detection of dependency problems, all in a way that is cross-platform and more reliable.", "date": "2015-01-27"},
{"website": "Atlassian", "title": "Getting to know io.js", "author": ["Ralph Whitbeck"], "link": "https://blog.developer.atlassian.com/getting-to-know-iojs/", "abstract": "Last week, Twitter was abuzz about an initial release of io.js . io.js is an npm compatible platform originally based on Node.js and is a fork of Joyent’s Node.js . The io.js team is made up mostly of the key contributors to Node.js. In August, the team created Node Forward which was an attempt by the community to help improve Node.js. A broad community effort to improve Node, JavaScript, and their ecosystem through open collaboration. Here we get a hint at why the community felt compelled to fork nodejs at this point: Some problems require broader ownership and contributorship than have traditionally been applied, while others are so dispersed between tiny projects that they require new collaborative space to grow. Node Forward is a place where the collaboration necessary to solve these issues can take place. Ultimately, the work of the community could not be released under the trademark restrictions of Node, so a fork was made and io.js was born. Isaac Schlueter a core contributor, provides a lot of the backstory behind the decision to fork on his personal blog . One key takeaway, it is the intention of io.js that the two projects will hopefully merge together sometime in the future. First, io.js introduces proper semantic versioning (semver) by tagging the release 1.0.0 because it’s such a divergence from Node.js. jQuery blogged about the importance of using semver in a recent blog post : One of those best practices is semantic versioning, or semver for short. In a practical sense, semver gives developers (and build tools) an idea of the risk involved in moving to a new version of software. Version numbers are in the form of MAJOR.MINOR.PATCH with each of the three components being an integer. In semver, if the MAJOR number changes it indicates there are breaking changes in the API and thus developers need to beware. io.js updated the V8 JavaScript engine up to 3.31.74.1 which is most notable for running ES6 features of the JavaScript spec without using the --harmony flag to enable them. The features listed below are available with io.js without specifying any flag: io.js comes with new experimental core modules: You can review the full list of changes in the io.js changelog . Running a JavaScript node application with io.js is the same as running it with node; the only exception is that the name has changed. Node.js io.js Node version manager (nvm) , which is a bash script that allows you to manage multiple versions of Node.js and now supports installing various versions of io.js. If you have nvm installed you can run the following command in your terminal to list the versions of io.js available: Then in your project folder you can install io.js by specifying the latest version listed. Note: At the time of this writing, I’d recommend only installing io.js via nvm. Many early adopters who’ve installed io.js through the installer on io.js homepage are saying that io.js inserts itself over node by replacing a symlink that links to node to now link to io.js. nvm allows you set a specific version for specific project folders. Want to test io.js out in an Atlassian Connect add-on? You can quickly get a HipChat add-on to run on io.js and utilize an ES6 feature like Generators by following these simple steps: This will install nvm and update your shell. You’ll next need to type exit at the command prompt to close your ssh session and restart the shell. This will start the Koa HipChat add-on server and you can register your add-on with a HipChat room using the following URL: If you are able to type /hello in the chat field and have the HipChat add-on reply back with “Hi” then congratulations! You are now running an io.js application that utilizes ES6 features such as Generators. The all important question you might be asking yourself, can I use io.js in my node add-ons today? Right now io.js is a little more than a week old and the latest version at the time of this writing, v1.0.3, is still listed as “Unstable” on the site. Services like nvm are still working the bugs out. Hosting companies haven’t announced support for it. Right now if you’re an early adopter and want to see a “Stable” io.js test your add-ons with io.js and file any issues you find . Ultimately, it’s too early to switch to io.js but keep an eye on it and maybe it’ll become the most popular JavaScript server platform to use. Ralph is a Developer Advocate for ecosystem developers.", "date": "2015-01-23"},
{"website": "Atlassian", "title": "Amplify your Bamboo workflow with git", "author": ["Steve Smith"], "link": "https://blog.developer.atlassian.com/amplify-bamboo-workflow-with-git/", "abstract": "Back in December I did a webinar on some of the advantages of using git with your Bamboo pipeline, leveraging some of the Bamboo and Stash integrations to create a feed-back loop of quality control. The transcript for this webinar is now available below the fold… Hello and welcome to this webinar, “Amplifying Your Bamboo Workflow With Git.” This webinar will hopefully show you a few cool tricks you can do with Bamboo and Stash to really enhance your productivity and code quality, while also making your life a bit easier as well. But first, I should probably introduce myself. My name is Steve Smith, I’ve been at Atlassian for over eight years now. Back in the day, I was the original company’s sysadmin at one of the earlier Sydney offices. But over the last five years I’ve been on the internal systems team. The internal systems team built the Atlassian ordering systems. These are the financial systems that you purchase Atlassian tools through. Obviously, this has some strong requirements for quality and safety and this does have some relevance to some of the things I’m going to talk about a bit further on. However, more recently, I moved to the Atlassian Amsterdam office in the Netherlands. This also has some relevance to leveraging Stash and Bamboo, for reasons I’ll touch on during the talk. But before we start, we should probably have a look at the recommended workflows for Git. It has a lot of relevance to what I’m going to talk about and enables some of the tips and tricks that I’m about to discuss. As you may know, Git enables some workflows that are somewhat different to workflows for more traditional version-control systems, such as Subversion or CVS. The key difference, when developing with Git, is that branching and merging is safer and faster than with older version-control systems, such as Subversion or CVS. This allows us to isolate work on individual features, or bug fixes, while other work continues in parallel. We only merge the work when it is completed and has been reviewed by other members of the team, something I will discuss in detail later on. While we are doing this branch-based workflow, it ought not break anything when we do merge back to the master branch. We should ensure that all tests are passing. To do this, we run all tests required on the master branch on the feature branch as well. This ensures the master branch constantly remains in a releasable state. This, by the way, is one of the central points of the continuous delivery workflow. By making sure that the stable branch is always in a releasable state, you enable a lot of powerful new workflows, such as continuous delivering and continuous deployment. But not everything can be detected by tests. Certain classes of problems in the code can be missed, either because tests have not been provided or because they entailed judgement calls by humans. Some things just require human input to make the best decision. The final stage of our future branch is to create a pull request and invite other team members to review our changes and check for issues that may not show up in tests. Code reviews and pull requests have a lot of benefits that we’re not going to be able to rehash here. There are a number of blog posts and articles on Atlassian and other sites that give you an overview of the key points and the best, most effective way to perform code reviews and pull requests. In particular, I recently wrote a retrospective blog posts about how the internal systems team adopted pull requests organically as a method of gaining peace of mind when we were modifying our financial systems. So we’ve seen that each branch and merge workflow enables lots of new opportunities to inspect and enhance our code quality as we are developing. But personally, I’m lazy. I want our tools to do as much possible for us. Bamboo and Stash maintain a huge amount of information about ongoing code activity and the quality of that code. We would like to leverage this to assist us as much as possible to maintain a high-quality code base at all time. So how can Stash and Bamboo assist with this workflow? This is where a build and commit feedback would be useful. If we can share information between our code repository and our build server, we can enable some powerful interactions that make our lives easier as well. The first part of this Stash/Bamboo interaction I want to look at is feeding forward. Feeding forward is where one of the tools in our pipeline provides information to the tools further down the chain to help them make intelligent decisions about the work required of them. In the case of Bamboo and Stash, Stash is able to push information to Bamboo about changes that occur within the Git repository as they happen in real time. Bamboo can then use this information to automatically perform actions on our behalf. Lets have a look at some of the actions that can be performed that will help you out on a day-to-day basis. The most powerful feature enabled by forward integration is that Bamboo can be informed about the creation and deletion of branches inside your Git repository. This enables Bamboo to create and remove branch plans as you branch and merge your features inside Git. If you are not familiar with them, branch plans are copies of old build tests and other quality-assurance metrics that are run against the master branch. These are copied to a new plan for that feature branch, and they are then run on every commit. This enables the branch and test workflow that I talked about at the beginning. By running all the quality-control tests against the feature branch before we perform the merge back to the master branch, we are ensuring that when we do so the master branch remains in a releasable state. This is absolutely core to continuous delivery and continuous deployment. In many cases, your master branch may actually trigger a release or a deployment to a production server at the time of merge, so it’s absolutely critical that your master branch must remain in a pristine state at all times. Just to give you a quick overview of how this looks inside Bamboo, here’s some real world build results from a branch I was working on for our ordering system. As you can see, the branch plan looks much like a normal Bamboo build plan result. It shows the commits that have been occurring on the branch. The results of the tests are run against that branch. But the major difference is that all the builds and tests are being run against my branch, rather than the master branch. But other than that, it is largely the same as if you are running a simple, single-branch build plan. But to further enhance this feature, Stash will also inform Bamboo when commits occur inside the Git repository. This allows Bamboo to trigger new builds whenever you commit to your feature branch, thereby speeding up your development test feedback loop. Instead of waiting for your tests to be triggered in Bamboo or manually triggering it for yourself, Stash will immediately inform Bamboo that a commit has occurred the moment you push to it. This greatly speeds up your build test feedback loop and provides an improved productivity. This is particularly effective if you are using a continuous integration pipeline that runs your light, fast tests at the start of your pipeline and then continuously increases the complexity of your tests as you go down the line, up until full-acceptance testing at the end which may run on the simulacrums of real world systems. By having the light tests at the start, you will often spot build and test issues early on in the pipeline. If your builds are triggered the moment you push to your Git repository, you will get this feedback much earlier in that feedback loop. This is important, as it speeds up your build test cycle greatly. More powerful though is the feedback portion of the feedback loop. This enables some of my favorite integration features between these two tools. I am someone who genuinely uses these tools on a day-to-day basis and discovering some of these build integrations are a real fun part of my working day sometimes. What I mean by feedback is that, as well as receiving information from Stash, Bamboo can push information about the build and the test results back into Stash. Stash can then use this information to help developers make informed decisions about the quality of the code and even to prevent problems before they actually happen. To illustrate this second part, I should give you a little bit more background. As I mentioned, until recently, I worked as developer on our ordering systems, but although most of my team is based out of Australia, I work out of the Amsterdam office which is an 11-hour time difference at this time of year. This often meant that, by the time I started work in the morning, all the other developers were in the process of committing their changes for the day and then going home. Often, I would come in in the morning, create a branch for a new feature I was going to work on that day and then start making commits. But sometimes I would find that my branch would immediately start failing builds in some weird place that had nothing to do with what I was changing at that time. Sometimes, I would actually spend hours tracking down the bug, discovering that the problem had nothing to do with the changes I have made. This is in spite of the fact that master branch builds were green when I went and checked them. What was happening here is that the developer may sometimes make a change directly onto the master branch. You know we are not supposed to, but it does happen sometimes. They may also merge a branch locally themselves and push it up — and then push it without testing, more importantly. There may even be a problem with a tricky merge that introduces an error. Although, frankly, this is exceedingly rare inside Git. It turns out, the change the developers made now breaks the build. It’s fine. Luckily, Bamboo would immediately run the test for the master and alert that the build has failed. This is why we run continuous integration servers, to rapidly find such problems and feed them back to the developer to make the change. No problem. The developer commits the fixes for this. They wait. The build is green again. All is well and good. Developer heads home for the night. This temporary breakage could have some knock-on effects. Imagine that, while this is going on, another developer — say one in Amsterdam — starts to work to fix a bug in the data access layer, for example. This developer does the right thing and creates a branch for his work, but once they start committing this branch and then push it up to the main Git repository which, in turn, triggers the builds. They see that builds immediately start failing. Unknown to them, they’ve branched from a bad point. They will then possibly think that they have introduced an error and spend hours tracking down a bug that really has nothing to do with them. How can Stash help with this? If you create your branches from within Stash directly, via the web UI, we can avoid this situation. If Bamboo is configured to feed the build states back into Stash, Stash can warn the developer at the time of creating a branch that they are likely to encounter issues by branching at this point. This requires you delegate branch creation to Stash, which is not to everyone’s taste but it’s often worth it to avoid these sorts of complicated issues — particularly when a lot of merging and branching is going on at the same time. It’s also worth noting in passing that you can even create branches in Stash, from within JIRA , if you’ve enabled certain integrations there. This will also take care of your branch naming for you, which cuts out a whole other set of problems that you can be introduced by having typos in your branch names. This is a quite complex subject in itself. I’ve really only touched on some of the really cool stuff that can be done there. If you are interested in learning about this really useful technique, my colleague Tim Pettersen recently did a webinar about this and a bunch of other cool integrations between Stash and JIRA. The webinar recording is available on the Atlassian YouTube channel. Go check that out, it is well worth the time. There’s a lot of really cool stuff in there. I have saved my favorite use of feedback until last. I have talked a bit at the start about how important code reviews and pull requests are. They promote code quality and code knowledge within the team and can help spot problems in the code that are often hard to test for. I particularly like pull requests, as they give me peace of mind when working on highly-critical code. I wrote a longer blog post about this recently and, if you wish, you can go check it out in blogs.atlassian.com. As I mentioned, the internal system team work on the ordering systems for Atlassian. Any major errors introduced into this code could potentially cost the company a lots of money. We process the actual financial transactions. So whenever I worked on areas of the code related to financial transactions or to business logic, I would always ensure that I got input from other members of the team who knew more about these areas than me. My strengths are really in the technical area, and when it comes to business and finance logic, I am not as strong. However, I know there are certain people on the team who are good at that and have lot more domain knowledge in this area then me. I will often pull them into pull requests and, by having them check my logic, I will often feel lot happier about the changes that I was making at that time. However, by their nature, pull requests and code reviews are more work-intensive than code-based testing, and we want to ensure that this time is well spent by the developers who are being pulled in and are asked to check other people’s code. We also want to leverage information from the build system to ensure code quality. Ideally, we want to use the information from the builds that have been occurring on the branch to help define a policy on what standards must be meet before we merge to the master branch. Stash provides this ability to define policy. Within each repository, we can set a number of parameters that must be met before a merge can be performed within that repository. One of these is that we can require a certain number of reviewers to approve the pull request before it is merged. How many reviews this is is up to you. It depends a lot on your team resources and the type of code you are working on, and also your team structure. By ensuring that all code is seen by other developers before it enters the wider code base, you can ensure that subtle issues are avoided and knowledge is passed around the team as well. Stash can even recommend reviewers to you at the time of raising the pull request by looking into the history of the code that you have modified, seeing who else has made changes there and making recommendations on who is the best person to be knowledgeable about this code base. More relevant to our discussion is that we can require that the builds must be passing before the merge can be completed. This gives the team reassurance that they are not going to merge broken branches back into the code, code base. It can also inform them during the code review that there may be problematic areas that have been missed or areas that they can possibly focus on. Here, I am showing how it looks in practice. This is, again, a real example from pull request from the ordering systems I have been talking about. In this case, we can see that two reviewers have agreed that the pull request is good in principle. However, Bamboo has informed Stash that the build has failed some tests. These broken builds prevent the merge from occurring until these tests have been fixed. Obviously, this is exceedingly powerful and prevents any breakage on the branches being merged into. From these real world examples, we can see that having your build server and your Git repository communicate creates opportunities to really improve code quality and eliminate wasted time due to temporary breakages and failing tests that might otherwise be missed. One thing that I often say is that, ultimately, software is written by humans for humans. We must include humans in the process of quality control, but if we can leverage the technology and information we already have available, we can reduce the burden on humans involved and better safeguard the quality of the code. Thank you for attending this webinar on Git and Bamboo integrations. Hopefully, it has been useful to you. Of course, I will be hanging around after this to answer some of your questions online. We also have some more webinars coming up covering other areas of Agile and quality assurance. Do keep an eye out for those. Once again, thank you for listening to this. Hopefully, it has been useful. We’ll show some cool, interesting integrations that can make your life easier and generally just useful on day-to-day basis. Thank you!", "date": "2015-01-20"},
{"website": "Atlassian", "title": "Quick tip: reverting an octopus-merge", "author": ["Tim Pettersen"], "link": "https://blog.developer.atlassian.com/reverting-an-octopus-merge/", "abstract": "Following up on yesterday’s blog about octopus merges, @emmajanehw was pondering how you could ‘unmerge’ an octopus merge if one or more of the branches turned out to be bad: @kannonboy @jshawl …. And how would you unmerge one of the branches if you needed to? — emma jane (@emmajanehw) January 16, 2015 It turned out to be a pretty interesting question, so I thought a I’d write a short article talking about what I found. However, before we look at a couple of ways you can revert an octopus merge, you should know that reverting a merge isn’t always the best idea: Reverting a merge commit declares that you will never want the tree changes brought in by the merge. As a result, later merges will only bring in tree changes introduced by commits that are not ancestors of the previously reverted merge. This may or may not be what you want. Basically, if you later decide that you do want to merge that branch in again, the fact that the commits that were reverted are still in the history of the target branch means that only changes from commits created since the revert will be included in the merge commit. With that in mind, let’s look at the options. For these examples, I’m going to use a simple repository containing an octopus merge commit with eight parents (of course) : If you want to follow along with the test repository, clone it and run git reset 383804b -- after each of the example commands below to reset the master branch to its inital state. If you want to get your repository back into the state that it was before the merge occurred and you haven’t pushed the merge commit to the server , the simplest option is to reset the branch. Since the first parent of a merge commit is the branch that you ran the merge command on, you can reset by looking up the the SHA of the merge commit ( 383804b in our example repository) and running: Resetting is effectively rewriting the history of your branch, which is typically not a good idea if your merge commit has already been pushed to the server. Even if you haven’t pushed, this method will remove all commits after the octopus merge was introduced , which might not be what you want either. So next let’s look at how to revert an octopus merge without rewriting history. Git doesn’t know or make any assumptions about which parent of a merge commit a particular branch used to point to. To fully revert an octopus merge, you have to specify which parent was the “mainline” commit: that is, the commit that contains the changes you want to keep around after the revert. The first parent of the merge commit is the tip of the branch that you ran the merge command from. If that’s the commit you want to revert back to (it usually is) you can simply look up the SHA of the merge commit and then revert all changes relative to it’s first parent using: -m refers to the position of the parent commit in the merge commit’s list of parents. This is pretty awkward – I’m not sure idea why the developer didn’t elect to just accept a SHA – but the position can be obtained from the list of parents output from git log . For example, if we wanted to keep the history of fabdf39 (the tip of the branch leg-4 from our example repository) we’d need to use -m 5 as it’s the fifth parent in the list: In both of these cases, we’re reverting all of the changes introduced by a merge commit except the original branch. Next, let’s look at what we need to do to remove the changes introduced by one particular branch in an octopus merge, leaving the changes intact. There’s no simple command that reverts the changes introduced by a single parent of an octopus merge. However, the git-revert command does let you specify a range of commits to revert. So to revert a branch, all we need to do is find a way of expressing the commits on that branch as a range. If we wanted to find the range of commits introduced by leg-3 in our example repository, we could simply compare it against leg-1 : (If you’re following along, you’ll need to checkout leg-1 and leg-3 locally or prefix the branch names with origin/ for the command above to work) We can pass the same commit range to git-revert to indicate that these are the changes we want to revert: Note that I’m not passing the -m flag that we used earlier, as we’re manually specifying which commits to revert, rather than asking Git to figure it out for us. We are passing the -n flag though. This makes Git apply the revert to the current index, but prevents it from actually commiting the changes. We have to manually commit the revert ourselves: If you don’t pass -n , Git will create a separate revert commit for every single commit that was introduced on the branch that you’re reverting, which is probably a little excessive. Thanks for reading! If you have any further questions about reseting, reverting or octopus merging feel free to hit me up on Twitter (I’m @kannonboy ).", "date": "2015-01-16"},
{"website": "Atlassian", "title": "git merge-distinct: staging multiple branches with octopus-merge", "author": ["Tim Pettersen"], "link": "https://blog.developer.atlassian.com/git-merge-distinct-octopus-merge/", "abstract": "git merge-distinct is a little tool that merges multiple branches containing non-conflicting changes into a single head, using git’s octopus-merge strategy. Why would you want such a thing? Because while it’s useful to develop in the isolation of a feature branch, it can often be useful to combine branches together for testing or deployment to a staging server. Due to logical or literal conflicts, this strategy won’t work for all branches. But there are a few use cases where this can be really handy. In fact, you’re looking at one right now. Atlassian’s developer blog has two environments: production (you’re looking at it) and staging, which is mostly used for review. We treat our blog like code – each article is written on a feature branch and reviewed with a pull request. We also practice continuous deployment at the branch level, which means that any time a feature branch is created or updated, our Bamboo server rebuilds the site and deploys it. It’s common practice to link to the staged version of your blog in your pull request, so reviewers can read a rendered version instead of the raw markdown. Rendered vs. Raw This worked great! Right up until our pool of authors grew and multiple articles were being developed concurrently. Then it became a game of “last update wins”: with a single staging server and multiple branches, only the most recently modified branch would be deployed to the server. If your branch was staged and awaiting review, too bad! It would get clobbered by the next push: This meant that links to staged articles would often 404, so reviewers would often have to build a particular branch locally. Building the site locally was not only a time suck, it also excluded certain non-technical users from being able to participate in the review process. Last time one of my staged articles was clobbered by a co-worker, I decided that it was time to fix the problem. I came up with three possible solutions that would make my articles constantly available for review: If we had multiple staging servers (one for each branch) we’d no longer clobber each others changes: However, this would quickly blow out our AWS bill. git branch --no-merge shows eight other blogs in development, so we’d need eight staging servers just to deal with the current pool of authors. If I push frequently enough, I’ll clobber everyone else’s changes! Muahahahaha! While this would be a quick and easy fix for my problem, it’s in clear violation of our fourth company value . And, quite frankly, a bit of a jerk move. Git supports a merge strategy named octopus-merge that allows you to merge more than two branches together (in some cases a lot more ). I figured that when there was more than one outstanding branch, I could merge them together and then deploy the result to the staging server: While it might look complicated, performing an octopus merge is relatively simple (the command is just git merge <branch0> <branch1>.. <branchN> ). However there are a few special requirements for our developer blog use case: With these requirements in mind, I created git merge-distinct . It’s written in Node.js and packaged with npm because it exceeds my personal complexity threshold for a shell script. When run with no arguments it will create a new merge commit from your current HEAD and all of the other local branches in your repository that contain non-conflicting changes: The reason git merge-distinct will never fail with conflicts is that it will never try to merge branches that modify the same path. Under the hood, it runs git branch --no-merge to determine which branches to merge into the current HEAD, iterates over them and ignores any branches that contain changes to the same path as a branch that has already been considered. To ensure only static content is merged, I decided to allow the user to specify which paths are allowed to be modified on branches that are candidates for merging through --exclude and --include options. For example, the following command would merge all branches containing only changes under app/posts/ that didn’t modify any .js files: To allow developers to opt out of having their changes merged (and subsequently staged), I decided to let the user provide a pattern specifying which branches to include. For example, the following command would merge all branches starting with feature/ : git merge-distinct also supports a couple of other options for customizing the merge commit: We’ve incorporated it into the Developer Blog build process using the Bamboo Node.js plugin , and now we’re no longer clobbering each others changes with every push. git merge-distinct is generic enough that it should work for other projects which are wholly or partially static, and possibly for other use cases where you have multiple branches that need to be combined in an automated fashion. You can check out the source or install it locally (assuming you have git, node.js and npm installed) with: Git is smart enough to recognize other binaries on your path starting with git- , so you can invoke it just like a standard git command using git merge-distinct . If you have any feedback, issues or other use cases you think it’d be useful for, let me know on Twitter (I’m @kannonboy ). If you found this article useful, you may also enjoy Reverting an Octopus Merge .", "date": "2015-01-15"},
{"website": "Atlassian", "title": "Awesome ways to use add-ons to build better add-ons", "author": ["Nick Wade"], "link": "https://blog.developer.atlassian.com/build-addons-with-addons/", "abstract": "Have you ever considered that the same awesome Atlassian products or applications that allow you to track, build and collaborate can help you as you’re developing an add-on? For example, you have an idea of something that would be useful for either yourself, your users or the community as a whole and, after fleshing it out, you start coding. As you’re coding and you start to integrate with the UI, you hit roadblocks figuring out various web-item and web-panel locations. You know you can add a link into a particular place but, for the life of you, you can’t figure out the exact code needed. It used to be that the only way to uncover the locations was to sift through all the documentation. After that, you needed to download the source code of the product and start digging. Note: If you’re an add-on developer building for Atlassian Stash then you've never felt this pain. That's because the awesome Stash developers built a way to find the web fragments into the application! It was during one of these “search party” sessions that I realized that the product itself, via the plugin system, had all of the information I was looking for. Fortunately, I like to write add-ons! So, I wrote an add-on for myself called Web Fragment Finder . The main part of Web Fragment Finder is 3 lines of code: Using this code we can easily pull out the web-item locations that are used by other add-ons and then do stuff with it. In the case of Web Fragment Finder, we turn around and generate a new add-on that places a web-item in each location and then uploads it back to the host application. Rinse and repeat for web-panels and web-sections. The end result? Adding a simple url parameter ?web.items&web.panels&web.sections to the URL shows all of the locations. Rejoice! You no longer have to comb through copious code and documentation to find just what you need. Even after I had done this, I found myself heading into the source code to figure out what context I had to hook my CSS and JS to in order to get things rendering properly in a P2 plugin environment. It looked like the cycle was going to start all over again. Then I realized that the plugin system has the info as well. So, back in to the “add-on generating add-on” world of Web Fragment Finder I went. A very similar line of code did the job: This time I wasn’t quite sure how to display this. I decided that since we already generate a zip file with the atlassian-plugin.xml , we would generate a bunch of JS snippets and add them in there. After that, we just declare them all in the atlassian-plugin.xml attached to the context. What about events? The atlassian-events library is core to most products nowadays with events flying about within the various systems. At some point you’ll want to be able see when an event was triggered and the data it contains. By registering a plain object to the event system, you’ll actually get all of the events. Using java reflection, we can review all of the attributes of the objects flying about in the event system. The list of ways you can make the system work for you can go on and on. In some cases you can even find information that you might not have thought of that can make your life easier. Once I had all of this under my belt, I started to get into more Atlassian Connect development. Finding the right web-item location for the atlassian-connect.json was always easy. However, getting the syntax right was sometimes challenging… especially when I only wanted to write a quick add-on to add a web-panel to a JIRA issue page. We solved this by creating an Atlassian Connect add-on named Customizer for JIRA . It generates static Atlassian Connect add-ons that get installed in your instance. It has an editor interface that allows you to easily reference various attributes available in a web-panel or web-item. It makes for rapid prototyping of Atlassian Connect add-ons possible and helps speed up your time to market for your customers. You can be a connect add-on developer without touching a single line of code! At first I found debugging rest API calls over HTTPS challenging. I used to spin up WireShark and trace the communication that way. It worked but was slow and cumbersome. So, I took a step back and realized that a servlet-filter could track this information by just grabbing the data and storing in a singleton before it gets harvested into the database. All of a sudden development gets really simple and I’m left with my IDE and the product I’m working on. We add new capabilities to Web Fragment Finder as we discover new needs. And we’ve made Web Fragment Finder available for free on the Atlassian Marketplace as our way of giving back to the ecosystem. Are there tools you've developed in order to make your job easier? Have you poked around on docs.atlassian.com in some of the underlaying libraries (SAL, ActiveObjects, Plugin system etc)? Explore the APIs of the add-ons and I think you might be surprised at how much information is available to help you. Once you’ve solved your problem, you might also consider giving back to the ecosystem so we can all write better add-ons. Happy coding! -Daniel Wester —", "date": "2015-01-12"},
{"website": "Atlassian", "title": "Stash Plugin Tutorial: Beer O’Clock Repository Hook", "author": ["Tim Pettersen"], "link": "https://blog.developer.atlassian.com/beer-o-clock-stash-plugin-tutorial/", "abstract": "One of my favorite Stash features is its plugin system. Since Stash is a git repository host, 99% of Stash users are software developers. And since developers often like to scratch their own itch , we’ve invested heavily in extensibility and customization to make plugins both powerful and simple to write. In this 30 minute video tutorial , I’ll walk you through building, deploying and debugging a simple repository hook plugin, from scratch. A plugin is an executable bundle of code that provides additional functionality to your Stash server. It does this by consuming services from the Stash API and implementing plugin points provided by the Stash SPI . There are many plugin points to choose from, allowing you to build your own features with custom UIs, REST APIs, pull request merge checks and repository hooks, among other things. The logic of this plugin is written in Java, but the actual amount of code is tiny, so you should be able to follow along even if you have little or no Java experience. Make sure you view the video in HD mode (at least 1080p) so you can read the text. View transcript If you’re already done some Stash plugin development, you might want to skip ahead to a specific part of the video to learn more about a particular topic. This tutorial covers: If you enjoyed the video, have questions or want to see a tutorial on a specific aspect of plugin development, drop me a line on Twitter (I’m @kannonboy ).", "date": "2015-01-08"},
{"website": "Atlassian", "title": "Share your Minecraft world with a Google Map on AWS", "author": ["Joe Clark"], "link": "https://blog.developer.atlassian.com/minecraft-maps-with-aws/", "abstract": "Minecraft is one of my favorite video games of the last few years and if sales numbers are anything to go by, over 17 million other people agree with me! One of the great things about Minecraft is the permanence of your achievements; if you build something awesome out of Minecraft blocks, it becomes an indelible part of the virtual world you play in. I love being able to share my Minecraft contraptions with my friends, but it can be really expensive to host a Minecraft multiplayer server to do this. Using some freely available tools and cheap Amazon AWS hosting, I’ll show you how to share your Minecraft world online for just a few dollars a month! Fortunately, numerous third-party mapping tools exist for this very purpose! These tools generate a custom “view” of your Minecraft world that doesn’t require running the full Minecraft server, or using the Minecraft client to access it. I’ve been using a mapping tool called Tectonicus , which loads your Minecraft save files and uses LWJGL to render the world into static images. The images are then loaded into the Google Maps JavaScript API, which has a nifty feature that allows custom images to be treated as map information, allowing users to pan and zoom across arbitrary image tiles. There’s even a custom map projection API for co-ordinates in the flat Minecraft world to be translated to latitude and longitude. To get up and running, download Tectonicus and create a simple XML configuration file (in the following example, be sure to update the outputDir and worldDir values to suit your local environment). Then you can invoke Tectonicus from the command-line: Once the Tectonicus process has finished running, your outputDir will now contain a map.html file and some other files and folders: You can open the map.html file in a browser and you’ll see the generated view. Pretty neat! Building a Tectonicus map to share online has a number of advantages: Speaking of static file-hosting, let’s find somewhere to host our map files! Even in 2015, it’s still going to cost you some money to serve static files on the Internet over HTTP. Amazon S3 (“Simple Storage Service”) is offered as part of the AWS Free Usage Tier , but even uploading a moderately-sized Tectonicus map to S3 a couple of times will quickly exceed the monthly free quota. Still, it’s hard to beat Amazon’s prices (3c per GB!) and AWS has some neat command-line tools that will make uploading our map a snap! If you haven’t used AWS before, head over to https://aws.amazon.com and sign up for an account. You’ll automatically be opted-in to the free usage tier and billed for any excess usage. Once logged in, access the S3 Management Console and create a new S3 bucket (hit the “create bucket” button). A bucket is just a named location for files in S3. You’ll need to select a unique name for your bucket, and select a region for the bucket to be hosted in. Keep in mind that AWS is a ‘pay-for-what-you-use’ service, and lots of people accessing your S3 bucket constitutes your usage. You should consider setting up billing alerts in your preferences, so that you’ll be notified if your bucket starts getting spammed with view requests which could cost you money. Once the bucket has been created, click on it in the console view, expand the “Permissions” section and then click “Add bucket policy”: In the bucket policy dialog, paste the following JSON snippet (make sure to replace tectonicus/ in the \"Resource\" property with your own Bucket name) and then click “Save”. Next, expand the “Static Website Hosting” section, select the “Enable website hosting” option and type “map.html” as the “Index Document”. Then, click “Save”. From this point on, anything you upload to this S3 Bucket will be completely open to the Internet , so be certain to use it just for your Minecraft map and not any other purpose. Keep note of the URL listed as the endpoint for this bucket. That’s the URL you’ll be using to share the map with others. Now we have an Amazon S3 Bucket ready to serve our map files; let’s upload them! We’re going to use the AWS Command Line Interface to upload our map. You can follow the instructions on the CLI download page if you haven’t installed this before. But, before we go any further, let’s see what we can do to reduce the amount of content we need to upload. Putting content into S3 is relatively expensive compared to simply viewing it, so anything we can do to reduce this will have a tangible impact on your monthly AWS bill! You can see how big your generated map is by calculating the size of the outputDir : My example world’s map is 126M. Not that big in the grand scheme of things, but uploading it multiple times will add up pretty fast. Let’s see what we can do to reduce it. The default Tectonicus config file generates multiple zoom layers for the Google Maps interface. We can drastically reduce the amount of content to upload by reducing the amount of zoom that we make available to others. Additionally, we don’t want long exploration trips in Minecraft to generate more world space that needs to be rendered and uploaded; Tectonicus allows the configuration file to specify a radius to limit the part of the map that will be rendered. Let’s create a new config file to take advantage of these options: This config has a couple of changes from our previous example: We’ve reduced the volume of the render by targeting an explicit subset of the world. As the Minecraft world is explored and more world data is generated, the Tectonicus map will still remain the same size. Additionally, we’ve reduced the fidelity of the render by essentially disabling the zoom feature. When multiple zoom levels are used in the map, Tectonicus must render the same patch of the world multiple times at different levels of detail, which produces an order of magnitude of image data relative to the number of zoom levels. These changes should drastically reduce the amount of image data generated by Tectonicus, although the exact amount will depend on the specific Minecraft world being rendered. You can delete your outputDir and re-run the Tectonicus command to see what a difference this makes. Much better! We cut the disk usage by two-thirds, but took a bit of a quality hit in order to get there. You might like to experiment with different configurations to find your ideal balance between hosting costs and image quality, but I’d suggest starting with low quality first and then moving up from there. Now we can upload our map to S3, but there’s actually even more we can cut from the transfer size. The Cache directory in the outputDir is actually just a working folder that Tectonicus uses to cut down on successive render times (it does a checksum to see which Chunks have changed and only re-renders the changed chunks). You should keep this folder on your local machine to speed up successive renders, but you don’t need to upload it to S3. Similarly, the changed.txt file is just a debug log of Tectonicus’ changed-chunk detection process and can also be omitted. The AWS CLI has a “sync” command which can be configured to exclude certain paths. It also has the benefit of doing a differential copy to the S3 bucket (meaning it won’t upload a file that already exists in the bucket if it hasn’t changed). Once you have the AWS CLI installed, it’s a simple one-liner to run the sync (don’t forget to change the outputDir and bucket-name values to your own). Once this script has finished running, you can open your S3 Endpoint URL in a web browser (I told you to remember it from earlier!) and the Tectonicus map interface should load automatically! You can re-run the Tectonicus render and then the sync command at any time to upload the changes to your Minecraft world to the S3 Bucket, and it will only transfer the changed files. At this point, you should leave your AWS account alone for a day, then log in to the AWS console tomorrow and see what your estimated usage bill was from that single upload. From there, you can extrapolate what your monthly usage would be. Keep in mind that the more frequently you perform the upload, the higher your monthly bill will be. Users who access and view your map will also accrue usage charges on your bill (you may notice I have not provided a link to my own Minecraft map!). From here, there are more enhancements you can make – try hosting the map HTML from a custom domain, or configure your Minecraft profile to automatically run the map update whenever you quit Minecraft. Many people have discovered the joy of software engineering after starting to tinker with the possibilities in Minecraft. Happy crafting, and watch out for creepers!", "date": "2015-01-06"},
{"website": "Atlassian", "title": "Securing your Git server against CVE-2014-9390", "author": ["Tim Pettersen"], "link": "https://blog.developer.atlassian.com/securing-your-git-server/", "abstract": "A critical vulnerability was identified in Git last week . This has been fixed in all maintained versions of Git (v1.8.5.6, v1.9.5, v2.0.5, v2.1.4, and v2.2.1) so upgrading is the best way to protect yourself. However a sensible second step is to secure your Git hosting server, so that pushes containing malicious trees are automatically rejected. This will prevent attackers from exploiting users who have yet to upgrade their local versions of Git. The fix outlined below is based on our Git hosting server, Atlassian Stash . But it will also work for other repository hosting solutions that use the native git-receive-pack binary to accept pushes from clients, so read on even if you aren’t yet using Stash for Git repository hosting. Git stores all of its data in the .git/ directory in the root of your repository. When Git checks out files, it has built-in safeguards to prevent it from overwriting the contents of .git/ . Prior to the vulnerability being fixed, these safeguards were not sufficient for protection on certain file systems. Although it is not possible to check out a tree named .git in your repository, it was possible to create a malicious tree with different case, for example .Git or .GIT . This is a problem on case-insensitive file systems, including OS X (HFS+) and Windows (FAT/NTFS), because Git would happily write out the contents of a maliciously constructed tree over the contents of .git/ . This vulnerability can be used to write the contents of any file in .git/ , including modifying or creating executable files in .git/hooks/ . These are scripts that run at various points of the lifecycle of certain Git commands: $ ls .git/hooks\n\napplypatch-msg            pre-applypatch        pre-rebase\n\ncommit-msg                pre-commit            prepare-commit-msg\n\npost-update               pre-push              update Being able to modify these scripts effectively allows an attacker to execute arbitrary commands on your machine. Defining malicious git aliases by overwriting the user’s .git/config is another potential attack vector. On Windows, users are also susceptible to a variation of this vulnerability where the tree git~1 is mapped onto .git by the filesystem. Git users on OS X are susceptible to another variation where the HFS+ filesystem ignores certain unicode codepoints. A tree named .giu200ct will be translated to .git and again overwrite the contents of your .git/ directory. To exploit this vulnerability, an attacker would need to craft a commit containing a malicious tree and push it to a repository frequented by the intended victim. The victim would then need to pull a branch or tag containing the malicious commit and subsequently check it out to be exploited. An attacker needs write access to a repository in order to push the malicious changes in the first place. The actual risk for most teams’ repositories is relatively low, as there is typically a high level of trust between those who have the necessary permissions to write to a repository. However, all developers should exercise caution when pulling from third party or untrusted repositories until they upgrade to a patched version of Git. The best fix is for users to upgrade the version of Git installed on their machines. This will ensure that they are protected when pulling from any repository, trusted or otherwise. A good second line of defence is to configure your Git server to ensure that malicious commits can not be pushed to them. There are two things you need to do to protect your Git server: Enable the following Git settings: receive.fsckObjects core.protectHFS core.protectNTFS Enabling receive.fsckObjects makes Git check the integrity of objects before a push is accepted, which is a pre-requisite for the other flags. The core.protectHFS and core.protectNTFS flags prevent the OS X and Windows vulnerabilities described above, respectively. Both default to true on their respective systems but will need to be enabled specifically on other platforms. Since clients could be using a different operating system to your server you should enable both. You can apply these settings one of two ways: or In Stash 3.2+ run the command in ``$STASH_HOME/shared/data/repositories`` In earlier versions of Stash run the command in: ``$STASH_HOME/data/repositories`` There will be a patched version of Stash in the near future that will apply these settings automatically, so if you work in a team where your developers are unlikely to exploit each other, you may opt to wait for the patch. If you’re concerned that one or more of your repositories may already have been compromised, you can check to see if it contains a malicious tree using git fsck (after upgrading Git). It will print out a warning if the repository contains a tree that maps to .git : You can run this in a loop and check the output to see if you have any such trees in your repositories: If you do find a malicious tree, you can then git rebase it out of the history of the affected repository. Note that you may see some other odd warnings relating to the structure of your repository that are not necessarily problems (e.g. notice: HEAD points to an unborn branch (master) for an emptry repository). If you’re using Stash and have configured Audit Logging to track the RepositoryPushEvent you can also trace who pushed the malicious tree in the first place. Keep an eye on @atlassian and @kannonboy for further updates on the vulnerability.", "date": "2014-12-20"},
{"website": "Atlassian", "title": "Atlassian update for Git and Mercurial vulnerability", "author": ["Nick Wade"], "link": "https://blog.developer.atlassian.com/git-security-update-dec2014/", "abstract": "The maintainers of the Git and Mercurial open source projects have identified a vulnerability in the Git and Mercurial clients for Macintosh and Windows operating systems that could allow critical files to be overwritten with unwanted files, including executables. Because this is a client-side vulnerability, Bitbucket and Stash themselves are not affected; however, we recommend that all client users of Git and Mercurial, including FishEye, Crucible, and SourceTree users, update their Git client…see all the details over here .", "date": "2014-12-19"},
{"website": "Atlassian", "title": "Feature Branches, Builds and Multiple Environments", "author": ["Nicola Paolucci"], "link": "https://blog.developer.atlassian.com/branches-builds-and-multiple-environments/", "abstract": "In our recent Dev Den Office Hours we were asked some very interesting questions. One that caught my attention and on which I elaborated a bit on was the following: “One question I’ve had a hard time finding info on is in setting up feature branching with multiple environments (dev, test, prod for example) […] I’d love to see a little more of a walk through of how that actually works. Would you have a branch for each environment? Or is there a better way?” Great question, the important thing to note is that it’s a bit broad. Software teams are all very diverse. The way you arrange your branching model and the way you sort out your builds can vary tremendously, depending on the type of software project that you’re working on. The concerns “How do I build my deliverables, my application?” and “How do I manage team work in branches?” can be often seen as orthogonal. You don’t always need to track the state of a specific environment with a branch. I’ll explain this a little bit. It’s easier to understand things starting from examples so let me give you a high-level view about how the Stash team works. From that, hopefully I’ll be able to distill some guiding principles on how to think about this. Branching is used to isolate pieces of work like features and bug-fixes, and to make sure that the quality of the stable branches stays high. So the Stash team keeps a master branch semi-stable. Let’s say in a RC-like state (release candidate) generally not production quality, but relatively stable. Feature branches are created off master while bug-fix branches are branched off the maintenance release line they refer to (for example release/3.5 for fixes that will appear in the next 3.5.x maintenance release or master if they target the next official release). At the end of a release cycle which–in the case of Stash, generally lasts around five weeks–we create a stabilization branch (you might call it a release branch or hardening branch). There we carefully evaluate the security and performance of the application. No new features will be allowed into it anymore. When it’s time, that hardening release branch becomes the final maintenance branch and official builds, tars and zips are created from it for distribution. For the purpose of this answer I’ll skip the details on how we handle the maintenance branches. Just note that we have some long-running branches and a semi-stable master branch. Now how does this translate into the deployment pipeline and the build environments? See below the build architecture of the Stash project. As you can see, you don’t really need to think about branches when setting up your build environment. The core part of this whole build infrastructure starts just in a single branch, which is master . Everything is kicked off via a master . Let me walk you through it. Read with me from the top. Note that we merge to master only completed pieces of work, feature branches or bug-fixes. Those are merged to master once all the tests pass and code review is completed. Whenever something is pushed to master a build is kicked off right away and separate plans are started to do style checks and coding standard checks, in addition to some other metrics. Some longer, slower parts of the build are off-loaded to other agents. Because the Stash application is relatively big and complex we have to, for example: At the moment all the above plans are triggered by a green master build. The oval that says “master soke nightly” is our heavy performance test . In addition to the above every time we have a green master build we schedule an automatic deployment onto our first level early access “dog-fooding” server, stash-dev as we call it. This server is used already by some of the teams internally at Atlassian including the Stash team itself. We also have some other builds kicked off, like builds to check the JDK8 compatibility. We’re also automatically building every pull request–at least the fast part of the battery of tests. We divide the tests for pull requests into two groups: a “quick” section that includes unit tests and Checkstyle checks that can run really fast and the more extensive functional tests, which take longer to run. The longer, more extensive tests are optional and developers can kick them off by simply pressing the ‘play’ button in the image on the right. When it’s time to cut an official release we have a Bamboo build that automates a large part of our release process. It creates the distribution, generates the documentation, uploads the documentation to developer.atlassian.com, uploads the release artifacts, etc. It’s highly automated – although there are still a few manual steps. The second level of “dog-fooding” happens at this time. Before we deliver a new release to the general public, we want to make sure to expose it to a wider internal Atlassian usage. For this we have stash.atlassian.com. Deployments to this server are also now mostly automatic. This long explanation was to send one major point across: branching model and build pipelines don’t necessarily have to be paired together in complex ways. How do you guys organize branches and deployment pipelines? Comment here or tweet at me @durdn and or @atlassiandev .", "date": "2014-12-18"},
{"website": "Atlassian", "title": "Using Stash and JIRA for development bliss", "author": ["Tim Pettersen"], "link": "https://blog.developer.atlassian.com/stash-jira-developer-workflow/", "abstract": "A couple of weeks ago I ran a webinar on how to enhance your Git development experience with JIRA and Stash. The half-hour allocated for Q&A wasn’t quite long enough, so I’m following up on some of the unanswered questions in blog form. If you didn’t catch the webinar, it’s now live on YouTube : Check it out if you’re interested in: For those who attended, thanks for the great questions! Here are the answers to some of the ones that I missed during the session: Can you customize which JIRA issue state to transition to when a branch is updated in Stash? (Lucy C) Yes. The JIRA/Stash integration makes no assumption about the structure of your JIRA workflow. Trigger configuration is built into the JIRA workflow editor so you can choose which repository events to listen to, and which transitions to trigger, based on your own workflow requirements. Does the new release screen warn you if your release branch contains code references issues that are NOT assigned to the release? (Agustín M B) Not yet . The release screen has been built in an extensible manner though, and warnings based on branches and assigned fix versions are a logical extension of what we’ve built so far. It’s likely that you’ll see these and other new release warnings in future releases of JIRA. Can I get a copy of the example development workflow that you demonstrated? (Thomas R) Sure thing! The workflow I demonstrated is a simple development workflow with separate states for review, testing, and waiting for deployment. Workflow triggers are bound to a couple of different transitions: The workflow is available as a JIRA Workflow Bundle or as XML . Does JIRA have an API I can use to replicate the Stash integration for other repository hosting solutions? (Hangsu M) Yes and no. Both JIRA and Stash are built on top of a rich plugin system that allows extension. An external developer theoretically could extend JIRA to have similar integration with third-party repository hosts, but it would be a non-trivial amount of effort to develop and maintain such an integration. Practically speaking, you’re likely to get the best JIRA and Git experience by using JIRA and Stash together. How are tests integrated to automatically run? (Josh L) My colleague Steve recently gave a great webinar on Stash & Continuous Integration which answers your question in more depth, but briefly: A developer creates or updates a branch in Stash. Stash notifies your CI server that the repository has been updated. Your CI server builds the changes and posts the results to Stash. (Optionally) Stash will prevent the branch from being merged into master until there is a successful build. If I am a current JIRA user, can I import my existing Git repositories into Stash? (Alex G) For sure! Since Git is a DVCS, migrating to a different Git hosting system is ridiculously easy. All you need to do is push your repository to Stash, and your linked JIRA will begin indexing your commit, branch, and pull request metadata automatically. Where does that Progress button live again? (Ryan M) To preview the new project progress view, you’ll need to install the JIRA 6.4 EAP and start your JIRA server with the following flag: The Progress link will appear on the left hand menu on your project view . When you create a feature branch, is a build plan for the branch automatically created in Bamboo? (Trey S) Yes! Bamboo’s automatic plan-branching handles this for you. The Stash & Continuous Integration webinar I mentioned above has more details. We are currently using a Git repository to store our issues. If we want to move to Stash/JIRA, what do we need to do in order to integrate the existing data? (Yair H) JIRA has built-in importers for various competing issue trackers. There’s also a CSV importer you can fall back on if there’s no explicit support for your current issue tracker. Does JIRA support deleting feature branches in Stash after they are merged? (Andrew S) Stash will offer to clean up your branch for you when you merge your pull request. Can I have Stash with the cloud versions of JIRA and Bamboo? (Luciano P) Yes, provided your Stash server is publicly addressable so that JIRA and Bamboo can reach it. Is it possible to distribute Git commit hooks with Stash? (Tom M) The most convenient way that we’ve found to distribute client-side Git hooks is to commit them to the repository and then symlink them to your .git/hooks directory. Stash also has a Repository Hooks API for implementing server-side hooks. In Stash, is there a way to prevent “ninja” commits directly to master? (Jamieson E) Yes! Stash branch permissions allow you to prevent direct updates to specific branches or branch patterns. How does Stash associate a pull request with an issue? (Mike K) A pull request will be associated with an issue if it: contains a commit that references the issue key in it’s commit message; contains the issue key in it’s title; or contains the issue key in it’s source branch. See Understanding triggers for a deeper explanation of how this works. How are you structuring the relationship between feature branches and builds such that you see when builds fail for a given issue? (Reuben K) Stash knows which commits relate to which issues and which commits have failing builds so long as you include the JIRA issue key in the commit message. If the most recent commit that references a particular issue has a failing build, then that issue is considered to have a failing build. How does Stash handle merge conflicts? (Jesse T) If there is a conflict, Stash will display the conflicting changes in the UI and prompt you to resolve them using your IDE. What happens if the pull request is rejected? (Tomas M) It’s up to you 🙂 There is a “Pull Request Rejected” event that you can bind to whatever issue transition makes sense for your teams workflow. Did you tell us that JIRA and Stash love each other and then say that they’re brother and sister? (Michael K) Err, yes . Platonic, familial love. Though JIRA and Stash get along better than some of the siblings I know. If you have further questions, feedback, or ideas on what you’d like to see in future webinars, please ping me on Twitter – I’m @kannonboy – or leave a comment below. If you want to hear more about Git, developer tools and what’s going on in Atlassian engineering, you can subscribe to our blog here .", "date": "2014-12-17"},
{"website": "Atlassian", "title": "How we designed our Kubernetes infrastructure on AWS", "author": ["Nick Young"], "link": "https://blog.developer.atlassian.com/kubernetes-infra-on-aws/", "abstract": "When it comes to infrastructure, Kubernetes and its associated ops discipline ClusterOps are definitely the hottest new tech to burst onto the scene for quite a while. Now, sometimes that means that something’s just generated a lot of hype, but in Kubernetes’ case, the hype is deserved. The project is moving very fast, has a friendly, active community, and is already an amazing system. The aim of this blogpost is to tell you about what we’ve done to run Kubernetes on AWS in a (hopefully) scalable, reliable, and repeatable way and explain why we did it like that. Kubernetes is an open-source project aiming to make it possible to run your Docker (or other container format) application in a scalable, reliable, and standard way across as much compute as you care to dedicate to it. It’s based on the work done at Google on Borg and Omega , and has been started and given an initial push by Google. But the project has definitely taken on a life of its own and has active contributions. Not only from a lot of individual contributors but also from other large companies like Redhat, and Huawei, with lots of other companies building products and services on top of it as well. Our team is the Kubernetes Infrastructure Technology Team. ( KITT for short 😉 , so we stuck with a Knight Rider theme for naming.) Before the formation of our team, many teams in Atlassian had started using Kubernetes. We spun up this team as a way to ensure we used economies of scale to our advantage by having one team build and run the diverse Kubernetes clusters that had sprung up, and also to allow other teams to use the power of Kubernetes’ declarative configuration and reconciliation loop without having to learn all of the fiddly bits. The specifics of what we do with Kubernetes is the story of how our team has evolved in the last year. We wanted three things: To that end, we decided that we would break-up the infrastructure into three layers each having defined demarcation points: administrative constructs, like certificates and base security groups) Once we decided on this layer-cake, we started by designing the first layer, which we named the FLAG . Our idea for the FLAG was to specify our configuration using a three-tuple of details: customer, environment, and region. Here, ‘customer’ means ‘internal customer’ – this platform is for Atlassian internal use only, ‘environment’ is an arbitrary designator (most environments use some variation on dev/stg/prod), and ‘region’ means AWS region. We used Terraform to implement the FLAG, as we wanted to have the ability to see what infra changes a particular config change will create before it’s deployed, as this was a requirement for our internal SOX-certification process. (remember the third goal!) As we built the platform, we discovered an additional requirement: There should be as few as possible (preferably one) API endpoint for customers to talk to. We preferred to follow the model of having single Kubernetes clusters exist inside only one AWS availability zone (AZ). (We pulled an idea from some stuff we’d read about GKE and called these ‘cells’). The intent here was to minimize blast radius, to make failure domains easy to understand and reason about, and to treat both compute instances and entire clusters as cattle. If something went seriously wrong in a cluster, we wanted the ability to throw it away and re-deploy a new one. With respect to High Availability (HA), redundancy, and Disaster Recovery (DR), our intent was (and is, eventually) to use cluster Federation. This will one day provide our internal customers a single endpoint to talk to, and the facility to select their own availability requirements. However, for now, Federation does not meet our requirements, and so, we’ve decided to change our cells to be able to be split across AZs. Luckily, our networking model as described below supports both use cases. So, recapping our three design decisions: Next on the task list was figuring out how we would design the network subnetting. In doing this, we wanted to ensure that it would be as scalable and extendable as possible. However, in doing the research for this, I found that the Kubernetes project provides very little to no information about designing the network that it runs on. We needed to spend a lot of time digging into detailed documentation to find what the important numbers to take into account are – this blogpost is also an attempt to make some rough info about this available outside Atlassian. For the diagrams below, I’ve included both variations – one cell == one AZ and one cell == three AZs. They’re broadly similar, but could serve as a starting point for your own networking design. We used 10.0.0.0/16 as the subnet for the VPC as a whole for purposes of illustration – we obviously don’t use that for real: If looking at this gives you a headache, you’re not the only one! A lot of this requires explanation. First, the size of the boxes is not related to the size of the subnet – there’s no way to make a remotely readable diagram spanning that many powers of two! Second, you can see how we’ve allowed for six cells across three AZs. In regions where there’s only two AZs, we will only need four cells. Providing for more than one Kubernetes cell per AZ allows for scaling in the event that a cell hits some capacity limit. I’ll explain more in a minute, but, in terms of IP addressing, we are able to scale to about 63,750 pods per AWS region. We expect we will not need this any time soon, but ‘640k should be enough for anyone’ is always in the back of our heads. Now, the multi-AZ-cell diagram: We’ve re-used a lot of the configuration we built for the single-cell case here. Important case-specific things to note here: We can still fit two cells into a FLAG, yay! (For all the network subnet talk in the rest of this post, I’ll be using CIDR notation , that is, /x to indicate the subnet size) There’s no entry for the network that contains the actual pod IP addresses. That’s because we allocate one /16 for that, and reuse it across cells, marked explicitly as non-routable everywhere we can. This is for two reasons. First, we wanted to do whatever we could do to teach people the new rule that Kubernetes strongly implies – namely that an IP address is not a valid identifier any more. We assign IP addresses to pods in a transient and reusable way, so we need to get people out of the habit of using IP addresses as static identifiers. Reusing the pod network and making it non-routable is a hard enforcement of this rule. Second, we didn’t want to have scaling problems about the number of pods in a cell for a long time. Initially, we started out using Flannel , and because of its requirement for having an equally sized subnet assigned to each worker node, using a /16 for the pod network allowed us to have 250 nodes running up to 255 pods each, for a total of 63750 pods, network-wise, in each cell. (Obviously, there are other capacity constraints here, especially the default 100-pod-per-node limit, but again, the intent was to allow this at a network level, and figure out the rest once we actually had a platform.) We’ve allocated some reasonably-sized networks in each AZ for three types of extra resources: The intent here is to situate resources required for things running on the cluster running as close as possible to the pods that need them. We also allow clear subnets for things like Open Service Broker providers to drop into. Separating these by functional type also allows for easier construction of network ACLs – we can block access to everything but datastore access ports (postgres, mysql ports, etc.) from the node subnets, for example, to make accessing the management interfaces of any instances in there harder. This pattern doesn’t change between the single-AZ and multi-AZ use cases. As before, we were trying to make sure that addressing wasn’t a scaling limitation, so ensuring that ~4000 addresses were available for each of these subnets made us more confident that the scaling limits we run into will be a bit down the road. We’ve given each single-AZ cell a /23 in total for all its node and other infrastructure bits, including both managers (API Server nodes), etcd nodes, and Worker nodes. Management and etcd nodes live within the /24 management network, and Worker nodes inhabit another /24. (This is where the maximum of 250 nodes per cell originates). Obviously, a multi-AZ cell has three times the nodes available, but I think we will run into other capacity limitations long before we need ~760 nodes. In the single-AZ case, we’ve allocated each cell a /21 network (2040 addresses) for its service network, and in the multi-AZ case, we use the first /21 of the single-AZ service networks. Each Service object on the Kubernetes API needs an address from this subnet, so we wanted to allow for quite a few – we’re assuming that the service:pod ratio will be below 1:10 at least. No basis for that, just a guess. Also, since doing this design, we’ve seen from Haibin Xie and Quinton Hoole’s talk at Kubecon EU ( https://youtu.be/4-pawkiazEg ) that, at least until the fixes they worked out are merged, more than that is not a good idea. We’ve been using this structure for about 8 months now, and have finally got to a stage where we have environments up and running that will be able to meet our compliance needs. They’re not quite ready, but they’re close. I hope that this can help others who are somewhere along their Kubernetes journey, and I’m happy to talk further about it either on twitter (@youngnick). Also, keep an eye on this blog for further posts about how our build and deploy workflow, how we do auth* for our clusters, and what we’re doing to secure the cluster internally.", "date": "2017-07-04"},
{"website": "Atlassian", "title": "Getting Git Right Hangout – December 9th", "author": ["Tim Pettersen"], "link": "https://blog.developer.atlassian.com/getting-git-right-hangout/", "abstract": "We’ve just completed another round of the Getting Git Right tour, spreading the love about the hottest DVCS across ten cities in North America and Europe. To cap off the tour, the presenters are getting together for a special Google Hangout to talk about developer workflow and the latest happenings in the world of Git. Topics to be discussed are: There’ll be plenty of time for questions from the audience, so please join us on December 9th with your Git gripes (or good words), workflow woes, and anything else you’d like to hear from Atlassian’s Git experts: @durdn , @tarkasteve , @devpartisan , @GraceFr , and myself ( @kannonboy ). Feel free to post questions in advance on our Google+ page . If you couldn’t come along to the Getting Git Right tour, we’ve posted a recording of the Seattle event on YouTube. Enjoy!", "date": "2014-12-03"},
{"website": "Atlassian", "title": "Git 2.2.0 is out!", "author": ["Tim Pettersen"], "link": "https://blog.developer.atlassian.com/git-2-2-0-released/", "abstract": "After a few short months git 2.2.0 has been declared final. This is big news, as it comes with a host of new useful features for improving your git workflow. Here’s the stuff we found useful at Atlassian: git archive is a command that lets you generate a zip or tarball of your repository contents at a particular revision. At Atlassian some teams use it to distribute the source code of our tools to our customers. As of 2.2.0 you can limit which files are included in the generated archive by using a pathspec (similar to an antglob ). This can be useful for extracting files of a certain type from your repository – such as documentation or static web resources. For example, you could extract the HTML man pages for git 2.2.0 into an archive named docs.zip by running the following command in the git project’s repository: For first time git users, git config will now auto-generate a .gitconfig with a username and email address based on their username, hostname and environment variables (like $EMAIL , if it’s set). This doesn’t affect existing users, but it’s nice to see the ongoing commitment the git team have to smoothing out git’s on-boarding experience. git stash is a handy little command that bundles up changes from your working copy and stashes them away so you can quickly switch context to work on something else. git stash list can later be used to display what you’ve previously stashed away. This is great! I’m perpetually distracted and contribute to several different projects, so recalling what’s on my stash stack isn’t always easy. However, the default output isn’t always as useful as it could be: Since I usually run git stash whilst under some pressure to fix something quickly, I don’t always come up with the most descriptive messages (or, as in the case of stash@{0} above, I don’t specify one at all and git substitutes in “WIP”). These messages are transitory in nature, so they don’t matter in the long run, but it does make recalling exactly what’s been done in each “stash” a little tricky. According to the documentation , git stash list can be passed any option that you might pass to the git log command to format the output. Armed with this knowledge, you might reasonably think that you could pass the -p flag (or --patch ) to git stash list to print out the diffs of each of your stashes. However, prior to git 2.2.0, you’d be wrong. The reason git stash list -p didn’t display anything useful is due to the way a stash is stored by git. Just like almost everything else in git, it’s stored as part of the DAG . A stash is actually stored as two commits: a commit containing any changes that were staged when you ran the git stash ; and a commit containing any changes that hadn’t been staged when you ran git stash . This commit has the “staged changes” commit and your previous HEAD as parents. If you’ve stashed something recently, you can see this structure yourself by running: Because the stash is a merge commit with two parents, -p will refuse to generate a patch because it doesn’t know which parent to diff against. As of git 2.2.0 git-stash.sh (that’s right, git stash is implemented as a shell script) passes the -m and --first-parent flags on to the underlying git log command, so you can now use git stash list -p to show the actual patches that you’ve stashed away: Not so much a workflow feature as it is useful for those who develop git-related tools (like us!). git fast-export now accepts an --anonymize flag that replaces potentially sensitive repository content with innocuous strings. This means our support engineers can get customers to send us anonymized versions of their repositories when they run into problems with git or our software. The new flag is also useful if you run into a hairy git problem and you want to post your repository structure on a forum without exposing your actual source code. This item is a big win for security minded folks. Git has supported signing tags and commits for a while, which verify that a particular person created a certain ref or commit. Mike Gerwitz tells a great git horror story about why you might want to deploy one or both of these techniques. As of git 2.2.0, you can sign the actual act of updating refs which occurs during a push. This is an important step in preventing man-in-the-middle attacks and any other unauthorized updates to your repository’s refs. git push has learnt the --signed flag which applies your GPG signature to a “push certificate” sent over the wire during the push invocation. On the server-side, git receive-pack (the command that handles incoming git pushes) has learnt to verify GPG-signed push certificates. Failed verifications can be used to reject pushes and those that succeed can be logged in a file to provide an audit log of when and who pushed particular ref updates or objects to your git server. GPG signing of pushes (or commits and tags for that matter) involves quite a bit of co-ordination overhead and the security benefits aren’t worth the hassle for many teams, particularly if you’re already hosting on a secured server with audit logging capabilities (such as Atlassian Stash ). But if you’re interested in learning more, check out the man pages for git receive-pack and git push for details on the new signing options. Invoking git help everyday now displays the “Everyday Git in 20 commands or so” man page that’s bundled with git. This is a decent reference, but can be a little brisk for developers who are relatively new to git. If you’re looking to boost your git knowledge, I’d recommend checking out our git site at atlassian.com/git for usage patterns and workflow advice, and then diving into the Pro Git ebook for a deeper look at how git works under the covers. If you’re into git aliasing, you may have already pimped out your git log command with a pretty format. Filipe Kiss has put together a colorful log alias that incorporates many of the important elements of your git history. Previously, the only way you could incorporate ref names in your log output by using the %d symbol, which expands to \" ($refnames)\" . This is awkward if you want to use something other than braces in your otherwise highly customizable log output format. As of 2.2.0, the %D symbol substitutes to a plain comma-separated list of ref names (tags, branches and HEAD ) or the empty string if there are no refs pointing at a particular commit. As usual, the git team have managed to squeeze yet more performance from their already blazingly fast VCS, and have rectified some issues from earlier releases. For the full list of changes you can view the release notes or check out the repository yourself and see what’s changed between tags with: If you’ve enjoyed this update, feel free to follow me on Twitter (I’m @kannonboy ) for occasional updates about git and musings about developer workflows.", "date": "2014-12-01"},
{"website": "Atlassian", "title": "So, you want to create an Atlassian Connect add-on in Java?", "author": ["Norman Atashbar"], "link": "https://blog.developer.atlassian.com/connect-for-java/", "abstract": "You have a cool idea for an Atlassian Connect add-on and, like me, you are still in love with Java. You start looking around to see how you can build one, and you end up landing on the Spring Boot starter app . It’s cool, you can get started developing an add-on in minutes, only by running a single command like this: It gives you a simple project in Spring Boot, which will start in a container and can be installed in Atlassian products. It is perfect, except it makes a big assumption. It assumes that you are going to start from scratch and build your product around the connect framework. Chances are you have a starter application which wires in all the things you decided to have in your stack. I have one of those here . This project is based on Spring Boot and React.js, has tests in different layers, uses isomorphic views, uses Bootstrap and LESS for views, and a few other things. So, I’m very unlikely to start from scratch. Fortunately, there is a section that talks about modifying an existing project, but since Spring Boot is a little magical you won’t know which bits and pieces you will need to modify in order to \"Connectify\" your existing application. In this blog post I will walk you through some changes I had to apply to my application in order to add Connect capabilities to it. Here are the changes I had to make, all in one place . Let me walk you through what I had to do. Add some dependencies to the Connect library. Add Liquibase configuration of Connect library to your existing configuration. Spring Boot starter app for Connect exports the changelog file in a specific path and uses a properties file to configure the path, overriding the default path that Spring Boot uses to locate the .yaml file. You might not need this, but I learned that I had to do this so that Spring Boot can load both my own and Connect’s configuration. If you don’t use Liquibase, you can exclude it when you are defining the JPA dependency. Otherwise your tests might fail for not being able to find the schema definition. Connect library exports the schema definition as db.changelog-master.yaml, make sure your definition file’s name doesn’t conflict with it. Add Connect descriptor to the project resources. Please note that the ‘baseUrl’ is a publicly addressable HTTPS endpoint. We’ll get to this later. Get rid of the main controller, handling root path, since it is already handled in Connect by redirecting to the descriptor. Ignore security for the paths that aren’t directly related to the add-on, and therefore don’t require a JWT token. Connect library configures your Spring Security library to expect a JWT token, which will be provided by the host application your add-on runs in. Up to this point, my add-on doesn’t do anything useful. With a few changes I could make it do something, like adding a new page to JIRA with a link in the navigation bar. I won’t go through that in details, as you can see the changes here . It is pretty much adding a module to the Connect descriptor, saving JWT token in the view, and using it to invoke REST resources. There are complete instructions in the Connect documentation , but the highlights are as follows. For my application, the instructions are here and it is as easy as this: Once it is started, you can check it out at http://localhost:8080 and expect to get the Connect descriptor back. In order to install the add-on on any Atlassian product, it needs to be reachable at a valid HTTPS address. Use a local tunneling tool like ngrok to map a proper URL to your application. Once you have a proper URL, you can modify your Connect descriptor so it advertises itself properly. Complete instructions on this topic is here. Hopefully you are familiar with the installation of add-ons in JIRA or Confluence. Otherwise please check the instructions here . Happy coding!", "date": "2017-03-07"},
{"website": "Atlassian", "title": "Why I was wrong about code reviews", "author": ["Steve Haffenden"], "link": "https://blog.developer.atlassian.com/why-i-was-wrong-about-code-reviews/", "abstract": "If you’ve ever had to go through a process of code review I’m sure you’ll be familiar with the time honoured tradition of raising a pull request and having your hard written code critiqued. This is something that I’ve traditionally disliked. It’s slow, it’s distracting, and I’ve never been a fan of people telling me I’m wrong. However, being new to the team I thought it best to do things the right way and I found myself relying heavily on the code review process. It wasn’t just a rubber stamp on my work, but a proces that helped me identify inconsistencies in my code style and if I’m honest to validate that I was still capable of writing Java. There’s a UK hiphop artist, Scroobius Pip and you’d be forgiven for not being familiar with him, but in the context of this blog post he’s important for penning the following lyrics: If your only goal’s to be as good as Scroobius Pip Then as soon as you achieve that your standards have slipped If your goal is always to improve on yourself Then the quest is never over no matter how big your wealth You see, it’s important that we all strive to be better at everything that we do. That doesn’t mean comparing ourselves to others, but rather taking the time to look at ourselves at to compare the way we do things now against the way we did things yesterday and the way we want to do things tomorrow. Improving our own skills and techniques is just good practice and it makes everyday a challenge. Code reviews are an integral part of development culture at Atlassian, but it’s easy to treat them with a certain amount of disdain. How often have you found yourself on the receiving end of a code review and only giving it a cursory glance before clicking the approve button or waiting util someone else completes a code review before submitting your own approval without checking the changes? And when you create a code review do you take the time to provide context around your changes or setup an instance to allow your reviewers to validate your changes easily? Have you ever had a code review where you’ve seen the feedback you received as a criticism of your ability rather than an opportunity to hone your skills? Now obviously, these are some fairly sweeping generalisations after all, every code review is unique but it’d be surprising if we hadn’t all had experiences similar to those listed at one time or another. The thing is, different people will identify with the role a code review plays in the different ways. A method of identifying errors in our code, a form of testing, a rubber stamping process, and human linting are all perceptions that might be conjured up when talking about this task. In order to make the most out of each code review it’s worth considering the benefits to our own developmet, or to put it more succinctly: How does this review to help me grow as a developer? The crux of the matter is that the code review process is a great opportunity to take stock of our work. It might be finding better, more effective ways of implementing solutions or practicing the valuable art of providing concise, easy to understand feedback. Code reviews could also help in improving our depth of knowledge in a specific product, library, or language; they may even just increase our ability to read, understand, and reason about other peoples code. There’s always something new to learn in even the most mundane of code reviews. Next time you’re starting a code review — whether it be as a reviewer or as someone creating a review — start by asking yourself how it can help you to grow as a developer. Hopefully it will lead to a more fruitful experience for you and everyone else involved and, as we all grow the quality of the things we produce will grow too. This post is featured in our ebook, \"Hello World! A new grad’s guide to coding as a team\" – a collection of essays designed to help new programmers succeed in a team setting. Grab it for yourself, your team, or the new computer science graduate in your life. Even seasoned coders might learn a thing or two. Read it online now Click here to download for your Kindle", "date": "2015-07-21"},
{"website": "Atlassian", "title": "Using ECMAScript 6 features today", "author": ["Jason Berry"], "link": "https://blog.developer.atlassian.com/using-ecmascript-6-features-today/", "abstract": "ECMAScript 2015 (6th edition, commonly referred to as \"ES6\") is the current version of the ECMAScript standard, and is a significant update to the language. These features are currently being implemented in major JavaScript engines, but why wait when you can make use of all the awesome today? I've been keen to use many of ES6's features in my day-to-day development, especially the modules syntax, and destructuring – which is particularly useful in doing away with the option object cruft that JavaScript developers have become used to writing in lieu of native support for named parameters. Destructuring allows us to take something like this: … and write it far more succinctly: This post details how Atlassian account (our single sign-on service) transitioned to using ES6 modules, and what we use to provide a full ES6 environment for development. Here's a brief look at the juicy goodness ES6 has to offer, courtesy of Luke Hoban's excellent ES6 features overview (recommended reading for in-depth descriptions and examples): arrows classes enhanced object literals template strings destructuring default + rest + spread let + const iterators + for..of generators unicode modules module loaders map + set + weakmap + weakset proxies symbols subclassable built-ins promises math + number + string + object APIs binary and octal literals reflect api tail calls Up until this point JavaScript has not had native support for modules and so the community has come up with some pretty nice work-arounds, the most popular being Asynchronous Module Definition (AMD) – designed for asynchronous loading of modules and dependencies in the browser – and CommonJS – the most common implementation being in Node. ES6 modules took the best of both worlds and came up with syntax which should make both camps happy (and lends itself nicely to static analysis to boot). Just used ES6 module syntax the first time. Very well thought-out. Screams \"argued over for many nights by very intelligent people\". — Dan Abramov (@dan_abramov) March 1, 2015 However, because of the lack of browser support for ES6 modules we still need to compile to something the browsers can understand in the meantime. For us, that meant using AMD – although I've been experimenting with SystemJS (and jspm , built on top of it) to potentially do away with the need to convert the modules to AMD. The two main ES6 to ES5 transpilers are Babel (formerly named \"6to5\") and Google's Traceur . We chose to use Babel for a few reasons: Traceur requires a heavyweight runtime, Babel only requires a few optional polyfills the code generated by Babel is closer to the original source Babel is a little more feature complete the web community's engagement with Babel is massive, and activity is extremely high Atlassian account is a relatively new code-base, so from the outset we've been able to use a modern front-end stack, harnessing all the power that Node and npm provide: Grunt Less AMD via RequireJS Karma test runner ESLint With this toolchain we use Babel via the grunt-babel plugin, transpiling our ES6 modules down to ES5 compatible AMD modules with Babel's AMD module formatter. The generated code ended up looking more or less like what we already had, but now we were using ES6 modules under the hood! Using the grunt-babel plugin it was trivial to use a glob expression in the Grunt configuration to specify the ES6 modules we wanted transpiled to ES5 compatible AMD modules. Babel transpiles most of your code down to being ES5 compatible, but in order to avoid lots of duplication it relies on the core-js polyfill for much of the newer functionality, and a fork of Facebook's regenerator to provide support for generator functions. Babel bundles this as a single browser-polyfill.js that you include before any of your compiled code. I included it just after the RequireJS config in both our base template and our Karma configuration. Attempting to include browser-polyfill.js more than once will throw an error, so ensure it only gets included once. While the default Babel polyfill does an excellent job at polyfilling ECMAScript 6 features, we had a few issues with some of the other things it polyfills: ECMAScript 5 features, which are unnecessary for the minimum browser versions we support ECMAScript 7 proposals, which are quite likely to change or be dropped entirely non-standard language features (e.g. window.setImmediate ) For this reason I built a very simple drop-in replacement for Babel's polyfill (which still uses core-js and Babel's regenerator fork), babel-es6-polyfill , that only includes the ES6 polyfills. When developing it's always nice to be able to debug your un-minified, un-batched source code and source maps provide a convenient way of being able to do that. Thankfully, with Babel this is as trivial as setting the sourceMap option and you're done. Depending on the folder structure of your project you may also want to set the sourceFileName option, too, so that you don't have to expand a bunch of folders in the browser's dev tools to see the original source. In converting over our tests to ES6 I used the karma-babel-preprocessor . Configuring Karma (via the configuration file, usually named karma.conf.js or similar) to preprocess your tests with Babel before execution is straightforward: You'll notice that we've enabled source maps for the tests too, which will help if you find the need to debug them. Given that JavaScript is a dynamic and loosely-typed language, it is especially prone to developer error. In order to mitigate this we use the static analysis utility, ESLint, to find problematic patterns or code that breaks certain conventions and guidelines in our codebase. ESLint allows you to specify environments that define which global variables are predefined and which rules should be on or off by default, as well as individual language options by use of the ecmaFeatures property. To enable a full ES6 environment requires a straightforward configuration: We may not have hover-boards yet, but we can use ES6 today. What are you waiting for? 😉", "date": "2015-07-16"},
{"website": "Atlassian", "title": "Three detailed HipChat changes we needed to help us scale during hyper-growth", "author": ["Nick Wade"], "link": "https://blog.developer.atlassian.com/hipchat-connections/", "abstract": "In today’s software markets it’s increasingly important to be agile and have the ability to release fast and often.  The recent release of the new HipChat web client shows us why in a neat example. In a recent blog post from our HipChat web engineers they detail the issues they saw when they brought their new architecture online for the first time at scale. Since the new web client became instantly very popular, HipChat saw some performance issues after the initial launch.  Issues like mass reconnections deadlocking servers and redundant DB connections that might not show up in local environments can really be a problem when at load and at scale. Previously, HipChat's web client attempted reconnection every 10–30 seconds following a disconnection. This time around, we wanted a better experience: reconnecting as \"automatically\" as possible, hoping users never noticed a thing. To do this, we decreased the connection retry from 10-30 seconds, down to 2 seconds. This drastically shortened time, combined with a surge of new users, strained our system. The initial reconnection attempts were too aggressive for the amount of traffic we saw. So, our first action was to quickly update the back-off rate and initial poll time to be more reasonable. Being able to release fast and often made this an easy fix. Then a related issue popped up when a session node failed and all the disconnected users tried to reconnect at once. As always, things get complicated when we consider this at scale (webscale). Let's say a large number of clients become disconnected at once due to a BOSH node failure. All of the clients on that node then tried to reconnect at the exact same time based on the connection retry setting outlined above. We've effectively just bunched all the reconnection requests into a series of incredibly high-load windows where all of the clients compete with each other. What we really want is more randomness. We implemented a heavily jittered algorithm design. This gives us the benefit of having the least number of competing clients, and encourages the clients to back off over time. Again being able to see this in real-time and pushing out a fix quickly minimized the chance of this occurring again. Finally, the last issue that cropped up was through normal monitoring.  Engineers noticed that the load seemed to be double what is normal. Since we knew session acquisition was our biggest pain point, we combed through our connection code, looking for ways to make it less expensive. We noticed that it was double-hitting Redis in some cases. A fix was quickly deployed. Developing for architecture that runs at scale and is always rock-solid is hard . Adding hyper-growth to that scale makes it even harder.  Being able to monitor your site in real-time and deploy changes and fixes fast are now essential. Otherwise you risk driving users aways due to poor performance that doesn’t get fixed quickly. Since we made these changes, distribution of load on our system has been much improved. For all the results and pretty charts, the full post is well worth the two-minute read on how we’re scaling a big, modern web service. One  that’s now passing billions of messages for our users per year. Thanks to Atlassian’s Open Company – No Bullshit value we all can learn from HipChat’s experiences and ability to be agile.", "date": "2015-04-24"},
{"website": "Atlassian", "title": "The new SVG HipChat loading screen – faster without React", "author": ["Dave Elkan"], "link": "https://blog.developer.atlassian.com/the-new-svg-hipchat-loading-screen-faster-without-react/", "abstract": "We love gifs on the HipChat team. We love them so much that we were temporarily misled by their sirens’ call. Here’s how we plugged our ears and continued on to the fabled land of SVG, optimizing our loading screen and reducing its size by 95%. Our original loading screen was a thing of beauty. A pure white screen sporting an animated, retina HipChat logo rolling over its tail into the future and beyond. This retina gif’s size (a chunky 53KB) was the source of its demise. It was simply too big for a loading screen, often arriving late and missing the party entirely. Size aside, the old loading screen was a part of the greater web app. This meant it would not appear until React and all of the other dependencies of the web client were loaded. That sounds counter-intuitive, but when we first launched the beta, the javascript payload was smaller. The loading screen was simply used to hide the UI whilst we fetched the initial HipChat session data. Since then we’ve added countless features, like inline file previews, increasing the download size. We’ve gone one better. The loading screen now loads before React. Infact, the SVG HipChat logo and animation CSS are inline in the HTML, so there’s no additional requests to make to show the loading screen. New users are precious and first impressions last. If you have to keep them waiting, captivate them. Quickly. After our initial web client beta release, we conducted an intense study of the new user on-boarding flow. It was then that we discovered our gif issue. In production, new users with an empty cache on a typical internet connection, often didn’t see our fancy loading gif at all. Our new web client already uses SVG symbol definitions for our status icons, so it was a no-brainer to add a HipChat logo along-side them. With this HTML and CSS: It looks a little like this: Advanced CSS animations are tricky. The animation loop needed to account for the logo’s tail striking the ground. Piecing something intricate like this together with the provided transition types (ease-in, ease-out, etc) is a great recipe for a self-induced lobotomy. Instead, thanks to Joel Unger on the design team, I was able to create rotation and translation keyframe rules from the original After Effects timeline. The loop is 49 frames at 29.97fps giving us a total length of 0.6 seconds. From here we can create percentage-based keyframes. i.e. At frame 22 (46% through the loop), when the tail hits the ground, we want to apply a rotation of 20 degrees and translation of 7 pixels up. * Please note this snippet does not include browser-prefixes currently required to work in all browsers.** From a 53KB gif to 1.8KB of HTML and CSS (both sizes gzipped). You can find all of the code for this CSS animation by viewing the source of this blog post (or by signing up for a HipChat account ).", "date": "2015-03-17"},
{"website": "Atlassian", "title": "Start the year with an effective QA process", "author": ["Glenn Martin"], "link": "https://blog.developer.atlassian.com/agile-qa-practices/", "abstract": "Ok, we're almost 2 months into the new year, so we're not exactly starting it anymore. By now, you're already feeling guilty about not using that gym membership you bought on January 2nd, and you've slipped more than once on your resolution to make the bed every day. (Every. Day.) So here's a chance to make a new New Year's resolution: stepping up your QA processes. This guide will help you better understand your team and make better decisions based on the domain you’re working in, then analyse your progress and make further changes based on what you've learned. Maybe you’re reading this and thinking \"Our QA game is just fine, thanks.\" And maybe it is. Maybe. But let me ask you this: Have you recently gone through a bad release? Are you just \"hoping\" it will go better next time? Most of the time you move from release to release, without any reflection on how your QA process is doing or even know how to measure it – which is a pretty good definition of \"insanity\". So enough hoping. Let’s have more doing. The first step is to analyse the team. I'm not just talking about the people on your team, but also the type of work you're doing and the whole domain you're working in. Is the work involved very front-end or back-end heavy? Do you rely on external APIs? Will you be integrating with other systems? Do you have a code dependency on other teams? Let’s assume your project involves integrating two existing systems within your organisation, working with two or more teams. The work will be mainly front-end oriented but there's some initial back-end work to get done. All teams involved are currently shipping on different cadences and following similar agile methodologies. Experience of members within the team is mixed from junior to senior level developers. So to take away what we’ve found about this new project: Team members might need to learn new codebase/systems Front-end heavy will mean cross-browser testing Multiple teams with different dev process are warning signs – testing the code version accuracy of multiple systems will require careful co-ordination of dependencies from the different platforms Communication could be a barrier between the two teams Based on the analysis, you now have a better understanding of how your team works and what could be some of the high-risk areas that you need to focus on. This is an example of some of the processes improvements you can implement. If you're interested, check out a more in-depth explanation of the various QA processes we use at Atlassian . Automation is a vital part in achieving fast development and testing speed. A great deal of developer and/or tester time could be wasted by constantly having to set up a test environment. Automation not only offers efficiency to your development team but could also decrease mistakes by having to do too many things manually. Have an infrastructure where your team members can quickly bring up a test environment with similar architecture to what the end users will be using. Your test environment needs to have a wide range of data, for example i18n characters, XSS attack strings, large string lengths, images, or documents – whatever content your product is designed for. You don’t want to be entering this data every time you test. What are these? Smoke tests are a subset of tests which cover vital functionality of your application that if failed will block your team from shipping. The proper way to have a suite of effective smoke tests is to have them running against an environment which gets upgraded. Not cleaned up every time and then upgraded with the latest code. Because the end user is not going to blow away their whole data and upgrade their system, right? So you need to replicate this process with some automation tests which test the core functionality of your product. Having a good suite of smoke tests reduces the amount of manual testing required and allows your team members to test the specific features that have been added. You should also add these tests as part of your release process as a kind of exit criteria. Use a dashboard to display various information to your team that help them work on a daily basis. Know how many bugs have been raised during the sprint/iteration? Status of builds? Any actions or warning signs? A properly configured dashboard is a very effective tool and most are very easy to connect to your development tools. There are several very good free dashboards out there. Atlasboard is one of these, created in-house here at Atlassian. Another dashboard option is dashing . All the above tools can be done with a scripting language. I recommend learning Ruby or Python since both are great, easy-to-learn languages with large libraries that do almost anything. Getting developer time to help you on these tools is usually difficult, so take the first step and learn to code. How do you know the above steps you have implemented are providing any benefit? Some of the metrics you'll want to check on a frequent basis include: (usually aligned with your sprint iteration) Speed of development (number of stories completed) Number of bugs Severity and types of bugs found Where in the development process are these bugs caught? Make sure you have your metrics well-defined (otherwise your numbers will be meaningless). It’s not enough just doing the above steps and thinking it’s all smooth sailing. Sometimes it can take a couple of times tweaking these steps before you get it right. So from measuring your team, you now have a better picture of how the team is progressing. I would also recommend holding retrospectives within your team, getting everyone to raise good and bad points, debating on those and coming up with some actions. Finally, remember to think of the QA process as a living organism within your team: you need to keep a constant eye on it to make sure it’s healthy.", "date": "2015-02-26"}
]