[
{"website": "Flickr", "title": "Flickr Shapefiles Public Dataset 2.0", "author": ["Neil Walker"], "link": "https://code.flickr.net/2011/01/08/flickr-shapefiles-public-dataset-2-0/", "abstract": "An embarrassingly long time ago we released the first public version of the Flickr Shapefiles . What does this have to do with Captain America and a cat? Nothing, really. Anyway, we haven’t completely forgotten about shapefiles and have finally gotten around to generating a new batch (read about Alpha Shapes to find out how it’s done). When Aaron did the first run we had somewhere around ninety million (90M) geotagged photos. Today we have over one hundred and ninety million (190M) and that number is growing rapidly. Of course lots of those will fall within the boundaries of the existing shapes and won’t give us any new information, but some of them will improve the boundaries of old shapes, and others will help create new shapes where there weren’t any before. Version 1 of the dataset had shapes for around one hundred and eighty thousand (180K) WOE IDs, and now we have shapes for roughly two hundred and seventy thousand (270K) WOE IDs. Woo. The dataset is available for download today, available for use under the Creative Commons Zero Waiver : http://www.flickr.com/services/shapefiles/2.0/ Little Johnny JSON Originally we provided the full dataset in our own home-grown XML format because, well, it seemed like a good idea. For version two we’re releasing the shapes in GeoJSON format. We think this is a Good Thing because unlike our  old XML format, at least one other person in the world already knows how to read and write GeoJSON. For example, Our friends over at Stamen Design and SimpleGeo have created a ridiculously easy-to-use JavaScript library called Polymaps which of course reads GeoJSON out of the box. With a few lines of JavaScript you can render the Flickr shapefiles and start using them without all that pesky XML parsing stuff: Or if GeoJSON doesn’t suit you you can use a free tool like ogr2ogr to convert it to something that does. Layers (photo by doug88888 ) The GeoJSON format allows grouping of features (and their related geometries) into FeatureCollection objects. A FeatureCollection seems to be roughly equivalent to a layer in a typical GIS, or a placetype in WhereOnEarth-speak . To make the dataset a little easier to manage we decided to break the shapes up into FeatureCollections based on placetype; one each for continents, countries, regions (states), counties, localities (cities), and neighbourhoods. Each of these is its own GeoJSON file in the dataset. Here’s an example of what one of our GeoJSON objects looks like: {\n  \"type\": \"FeatureCollection\",\n  \"name\": \"Flickr Shapes Public Dataset 2.0 - Regions\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"id\": 2344541,\n      \"properties\": {\n      \"woe_id\": 2344541,\n      \"place_id\": \"Cxf0SmObApi9R9T8\",\n      \"place_type\": \"region\",\n      \"place_type_id\": 8,\n      \"label\": \"Barbuda, AG, Antigua and Barbuda\",\n    },\n    \"geometry\":\n    {\n      \"type\": \"MultiPolygon\",\n      \"created\": 1292444482,\n      \"alpha\": 0.03,\n      \"points\": 118,\n      \"edges\": 17,\n      \"is_donuthole\": 0,\n      \"link\": {\n        \"href\": \"http://farm6.static.flickr.com/5209/shapefiles/2344541_20101215_724c4cae47.tar.gz\",\n      },\n      \"bbox\": [-62.314453125,17.086019515991,-61.690979003906,17.93692779541],\n      \"coordinates\": [\n        [\n          [[-61.739044,17.587740], [-61.735268,17.546171], [-61.690979,17.426649], [-61.765137,17.413546]\n... etc A file is a single FeatureCollection object which holds an array of Features, which each hold a Geometry which is a MultiPolygon, which holds an array of Polygons which in turn each consist of an array of LinearRings. Got it? You’ve Been Superseded We’ve also included a separate file/layer called flickr_shapes_superseded.geojson. This is a FeatureCollection that consists of all the WOE IDs that have been “superseded”. Occasionally (too often?) a WOE ID needs to be retired and replaced with a new one . We keep up to date with these and are always reverse-geocoding against the latest WOE IDs. However, there are plenty of old photos (and Flickr shapes) that have been assigned to one of these old IDs, and we have shapes for them (currently a little more than nine thousand). A simple solution might be to just re-assign these photos to the new WOE IDs (when a WOE ID is retired its replacement is specified), or even just re-run the reverse-geocoding process. This may not be what the owner of the photo wants; it might come out with a different result than they had at first (which they may have been perfectly happy with), if the size and location of the WOE rectangle changed (which it probably did). So it’s a problem without a clear solution. And since we have data for these old WOE IDs we’ve included their associated shapes in the dataset. In most cases there will be shapes corresponding to the new WOE IDs that will over time become more and more accurate as more photos get uploaded and end up being assigned to the new WOE IDs. But you can have the old ones too. WTF Sometimes, Clustr just gets it wrong. As mentioned in previous posts many of the Flickr shapes are just plain weird. It may be due to a lack of data (i.e. source photos), a weakness of the algorithm, an inappropriate choice of the alpha parameter or (shame!) a plain old bug. One of the things that surprised us was that Clustr was not supposed to output inner rings , or polygons with holes. It turns out that it does. In the GeoJSON output this can cause some weirdness (depending on what you use to render the shapes) since the GeoJSON is formatted with the assumption that each ring is a distinct polygon, instead of possibly one part of a single polygon with holes in it. This and other weirdness are known issues and something we shall strive to fix in the future, however we felt it best to release the existing dataset now rather than spend forever trying to get it perfect, and end up not releasing anything at all. You may also notice that unlike version 1 of the dataset, there is only a single shape per WOE ID. All of the previous versions of the shapes are still available via the Flickr API , but in the interests of keeping the file size down we’ve limited this download to just the latest versions of each shape. And as always, there’s lots more to do.", "date": "2011-01-8,"},
{"website": "Flickr", "title": "The Joy of Popup Windows", "author": ["ysaw"], "link": "https://code.flickr.net/2011/02/01/the-joy-of-popup-windows/", "abstract": "The Joy of Popup Windows As you’ve probably noticed, we’ve been working on the login process here at Flickr. In an effort to make this flow as painless as possible we’ve moved to a “contextual” login, which is to say, logging in doesn’t require you to leave the page you are looking at. We accomplished this by using a popup window. We initially considered using an in-page modal dialog box instead of a popup, but quickly dismissed this approach for two security reasons. Firstly, to prevent phishing and cross site attacks it’s very important that login forms are not posted cross domain (Yahoo!’s Membership Platform authentication happens on yahoo.com, not flickr.com). Secondly the URL of the login page should never be hidden from the user. An in-page modal fell foul of both of these concerns. A more suitable solution would be the popup browser window, once a favorite of advertisers. Our initial approach Revisiting our turn of the millennium Javascript for opening windows with the BOM, things looked simple enough: call window.open , make sure the window opened, then set a timeout to see if the window was closed: function waitForWindowClose() {\n\tif (web1999Window && web1999Window.closed) {\n\t\t// the pop-up was closed, done with auth etc.\n\t\twindow.location.reload();\n\t} else {\n\t\t// check again in a moment\n\t\tsetTimeout(waitForWindowClose, 1000);\n\t}\n}//cute function name by Scott Schiller\nfunction webCirca1999(url) {\n\ttry {\n\t\tweb1999Window = window.open(url,'newWindow','width=640,height=650');\n\t} catch(e) {\n\t\t//handle blocked popup...\n\t}\n\ttry {\n\t\t//this is to check if it really opened\n\t\tif (web1999Window.focus) {\n\t\t\tweb1999Window.focus();\n\t\t}\n\t} catch(ee) {\n\t\t// also handle failure\n\t}\n\twaitForWindowClose();\n\treturn false;\n} Unfortunately Yahoo!’s new user signup process closed our popup and opened a new one of their own. This triggered our waitForWindowClose{} function, web1999Window && web1999Window.closed evaluated to true, and the Flickr page refreshed, even though the user hadn’t logged in yet. The next thing we tried was watching window focus. When the window blurred after clicking on the login popup, we’d know the popup was open, and when focus came back we’d check the user’s login credentials. But focus tracking also had its problems. Some browsers didn’t always report the window blur event when a popup was open, resulting in inconsistent behavior. It was also possible for the user to accidentally focus back on the window and kill the flow. We tried several ways to track focus, and although behavior got better, it was never 100% reliable. For a feature like login that was obviously a problem. The Cookie Solution Ultimately we settled on a solution that forgot about popup tracking and window focus and instead concentrated on checking for the user’s login cookies. The code below polls to see if the login cookie is set, then makes an AJAX request to verify it, then refreshes the Flickr page: function pollCookie(){\n\n\tif(document.cookie.match(sessionRegex)){\n\t\tauthInProgress = false;\n\t\tcheckAuth();\n\t}else{\n\t\tY.later(20, this);\n\t}\n\n} This has the benefit of being very responsive, avoids the nightmare of focus tracking, and is low impact (as there’s no need for us to poll the server). So today we have a reliable contextual login. Our popup system also made it possible to add google and facebook login with less pain than usual.", "date": "2011-02-1,"},
{"website": "Flickr", "title": "A YUI3 Module for the Flickr API", "author": ["Eric Gelinas"], "link": "https://code.flickr.net/2011/02/23/a-yui3-module-for-the-flickr-api/", "abstract": "During the redesign of our photo page, we created a YUI3 API  module , used for all Flickr API transactions. This module has been designed to load component modules as needed, which makes for a speedy initial load. For example, the YUI3 IO module , which handles XMLHttpRequests in the YUI3 library, is loaded only if writing to the Flickr API. Read requests are all handled by the Get Module , which is included in the core of  YUI3. A byproduct of this approach is that read requests can be made cross domain, without a proxy. Reading from Flickr Simply include gallery-flickr-api in your requirement definition. Then use the Y.flickrAPI.callMethod function, passing the Flickr API method name and callback as arguments. Have a look at the YUI3 Gallery page for more information about using gallery modules. Read requests can be made cross-domain without a proxy, since the request and response will be loaded as a script resource instead of using XMLHttpRequest. YUI().use('gallery-flickr-api', function(Y) {\n\nY.flickrAPI.callMethod('flickr.panda.getPhotos',\n{\napi_key: 'your_api_key',\npanda_name: 'ling ling',\nextras: 'license, date_taken, owner_name',\nper_page: 50,\npage:1\n},\n{\nsuccess : function(response_object) {\n//Success Code\n},\nfailure : function(response_object) {\n//Failure Code\n}\n});\n\n}); Writing to Flickr Okay, writing to Flickr API (i.e. post requests) is a little trickier. You will need to make sure that your API key is enabled to read and write . As soon as you make the first API request to write, the YUI3 IO module will be loaded (if it is not already). It is important to note that write requests can only be done on the same domain, which is the standard security restriction of the XMLHttpRequest for browsers. POST requests to the Flickr API, will require some sort of proxy which will accept the post request on your domain and handle the cross-domain communication with api.flickr.com YUI().use('gallery-flickr-api', function(Y) {\n\nY.flickrAPI.callMethod('flickr.favorites.add',\n{\napi_key: 'your_api_key',\nphoto_id: a_photo_id\n},\n{\nsuccess : function(response_object) {\n//Success Code\n},\nfailure : function(response_object) {\n//Failure Code\n}\n});\n\n}); We hope that you make use of this new module. It is a great way to get quick prototypes and full scale applications made using the Flickr API in a short amount of time. See you in the App Garden !", "date": "2011-02-23,"},
{"website": "Flickr", "title": "Inspiration Tuesday", "author": ["Eric Gelinas"], "link": "https://code.flickr.net/2011/03/02/inspiration-tuesday/", "abstract": "When I poke my head above my monitor at FlickrHQ, I can see a lot of headphones. Ever wonder what inspires us to write code (besides fried chicken sandwiches and cocktails)? Well I did a little snooping in the playlists of a few Flickreenos: Zack Top Tracks: Fitz & The Tantrums – MoneyGrabber Ear Infections mix – Goldenchyld Live In San Diego – Goldenchyld Little Birdy – Brother Parliament – Red Hot Mama Ross Top Tracks: David Gray – Flame Turns Blue Kings of Convenience – Singing Softly to Me Grizzly Bear – Southern Point The Roots – The Next Movement Iron & Wine – Jezebel Trevor Top Tracks: Crawler – Cosmic Tide Quest for Fire – Next to the Fire Bison BC – Die of Devotion Radiohead – Little by Little DMX Krew – Do It All Nite Scott Top Tracks: Wax Tailor – No Pity Ray Charles – Eleanor Rigby Steve Martin – How to Meet a Girl KRS-One – MC’s Act Like They Don’t Know Quantic – Snakes in the Grass Oh, and I suppose I should share mine too: •  Blueprint Car Crash – Gun Moll •  Little Dragon – Come Home •  Angus & Julia Stone – Black Crow •  Plants and Animals – Celebration •  of Montreal – Coquet Coquette Find even more of what inspires us on our FlickrHQ Last.Fm Group Photos by waferbaby", "date": "2011-03-2,"},
{"website": "Flickr", "title": "Refreshing The API Explorer", "author": ["Paul Mison"], "link": "https://code.flickr.net/2011/06/01/refreshing-the-api-explorer/", "abstract": "Most people know that Flickr has an API. As it wouldn’t be much use without documentation , we have that, too. (There’s even a list of methods and information about each available via the API itself.) What if I told you there was also a way to experiment with it from the comfort of your browser, no coding required? Sink Explorer by Zabowski Well, that’s what Flickr’s API Explorer offers. It’s an easy way to customise requests by filling in simple form fields, whether the method requires authentication or not, and to see the responses that are returned. It’s great for one-off prototype scripts where you quickly want to find some data, for seeing whether a method does what you think it does, or to sanity-check some code that’s not doing the right thing. It’s been around for years, but nobody ever seems to have made much of a fuss about it. (The only mention I can find on this blog is an interview with a certain API developer singing its praises.) However, it’s needed a little attention to bring it up to date, and so I made some time to teach it a few new tricks. Firstly, it now offers a choice of output response. While it doesn’t offer every format that the API does, the three (and a bit) available – the default XML , JSONP (or raw JSON), and PHP serialized data – should cover a lot of ground. Secondly, the Explorer pages now have proper URLs, so it’s possible to link to the API method for fetching the list of pandas , for example. Finally, for the most popular of those response types – XML and JSON(P) – responses are now pretty-printed and syntax highlighted, as are the examples in the API documentation pages for each method. That is to say, the returned values are indented and have line breaks, while the name, attributes and quoted values of the elements are coloured appropriately. Now that you know that it exists, and that it’s all freshened up with spiffy features, why not go and play around ? Have fun!", "date": "2011-06-1,"},
{"website": "Flickr", "title": "Flickr now Supports OAuth 1.0a", "author": ["jfanaian"], "link": "https://code.flickr.net/2011/06/21/flickr-now-supports-oauth-1-0a/", "abstract": "We’re happy to announce that Flickr now supports OAuth! This is an open standard for authentication, which is now fully supported by the Flickr API. You can get started by going to our OAuth documentation . As part of this announcement, we would also like to note that the old Flickr authentication is now deprecated, and is expected to be disabled early 2012. I’m Guarding the Door by Frenck’s Photography OAuth is very similar to the old Flickr auth in a lot of ways. You start by getting a request token (frob in the old flow), redirecting the user to the authentication page, and then getting a token which can be used to make authenticated requests. With proper OAuth support, though, you will be able to use one of the many libraries available in a variety of languages to get started. In addition to this, we have streamlined the authentication process across desktop, mobile and web, and have simplified the user experience by removing the anti-phishing step for the Desktop flow, which is no longer necessary. Currently, we only support OAuth 1.0a, but we have plans to eventually support OAuth 2.0. The decision was based on the fact that OAuth 2.0 is still an evolving definition that is rapidly changing. We wanted to make the transition to OAuth seamless to the user, so we created a method to exchange an old token, with an OAuth token. The application has to simply make an authenticated request to flickr.auth.oauth.getAccessToken , which returns an OAuth auth token and signature for that user which are tied to your application. The exchange is meant to be final, so the old authentication token is scheduled to expire 24 hours after this API method is called. Now, it’s your turn! Go read our OAuth documentation if you already have an application, or visit our developer guide for more information on how to get started. If you experience any problems, or have any questions or suggestions regarding our OAuth implementation, please post to our developer mailing list .", "date": "2011-06-21,"},
{"website": "Flickr", "title": "Don’t be so PuSHy", "author": ["Neil Walker"], "link": "https://code.flickr.net/2011/06/30/dont-be-so-pushy/", "abstract": "You know three things that would be cool? the ability to subscribe to the output of a Flickr API call in a feed aggregator the ability to get the results of Flickr API calls as… Oh wait. That was a while ago. Wouldn’t it be great if you didn’t have to poll our API over and over just to see if photos were there, only to find out you waited too long since last time and now because of the results size limits you can’t get them all and you have to figure out how many you missed and then you have to make another call with the right offset to get those results but of course in between then and now the result set changed a bit so you aren’t sure if you really got them all and… Wouldn’t it be great if Flickr had something that could just PuSH photos to you as they appeared, kind of like this ? Introducing the new (and experimental) flickr.push API methods. These allow you to subscribe to new uploads and updates from your contacts and favorites from your contacts. Let’s dive right in and see exactly how it all works: The 20,000 ft overview is basically this: You make an API call to Flickr asking to subscribe to one of several different photo feeds, providing a callback URL in the arguments. A little verification dance ensues during which we make a request to your callback URL. If you respond appropriately we’re all good and from then on… Live(-ish) updates are POSTed from Flickr to your callback URL in Atom 1.0 format. Subscribing The subscription system is based as closely as possible on Google’s Pubsubhubbub protocol , with a few wrinkles. One is that Flickr acts as the hub and the publisher all rolled in to one. We’re obviously not really “publishing” separate feeds of every single user’s contacts’ photos and faves to a central hub somewhere, we only create the feeds on demand when someone subscribes to them. So we couldn’t, for example, publish them all to a 3rd-party hub like Superfeedr . But the whole pubsubhubbub metaphor still works pretty well. Another difference is that the subscription happens via an authenticated API call and not an HTTP POST ; hopefully the reasons for this are obvious. We’ll get into them in detail a little bit later. But even though the mechanism for the subscription request is different we’ve tried to follow the protocol as closely as possible and keep the parameters the same. The Google PubSubHubbub Core 0.3 section on how the subscription flow works is a good place to start, and the rest of this post assumes you’ve read that and more or less understand what the interactions should be between hub and subscriber. Done? OK, here are the methods: flickr.push.getTopics This method just tells you what you can subscribe to. It returns something like this: < rsp stat=\" ok \"> < topics > < topic name=\" contacts_photos \" />\n    < topic name=\" contacts_faves \" /> < /topics > < /rsp > yeah, yeah, you already get that part. You can currently subscribe to contacts’ photos (new uploads and updates) or contacts’ faves. So subscribe already! flickr.push.subscribe This method (which requires an authentication token with read permissions) takes almost exactly the same arguments as a “proper” PubSubHubbub subscribe HTTP request would. Wee differences: topic – unlike the topic argument in the HTTP version (which is a URL), this is just one of the topic types returned by flickr.push.getTopics . secret – currently not supported, so this parameter is omitted. callback – this must be unique, i.e. you can’t use the same URL for more than one subscription. Everything else works as you would expect – verification (either synchronous or asynchronous), the hub challenge string, subscription expiration/refreshing, unsubscribing (with flickr.push.unsubscribe ) etc. Which brings us to: flickr.push.getSubscriptions This method also requires an authentication token with read permissions, and returns a list of subscriptions for the authenticated user, like so: < rsp stat=\" ok \"> < subscriptions > < subscription topic=\" contacts_photos \" callback=\" http://example.com/contacts_photos_endpoint?user=12345 \" pending=\" 0 \" date_create=\" 1309293755 \" lease_seconds=\" 0 \" expiry=\" 1309380155 \" verify_attempts=\" 0 \" />\n    < subscription topic=\" contacts_faves \" callback=\" http://example.com/contacts_faves_endpoint?user=12345 \" pending=\" 0 \" date_create=\" 1309293785 \" lease_seconds=\" 0 \" expiry=\" 1309380185 \" verify_attempts=\" 0 \" /> < /subscriptions > < /rsp > Oh yeah, the docs: flickr.push.subscribe flickr.push.unsubscribe flickr.push.getTopics flickr.push.getSubscriptions Feeds The format of the feed that gets posted to your endpoint is currently limited to Atom 1.0, i.e. exactly what you’d get from something like http://api.flickr.com/services/feeds/photos_public.gne?id=_YOUR_NSID_HERE_&format=atom For the contacts_faves topic type it’s the same thing but with the addition of the atom:contributor element to indicate the user who faved the photo. And that’s about it. Questions? Privacy and Restrictions The astute observer may notice that not all photos are being sent in the PuSH feeds. Since this is a new (and experimental) feature for Flickr, we’ve basically turned all of the privacy/safety restrictions on it up to 11, at least to start with. PuSH feeds currently only contain images that have public visibility and safe adultness level. In addition, users with their “Who can access your original image files” option set to anything other than “anyone” and users who are opted out of the API will not have their photos included in PuSH feeds. While this may be a bit restrictive (for example since the API call is authenticated and the photos are coming from your contacts technically you should be allowed to see contacts only or friends/family photos for contacts that allow it),  we feel that since this is a new thing it’s better to start conservative and see how the feature is being used. It’s possible that we may relax some of these restrictions in the future, but for now a PuSH feed is essentially what a signed-out user could get just by grabbing the RSS feeds from various people’s photostreams. You will also notice that for now we’ve limited the feature to pro account holders only. So… What? So what can you do with it? There’s the obvious: any web application which currently does some kind of polling of the Flickr API to get photos for its users can potentially be altered to receive the push feeds instead. More timely updates, cheaper/simpler for the application and as it turns out cheaper for Flickr, too – it’s often easier on our servers to push out events shortly after they happen and we’ve got them (often fresh in our cache) than it is to go and dig them up when they’re asked for some time later. Some of the more interesting things that we hope these API methods will enable revolve around the more real-time nature of the events they expose. As an example of what’s possible in this space, Aaron Cope has created a little application he calls “Pua”. It’s a wonderfully simple way to surf Flickr without having to do much of anything; Pua takes you on a ride through your contacts’ photos and favorites, as they happen. Have a read about exactly what it is, why it’s called Pua and why he made it. If you ask nicely maybe Pua will give you an invite code. Later Hopefully there will be much more to come. Finer-grained controls on the subscriptions (safety levels, visibility levels, restricting to just new uploads or only certain types of updates, lightweight JSON feeds, etc.), new types of subscriptions (photos of your friends/family, photos from a particular location, photos having a particular tag, something to do with galleries…), and maybe some other stuff we haven’t thought of yet. Hey, wouldn’t it be cool if you didn’t need to run a web server on the other end to be the endpoint of the feeds? Let us know what you’d like to see! What works, what doesn’t, what we got wrong and how to make it more useful to the people who want to Build Stuff (that’s you). Fine Print The Flickr PuSH feeds are part of the Flickr API, and thus fall under the API Terms of Service Agreement . This means all the usual things about respecting photo owners’ copyrights and all also the other good bits about API abuse. In other words, don’t try to subscribe to all of Flickr. Trust me, we’ll notice.", "date": "2011-06-30,"},
{"website": "Flickr", "title": "Join Us at the NYC Photo Hack Day!", "author": ["Ross Harmes"], "link": "https://code.flickr.net/2011/08/16/join-us-at-the-nyc-photo-hack-day/", "abstract": "Are you a developer with a great idea for a photo hack? Are you going to be in New York on August 20th and 21st? If so, join us at the Photo Hack Day ! Our very own Paul Mison will be on hand to give a presentation on the Flickr API and to answer any questions you have while hacking. The winner will have their hack featured on the NASDAQ billboard in Times Square (among other prizes ), and all participants will get a year of free Flickr Pro. Sign up to reserve your spot at the event, which will be held at General Assembly in Manhattan. If you’re participating in the Hack Day, Flickr engineers will be available to help on IRC: #flickrapi on chat.us.freenode.net . You can also go to developer.flickr.com for more information on the API and developing with Flickr photos. We will be announcing some new and improved APIs in advance of the Hack Day, so watch this space!", "date": "2011-08-16,"},
{"website": "Flickr", "title": "Engage Kitten Hose", "author": ["Neil Walker"], "link": "https://code.flickr.net/2011/08/19/engage-kitten-hose/", "abstract": "A little while ago we released some new API methods that enabled real-time updates for new photo uploads to be pushed out via a PubSub-like subscription system. Initially you could only subscribe to photos from your contacts and favorites from your contacts. Which was pretty neat, but that barely scratches the surface of stuff that happens on Flickr that people might be interested in. So we added some more stuff to subscribe to. Calling flickr.push.getTopics now gets you: <rsp stat=\"ok\">\n  <topics>\n    <topic name=\"contacts_photos\" display_name=\"photos from your contacts\" />\n    <topic name=\"contacts_faves\" display_name=\"favorites from your contacts\" />\n    <topic name=\"photos_of_contacts\" display_name=\"photos of your contacts\" />\n    <topic name=\"photos_of_me\" display_name=\"photos of you\" />\n    <topic name=\"my_photos\" display_name=\"your photos\" />\n    <topic name=\"my_faves\" display_name=\"your favorites\" />\n    <topic name=\"geo\" display_name=\"photos from an area (geo)\" />\n    <topic name=\"commons\" display_name=\"photos from the Flickr Commons\" />\n    <topic name=\"tags\" display_name=\"photos with a tag (or tags)\" />\n  </topics>\n</rsp> The details for the extra arguments required by the new topic types are part of the flickr.push.subscribe API method documentation. The my_photos and my_faves topic types are exactly like contacts_photos and contacts_faves , just scoped to your account. The photos_of_me and photos_of_contacts topic types create subscriptions that receive events when you or your contacts are tagged in a photo. The really interesting ones though are the next 3: commons , geo , and tags . The Commons One of the great new subscription types is for photos from the Flickr Commons . Set the topic type to commons, and set the nsids argument to a comma-separated list of NSIDs of Commons institutions you’re interested in (get them by calling flickr.push.getInstitutions ) or just leave nsids empty to get all uploads and updates from the Commons. Geo Subscriptions With the geo topic type you can subscribe to photos from a particular area, specified as either a set of WOE ID s (also, here ), a set of Flickr Place IDs, or by a point and a radius. The radial query is the obvious choice for creating subscriptions that aren’t well-known areas, or say, user-specified by dragging a circle on a map. WOE IDs on the other hand are incredibly useful since they represent many well-known geographic features. For example, London, England: WOE ID 44418 . Or, if you’re like me and completely mad about aviation and anything that flies, there are convenient WOE IDs for airports. Go somewhere like this , grab a bunch of airport codes for interesting places, use the Y! GeoPlanet APIs or the flickr.places.find method) to resolve the airport codes into WOE IDs and put them into a comma-separated list in the woe_ids argument of flickr.push.subscribe and watch the aviation photos roll in. For example: Hong Kong International, China (HKG): 24875607 Innsbruck Airport, Austria (INN): 12510823 Keflavik International Airport, Iceland (KEF): 12513445 Le Bourget Airport, Paris (LBG): 22137770 Kuala Lumpur International Airport, Malaysia (KUL): 28752278 etc. Tags We’ve also added a very basic tag subscription type. With the topic type set to tags , you can provide a comma-separated list of tag names in the tags argument of flickr.push.subscribe and receive uploads and updates from photos containing any (i.e. OR mode) of those tags. Try something like kitten,cat (We’re not responsible for your bandwidth bill). The tags you specify should be all lower-case and not contain any spaces, so if you want to match something like “Justin Bieber” you’ll need to specify “justinbieber”. Coming soon: machine tags! robots:zomg=yes … Warning: Code Alert! In case you missed it, last month Kellan devoted a little Sunday-morning hack time to whipping up a little example of how to get started with the Flickr real-time APIs. It’s got a little bit about authentication, how to subscribe, processing a feed, and even some PHP. Thanks Kellan!", "date": "2011-08-19,"},
{"website": "Flickr", "title": "In the privacy of our homes", "author": ["Trevor Hartsell"], "link": "https://code.flickr.net/2011/08/30/in-the-privacy-of-our-homes/", "abstract": "The best thing about working at Flickr is that my coworkers all love the site and product ideas can come from anyone. Recently, the Flickr staff had to work from home while our office was disassembled and relocated a few floors down. A chance to sleep in and start the weekend early? Or get together with a few ambitious coworkers and hack together a new feature? We met at Nolan’s house, ate a farmer’s breakfast, and brainstormed. We wanted to build something fun, which a few of us could start working on that morning and have a demo ready by the end of the day. Something suited to the talents and interests of the people in the room. Secret Faves? Risqué Explore? Bert wanted to help people geotag more photos, but he wanted more sophisticated privacy controls first. I’d been using a simple web app that I built with the Flickr API to manage the geo privacy of my photos, and it seemed like something more people should have access to. So we had a need. We had a proof of concept. We had enough highly caffeinated engineers to fill a small dinner table. “Let’s build geofences.” What the beep is a geofence? You probably know what geotagging is. It’s nerd-speak for putting your photos on a map. Flickr pioneered geotagging about five years ago, and our members have geotagged around 300 million photos and videos. We’ve always offered the same privacy settings for location data that we offer for commenting, tagging, and who can see your photos. You have default settings for your account, which are applied to all new uploads, and which you can override on a photo-by-photo basis. This works well for most metadata. I have a few photos that I don’t want people to comment on or add notes to, but for the most part, one setting fits all my needs. But geo is special. I often override my default geo privacy. Every time I upload a photo taken at my house, I mark it “Contacts only”. Same for my grandma’s house. And that dark place with the goats and candles? Sorry, it’s private. Managing geo privacy by hand is tedious and error prone. Geofences make it easier. Geofences are special locations that deserve their own geo privacy settings. Simply draw a circle on a map, choose a geo privacy setting for that area, and you’re done. Existing photos in that location are updated with your new setting, and any time you geotag a photo in that area, it gets that setting too. Geofences are applied at upload time, or when you geotag a photo after uploading it. It doesn’t matter how you upload or tag your photos: The Organizr , the map on your photo page, and the API all use geofences. If you’re ready to dive in, visit your account geo privacy page and make your first geofence. Boring details When dealing with privacy, we need to be conservative, reliable, and have clearly defined rules. The geofences concept is simple, but the edge cases can be confusing. No geofences Common use case What happens when you have overlapping geofences? What if you move a photo from outside a geofence to inside one? Where does your default geo privacy setting fit in? The vast majority of Flickr members will never encounter these edge cases. But when they do, Flickr plays it safe and chooses the most private setting from the following options: The member’s default geo privacy The current geo privacy of the photo (if it was already geotagged) Any geofences for the new location If your account default is more private than your geofences, the geofences won’t take effect. If you have overlapping geofences for a point, the most private one will take effect. If you move a photo whose location was private into a contacts-only geofence, it will stay private. Overlapping geofences More private account default As a reminder, here’s the ranking of our privacy settings: Note that friends and family are at the same level in the hierarchy. Your family shouldn’t see locations marked as friends only, and vice versa. With that in mind, what should Flickr do when someone geotags a photo where friends and family overlap? Maybe he wants both friends and family to see it, or maybe he wants neither friends nor family to see it. To really be safe, we must make that location completely private. Friends and family overlap Go forth and geotag A few years ago, privacy controls like this would have been overkill. Geo data was new and underused, and the answer to privacy concerns was often, “you upload it, you deal with it.” But today, physical places are important to how we use the web. Sometimes you want everyone to know exactly where you took a photo. And sometimes you don’t.", "date": "2011-08-30,"},
{"website": "Flickr", "title": "The Code Behind Geofences", "author": ["Nolan Caudill"], "link": "https://code.flickr.net/2011/09/01/the-code-behind-geofences/", "abstract": "We launched geofences earlier this week. I want to give you a glimpse into some of the code that runs the feature. Geofences are defined by three things: a coordinate pair, a radius, and a privacy level. We store the coordinates (ie, decimal latitude/longitude) in our database as integers, the radius as an integer representing meters, and the privacy level as an integer that maps to a constant in our code. When an image is geotagged, we have to determine which geofences the point falls within. We do some complex privacy calculations that Trevor talks about in his introductory post . First, we fetch all of your geofences from the database and then loop through each one, determining if the photo falls in any geofences. We limit the number of geofences you can create to ten, so running a point through 10 calculations is not that expensive. We use the great-circle distance formula to figure out how far the image is from the center of a geofence. If this distance is less than the radius of the geofence, the geofence applies. MySQL is a Great Hammer When you create a geofence and want to apply it to existing photos, the backfill is a more involved process. In order to grab just the photos we care about for a geofence, we have to limit the number of photos that we select due to performance. As mentioned, we store the latitude and longitude as integers in MySQL. Since radial queries are next to impossible with this setup, we do a first pass with a bounding box formula that encompasses the geofence and then use our great-circle distance formula to cull the photos that don’t fall in the geofence. The bounding box formula we use takes into consideration geographic gotchas, like the poles and the 180 degree discontinuity (ie, the International Date Line). Since a geofence could overlap the International Date Line, we have to modify the DB query in doing the longitude part of our query since doing a simple BETWEEN won’t work. When the bounding box doesn’t cross the IDL, we can do “SELECT id FROM Table WHERE longitude BETWEEN $lon1 and $lon2” and when it does we do a “SELECT id FROM Table WHERE longitude < $west_lon AND longitude > $east_lon”. In short, we use a query that MySQL is a good at it to limit our initial dataset and then use a little post-processesing of the data to get the points we actually care about for the geofence. After we have this bundle of photos that the geofence backfill applies to, we then run our normal geoprivacy calculations, lowering the privacy if the combination of fences calls for it. Concurrency, or the world keeps turning The introduction of the geofences backfill pane also introduced some concurrency issues, which were fun to deal with. The contract that we wanted to create for the user was that the calculated privacy that they saw in the preview pane for the backfill would exactly match the result of running the backfill. This seemed intuitive but the implementation was tricky. There are two things that can affect a photo’s geoprivacy: the user’s default privacy and the geofences that exist. These things can change through time, that is between the time that a user hits ‘apply’ on the preview pane and when the backfill finishes running. Whenever you deal with mutable state through the lifetime of a process, you have to change how you treat this state. Who can modify it? What can they modify? And what processes see this updated object? The easiest way to deal with mutable state is to make it immutable. We do this by storing the state of the user’s geoprivacy world (that is, the default geoprivacy and the geofences) alongside the backfill task, so that when the task runs it uses this state instead of querying the DB, which is mutable, possibly having changed since the user hit the ‘apply’ button. Storing this state ensures that regardless of how the user modifies their geoprivacy settings, the result of the backfill will match exactly what they saw in the preview pane, providing a consistent view of the world. We also added the restriction of only allowing one backfill task per user to be running at a time, which simplified our bookeeping, our mental model of the problem, and the user’s expecation of what happens. Lessons Learned Geo, by itself, is not easy. The math is complex (especially for someone like me a few years out of school). Privacy is even harder. Since the project evolved quite a bit while we were building it, having a large amount of automated testing around the privacy rules gave us the confidence that we could go forward with new privacy demands without introducing bad stuff into code that was “done”. We learned to give ourselves plenty of time to get all this stuff right, since geo has so many edge cases and violating privacy is an absolute no-no. During the project, we had to keep balance between our users’ privacy and their expectations, all while keeping a complex and new feature understandable and even fun to play with. The only way we could do this was being open and honest between engineering, design and the product team about what we were building. This was a total team effort from Flickr, and we’re very proud of the end result and the control that it gives our users over their presence on the Internet. Also, if you’re interested in working on fun projects like this one, we’re hiring !", "date": "2011-09-1,"},
{"website": "Flickr", "title": "Creating an interface for geofences", "author": ["Eric Gelinas"], "link": "https://code.flickr.net/2011/09/06/creating-an-interface-for-geofences/", "abstract": "When we began prototyping geofences , our goal was to encourage people to geotag more photos without having to be too concerned about privacy. Introducing people to geofences The term “geofence” may sound complicated, but our implementation is quite simple. A geofence is a user-defined boundary around a specific area on a map. We decided to keep the creation and editing process similar to geotagging on the photo page. We understand from developing the photo page map that it is important to provide a way to search for a location as well as simply drop something in the right place on the map. The geofence is represented by a selector-circle on a modal map panel with simple edit controls on the side. In previous map interfaces we used canvas elements to generate scalable components like our new selector-circle. One example of this would be the location circle in photos nearby . Our resident browser performance task-master, Scott Schiller , decided this time to compare the performance of continuing to use canvas with that of the now widely supported css border-radius feature. Using border-radius made for a smoother performing interface in our tests against the early prototypes of geofences. We loved the result. The only noticeable downside to our current implementation is that it attaches a click handler to the entire element which is displayed as a selector-circle with css. This means that the actual click-able area is not a circle at all, it is a square. We are currently working to find the most elegant way to get around this issue. The code below shows one possible solution: var root_node   = Y.one('#circle_container_node'),\n    circle_node = root_node.one('.selector-circle');\n\nfunction getCenterPoint() {\n\tvar half_node_width = circle_node.get('offsetWidth')/2;\n\t\n\treturn [(circle_node.getX()+half_node_width),(circle_node.getY()+half_node_width)];\n}\n\nfunction getDistanceFromCentroid(xy) {\n\t\n\tvar centroid_point = getCenterPoint();\n\n\treturn Math.sqrt(Math.round(Math.pow(centroid_point[0]-xy[0], 2.0)) + Math.round(Math.pow(centroid_point[1]-xy[1], 2.0)));\n}\n\ncircle_node.on('click',function(e){\n\t\n\tif(getDistanceFromCentroid([e.pageX,e.pageY]) < half_node_width) {\n\t\t//handle\n\t}\n\t\n}); Sustainably supporting a sill relevant, but older browser Don’t you still have a fair amount of users on IE7, which does not support border radius? Yes, we do, and thanks for pointing that out so eloquently. We decided to use the CSS3PIE framework which uses clever tricks to make IE7 support modern css features like border-radius, box-shadow, border-image, multiple background images and linear-gradient as background image. What is awesome about using CSS3PIE, is that it allows us to give IE7 users a nearly identical experience as that enjoyed by modern browser users without a lot of branched code. .selector-circle {\n\tborder:solid 8px rgba(68, 68, 68, .4);\n\tbackground-color:rgba(0, 100, 220, .5);\n\tborder-radius: 999px;\n        /*Neeed for CSSPIE to support rgba*/\n        -pie-background:rgba(0, 100, 220, .5);\n} Relative sizing We determined a sensible size range of 50 to 10,000 meters for a geofence, then dreamt up a few ways to allow folks to select an appropriate size within this range on the map. A lot of people (myself included) are not very good at judging distance in kilometers or miles without a visual example. We establish a connection between changing to the radius dropdown menu, the size of the selector-circle, and the zoom level of the map by making sure that a change to one is reflected in all of them. Something had to be done to keep the geofence visible on the map even if the selector-circle has become too small to be seen in a selected zoom level. The selector-circle was given a center-point which would not change with changes of radius or zoom. It is always visible even if the surrounding fence is too small Geofences are privacy defaults, not filters A geofence is only applied as a default to at the time of uploading or geotagging a photo. It will not, on it’s own, affect photos already in that area. We created a process which one could follow to apply geofences to existing photos as a convenience. Nolan Caudill goes into detail about the code behind this process in his post from last week . Now, imagine if you did not realize that a large number of your photos, which you intended to stay public were in an area covered by a geofence. After applying the geofence settings, they would be suddenly hidden from the map. Changing all of those photos individually or even as a batch in the organizr would be, what we in the industry call, a massive honking bummer. A final step was added to the end of the creation and edit of geofences to show which photos could be affected. We display a list of potentially affected photos with a color-coded indicators of what their privacy will be after the geofence settings are applied to help people to make as informed a choice as possible. In most cases you will get an idea of which photos will be affected right away, but areas with lots of photos can be carefully assessed as well. Showing all of the fences at once In the prototype phase, geofences were displayed in a list and the only way to see them on a map was individually. We decided to create a map which displays all of the circles, in one place, over the list, to make it easier to see how fences might interact. One great example is that if two fences of similar privacy value (like family and friends for instance) overlap, the intersecting area is considered private. With this in mind, it is nice to at least be able to see on the map where your fences cross. In some later release, we would love to show an appropriate red color-code in these intersecting areas. Most people will create geofences with names like Home, Office, and School. For most people, these places are pretty close to each other. Some people might have places which they want to protect and are further apart. It is important for the map flexible enough to include the entire globe if you were to have one or two geofences abroad. We use a center-point to represent geofences which are too small to be seen, and we calculate the best zoom and center of the map when we load of the geo preferences page. World view Neighborhood view", "date": "2011-09-6,"},
{"website": "Flickr", "title": "Realtime Photo Sharing: Flickr Photo Session", "author": ["Ross Harmes"], "link": "https://code.flickr.net/2011/10/05/realtime-photo-sharing-flickr-photo-session/", "abstract": "This is a guest post by Jason Gabriele. Jason is a developer from the Yahoo! Mobile Platform/CPG team, who has worked in the mobile space for years developing on devices ranging from a Motorola V3 flip phone to modern devices like the iPhone. Over the past few months, his focus has been on the Photo Session project working mostly on the frontend UI. Photo Session started as a Hack Day project by Iain Huxley. The idea came from the desire to share and discuss photos live with friends, like being able to share photos from a recent vacation with his mother in Australia. He then created a demo which could provide realtime photo viewing synced across multiple users located anywhere in the world. At a Yahoo! Hack Day event, Photo Session was selected as a winner and Iain was awarded a Yahoo-branded beach towel. Later, the project evolved into a Flickr photo sharing project with chat provided by Yahoo! Messenger. Photo Session Photo Session allows you to share photos in realtime with your friends. You can slide through photos, draw on them, zoom in and out, and send messages with others present in the session using the built-in chat window. Users without Flickr accounts can still join as guests but can’t use certain features like the drawing tool or chat. Given the requirement to allow guest participants, we had to limit the photos to publicly-accessible photos only for the first release. The Basics Create a Photo Session by visiting a set or photo stream and clicking on “Start a Photo Session” under the share menu Once in the photo session, advance through photos by clicking and dragging or using the arrow buttons (desktop only) Start drawing mode by clicking the pencil icon (must be logged into Flickr) and begin drawing on the photo. Your drawing will be shared with others in the session but will be cleared as soon as drawing mode is disabled Zoom in by using the +/- buttons or using the scroll wheel on the desktop, or by pinch-zooming on the iPhone and iPad You can view details about the photo using the Information icon in the lower-left on desktop browsers, or by clicking the “i” icon in the lower-right on the iPhone You can leave the group at any time and browse on your own using the “Browse on your own” button (not available on the iPhone). Return to the group session by simply clicking the button again You can hide the Photo Session toolbars by doing a single click on the current photo. Click again to bring them back Browser Support Since the primary use of Photo Session is sharing photos, we wanted to make sure the experience felt fast and responsive. This meant we would need to use features like CSS transforms and other CSS3 properties only available in the latest browsers. We also needed the rendering to be fast enough to support users rapidly advancing through photos. We decided to support Chrome, Firefox, IE9, Safari and iOS for our first launch. This would also simplify the QA process. We plan on adding support for Android in the future. Future Photo Session is in preview mode, so we may change features in the future depending on how people use them. We already have plans for some new features, but we would still love to hear from you so please provide feedback in the Flickr forums . We hope Photo Session makes sharing photos with your family and friends a more social and interactive experience!", "date": "2011-10-5,"},
{"website": "Flickr", "title": "How Photo Session Brings Photos Alive", "author": ["Ross Harmes"], "link": "https://code.flickr.net/2011/10/07/how-photo-session-brings-photos-alive/", "abstract": "This is a guest post by Teruhisa Haruguchi: Frontend Engineer in the Mobile & Presentation Service group. Main developer on the Flickr Photo Session project. Recent graduate from Cornell University with a concentration in Computer Graphics. Home country Japan. Keeping everyone connected around photos This post dives down into the inner working of Photo Session, which enables user to have a real time photo sharing experience on multiple platforms. In order to synchronize the viewing experience on different browsers, we need to have a mechanism to pass the state of one browser to other browsers. There are emerging technologies that would allow messages to be passed to the browser by opening a connection (ex. WebSockets), but our goal was to enable this feature for multiple platforms. To support this requirement, we chose CometD as our server technology which enable browsers on multiple platform to handle push notification. CometD also allowed us to setup a server cluster to handle large number of requests. Bayeux Protocol (a.k.a. Push Notification thingy) The mechanism in which messages are passed around is as follows. As different browsers join a Photo Session, we connect them to one of our servers and leave that connection open. This is referred to as long polling. When Client X posts a message, it is first collected by the server it is connected to, Server A. Server B has a long polling connection open via Oort to listen for any message that comes through Server A. OortClient is simply a server-side client that enables message passing between servers. It employs the same mechanism to push message from server to client. Once Server B receives the message via Oort, it will then relay the message down to Client Y by using the same mechanism used by Oort. This lightweight transport mechanism allows us to pass messages around with minimum latency. However, this comes with few limitations. One, it does not guarantee message ordering. Client Y would not know the message order which client X submitted the them. To hide this limitation, we simply pick the very last message that is received for any client. There would be cases where users will be temporarily be out of sync when multiple messages are passed around at once, but it will be correctly updated with subsequent messages. Another limitation is the lack of guarantee on message delivery. Since we do not check for ACK during message delivery, the client X does not know if the message has been received by client Y. Again, most of the message delivery limitation is masked by checking for the last message.", "date": "2011-10-7,"},
{"website": "Flickr", "title": "Talk: Real-time Updates on the Cheap for Fun and Profit", "author": ["Nolan Caudill"], "link": "https://code.flickr.net/2011/10/11/talk-real-time-updates-on-the-cheap-for-fun-and-profit/", "abstract": "Superheros Neil and Nolan (N&N) just gave their talk on our real-time PuSH system at Web 2.0 in New York. You can download the slides , or view the contents I’ve oh-so-roughly transcribed here: Oh Hai My name is Nils. My name is Neil Walker, I’m an Engineer at Flickr. I mostly work on the back-end infrastructure portion of things. I’m going to be telling you about a system Nolan and I built to send real-time updates about things that happen on Flickr out to the things and people that want to know about them. My portion of the talk is mostly going to be concerned with the back-end. How did we build the guts of the system and what we think are the key components. What I’m not talking about. This talk isn’t about building Twitter, or a full-scale pubsub system built from the ground-up to handle millions of updates per second. Instead it’s geared around what you can do with less. What if you’ve got an existing site with lots of functionality already built and you want to add some real-time notification capabilities to it? What can you do if you don’t have a lot of resources to throw at the problem? It turns out that you can get pretty far with some bits and pieces that many sites will already have, and how to fit those bits together is what we’re going to cover. This is Nolan Hi, I’m Nolan Caudill, a backend engineer at Flickr. I work on most of our backend systems, but focus mainly on geo, the API, internationalization and localization, and general site performance. The Fun Part I’m going to focus mainly on why you would want to use the new push api, what problems it solves, and why it’s better in some cases than our traditional pull-based APIs. Also, I’m going to talk about how you get up and running with the new APIs. We know it’s a bit of mindflip from the traditional APIs and we want to help you with the transition on getting up and running with it. So this was the Wired cover for March 1997. From the article: “Media that merrily slip across channels, guiding human attention as it skips from desktop screen to phonetop screen to a car windshield. These new interfaces work on the emerging universe of networked media that are spreading across the telecosm.” Whatever that means. So that was almost 15 years ago. “Kiss your browser goodbye”! didn’t exactly pan out. Flickr PuSH But jumping forward to now, we DO have all those screens. Most of us have 3, 4 or more devices with high-resolution displays on them, almost all of them with web browsers, and varying levels of human interfaces. Some of them even have cameras. Lots of us have 2nd or even 3rd monitors at work. For Flickr, having apps that can be used to explore photos on all those different screens regardless of user interface seems only natural, and that’s what we hoped to inspire with a real-time API. Before we get into the details I’d like to show a simple little application that Aaron Cope, who used to work at Flickr, built on top of our PUSH API, just to see what would happen. Basically it’s a simple web page that can be run full-screen on almost any device that has a browser. It lets you subscribe to various streams on Flickr like photos from your friends, photos that your friends favorite, photos with particular tags, and photos from a location. It then receives these photos in more or less real-time and displays them full-screen with the title overlaid on top of the photo. That’s it. No controls, no user interface after the initial point of telling it what you’re interested in. So Flickr being a social website, you start to get photos of your friends, and the things that happen to them. You also get photos of things that your friends are interested in. As they fave them, they show up in your live stream. Then you might fave the same image, and more of your friends see it, fave it, and so on. It’s a very natural way for a popular object to percolate around your network, and often you don’t have to worry about missing something good because if it’s popular it’ll bubble back up as another one of your contacts faves it later on. Interesting things start happening when you have devices with screens AND cameras – a photo makes it into someone’s stream, they take a photo of it and upload that, which makes it back into the original uploader’s stream, who faves it, etc. and you get a sort of live conversation taking place with photos. Knowing that when you upload a photo it’s going to appear on someone’s monitor or widescreen TV adds a new dimension to photosharing. Eventually it can get a bit ridiculous. A bit of history. Anyway, we’ll go back to how all this got started at Flickr. – A while ago we had the need to get new public uploads/updates to search partners in a timely manner. Why? – Being crawled is fine but is expensive for the crawler and crawlee and isn’t always accurate. – Making a specific API for that purpose could do it but is probably going to be clunky, require too much work by both parties, and means that whatever it is that’s hitting your API can potentially affect your site performance, or other API users. Pushing out updates where you control the flow is really the way to go. – What did we build? ‘headers’ => array( ‘Content-type’ => ‘application/atom+xml’, ‘User-Agent’ => ‘Flickr Kitten Hose’ ) Basically a firehose of public uploads/updates (incl. deletions/privacy changes) that we can aim somewhere and turn it on. Similar to the Twitter firehose, but for photos uploaded to Flickr. So essentially stuff happens on Flickr, we transform the events into some easily parseable format, bundle everything up into reasonable-sized blobs and POST it to a web server somewhere that consumes the data. PubSubHubbub: Thanks, Google Rather than invent a specific format/protocol we decided to pick something familiar. Something that already has a spec, is well-documented and is well-understood. So we picked a common Publish-Subscribe protocol. Google’s PubSubHubbub has definitions for how to publish a feed to a hub, how to subscribe and unsubscribe to topics and verify subscriptions and all the other good things that people who write specs like to specify. Of course we didn’t actually need a lot of the spec to accomplish our goals for just the firehose bit, but we did have in the back of our minds the thought of expanding on what we built. So choosing at the start to meet an accepted spec was probably a good idea, and allowed us to get going without having to think too hard about what might come later – because someone else already figured out how that stuff should work. What? A hub with no subscription control and a single hard- coded endpoint. If you want to fit the firehose idea into the PubSub metaphor then essentially what we built at first was a hub (i.e all of Flickr) that has one hard-coded “topic” that can be subscribed to – namely all public uploads & updates. Except that the subscribing and unsubscribing part involves humans agreeing to things and then turning the firehose on, instead of machines POSTing to web servers and receiving callbacks, etc. But the pubsub metaphor still fits. How? So how did we build it? What I’m going to get into now are what I think are the key pieces of the back-end. I’ll be glossing over some parts of it that are pretty basic and not terribly interesting in the interests of focusing on the good stuff. Async task system Gearman, etc The first (and probably the most critical) part of what we built is an asynchronous job queue, or what we often call our Offline Task System. Essentially it’s a way of de-coupling expensive work from the web servers. Sooner or later you end up wanting to perform an operation that takes longer than you want to make a user at the other end wait (or places more load on your web servers than you care to suffer) and so you want to off-load that work somewhere else. We use this concept EVERYWHERE. Examples: Modifying/deleting large batches of photos. Computing recommendations for who you might want to add as a contact. We have several hundred different tasks, and there are often thousands of them running in parallel. We built our own that fits our specific needs, but a great open-source example is Gearman. Mature, easy to set up and use and scales quite well. Stuff happens on flickr. So the very start of the flow of our system begins when stuff happens on Flickr. The obvious example is a photo upload, often involving cats. Other things we’re interested in are updates: Changing the title, the description or other meta-data of a photo. Also we want to provide updates when photos are deleted (or have their visibility switched to private, which in terms of updates should look the same as a deletion) Insert a task. olt::insert(‘push_new_photo’, $photo_id); When any of these things happen, we simply insert a task into our offline task system. It’s very low cost, doesn’t block and once that’s complete everything else that happens is completely de-coupled for the web servers. Task runs. Insert the event into a queue When the task runs, all it does is take the event it represents, transforms it into a little blob of JSON and sticks it in a queue. Redis Lists RPUSH kitten_hose blob_of_json For the queues we chose to use Redis Lists. For those of you not familiar with Redis yet, it’s an open-source, memory-only, key-value store that supports structured data. Think memcached with commonly-used data structures like hashes, sets, and lists and efficient operations on them. At this point Redis is fairly stable and reliable, the performance is fantastic, and it’s dead-simple to use. We accept a little bit of a HA compromise for the fact that it’s RAM-only and not clustered (yet), so if it’s down you’ll drop some updates on the floor. But our goal here is just building a firehose for updates – not a 100%-reliable archive of user activity. We can happily accept the trade-offs that using Redis implies. Uhhh Why didn’t we just insert into the queue in the first place? At this point it might occur to you that instead of inserting a task (into a queue) that when it runs just inserts something else into another queue, why not just insert the thing into the queue in the first place? We’ll get to that in a bit. However, two useful properties of our task system are that 1) you can insert tasks with a delay before they run, and 2) task inserts with the same arguments as an existing task fail silently – i.e. you only get the one task. The upshot is that you can set a delay on things like updates so that a number of updates that happen over a short period (example) only generate a single task – and when it runs it finds all of them. Then what happens? Summary: So at this point you’ve got events happening all over Flickr, uploads and updates (around 100/s depending on the time of day), all of them inserting tasks, and the task system grabbing each event and sticking it in a queue, in the form of a Redis list. The next step is obviously to consume the queue. Cron Insert more tasks! Consume the queue LPOP kitten_hose You could have a daemon that sits there and consumes the queue and makes posts to the endpoint and you’re done. Basically what we do is have a cron job that periodically looks at the queue and inserts one or more tasks (depending on the size of the queue) whose job is to drain a particular number of updates from the queue and send them to the endpoint. At first this is going to seem needlessly complicated but you’ll see why when we get to generalize the system for an arbitrary number of subscribers. One thing it does however is provide a convenient way to throttle and buffer the output – depending on what your endpoint can handle you can twiddle the knobs on your task system to run more jobs in parallel, or turn it down to choke off the firehose (and maybe drop updates on the floor if the queue gets too big…). It gives you flexibility by decoupling the delivery mechanism. Draw me a picture It’s probably time for a simple diagram: Things happening on Flickr trigger the creation of tasks, that run asynchronously, and stuff things into a queue in Redis. Periodically more tasks get triggered to drain the queue and push whatever they find back out to the eventual destination. It worked pretty well. Didn’t take the site down Backfill 3B photos So it turns out that it all actually worked pretty well. The decoupling of everything using the asynchronous task approach meant that it had pretty much zero impact on the rest of the site. We could develop it live, experiment with capacity etc without having any fear of impacting site performance for our users because of how loosely-coupled all the pieces were – if anything goes wrong the damage is limited to that little piece. Eventually we decided to run a backfill to a 3rd-party search index of all of our public photos back to the beginning of Flickr (somewhere around 3 billion) and we were able to complete it in 2 weeks, using our existing task system and the only new hardware being one redis box for the queues. So, it worked really well! Photos from your friends So we started to ask what if we wanted to make it into something that might be useful on an individual level? It seemed to be pretty reliable and low-impact to the site and showed promise of scaling fairly well. The obvious thing that would be of interest on a user level would be photos from your contacts; Here’s my endpoint, POST stuff to me when my friends upload new photos (or change existing ones). Other things that could be of interest are when your contacts favorite a photo, or when you or your contacts are tagged in a photo, or when someone anywhere on Flickr tags a photo with the tag “kitten”. But let’s start with photos from your contacts. What? A list of users (we have those!) An endpoint (URL) Looking at what we’d need to add to the system to support user-specific subscriptions there’s the obvious: a record of the subscription somewhere in a database, the callback mechanism as per the pubhub spec etc. I’m going to gloss over that because it’s pretty straightforward and not really interesting. What we need to look at to manage the updates and figure out who gets what though is basically a mapping of users (the contacts who upload photos) to endpoints (i.e. subscribers). Moar Redis SADD user_1234 endpoint_5678 So we turned to redis again. Redis offers a set datastructure that we can use to maintain this relationship pretty easily. When someone wants to subscribe to photos from their contacts, we create a redis set for each of their contacts and add to it a pointer to the endpoint. The task again. SMEMBERS user_1234 foreach $endpoint RPUSH $endpoint json_blob So now we go back to the Task that gets inserted in response to something happening on Flickr. Where it used to just insert an event into “The” single queue, now it looks at the set of endpoints that are interested in uploads from that user and inserts the event into the queue for each of them. Hopefully now you can see why we have a task do this rather than do it on the web server while the user waits – it’s still going to be quick because the queue inserts in redis are constant-time, but if you have a lot of contacts they could add up. Best to just de-couple it all and not worry about running into that problem. Cron again. Insert tasks for each queue (endpoint) Now we turn to draining the queues, and again you can see where the task system comes in. With a large number of endpoints all the cron job has to do is run through each of them, look at how many events need to be consumed and insert an appropriate number of tasks. So scaling the output part of the system (and actually most of the input too) then becomes a matter of scaling your task system – and that’s a problem that’s already been solved. Cache is your friend. One thing that plays a key role in the system that I haven’t talked about is a caching layer. Almost any site of a reasonable size these days has a memcached layer (or something similar) in between the front-ends and the databases. One of the great things this for a push system is that because you’re dealing with things as they happen, all the objects that you typically need to access to build your update stream are almost always in cache – because they were just accessed. So it turns out that not only does a system like this have almost no impact on your normal page serving time (because of all the de-coupling), it also ends up having very little impact on your databases, due most things being in cache. Redis Numbers 1000 subscriptions 50K keys (queues) / 300 MB 100 qps – 8-core / 8GB 5% cpu And a few numbers showing how Redis is performing as our little DIY queueing and subscription system. 1000 subscriptions for various different things takes around 50K keys, consuming 300 MB of ram, and at about 100 qps on an 8-core Linux box it’s barely ticking over. 3 Things: Cache, Tasks, & Queues So putting all the pieces together it’s actually pretty simple, and relies upon things that you probably already have: a caching layer, some kind of asynchronous task system and rudimentary queueing system. Even if you don’t have all of these they’re pretty well-understood pieces and there are lots of open-source options to choose from: Just grab memcached, gearman, and redis and off you go. We think you can go a long way to building the back-end of a decent push update system with just these simple pieces. So that’s the end of my portion of the talk, and now I’ll turn it over to Nolan to talk more about the front-end, and how to make it easier for a client to consume updates. Why Push? And what’s in it for me. So now that Neil’s explained the original reasoning and how we built a system for real-time push, the question is why, as an API consumer, would you want to use it which leads into how to get started with it. Flickr API = Great Success Tens of thousands of keys making hundreds of millions of calls per day. By any measure, Flickr’s tradtional pull-based API has been incredibly successful. Developers that have used our API range from Fortune 10 businesses to PhD students to hundreds of thousands of other developers that just want to do something fun with photos and the data around them. I checked the numbers this wekend and and on average, we’ve got around 10,000 different API keys making hundreds of millions of calls per day. It’s easy. Just use your browser. One major reason for its popularity is due to how easy it is to use. When you combine data that people want with an API that’s easy to use, you thousands and thousands of apps built with it. There’s effectively no barrier to entry to get started with using Flickr’s API, if you have a browser, you can access every single API method. And if you want to do this programatically server-side , say a nightly-cronjob that fetches your own uploads, you can can just do a simple ‘curl’. Either way, our pull-based API is a one-liner: whether that line is a URL in your browser, or a one-line shell script. We’ll curl it for you. It’s even simpler than that, if you need it to be. Even if you don’t have a server, or don’t want to read our documentation, just use the API Explorer on Flickr. Every API method we publicly support is represented here, not just in documentation, but on an actual real-life form for you to build queries with. Just fill out the form, press enter, and we’ll curl it for you and spit out the results in pretty JSON or XML format. One side benefit of this pull-based API is that it also makes debugging easy. You can run tons of API queries as fast as you can debug. This is huge when building a new system. If you formed the call correctly, you get data. If you messed up, modify, rinse, and repeat. Flickr is a visual news feed. So back to “why push”… I’m the kind of programmer that needs to have a fairly-focused work environment, but I also like to know what’s going on in the world at the same time. Twitter is a really great news-before-its-News service, but for me it’s a bit too distracting to have running on a screen next to my code. I’ve recently hooked up my second monitor to show me two Flickr real-time feeds: the first of which is the Commons, our group of accounts belonging to museums and various historical archives, which is motivational for me, reminding me that Flickr is really something special with real history being stored on it. The second feed is what I consider my glimpse into what’s going on around me, the stuff that immediately affects me. So I’ve hooked up a push feed of photos geotagged a half kilometer around my house and a half-kilometer around the office. Around my house, it’s usually people taking pictures of the Painted Ladies in Alamo Square but then I saw this picture come across. When your entire neighborhood is made up of densely-packed, 50+ year old homes made of questionable building materials, fires are scary things. I then jumped on Twitter and found out there was a multiple-alarm fire just a few blocks from my house. Also, around the office, just this past week, I saw pictures from San Francisco’s chapter of the OccupyWallStreet protests. The news wasn’t on the ground yet but here I was seeing live and extremely relevant things to me. Like they say, a picture can often say more than a thousand words. Subscribing to a real-time stream of photos from a specific location is about as hyperlocal as you can be. Maybe I wouldn’t follow the right people on Twitter, or I wouldn’t check the local news site until later that evening, but people do take pictures of important things and events and post them to the site, and it doesn’t get much more timely than real-time. Push is the opposite of Pull. And other obvious facts. Ok, so back to the technology. Push is a completely different animal than our pull-based APIs. Consuming a real-time push feed flips most things about our API on its head. For starters, you’ll need a full-fledged, always-on web server that is exposed to the public Internet. And running on this web server will be a software that is based on a protocol described in a spec-with-a-capital-S. This demands a lot more from the developer than just a one-line curl. Pull: Hit this URL and read Push: Read this spec and wait With our regular API, you could basically treat it as a function call, that is “ask for something, get something”. With Push, now you’re implementing full protocol where the data will come *eventually*. Now you’ll need to wade through pages of dense and sometimes ambiguous instructions, so you can wait for us to tell you that your friend uploa a picture. Another big but obvious difference between the push and pull API is that with Push, your application only gets what we send to you. There’s no looking back in time. And due to this read-spec, write-code, and wait and wait some more debug cycle, it takes awhile to get the endpoint working correctly. Personally, I have access to the Flickr codebase and I still had questions about how to make my endpoint do the right thing and it took me a few good hours one morning and more than one cup of coffee get working right. So, seriously, why Push? Because it’s fantastic. Well now that I’ve made Push seem a little scary, seriously, why Push? The main thing, I think, is that you we give you things as they happen. If you are building a site that uses Flickr data, would you rather us send y data when we get it, or set up a bunch of cronjobs that fetch possibly non-changing data? Even if you like your cronjobs, what happens when yo have 100 users? 1000 users? 1000000? I do want to make one note about what we mean by “real-time.” As you saw from Neil’s presentation, there are levels of queues and consumers, each having a non-zero delay. We get you the photo data as soon as we can, but this might be a few seconds after we receive up to a minute or two. We understood we were building a real-time feed for photos uploaded to a website, and not sending direction data to a nuclear warhead. It was okay if things were off by a few seconds. Flickr Globe Also, receiving real-time photo updates are great for certain types of problems. So, we had the press in the office recently and we wanted to build something cool to run on our big screen behind the developers as we worked. Using the real-time push feed, one of our front-ends, Phil Dokas, was able to build an interactive globe, that charted where on Earth our photos were being uploaded from in real-time. The great thing about this is, not only was it really pretty looking visualization, but it impressed upon me the magnitude of Flickr being used. The site felt really organic: you could see it being used and growing. For me, it was one of those ‘pale blue dot’ moments that Carl Sagan talked about, where you get a sense of the bigger picture of what you work on. When you work deep on the backend of Flickr, it’s easy to forget that these are real people from literally everywhere on Earth, uploading things they want to remember. And using the pushed data made this really easy to build. And for reference, this about 3 minutes worth of publically geotagged photos. I want to use it, BUT… …it sounds hard. So, we think this stuff is awesome, and we want you to use it but we do understand it’s fiddly. The spec is dense and debugging is painful. So we’re going to give you a headstart on the whole thing. We’ve written a tiny web server that handles all the subscription handshake stuff and the parsing of photo data and we put it on GitHub, and open sourced it. flickr-conduit https://github.com/mncaudill/flickr-conduit So we’re introducing, flickr-conduit which is a simple server written in node.js that handles the subscription stuff, keeps tabs on when to unsubsc users, and then finally receives the Flickr posts. There’s a lot of moving parts to setting up a push endpoint: handling everything from getting users authenticating your API key, telling you what topics they’re interested in, setting up the subscription between your callback and Flickr, and then figuring out how to get those events to the use when Flickr sends them to you. In the conduit repository, I’ve included a server that implements the push protocol and then just fires off events in JavaScript when Flickr sends something. I’ve also included a PHP application that handles authenticating the user, letting them pick their topics, and then finally showing them the photos they come in as just a demo of what it can do. I didn’t have time to get a slide up for it, but Tom Carden from Bloom, took the PHP application I made and got it ready so that you can easily de it to heroku. It’s great and if you want to run your own conduit-server demo, I’d advise checking it out. To start with, Conduit is the piece that represents your callback endpoint. You tell Flickr that this is your endpoint and it handles the rest of the subscription handshake stuff and when Flickr posts data to you server, it goes to your conduit server. Pub/Sub all the way down After I finished building this, I realized that unintentionally I took the idea of pub/sub, that is having a many subscribers expressing interesting in many published streams, and kept that spirit up all the way through to the end of my little node.js server and explaining this model sheds some light on how the conduit server works. First, as Neil described, Flickr runs a pub/sub service. With conduit, you can run your own pub/sub server that talks to our published streams. And inside of conduit, we let your application code subscribe to certain events that conduit itself receives Flickr post events, and publishes them to your app code to handle how you wish. So there are technically 3 different pub/sub hubs before your app code, which sounds a little complicated, but if you grasp the mental model of how pub/sub works, you understand the full system. Also, keeping the pub/sub levels decoupled from each other provides a lot of simplicity and flexibility. So, this is a small digression about architecture that I stumbled across while building, that when you’re building systems in the small, it sometimes makes a lot of sense to model the smaller system after the larger system. This may not be groundbreaking for some, but I just found it really neat. I know ‘curl’. Now, tell me about Push. With Conduit, we’ve removed a fair amount of the fiddly bits so you can get to doing the fun parts later. I’m going to step you through each of the steps of the whole pubsub flow to show you what each part does. First, subscribe the user to a topic. /callback?sub=user1234-contacts-faves First, you’ll want to subscribe the user to a topic. Topics can be anything from their own uploads, to their contacts uploads, to their contacts faves and even photos being geotagged at a specific place. During this subscription, you’ll specify a callback. Once you have conduit running, the callback URL you give Flickr in your subscription should p to the conduit server. One of the tricky parts of handling a multi-user pub/sub server is that you can have many users attached that are waiting for many different strea When Flickr posts a piece of data to your server, you only know the URL that it came in on, so the callback URL needs to be significant so you ro this piece of data to right consumer. One way to handle this is just create a globally unique identifier for a particular subscription, and tuck that in a database and then use that to ma URL to a subscribed user. Personally, I’m a fan of reducing the number of moving parts as much as possible so I just use something simple, like the example, and make th URL itself identifiable. So for example if user 1234 wants to subscribe to her contact’s faves, the URL could simply be /user1234-contacts-faves. As long as this callback URL creation algorithm is repeatable, it’s easy to have a common dictionary throughout your app of how to handle a particular subscription. The part of the play where Flickr asks a question. Almost immediately, after making the API call to subscribe the user to a topic, Flickr will respond with a ‘subscribe’ request. Here Flickr is basically asking, “Hi. We just received a subscription request for this callback. Did you send this? If so, repeat the magic password back to me.” All this is described in detail in the spec but you don’t need to know any of that. Conduit handles all of this for you. Debugging: the waiting game. As I mentioned earlier, debugging the server was the most boring and thus most frustrating part of building an endpoint. I’d write some code, and to see if did the right thing, I had to wait for on my subscribed events happen and get sent to me. One shortcut I found is that I could subscribe to my own faves, and then I go through a test account and fave photos, therefore forcing events. This sped things up quite a bit. Someone’s at the door! /callback?sub=user1234-contacts-faves maps to user1234-contacts-faves When an event does occur, Flickr will post it to your callback URL. Conduit then takes this callback URL and through whatever method you decide, creates an event name for it and then passes into a internal structure, In my running example, the callback you see up there gets the event name of “user1234-contacts-faves”. Like I said, you could do this mapping of URL to event name however you like, but this is simple and works for me. The emitter emits. user1234-contacts-faves Conduit exposes something that node.js calls an “emitter’. It’s basically a pub/sub structure itself. So when a post comes in and you decode the callback URL into an event name, you tell Conduit about it. Effectively you’re saying, “Hey I just received some data and it has this event name. Give this data to anyone that is interested in user1234- contacts-faves.” Finally, fun photo data. Your application code registers with this emitter what events it’s interested in and when they come in, they do something fun with them. Now I’m going to explain a bit about the mini-app that I bundled with the conduit server to shows how this works. Real-time stream Flickr + conduit + node.js + socket.io. I wanted the final product to be a simple webpage that when my subscriptions were posted to, I’d see them simply shown up on the page. I glued a few fun tools together and got something really interesting. PHP is my engine. Handles the PuSH subscribing and printing out my JS. This entire sample app could easily have been written in JS, but PHP is what I what do all day at work and my JavaScript is a little rusty and I wanted to get something up and running quickly. I was able to grab former-Flickr engineer, now Etsy CTO, Kellan Elliot-McCrae’s flickr.simple.php’s library to handle the authentication of the use well as posting of subscription requests. So the PHP app I built, first authenticates the user with Flickr and then presents the list of real-time topics he or she can subscribe to. The user checks the topics they’re interested in and then gets redirected to a screen where the images will show up as they come in. I create the unique callback URLs and at the same time dump these event names into the Javascript on the page so that my socket.io code can my node server, what events it’d like to receive when they come in. Meanwhile, back on the server… As mentioned earlier, Flickr will post the photo data to the conduit server to my callback URLs. This callback URL directly becomes the event name, mapping exactly to the what the browser told socket.io that it was interested in. When this event comes in, conduit hands it to the emitter, which then broadcasts the event back out. The server-end of the socket then receives this and pumps down into the browser’s open arms. And back to the browser. The browser then receives the photo data and inserts it into the page. That was a whole lot of engineering just to get an image up on the screen and we know it, but with flickr-conduit, you can skip most of the details and just write fun code. This actually turns out to be a great way to explore Flickr. On my extra monitor at work, I’ve subscribed to my contacts’ faves and all the updates from our Commons area (which includes museums and various archives) to get a weird blend of 100 year old pictures and things my friends find interesting. So, in summary, this is the why and the how of Flickr added a real-time push feed on the cheap. Since we know the spec is dense, and there are several moving parts, we also hope flickr-conduit helps out new developers that will hopefully le to love our new real-time feeds as much as they love our existing pull feeds. Thanks. Related links: conduit: https://github.com/mncaudill/flickr-conduit tom’s conduit links: https://github.com/RandomEtc/flickr-conduit-front , https://github.com/RandomEtc/flickr-conduit-back pubsubhubbub spec: http://pubsubhubbub.googlecode.com/svn/trunk/pubsubhubbub-core-0.3.html pua: http://pua.spum.org ( http://www.aaronland.info/weblog/2011/05/07/fancy/ ) nolan’s conduit: http://nolancaudill.com/projects/conduit/ flickr globe: http://nolancaudill.com/~pdokas/flobe/ kellan’s post: http://laughingmeme.org/2011/07/24/getting-started-with-flickr-real-time-apis-in-php/ neil’s flickr post: http://code.flickr.com/blog/2011/06/30/dont-be-so-pushy/", "date": "2011-10-11,"},
{"website": "Flickr", "title": "Pleiades: A guest post", "author": ["Daniel Bogan"], "link": "https://code.flickr.net/2011/12/16/pleiades-a-guest-post/", "abstract": "I recently asked our friend Sean from Pleiades (which I will *never* be able to spell correctly) to write up a lil’ guest post on how we did something cool with Flickr machine tags and ancient sites of the world – and here it is! Intro I’m Sean Gillies, a programmer at ISAW , the Institute for the Study of the Ancient World at New York University. I’m part of the Digital Programs team, which develops applications for researchers of ancient civilizations. Most of my work is on a gazetteer and graph of ancient places called Pleiades . It identifies and describes over 34,000 places in antiquity and makes them editable on the web. A grant from the U.S. National Endowment for the Humanities ( NEH ) running through April 2013 is allowing Pleiades to bulk up on ancient world places and develop features that can support ambitious applications like the digital classics network called Pelagios . Background In August of 2010, Dan Pett and Ryan Baumann suggested that we coin Flickr machine tags in a \"pleiades\" namespace so that Flickr users could assert connections between their photos and places in antiquity and search for photos based on these connections. Ryan is a programmer for the University of Kentucky’s Center for Visualization and Virtual Environments and collaborates with NYU and ISAW on Papyri.info . Dan works at the British Museum and is the developer of the Portable Antiquities Scheme’s website: finds.org.uk . At about the same time, ISAW had launched its Flickr-hosted Ancient World Image Bank and was looking for ways to exploit these images, many of which were on the web for the first time.  AWIB lead Tom Elliott, ISAW’s Associate Director for Digital Programs, and AWIB Managing Editor Nate Nagy started machine tagging AWIB photos in December 2010. When Dan wrote \"Now to get flickr’s system to link back a la openplaques etc.\" in an email, we all agreed that would be quite cool, but weren’t really sure how to make it happen. As AWIB picked up steam this year, Tom blogged about the machine tags. His post was read by Dan Diffendale , who began tagging his photos of cultural objects to indicate their places of origin or discovery. In email, Tom and Dan agreed that it would be useful to distinguish between findspot and place of origin in photos of objects and to distinguish these from photos depicting the physical site of an ancient place. They resolved to use some of the predicates from the Concordia project, a collaboration between ISAW and the Center for Computing in the Humanities at King’s College, London (now the Arts and Humanities Research Institute), jointly funded by the NEH and JISC . For findspots, pleiades:findspot=PID (where PID is the short key of a Pleiades place) would be used. Place of origin would be tagged by pleiades:origin=PID . A photo depicting a place would be tagged pleiades:depicts=PID . The original pleiades:place=PID tag would be for a geographic-historic but otherwise unspecified relationship between a photo and a place. Concordia’s original approach was not quite RDF forced into Atom links, and was easily adapted to Flickr’s \" not quite RDF forced into tags \" infrastructure. I heard from Aaron Straup Cope at State of the Map (the OpenStreetMap annual meeting) in Denver that he’d seen Tom’s blog post and, soon after, that it was on the radar at Flickr.  OpenStreetMap machine tags (among some others) get extra love at Flickr, meaning that Flickr uses the machine tag as a key to external data shown on or used by photo pages. In the OSM case, that means structured data about ways ways and nodes , structured data that surfaces on photo pages like http://flickr.com/photos/frankieroberto/3396068360/ as \"St George’s House is a building in OpenStreetMap.\" Outside Flickr, OSM users can query the Flickr API for photos related to any particular way or node, enabling street views (for example) not as a product, but as an grassroots project. Two weeks later, to our delight, Daniel Bogan contacted Tom about giving Pleiades machine tags the same kind of treatment. He and Tom quickly came up with good short labels for our predicates and support for the Pleiades machine tags went live on Flickr in the middle of November. The Pleiades machine tags Pleiades mainly covers the Greek and Roman world from about 900 BC – 600 AD. It is expanding somewhat into older Egyptian, Near East and Celtic places, and more recent Byzantine and early Medieval Europe places. Every place has a URL of the form http://pleiades.stoa.org/places/$PID and it is these PID values that go in machine tags. It’s quite easy to find Pleiades places through the major search engines as well as through the site’s own search form . The semantics of the tags are as follows: pleiades:depicts=PID The PID place (or what remains) is depicted in the photo pleiades:findspot=PID The PID place is where a photo subject was found pleiades:origin=PID The PID place is where a photo subject was produced pleiades:where=PID The PID place is the location of the photo subject pleiades:place=PID The PID place is otherwise related to the photo or its subject At Pleiades, our immediate use for the machine tags is giving our ancient places excellent portrait photos. On the Flickr Side Here’s how it works on the Flickr side, as seen by a user. When you coin a new, never before used on Flickr machine tag like pleiades:depicts=440947682 (as seen on AWIB’s photo Tombs at El Kab by Iris Fernandez), Flickr fetches the JSON data at http://pleiades.stoa.org/places/440947682/json in which the ancient place is represented as a GeoJSON feature collection. A snippet of that JSON, fetched with curl and pretty printed with python $ curl http://pleiades.stoa.org/places/440947682/json | python -mjson.tool is shown here: {\n    ...\n    \"id\": \"440947682\",\n    \"title\": \"El Kab\",\n    \"type\": \"FeatureCollection\"\n  } [Gist: https://gist.github.com/1488270 ] The title is extracted and used to label a link to the Pleiades place under the photo’s \"Additional info\". Flickr is in this way a user of the Pleiades not-quite-an-API that I blogged about two weeks ago. Flickr as external Pleiades editor On the Pleiades end, we’re using the Flickr website to identify and collect openly licensed photos that will serve as portraits for our ancient places. We can’t control use of tags but would like some editorial control over images, so we’ve created a Pleiades Places group and pull portrait photos from its pool. The process goes like this: We’re editing (in this one way) Pleiades pages entirely via Flickr. We get a kick out of this sort of thing at Pleiades. Not only do we love to see small pieces loosely joined in action, we also love not reinventing applications that already exist. Watch the birdie This system for acquiring portraits uses two Flickr API methods: flickr.photos.search and flickr.groups.pools.getPhotos . The guts of it is this Python class : class RelatedFlickrJson(BrowserView):\n\n      \"\"\"Makes two Flickr API calls and writes the number of related\n      photos and URLs for the most viewed related photo from the Pleiades\n      Places group to JSON like\n\n      {\"portrait\": {\n         \"url\": \"http://flickr.com/photos/27621672@N04/3734425631/in/pool-1876758@N22\",\n         \"img\": \"http://farm3.staticflickr.com/2474/3734425631_b15979f2cd_m.jpg\",\n         \"title\": \"Pont d'Ambroix by sgillies\" },\n       \"related\": {\n         \"url\": [\"http://www.flickr.com/photos/tags/pleiades:*=149492/\"],\n         \"total\": 2 }}\n\n      for use in the Flickr Photos portlet on every Pleiades place page.\n      \"\"\"\n\n      def __call__(self, **kw):\n          data = {}\n\n          pid = self.context.getId() # local id like \"149492\"\n\n          # Count of related photos\n\n          tag = \"pleiades:*=\" + pid\n\n          h = httplib2.Http()\n          q = dict(\n              method=\"flickr.photos.search\",\n              api_key=FLICKR_API_KEY,\n              machine_tags=\"pleiades:*=%s\" % self.context.getId(),\n              format=\"json\",\n              nojsoncallback=1 )\n\n          resp, content = h.request(FLICKR_API_ENDPOINT + \"?\" + urlencode(q), \"GET\")\n\n          if resp['status'] == \"200\":\n              total = 0\n              photos = simplejson.loads(content).get('photos')\n              if photos:\n                  total = int(photos['total'])\n\n              data['related'] = dict(total=total, url=FLICKR_TAGS_BASE + tag)\n\n          # Get portrait photo from group pool\n\n          tag = \"pleiades:depicts=\" + pid\n\n          h = httplib2.Http()\n          q = dict(\n              method=\"flickr.groups.pools.getPhotos\",\n              api_key=FLICKR_API_KEY,\n              group_id=PLEIADES_PLACES_ID,\n              tags=tag,\n              extras=\"views\",\n              format=\"json\",\n              nojsoncallback=1 )\n\n          resp, content = h.request(FLICKR_API_ENDPOINT + \"?\" + urlencode(q), \"GET\")\n\n          if resp['status'] == '200':\n              total = 0\n              photos = simplejson.loads(content).get('photos')\n              if photos:\n                  total = int(photos['total'])\n              if total < 1:\n                  data['portrait'] = None\n              else:\n                  # Sort found photos by number of views, descending\n                  most_viewed = sorted(\n                      photos['photo'], key=lambda p: p['views'], reverse=True )\n                  photo = most_viewed[0]\n\n                  title = photo['title'] + \" by \" + photo['ownername']\n                  data['portrait'] = dict(\n                      title=title, img=IMG_TMPL % photo, url=PAGE_TMPL % photo )\n\n          self.request.response.setStatus(200)\n          self.request.response.setHeader('Content-Type', 'application/json')\n          return simplejson.dumps(data) [Gist: https://gist.github.com/1482469 ] The same thing could be done with urllib, of course, but I’m a fan of httplib2. Javascript on Pleiades place pages asynchronously fetches data from this view and updates the DOM. The end result is a \"Flickr Photos\" section at the bottom right of every place page that looks (when we have a portrait) like this: We’re excited about the extra love for Pleiades places and can clearly see it working. The number of places tagged pleiades:*= is rising quickly – up 50% just this week – and we’ve gained new portraits for many of our well-known places. I think it will be interesting to see what developers at Flickr, ISAW, or museums make of the pleiades:findspot= and pleiades:origin= tags. Thanks We’re grateful to Flickr and Daniel Bogan for the extra love and opportunity to blog about it. Work on Pleiades is supported by the NEH and ISAW. Our machine tag predicates come from a NEH-JISC project – still bearing fruit several years later.", "date": "2011-12-16,"},
{"website": "Flickr", "title": "Welcome to the Flickr DevBlog!", "author": ["Kellan Elliott-McCrea"], "link": "https://code.flickr.net/2008/04/16/welcome-to-the-flickr-devblog/", "abstract": "Congratulations, you’ve found Code.Flickr, our new developer community site, and our new DevBlog. The DevBlog is being written by the Flickr developers for the larger Flickr development community.  We’ll be covering changes to the API (look for a post covering video in the Flickr API soon), cool Flickr related projects we discover, writing tutorials on Flickr API methods, and most anything else which catches our whimsy.  If you have something you like covered, you can let us know on [in this thread] in the Flickr API group . Its also the place to keep up on Flickr;s open source projects like the new open source Uploadr 3.0. A Quick Tour Beyond the blog , we’ve also got forums , a ticket tracker and a public SVN repository . And we’ve got rainbows!  And gears!", "date": "2008-04-16,"},
{"website": "Flickr", "title": "Flickr Uploadr, start to finish now", "author": ["Richard Crowley"], "link": "https://code.flickr.net/2008/04/16/flickr-uploadr-start-to-finish-now/", "abstract": "Starting at Flickr a short nine months ago, I was given the state of the Flickr Uploadr and told to make it better.  Better meant many things.  It meant cross-platform so we could move forward with one codebase.  It meant localized in all of Flickr’s languages without hackery.  It meant new features that would make uploading easier and encourage people to add metadata to their photos.  And while we didn’t explicitly talk about it at the time, better meant open source. Settling on a platform Straight C?  No.  Java Swing? Adobe AIR ? XULRunner ?  So many choices, each with advantages and disadvantages.  I ended up choosing to work with Mozilla’s XULRunner, which is what makes up the guts of Firefox and Thunderbird.  The main advantages of XULRunner were the ability to link in outside code libraries (like GraphicsMagick ) and the availability of real multithreading. Learning the hard way Since the project began I’ve jumped more than a few hurdles.  I documented many of the more exciting problems on my blog ( rcrowley.org ) as I went.  Crash course follows: Cross-platform XPCOM (a howto) Working from Mark Finkle’s crash course for Windows got me halfway and some other scattered resources helped to piece together the skeleton of an app that will run on both Windows and OS X.  The code has evolved quite a bit since then but this process got me on my feet. XUL overlays demystified As apps grow you naturally need to break files up to save your sanity.  I never found the crystal clear example of overlays that I wanted, so after I trial-and-errored my way out of the corner, I wrote out this common use case that Uploadr uses in several places. Threading in Gecko 1.9 I’ve been developing against XULRunner 1.9 (and therefore Gecko 1.9) which are the underpinnings of Firefox 3.  The thread primitives made available in 1.9 are much nicer than in Gecko 1.8.  Uploadr uses a background thread for event queuing and this is a stripped down example of that same pattern. MD5 in XULRunner (or Firefox extensions) The Flickr API requires developers to sign calls with MD5.  MD5 is built right into PHP but is conspicuously missing from JavaScript.  There are JavaScript implementations out there but (just for kicks), here’s how to take advantage of Mozilla’s built-in hashing library. Fun with Unicode! Flickr has, from the very beginning, been an international place.  Well before it was available in eight languages, it would accept user input in any language through the magic of UTF-8.  Uploadr carries on this tradition but to bridge the gap between Windows’ UTF-16 Unicode support and GraphicsMagick’s non-Unicode-iness, some hacks had to be liberally applied.  This code has changed a bit since, so check the latest out in Flickr Subversion . Video interview with the Yahoo! Developer Network Jeremy Zawodny from the Yahoo! Developer Network came up to San Francisco to chat about the new Flickr Uploadr a few months back.  We talked about the development process, open source and where the future might lead. The future is here now with an extension API ready for use in version 3.1.  Check out the documentation and helloworld extension or check out the full source code and build tools .", "date": "2008-04-16,"},
{"website": "Flickr", "title": "Web 2.0 Expo: You’re in Our Town Now", "author": ["Kellan Elliott-McCrea"], "link": "https://code.flickr.net/2008/04/18/web-20-expo-youre-in-our-town-now/", "abstract": "O’Reilly made the understandable, if unfortunate, mistake of scheduling a conference about 6 blocks from FlickrHQ .  And we’ll be descending on next week’s Web 2.0 Expo in droves.  Besides lurking around in the halls, a handful of us have successfully bluffed our way onto the schedule. Which means next week is your chance to come see the people who make Flickr talk about doing just that. Thursday, 04/24/2008 A Flickr Approach to Making Sense of the World Dan Catt will be holding forth on “A Flickr Approach to Making Sense of the World” , covering all things geo and nerdy, and bending very large geo datasets to your will.   Thursday  1:30pm – 2:20pm, Room 2006 The Next Generation of Tagging Don’t go anywhere, because right after the Reverend, Kakul Srivastava (Senior Director of Product Management), will be exploring the insights that arise at scale, and how to cope with them “The Next Generation of Tagging: Searching and Discovering a Better User Experience” Thursday  2:40pm – 3:30pm, 04/24/2008, Room 2006. Friday, 04/25/2008 The Power of Online Communities On Friday we’ll be kicking it off with some head to head scheduling.  Our Community Manager Heather Champ will be bringing a ridiculous amount of expertise to The Power of Online Communities: Lessons from the Best of the Consumer & Business Community Managers .  Friday 2:40pm – 3:30pm, 04/25/2008, Room 2009. Capacity Planning for Web Operations While across the hall John Allspaw will be explaining the art of Capacity Planning for Web Operations . (less politely known as, “If on the off chance your website doesn’t suck, you’re probably still screwed, ain’t you?”)  John is literally writing the book on this subject, as well has helping plan Velocity . Friday 2:40pm – 3:30pm, 04/25/2008, Room 2010. Casual Privacy And in the anchor slot (thanks guys!), Kellan Elliott-McCrea (that’s me), will be talking about Casual Privacy , on techniques and algorithms for making lightweight privacy easy, while actually enhancing the sharing that is the heart of Web 2.0.  (you might have seen the 5 minutes version last year at Ignite) 3:50pm – 4:40pm Friday, 04/25/2008, Room 2006. Friends of Flickr Perennial FoF Tom Coates of Yahoo Brickhouse and Fireeagle , will be giving his Designing for a Web of Data talk, and joining Matt Jones of Dopplr for Polite, Pertinent, and… Pretty: Designing for the New-wave of Personal Informatics . (Polite, Pertinent, Pretty, and British!) And intriguingly, Children of Flickr: Making the Massively Multiplayer Social Web .  But hopefully not in a “ … of the Corn” kind of way. See you next week!", "date": "2008-04-18,"},
{"website": "Flickr", "title": "Making a better Flickr Web Uploadr (Or, “Web Browsers Aren’t Good At Uploading Files By Themselves”)", "author": ["Scott Schiller"], "link": "https://code.flickr.net/2008/04/22/making-a-better-flickr-web-uploadr-or-web-browsers-arent-good-at-uploading-files-by-themselves/", "abstract": "Sometimes when browsers won’t do what you want by themselves, you have to get creative. A Brief History Of Web Uploading As any developer who’s suffered through form-based uploading will understand, browsers have very limited native support for selecting and uploading files. While useable, Flickr’s form-based upload needed a refresh that would allow for batch selection and other improvements. After some consideration, Flash’s file-handling capabilities combined with the usual HTML/CSS/JS looked to be the winning solution. In the past, ActiveX controls and Firefox extensions provided enhanced web-based upload experiences on Yahoo! Photos, supporting batch uploads, per-file progress , error reporting and so on; however, the initial browser-specific download/install requirement was “just another thing in the way” of a successful experience, not to mention one limited to Firefox and Internet Explorer. With Flickr’s new web Uploadr, my personal goals were to minimize or eliminate an install/set-up process altogether whenever possible, while at the same time keeping the approach browser-agnostic. Because of Flash’s distribution amongst Flickr users, it was safe to have as a requirement for the new experience. (In the non-flash/unsupported cases, browsers fall through to the old form-based Uploadr.) And Now, For Something Completely Different By using Flash to push files to Flickr, a number of advantages were clear over the old form-based method: Batch file selection File details (size, date etc.) for UI, business logic Improved upload speed (faster than native browser form-based upload) “Per-file”, asynchronous upload (as opposed to posting all data at once) Upload progress reporting (per-file and overall) Flash is able to do batch selection through standard operating system dialogs, report file names and size information, POST file data and read responses. Flickr’s new web Uploadr uses these features to provide a much-needed improvement over the old form-based Uploadr. The Flash component was developed by Allen Rabinovich on the Yahoo! Flash Platform Team. http://developer.yahoo.com/flash/ This Flash-based upload method did come with a few technical quirks, but ultimately we were still able to make signed calls to the Flickr API and upload files. Now You Can, Too! The Flash and client-side code which underlies the Flickr Web Uploadr is part of the Yahoo User Interface Library , available as the YUI Uploadr component. It’s The Little Things That Count: UI Feedback Given that Flash reports both file size and bytes uploaded, it made sense to show progress in the UI. In addition to per-file and overall progress in-page, the page’s title as shown in a browser window or tab also updates to reflect overall progress during upload – for example, “(42% complete) Flickr: Upload Photos” Under Firefox, an .GIF-based “favicon” replaces the static Flickr icon, showing animation in the browser address bar while uploading is active. This combined with the title change is a nice indication of activity and status while the page is “working”, a handy way of checking progress without requiring the user to work to bring the window or tab back into focus. In showing attention to detail in the UI and finding creative solutions to common browser drawbacks, a much nicer web upload experience is most certainly possible. Scott Schiller is a front-end engineer and self-professed “DHTML + web standards evangelist / resident DJ and record crate digger” who works on Flickr. He enjoys making browsers do nifty things with client-side code, and making designers happy in bringing their work to life with close attention to detail. His personal site is a collection of random client-side experiments. http://flickr.com/photos/schill/", "date": "2008-04-22,"},
{"website": "Flickr", "title": "Inside Photophlow: an interview with Neil Berkman", "author": ["Kellan Elliott-McCrea"], "link": "https://code.flickr.net/2008/04/24/inside-photophlow-an-interview-with-neil-berman/", "abstract": "I knew when we started talking about Code.Flickr I wanted to have interviews with third party developers, and I knew that I wanted my first interview to be with Neil Berkman , one of the engineers behind the amazing Photophlow 1. Can you say a bit about what Photophlow is for people who don’t know? Photophlow is a web application for real-time group Flickr browsing.  As you search and share photos the group you’re browsing with sees the same things you are instantly.  You can comment on photos, fave them, tag them and more, and all of this is shared with the group in real-time.  Photophlow is meant to be used for all types of interactions around photos – organized activities such as group critiques and tutorials, as well as just plain hanging out and sharing. We also integrate with some other services like Twitter and Tumblr.  For example you can send out a Twitter message with a link to your Photophlow room to invite your followers to a real-time conversation over photos.  We also integrate with the major IM networks to notify you instantly when things happen in Photophlow, like someone commenting on one of your photos. If you’d like to a quick tour of Photophlow I’d recommend this screencast . (editor note:  Also checkout the photophlow group !) 2. How are you integrating with Flickr?  What services or API methods do you use? We use quite a bit of the API.  Your identity on Photophlow is your Flickr identity, so of course we take advantage of authentication.  We currently have two types of rooms – “personal rooms” tied to a user and rooms based on Flickr groups.  We use the contacts and groups API’s to give control over privacy. We take advantage of almost all of the API methods for browsing or searching for photos.  And of course tagging, commenting and faving all go through the API. One of my favorite features makes use of machine tags .  We let you specify custom “photo emotes” – for example if you type /smile as a chat message we’ll show a photo you’ve tagged as phlow:emote=smile .  Another makes use of the Yahoo Term Extraction API .  We use this to determine interesting words and phrases in chat messages and we turn these into Flickr search links.  Keying off of the conversation like this works really nicely for discovering areas of Flickrspace that you might not discover otherwise, and the results you get from clicking on a random phrase are often very funny and unexpected. 3. What if you favorite part of working with the Flickr APIs? The nicest thing about it is the completeness.  So far we’ve found that almost everything we’ve wanted to do has been possible. 4. What (if any) where the challenges? The major challenges we face are due to the unique real-time group nature of our application.  We’d like to be responsible consumers of the API so we set some restrictions for ourselves, such as never making a separate API call for each person in the room.  For example when we display a photo we don’t indicate whether it’s already a fave because we’d need to make this call multiple times.  We turned this into a feature – if you “re-fave” a photo we delete your previous fave and add it again, moving it to the top of your list. 5. What else should I have asked you? (I’m new at this!) How about “what would you like to see added to the API?” One is “invite photo to group”.  Some group admins are using Photophlow to review photos to invite to their pool.  It would be great if we could allow them to actually issue the invitations from within Photophlow. Another, much larger one would be the ability to invite your Flickr contacts to use a Flickr-based application.  This would take a lot of work to ensure that it could be done in a non-spammy way.  Even I have mixed feelings about it but as an app developer it would be nice to allow people to use their existing connections on Flickr to be able spread the word about an application more easily. 6. Are you using any open source components in Photophlow, especially any that relate to Flickr?  Are you planning to release any? Like everybody these days we make heavy use of open source.  The part of Photophlow that interacts with Flickr is developed using Ruby on Rails, and we use the Ruby API for Flickr.  We’ve hacked this up a bit and may either clean it up and contribute back or take another look at the current Ruby Flickr interfaces and see if we might want to switch. 7. What is next? Are you planning to build more with Flickr? Enhance your current app, or build something new? Is there an application you’re hoping someone else would build? We plan on improving Photophlow in a number of ways.  A big one is to provide more explicit support for events such as critiques and competitions.  There are a number of fun features we’d love to build, for example the ability to add notes to photos and share these in real-time. We’re also building a new web application called Videophlow, which allows for group synchronous video viewing with a “shared remote control”.  Initially this will support Youtube but we’re planning to support other services and would love to include Flickr video as well. Thanks so much Neil and good luck at Launchpad today ! Have you got a neat Flickr project folks should know about?  Let us know in recommendations for the DevBlog thread! Photo: “photophlow” by d.j. paine", "date": "2008-04-24,"},
{"website": "Flickr", "title": "FireDopplGängEaglr", "author": ["Aaron Straup Cope"], "link": "https://code.flickr.net/2008/04/30/firedopplgangeaglr/", "abstract": "First, there was filtr but that’s another story entirely . The point being that I gave up carrying around a capital-C camera a few years ago choosing instead to make do with cameraphones and the availability of cheap, unlimited data-plans in the U.S. I am mostly lazy and can’t really be bothered to shuttle photos around from one device to another only to move them again to the giant device in the sky called Flickr. Before filtr I relied on the upload by email feature to snag a photo and quickly share it with the future-past but the desire to touch up — or filter — the photos before upload meant that I needed to write my own service to accept, process and then upload pictures to Flickr using the API . Which is what I want to talk about. Sort of. Read the rest of “FireDopplGängEaglr” for thoughts on FireEagle, Dopplr, place, and the DWIM engine.", "date": "2008-04-30,"},
{"website": "Flickr", "title": "Twitter API updates, FireEagle and new Flickr API fun", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2008/04/30/twitter-api-updates-fireeagle-and-new-flickr-api-fun/", "abstract": "Last night twitter released their next batch of API improvements , of course the one that caught my eye was … “[NEW] /account/update_location.[xml|json] – sets the location for the authenticated user to the string passed in a “location” parameter. Nothing fancy, no geocoding or normalization.  Just putting this out there so developers can start playing with how geolocation might fit into their Twitter applications.” … which is nice as it’s just thrown in there as a ‘what if’ type of thing. There’s no direct reason for twitter to have location stuff, (well no more than Flickr I guess)  but everyone knows that everyone wants it. It’d be great if you didn’t have to update twitter yourself and there was something else out there that could do it for us. Read the rest of “Twitter API updates, FireEagle and new Flickr API fun” for more on Twitter’s location API, FireEagle, and Flickr’s not-a-geocoder.", "date": "2008-04-30,"},
{"website": "Flickr", "title": "Videos in the Flickr API", "author": ["Kellan Elliott-McCrea"], "link": "https://code.flickr.net/2008/05/01/videos-in-the-flickr-api/", "abstract": "It isn’t just you, the pictures really did start moving, some of them at least. Which is an attempt at humor to cover the fact that this post is very belated. Presumably you’ve noticed that folks are uploading videos to Flickr, and you’re wondering how to work with video in the API?  I’ll try to recap, and expand upon the info in this thread in the API group . Long Photos First thing to understand is as far as Flickr is concerned videos are just a funny type of photo.  Your API application can ignore that video exists and everything should go on working.  This means: you can display a preview of a video by treating it exactly like any other photo on Flickr. photos AND videos are returned by any method which used to return just photos you can get info about a video like you would a photo. Videos, photos, and “media” If you’re calling one of the dozens of API methods including flickr.photos.search() and … that return what we call a “standard photo response” then that API method takes an “extras” argument. extras is a comma separated list of additional metadata you would like included in the API response. With the launch of video we’ve added a new extra: “media”.  Included media in your list of requested extras and we’ll include a new attribute media=photo or media=video with each photo element. Like so: <photo id=\"2345938910\" owner=\"35468159852@N01\" secret=\"846d9c1be9\" server=\"1423\" farm=\"2\" title=\"Naughty Dandelion\" ispublic=\"1\" isfriend=\"0\" isfamily=\"0\" media=\"video\"/> Additionally if you’re calling flickr.photos.search() you can filter your results by media type by passing `media=photos` or `media=videos` as an additional search argument. (not to be confused with the extras of the same name) Default is “media=both” returning both photos and videos. Displaying videos: just funny photos For each uploaded video we generate a JPG preview in a range of sizes . Identical to what we do for photos. Read the documentation for flickr.photos.getSizes() to get you started on how to display Flickr photos. Playing videos: constructing the embed code We don’t currently provide a way to get to the FLV for a video. (the Flash encoded video file)  We’re looking into making this possible.  In the mean time if you want to display watch-able videos you’ll need to embed our video player. In addition to the photo height and width of the preview images, videos also have a stream height and stream width which we set when we process videos during upload.  While you can make the video player any size you want the videos are going to look much better if displayed at the proper size. You can get the stream height and stream width and the URL for the video player using the standard flickr.photos.getSizes() method: <sizes canblog=\"1\" canprint=\"1\" candownload=\"1\">\n<size label=\"Square\" width=\"75\" height=\"75\" source=\"http://farm2.static.flickr.com/1423/2345938910_846d9c1be9_s.jpg\" url=\"http://www.flickr.com/photos/revdancatt/2345938910/sizes/sq/\" media=\"photo\"/>\n... standard getSizes stuff ...\n<size label=\"Video Player\" width=\"500\" height=\"375\" source=\"http://www.flickr.com/apps/video/stewart.swf?v=49235&photo_id=2345938910&photo_secret=846d9c1be9\" url=\"http://www.flickr.com/photos/revdancatt/2345938910/\" media=\"video\"/>\n</sizes> Alternately the stream width and height are included in the new video element returned by flickr.photos.getInfo() : ...\n<video ready=\"1\" failed=\"0\" pending=\"0\" duration=\"14\" width=\"500\" height=\"375\" />\n</photo>\n.... Generating Embed Code The player takes a height, a width, a photo id, a photo secret (required for playing non-public videos), and the argument flickr_show_info_box , which when set to layers over top of the video videographer, and video title info when the video isn’t playing. I’m not going to go over in depth the markup for the player, but here is a quick and dirty PHP function for generating it: #\n# takes a \"Video Player\" source from flickr.photos.getSizes() and optional display arguments\n#\n\nfunction flickr_video_embed($video_url, $width=\"400\", $height=\"300\", $info_box=\"true\") {\n\n    $markup = <<<EOD\n<object type=\"application/x-shockwave-flash\" width=\"$width\" height=\"$height\" data=\"$video_url\"  classid=\"clsid:D27CDB6E-AE6D-11cf-96B8-444553540000\"> <param name=\"flashvars\" value=\"flickr_show_info_box=$info_box\"></param> <param name=\"movie\" value=\"$video_url\"></param><param name=\"bgcolor\" value=\"#000000\"></param><param name=\"allowFullScreen\" value=\"true\"></param><embed type=\"application/x-shockwave-flash\" src=\"$video_url\" bgcolor=\"#000000\" allowfullscreen=\"true\" flashvars=\"flickr_show_info_box=$info_box\" height=\"$height\" width=\"$width\"></embed></object>\nEOD;\n    return $markup;\n\n} Videos in the feeds Videos, unsurprisingly, are included in all of the various RSS and Atom feeds which contain photos.  For each video entry we include a MediaRSS content element that points to the SWF player, and has a content type of “application/x-shockwave-flash”.  Additional we include the stream height and width as the height and width elements in the content element. In RSS 2.0 feed we also include an enclosure element. Uploading Videos Upload videos just like you would a photo.  We’ll do the magic to figure out whether the uploaded file is a video or a photo.  You’ll generally want to use the asynchronous upload methods as videos tend to be larger, and take more time to upload. Videos need to be “transcoded” — turned into an FLV which is playable on the Web.  As this takes time videos aren’t always immediately available for viewing.  You can check the processing status of a video using flickr.photos.getInfo() , and examining the video element. <video ready=\"1\" failed=\"0\" pending=\"0\" duration=\"14\" width=\"0\" height=\"0\"/> ready is watchable, pending is still being transcoded, and failed videos need to be re-uploaded. (possibly in a different format) More Questions? We’ve got an open thread in the Flickr API group discussing video.", "date": "2008-05-1,"},
{"website": "Flickr", "title": "Flickr at XTech (and slides from SXSW…)", "author": ["Simon Batistoni"], "link": "https://code.flickr.net/2008/05/03/flickr-at-xtech-and-slides-from-sxsw/", "abstract": "Once upon a time, webheads used to talk about “conference season”, but it seems that these days there’s always a conference running somewhere. Having dispensed with Web 2.0 , we’re now turning our attention to XTech 2008 , which takes place in Dublin, Ireland from Tuesday May 6th to Friday May 9th. The Flickr team has two talks lined up for those attending XTech. Through a freak scheduling accident they’re back-to-back, giving attendees the exciting opportunity to experience one and a half straight hours of pure Flickr-related goodness: 11:00am, Thu May 8th, Goldsmiths 1, Kellan Elliott-McCrea talks about Advanced OAuth Wrangling 11:45am, Thu May 8th, Goldsmiths 2, Simon Batistoni (that’s me…) presents “Ni Hao, Monde: Connecting communities across cultural and linguistic boundaries” Hopefully we’ll see some of you there! Looking backwards for a moment, this is also a chance to re-post the slides from my previous internationalization-themed talk, “Taking Over the World, The Flickr Way” which I gave at South by Southwest in March. This hour-long session was a high-level overview of some of the challenges and solutions we stumbled upon during the internationalization and localization of Flickr.com which we undertook in the first half of 2007: Versions of the slides in other formats (keynote, swf, pdf) are available here . As for Web 2.0 Expo, some of the team’s presentations should be showing up here in the next week or so, starting with slides from the delectable John Allspaw.", "date": "2008-05-3,"},
{"website": "Flickr", "title": "Slides: Capacity Planning for Web Operations", "author": ["John Allspaw"], "link": "https://code.flickr.net/2008/05/05/slides-capacity-planning-for-web-operations/", "abstract": "You can grab the slides as a PDF from my Web 2.0 Expo talk Capacity Planning for Web Operations , or flip through them below. | View | Upload your own", "date": "2008-05-5,"},
{"website": "Flickr", "title": "Trickr, or Humanising the Developers (Part 1)", "author": ["Daniel Bogan"], "link": "https://code.flickr.net/2008/05/29/trickr-or-humanising-the-developers-part-1/", "abstract": "We busy little nerds of Flickr may act and smell like a bunch of psychotic monkey-bots, but beneath our filth-drenched metal exteriors beat the fleshy hearts of a thousand delicate human flowers. We feel, we love . Ever wondered what keeps us well-oiled? I did, so I asked people, “What do you use to get the job done?” And this, my dubious friends, is how we (rick)roll. [flickr_staff_buddy_icon name=straup] Aaron , Ce sera mauvais français parce que j’ai utilisé l’Internet You’re still wrong Emacs (dired-mode and shell-mode and M-x goto-line) Glimpse (and alias grep grep -n -r -e) Tabs and virtual workspaces [flickr_staff_buddy_icon name=bees] Cal , Baconmeister Fuck off and die shitty pc laptop w/ xp pro twin 20″ monitors noted explorer ff & thunderbird putty / pageant / plink winscp cygwin msys & mingw wireshark paint shop pro 5 miranda calc & chamap tortoise cvs/svn beyond compare apache/mysql/php ms office w/ visio itunes [flickr_staff_buddy_icon name=revdancatt] Dan , The Rev. I don’t do quotes 2 Machine setup; MacBook Pro for writing code, TextWrangler (off-white Lucida Grande 11pt font on blue background, for reduced eye-strain), Safari & Opera for (final) testing. PC for testing, with IE6,7,blah + MS Script debugger. Most testing takes place in Firefox + Firebug (cannot live without firebug). Monitor rotated 90 degrees to give Firebug more real estate for hacking around the dom, editing js script on the fly, etc. Extras: Headphones Pink noise generator (volume set to one notch down from default) to drown out background noise. Ocean Waves generator to take the edge off the pink noise. last.fm radio tag search “ambient” to take the edge off the pink noise and ocean waves. Quicksilver , to get stuff done quickly. Visor for quick terminal access. WriteRoom , for taking notes (again off-white text on a blue background). [flickr_staff_buddy_icon name=dunstan] Dunstan , He’s like, got a dog and stuff Mistakenly included on the engineering mailing list for 505 days and counting Macbook Pro Textmate Photoshop Transmit Safari Firefox+Firebug IE (in Parallels) Quicksilver [flickr_staff_buddy_icon name=eric] Eric , Teenage Mutant Ninja Scripta Please, just work Powerbook, BBEdit, Perl scripts to manage scp+cvs+Flex+compression, Firefox w/ Firebug and Webdev toolbar, Flex 3 SDK, Terminal, nano, Parallels. [flickr_staff_buddy_icon name=kellan] Kellan , Rebellious off-worlder I’d rather be building cloud castles MBP, a hot-rodded version of Textmate, QuickSilver+Terminal.app (what’s the Finder?), Thunderbird + keyconfig for threading and archiving, SSHKeychain, grep, awk, tree, QuickProxy for Firefox (2.x), WordPress.com, last.fm, Pandora, and Adium. used to use PHPfi, but less lately. [flickr_staff_buddy_icon name=murphy_slaw] Murphy , Secret ops mole … –force –yes –quiet > /dev/null 2>&1 MacBook Pro iTerm vim, kill, screen, awk, rsync, mtr, nmap, strace, gdb Wireshark SSH Agent Thunderbird + Enigmail Firefox + Firebug + SwitchProxy + Nagios Checker Adium Caffeine Home Zone Coming soon: more responses!", "date": "2008-05-29,"},
{"website": "Flickr", "title": "Visualizing 4.5 years of Flickr development", "author": ["Cal Henderson"], "link": "https://code.flickr.net/2008/06/26/visualizing-flickr-development/", "abstract": "We were impressed with Michael Ogawa’s code_swarm project , so were understandably excited when he made the source available (under the GPL v3). We sprang into action, avoiding the real work we were supposed to be doing and created some visualizations of the main Flickr subversion repository. In this visualization, blue represents PHP, green is HTML, red is Java, purple is CSS and JavaScript, Cyan is Flash and ActionScript, with yellow filling in for everything else. Myles took it a step further, using the tool to visualize our internal bug tracking system. In this movie, each node represents an issue, flashing red as it was opened, orange as it was assigned, blue as we argued about what to do and final green when it was resolved. This required a little modification of the software to allow for states on nodes, so that the node color can change as the state changes. Myles has also been working on some modifications to improve upon the abrupt endings. New movies might get posted here if they’re awesome enough. We’re hard at work (well, sort-of-work) thinking up new things to visualize and new ways to present the data. If you have some bright ideas, why not post them in the code forum .", "date": "2008-06-26,"},
{"website": "Flickr", "title": "WebMonkey, Flickr API and Python", "author": ["Kellan Elliott-McCrea"], "link": "https://code.flickr.net/2008/06/27/webmonkey-flickr-api-and-python/", "abstract": "The reborn-phoenix-like-from-the-ashes WebMonkey has a tutorial up on Getting Started with the Flickr API using the Python API library . Covers fetching favorites (an under mashedup feature if there ever was one), and plotting geotagged photos on Google Maps.", "date": "2008-06-27,"},
{"website": "Flickr", "title": "Trickr, or Humanising the Developers (Part 2)", "author": ["Daniel Bogan"], "link": "https://code.flickr.net/2008/06/30/trickr-or-humanising-the-developers-part-2/", "abstract": "Back for more, my wee little tin miners? As we say ’round these parts, Onwards ! But first thanks to Rafe , and Stephen for playing along. [flickr_staff_buddy_icon name=mylesdgrant] Myles , POWER USER! Testing on dev is for the unconfident. Macbook Pro, Textmate, scp, Firefox, Firebug, ack, Quicksilver, iTerm, vi, Mail.app. [flickr_staff_buddy_icon name=norby] Norby , Ops Succubus Sleep! What is it with you people and sleep? Y!-issue MBP (upgraded to Leopard w/ spaces) Terminal, ssh, Safari, Firefox, Nagios, vi tunnels >> VPN, have more than one RSA key if you work remotely :) [flickr_staff_buddy_icon name=laloyd] Paul L , Roaming (not Roman) Yeti Gngghhhghghhh MacBook Pro, JDK 1.6, IntelliJ Idea 7.03, iTerm, one reliable crazy Canadian-Russian. [flickr_staff_buddy_icon name=schill] Schill , Lil’ Javascript charmer [ This space left intentionally blank ] MBP, TextMate, A-grade (Fx/Safari/Opera) browsers, IE 6/7/8 + MS Script debugger via Parallels. GIMP for the odd image edit. Dell 24″. Fun stuff: Beyerdynamic DT-880 headphones, iTunes + last.fm + SOMA FM for muzak. Finger rocket defense system. Grande dark roast in the AM. [flickr_staff_buddy_icon name=sm] Serguei , Ex-KGB Field Agent Comments are lying, code tells the truth! PC laptop. JDK 1.6, IntelliJ Idea 7.03, FAR manager, SecureCRT ssh client. kill, especially in its most radical form kill -9. And tail -f , I can watch logs for hours, it’s better than TV. [flickr_staff_buddy_icon name=hitherto] Simon , Totally bi(linguisticalated) Um, no, that won’t work in French MBP Textmate Firefox 3 Safari Parallels (for testing in IE) Apple Terminal Apple Mail Quicksilver grep, vi, perl, dozens of bash shortcuts. and my personal favourite for code review : “cvs diff | mate”", "date": "2008-06-30,"},
{"website": "Flickr", "title": "Wildcard Machine Tag URLs", "author": ["Aaron Straup Cope"], "link": "https://code.flickr.net/2008/07/18/wildcard-machine-tag-urls/", "abstract": "Photo by cackhanded If you’re not already familiar with machine tags the easiest way to think of them is being like a plain old tag but with a special syntax that allows users to define additional structured data about that tag. In turn the magic space hamsters that run the site have been trained to recognize, index and allow for searches across multiple facets of a given machine tag. Machine tags have three parts : a namespace which is like a subject or a topic;  a predicate which is a like a property of that topic; a value which is … well, a value. For a more thorough introduction to the subject I’d recommend reading the announcement we made in the Flickr API discussion group when machine tags were first added to the site. If you’d like to know even more, after that, there is good collection of links available on del.icio.us . Which brings us to the part where I tell you that we’ve added the ability to search for machine tagged photos in plain old tag URLs (as well as in tag searches on the Flickr search page ) using the facetted query syntax that has always been available in the API. For example : All the photos tagged “flickr:user=bees”, aka Cal : http://www.flickr.com/photos/tags/flickr:user=bees That’s a trick, really. You’ve always been able to do this since machine tags are just tags. The New-New means you can be even more granular in what you are looking for. How about : All photos with Flickr users : http://www.flickr.com/photos/tags/flickr:user= Or Upcoming.org users : http://www.flickr.com/photos/tags/upcoming:user= Or even Facebook users : http://www.flickr.com/photos/tags/facebook:user= Or simply all users regardless of service (or namespace) : http://www.flickr.com/photos/tags/*:user= Maybe, all the photos in the flickr namespace : http://www.flickr.com/photos/tags/flickr:*= But, seriously, back to Cal : Cal, across services (or namespaces) : http://www.flickr.com/photos/tags/*:user=bees All Cal. All the time : http://www.flickr.com/photos/tags/*:*=bees And no, you can not do this. No ponies for you if you try  : http://www.flickr.com/photos/tags/*:*= The wildcard URL syntax is also available for an individual user’s tags : These are all my photos that have been machine tagged with either a Geonames , Places or GeoPlanet (née Where on Earth ) locality ID : http://www.flickr.com/photos/straup/tags/*:locality= Or photos in the George Eastman House’s photostream that were developed using the daguerrotype process : http://www.flickr.com/photos/george_eastman_house/tags/photo:process=daguerreotype Now for the list of caveats and Known-Knowns : At the moment it is still not possible to poke around the hierarchy of a given machine tag : all the predicates for a namespace; all the unique pairs of namespace and predicates; that sort of thing. It is On The List ™ and hopefully we can offer up something for you to play with, even if it’s just in the API to start with, shortly. Values in wildcard URLs should are treated the same way regular tags are in URLs. That is “san francisco” becomes “sanfrancisco” or in machine tag speak : *:*=sanfrancisco . In the examples above, I’ve illustrated namespaces that are used to denote one service or another. It is important to remember that there are no rules about what can or should be a namespace. Like tagging, the hope is that the various communities will arrive at and adapt a consensus according to their needs. Photo by straup In the meantime, kick back and enjoy photos taken by people on their Dopplr trips , photos by people who really really like airplanes or photos by people who are interested in possums (not to mention all manner of marsupials ) or whatever else comes to mind!", "date": "2008-07-18,"},
{"website": "Flickr", "title": "Location, keeping it real on the streets, yo!", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2008/08/08/location-keeping-it-real-on-the-streets-yo/", "abstract": "Or something like that anyway. Over on the artsy Flickr Blog we Introduce a new way to geotag , which has a nice pop-up map, which you can drag around, or just enter in the latitude/longitude by hand, something amazingly you couldn’t just do before. However, ‘ere on the Dev blog there’s something else that interests us about this. A thing we call “Corrections” and it’s tucked down at the bottom when you go edit a photo’s location … So what’s it all about? Well something we’ve been fighting, tweaking, exploring for the last forever is getting the Reverse Geocoding working in a way that makes everyone happy. In a nutshell Reverse Geocoding is when you take a Latitude and Longitude and then say where that point is. Ideally in a way that’s sensible to humans. In the above image you ask, “Where’s 37.7947N 122.402W?” and we used to say “San Francisco”. We used to stay at the general city level because it turns out that people get more upset at being told they live/work/take photos in the wrong neighborhood than having no neighborhood at all. Turns out the solution we’ve gone with it just simply to ask, at least in the first instance … Due to wonderful design we hopefully make this all look very easy, but let me tell you, it’s not, it’s all terribly, terribly hard. But more on that in a second. So now on photos we’re saying which Neighborhood we think the photo (or video) is taken in, and let you tell us otherwise. The grand idea is that if enough people start to ‘correct’ a neighborhood then we can feed that knowledge back into the system and slowly over time get our database to match what our actual users say. The problem has moved slightly from “Why does it say my photo was taken in xxxx” to “Why doesn’t it list alternative place xxx”, but that’s a slightly better problem … anyway … I’ll cover how people can start to get this back out again, so we can all share the love in another post soon. On a slightly more philosophical level, it’s a never ending process. We’ll never reach a point where we can say “Right that’s in, all borders between places have been decided”. But what we should end up with are boundaries as defined by Flickr users. As an example a lot of our UK neighborhood data comes from government and local council records (probably). And while that data is very good for the purpose it was originally gathered for, it turns out people can be very specific and touchy about someone telling them they live in Upper Tinshire or Lower Tinshire, when clearly they live in the other one, because obviously this side of the street all the way up to the Post Office is Upper Tinshire and the Fox and Hound Inn Round-a-bout represents the border, of course, Duh! And this goes on around the world all the time. For us, it’s a first small step into an experiment, and actually a pretty big experiment as we’re potentially accepting “corrections” from our millions and millions of users. We’re not quite sure how it’ll all turn out, but we’re armed with Maths, Algorithms and kitten photos. Rock on! Related: A while ago the lovely people at O’Reilly asked us if we’d like to talk about something at Where 2.0 . Which was nice, and as we didn’t have anything to particularly promote or sell it left us free to talk about what was interesting to us, which turned out to be this very thing. So if you want to see two people say “Errrrr” and “Ummmmm” a lot (according to Aaron, I haven’t seen it myself as I can’t stand watching myself talk) for 15mins saying pretty much what I typed up there, but with a cool bit by Aaron where he talks about how hard Reverse Geocoding is, then here’s a video … The Video And here are the slides… The Slides | View | Upload your own", "date": "2008-08-8,"},
{"website": "Flickr", "title": "Defining the boundaries we are all within", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2008/08/18/defining-the-boundaries-we-are-all-within/", "abstract": "Last week I made a blog post about what we call ‘corrections’ and because a picture is worth a thousand words, here’s where people have been fixing things in Europe … … and over in the US … … as expected most of the corrections to neighborhoods are taking place in major cities. Also seemingly most of the UK, presumably because the population is high and our current data is messy (or too abstract) there. As we get more of this stuff back, the process of feeding it into the system will get underway (in some form or other). I wonder that as that happens, we’ll see the corrections move away from already heavily corrected locations like cities, or if they’ll continue to be areas that appear to have highly contested borders. Only time will tell I guess, we’ll keep tracking it. Map extracts taken from this world map by Serguei.", "date": "2008-08-18,"},
{"website": "Flickr", "title": "Standard Photos Response, APIs for a civilized age.", "author": ["Kellan Elliott-McCrea"], "link": "https://code.flickr.net/2008/08/19/standard-photos-response-apis-for-civilized-age/", "abstract": "Funny story.  I went to write a blog post and when the time came to link to the documentation of our standard “standard photos response” structure, I found we had never documented it! Okay.  Maybe that wasn’t so funny. But anyway, this is the blog post before that other blog post, so that when I write that other blog post I’ve got something to point to. And besides you should know this stuff if you’re using the API.  It’s good stuff. Standard Photos Response The standard photos response is a data structure that we use when we want to return a list of photos.  Most prominently the ever popular swiss-army-API flickr.photos.search() uses it, but also methods like flickr.favorites.getList() or flickr.groups.pools.getPhotos() . Beyond a common structure that gets serialized across all our different API response formats , standard photos response methods share a common set of arguments for sorting and paging (after all these are lists of photos), and the special extras argument. Standard Photo Response, the XML Serialization You’re basic standard photos response looks like this.  It’s just an envelope for delivering a list of photos. <rsp stat=\"ok\">\n  <photos page=\"1\" pages=\"7\" perpage=\"100\" total=\"608\">\n    <photo id=\"2777191844\" owner=\"51035734193@N01\" secret=\"653a19d017\" server=\"3059\" farm=\"4\" title=\"FAIL\" ispublic=\"1\" isfriend=\"0\" isfamily=\"0\"/>\n    <photo id=\"2771521705\" owner=\"51035734193@N01\" secret=\"1878507379\" server=\"3178\" farm=\"4\" title=\"In the street\" ispublic=\"1\" isfriend=\"0\" isfamily=\"0\"/>\n  </photos>\n</rsp> We’ve got the standard Flickr <rsp> root element, a <photos> container describing the full list and the page we’re on, and some <photo> elements that include everything we need to display a photo . extras : the concept Another largely undocumented deep structure in the Flickr API is a distinction between getList() and getInfo() methods.  We tend to return a pared down list of identifiers, and provide methods for getting more info about individual items.  Generally it’s a very useful pattern, and saves us all bandwidth, processing, and data rot. However sometimes (often?) you’re wanting to display a bunch of photos, and having to roundtrip to call flickr.photos.getInfo() for every single one of them is annoying.  (not to mention slow, and likely to get you frowned upon by our ops team) That’s where extras come in.  The idea behind extras is you can selectively enrich the bare bones list I showed you earlier with the metadata you need to display your bunch of photos, without the round trip, and without fetching more then you’ll need. extras : the details There are currently 13 different extras available, and we add new ones periodically as new concepts come online, or you make a compelling enough case for them. As of today they are: license , date_upload , date_taken , owner_name , icon_server , original_format , last_update , geo , tags , machine_tags , o_dims , views , media . license Is the photo “All rights reversed”? Licensed under one of the CC license? <photo id=\"2777191844\" owner=\"51035734193@N01\" secret=\"653a19d017\"\n server=\"3059\" farm=\"4\" title=\"FAIL\" ispublic=\"1\" isfriend=\"0\" isfamily=\"0\" \n license=\"3\"/> license=”3” means Attribution-NonCommercial-NoDerivs , you can get our mappings with flickr.photos.licenses.getInfo() . date_upload , date_taken , last_update When was the photo uploaded to Flickr?  When do we think it was taken? When was its metadata last twiddled? <photo id=\"2772368826\" owner=\"51035734193@N01\" secret=\"1078392104\"\n server=\"3122\" farm=\"4\"       title=\"Finger on the button\" ispublic=\"1\" isfriend=\"0\" isfamily=\"0\"\n license=\"3\"  dateupload=\"1219006901\" datetaken=\"2008-08-17 12:38:06\" \ndatetakengranularity=\"0\"     lastupdate=\"1219117103\"/> Yes the extras param is called date_upload the attribute is dateupload , what can I say, legacy.  Notice also that dateupload and lastupdate are epoch seconds, while datetakengranularity is probably best ignored. owner_name and icon_server Everything you need to properly credit the photographer, including their name, and the info necessary to display their buddyicon . <photo id=\"2772368826\" owner=\"51035734193@N01\" secret=\"1078392104\" \nserver=\"3122\" farm=\"4\" title=\"Finger on the button\" ispublic=\"1\" isfriend=\"0\" isfamily=\"0\" \nownername=\"kellan\" iconserver=\"54\" iconfarm=\"1\"/> geo If the photo was geotagged include the latitude, longitude, and accuracy of the geotagging. <photo id=\"2772368826\" owner=\"51035734193@N01\" secret=\"1078392104\" \nserver=\"3122\" farm=\"4\" title=\"Finger on the button\" ispublic=\"1\" isfriend=\"0\" isfamily=\"0\" \nlatitude=\"40.714666\" longitude=\"73.957333\" accuracy=\"16\"/> tags and machine_tags Note these are the “clean” versions of the tags and machine tags, which means spaces, and most punctuation will have been stripped.  Safe to display in HTML, and useable as URL fragments. <photo id=\"2772368826\" owner=\"51035734193@N01\" secret=\"1078392104\" \nserver=\"3122\" farm=\"4\" title=\"Finger on the button\" ispublic=\"1\" isfriend=\"0\" isfamily=\"0\" \ntags=\"nyc streetart williamsburg ph:camera=iphone3g\" \nmachine_tags=\"ph:camera=iphone3g\"/> original_format and o_dims Assuming you’re making API calls as a member who is authorized to download a photo (e.g. the photographer) you can ask for details about the unmodified, full resolution photo that was uploaded.  Get the original file format, the secret needed to construct the URL to the photo, and what the original’s dimensions are. <photo id=\"2772368826\" owner=\"51035734193@N01\" secret=\"1078392104\" \nserver=\"3122\" farm=\"4\" title=\"Finger on the button\" ispublic=\"1\" isfriend=\"0\" isfamily=\"0\" \noriginalsecret=\"xxxxxxxx\" originalformat=\"jpg\" o_width=\"1200\" \no_height=\"1600\"/> views How many times has this photo been viewed by folks other then the person who uploaded it? <photo id=\"2772368826\" owner=\"51035734193@N01\" secret=\"1078392104\" \nserver=\"3122\" farm=\"4\" title=\"Finger on the button\" ispublic=\"1\" isfriend=\"0\" isfamily=\"0\" \nviews=\"9\"/> media Is it a photo?  Or a video?  Has it been processed and is it ready for displaying? ( media_status is a lot more useful for videos) <photo id=\"2771521705\" owner=\"51035734193@N01\" secret=\"1878507379\" \nserver=\"3178\" farm=\"4\" title=\"In the street\" ispublic=\"1\" isfriend=\"0\" isfamily=\"0\" \nmedia=\"photo\" media_status=\"ready\"/> Wow.  What a list.  Really, what more could anyone ever want?  (that’s rhetorical) The punch line That’s standard photos responses, and how to use extras (paging and sorting is left as an exercise to the reader).  Mastering the format is the key to building both interesting and performant API applications.  use the metadata, love the metadata, and ditch the round trip. And now for the that next blog post I mentioned ….", "date": "2008-08-19,"},
{"website": "Flickr", "title": "Kitten Tuesday", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2008/08/19/kitten-tuesday/", "abstract": "Personally, I’m slightly worried that Kellan’s last post didn’t have enough pretty pictures in (as fascinating as it was). We’re also looking at sorting out the font/style of the blog if you’re reading the it on the interwebs, it’s a little, hummmm, yucky at the moment. By way of an apology here’s a kitten. Photo by alasam", "date": "2008-08-19,"},
{"website": "Flickr", "title": "API Responses as Feeds", "author": ["Kellan Elliott-McCrea"], "link": "https://code.flickr.net/2008/08/25/api-responses-as-feeds/", "abstract": "You know three things that would be cool? the ability to subscribe to the output of a Flickr API call in a feed aggregator the ability to get the results of Flickr API calls as KML (or GeoRSS) if all those devices that support RSS feeds (like photo frames!) also supported the Flickr API. You see where I’m going with this don’t you? You can already specify that you want the output format of a Flickr API call to be REST  (POX), XML-RPC, SOAP (shudder, not sure that one still works), JSON, or serialized PHP.  We always wanted to support formats like KML, or Atom but we were never quite sure how to represent the results of a call to flickr.photos.getInfo() or flickr.photos.licenses.getInfo() as a KML. Last week we finally got around to pushing out our 80% solution — an experimental response format for API methods that use the standard photos response format that allows you to request API responses as as one of our many feed formats . You can now get the output of flickr.photos.search() , or flickr.favorites.getList() as Atom, or GeoRSS, or KML, or whatever. API Feed Types The syntax is \"&format=feed-{SOME_FEED_IDENTIFER}\" where the feed identifiers follow the same convention you use when fetching…feeds. feed-rss_100, API results will be returned as a RSS 1.0 feed feed-rss_200, API results will be returned as a RSS 2.0 feed feed-atom_10, API results will be returned as a Atom 1.0 feed feed-georss, API results will be returned as a RSS 2.0 feed with corresponding GeoRSS and W3C Geo elements for geotagged photos feed-geoatom, API results will be returned as a Atom 1.0 feed with corresponding GeoRSS and W3C Geo elements for geotagged photos feed-geordf, API results will be returned as a RSS 1.0 feed with corresponding GeoRSS and W3C Geo elements for geotagged photos feed-kml, API results will be returned as a KML 2.1 feed feed-kml_nl, API results will be returned as a KML 2.1 network link feed And remember, format is an API arg, and needs to be included in your API signature if you’re making a signed API call. Namespaces, pagination, bits and bobs You’ll find the feeds now include the venerable xmlns:flickr=”urn:flickr:” namespace.  This is used to declare bits that don’t fit elsewhere like pagination. Pagination information is passed as a single namespaced (‘urn:flickr:) element under the feed’s root (or “channel” element if it has one). For everything but RSS 1.0 based feeds it looks like this: <flickr:pagination total=\"480\" page=\"1\" pages=\"96\"\nper_page=\"5\" /> For RSS 1.0 we do a little RDF dance: <flickr:flickr>\n     <pagination>\n             <total>480</total>\n             <page>1</page>\n                 <pages>32</pages>\n             <per_page>15</per_page>\n     </pagination>\n</flickr:flickr> Speaking of pagination, to start with we’ve enforced a maximum “per page” limit of 15. If people have a reasonable use case we may consider raising the limit but otherwise we need to account for the extra data/size that feed formats add. You’ll also find some extras like <entry>\n    ...\n    <flickr:views>3</flickr:views>\n    <flickr:original type=\"png\" href=\"http://farm4.static.flickr.com/3074/2783931781_12f84e4079_o.png\" width=\"640\" height=\"480\" />\n</entry> Error Handling If an error occurs, the API will return a 400 HTTP status code. Flickr error codes and message are returned as X-FlickrErrCode and X-FlickrErrMessage HTTP headers. For example: X-FlickrErrCode: 111\nX-FlickrErrMessage: Format \"feed-lolcat\" not found Caveats and Warnings This is EXPERIMENTAL.  It might change, it might go away, but we hope not.  We also could potentially make it better based on all your awesome feedback. This is not available for all methods. If you call photos.getInfo and ask for a feed response format all you will get is an error. Just like any API call (or feed usage or really anything else) you are required to respect the copyright of the photographer. These are still API calls.  All the usual rules about usage apply. You are still bound by the Flickr API TOU and any other rules, capricious or not, we apply to API usage.  Including rate limits.  If you feed this in to an overly-aggressive aggregator we will make your API key cry. (In an entirely non-creepy way) Putting it together: ego feeds Turns out ‘kellan’ is a popular baby name these days, so whenever I go ego surfing Flickr I tend to see pictures of two year olds.  This changes (for now) when I limit my searching to my friends photos.  Using API feeds I can now build an ego feed of photos from my friends, like so: flickr.photos.search:\n   user_id => 51035734193@N01,\n   contacts => all,\n   text => kellan,\n   sort => date-posted-desc,\n   api_key => {API_KEY}\n   auth_token => {AUTH_TOKEN}\n   format => feed-atom_10\n\napi.flickr.com/services/rest/?auth_token=xxxx&user_id=51035734193%40N01&\n   contacts=all&format=feed-atom_10&sort=date-posted-desc&text=kellan\n   &api_key=xxxx&method=flickr.photos.search&api_sig=xxxx Putting it together: near home Or a KML feed of the most interesting, safe, CC licensed photos, within 10 kilometers of a point (say your home), suitable for remixing: flickr.photos.search:\n   license => 1,2,4,5,7,\n   sort => interestingness-desc,\n   lat => 40.661699,\n   lon => -73.98947,\n   radius => 10,\n   safe_search => 1,\n   api_key => {API_KEY}\n   format => feed-kml\n\napi.flickr.com/services/rest/?auth_token=xxx&license=1%2C2%2C4%2C5%2C7&\n  lat=40.661699&lon=-73.98947&radius=10&safe_search=1\n  &format=feed-atom_10&api_key=xxxx&method=flickr.photos.search\n  &api_sig=xxxxx You get the idea.", "date": "2008-08-25,"},
{"website": "Flickr", "title": "Flickr [heart] Burning Man [heart] OpenStreetMap", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2008/08/25/flickr-heart-burning-man-heart-openstreetmap/", "abstract": "[ new map tiles ] Everybody loves Burning Man! Well I don’t, but then I’m grumpy like that. Anyway, imagine my excitement waking up this morning knowing that Burning Man 2008 had just started. Here’s where the photos will start appearing as people get re-hydrated and find an internet connection … Imagine my excitement even more when I saw on Mikel’s blog that OpenStreetMap (OSM) had pushed new map data (and tiles) out the door for this years burning man, see Burning Man Earth Information Release for no more information what-so-ever ;) Hopefully Mikel will update soon with all the work that went into it. A quick tile shuffle later, like what we did for Beijing , and we once more have OSM live in Flickr … So when all those burners come back, it should be easily for them (you know, relatively) to drop those photos onto the map. Why not go see the new map for yourself, it’s rather pretty. George summed it up well when she said … “That’s part of what appealed to us so much about a fantastic project called OpenStreetMap – a free, editable map of the world, made by the people in it. What an exciting prospect to be able to see maps that are alive and have been lovingly created by citizen cartographers all over the world.” It’s the power of The Creative Commons (and even more importantly, people) that make stuff like this work, and obviously we’re hoping to continue to do more. The glib answer I give for why this this sort of thing is important is so I can say “If you’re upset that we don’t have map coverage for where you are, you can grab some friends, go out, and make some”. As sort-of true as that is, probably a better answer is to read Mikel’s posts on Mapping the West Bank and Ups and Downs Mapping the West Bank (once his server recovers from whatever is hitting it, not us!). Which will hopefully illustrate far better than Burning Man why user created mapping data that can be used by anyone willing to use the CC Attribution-Share Alike license, is important.", "date": "2008-08-25,"},
{"website": "Flickr", "title": "Kitten Tuesday", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2008/08/26/kitten-tuesday-2/", "abstract": "I can’t believe it’s Tuesday again already! And only two posts since the last kitten, looks like we have some catching to do too keep up with that other blog . But to show we can be just as Photoggy as them … Photo by Clevergrrl", "date": "2008-08-26,"},
{"website": "Flickr", "title": "Machine Tags, last.fm and Rock’n’Roll", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2008/08/28/machine-tags-lastfm-and-rocknroll/", "abstract": "You Rock, go to gigs and take photos, yes? Love last.fm too? Yeah well, so do we. Turns out they love us as well. For over a year now they’ve been encouraging their users to machine tag flickr photos with last.fm event ids. This is what Martin had to say … “As a geek I’m quite intrigued by Flickr’s machine tags feature we’re using to create this Last.fm/Flickr integration — it can become the basis for a number of interesting Flickr tools, and I’m confident people will come up with all kind of great ideas. (I’m personally waiting for someone to develop a Flickr tool to automatically geotag your event photos based on the venue address provided by Last.fm).” (It takes us a while, but we’ll probably get round to that geotagging thing ‘soon’ Martin) Here’s how they do it; from a specific event page on last.fm you’ll see this … … telling you which machine tag to use. You took some photos at the gig? Well then, throw the tag in there and computers will automatically do the rest. From last.fm’s end, they grab the photos from flickr to show on each event page … it’s also a great way to find other people who were at the same gig as you! From our end (as of a few weeks ago thanks to Cal ) it’ll look like this … … a rather fetching last.fm icon, giving you the badge of honor telling everyone that you were really there and therefore how you loved LCD Soundsystem before everyone else. If its an event we know about then we’ll already have the name. If its a brand spanking new event, we’ll get our system to talk to last.fm’s system, last.fm’s system will invite our system in for coffee, our system will play hard to get for a while, and then in the morning over fried eggs and bacon our system will have the new event name (honestly this is how it works, you should Cal’s code!). So how many photos are tagged with last.fm events? Well around 621,793 last time I checked. See more photos from Heineken Open’er Festival 2007 ( last.fm ). Photo by alex-pl", "date": "2008-08-28,"},
{"website": "Flickr", "title": "Kitten Tuesday", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2008/09/02/kitten-tuesday-3/", "abstract": "Last Tuesday our \"sister\" site blog.flickr had \" other ideas \" about what Kitten Tuesday means. With a positive plethora of photos and videos they go totally to town with kittens, and why the heck not? If you’re into an excess of artistic bourgeoisie I guess, meanwhile, over here on the Dev blog, we’re all about simplicity, efficiency and optimization. Where Kitten Tuesday is just that, a Kitten on Tuesday, otherwise we may as well have Furry Friday Fiesta. So without further ado, A reader writes: \"We’re loving Kitten Tuesdays. Any chance you could consider JJ for a future appearance?\" Why of course, here’s JJ … Photo by Chubby Bat", "date": "2008-09-2,"},
{"website": "Flickr", "title": "Open! Hack! Day!", "author": ["Ross Harmes"], "link": "https://code.flickr.net/2008/09/03/open-hack-day/", "abstract": "Once again Yahoo! is opening its doors to a horde of unwashed hackers. You should join us! In case you missed last years Beck-filled extravaganza, Open Hack Day gives developers 24 hours to build a cool hack and demonstrate it to a packed audience and celebrity judges. Better yet, some of the Flickr staffers will be on hand to answer any of your burning API questions and judge the Best Flickr Hack category (and yes, there are prizes). Open Hack Day kicks off Friday with talks on Yahoo! APIs and technologies. Flickr-related talks include: Getting Started with the Flickr API – Friday, Sept. 12 from 11:00am to 11:50am Building a Purple Pedal GPS-Flickr Bike – Friday, Sept. 12 from 12:00pm to 12:50pm It runs from Friday, Sept. 12 to Saturday, Sept. 13, at Yahoo! HQ in Sunnyvale, CA. If you’re interested, go to the Open Hack Day website to register.", "date": "2008-09-3,"},
{"website": "Flickr", "title": "Who’s On First?", "author": ["Aaron Straup Cope"], "link": "https://code.flickr.net/2008/09/04/whos-on-first/", "abstract": "Normally we don’t talk about upcoming features but in recent weeks we’ve been hard-pressed to keep a lid on our renewed excitement for kittens . We’re betting 2009 will be a big year for kittens and it is why Dan and Heather are taking so much time to hash out the details for the new flickr.kittens API framework which we’re confident will re-embiggen-ize the how, the what and the why of photo-sharing! To pass the time, until then, I’m going to talk about some of the geo-related API methods that have been released in the last few months, but perhaps not properly explained in detail and introduce a new minty-fresh method which hasn’t been discussed at all! Cake for breakfast First, the new stuff. Places for a user We’ve added a new authenticated method to the flickr.places namespace called flickr.places.placesForUser which returns the top 100 unique places, scoped by place type , where a user has geotagged photos. For example, I’ve geotagged photos in the following countries: # ?method=flickr.places.placesForUser& place_type=country <places total=\"7\">\n\t<place place_id=\"4KO02SibApitvSBieQ\" woeid=\"23424977\"\n\t\tlatitude=\"48.890\" longitude=\"-116.982\"\n\t\tplace_url=\"/United+States\" place_type=\"country\"\n\t\tphoto_count=\"1264\"> United States </place>\n\t<place place_id=\"EESRy8qbApgaeIkbsA\" woeid=\"23424775\"\n\t\tlatitude=\"62.358\" longitude=\"-96.582\"\n\t\tplace_url=\"/Canada\" place_type=\"country\"\n\t\tphoto_count=\"307\"> Canada </place>\n\n\t<place place_id=\"3s63vaibApjQipWazQ\" woeid=\"23424950\"\n\t\tlatitude=\"39.895\" longitude=\"-2.988\"\n\t\tplace_url=\"/Spain\" place_type=\"country\"\n\t\tphoto_count=\"67\"> Spain </place>\n\t<place place_id=\"6immEPubAphfvM5R0g\" woeid=\"23424819\"\n\t\tlatitude=\"46.712\" longitude=\"1.718\"\n\t\tplace_url=\"/France\" place_type=\"country\"\n\t\tphoto_count=\"60\"> France </place>\n\t<place place_id=\"DevLebebApj4RVbtaQ\" woeid=\"23424975\"\n\t\tlatitude=\"54.313\" longitude=\"-2.232\"\n\t\tplace_url=\"/United+Kingdom\" place_type=\"country\"\n\t\tphoto_count=\"34\"> United Kingdom </place>\n\t<place place_id=\"mSCQNWWbAphdLH6WDQ\" woeid=\"23424812\"\n\t\tlatitude=\"64.950\" longitude=\"26.064\"\n\t\tplace_url=\"/Finland\" place_type=\"country\"\n\t\tphoto_count=\"24\"> Finland </place>\n\n\t<place place_id=\"mpa01jWbAphICsyCsA\" woeid=\"23424853\"\n\t\tlatitude=\"42.502\" longitude=\"12.573\"\n\t\tplace_url=\"/Italy\" place_type=\"country\"\n\t\tphoto_count=\"8\"> Italy </place>\n</places> The response format is (almost) like all the other places methods, which I guess makes it a standard places response though we haven’t gotten around to standardizing it like we have with photos . Places responses will always contain a “place ID” and a “WOE ID”, a “latitude” and a “longitude”, a “place type” and a “place URL” attribute. They usually contain an “accuracy” attribute but it doesn’t make any sense in the a list of places for a user since the photos, clustered by place type,  may have been geotagged at multiple zoom levels. In this example, we’ve also added a “photo count” attribute since that’s an interesting bit of information. The list of place types with which to scope a query by is limited to a subset of the place types in the Flickr location hierarchy, specifically: neighbourhoods, localities (cities or towns), regions (states) and countries. While place_type is a required argument for the method, there are two other optional parameters you can use to filter your results. Places for a user (and a place) The first is woe_id and ensures that places of a given type also have a relationship with that WOE ID. For example, these are all the localities for my geotagged photos taken in Canada (WOE ID 23424775): # ?method=flickr.places.placesForUser&place_type=locality& woe_id=23424775 <places total=\"5\">\n\t<place place_id=\"4hLQygSaBJ92\" woeid=\"3534\"\n\t\tlatitude=\"45.512\" longitude=\"-73.554\"\n\t\tplace_url=\"/Canada/Quebec/Montreal\" place_type=\"locality\"\n\t\tphoto_count=\"221\"> Montreal, Quebec </place>\n\t<place place_id=\"63v7zaqQCZxX\" woeid=\"9807\"\n\t\tlatitude=\"49.260\" longitude=\"-123.113\"\n\t\tplace_url=\"/Canada/British+Columbia/Vancouver\" place_type=\"locality\"\n\t\tphoto_count=\"59\"> Vancouver, British Columbia </place>\n\t<place place_id=\"zrCws.mQCZj_\" woeid=\"9848\"\n\t\tlatitude=\"48.428\" longitude=\"-123.364\"\n\t\tplace_url=\"/Canada/British+Columbia/Victoria\" place_type=\"locality\"\n\t\tphoto_count=\"9\"> Victoria, British Columbia </place>\n\n\t<place place_id=\"WzwcDQCdAJsL\" woeid=\"4177\"\n\t\tlatitude=\"44.646\" longitude=\"-63.573\"\n\t\tplace_url=\"/Canada/Nova+Scotia/Halifax\" place_type=\"locality\"\n\t\tphoto_count=\"3\"> Halifax, Nova Scotia </place>\n\t<place place_id=\"XlQb2xedAJvr\" woeid=\"4176\"\n\t\tlatitude=\"44.673\" longitude=\"-63.575\"\n\t\tplace_url=\"/Canada/Nova+Scotia/Dartmouth\" place_type=\"locality\"\n\t\tphoto_count=\"1\"> Dartmouth, Nova Scotia </place>\n</places> (Most) places for a user (and a place) The second optional parameter is threshold which requires a place have a minimum number of photos associated with it in order to be included in the result set. Any place that falls below the threshold will be rolled-up in to its parent location. For example, if we search for localities in Canada but also a minimum threshold of 5 photos per location the towns of Halifax and Dartmouth are rolled up in to the province of Nova Scotia: # ?method=flickr.places.placesForUser&place_type=locality&woe_id=23424775& threshold=5 <places total=\"4\">\n\t<place place_id=\"4hLQygSaBJ92\" woeid=\"3534\"\n\t\tlatitude=\"45.512\" longitude=\"-73.554\"\n\t\tplace_url=\"/Canada/Quebec/Montreal\" place_type=\"locality\"\n\t\tphoto_count=\"221\"> Montreal, Quebec </place>\n\n\t<place place_id=\"63v7zaqQCZxX\" woeid=\"9807\"\n\t\tlatitude=\"49.260\" longitude=\"-123.113\"\n\t\tplace_url=\"/Canada/British+Columbia/Vancouver\" place_type=\"locality\"\n\t\tphoto_count=\"59\"> Vancouver, British Columbia </place>\n\t<place place_id=\"zrCws.mQCZj_\" woeid=\"9848\"\n\t\tlatitude=\"48.428\" longitude=\"-123.364\"\n\t\tplace_url=\"/Canada/British+Columbia/Victoria\" place_type=\"locality\"\n\t\tphoto_count=\"9\"> Victoria, British Columbia </place>\n\t<place place_id=\"QpsBIhybAphCEFAm\" woeid=\"2344921\"\n\t\tlatitude=\"44.727\" longitude=\"-63.587\"\n\t\tplace_url=\"/Canada/Nova+Scotia\" place_type=\"region\"\n\t\tphoto_count=\"4\"> Nova Scotia, CA </place>\n</places> Note that we only roll up a single level so if, like the Halifax and Dartmouth, a locality gets rolled up in to its parent region it may still have fewer photos associated with it than the threshold you passed with the method call. If you think this is crazy talk and/or have some other use case we haven’t considered with this model tell us why and we can revisit the decision. Finally, as mentioned above the flickr.places.placesForUser method requires that you include an auth token with minimum read permissions. As always, please take extra care to respect people’s senstitivies when it comes to location data and, above all, don’t be creepy . Meanwhile, back at the Ranch About a week ago, I was asked whether it was possible to use the Flickr API to get a feed of geotagged photos (for a particular place/radius) sorted by interestingness, filtered by CC license to which I quickly replied: WOE IDs, the flickr.places APIs and the “radius” parameter in the flickr.photos.search method should get you what you need! (Also the API responses as syndication feeds support, if you’re being finnicky about the question.) Which got me thinking that while we’ve told people about WOE IDs and the Places API we haven’t really made a lot of noise about the addition of radial queries and the has_geo flag to the flickr.photos.search method. We’ve mentioned it in passing, here and there, but never really tied it all together. So, let’s start with WOE IDs: WOE (short for Where On Earth ) IDs are the unique identifiers in the giant database of places that we (and FireEagle and the rest of Yahoo!) use to keep track of where stuff is. They are also available to you and the rest of the Internets via the public GeoPlanet API . In Flickr, every geotagged photo has up to (6) associated WOE IDs representing a neighbourhood, locality, county (if present), region, country and continent. Geocoding Using the example above, you could start with an API call to the flickr.places.find method which is like an extra-magic geocoder: It not only resolves places names to latitude and longitude coordinates but also returns the WOE ID for the place that contains it. Searching for “San Francisco CA” returns WOE ID 2487956 which you can use to call the flickr.photos.search method asking for all the photos geotagged in the city of San Francisco sorted by interestingness with a Creative Commons Attribution license . Something like this: # ?method=flickr.places.find& query=San+Francisco+CA <places query=\"San Francisco CA\" total=\"1\">\n\t<place place_id=\"kH8dLOubBZRvX_YZ\" woeid=\" 2487956 \" latitude=\"37.779\"\n       \tlongitude=\"-122.420\" place_url=\"/United+States/California/San+Francisco\"\n       \tplace_type=\"locality\">San Francisco, California, United States</place>\n</places>\n\n# ?method=flickr.photos.search& license=4 & sort=interestingness-desc # \t& woe_id=2487956 &extras=tags,geo,license\n\n\n<photos page=\"1\" pages=\"289\" perpage=\"100\" total=\"28809\">\n\t<photo id=\"145874931\" owner=\"37996593020@N01\" secret=\"b695138626\" server=\"52\"\n       \tfarm=\"1\" title=\"bridge\" ispublic=\"1\" isfriend=\"0\" isfamily=\"0\"\n       \ttags=\" sanfrancisco bridge water night reflections geotagged lights\n       \tbaybridge geolat3779274 geolon12239096 sfchronicle96hours\n       \tsanfranciscochronicle96hours\" latitude=\"37.79274\" longitude=\"-122.39096\"\n       \taccuracy=\"16\" license=\" 4 \"\n\t\tplace_id=\"kH8dLOubBZRvX_YZ\" woeid=\" 2487956 \"/>\n\n\t<!-- and so on -->\n</photos> Reverse geocoding You can also lookup the WOE ID for any set of latitude and longitude coordinates. Imagine that you are standing in front the infamous Wall of Rant in San Francisco’s Mission district and you’d like to see photos for the rest of the neighbourhood. If you know your geo coordinates you can call the flickr.places.findByLatLon method to reverse geocode a point to its nearest WOE ID which can then be used to call the trusty flickr.photos.search method. Like this (without the photos.search part since it’s basically the same as above): # ?method=flickr.places.findByLatLon& lat=37.752969 & lon=-122.420844 <places latitude=\" 37.752969 \" longitude=\" -122.420844 \" accuracy=\"16\" total=\"1\">\n\t<place place_id=\"C.JdRBObBZkMAKSJ\" woeid=\" 2452334 \"\n\t    latitude=\"37.759\" longitude=\"-122.418\"\n\t    place_url=\"/United+States/California/San+Francisco/ Mission \"\n\t    place_type=\"neighbourhood\"/>\n</places> Nearby But what if you just want to see photos nearby a point? Radial queries, recently added to the photos.search method, allow you to pass lat and lon parameters and ask Flickr to “show me all the photos within an (n) km radius of a point”. You have always been able to do something like this using the bbox parameter but radial queries differ in two ways: They also save you from having to calculate a bounding box which is, you know, boring. Results are sorted by distance from the center point (you can override this by setting the sort parameter). Trying to do the same with a bounding box query would mean fetching all the results first and then sorting them which both expensive and boring and breaks the pagination model. Radial queries are not meant for pulling all the photos at the country or even state level and as such the maximum allowable radius is 32 kilometers (or 20 miles). The default value is 5 and you can toggle between metric and imperial by assigning the radius_units parameter a value of “km” or “mi” (the default is “km”). # ?method=flickr.photos.search& lat=37.752969 & lon=-122.420844 # \t& radius=1 &extras=geo,tags&min_taken_date=2008-09-01+00%3A00%3A00\n\n\n<photos page=\"1\" pages=\"1\" perpage=\"100\" total=\"9\">\n\t<photo id=\"2820548158\" owner=\"29072902@N00\" secret=\"b2fc694880\" server=\"3288\"\n\t\tfarm=\"4\" title=\"20080901130811\" ispublic=\"1\" isfriend=\"0\" isfamily=\"0\"\n\t\tlatitude=\" 37.751166 \" longitude=\" -122.418833 \" accuracy=\"16\"\n\t\tplace_id=\"C.JdRBObBZkMAKSJ\" woeid=\" 2452334 \"\n\t\ttags=\"moblog shinobu\"/>\n\n\t<!-- and so on -->\n</photos> See that min_taken_date parameter? Like all geo-related query hooks in the flickr.photos.search method you need to pass some sort of limiting agent: a tag, a user ID, a date range, etc. If you insist on passing a pure geo query we will automagically assign a limiting agent of photos taken within the last 12 hours . (I can) has geo Finally (no, really) if you just want to keep things simple and use a tag search you can also call the flickr.photos.search method with the has_geo argument which will scope your query to only those photos which have been geotagged. For example, searching for all geotagged photos of kittens taken in tokyo: # ?method=flickr.photos.search& tags=tokyo,kitten &tag_mode=all \n# \t& has_geo=1 &extras=tags,geo\n\n<photos page=\"1\" pages=\"1\" perpage=\"100\" total=\"60\">\n\t<photo id=\"2619847035\" owner=\"27921677@N00\" secret=\"0979aed596\" server=\"3011\"\n       \t\tfarm=\"4\" title=\"Kittens in Akiba\" ispublic=\"1\" isfriend=\"0\" isfamily=\"0\"\n       \t\ttags=\"japan cat tokyo kitten crowd akihabara unexpected\"\n\t\tlatitude=\" 35.698263 \" longitude=\" 139.771977 \" accuracy=\"16\"\n\t\tplace_id=\"aod14iaYAJ1rDE.R\" woeid=\" 1118370 \"/>\n\n\t<!-- and so on -->\n\n</photos> Maps are purrrr-ty Which is a nice segueway in to telling you that on Tuesday we turned on the map-love for the city of Tokyo , in addition to Beijing and Black Rock City (aka Burning Man) , using the Creative Commons licensed tiles produced by the good people at Open Street Maps . We’re pretty excited about this but rather than just showing another before and after screenshot we decided that the best way to showcase the new tiles was with… well, kittens of course! Sophie’s Choice by tenaciousme Enjoy! That’s a lot to digest in one go but we promise there won’t be a pop quiz on Monday. Hopefully there’s something useful for you in the twisty maze of possibilities, now or in an oh yeah, what about… moment in the future, and maybe even the seed for a brand new API application. Kittens!", "date": "2008-09-4,"},
{"website": "Flickr", "title": "Kitten Tuesday – Office Hygiene Edition", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2008/09/09/kitten-tuesday-office-hygiene-edition/", "abstract": "Our very own frontend engineering, scrumjaxing Scott (tagged in our backend system as; \"dj, flickr, javascript, super star\") sez … \"Correcting many months of breakfast bagel poppyseeds, crumbs and the occasional coffee drip; these things were sustainable events, but a beer spill was the thing that finally put it out of commission.\" Setting aside the whole how do you get beer spilt into your office keyboard question * , Scott, if you’re going to clean your keyboard perhaps we should get one of these for the office instead … … for a far more thorough solution? Photos by .schill and Mrs eNil * Answer, because that’s how we roll at flickrhq.", "date": "2008-09-9,"},
{"website": "Flickr", "title": "Cal Henderson Tuesday", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2008/09/16/cal-henderson-tuesday/", "abstract": "We’re taking a break from Kittens this week. Mainly because I’m on Paternity Leave (see Kitten Tuesday for details) and therefore don’t have all week at work to select a suitable kitten, and no-one else on the engineering team is kitten qualified enough. So instead I give you our very own Cal Henderson (the fellow who has kept Flickr from crashing all these years) presenting at djangocon , on the subject of “Why I Hate Django”. We don’t use Django here at Flickr (because Cal hates it of course) but we do do a lot of scaling, and not only does Cal give a bit of insight into scaling and how things work here at Flickr, but he also talks about kittens a bit, which is nice. So if you have an hour to spare (and frankly if you work with the internets you probably do) this is worth a watch… [Link for RSS readers: http://www.youtube.com/watch?v=i6Fr65PFqfk ]", "date": "2008-09-16,"},
{"website": "Flickr", "title": "Kittens from Ipanema", "author": ["Kellan Elliott-McCrea"], "link": "https://code.flickr.net/2008/09/23/kittens-from-ipanema/", "abstract": "Sssssh! Dan is putting the baby down, so I thought I’d sneak in and do a “Kitten Tuesday” blog post when he wasn’t looking. This blog post is in celebration of our OSM maps in Mexico City , Rio de Janeiro , Buenos Aires , and Sau Paulo , and of course the fabulous “Kittens from Ipanema” interlude you’ll get if you try the sneak preview of the new Flickr home page .", "date": "2008-09-23,"},
{"website": "Flickr", "title": "Flickr Engineers Do It Offline", "author": ["Myles Grant"], "link": "https://code.flickr.net/2008/09/26/flickr-engineers-do-it-offline/", "abstract": "It seems that using queuing systems in web apps is the new hottness . While the basic idea itself certainly isn’t new, its application to modern, large, scalable sites seems to be. At the very least, it’s something that deserves talking about — so here’s how Flickr does it, to the tune of 11 million tasks a day. But first, a use case! Every time you upload a photo to Flickr, we need to tell three different classes of people about it: 1) You, 2) Your contacts, 3) Everyone else. In a basic application, we’d have a table of your photos and a table containing people who call you a contact. A simple join of these tables, every time someone checks the latest photos from their contacts, works fine. If you’re browsing around everyone’s photos or doing a search, you can just check the single photos table. Obviously not going to fly here. For scale, Flickr separates these three lookups into three different places. When you upload that photo, we immediately tell you about it, and get out of your way so you can go off and marvel at it. We don’t make you wait while we tell your contacts about it, insert it into the search system, notify third-party partners, etc. Instead, we insert several jobs into our queueing system to do these steps \"later\". In practice, every single one of these actions subsequently takes place within 15 seconds of your upload while you’re free to do something else. Currently, we have ten job queues running in parallel. At creation, each job is randomly assigned a queue and inserted — usually at the end of the queue, but it is possible for certain high-priority tasks to jump to the front. The queues each have a master process running on dedicated hardware. The master is responsible for fetching jobs from the queue, marking them as in-process, giving them to local children to do the work, and marking them as complete when the child is finished. Each task has an ID, task type, arguments, status, create/update date, retry count, and priority. Tasks detect their own errors and either halt or put themselves back in the queue for retry later, when hopefully things have gotten better. Tasks also lock against the objects they’re running on (like accounts or groups) to make sure that other tasks don’t stomp on their data. Masters have an in-memory copy of the queue for speed, but it’s backed by mysql for persistent storage, so we never lose a task. There are plenty of off-the-shelf popular messaging servers out there . We wrote our own, of course. This isn’t just because we think we’re hot shit, there are also practical reasons. The primary one was that we didn’t need yet another piece of complicated architecture to maintain. The other was that for consistency and maintainability, the queue needed to run on the exact same code as the rest of the site (yes, that means our queuing system is written in php). This makes bug fixes and hacking easier, since we only need to do them in one place. So every time we deploy , the masters detect that, tell all their children to cleanly finish up what they’re doing, and restart themselves to take advantage of the latest and greatest. It’s not just boring-old notifications, privacy changes, deletes, and denormalization calculations for our queue, no. The system also makes it easy for us to write backfills that automatically parallelize, detect errors, retry, and backoff from overloaded servers. As an essential part of the move-data-to-another-cluster pattern and the omg-we’ve-been-writing-the-wrong-value-there bug fix, this is quickly becoming the most common use of our queue. The Flickr engineering team is obsessed with making pages load as quickly as possible. To that end, we’re refactoring large amounts of our code to do only the essential work up front, and rely on our queueing system to do the rest.", "date": "2008-09-26,"},
{"website": "Flickr", "title": "What’s in a Resource?", "author": ["Simon Batistoni"], "link": "https://code.flickr.net/2008/10/08/whats-in-a-resource/", "abstract": "“Flickr is incredibly lucky, because the core of what we do is focused around images.” If you’ve ever heard me talk about Internationalizing Flickr, you’ve probably heard me say those words. And it’s true – more than almost any other website, we deal primarily with content which is language-agnostic (and, to a great extent, culturally agnostic). No matter where we live or what language we speak, our reactions to, say, fluffy kittens have a remarkably similar range (being, approximately, “aww”, “ew” or “achoo!”…) When we first began to define what Flickr’s international incarnation would look like, our primary concern was preserving this global, cross-cultural feeling – a sense that our members’ photos and our visitors come from all over the world, but that the images on Flickr can “speak” to anyone. It’s for that reason that Flickr isn’t divided into national silos – there’s no Flickr France, Flickr Brazil or Flickr USA. Much like Highlander , there can be only one Flickr, and it happens to be accessible in a multi-lingual interface. All well and good, so far. But the biggest issue I wrestled with (and occasionally still do) was what to do with the site’s URLs. The structure we planned for the site required us to take a definite position on a philosophical issue – what, in actual fact, constitutes the “Resource” defined by the Uniform Resource Locator? There are two possible schools of thought on this – one which would argue that the photo page http://www.flickr.com/photos/hitherto/257018778/ with a French interface is materially different to the same page when presented with an English interface. The French page, we might argue, should be named http://fr.flickr.com/photos/hitherto/257018778/ , http://www.flickr.com/fr-FR/photos/hitherto/257018778/ or something similar. On the other hand (and, in fact, the hand we eventually chose), the real “resource” here is the photo (and associated metadata) which the page points to – the interface is immaterial to the content we’re presenting. The big advantage of this approach, especially in a multi-lingual world, is that everyone gets the experience best suited to them. A Korean friend, for example, can send me a link to a photo and I will see it on Flickr with the English interface familiar to me. She, meanwhile, can happily browse Flickr in Korean. Things perhaps get murkier if we consider other areas of the site – the FAQs at http://flickr.com/help/faq/ , for example. Here, all the content is provided by Flickr, and since all of it is in a particular visitor’s chosen language, the entire resource is different. Even here, though, the principle can be made to apply. If an English-speaking German friend asks where they can get help on Flickr, I don’t have to know which language they prefer to view the site in; I can just point them to the FAQ page, and they will have access to the resource (“help with Flickr”) which they needed. Now, admittedly, working for a large multi-national company puts me in contact with more than my fair share of people who speak multiple languages, so maybe this matters more to me than to most people. But as our society grows more connected and more mobile, I like to think that the incidences of these kinds of cross-cultural exchanges will only grow. The biggest downside to our current URL approach comes when search engines try to index our content. Since we don’t have language-specific URLs (and search engine crawlers aren’t in the habit of changing their language settings and retaining the necessary cookies), everything which search engines index on Flickr comes from our English-language interface. As it happens, depending on how smart the search engines are feeling, this isn’t too much of a problem – we do try to surface photo titles and descriptions so that the abstracts make sense. Still, the results returned by Yahoo! and Google for “Buzios” (a beach resort peninsula near Rio in Brazil) give some idea of the nature of the problem. Occasionally, when I’m hit by a case of perfectionism, such things keep me awake at night. And I’m sure that someone, somewhere is wailing and gnashing their teeth over how “Flickr are doing URLs wrong”. All in all, though, I think we made the right decision.", "date": "2008-10-8,"},
{"website": "Flickr", "title": "Flickr Digs Ganglia", "author": ["Gil Raphaelli"], "link": "https://code.flickr.net/2008/10/13/flickr-digs-ganglia/", "abstract": "A number of people have asked us: where did the pretty graphs from last week’s post come from? The answer: Ganglia. It takes a lot of hardware and software to make a site like Flickr run smoothly.  The operations team is responsible for scaling up our monitoring platform to collect all of the metrics we need to make informed decisions about where and when to add new hosts and how urgently, understand how different types of hardware perform with similar real life workloads, determine bottlenecks, and more.  Flickr uses Ganglia to collect, aggregate, and visualize those metrics. So, what is Ganglia?  Briefly, \"Ganglia is a scalable distributed monitoring system for high-performance computing systems such as clusters and grids\"* originally developed at the University of California, Berkeley.  Ganglia is typically run by administrators of High Performance Clusters, large groups of machines working together to complete tasks.  While we have some machines organized into the traditional cluster configuration, for example for log crunching, we simply define a cluster under Ganglia as a group of machines that do similar things but don’t necessarily interact with one another.  For example, all of our web servers in each site are one cluster and all caches another.  Our databases are broken up into multiple clusters based on functionality.  This way, boxes that should be doing the same kind of work can be easily compared against one another. Once Ganglia is up and running you’ll see a number of system level statistics reported by each node.  You can also easily report custom metrics and have them appear on graphs along with the built in metrics.  Before the latest release (the 3.1.x line), this meant some code that calls gmetric and entry in the crontab for scheduling execution of that code.  Ganglia 3.1 offers an additional facility for injecting custom metrics that easy to use and offers some additional power and flexibility over the gmetric + cron approach. For more information on how Ganglia works, see the excellent references on the Ganglia Wiki .  The community is active and very helpful. If you’re just getting started with Ganglia, here are some pointers to save you headaches later: Don’t use the default multicast address 239.2.11.71 for any clusters.  You will start gmond without a config file and it will join that first cluster you defined and you will not be happy that your summary graphs are messy. Do put your RRDs on a ramdisk/tmpfs.  Your disk(s) will thank you. Don’t forget to setup a job to sync those files to some persistent storage periodically – we do it every 10 minutes. Do consider the frequency of data collection and how long you need to keep the data around (and at what resolution).  Do you need to know what the 5 minute load average was on your hosts one year ago today (maybe in relation to other metrics)?  Do you need to know how many uploads per second those hosts were doing then (certainly)?  While individual metric stores can be resized to keep data around for various amounts of time, typically it’s easier to find the least common denominator – the largest amount of time you think you need this information for – and set that in gmetad.conf.  The default gmetad config stores data as: 15 second average for about an hour 6 minute average for about a day 42 minute average for about a week 2 hour 48 minute average for about a month 1 day average for about a year Once you’re up and running with Ganglia you’ll have access to something like the graph from the last post, what we call a stacked graph. Stacked graphs are intended to provide a quick overview of the relative performance of each host in a cluster.  The code has been submitted to the Ganglia project for all to use. Check out http://ganglia.info for more information and stay tuned for more posts about how Flickr uses Ganglia.", "date": "2008-10-13,"},
{"website": "Flickr", "title": "Kitten Tuesday", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2008/10/14/kitten-tuesday-4/", "abstract": "Yay it’s Tuesday and that can mean only one thing! I popped this one into the Kitten Offline Queue three weeks ago and it’s finally made it. With a little tuning I’m told we could easily scale to Kitten November. Photo by Daveblog", "date": "2008-10-14,"},
{"website": "Flickr", "title": "5 Questions for Jim Bumgardner", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2008/10/14/5-questions-for-jim-bumgardner/", "abstract": "We’ve been keeping a careful eye on our Sister Blog to see what they’re up to. Something that’s particularly caught our eye is \" 5 Questions \", asking the same 5 questions to the Flickrverse, with the last question being who we should ask next. And so, we hope, it goes on and on. This is our version, asking questions of those that develop, hack and fiddle with Flickr in new and interesting ways. Of course we couldn’t start with anyone else but KrazyDad (aka Jim Bumgardner). Jim founded the Flickr Hacks group back in the day, a great place to hang out and ask question if you want to learn how to bend Flickr to your will. In 2006 he also coauthored the Flickr Hacks book for O’Reilly and happily for us he hasn’t stopped tinkering with Flickr yet. So, without any further ado, 5 Questions for Jim Bumgardner: 1. What are you currently building that integrates with Flickr, or a past favorite that you think is cool, neat, popular and worth telling folks about? Or both. Jim: It seems like I’m always building something that integrates Flickr. A recent favorite is this interactive mosaic that shows the most interesting photos of the week. The photos are arranged to form a spiral, a form that appears quite frequently in my work. www.coverpop.com/pop/flickr_interesting I have a \"cron job\" which runs on one of my computers at home, which updates this mosaic every week, so the photos in it are always fresh. Incidentally, I prefer to call this process a \"cron joy.\" Oh, nerd humor… 2. What are the best tricks or tips you’ve learned working with the Flickr API? Jim: I think every Flickr hackr should have access to a powerful high level graphics library. My library of choice is ImageMagick combined with the Perl programming language (it also works nicely with Ruby), but the GD library, which works with various languages, and PIL, for Python, are also good. I not only use ImageMagick for building mosaics and graphs, but also for \"under the hood\" kinds of things, like measuring the average colors of photos for the Colr Pickr (see below). 3. As a Flickr developer what would you like to see Flickr do more of and why? Jim: One of the very first Flickr hacks I made was the Colr Pickr www.krazydad.com/colrpickr/ …which allows photos to be selected by color. Since that appeared, I’ve worked on, and seen some fancier variations on the concept, that allow larger quantities of Flickr photos to be selected using multiple colors. But all these systems, require that thousands or even millions of thumbnails be downloaded and analyzed for color. This is because Flickr does not supply \"average color\" information in its APIs, and cannot provide the color search functionality that this data would enable. I would like to see Flickr provide, via it’s APIs, the three most common colors in each photo (using cluster analysis), and provide a way to search for photos which match one, two, or three colors. These parameters, similar to geocode searches, would need to be combined with some other search parameters, such as tags, to narrow the field down. A feature like this would be a godsend to designers. I’ve got sample code for the color analysis, if anyone’s interested… :) 4. What excites you about Flick and hacking? What do you think you’ll build next or would like someone else to build so you don’t have to? Jim: One thing that excites me is the ability to access large quantities of photos that contain valuable metadata, such as the time the photo was taken, or the geocoded location. I used the ‘date taken’ data to construct this very cool graph of sunsets: While most digital cameras store the time within photos, these days, not enough of them automatically store the location. We have to rely on photographers adding the geocoded information manually, and sadly, not enough of them are geeky enough to do it. I’m looking forward to the day, a few years from now, when most of the new photos on Flickr will also contain geocoded information, as this will enable me to make apps which enable a kind of instant photo-journalism of heavily photographed events, such as rallies and parades. We’re seeing the beginnings of these kind of apps now, but we’re barely scratching the surface. 5. Besides your own, what Flickr projects and hacks do you use on a regular basis? Who should we interview next? Jim: GustavoG has made some amazing graphs which exploit and illustrate the Flickr social network. Dan: Thank you, Jim. Next up for our thrilling installment of 5 Questions, GustavoG . Images from krazydad / jbum , earthhopper and GustavoG .", "date": "2008-10-14,"},
{"website": "Flickr", "title": "A Little About The Flickr Bike", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2008/10/20/a-little-about-the-flickr-bike/", "abstract": "A sum of its parts @ Yahoo! Video (for ppl using RSS readers) Shanan’s been driving me insane riding the Flickr Bike around the office. If you don’t know about the Flickr Bike, in a nutshell we have 20 purple bikes tricked out with Nokia N95s geotagging photos and uploading them to Flickr (of course) as people cycle round, there’s a video ( Flickr’s handlebar cameras ) over on cnet, and a frankly awesome write up on Lifehacker . This being the Dev Blog we’re interested in looking under the hood, to use an awful metaphor, and finding out how things work. Fortunately for us Josh wrote up a blog post last week, including links to source code and all that good stuff, meaning I can simple point to that, Huzzah for the internet! Read techy stuff over at ~~> Coding a Networked Bike If you’re more of a moving pictures with sounds than words person, then Lifehacker has 6 videos up that are worth a watch: The Making of the Flickr Bikes . The seconds one of which I included at the start of this blog post. Oh and fyi, they’re Electra bikes painted #7B0099 (Pantone 2602 C).", "date": "2008-10-20,"},
{"website": "Flickr", "title": "Kitten Tuesday, ASCII Edition", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2008/10/21/kitten-tuesday-ascii-edition/", "abstract": "Jason over at Nihilogic has a post up called Getting your ASCII on with Flickr … “You simply enter the search query of your choice and click the “Asciify” button. A request is then sent off to the Flickr server and any photos that match the query are returned. The image data is analyzed and turned into ASCII characters.” Obviously I made a Kitten and you too can get flickrasciified over here . Orginal Photo by pontman .", "date": "2008-10-21,"},
{"website": "Flickr", "title": "Lessons Learned while Building an iPhone Site", "author": ["Ross Harmes"], "link": "https://code.flickr.net/2008/10/27/lessons-learned-while-building-an-iphone-site/", "abstract": "A few weeks ago we released a version of the Flickr site tailored specifically for the iPhone. Developing this site was very different from any other project I’ve worked on; there seems to be a new set of frontend rules for developing high-end mobile sites. A lot of the current best practices get thrown out the window in the quest for minimum page weight and fastest load times over slow cellular connections. Here are a few of the lessons we learned (sometimes painfully) while developing this site. 1. Don’t Use a JavaScript Library or CSS Framework This was one of the hardest things for me to come to terms with. I’m a huge fan of libraries, especially YUI , mostly because they allow me to spend my time creating new stuff instead of working around crazy browser quirks. But these libraries walk a fine line; by definition, they must work across a wide array of browsers and offer enough features to make them worth using. This means they potentially contain a lot of code that you don’t care about and won’t use. This code is dead weight to your site. With such a high percentage of normal web users on broadband connections, we’ve gotten cavalier about what we can include in our pages. 250 KB of JavaScript or more isn’t uncommon for a large site these days. But for sites that are meant to be viewed over slow cellular connections like EDGE, 250 KB is an impossible amount of data. The only way to get the size of your JavaScript down is to selectively pull code out of libraries, and include only what you use. This means you can rip out code meant only for browsers that you won’t support (modular libraries like the new YUI 3.0 allow you to only include the code you use, preventing this problem somewhat). The same goes for CSS. Frameworks make development faster and your final product more robust, but they, like the JavaScript libraries, include code for situations you won’t have to deal with. Every line in your CSS must be custom; each property must be scrutinized to ensure it’s needed. 2. Load Page Fragments Instead of Full Pages When navigating through a site, most of what changes from page to page is the actual content; the JavaScript, CSS, header and footer stay mostly the same. We can use this to our advantage by only loading the part of each page that changes. We did this by hijacking all links of the page: when a link is clicked, we intercept the event, fetch the page fragment using Ajax, and insert the HTML into a new div. This has several benefits: Since you control the entire life cycle of the page fetch, you can display loading indicators or a wireframe version of the page while new pages load All pages that have been fetched will exist within the DOM; clicking the back button (or clicking on a link for a page that has already been fetched) results in an instantaneous page load The page fragments are extremely small; ours are about 800 bytes (gzipped) on average Using this system complicates your code a bit. You need JavaScript to handle the hijacking, the page fragment insertion, and the address bar hash changes (which allow the back and forward buttons to work normally). You also need your backend to recognize requests made with Ajax, and to only send the page content instead of the full HTML document. And lastly, if you want normal URLs to work, and each of your pages to be bookmarkable, you will need even more JavaScript. Despite these downsides, the benefits can’t be ignored. The extra JavaScript code is a one-time cost, but the extra page content that we would have downloaded is saved for every page load . We found it was worth the complication and additional JS in order to dramatically reduce the time it took to load each page. 3. Don’t Build for Just One Device It’s really tempting to build the site for just the iPhone: you can use modern CSS (including things like CSS3 selectors and transformations), you don’t have to hack around annoying browser quirks, and testing is extremely easy. But any single device, even one as ubiquitous as the iPhone, has a limited share of the mobile market, especially internationally. Rarely can you justify the cost of creating a one-off site for a very small number of your users. Luckily the current generation of high-end mobile browsers is excellent in terms of support for modern features. Many phones use a WebKit derivative, including the iPhone, and Symbian and Android phones. Other phones either come with or can use Opera Mobile or the new mobile version of Firefox (called Fennec ). For the most part, very few changes are needed in order to support these browsers. Most of the differences lie in layout. It’s important to structure your pages around a grid that can expand as a percentage of the page width. This allows your layouts to work on many different screen sizes and orientations. The iPhone, for example, allows both landscape and portrait viewing styles, which have vastly different layout requirements. By using percentages, you can have the content fill the screen regardless of orientation. Another option is to detect viewport width and height, and use JavaScript to dynamically adjust classes based on those measurements (but we found this was overkill; CSS can handle most situations on its own). 4. Optimize Everything The browsers on mobile devices operate under much stricter constraints than their desktop cousins. Slower CPUs, smaller amounts of memory, and smaller hard drives mean that less data can be cached. On the iPhone, for instance, only files smaller than 25 KB are cached. This puts very specific limits of the size of your files. For a large site like Flickr, 25 KB worth of JavaScript and CSS barely scratches the surface. To put our files under the limit, we ran everything through the YUI Compressor using the most aggressive settings. We ran all images through compression tools as well (we like pngout and Smushit ), reducing each image file by an average of 40%. We also made heavy use of sprites, where possible. In the end, we were able to go from 90+ second load times over EDGE to less than 7 for an empty cache experience. Using page fragments, we are able to load and display new pages in under a second (though the images in those pages take longer to load). These are not trivial gains, and make the difference between a good mobile experience and a one that is so awful the user gives up halfway through the page load. 5. Tell the User What is Happening Once we hijacked all clicks actions in order to load page fragments, it wasn’t always clear to the user if anything was happening when they clicked on a link. This is especially true on touch devices, where it is difficult to know if the device even detected your action. To combat this problem, we added loading indicators to every link. These tell the user that something is happening, and reassures them that their action was detected. It also makes the pages seem to load much faster, since something is happening right away; if the indicators weren’t there, it would seem like nothing was happening for a few seconds, and then the page would load suddenly. In our testing, these indicators were the difference between a UI that seems snappy and responsive, and one that seemed slow and inconsistent. One Easy Option The iUI framework implements a lot of these practices for you, and might be a good place to start in developing any mobile site (though keep in mind it was developed specifically for the iPhone). We found it especially useful in the early stages of development, though eventually we pulled it out and wrote custom code to run the site.", "date": "2008-10-27,"},
{"website": "Flickr", "title": "Counting & Timing", "author": ["Cal Henderson"], "link": "https://code.flickr.net/2008/10/27/counting-timing/", "abstract": "Here at Flickr, we’re pretty nerdy. We like to measure stuff. We love measuring stuff. The more stuff we can measure, the better our understanding of how different parts of the website work with each other gets. There are two types of measurement we especially like to do – counting and timing. These exciting activities help us to know what is happening when things break – if a page is taking a long time to load, where is that time being spent and what task have we started to do more of. Counting Our best friend, when it comes to stats collection and display is RRDTool , the Round-Robin Database. RRD is a magical database that uses a fixed amount of space for storing stats over time. It does this by storing data in decreasingly lower resolution as time goes by, keeping high resolution per-minute data for the last few hours, a lower per-day resolution data for times last year (the resolution is actually configurable). Since you generally care less about detailed data in the past, this works really well. RRD typically works in two modes – you feed it a number every so often and that number can either be a gauge (such as the current temperature) or a counter (such as the number of packets sent through a switch port). Gauges are easy, but RRD is clever enough to know how counters change over time. It then lets you graph these data points in interesting ways. The problem with RRD is that you often want to count some kind of event that doesn’t have a natural counter associated with it. For instance, we might want to count how many times we connect to a certain cache server from a particular piece of code, so that we can graph it. We know when we connect to it in the code, but there’s no global number tracking that count. We need some central counter which we increment each time we connect, which we can then periodically feed into RRD. This tends to get quickly further complicated by having multiple machines on which the code runs – every one of Flickr’s web servers will run this code, but we care about how many times they connect to the cache server in total . The easiest way to do this is to use a database. When we connect to the cache server, connect to the database and increment a counter. Some other job can then periodically connect to the database, read the counter and store it in RRD. Simple! However, the operation of connecting to the database and performing the write does not have zero cost. It takes some amount of time which may appear trivial at first, but doing that several hundred times a second (lots of the operations we want to track happen very often), multiplied by hundreds of different things we want to track quickly adds up. It’s a little bit quantum theory-ish – the act of observing changes the observed reality. When we’re counting very fast operations, we can’t spend much time on the counting. There’s luckily an operation which we can perform that’s cheap enough to do very often while allowing us to easily collect data from multiple servers at once: UDP . The User Datagram Protocol is TCP’s ugly kid brother. Unlike TCP, UDP does not guarantee delivery of packets, nor the order in which they’ll be received. In return, they’re a lot faster to send and use less bandwidth. When counting frequent stats, losing the odd packet doesn’t really hurt us, especially when we can also graph when we’ve been dropping packets (under Linux, try netstat -su and look for “packet receive errors”). Changing the UDP buffer size (kernel variable net.core.rmem_max ) allows you to queue more packets before processing them, if you find you can’t keep up. In our client code, this is very easy to do. Every language has a different approach (of course!), but they’re all straight forward. Perl my $sock = new IO::Socket::INET(\n\t\t\tPeerAddr\t=> $server,\n\t\t\tPeerPort\t=> $port,\n\t\t\tProto\t\t=> 'udp',\n\t\t) or die('Could not connect: $!');\n\n\tprint $sock \"$valuen\";\n\n\tclose $sock; PHP $fp = fsockopen(\"udp://$server\", $port, $errno, $errstr);\n\tif ($fp){\n\t\tfwrite($fp, \"$valuen\");\n\t\tfclose($fp);\n\t} The trickier part is the server component. We need something that will receive packets, increment counters and periodically save the counter to RRD files. At Flickr we developed a small Perl daemon to do this, using an alarm to save the counters to RRD. The UDP packet simply contains the name of the counter to increment. We can then start to track interesting operations over time: The next stage was to change our simple counters into ones which recorded state. We’re performing a certain operation 300 times a second, but how often is it failing? RRD allows you to store multiple counters in a single file and then graph them together in a variety of ways, so all we needed was a little modification to our collection daemon. The red area on the left shows some failures. Some tasks have very few relative failures, but which are still important to see. That’s easy enough, but just producing two graphs with different Y-axis scales (something that RRD does auto-magically for us). Timing Counting the number of times we perform a certain task can tell us a lot, but often we’ll want to know how long that task took. The simplest way to do this is to perform a task periodically and graph the result. This is pretty simple and something that we use Ganglia for to good effect. The problem with this is that it just tells us the time it took to perform our single test task. This is useful for looking at macro problems, but is useless against tasks that can take a variable amount of time. If a database server is processing one in ten tasks slowly, then that will appear on this graph as a spike (roughly) every ten samples – it’s impossible for us to tell the different between a server that processes 10% of requests slowly and a sever that has periodic slow periods. We could take an average time (but storing an accumulator along with the counter, and dividing at the end of each period) and this gets us closer to useful data. What we really need to know, however, is something like our standard deviation – both what our average is and where the bulk of our samples lie. This allows us to distinguish between a request that always takes 20ms and one that takes 10ms half of the time and 30ms half of the time. To achieve this, we changed our collection daemon to keep a list of timings. At the end of each sampling period (somewhere between 10s and 60s, depending on the thing we’re monitoring), we then sort these times, find the mean and the first and third quartiles . By storing each of these values (along with a min and max) into RRD every period, we can show the average timing along with the tolerance. So our task takes 250ms on average, with 75% finishing within ~340ms and 25% finishing within ~190ms. The light green shaded area shows the lowest 25%. We don’t shade the upmost 25%, since random spikes can cause the Y-axis to become to large that it makes the graph difficult to read. We don’t have this problem with the lowest quartile, since nothing can be faster than zero (and we always show zero on the Y-axis to avoid scale bias). Bringing it all together The next step is to tie together the counting with the timing, to allow us to see how the rate of an operation effected the time which it took to perform. By simply lining up graphs below each other, we can easily see relationships between these different measures: Of course, we like to go a little measurement-crazy, so we tend to sample as many things as we can to look for patterns. This graph from our Offline Task System shows how often we run a particular task, how long it took to execute but also how long it was queued before we ran it. The code for out timing daemon is available in the Flickr Code repository and through our trac install , so you can have a play with it yourself. It just requires RRDTool and some patience. Get graphing!", "date": "2008-10-27,"},
{"website": "Flickr", "title": "KrazyDad  »  Sunrise, Sunset", "author": ["Kellan Elliott-McCrea"], "link": "https://code.flickr.net/2008/10/30/krazydad-sunrise-sunset/", "abstract": "KrazyDad who kicked off our developer 5 questions series the other week, has a post up looking at animated visualizations of sunset and sunrise. If you pay close attention to the differences between the two photos, particularly Florida and the Great Lakes, you’ll see that people prefer to photograph the sun sinking (or rising) behind a body of water. Hence eastern-facing coastlines tend to accumulate more sunrise photos, and western-facing coastlines tend to accumulate sunset photos. – KrazyDad  »  Sunrise, Sunset", "date": "2008-10-30,"},
{"website": "Flickr", "title": "The Shape of Alpha", "author": ["Aaron Straup Cope"], "link": "https://code.flickr.net/2008/10/30/the-shape-of-alpha/", "abstract": "We have a lot of geotagged photos Almost ninety million, as I write this, and the numbers keep growing especially as nearly every new smart phone released to market has not only a camera but also the ability to capture location information with it. For every geotagged photo we store up to six Where On Earth (WOE) IDs. These are unique numeric identifiers that correspond to the hierarchy of places where a photo was taken: the neighbourhood, the town, the county, and so on up to the continent. This process is usually referred to as reverse-geocoding . Over time this got us wondering: If we plotted all the geotagged photos associated with a particular WOE ID, would we have enough data to generate a mostly accurate contour of that place? Not a perfect representation, perhaps, but something more fine-grained than a bounding box. It turns out we can. So, starting today there are 150,000 (and counting) WOE IDs with proper (-ish) shape data, available via the Flickr API . What kind of shapes, you ask? Continents: Countries: States and cities: Even neighbourhoods: Each one of those illustrations represents the boundaries of a particular place whose outline was generated using nothing but the latitudes and longitudes of the geotagged photos associated with that location’s WOE ID . No GIS information was harmed in the creation of these shapes. How cool is that?! How does it work? The short version is: Scary and complicated maths. The longer version is: We are generating alpha shapes using the set of unique latitudes and longitudes associated with a WOE ID. The long version, to quote Tran Kai Frank Da and Mariette Yvinec , is: “Imagine a huge mass of ice-cream making up the space … and containing the points as hard chocolate pieces. Using one of those sphere-formed ice-cream spoons we carve out all parts of the ice-cream block we can reach without bumping into chocolate pieces, thereby even carving out holes in the inside (eg. parts not reachable by simply moving the spoon from the outside). We will eventually end up with a (not necessarily convex) object bounded by caps, arcs and points. If we now straighten all round faces to triangles and line segments, we have an intuitive description of what is called the alpha shape…” (There are also some useful illustrations of what that all means on Francois Belair’s Everything You Always Wanted to Know About Alpha Shapes But Were Afraid to Ask website.) The community of authority and the authority of community This is the important part: Many, if not most, of these shapes will look a little weird. Possibly even “wrong”. This is both okay and to be expected for a few reasons, at least. Sometimes we just don’t have enough geotagged photos in a spot to make it is possible to create a shapefile. Even if we do have enough points to create a shape there aren’t enough to create a shape that you’d recognize as the place where you live. We chose to publish those shapes anyway because it shows both what we know and don’t about a place and to encourage users to help us fix mistakes. If the shape of the neighbourhood is incomplete or looks weird why not consider organizing a photowalk around its edges and when you get home add them to the map. The next time we generate a new shapefile for that neighbourhood it should look more like the place you recognize! We did a bad job reverse geocoding photos for a particular spot and they’ve ended up associated with the wrong place. We’ve learned quite a lot about how to do a better job of it in the last two years but there is a limit to how much human awareness and subtlety we can codify. Sometimes, the data we have to try and work out what’s going on is just bad or out of date. Ultimately, that’s why we added the tools to help users correct their geotagged photos so that we can adjust things to their understanding of the world and begin to map facts on the ground rather than from on high. We are not very sophisticated yet in how we assign the size of the alpha variable when we generate shapes. As far as we can tell no one else has done this sort of thing so like reverse-geocoding we are learning as we go. For example, with the exception of continents and countries we boil all other places down to a single contiguous shape. We do this by slowly cranking up the size of the ice cream scoop which in turn can lead to a loss of fidelity. Does the shape of Florida, or of Italy, include the waters that lie between the mainland and the surrounding islands? It’s not usually the way we imagine the territory that a place occupies but I think it probably does. On the other hand, including the ocean between California and Hawaii as part of the United States would be kind of dumb. Are any of these the correct decisions? We’re not sure yet. A concrete example to illustrate the point. Here are two versions of the island of Montreal , each generated from the same set of coordinates. The version on the left used an alpha number (an ice cream spoon) large enough to ensure that only a single shape was created compared with the version on the right that allowed for two shapes. What’s going on, then? All those photos taken in St. Jean-sur-Richelieu (20 minutes south of Montreal) were added to the map back when we first added geotagging to the site and the information about the province of Quebec was not as detailed as what we have now. Ultimately, we decided to include the version on the left because as Matt Jones recently said : “ The long here that Flickr represents back to me is becoming only more fascinating and precious as geolocation starts to help me understand how I identify and relate to place. The fact that Flickr’s mapping is now starting to relate location to me the best it can in human place terms is fascinating – they do a great job, but where it falls down it falls down gracefully, inviting corrections and perhaps starting conversation .” As with any visualization of aggregate data there are likely to be areas of contention. One of the reasons we’re excited to make this stuff available is that much of it simply isn’t available anywhere else and the users and the developer community who make up Flickr have a gift for building magic on top of the API so we’re doubly-excited to see what people do with it. That said please remember that this it is an aggregate view of things and should be treated more like the the zeitgeist of a place and not as capital-C confirmed facts on the ground or our taking sides in any conflicts (real, imagined or otherwise) between friends and neighbours. These are not maps you should use to guide your spaceship back to Earth but they’re probably good enough to explore the possibilities. Clustr Meanwhile, back in the nuts-and-bolts department: The actual alpha shapes are generated by a program called Clustr, written for us by the fantabulous Schuyler Erle . Clustr is a command-line application written in C++ that takes as its arguments the path to a file containing a list of points (the hard chocolate pieces) and an alpha parameter (the ice cream spoon) and generates a shapefile describing the contour (the alpha shape) of that list. Anecdotally, we’ve seen Clustr plow through a file with four million unique coordinates (representing the continental United States, Alaska and Hawaii) in under three minutes on some pretty modest hardware. The shapedata for a WOE ID is available via the Flickr API using the flickr.places.getInfo method. Not all places have shape data yet so the root <place> element contains a has_shapedata attribute for checking at a glance. Otherwise you can test for the presence of a <shapedata> element. It will look like this: <place place_id=\"4hLQygSaBJ92\" woeid=\"3534\"\n\tlatitude=\"45.512\" longitude=\"-73.554\"\n\tplace_url=\"/Canada/Quebec/Montreal\" place_type=\"locality\"\n\tname=\"Montreal, Quebec, Canada\" has_shapedata=\"1\" >\n\t\n   <!-- all the usual places hierarchy elements --> <shapedata created=\"1223513357\" alpha=\"0.012359619140625\"\n      count_points=\"34778\" count_edges=\"52\">\n      <polylines>\n         <polyline>\n            45.427627563477,-73.589645385742 45.428966522217,-73.587898254395, etc...\n         </polyline>\n      </polylines>\n   </shapedata> </place> Sometime next week, we will also include links to a real live ESRI shapefile , the well-known and mostly-loved lingua franca of the GIS community, for each WOE ID. They were supposed to be included with this release but because of a last minute glitch they need to be prettied up a little first. We think that the inclusion of the polylines will be enough to keep people busy until then. Shapefiles will be included, in the API, as a link to a compressed file you can download separately. For example: Update: The first round of (ESRI) shapefiles have been reprocessed and are now available via the API. Shapefiles are included as a link to a compressed file you can download separately. For example: <urls> <shapefile>\n          http://farm4.static.flickr.com/9999/shapefiles/3534_20081020_S33KR3TSHAPE.tar.gz\n       </shapefile> </urls> Our plan is to generate new renderings on a relatively constant basis, something like every month or two, though we haven’t firmed up any of those details yet. We’ll post about them here or on the API mailing list as things are worked out. But wait, there’s more! Along with the shape data the source code for Clustr is available in the Flickr Code repository and through our trac install , distributed under the GPL (version 2) . Clustr has two major dependencies not included with the source that you will need to install yourself in order to use. They are the Computational Geometry Algorithms Library (CGAL) and the Geospatial Data Abstraction Library (GDAL). Both are relatively straightforward to install on Linux and BSD flavoured operating systems; Windows and OS X are still a bit of a chore. You probably won’t be able to download Clustr and simply plug it in to your awesome web-application today but I am hopeful that in time the community will develop higher level language (Perl, Python, Ruby, you name it…) bindings to make it easier and faster to write tools that build on the work we’ve done so far. photo by Julian Bleecker By the way, there is a still a known-known bug in Clustr rendering interior rings (the donut holes where there are no geotagged photos) in shapefiles. Specifically, they holes are rendered as actual polyline records. You can see an example of the problem in the screenshot of the shapefile for North America, above. We hope to have a proper patch in place by the time we make the ESRI files available next week. As it is since the problem only manifests itself for countries and continents it seemed like a reasonable thing for a version 0.1 release. Update: Clustr 0.2, with a fix for errant interior rings , has been checked in to the code.flickr.com SVN repository . Finally, these are early days and this is very much a developer’s release so we look forward to your feedback and also hope you will be understanding as we learn our way around the gotchas and quirks that will no doubt pop up. In other geostuffs In other geostuffs, we have enabled Open Street Maps tiles for two more cities: Baghdad and Kabul and George has written a fantastic post highlighting some of the photos we’ve found in both cities so go and have a look. Map data CCBYSA 2008 OpenStreetMap.org contributors Enjoy!", "date": "2008-10-30,"},
{"website": "Flickr", "title": "5 Questions for Paul Mison", "author": ["Kellan Elliott-McCrea"], "link": "https://code.flickr.net/2008/11/06/5-questions-for-paul-mison/", "abstract": "One of the key goals for this blog when we launched it 9 months ago was to be a channel for the voice of the Flickr development community.  Most importantly all the amazing developers building on our APIs. Which is by way of introducing our third developer interview here at code.flickr, our first was with Neil Berkman of Photopholow , while our 2nd, and first in the 5 questions format was with Jim Bumgardner a few weeks ago. We’ll be posting an interview from GustavoG (as tapped by Jim) soon, but in the mean time I want to post an interview from Paul Mison aka blech .  Paul is an active participant in the Flickr API group , and I’ve personally been using and loving his new project SnapTrip .  And he likes rainbows, which makes him good by us. 1. What are you currently building that integrates with Flickr, or a past favorite that you think is cool, neat, popular and worth telling folks about? Or both. Paul: My current main project is snaptrip , which works with Dopplr , a website for sharing personal travel information, as well as with Flickr. It helps you to find photos associated with a trip listed in Dopplr, to label them so that Dopplr can find them easily later on, and also lets you geotag them (if you haven’t already). I’ve also written a Greasemonkey user script so that you can see when other people’s photos were taken during a Dopplr trip. In the past I’ve mainly written command line tools; I used a script to migrate images out of my previous home-brewed image hosting to Flickr, and I have another that applies machine tags for EXIF properties (since tags make it easy to find photos; here’s all my photos with a focal length of 50mm ). My great lost project was a web application called groupr that would let you see all the photos in groups you’re a member of, but unfortunately the platform I built it on was withdrawn, and I should rebuild it elsewhere (possibly on Google’s App Engine, like snaptrip; possibly not). 2. What are the best tricks or tips you’ve learned working with the Flickr API? Paul: The API Explorer is wonderfully useful, and it should be everyone’s first port of call when developing an application. Beyond that, I’d advise picking an API framework that does the tricky things (notably, user authentication – I’ve handcoded it myself, and it can be fiddly), but otherwise gets out of your way. snaptrip uses Beej’s Python Flickr API , and I’ve previously used flickraw (for Ruby) and Flickr::API (for Perl). All three are nice and minimal, so that when new API methods are added, you don’t have to update your library. flickraw uses JSON internally, too, which is very nice if (like me) you lean towards dynamic, rather than static, languages. For Greasemonkey scripts, this post from mortimer? about using API calls in GM scripts is great. 3. As a Flickr developer what would you like to see Flickr do more of and why? Paul: Well, a relatively minor request would be for JSON in the API Explorer. More seriously, there’s an entire class of methods I wish existed for groups. For example, the only way to track conversations at the moment is the group RSS feed, which isn’t segregated by thread. There’s no way to find out a group’s administrators or moderators. A final example is that the queue of photos awaiting approval isn’t exposed to the API. While I’m not sure I’d have used all of these in groupr, some of them would have been very handy. Another small request would be for more extras , especially a few method-specific ones. In particular, I’d love to see ‘favedate’ on flickr.favorites.getPublicList. 4. What excites you about Flickr and hacking? What do you think you’ll build next or would like someone else to build so you don’t have to? Paul: For all that my answer to the last question was a demand for more methods, Flickr is exciting both because of the wealth of photography there, and the richness of the methods of getting at it. The geographical data, and access to it, that has emerged over the last year is really interesting, and I’d love to do something with it (and intend to, somehow, in snaptrip). However, for a complete new project, I’ve been poking for a year or so at making your Flickr favourites look a bit nicer, and maybe within another year I’ll actually have something out publicly. (I really need a simple job queue; anyone?) 5. Besides your own, what Flickr projects and hacks do you use on a regular basis? Who should we interview next? Paul: Well, as a Mac user, I’m a fan of Fraser Speirs Flickr Export for getting my images onto the site in the first place, and if I upgrade to an iPhone I look forward to playing more with Exposure. On the web, Dario Taraborelli’s Group Trackr is very nice, and I use fd’s Flickr Scout every now and again to see if anything’s hit Explore. However, I think the most impressive recent project I’ve seen is a desktop app written using Adobe’s Air, called Destroy Flickr , by Jonnie Hallman. There were a lot of subtle UI techniques to hide the latency inherent in talking to a network service, so I think he’d be a great choice for an interview. Kellan: Thanks Paul!  And y’all be looking out for an interview with Jonnie and his oddly named project.", "date": "2008-11-6,"},
{"website": "Flickr", "title": "EXIF, Machine Tags, Groupr, and more Paul Mison", "author": ["Kellan Elliott-McCrea"], "link": "https://code.flickr.net/2008/11/12/exif-machine-tags-groupr-and-more-paul-mison/", "abstract": "After we published Paul’s interview last week , he wrote in to let us know Groupr had been dusted off and was alive and well again.  And followed it up with a post on machine tags, and automated EXIF extraction , two of our favorite topics here at Code.Flickr: Why bother with such a thing? Flickr will extract EXIF metadata, but it won’t allow you to do any aggregate queries on it.  By extracting all the data from my photos into machine tags (and a local SQLite database), it becomes possible to point people at all the photos taken at the wide end of my widest lens, or those taken with a particular make of camera (and to do more complex queries locally). – Flickr, EXIF, Machine Tags", "date": "2008-11-12,"},
{"website": "Flickr", "title": "On UI Quality (The Little Things): Client-side Image Resizing", "author": ["Scott Schiller"], "link": "https://code.flickr.net/2008/11/12/on-ui-quality-the-little-things-client-side-image-resizing/", "abstract": "Make it good. As front-end engineers, our job is ultimately to produce and deliver the front-end “experience.” That is, in addition to the core service (eg., a photo sharing site) which you are providing, you are also responsible for replicating and maintaining the quality of the visual aesthetic, including attention to detail in your UI. Making it really good. It’s “the little things” – small layout changes, single-pixel margin tweaks and color fiddling done as part of this process – that can sometimes seem overly nit-picky and risk being overlooked or sacrificed in order to meet deadlines, but the result of this effort has value: It’s quite obvious when UI polish has been done, and it’s something everyone can appreciate – even if they don’t know it. Getting the fine details down “just so” can take extra time and effort, but in my experience has always been justified – and whenever your work is under a microscope, attention to detail is a wonderful opportunity to make it shine. An exercise in nit-picking With a recent home page redesign on Flickr, we found ourselves needing to show thumbnails for photos at a 48×48 pixel square size, smaller than the standard 75×75 square (currently the smallest available from the servers.) Interestingly (and not surprisingly), browsers differ in rendering images at non-native sizes, with some providing noticeably better results than others. Internet Explorer 6 in particular looked to be the least smooth when scaling images down, but some interesting workarounds are available. Tweaking image resampling in IE IE 7 supports high-quality bicubic image resampling, producing results akin to what you’d see in most image editing programs. It’s disabled by default, but you can enable it using -ms-interpolation-mode:bicubic; . It’s probably safest to apply this only to images you are explicitly showing at non-native sizes. IE 6 is a riskier proposition, but can show improved image resizing when the AlphaImageLoader CSS filter is applied, the same filter commonly used for properly displaying PNGs with alpha transparency. For example, filter:progid:DXImageTransform.Microsoft.AlphaImageLoader (src='/path/to/image.jpg', sizingMethod='scale'); . While there is no transparency to apply here, the resizing method applied gives a higher-quality result. Further investigation After looking at IE 6 and 7, I was curious to see how other browsers handled image resizing. Opera is a little rough around the edges relative to others, but overall the quality of scaled images is pretty good. Findings UI quality is an ongoing process, the result of a shared interest between design and engineering groups in nit-picking, always trying to make things better. This particular image resizing example came out of a home page redesign, but was by no means the first use of it on the site. Despite the occasional limitations and peculiarities of the client-side, one of its upsides is its flexibility: You can often find creative solutions for just about any quirk.", "date": "2008-11-12,"},
{"website": "Flickr", "title": "5 Questions for Gustavo", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2008/11/19/5-questions-for-gustavo/", "abstract": "Following our second interview Kellan snuck in 5 Questions for Paul Mison before I managed to tap Jim’s suggestion of Gustavo . I think Drift Words sums Gustavo up nicely with “Although he’s far too clever, he makes up for it by using his polymath powers for good.” … and charts and stunning graphs. It’s always a pleasure to see what Gustavo comes up with. And on that note … 1. What are you currently building that integrates with Flickr, or a past favorite that you think is cool, neat, popular and worth telling folks about? Or both. Gustavo: First of all I must say: I’m not really a flickr tool developer. The only real tools I created are the quite minimalistic FlickRandom and Contact Crossing (very kindly hosted by Jim Bumgardner ). Rather, I use flickr’s API to mine flickr’s database for interesting information, usually leading to some visualizations or at the very least a bunch of graphs . One such visualization led to a browser for exploring related groups. Not a full-fledged tool since it explores static and outdated data. In other words, what I do with flickr’s API is not tool development but analyses , trying to figure out the structure and dynamics of flickr’s social network , group content and participation , etc. The most recent analysis I performed attempted to understand how exposure to photo content flows through the contact network. Starting from a basic pattern combining three types of network relations, I quantified the importance of a user’s social network in determining his or her exposure to content of interest. 2. What are the best tricks or tips you’ve learned working with the Flickr API? Gustavo: Cache, cache, cache. Depending on what you want to do, you might find yourself retrieving certain bits repeatedly, and you’ll definitely want to build a good local cache. Be careful though, as it’s all too easy to underestimate the size and complexity of the data. Your cache might unexpectedly morph from a speed asset to a sluggish monster. Another potential problem from underestimating the size and complexity of the data is that you might suddenly discover that your code issued many more API calls than you expected. Remember to play nice, and keep an eye on your API key usage graphs. Expect unexpected responses: It’s always good to make the code smart enough to realize the server’s response is not exactly what was expected, be it in completeness, format, special characters in free text, etc. Finally, still on the theme of unexpected responses: Grow a thick skin, as there will always be someone who will misinterpret your motives for developing a tool that “uses” their information. 3. As a Flickr developer what would you like to see Flickr do more of and why? Gustavo: Let me suggest two: one very simple, the other very complex. The simple one is a “no special perms” authentication level. For some purposes, as a developer I’d like to have the user authenticate for the sole purpose of knowing who they are – I might want to give certain results for their eyes only, or someone else might want to allow certain actions to happen just once per user, etc. At the moment, the minimal level of authentication requires that the user entrust the developer with access to private data. Many users will rightfully decline giving such access, and as a developer I’d rather not have to even request and disclaim, when I don’t need it. The more complex one requires a bit of background. Almost four years ago, there were various discussion threads on the topic of finding interesting content . Back then I took a stab at using network information to discover interesting photos – first looking for people who post similar stuff, then looking for people who share faved photos (which striatic called neighbors ) and finally, actual photo suggestions. As an aside, the algorithm worked quite well even for people with a hundred or so favorites (I had just 21 faves!); people today have many thousand favorites – a very rich data set to start from. I went on to produce suggestion lists for many people, but that was always me, manually running scripts. Since then, other developers created interesting tools for content-driven content discovery, including Flickr Cross-Recommendations , inSuggest and flexplore . The reason I never tried to create a stand-alone tool for this is that I quickly realized that to make it work reasonably fast, I essentially needed to replicate flickr’s database. If you take all my favorites, then list all the people that faved those photos, and finally enumerate all their favorites… and you then repeat this exercise for whoever requests it, you either need to use many thousand API calls per visitor (and wait!), or you need to create a huge cache, covering unpredictably disparate segments of flickr’s database. A massive cache that would grow stale very quickly unless people used the tool continuously. If you try to include extra information into the scoring scheme (say tags, color data, group membership…) the network use and storage requirements grow even worse. In other words, this project appears to be completely unsuitable for a developer without direct access to flickr’s database… and flickr’s content suggestion system (Explore) doesn’t provide any personalization tools: we all see the same. The only effective solution I can suggest (beyond flickr developing a personalized Explore, or flickr sandboxing third party developers) is the creation of high level API calls, embodying some method of complex querying into the database. For example: A higher level API call could accept a list of users as query, and return a list of photos sorted by the number of people (from the query list) who faved them. With the current API, one has to retrieve the full favorites list for each person, collate the results, and discard the vast majority of the information that was transferred. A higher level API method could make such intermediary results invisible and save much bandwidth, latency and data replication. Hey, you asked. ;) 4. What excites you about Flick and hacking? What do you think you’ll build next or would like someone else to build so you don’t have to? Gustavo: There’s a number of things that attract me to flickr hacking. There’s of course the vast and ever-growing amounts of data, and the fact that it’s not “more of the same”: The diversity of data types (photos, users, metadata, groups, etc) and relations make this complex system fascinating to study. There is also the human side: People really care about their stuff. People get excited, for example, when they recognize their user icon in a visualization of the social network, and immediately want to explore around. Last but not least, I really appreciate the openness of the flickr API. I’m amazed that such wealth of information is shared so freely! As for “what next”, I have a couple of ideas. I’ll only hint that the word association analysis was a fun first step. :) 5. Besides your own, what Flickr projects and hacks do you use on a regular basis? Who should we interview next? Gustavo: dopiaza as Utata ‘s architect. Dan: Thank you, Gustavo. Next up (unless Kellan gets in first!) dopiaza . Images from GustavoG , striatic and malanalars .", "date": "2008-11-19,"},
{"website": "Flickr", "title": "Boundaries, a tool to explore Flickr’s shapefiles", "author": ["Kellan Elliott-McCrea"], "link": "https://code.flickr.net/2008/11/20/boundaries-a-tool-to-explore-flickrs-shapefiles/", "abstract": "A few weeks ago we released our shapefiles via the API , and while most people were excited, some folks were a bit confused about what it all meant.  Which is why Tom Taylor’s beautiful Boundaries application is so exciting.  It helps you visualize the Flickr community’s twisty changing complex understanding of place. You throw in a place name and it’ll attempt to find that place’s neighbours, using Yahoo GeoPlanet. For each of those neighbours, it uses the WOE ID to fetch and render the polygon from shapefile that Flickr returns. – Tom Taylor: Flickr boundaries, now free to play with", "date": "2008-11-20,"},
{"website": "Flickr", "title": "Lightweight Layout Management in Flash", "author": ["Ashot Petrosian"], "link": "https://code.flickr.net/2008/11/21/lightweight-layout-management-in-flash/", "abstract": "Resizable apps are groovy.  Users want content to fill the screen, and as the variance in screen resolutions has grown this has become more and more important.  To make this happen you can either write manual positioning code to place and size items on window resize, or you can define a hierarchy, some properties on elements of the hierarchy and have a layout engine do the work for you. Up until a few years ago, there were no good pre-built solutions to do this in Flash so the only option was to roll your own framework. Nowadays there are several floating around, including this one from the flash platform team at Yahoo!, and of course there is the Flex/ MXML route. XML based UI markup seems to be all the rage these days, but what I found personally through my experience building the jumpcut tools is that this “responsible” approach has drawbacks. A hierarchy of HBoxes and VBoxes can be a lot of overhead for simple projects, and can become unwieldy and rigid when you are trying to develop something quickly. If you are doing anything weird in your UI (overlapping elements, changing things at runtime) it can also feel like trying to fit a square peg in a round hole. The Best of Both Worlds So is there any way to get at a sweet-spot, something quick and flexible, yet powerful? The first thing that probably comes to mind if you are a web developer is the CSS/HTML model. In fact, I think would be a rather good option if not for one missing piece, the unsung hero of the web: the HTML rendering engine. If anyone wants to implement Gecko or WebKit in ActionScript I think that would be a fun project, but because the web ecosystem evolved with the markup user and not the layout engine implementer in mind it’s rather complicated . There is also the fact that this model is based on the assumption of a page of fixed width but infinite height which doesn’t exactly suit our purposes. What we can borrow though is the philosophy of good defaults and implicit rather than explicit layout specification. When you are building out a layout with HTML/CSS you can just drop stuff on the page and things more or less work and resize as you’d expect, then selectively you can move things around / add styling and padding and so on. To do something similar we can take advantage of one of the nice new features of Actionscript 3, the display hierarchy, which allows us to easily traverse, observe, and manage all the DisplayObjects at run time, without keeping track of them on our own as they are created. This way we can recurse through the tree, look for the appropriate properties, and apply positioning and sizing as necessary. If the properties don’t exist we’ll just leave things alone. When you start your project you can just write code to position stuff manually, then as things get more complex and start to solidify you can begin using layout management where it makes sense. I’m a bit of a commitment-phobe myself, so this really appeals to me; what’s especially nice is that you easily plug this in to any existing codebase. Some Code So, to get this working all we really need is a singleton that listens to the stage resize event, traverses the tree and places/sizes objects. Mine looks something like this: /*..*/\nstage.addEventListener(Event.RESIZE, this.onResize);\n/*..*/\n \nfunction onResize(e:Event){\n\tthis.traverse(root);\n}\n \nfunction traverse(obj:DisplayObject){\n\n\tfor (var i=0; i  < obj.numChildren; i++){\n\n\t\tvar child = new LayoutWrapper(obj.getChildAt(i));\n\t\tif(child.no_layout)\n\t\t\tcontinue;\n\t   \n\t\t//layout code to determine how to place and size children\n\t\t//child.x = something\n\t\t//child.y = something\n\n\n\t\tchild.setSize(W, H); // pre-order\n\n\n\n\n\n\t\tthis.traverse(child);\n\n\t\tchild.refresh(); // post-order\n\t   \n\t}\n} Two things of note here. First is the LayoutWrapper which I use to set defaults for properties and functions that don’t necessarily exist on all objects, so sort of like slipping in a base class without having to alter any code. I use flash_proxy to make this easier, but you don’t have to do that. One thing you will want to do is store dummy values for width and height (something like _width and _height maybe as a throwback) so you can assign and keep track of the width and height of container elements without actually scaling the object.  You’ll also want to decide how you want to handle turning on / off the layout management and what defaults make sense for you.  In the above code I’m doing opt-out, using a no_layout variable to selectively turn off layout management where I don’t need it. Second, I have two hooks for sizing, setSize on the way down, and refresh on the way up. This is useful in some cases because what you want to do at higher levels may depend on the size of children at lower levels which can change on the way down. The layout code itself is outside the scope of this blog post, but I’ve found that a vertical stack container, a horizontal stack container, padding, and spacing is all I’ve needed to accomplish just about any UI.  Of course, you can and should implement any layout tools that are useful to you; different projects may call for different layout containers and properties. Some Fun One thing that we are doing by using this approach is pushing the specification of the display tree into run-time. The analogy with HTML here is the part of the DOM that is generated by visible source vs the part of the DOM that is generated by JavaScript. Going all cowboy like this does have its drawbacks.  Specifically it can get a bit confusing as to what the tree looks like, or why this or that is not showing up, where is this Sprite attached, etc. Web developers have the Firebug inspect tool for a very similar problem, which I’ve found rather handy. We can get a poor-man’s version of something like this by writing some code that pretty prints the tree under an object on rollover or use the Flex debugger, but, depending on the size of your project, this can get unwieldy rather quickly. As an excuse to play with the groovy new flare visualization library recently I tried pointing it at the display tree.  The result is rather useful for debugging, especially if you have a lot of state changes / masking / weirdness (plus you look like you are using a debugging tool from the friggin future). Some nice things I’ve added: on roll over the object gets highlighted, I can see an edit some basic properties and see them update live. There’s quite a bit of room here to do something Firebug like, with drag and drop reparenting and the like (but be careful, not too interesting or you’d wind up reimplementing the Flash IDE). If you’d like to try this yourself, this is the code to populate the flare data structure with the display tree: public function populateData(){\n\t// create data and set defaults\n\tthis.graph = new Tree();\n\tvar n = this.graph.addRoot();\n\tn.data.label = String(root);\n\tthis.populateDataHelper(n, root);\n}\n\n \npublic function populateDataHelper(n, s){\n\n\ttry{ // need this in case of TextFields\n\t\tvar num_children = s.numChildren;\n\t}\n\tcatch(e:Error){\n\t\treturn;\n\t}\n\t\n\t//limit in case you have extremely long lists \n\t//which can dominate the display\n\tnum_children = num_children < 20 ? num_children : 20; \n\n\tfor(var i=0; i  < num_children; i++){\n\t\tvar child = s.getChildAt(i);\n\n\t\tif(\n\t\tchild != this //don't show the layout visualizer itself!\n\t\t&& child.visible //don't show items that are not visible\n\t\t){\n\t\t\tvar c = this.graph.addChild(n);\n\t\t\tc.data.sprite = child;\n\t\t\tc.data.label = String(child);\n\t\t\tthis.populateDataHelper(c, child)\n\t\t}\n\t}\n  \n} Then just apply a visualization to this tree; I've been simply using the demo examples which has been sufficient for my uses, but obviously you can write your own as well. Here it is in action on the flickr slideshow:", "date": "2008-11-21,"},
{"website": "Flickr", "title": "[change log] Add as contact link", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2008/12/12/change-log-add-as-contact-link/", "abstract": "Change happens. On Flickr it happens quite a lot, although it tends to be just the large stuff that gets the coverage, see Improved Contacts Management over on the main Flickr blog for example. I’ve started [change log] posts as an attempt to highlight some of the smaller changes that go on. And… the new Add as a contact link seems like a good start. For quite a while now you could add someone from any page that featured their buddy icon. Hovering over it and clicking the down arrow opens up a whole selection of options, including adding them as a contact. It’s still a great way of getting to pages about that user or adding them as a contact from places such as group discussions. As of last week Eric dropped a slightly more obvious Add so-and-so as a contact link onto the photostream page. With the extra bonus of also saying what their current contact status is should you ever decided to change it. Sometimes it’s the little things that make a world of difference, so yay Eric!", "date": "2008-12-12,"},
{"website": "Flickr", "title": "Machine Tag Hierarchies", "author": ["Aaron Straup Cope"], "link": "https://code.flickr.net/2008/12/15/machine-tag-hierarchies/", "abstract": "something:somethingelse=somethingspecific With apologies to Jeremy Keith If you’re not already familiar with machine tags the easiest way to think of them is being like a plain old tag but with a special syntax that allows users to define additional structured data about that tag. If you’d like to know more, the best place to start is the official announcement we made about machine tags in the Flickr API group . If you want to know even more, still, take a look at: Machine Tag Wildcard URLs Machine Tags, last.fm and Rock’n’Roll Flickr, EXIF, Machine Tags , by Paul Mison Okay! Now that everyone is feeling warm and fuzzy about machine tags: We’ve added (4) new API methods for browsing the hierarchies of machine tags added to photos on the site. These are aggregate rollups of all the unique namespaces, predicates, values and pairs for public photos with machine tags. For example, lots of people have added exif: related machine tags to their photos but there hasn’t been a way to know what kind of EXIF data has been added: exif:model ? exif:focal_length ? exif:tunablaster ? Or what about all the planespotters who have been diligently adding machine tags to their photos using the aero namespace: What are the predicates that they’re tagging their photos with? Those are the sorts of things these methods are designed to help you find. Sort of like wildcard URLs but for metadata instead of photos. Uh, sort of. Anyway, the new methods are: flickr.machinetags.getNamespaces This returns a list of all the unique namespaces, optionally bracketed by a specific predicate. For example, these are all the namespaces that have an airport predicate: # ?method=flickr.machinetags.getNamespaces&predicate=airport\n\n<namespaces predicate=\"airport\" page=\"1\" total=\"2\" perpage=\"500\" pages=\"1\">\n\t<namespace usage=\"1931\" predicates=\"1\">aero</namespace>\n\t<namespace usage=\"3\" predicates=\"1\">geo</namespace>\n</namespaces> flickr.machinetags.getPredicates Like the getNamespaces method this returns a list of all the unique predicates, optionally bracketed by a specific namespace. For example, these are all the predicates that use the dopplr namespace: # ?method=flickr.machinetags.getPredicates&predicate=dopplr\n\n<predicates namespace=\"dopplr\" page=\"1\" total=\"4\" perpage=\"500\" pages=\"1\">\n\t<predicate usage=\"4392\" namespaces=\"1\">tagged</predicate>\n\t<predicate usage=\"1\" namespaces=\"1\">traveller</predicate>\n\t<predicate usage=\"7780\" namespaces=\"1\">trip</predicate>\n\t<predicate usage=\"4269\" namespaces=\"1\">woeid</predicate>\n</predicates> flickr.machinetags.getValues At this point, the pattern should be pretty straightforward. This method returns all the unique values for a specific namespace/predicate pair. For example, these are some of the values associated with the aero:tail machine tag (yes, really, airplane tail models!): # ?method=flickr.machinetags.getValues&namespace=aero&predicate=tail\n\t\t\n<values namespace=\"aero\" predicate=\"tail\" page=\"1\" total=\"1159\" \n\tperpage=\"500\" pages=\"3\">\n\t<value usage=\"1\">01-0041</value>\n\t<value usage=\"1\">164993</value>\n\t<value usage=\"2\">26000</value>\n\t<value usage=\"1\">4k-az01</value>\n\t<value usage=\"1\">4l-tgl</value>\n\t<value usage=\"1\">4r-ade</value>\n\t<!-- and so on... -->\n</values> flickr.machinetags.getPairs Finally, the getPairs method returns the list of unique namespace/predicate pairs optionally filtered by namespace or predicate. Rather than including yet-another giant blob of XML, here’s a pretty picture of the metro stations in Munich instead: A few things to note Certain namespace/predicate pairs have been special-cased to return a single value. As of this writing they are: geo:lat (and variations) geo:lon (and variations) file:name file:path anything :md5 If people have particular reasons for needing or wanting these we’re open to the idea but otherwise the cost of storing all the variations and the\tdubious uses for returning them in the first place made us decide to exclude them. Now what? photo by Daveybot Well, that’s what we’re hoping you’ll tell us. Machine tags have been chugging away quietly since we announced them almost two years ago and despite being a bit nerd-tastic and awkward to explain we’ve been thrilled to see how people have been finding their own use for them. It turns out we have photos for 31, 594 unique Last.FM events all because people added lastfm:event machine tags to their photos. Researchers at Lewis & Clark College, in Portland, have been “developing an educational collection of contemporary ceramics images using the photo sharing site Flickr” and machine tags. Shortly after the launch of Google’s Street View feature Mikel Maron “took GeoRSS feeds from Upcoming, grabbed lat/long and associated that with a panoid via the RESTful ‘api’ and pulled down the images … and then uploaded the panos to flickr with machine tags “. The Utata Collective uses a combination of groups on Flickr and machine tags to collect photos for their projects. And the list goes on. The trick with machine tags has always been to make them both invisible (or at least barely visible) to those people who don’t care about them but also as easy as tags to pick up and use for those people who do or who wonder whether they might be the tool they were looking for. One thing we didn’t do very well, though, until the release of the machine tag hierarchy APIs was give people a way to learn about machine tags. The only way to find out which machine tags people were using was to hop-scotch your way around people’s photostreams or to be part of a larger community having a discussion about which tags to use. Oops. Which is why it was extra-fantastic when a few short days after we announced the machine tag hierarchy methods on the API list, the ever prolific and awesome Paul Mison wrote back and said : The obvious thing to build on top of these … is\tsome sort of graphical machine tag browser, a bit like the Mac OS X /\tiPod column view browser. So I did. http://husk.org/code/machine-tag-browser.html This is entirely self-contained in one file (except for loading jQuery from Google and (cough) the pulser from Flickr). It uses JavaScript to get a full list of namespaces, giving you the option to drill down into predicates and the values available for that namespace/predicate\tpair. We’re hoping that this provides a little more raw material to play with and maybe find some magic and that you’ll tell us what comes next. Yay! Oh yeah, the actual API methods flickr.machinetags.getNamespaces flickr.machinetags.getPredicates flickr.machinetags.getValues flickr.machinetags.getPairs Enjoy! In the coming weeks we’ll also try to gather most of the blog posts and other writings about machine tags and put them with the rest of the API documentation .", "date": "2008-12-15,"},
{"website": "Flickr", "title": "Web Ops Visualizations Group on Flickr", "author": ["John Allspaw"], "link": "https://code.flickr.net/2008/12/17/web-ops-visualizations-group-on-flickr/", "abstract": "Like lots of operations people, we’re quite addicted to data pr0n here at Flickr. We’ve got graphs for pretty much everything, and add graphs all of the time. We’ve blogged about some of how and why we do it. One thing we’re in the habit of is screenshotting these graphs when things go wrong, right, or indifferent, and adding them to a group on Flickr. I’ve decided to make a public group for these sort of screenshots, for anyone to contribute to: http://flickr.com/groups/webopsviz/ You should realize before posting anything here, that you might want to think about if you want everyone in the world to see what you’ve got. I’ve made a quick FAQ on the groups page, but I’ll repeat it here: Q: What is this? A: This group is for sharing visualizations of web operations metrics. For the most part, this means graphs of systems and application metrics, from software like ganglia, cacti, hyperic, etc. Q:Who gets to see this? A: This is a semi-public group, so don’t post anything you don’t want others to see. For now, it’ll be for members-only to post and view.  Ideally, I think it’d be great to share some of these things publicly. Q: What’s interesting to post here? A: Spikes, dips, patterns. Things with colors. Shiny things. Donuts. Ponies. Q: My company will fire me if I show our metrics! A: Don’t be dense, and post your pageview, revenue, or other super-secret stuff that you think would be sensitive. Your mileage may vary. So: you’ve got something to brag about? How many requests per second can your awesome new solid-state-disk database do? You got spikes? Post them!", "date": "2008-12-17,"},
{"website": "Flickr", "title": "We ate 10 fuckin terabytes yesterday in uploads. Not bad.", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2008/12/30/we-ate-10-fuckin-terabytes-yesterday-in-uploads-not-bad/", "abstract": "allspaw sez “ We ate 10 fuckin terabytes yesterday in uploads. Not bad. “", "date": "2008-12-30,"},
{"website": "Flickr", "title": "Pancake Friday", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2009/01/02/pancake-friday/", "abstract": "This one goes out to all the other developers that drag themselves into the office on the second day of the year just before a weekend (and those online over the holidays, just incase). For the record we are going to Rock 2009 and talk about it here, but first time to grab breakfast. Photo from keithdavisyoung .", "date": "2009-01-2,"},
{"website": "Flickr", "title": "Front-end Performance: Doing More With Less", "author": ["Scott Schiller"], "link": "https://code.flickr.net/2009/01/07/front-end-performance-doing-more-with-less/", "abstract": "In a recent talk on ajax performance , Douglas Crockford explained the practice of “streamlining” code as including algorithm replacement, work avoidance and code removal. Applying this to page load time and performance, browser “work” can be avoided by reducing the number of HTTP requests made. In the most common case this can mean eliminating images entirely from a design, or reducing multiple image references to a single sprite image where possible. Shaving off image requests is a wonderful way to incrementally improve both perceived and measured performance. The use of sprites alone is not a new technique, but the approach taken in this case involves a low-cost, low-risk update to a common component based on legacy code, reduces the number of HTTP requests and simultaneously improves the UI – all good things! Legacy code: The good and the bad Flickr has UI components and widgets which have worked well for years, but can also benefit from newer development techniques such as sprite-based images. Rounded-corner dialogs including alpha-transparent drop-shadows are two prime examples, obvious subjects for optimization. There is some risk in modifying code which may be used in hundreds of places, so minimizing changes to the HTML structure is important. In this particular case, legacy table-based code was used to do layout for a rounded-corner dialog, and a separate table element was used to position a drop-shadow underneath it. Both elements used separate images for each edge of the box, resulting in eight HTTP requests for the dialog and another eight for the shadow. Dialog and drop shadow performance optimizations (border-radius + CSS sprites) save up to 15 HTTP requests, in this example (adding a note to a photo on Flickr.) Retrofitting legacy code in a low-risk way In modifying the drop-shadow code while maintaining backwards-compatibility, the src property of each corner image was set to a transparent 1×1 GIF image; width and height were already specified on these elements, so the layout is retained as if the original image were being used. Additionally, a CSS class applied the sprite as a background image and a second class provided the background-position offset. For the rounded corner dialogs, eight image requests were completely eliminated for browsers supporting CSS’ border-radius property. Additionally, the rounded corners are now anti-aliased in several browsers – a further improvement over the old GIF-based “jaggies.” While not as “clean” as a complete code rewrite, this incremental approach improved performance and did so with minimal chance of introducing bugs in a global component. Related resources Some helpful, free tools were used in creating both the sprite and CSS for these performance tweaks. Smushit.com Web-based image optimizer, shave (potentially many) bytes off GIF/PNG images CSS sprite generator Upload a .zip of images, set some parameters, and get a sprite with the CSS generated automagically! Quite handy.", "date": "2009-01-7,"},
{"website": "Flickr", "title": "Living In the Donut Hole", "author": ["Aaron Straup Cope"], "link": "https://code.flickr.net/2009/01/12/living-in-the-donut-hole/", "abstract": "video by origami madness A long long time ago ( 2005 ) in a galaxy far far way ( Vancouver ) when we joined Yahoo! and moved FlickrHQ to the Bay Area all but one or two members of the team lived within ten square blocks of each other in San Francisco’s Mission District . John Allspaw , a long-time resident of the Mission used to regale us with stories of one of the neighbourhood’s notable quirks commonly referred to as the “donut hole”: The rest of the city could be covered in fog, or raining, but the moment you crossed over in the Mission the sky would open up and the entire neighbourhood would be bathed in sunshine. When John and George Oates and I used to car pool between the city and the offices in Sunnyvale , we would drive up and down highway 280 and sure enough as you approached the city, at the end of the day, you would drive into an enormous blanket of fog the moment we passed the airport in Millbrae. And as soon as we’d pulled off the San Jose exit there would be an open stretch of clear sky all the way to Civic Center where it would stop again just as suddenly. Some mornings, when I look out my kitchen window at the clouds hanging over Diamond Heights I like to pretend I can see the curvature of the inside of the donut hole itself. I was reminded of all this the other morning when I was generating some visualizations based on the shapefiles that are derived from the almost 100 million geotagged photos on Flickr . The larger, blue, contour is the “shape” of the city of Paris (or WOE ID 615702 ) according to Flickr. The smaller white contours are the child neighbourhoods of that WOE ID with public, geotagged photos. So, what’s going on then? The first outline maps roughly to the extremities of the RER , the communter train that services Paris and the surrounding suburbs. This is a fairly accurate representation of the “greater metropolitain” area of Paris. Metropolitain areas, increasingly common in both popular folklore and government administrivia as more and more people shift from rural to urban living , are noticeably lacking from the Flickr hierarchy of place types and a subject probably best left for another blog post. The rest, taken as a whole, follow closer to the shape of the old city gates that most people think of when asked to imagine Paris. Which one is right? Well, both obviously! Cities long ago stopped being defined by the walls that surround(ed) them. There is probably no better place in the world to see this than Barcelona which first burst out of its Old City with the construction of the Eixample at the end of the 19th century and then again, after the wars, pushed further out towards the hills and rivers that surround it. There are lots of reasons to criticize urban sprawl as a phenomenon but sprawl, too, is still made of people who over time inherit, share and shape the history and geography they live in. Whether it’s Paris, Los Angeles, William Gibson’s dystopic “ Boston-Atlanta Metropolitain Axis ” (BAMA) or the San Francisco “Bay Area” they all encompass wildly different communities who, in spite of the grievances harboured towards one another, often feel as much of a connection to the larger whole as they do to whatever neighbourhood, suburb or village they spend their days and nights in. That’s one reason I think it’s so interesting to look at the shape of cities and see how they spill out beyond the boundaries of traditional maps and travel guides. In the example above the shape for Paris completely engulfs the commune of Orly , 20 kilometers to the South of central Paris, which makes a certain amount of sense . It also contains Orly airport which isn’t that notable except that we treat airports as though they were cities in their own right because the realities of contemporary travel mean that airports have evolved from being simple gateways to captial-P places with their own culture, norms and gravity. So, now you have cities contained within cities which most people would tell you are just neighbourhoods. We’re recently finished rendering the second batch of shapefiles and looking ahead I am wondering whether we should also be rendering shapes based on the relationship of one place to another. Rendering the shape of the child places for a city or a country (you can do this using the handy, if awkwardly named, flickr.places.getChildrenWithPhotosPublic API method) would allow you to see a city’s “center” but also provide a way to filter out parts of a shape with low Earthiness (aka water). The issue is not to prevent, or correct, shapes that provide a “false” view because I don’t think they do. As Schuyler observed, while we were getting all this stuff to work in the first place, and testing the neighbourhoods that meet San Francisco Bay they are really the shapes of people looking at the city. They are each different, but the same. But maybe we should also map the neighbourhoods that aren’t considered the immediate children of a city but which overlap its boundaries. What if you could call an API method to return the list or the shape of a place’s “cousins”? What could that tell us about a place? What does all of this have to do with donuts ? Nothing really, but it’s a nice way to think about the problem and since we have a long and storied tradition of silly names for projects I imagine this one will stick too. There are no fixed dates yet for when, or whether, any of this will make its way in to the API but quite a lot of it could be done with API methods already available today. One change we have made is to add a new flickr.places.getShapeHistory API method which include pointers to all the shapefiles that have been rendered for a place. I have dim and distant memories of possible reasons why not to do this, in the past, but the exercise in making donut shapes makes me think I was wrong. The more data and “nubby bits” that people have to work with the more interesting it will be for everyone. Enjoy!", "date": "2009-01-12,"},
{"website": "Flickr", "title": "5 Questions for Jonnie Hallman", "author": ["Kellan Elliott-McCrea"], "link": "https://code.flickr.net/2009/01/15/5-questions-for-jonnie-hallman/", "abstract": "If you reach way back into the dusty, distant corners of your mind, you might remember 2008.  Its fading but there.  And back in 2008, Jonnie Hallman was tapped as our next 5 questions interviewee. And what better way to kick off 2009, then with these great answers. Jonnie Hallman has built a cult following around his award winning desktop app, DestroyFlickr : “alternative view of Flickr”.  He also recently turned his creative/destructive eye towards Twitter , with DestroyTwitter .  Download them, taken for a spin, come back with your understanding altered. On a sad note, Jonnie tapped Jonathan Harris as the next link in the interview chain, but I was totally unable to get in touch with him, so we’ll be kicking off a new chain soon. But onward! 1. What are you currently building that integrates with Flickr, or a past favorite that you think is cool, neat, popular and worth telling folks about? Or both. Jonnie: I’ve spent the past year or so working on an Adobe AIR app called DestroyFlickr . It takes an alternative approach to viewing Flickr content, focusing on the UI and its relationship to the photos. It actually began as a first-time experiment with both AIR and APIs in general. Prior to DestroyFlickr , I only had experience with programming websites, but quickly realized how much I prefer developing software—I like the idea of consistently improving a single project by adding new features and fixing bugs. The app uses a system of “workspaces” and “canvases” to organize the environment. Each user/contact has his/her own workspace with four canvases—“profile”, “photos”, “photo” and “contacts”. A user can switch between canvases easily and return to previous ones without reloading them. In the beginning, I focused on features that felt obvious to me: a dark environment that’s easy on the eyes and beneficial to the photographs, the ability to flip through pages of photos in a matter of seconds, and the option to see a photo at a larger resolution without reloading it altogether. About six months into development, when DestroyFlickr started to get serious, I began implementing features that would really improve the average user’s workflow: the ability to present fullscreen, drag and drop downloading and uploading, and one-click download-all for easy backup. I originally abandoned DestroyFlickr after a few months because the only people using it were a handful of my friends who I actually had to persuade to use it. Learning of the Adobe Design Achievement Awards competition really saved it. I can’t thank them enough for all the attention DestroyFlickr has gotten since. It went from 28 users to over 6,000 in a little over a month. Now, it’s at about 7,500 and still going strong. 2. What are the best tricks or tips you’ve learned working with the Flickr API? Jonnie: I think the most important thing to do before starting any Flickr project is to write a very simple and robust wrapper for calling methods. I use a “Method” class that has a single static function, “call”, which handles everything and returns a loader. By setting up an adaptive system, I was able to avoid hundreds of lines of redundant code. Regarding specific methods, the flickr.photos.search is certainly the APIs best kept secret. I’ve found it to be extremely useful on a number of levels and really wish the API had more methods like it. 3. As a Flickr developer what would you like to see Flickr do more of and why? Jonnie: I’d like to see more parameters and methods for groups and contacts. I know contacts is a touchy subject since you run the risk of apps that add contacts by the thousands, but I think Flickr’s been on the ball with filtering out malicious keys. The majority of requests I get from users ask for features that can’t be written because the API is a bit limited in those parts. Overall, I find the API to be extremely generous, but there are a few essential areas that aren’t as open as others. Specifically, a parameter that’s missing is the number of times a photo has been called someone’s favorite. I’d also really like to be able to retrieve my API key stats. 4. What excites you about Flickr and hacking? What do you think you’ll build next or would like someone else to build so you don’t have to? Jonnie: The endless possibilities excite me. Flickr provides such a cohesive and substantial API with both a testing ground and stat tracker. Regarding what’s next, I’m going to continue adding onto DestroyFlickr between work and school, so anything new will be piled on top. I have plans of adding a workspace for groups and possibly a canvas for sets, but I need to approach both with caution and make sure I have a solid plan before I start any writing. I thought about making a separate uploader or backup app, but I think I’d rather make DestroyFlickr an all-in-one. 5. Besides your own, what Flickr projects and hacks do you use on a regular basis? Who should we interview next? Jonnie: Prior to DestroyFlickr, I tried out just about every Flickr app listed on the site just to see what’s possible. What caught my eye, though dormant for about two years now, is Jonathan Harris’ Time Capsule . I’d love to see what he has to say about the API as it is now and how he’d improve upon it. Bonus question: what’s up with the name “DestroyFlickr”? Jonnie: DestroyFlickr’s name is based off my mantra, “Destroy Today,” meaning to make the most of each day by destroying it. I see destruction as a form of creation. DestroyFlickr is not out to literally destroy Flickr, but rather develop upon it and take advantage of what Flickr provides.", "date": "2009-01-15,"},
{"website": "Flickr", "title": "5 Questions for Fraser Speirs", "author": ["Kellan Elliott-McCrea"], "link": "https://code.flickr.net/2009/01/22/5-questions-for-fraser-speirs/", "abstract": "If you’ve been around Flickr for a bit then you’ve seen probably seen Fraser Speirs photos.  If you’re a Mac user then you’ve almost certainly encountered his code.  He built the Flickr Uploadr 2.0 for the Mac, and the hugely popular FlickrExport for direct uploading from iPhoto and Aperture.  Lately his DarkSlide has brought Flickr browsing, discovery, and sharing to the iPhone. 1. What are you currently building that integrates with Flickr, or a past favorite that you think is cool, neat, popular and worth telling folks about? Or both. Fraser: Most of the last year has been all about putting Flickr on the iPhone.  I started back in March, when the first iPhone SDK was released, and cranked the code handle probably harder than I ever have before.  Hours turned into days, days to weeks and weeks to months but Exposure was ready for day one of the App Store.  It’s been rolling along steadily ever since, particularly with the recent 1.5 update and its cool new name – Darkslide . The thing that I’ve most enjoyed seeing people get immense pleasure from in Darkslide is the Near Me feature.  Near Me takes your iPhone’s current location and searches Flickr for photos near that point. People have found everything from great pubs nearby to, um, slightly compromising pictures of their neighbours. 2. What are the best tricks or tips you’ve learned working with the Flickr API? Fraser: For anyone developing desktop or mobile applications against the Flickr API, you must, must, must do all your network operations asynchronously.  Anything else produces a quite terrible user experience.  Hint: UI animation can give you a few fractions of a second to make it feel faster to the user, and perceived speed is often more important than actual speed. Secondly, if you’re working on a mobile device, download thumbnails in parallel (though not so much as to make the servers cry).  Early versions of Darkslide downloaded 75px thumbnails one at a time, and the network latency turned out to be about as long as the time taken to download a thumbnail. The experience was painful.  Newer releases request about ten at a time, and the user experience is much improved.  Beware latency! 3. As a Flickr developer what would you like to see Flickr do more of and why? Fraser: I’d like to see some queries turned around a little faster.  When you’re working on mobile devices, the user’s attention span is oftentimes quite short.  Between a couple of seconds in network latency and a couple of seconds turnaround time on the query, then the download time for a chunk of XML, it can sometimes get a little sluggish. API-wise, I really would like an API for group discussion threads.  A good number of users ask me about Darkslide support for group discussions on a regular basis. Finally, a plea to keep the mathematics of maps simple for mathematical dullards like myself.  I know programmers are all supposed to be maths wizards, but I truly stink at maths.  I love that flickr.photos.search can do a radial search around a point, rather than requiring me to compute a bounding box.  It would be great if all maps-related API let us do this kind of thing.  I’m still trying to get my head around the location corrections API :-) 4. What excites you about Flick and hacking? What do you think you’ll build next or would like someone else to build so you don’t have to? Fraser: What excites me?  Well, I love photography so I love making things that make it easier for me to look at great pictures.  I love that Flickr is becoming the web’s de facto repository for photography, and I love helping to make that happen. I’m also excited to see that the Twitter/Flickr axis is starting to turn into a very powerful tool in the hands of ordinary people.  I’m writing this a couple of hours after US Airways flight 1549 went down in the Hudson River.  Where did I get my first news and photos of this event? Not the BBC – who had nothing up at the time – but news from Twitter and photos from Flickr. One of the first great photos of US1549 was taken by Twitter user jkrums and posted to TwitPic. It was later reposted to Flickr after TwitPic went down under the load.  Maybe next time that will be an iPhone user using Darkslide and putting the photo on Flickr first. This is power in the hands of ordinary people and, as a citizen of the world’s biggest surveillance society (one CCTV camera for every 14 people in the UK), I think it will be increasingly essential that citizens have this kind of global reach. 5. Besides your own, what Flickr projects and hacks do you use on a regular basis? Who should we interview next? Fraser: I’m a big Twitter user and one simple-but-not-that-obvious thing I’ve done is to pipe my Flickr activity RSS feed into Twitter using TwitterFeed.com.  I also write my blog with Daniel Jalkut’s MarsEdit blog editor , which has Flickr integration in its “Media” panel – this lets you drag and drop pictures from Flickr as HTML tags right into a blog post. Great timesaver.  I also use Scout to see the occasional time that one of my photos makes it to Explore :-) Whom should you interview next? I’d like to hear from Matt Biddulph at Dopplr .  I’m very interested in where Flickr photos end up, and Dopplr are making interesting use of Flickr photos in the things they do, such as producing personal travel reports with CC-licensed Flickr photos .", "date": "2009-01-22,"},
{"website": "Flickr", "title": "100,000,000 geotagged photos (plus)", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2009/02/04/100000000-geotagged-photos-plus/", "abstract": "Over the weekend we broke the Hundred Million geotagged photos, actually 100,868,302 at last count, mark. If we remember that we passed the 3 billion photos recently and round the figure down a little that means ( does calculations on fingers ) that around 3.333% of photos have geo data, or one in every 30 photos that get uploaded. In the last two and a half years there have been roughly as many geotagged photos as the total photos upload to Flickr in its first two years of existence. Of those, around two thirds have public geotags and can be searched for on the map or via the API, and about 33 million have some level of private geotags. I should probably mention at this point that if you go directly to the map and click the dots icon in the top right that you’ll see a smaller number. This is because we added a rolling upload-date to the initial search to return the most interesting photos in the last month or so, rather than always have the same (all-time) photos show up forever, possibly reinforcing their interestingness. Anyway, not bad really.", "date": "2009-02-4,"},
{"website": "Flickr", "title": "Things I’m Standing Next To", "author": ["Aaron Straup Cope"], "link": "https://code.flickr.net/2009/02/09/things-im-standing-next-to/", "abstract": "“The problem with these geolocative services is that they assume you’re a precise, rational human, behaving as economists expect. No latitude for the unexpected; they’re determined to replace every unnecessary human interaction with the helpful guide in your pocket.” — Tom Taylor Back in June of 2008 we added the ability to perform radial queries in the photos.search API method . One of the earliest developers to use this feature was Frasier Speirs who used it for the Near Me feature of Darkslide , his Flickr application for the iPhone originally named Exposure . It is difficult to overstate how impressive, and important, tools like Google Earth and PhotoSynth are visualizing geographies and in pushing the boundaries of what is possible both technically and conceptually. But both do so at the expense of what Scott McCloud calls “ the magic in the gutter “. On the subject of the gutter, the space between the individual panels in a comic book, McCloud writes : “From an axe-murderer pursuing a frightened man in one panel to an ambiguous shriek in the next, what happened? You killed a character in your mind. The artist did nothing of the sort. Closure is the work done by a reader which takes two juxtaposed images and unifies them into a single idea.” That’s one of the things I like the most about “Near Me” and the ability to perform radial queries using the Flickr API: It affords a representation of place spun from a thicker, coarser, yarn. Less precise, maybe, but richer in its own way. With radial queries there is the knowledge that all the photos were taken close to one another — so close in some cases you can almost walk down a city block from one photo to the next — but the results are staggered , often overlapping one another, indoors and outdoors in time and space. That staggering is the breathing room , the gutter, that lets viewers discover, imagine and create their own connections, their own closures, from not just a one history of a place but also the patterning of all those who’ve passed through it. “Whether mapping lost lakes of a different era or tracing the edges of disappeared lagoons that still haunt the streets of San Francisco — or reminding urbanites of the sport-fishing possibilities beneath Manhattan — we are alive within laminations we will never fully map or comprehend.” — Geoff Manaugh Earlier last week we enabled a quiet little feature that, hopefully, allows you to navigate some of that same mystery and serendipity in the 100 million geotagged photos on Flickr. We call it “nearby” and it is available for any geotagged photo on the site. Nearby starts with a geotagged photo and then queries for other geotagged photos within a one kilometer radius. You can order the results by time and distance and interestingness but the important part is that they are photos, well, nearby to the photo you are looking at . Nearby is a deliberately fuzzy concept. Nearby in St. Peter’s Square in Rome might mean the person directly in front of you. Nearby in the streets of a small town might be the beautiful garden behind the fence and around the corner. Nearby encourages people to poke around and discover their surroundings, as though they were on foot and everything was just a short walk away. For example, I went to London recently and I while I was there the lovely crew at Dopplr took me around the corner from their offices to the Whitecross Street Market for lunch. We oggled the cheese and I had a chorizo sandwich and we all took lots of pictures. This is a picture of my sandwich and these are the photos taken nearby, including one of me taking a picture of my sandwich: I have no idea what’s up with the creepy smile. I blame… the jet-lag. If you look carefully at the screenshot above, you can see that I’ve defined “nearby” to mean only photos taken by my contacts on the same day and sorted by their distance from my delicious sandwich. This is a very specific and personal view and its value for me is as a record of a shared experience with friends. But if I strip away all the conditions and visit the default “nearby” page today I can see the snow that’s recently fallen on London and the graffiti that I walked passed, but didn’t photograph, in Shoreditch and Cal hanging out in the Moo/Dopplr offices . I can watch the place around my photo grow up while also nosing around in the shoebox of its past and, at the same time, someone else can see that I ate a sandwich on Whitecross street . The default search criteria for nearby is all photos taken in the last month sorted by date taken (most recent, first) but there are lots of different ways to define what nearby looks like using the available filters: Who took a photo? (For the sake of brevity, “you” can also be taken to mean another photographer.) Everyone Everyone by you Only you Only your contacts When was it taken? In the last month Today, you know today The day that the photo was taken All time How should those photos be sorted? Date taken By distance from the center point By interestingness More sophisticated filters, like explicit date ranges, aren’t yet avaliable but we’re definitely thinking about them and we’d love to hear how people would like to use nearby. The links for nearby pages are currently only available from the modal “map” dialog on individual photo pages or by URL hacking which is really just fancy talk for adding /nearby to the URL of any (geotagged) photo page. Here’s a picture of another sandwich, this time a Philly Cheesesteak , taken in San Francisco’s Mission district: http://www.flickr.com/photos/straup/3199232007/ And here are nearby photos: http://www.flickr.com/photos/straup/3199232007 /nearby Or a photo of the Sydney Cricket Ground , from the Powerhouse Museum’s Collection, taken in 1900: http://www.flickr.com/photos/powerhouse_museum/3022876525/ And photos taken nearby, on the very same grounds, 100 years later: http://www.flickr.com/photos/powerhouse_museum/3022876525 /nearby?taken=alltime&sort=distance photos by Tyrrell Photographic Collection, Powerhouse Museum and brettm8 Just like that! Well okay, we added a couple query parameters (to that last link) which most people aren’t ever going to do but you get the idea. We’ve also added a special page, which you can link to with any old latitude and longitude and and we’ll show you photos near that point. This isn’t a page that we expect people to visit directly (typing all those numbers in the location bar is pretty boring, really) but rather we hope that it will be used by third-party applications and devices which are location aware and can fire up a web browser. For example, if I were wandering around the Metropolitan Museum of Art , in New York City: http://www.flickr.com/nearby/ 40.779274,-73.963265 In the not so distant future, when web browsers are able to read your location using a GPS signal or wifi triangulation then [REDACTED BY KITTENS] but for now this is just a little something to bridge the difference and a hook for people to use in their applications. In the Easter Egg department we’ve also added support for geohashes to the special “lat, lon” pages. The Wikipedia entry for geohashes describes them as “a hierarchical spatial data structure which subdivides space into buckets of grid shape … offering properties like arbitrary precision and the possibility of gradually removing characters from the end of the code to reduce its size (and gradually lose precision).” They’re also just short(er). When hashed, 40.779274,-73.963265 becomes dr5ruzts7p8k which is all kinds of weird but that is true of all URL shortening services. Anyway, if the length of your URLs is an issue and the extra 8 characters are really going to make a difference you can sacrifice some of the built-in semantics in the longer version and use the following to link to the same photos from the Met: http://www.flickr.com/nearby/ dr5ruzts7p8k Finally, during the time its taken me to write this blog post I came across the SnarkMarket blog and a piece quoting Ed Folsom’s account of Walt Whitman’s experience of “urban affection” which I think is a nice ribbon to wrap it all up with: Whitman feels the power of the city of strangers. He’s looking at a city of strangers and how something we might now call urban affection begins to develop. How do you come to care for people that you have never seen before and that you may never see again? Every day we encounter people, eyes make contact, we brush by people, physically come into contact with them, and may never see them again. … “If I were doing that activity that person would be me. If I were wandering the other way, rather than this way, that person could be me.” photos by Paul Hammond", "date": "2009-02-9,"},
{"website": "Flickr", "title": "Birthday Kitten Tuesday", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2009/02/10/birthday-kitten-tuesday/", "abstract": "It’s been a while since we last had a Kitten Tuesday so how better to bring it back for a week than with a Birthday?! Five years ago today Flickr was launched, powered by hardworking nerds … … five years later it’s now powered by hardworking nerds, Magic Donkeys, Pandas, APIs, Users and Evil Cats. Yay us! Photo by anselor", "date": "2009-02-10,"},
{"website": "Flickr", "title": "[changelog] Yahoo! updated map tiles and some OSM ones.", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2009/02/13/changelog-yahoo-updated-map-tiles-and-some-osm-ones/", "abstract": "Recently Yahoo!! released new map tiles … ah heck, actually it was a couple of months ago, I just suck at writing blog posts … anyway , here’s their post from last December: Yahoo! Maps – New International Coverage which goes on to say … “We’ve added detailed coverage to 45 new countries, with new data in a further 30 countries, to make it easier for you to navigate to exotic locales as you plan for your winter travel.” I’m as keen as the next person to have 45 new countries, so I bumped our version of the maps API used over here at Flickr to make use of the new tiles. Which gives us more to work with for many bits of Albania, Andorra, Argentina, Australia, Austria, Bahrain, Belarus, Belgium, Bosnia & Herzegovina, Botswana, Brazil, Bulgaria, Canada, Croatia, Cyprus, Czech Republic, Denmark, Estonia, Finland, France, Germany, Gibraltar, Greece, Hong Kong, Hungary, Iceland, India, Indonesia, Ireland, Italy, Kuwait, Latvia, Lesotho, Liechtenstein, Lithuania, Luxembourg, Macau-China, Macedonia, Malaysia, Malta, Mexico, Moldova, Monaco, Montenegro, Namibia, Netherlands, New Zealand, Norway, Oman, Philippines, Poland, Portugal, Qatar, Romania, Russia, San Marino, Saudi Arabia, Serbia, Singapore, Slovakia, Slovenia, South Africa, South Korea, Spain, Swaziland, Sweden, Switzerland, Taiwan, Thailand, Turkey, US, United Arab Emirates, United Kingdom and [catch breath], Vietnam. Often we’ll try to improve our coverage with the help of (the frankly awesome) OpenStreetMap (OSM) peeps, as previously mentioned in; Around the World and Back Again, Flickr [heart] Burning Man [heart] OpenStreetMap and More new map tiles . With this roll out we’ve actually removed some OSM tiles. Mexico City, Rio de Janeiro, Sao Paulo are now covered in more detail by Yahoo! … here’s the tiles for Mexico City … … which as you can see are pretty good. But from OpenStreetMap we’ve added Accra , Algiers , Cairo , Harare , Kinshasa , Mogadishu and Nairobi , you can see the before and after for Nairobi below … Before After, with OSM Tiles … which as you can see are also pretty good :) Clearly we can’t spend forever swapping tiles in and out, well maybe we can, but at some point there’ll probably be a better way of managing this kind of thing. In the meantime though, this being the code blog, and just for fun, here’s a little bit of what happens in the world of JavaScript. Warning, some code ahead! This check_map function is called each time you move, zoom or switch map views … check_map: function(map_obj, parent_node) {\n\t\n\t//\tGrab the focus, zoomlevel and maptype of the current view\n\tvar lat_lon\t=\tmap_obj.getCenterLatLon();\n\tvar zl \t\t=\tmap_obj.getZoomLevel();\n\tvar map_type \t=\tmap_obj.getCurrentMapType(); Then we check the location against a list of places we want to load OSM tiles for, we could do this any number of ways, but pragmatically this just works (scroll the below code all the way to the right for the interesting bits, if you’re not reading this in an RSS feed) … //\tNow see if we match any of these following tests to work out if we need to switch to osm or not.\n\t//\tThe meta-test is if we are in MAP mode.\n\tvar in_the_zone = false;\n\tif (map_type == 'YAHOO_MAP') {\n\t\tif (zl < 8 && lat_lon.Lat >= 39.8558502197 && lat_lon.Lat <= 40.0156097412 && lat_lon.Lon >= 116.2662734985 && lat_lon.Lon <= 116.4829177856) in_the_zone = true;\t\t// Beijing\n\t\tif (zl < 7 && lat_lon.Lat >= 40.735551 && lat_lon.Lat <= 40.807533 && lat_lon.Lon >= -119.272041 && lat_lon.Lon <= -119.163379) in_the_zone = true;\t\t\t\t// Black Rock City, 2008\n\t\tif (zl < 9 && lat_lon.Lat >= 35.46290704974905 && lat_lon.Lat <= 36.02799998329552 && lat_lon.Lon >= 139.21875 && lat_lon.Lon <= 140.27069091796875) in_the_zone = true;\t// Tokyo\n\t\tif (zl < 8 && lat_lon.Lat >= -34.6944313049 && lat_lon.Lat <= -34.4146499634 && lat_lon.Lon >= -58.6389389038 && lat_lon.Lon <= -58.2992210388) in_the_zone = true; \t\t// Buenos Aires\n\t// Yahoo Better\tif (zl < 8 && lat_lon.Lat >= 18.9466495514 && lat_lon.Lat <= 19.6985702515 && lat_lon.Lon >= -99.5071487427 && lat_lon.Lon <= -98.7429122925) in_the_zone = true; \t// Mexico City\n\t// Yahoo Better\tif (zl < 8 && lat_lon.Lat >= -23.0837802887 && lat_lon.Lat <= -22.7662200928 && lat_lon.Lon >= -43.7946395874 && lat_lon.Lon <= -43.1328392029) in_the_zone = true; \t// Rio de Janeiro\n\t// Yahoo Better\tif (zl < 8 && lat_lon.Lat >= -24.0083808899 && lat_lon.Lat <= -23.3576107025 && lat_lon.Lon >= -46.8253898621 && lat_lon.Lon <= -46.3648300171) in_the_zone = true; \t// Sao Paulo\n\t\n\t\tif (zl < 8 && lat_lon.Lat >= -35.2210502625 && lat_lon.Lat <= -34.6507987976 && lat_lon.Lon >= 138.4653778076 && lat_lon.Lon <= 138.7634735107) in_the_zone = true; \t\t// Adelaide\n\t\tif (zl < 8 && lat_lon.Lat >= -34.1896095276 && lat_lon.Lat <= -33.5781402588 && lat_lon.Lon >= 150.5171661377 && lat_lon.Lon <= 151.3425750732) in_the_zone = true; \t\t// Sydney\n\t\tif (zl < 8 && lat_lon.Lat >= -27.8130893707 && lat_lon.Lat <= -27.0251598358 && lat_lon.Lon >= 152.6393127441 && lat_lon.Lon <= 153.3230438232) in_the_zone = true; \t\t// Brisbane\n\t\tif (zl < 8 && lat_lon.Lat >= -35.4803314209 && lat_lon.Lat <= -35.1245193481 && lat_lon.Lon >= 148.9959259033 && lat_lon.Lon <= 149.2332458496) in_the_zone = true; \t\t// Canberra\n\t\tif (zl < 8 && lat_lon.Lat >= -38.4112510681 && lat_lon.Lat <= -37.5401115417 && lat_lon.Lon >= 144.5532073975 && lat_lon.Lon <= 145.5077362061) in_the_zone = true; \t\t// Melbourne\n\t\n\t\tif (zl < 8 && lat_lon.Lat >= 33.2156982422 && lat_lon.Lat <= 33.4300994873 && lat_lon.Lon >= 44.2592010498 && lat_lon.Lon <= 44.5364112854) in_the_zone = true; \t\t// baghdad\n\t\tif (zl < 8 && lat_lon.Lat >= 34.4611015320 && lat_lon.Lat <= 34.5925598145 && lat_lon.Lon >= 69.0997009277 && lat_lon.Lon <= 69.2699813843) in_the_zone = true; \t\t// kabul\n\t\n\t\tif (zl < 10 && lat_lon.Lat >= -4.3634901047 && lat_lon.Lat <= -4.3009300232 && lat_lon.Lon >= 15.2374696732 && lat_lon.Lon <= 15.3460502625) in_the_zone = true; \t\t// kinshasa\n\t\tif (zl < 10 && lat_lon.Lat >= 2.0093801022 && lat_lon.Lat <= 2.0614199638 && lat_lon.Lon >= 45.3139114380 && lat_lon.Lon <= 45.3669013977) in_the_zone = true; \t\t\t// mogadishu\n\t\tif (zl < 10 && lat_lon.Lat >= -17.8511505127 && lat_lon.Lat <= -17.7955493927 && lat_lon.Lon >= 31.0210304260 && lat_lon.Lon <= 31.0794296265) in_the_zone = true; \t\t// harare\n\t\tif (zl < 10 && lat_lon.Lat >= -1.3165600300 && lat_lon.Lat <= -1.2379800081 && lat_lon.Lon >= 36.7483406067 && lat_lon.Lon <= 36.8735618591) in_the_zone = true; \t\t// nairobi\n\t\tif (zl < 10 && lat_lon.Lat >= 5.5237197876 && lat_lon.Lat <= 5.5998301506 && lat_lon.Lon >= -0.2535800040 && lat_lon.Lon <= -0.1586299986) in_the_zone = true; \t\t\t// accra\n\t\tif (zl < 10 && lat_lon.Lat >= 30.0068798065 && lat_lon.Lat <= 30.1119003296 && lat_lon.Lon >= 31.2149791718 && lat_lon.Lon <= 31.3111705780) in_the_zone = true; \t\t// cairo\n\t\tif (zl < 10 && lat_lon.Lat >= 36.6997604370 && lat_lon.Lat <= 36.8181610107 && lat_lon.Lon >= 2.9909501076 && lat_lon.Lon <= 3.1476099491) in_the_zone = true; \t\t\t// algiers\n\t\n\t} If we found a match up there, then \"in_the_zone\" would have been set, in which case we'll check to see if were already showing osm tiles, if not we'll tell the YAHOO API to use our OSM tiles rather than the YAHOO ones ... //\tok, if we've match one of the above tests then we are in osm land ...\n\tif (in_the_zone) {\n\t\n\t\t//\tif we aren't already osming, then we need to switch the map tiles.\n\t\tif (!this.osming) {\n\t\t\t\n\t\t\t//\tSay that we are now osming\n\t\t\tthis.osming = true;\n\t\t\t//\tPut the new tiles in\n\t\t\tYMapConfig.tileReg=['/our_openstreetmap_tile_broker.xxx?t=m&','/our_openstreetmap_tile_broker.xxx?t=m&'];\n\t\n\t\t\t//\tTell the maps to clear out the tile cache and load in the new tiles.\n\t\t\tmap_obj._cleanTileCache();\n\t\t\tmap_obj._callTiles();\n\t\t}\n\t} else { \"YMapConfig.tileReg=\" tells the map API to load tiles from us rather than the default ones, hint, you should set this to your own tile server ;) \"_cleanTileCache()\" and \"_callTiles()\" are two undocumented functions (and therefor may break at some point) that allows you to force the map to load in new tiles. Otherwise the current view of the map will still show the Yahoo tiles and not use the new ones until you next pan the map around. Again there's probably a better way of doing this, but there's still a lot to be said for it-just-works! The next bit runs if we're not \"in the zone\" to see if we need to turn the OSM tiles back off. //\totherwise we are not looking at an osm spot, in which case check to see if\n\t//\twe *were* looking at one, and if so, turn it all off again\n\t\tif (this.osming == true) {\n\t\t\t//\tsay that we are no longer osming\n\t\t\tthis.osming = false;\n\t\t\t//\tturn the tiles back\n\t\t\tYMapConfig.tileReg=this.oldTileReg;\n\t\t\t\n\t\t\t//\tTell the maps to clear out the tile cache and load in the new tiles.\n\t\t\tmap_obj._cleanTileCache();\n\t\t\tmap_obj._callTiles();\n\t\t}\n\t} Which is roughly the opposite of turning them on. The only thing to note is that is the line ... YMapConfig.tileReg=this.oldTileReg ... when we created the map I used this ... //\trecord what the old tiles were\nthis.oldTileReg=YMapConfig.tileReg; ... to record where YMapConfig thinks it aught to be loading tiles from when in map view (as opposed to hybrid or satellite view) so it can be reset when you need it. And that's pretty much it. I've chopped out some distracting bits here or there to make it clearer and no-one else is likely to do it like this, but I find sharing to be cathartic (adjective not noun) :) For a smidge of further reading this is a handy article from A List Apart: Take Control of Your Maps by Paul Smith.", "date": "2009-02-13,"},
{"website": "Flickr", "title": "5 Questions for David Wilkinson", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2009/02/13/5-questions-for-david-wilkinson/", "abstract": "On the last 5 Questions I posted, I ended with “Thank you, Gustavo. Next up (unless Kellan gets in first!) dopiaza.” Well Kellan did indeed get in first, with both Jonnie Hallman and Fraser Speirs . So now, without any more procrastination, I present David Wilkinson, “ Utata’s architect” as described by Gustavo … 1. What are you currently building that integrates with Flickr, or a past favorite that you think is cool, neat, popular and worth telling folks about? Or both. David: I guess one of the most visible things that I’ve built so far is the Utata web site ( http://www.utata.org/ ) and the tools that surround that. Utata, the Flickr group , is a pretty lively community, with over 16,000 members and a pool of over 270,000 photos. In Utata, we like to encourage people to participate, and our web site expands on the group pool and discussion threads on Flickr with a variety of projects for members to participate in, blogs that feature photos picked from the pool, and a wide variety of articles, notices and links. The key thing we’ve tried to do in Utata is to ensure that participation is simple and straightforward. Anyone can come along and join in – the only requirement is that you have a Flickr account. Some of the most popular parts of Utata are undoubtedly the group projects – there are typically about three or four group projects in progress at any time, each with their own theme. To take part in a project, all members need to do is tag their photo with the specified project tag and it will then automatically get included in the project pages on the Utata web site. We do all of the hard work of keeping track of which photos are in which project, and presenting the project pages to the world. It is participation made easy – “tag and run”. To make all that happen, there’s a whole pile of stuff in place behind the scenes. We have a sizeable set of PHP classes, a bunch of perl scripts and a MySQL database, that between them keep track of who all the Utata members are and which photos have been added to which project. The end result of all of this is a pretty smooth and largely automated system. A bunch of cron scripts keep the project database up to date and we have a private administration site that allows Utata administrators to quickly and easily perform a wide variety of tasks: they can set up and publish new projects, create polls, update member details, post new blog entries and so on. We also have HAL, the UtataBot , who keeps a watchful eye on the group pool, making sure that members abide by the rules and that we don’t just become a general dumping ground [RevDanCatt: here’s another pool cleaning bot, hipbot ] . There are also the Utata badges that show off the latest contributions to the group pool. Actually, it’s only now, as I sit and write this list, that I realise just how much code there is sitting there on the Utata servers. One of the key parts of the system is the list of projects and their entries. Utata is a popular site, and it’s clearly not feasible to go back to Flickr to search for project entries each time a page is viewed, so a good deal of the code that runs Utata is devoted to maintaining our local copy of that data – keeping the cached data up to date and retrieving less commonly used elements from Flickr on demand as various pages are viewed. Given the number of projects held on Utata, and the size of the membership, it’s important to make efficient use of the cache, and I’ve tried a number of different strategies to minimise the number of API calls required. The biggest thorn in my side here is the limitation in flickr.photos.search that you can only ever get back the first 4000 photos for a tag-based search. I have some work-arounds to get around that, but if that limitation was ever increased I could make the whole thing far more efficient. An interesting, but little known fact about Utata, is that it has its own API , which if you were to take a close look at, might well look rather familiar – it uses the Flickr API as its role model. There are only has a limited number of methods right now, but the framework allows more to be added quite easily. The idea behind the API is to enable other interesting applications to be written. There are only a few of these external applications in existence at the moment – there’s a stand-alone project browser, which I don’t think anyone other than me uses. A more interesting one is the Utatascreensaver, which cycles through photos from the Utata project of your choice. As well as Utata, I dabble with a variety of other projects. The main thing I’m working on right now is a revamp of my Set Manager ( http://www.dopiaza.org/flickr/setmgr/ ). One of the first comments I ever posted to Flickr Ideas was a request for Smart Sets – sets that could dynamically update themselves. You can think of them as the photographic equivalent of SmartPlaylists in iTunes. At the time, I was just starting to experiment with the Flickr API, so I decided to try my hand at building an application to create sets automatically. It’s not especially complicated, it’s really just a simple wrapper around flickr.photos.search, but it has certainly struck a chord with people, with currently over 25,000 authenticated users. It hasn’t changed very much since it was first written, so I figured it was time to give it something of an overhaul. I’ve recently been playing around with Adobe’s Flex framework and decided that revamping the set manager would give me a good excuse to get to learn more about Flex and understand how it all hangs together – and so Version 2 of the set manager is now well under way. This new version allows you to not only build sets based on tags, interestingness, date and so on, it also allows you build sets based around location – the places API provides a very easy way to get access to all of the photos in a given location. For a more visual approach to set building, I’ve also hooked in Google Maps (sorry, Yahoo!) using their rather nifty Flash component to allow areas of interest to be easily chosen. This new Set Manager also piggy-backs on some of the work I did on Utata – it now exposes an API, built using the same framework as the Utata API, to allow the Flex application to communicate with the server. Most of the Set Manager revamp has now been completed, and few Flickr stalwarts have been helping test it all out. The plan is to get this new version up and running by the end of the year – all I need is a little more spare time to finish things off… 2. What are the best tricks or tips you’ve learned working with the Flickr API? David: Gustavo sneaked in ahead of me here and got in first with some of the best answers, but the main one is worth repeating – cache your data. Every API call you make can add several seconds on to the run time of your application or the load time of your web page, so you should really think carefully about whether each API call is necessary. If you think you’re going to need that data again in the near future, store it away somewhere – in a database, in the user’s session, anywhere. A good caching strategy can make the difference between a snappy responsive web site and a painfully slow, treacle-laden disaster. One thing that seems to catch out a lot of people is the 4000 item limit to searches, which seems to be poorly documented (i.e., not at all). If you’re not aware of this limitation, it can suddenly rear it’s ugly head when you’re least expecting it – and usually when it’s least convenient. I remember the first time a Utata tag exceeded 4000 photos (the utatafeature tag currently returns over 16000 photos) and that certainly caused a few headaches. The only solution to this problem seems to be to construct your searches so that they’re sure to return less than 4000 matches. For the Utata tag searches, I ended up breaking the search down into a set of searches using smaller date ranges and aggregated the results. It’s not perfect, but it seems to be holding up. And if you build an application or tool and release it out into the world, be prepared for an endless stream of emails from people who want it to do something just a little bit different from what it does right now… 3. As a Flickr developer what would you like to see Flickr do more of and why? David: Well, more Kitten Tuesdays are clearly an obvious priority. Apart from that? Well, here goes: One of the biggest things I would like to see is a better way for Flickr members to allow API access to their photos. At the moment, users can either opt in or opt out of third-party API searches – it’s an all or nothing thing. There are good reasons for wanting to opt out of searches. It is all too easy, for example, to get an API key and create a rogue application: one that might display photos in an unwelcome context – on a commercial web site, perhaps. Of course, applications that violateFlickr’s terms of service tend to get shut down pretty quickly, but someone has to spot them and bring them to Flickr’s attention before that can happen. Threads regularly appear in the help forum about this, and many people choose to opt-out of public API searches to avoid having their photos show up in places they don’t expect. The problem with opting out is that members can then no longer make use of some of the applications and services that they would actually like to have access to their photos. Utata is a good example – we have many members who would like to take part in the projects but can’t because they are not willing to opt back in to API searches on a global basis. I would like to see the current opt-out system expanded to allow members who opt out to then opt back in to API searches for selected applications – an API application white list, if you like. That would allow people to explicitly choose which applications were trusted and so grant them access. There are a few changes to existing API methods that I’d really like to see: – That 4000 photo limit I keep mentioning – it would be really nice if that could be lifted, even if only for certain search types, such as time-ordered. – flickr.photos.getInfo returns a whole host of useful information about a photo, but if I want to display that photo somewhere, I also invariably need to know what sizes are available to me. flickr.photos.getSizes gives me that, but that’s now two API calls. I almost always end up calling these two methods as a pair – it would be really nice if flickr.photos.getInfo could optionally return the sizes as well. – One of the most common feature requests I get for my set manager is the ability to sort by number of favourites, or by views, or by number of comments. Allowing flickr.photos.search to do this would make a lot of people very happy. – and I know it’s been mentioned quite recently, but searching on EXIF data would be very nice… 4. What excites you about Flick and hacking? What do you think you’ll build next or would like someone else to build so you don’t have to? David: The thing I love about Flickr is that it’s not just about the photographs. The social network – that jumbled mass of communities that live within Flickr – is really the thing that makes Flickr so compelling. Without those interactions and relationships between people, Flickr would be just another web site full of photos, but as Gustavo’s graphs featured in the last 5 Questions help demonstrate, there’s much more depth to Flickr than a simple on-line photo gallery. It is quite fun building tools to help people perform particular tasks more easily – creating sets, uploading photos, managing group subscription, and so on – but all that’s not a patch the satisfaction gained from building tools that help whole new communities develop. That’s what makes Flickr for me: the people. What will I build next? I don’t know. There’s always something to tinker with, and you never quite know where that tinkering will lead. The whole shapes thing looks quite interesting, but I haven’t had chance to really have a proper look at that yet. That might well be next on my list to play with – who knows? 5. Besides your own, what Flickr projects and hacks do you use on a regular basis? Who should we interview next? David: The Flickr tool I’m using the most right now is Jeffrey Friedl’s ‘Export to Flickr’ Lightroom Plug-in ( http://regex.info/blog/lightroom-goodies/flickr/ ). It’s a super little application – it integrates seamlessly with Adobe Lightroom, and does a brilliant job of streamlining the upload process. It’s a very well thought out piece of software – definitely my favourite Flickr tool of 2008. Who to interview next? Sam Judson ( http://www.flickr.com/photos/samjudson/ ). Sam was technical editor on my Flickr Mashups book (did I forget to mention that I wrote a whole book on building Flickr applications? Flickr Mashups , published by Wrox. Buy one now.). Anyway, as I was saying, Sam was technical editor on my book, and did a great job there, helping point some of my most embarrassing errors before they gotcommitted to print. He knows lots about the API and has been using it for even longer than me. He’s the author of the FlickrNet API library ( http://www.codeplex.com/FlickrNet ) and a regular contributor to both the Flickr API group and the developer mailing list . Dan: Thank you, David, and sorry about taking forever to post your answers :) Next up Sam Judson . Photos by dopiaza, Malingering and Axel Bührmann .", "date": "2009-02-13,"},
{"website": "Flickr", "title": "Dorks!", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2009/02/14/dorks/", "abstract": "http://flickr.com/photos/tags/1234567890/ ☆＼（*＾▽＾*）／☆", "date": "2009-02-14,"},
{"website": "Flickr", "title": "[changelog] Delete Tag Cross Things inspired by Delete Tag Cross Things", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2009/02/14/changelog-delete-tag-cross-things-inspired-by-delete-tag-cross-things/", "abstract": "Proving that no change is too small for [changelog] (and that I need to keep on top of them a little more) as spotted over in the Help Forum: “tags look different” thread , a change in the deleting “x” on Tags. Dunstan and Ross snuck^H^H^H^H^H rolled out the change out on Friday and it looks something like this … Worth noting that Dunstan goes on to say “ They all might end up a bit smaller, and a bit lighter. We’ll just live with them for a little bit and see how they go. ” … at which point I’ll post another changelog or not, depending on what happens :) As no change is without a protest group feel free to register your outrage over in the Flickreenos Against **New Xs** Group. Or at least until the next change comes out, at which point the protest group will move onto that.", "date": "2009-02-14,"},
{"website": "Flickr", "title": "Mise en hack", "author": ["Kellan Elliott-McCrea"], "link": "https://code.flickr.net/2009/02/16/mise-en-hack/", "abstract": "Our very own waferbaby has been rocking it out with his series, “The Setup” , on hacker mise en place . (and by hacker we mean in every sense of the word, from _why the lucky stiff , to YACHT , to David Lanham ) And if you missed it, waferbaby had the Flickr dev team do our own little version of the “The Setup” last May, published for posterity at Trickr, or Humanising the Developers (Part 1) and (Part 2) The Setup , check it out.", "date": "2009-02-16,"},
{"website": "Flickr", "title": "Found in space", "author": ["Neil Kandalgaonkar"], "link": "https://code.flickr.net/2009/02/18/found-in-space/", "abstract": "The Jellyfish Nebula , by DJMcCrady , with annotation by Astrometry.net A robot intelligence has invaded Flickr. The “blind astrometry server” is a program which monitors the Astrometry group on Flickr, looking for new photos of the night sky. It then analyzes each photo, and from the unique star positions shown it figures out what part of the sky was photographed and what interesting planets, galaxies or nebulae are contained within. Not only does the photographer get a high-quality description of what’s in their photo, but the main Astrometry.net project gets a new image to add to its storehouse of knowledge. Needless to say this is one of the coolest uses of Flickr groups and the API that we’ve ever seen. I recently discussed the project with team member Christopher Stumm , since he was the one who had the idea to hook it into Flickr. With Astrometry.net, you’re distributing the work of cataloguing the sky to amateurs. Are we still in an age where the average person can make contributions to astronomical science? Christopher Stumm Definitely. There’s a large number of excellent amateur setups out there, and they discover supernovae and minor planets regularly. Although science-grade data is generated, it can be difficult to use because it’s hard to find, and there’s typically no useful meta-data. We’re hoping to help with that problem. The catalog we use to solve images was put together from surveys during the last 50 years – that’s a long time! We believe that if the information generated by the amateur astronomer community is harnessed we could build an open-source sky survey much faster. On top of that, we would be able see what areas of the sky have interesting activity. Right now we’re using images from around the web to calculate the path comet Holmes took through the sky. Your scale and rotation invariant hashing algorithm is fiendishly clever. Where does it come from? It’s an adaptation of an old idea in computer vision — “geometric hashing” — to astronomical images.  It was originally created by researchers who were trying to model associative memory; “that shape reminds me of something I’ve seen before”.  The idea works great for astronomical pictures, because stars are easy to locate exactly.  By adding a fast search method for similar-shaped arrangements of stars, and a check that eliminates coincidental matches, we’re able to match an image against the whole sky, usually in a matter of seconds. Actually, sometimes we still do get false matches, but it turns out that that’s almost always because of some problem in either the image or our reference. These are places where we could use many amateur astronomer images to patch our reference catalog. Have there been any surprises about the data you’ve received, or the response you’ve gotten? Any discoveries? One thing that has surprised me has been the amount of positive feedback we’ve gotten. The project was covered on a few sites including kottke.org , Reddit , and O’Reilly’s Make magazine . After reading about it people had to test it out, so we saw pictures from people’s back yards, some people tried to fool the system by inputting hand-drawn images, and one person even passed in a screen shot from an iPhone application which shows you the night sky. The submissions range from amazingly high quality to images where someone just took their camera and pointed it at the night sky with some trees and a house included too . I was surprised how robust our solver was to some of the obstructions we find in the images. Overall I think it shows people are curious about the sky, but it can be pretty very overwhelming due to its sheer size. Hopefully we help make it a little more approachable by showing people what’s in their picture. Why Flickr? How has the experience been? I was a Flickr user , liked the service, and believed it offered much of what we were looking for at the time. Flickr had a huge user base which meant it was familiar and would be easy for users to upload images, submit them to our group, and offered basically unlimited scalability. The image annotations offered by Flickr were really the icing on the cake and made the whole feature much more compelling, especially to people who aren’t astronomy buffs such as myself. On top of that the API made it easy to hook into. I can’t really complain about anything so far with the whole experience, it’s been a lot of fun.", "date": "2009-02-18,"},
{"website": "Flickr", "title": "YUI Blog: Improving The Flickr Upload Exprience With YUI Uploader", "author": ["Scott Schiller"], "link": "https://code.flickr.net/2009/02/26/yui-blog-improving-the-flickr-upload-exprience-with-yui-uploader/", "abstract": "Visual analogy of simultaneous file uploading. Also, internet/pipe joke goes here. As a site which has many nifty JavaScript-driven features, Flickr makes good use of the Yahoo! User Interface library for much of its JavaScript DOM, Event handling and Ajax functionality. One of the fancier widgets we’ve implemented is a flashy browser-based Web Uploadr which uses the YUI Uploader component (a combination of JavaScript and Flash) which allows for faster batch uploads, progress reporting, a nicer UI and overall improved user experience. Head over to the YUI Blog and check out how Flickr uses YUI Uploader to provide a faster, shinier upload experience.", "date": "2009-02-26,"},
{"website": "Flickr", "title": "Videos in the Flickr API: Part Deux", "author": ["Myles Grant"], "link": "https://code.flickr.net/2009/03/02/videos-in-the-flickr-api-part-deux/", "abstract": "We hope it’s obvious that long photos are taken extremely seriously here at FlickrHQ . To that end, it’s important that videos be first class citizens in our api. And with today’s launch of video for free users and the HD output , we have some important new api changes to share with you. So if you’re an uploader developer, a mobile application developer, or you make those things that are changing the way that people consume their tv and other media (nudge-nudge Boxee ), take a look at these new additions to the api and make our long photos feel right at home in your apps. With the addition of our HD output, all videos are now transcoded into 2 or 3 MP4s. Since these are in a widely-supported format, we thought it would be interesting to make them available to third-party developers. flickr.photos.getSizes will give you semi-permanent urls to the 700k site output, the mobile-optimized output, the 2mbps HD output (if available), and the original video (if the call is authenticated by the video owner and the owner is a pro member). For example: <rsp stat=\"ok\">\n<sizes canblog=\"1\" canprint=\"1\" candownload=\"1\">\n<size label=\"Square\" width=\"75\" height=\"75\" source=\"http://farm4.static.flickr.com/3436/3232057393_815b1c5d26_s.jpg\" url=\"http://www.flickr.com/photos/mylesdgrant/3232057393/sizes/sq/\" media=\"photo\"/>\n<size label=\"Thumbnail\" width=\"100\" height=\"56\" source=\"http://farm4.static.flickr.com/3436/3232057393_815b1c5d26_t.jpg\" url=\"http://www.flickr.com/photos/mylesdgrant/3232057393/sizes/t/\" media=\"photo\"/>\n<size label=\"Small\" width=\"240\" height=\"135\" source=\"http://farm4.static.flickr.com/3436/3232057393_815b1c5d26_m.jpg\" url=\"http://www.flickr.com/photos/mylesdgrant/3232057393/sizes/s/\" media=\"photo\"/>\n<size label=\"Medium\" width=\"500\" height=\"281\" source=\"http://farm4.static.flickr.com/3436/3232057393_815b1c5d26.jpg\" url=\"http://www.flickr.com/photos/mylesdgrant/3232057393/sizes/m/\" media=\"photo\"/>\n<size label=\"Original\" width=\"1280\" height=\"720\" source=\"http://farm4.static.flickr.com/3436/3232057393_204af3bcff_o.jpg\" url=\"http://www.flickr.com/photos/mylesdgrant/3232057393/sizes/o/\" media=\"photo\"/>\n<size label=\"Video Player\" width=\"640\" height=\"360\" source=\"http://www.flickr.com/apps/video/stewart.swf?v=1233362721&photo_id=3232057393&photo_secret=815b1c5d26\" url=\"http://www.flickr.com/photos/mylesdgrant/3232057393/\" media=\"video\"/>\n<size label=\"Site MP4\" width=\"640\" height=\"360\" source=\"http://www.flickr.com/photos/mylesdgrant/3232057393/play/site/815b1c5d26/\" url=\"http://www.flickr.com/photos/mylesdgrant/3232057393/\" media=\"video\"/>\n<size label=\"Mobile MP4\" width=\"480\" height=\"360\" source=\"http://www.flickr.com/photos/mylesdgrant/3232057393/play/mobile/815b1c5d26/\" url=\"http://www.flickr.com/photos/mylesdgrant/3232057393/\" media=\"video\"/>\n<size label=\"HD MP4\" width=\"1280\" height=\"720\" source=\"http://www.flickr.com/photos/mylesdgrant/3232057393/play/hd/815b1c5d26/\" url=\"http://www.flickr.com/photos/mylesdgrant/3232057393/\" media=\"video\"/>\n<size label=\"Video Original\" width=\"1280\" height=\"720\" source=\"http://www.flickr.com/photos/mylesdgrant/3232057393/play/orig/123fakest/\" url=\"http://www.flickr.com/photos/mylesdgrant/3232057393/\" media=\"video\"/>\n</sizes>\n</rsp> Use this if you’re lazy. If you’re not lazy and want to be one of the cool kids, you can construct the urls to these outputs yourself, assuming you have the nsid or custom url of the owner, the video id, and the secret (or original secret). Requesting the HD output for a video that doesn’t have it (because it’s less than 720 pixels high, or the owner is a free member), will deliver a 404. An example url: http://www.flickr.com/photos/{user-id|custom-url}/{photo-id}/play/{site|mobile|hd|orig}/{secret|originalsecret}/ If you’re developing uploading tools for free users, flickr.people.getUploadStatus has new attributes to determine whether a user is allowed to upload any more videos or not. If the user is a free user, the output of this method will now look something like this: <rsp stat=\"ok\">\n<user id=\"88251462@N00\" ispro=\"0\">\n<username>Dev Myles</username>\n<bandwidth max=\"107373568\" used=\"0\" maxbytes=\"104857600\" usedbytes=\"0\" remainingbytes=\"104857600\" maxkb=\"104857\" usedkb=\"0\" remainingkb=\"104857\" unlimited=\"0\"/>\n<filesize max=\"10485760\" maxbytes=\"10485760\" maxkb=\"10240\" maxmb=\"10\"/>\n<sets created=\"7\" remaining=\"lots\"/>\n<videosize maxbytes=\"157286400\" maxkb=\"153600\" maxmb=\"150\"/>\n<videos uploaded=\"0\" remaining=\"2\"/>\n</user>\n</rsp> Note the new <videos> element in the response, which will let you know how many more videos this user can upload this month. And as always, if you feel like we’re missing something, please let us know .", "date": "2009-03-2,"},
{"website": "Flickr", "title": "Panda Tuesday; The History of the Panda, New APIs, Explore and You", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2009/03/03/panda-tuesday-the-history-of-the-panda-new-apis-explore-and-you/", "abstract": "The New APIs I’ll cut straight to the chase on this one, we’ve just launched two new API methods; flickr.panda.getPhotos flickr.panda.getList The first call ( flickr.panda.getPhotos ) returns you a list of photos that the mystical Flickr Pandas are currently interested in, in the following format … <photos interval=\"60000\" lastupdate=\"1235765058272\" total=\"120\" panda=\"ling ling\">\n    <photo title=\"Shorebirds at Pillar Point\" id=\"3313428913\" secret=\"2cd3cb44cb\"\n        server=\"3609\" farm=\"4\" owner=\"72442527@N00\" ownername=\"Pat Ulrich\"/>\n    <photo title=\"Battle of the sky\" id=\"3313713993\" secret=\"3f7f51500f\"\n        server=\"3382\" farm=\"4\" owner=\"10459691@N05\" ownername=\"Sven Ericsson\"/>\n    <!-- and so on -->\n</photos> When calling this API method please ensure that your code uses the lastupdate and interval attributes to determine when to request new photos. lastupdate is a Unix timestamp indicating when the list of photos was generated and interval is the number of seconds to wait before polling the Flickr API again. The second call ( flickr.panda.getList ) tells you which of the Mystical Flickr Pandas are around to request photos from. Ling Ling and Hsing Hsing both return photos they are currently interested in, both have slightly different tastes in photos depending on their mood. The (currently) third Panda Wang Wang returns photos that have recently been geotagged, not quite realtime but close. Important! No-one fully understands the whims and fancies of the Pandas at any moment in time, these APIs give you a direct dump of what they are looking at (filtered for safe and public photos only) at this very moment. We cannot promise that this will not include ladies (or men) in bikinis, or possibly even ladies not in bikinis. You should keep this in mind while planning for your target audience, I’ll touch on this a little bit more in the Implementation Hints section. Philosophy We’re always toying around with different ways to find and surface “ interesting ” photos. One method that people are generally familiar with is the Explore page, of which there’s always a lot of discussion. A good starting place for more Explore information is here with more general discussion in the Secrets Of Explore group. But behind Explore is an amount of “Secret Sauce” a lot of which focuses on activity around a media object . The Explore page is generally a daily experience, although things often pop in and out during the day. What we are doing with the Pandas is exposing a more real time experience, letting them each focus on various aspects of that activity, the first two Ling Ling and Hsing Hsing are fairly introspective and have different views of Flickr, but both focus on what they’ve noticed going on recently . The third Wang Wang is more a panda of the world and concentrates solely on geotagging. If you wanted to show photos being geotagged nearly as they happen, Wang Wang is the way to go. From a numbers point of view, Explore shows 500 photos from the last 7 days … a Panda can get through around 150,000 photos per day. A couple of caveats; Just because a Panda notices a photo, doesn’t mean that photo will make it into Explore, or even be destined to be considered by the Magic Donkey for Explore. Conversely, because a photo has made it into Explore doesn’t necessarily mean that a Panda will have noticed it. Also, because of the complexity and amount of stuff going on, no-one really knows exactly what Ling Ling and Hsing Hsing will find interesting, but that won’t stop us from tweaking the underlining code now and then. Also, you may have noticed that this is one of the less serious APIs, it’s both experimental and there to hopefully encourage fun and play. Please respect the Pandas and don’t abuse them, help us protect this endangered species and keep them alive. Implementation Hints So, why are we doing all this messing around with “Pandas”? Well, mainly because we want to give developers another stream of photos with which to play with. For example you could build your own Flickr Zeitgeist Badge or slideshow or tool to just show stuff that’s going on over at Flickr. We built this … … (many people wished we hadn’t) … the Rainbow Vomiting Panda of Awesomeness as an experiment (which used Ling Ling fwiw). Since then we’ve been tweaking the backend, and now finally released the API so you can all have a go. It’s a stream of, on average, more interesting photos then you’d generally get from polling Everyone’s photos. The quality is pretty good, the best thing to do is watch The Panda for a while and figure out if a ) you want to build something with a live stream of photos b ) you can build something more better than a vomiting panda (which lets face it, it pretty hard to top!). This is what I learnt while building it, it may help you too … When you get a packet of photos back from a panda, its around 60 seconds worth of stuff that the Panda found interesting. The packet is sorted by what the Panda found most interesting in that timeframe to least . Because people can often find ladies in bikinis interesting (for some reason) there can sometimes be some activity around ladies in bikinis that the Pandas may pick-up on. However, Pandas tend not to find ladies in bikinis that interesting (which may have something to do with why they’re in such peril) and when those photos appear they tend to happen in lower half of the packet of photos. So in the case of the Vomiting Panda above, I just threw away the second half of each packet, rather than go through all of the photos before asking for the next packet. I found this to be a good thing to do anyway, even though there’s always a lot of amazing stuff going on on Flickr the tail end of some packet wasn’t always quite as amazing as I wanted. You may want to do things differently, but that’s just what I found while working with the APIs. Send us stuff Anyway, on that note, at some point I’ll try to do a round up of stuff people have built on these APIs. If you want to post what you build to the Flickr Hacks group and/or FlickrMail me I’ll start to build up a list and take it from there. Photos by ohad* and psd .", "date": "2009-03-3,"},
{"website": "Flickr", "title": "Moar Panda: Is a Firehose of Snowflakes a Nor’easter?", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2009/03/04/moar-panda-is-a-firehose-of-snowflakes-a-noreaster/", "abstract": "Kellan in his blog post over here: Is a Firehose of Snowflakes a Nor’easter? correctly points that that I may have been slightly cagey about the awesomeness of what we actually put out yesterday , by saying … “But because the documentation is quirky, I think people missed the significance. These are Flickr real time data APIs.” My excuse is that’s my natural instinct to sneak new features past the lawyers, by dressing them up in a non-serious fashion :) But in reality, this is just an excuse to post the following photo … … see there I go again. Offsetting the important with light humor, go read Kellan’s post now for he is smart and knows what he’s talking about. Now stop trying to get on explore & enjoy yourself Photo by Las Tonterias points out", "date": "2009-03-4,"},
{"website": "Flickr", "title": "[changelog] Revision of the Places page, also Neighborhoods", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2009/03/16/changelog-revision-of-the-places-page-also-neighborhoods/", "abstract": "A slightly overdue (and longer than it was supposed to be) post, considering this happened a while ago, but I thought I’d mention a few subtle updates to the Places page. Even before that though we’ve added neighborhood links to the Photo pages, before we just listed the neighborhood … … now its a link through to the Places page itself, which look rather like this … Meaning that from a photo that’s been geotagged you’ll be able to get more of a feel for the local area. Obviously this work better in large Cities where the neighborhoods themselves can be as big as towns , while in the towns you’re more likely to find the one or two photographers who count each neighborhood as their stomping ground. I guess that’s the big addition, but we also tweaked a few other things at the same time, here’s a before and after shot, you’ll probably need to click through to the larger size if you want to see the details. On the left side of each Places page we’ve moved different elements around, pushing the search further up, the date/time down and scrapping the weather altogether now that we’ve established that it rains in London. The functional changes over on the right involved moving the title, attribution, Next &  Previous buttons off the photo. When we launched Places we didn’t have Videos, and now we do and the old position clashed with the video controls. The other benefit of the Next/Prev moving to above the photo is that they dont jump around as the photos resized. We also added key controls, now you can just press the forward and back cursor buttons on the keyboard to keep going through photos, power user tip! The “paging” buttons no longer hover over the top of the thumbnails, as they were … Annoying. Not always obvious. On a more technical level, now only geotagged photos appear on the Places page, and where possible the location shown on the map (sometimes with Neighborhoods, due to the nature of the beast, they can be just off the edge of the map). Big obvious arrow demonstrating the feature :) … When we first launched the Places page we wanted to make sure that each location had plenty of photos, so we used a combination of geotagging and tags/description to automate the selection of them. Which lead to interesting results such as the city of Reading in England featuring a lot of photos about books (tagged reading, natch). Now that we have over 100 Million geotagged photos we’ve switched to “just” them. We also factor in the Season a photo was taken in to select the interesting ones, to give us a bit more change in the first photos you see and too keep them relatively, well, seasonal. We’ll probably tweak this again soon to get them to rotate even more often, but still working through that one. City Colours and Endless Photos As mentioned above we moved the time down and, partly for whimsy, partly because they’re really useful, used the Dopplr colour to display it and link their pages. Here’s our Los Angeles page and Dopplr’s Los Angeles page , Dopplr decided to use Flickr to select photos for each City they know about, so we thought we borrow their colour in return :) You can read more about how Dopplr (and therefor us) calculate the colour for a place over on their blog: In rainbows and Darker city colours . Finally, because I’ve gone on enough already, the old design used to have two rows of thumbnails under the main photo and a total of 72 photos, meaning there were 6 “pages” of thumbnails. When you got to the last page, that was kinda it, you couldn’t go any further. Instead there’s now just one row, but I bolted on the API, so it keeps trying to load more and more photos as you get close to the end of the current lot. Instead of just 72 photos for Los Angeles there’s now the full (currently) nearly half million 444,594 photos. Which reminds me, we should probably add a Slideshow that that page :) And that’s the revised Places page. [Edit: Oh and I know we’ve just launched Stats (again), but it’s nice to give the dev a couple of days to recover before forcing them to write a changelog about it ;) ]", "date": "2009-03-16,"},
{"website": "Flickr", "title": "[changelog] High Definition Kitten Tuesday (New HD Search option)", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2009/03/17/changelog-high-definition-kitten-tuesday-new-hd-search-option/", "abstract": "The other day Myles added the option to search for just High Definition video to the Advanced Search page. Clearly just so I can use it to find HD Kitten Videos … Thoughtfully I’ve highlighted the option here … Video by kathrynriechert .", "date": "2009-03-17,"},
{"website": "Flickr", "title": "Building Fast Client-side Searches", "author": ["Ross Harmes"], "link": "https://code.flickr.net/2009/03/18/building-fast-client-side-searches/", "abstract": "Yesterday we released a new people selector widget (which we’ve been calling Bo Selecta internally). This widget downloads a list of all of your contacts, in JavaScript, in under 200ms (this is true even for members with 10,000+ contacts). In order to get this level of performance, we had to completely rethink how we send data from the server to the client. Server Side: Cache Everything To make this data available quickly from the server, we maintain and update a per-member cache in our database, where we store each member’s contact list in a text blob — this way it’s a single quick DB query to retrieve it. We can format this blob in any way we want: XML, JSON, etc. Whenever a member updates their information, we update the cache for all of their contacts. Since a single member who changes their contact information can require updating the contacts cache for hundreds or even thousands of other members, we rely upon prioritized tasks in our offline queue system . Testing the Performance of Different Data Formats Despite the fact that our backend system can deliver the contact list data very quickly, we still don’t want to unnecessarily fetch it for each page load. This means that we need to defer loading until it’s needed, and that we have to be able to request, download, and parse the contact list in the amount of time it takes a member to go from hovering over a text field to typing a name. With this goal in mind, we started testing various data formats, and recording the average amount of time it took to download and parse each one. We started with Ajax and XML; this proved to be the slowest by far, so much so that the larger test cases wouldn’t even run to completion (the tags used to create the XML structure also added a lot of weight to the filesize). It appeared that using XML was out of the question. BoSelectaJsonGoodFunTimes: eval() is Slow Next we tried using Ajax to fetch the list in the JSON format (and having eval() parse it). This was a major improvement, both in terms of filesize across the wire and parse time. While all of our tests ran to completion (even the 10,000 contacts case), parse time per contact was not the same for each case; it geometrically increased as we increased the number of contacts, up to the point where the 10,000 contact case took over 80 seconds to parse — 400 times slower than our goal of 200ms. It seemed that JavaScript had a problem manipulating and eval()ing very large strings, so this approach wasn’t going to work either. Contacts File Size (KB) Parse Time (ms) File Size per Contact (KB) Parse Time per Contact (ms) 10,617 1536 81312 0.14 7.66 4,878 681 18842 0.14 3.86 2,979 393 6987 0.13 2.35 1,914 263 3381 0.14 1.77 1,363 177 1837 0.13 1.35 798 109 852 0.14 1.07 644 86 611 0.13 0.95 325 44 252 0.14 0.78 260 36 205 0.14 0.79 165 24 111 0.15 0.67 JSON and Dynamic Script Tags: Fast but Insecure Working with the theory that large string manipulation was the problem with the last approach, we switched from using Ajax to instead fetching the data using a dynamically generated script tag. This means that the contact data was never treated as string, and was instead executed as soon as it was downloaded, just like any other JavaScript file. The difference in performance was shocking: 89ms to parse 10,000 contacts (a reduction of 3 orders of magnitude), while the smallest case of 172 contacts only took 6ms. The parse time per contact actually decreased the larger the list became. This approach looked perfect, except for one thing:  in order for this JSON to be executed, we had to wrap it in a callback method. Since it’s executable code, any website in the world could use the same approach to download a Flickr member’s contact list.  This was a deal breaker. Contacts File Size (KB) Parse Time (ms) File Size per Contact (KB) Parse Time per Contact (ms) 10,709 1105 89 0.10 0.01 4,877 508 41 0.10 0.01 2,979 308 26 0.10 0.01 1,915 197 19 0.10 0.01 1,363 140 15 0.10 0.01 800 83 11 0.10 0.01 644 67 9 0.10 0.01 325 35 8 0.11 0.02 260 27 7 0.10 0.03 172 18 6 0.10 0.03 Going Custom Having set the performance bar pretty high with the last approach, we dove into custom data formats. The challenge would be to create a format that we could parse ourselves, using JavaScript’s String and RegExp methods, that would also match the speed of JSON executed natively.  This would allow us to use Ajax again, but keep the data restricted to our domain. Since we had already discovered that some methods of string manipulation didn’t perform well on large strings, we restricted ourselves to a method that we knew to be fast: split(). We used control characters to delimit each contact, and a different control character to delimit the fields within each contact. This allowed us to parse the string into contact objects with one split, then loop through that array and split again on each string. that.contacts = o.responseText.split(\"\\c\");\n\nfor (var n = 0, len = that.contacts.length, contactSplit; n < len; n++) {\n\n\tcontactSplit = that.contacts[n].split(\"\\a\");\n\n\tthat.contacts[n] = {};\n\tthat.contacts[n].n = contactSplit[0];\n\tthat.contacts[n].e = contactSplit[1];\n\tthat.contacts[n].u = contactSplit[2];\n\tthat.contacts[n].r = contactSplit[3];\n\tthat.contacts[n].s = contactSplit[4];\n\tthat.contacts[n].f = contactSplit[5];\n\tthat.contacts[n].a = contactSplit[6];\n\tthat.contacts[n].d = contactSplit[7];\n\tthat.contacts[n].y = contactSplit[8];\n} Though this technique sounds like it would be slow, it actually performed on par with native JSON parsing (it was a little faster for cases containing less than 1000 contacts, and a little slower for those over 1000). It also had the smallest filesize: 80% the size of the JSON data for the same number of contacts. This is the format that we ended up using. Contacts File Size (KB) Parse Time (ms) File Size per Contact (KB) Parse Time per Contact (ms) 10,741 818 173 0.08 0.02 4,877 375 50 0.08 0.01 2,979 208 34 0.07 0.01 1,916 144 21 0.08 0.01 1,363 93 16 0.07 0.01 800 58 10 0.07 0.01 644 46 8 0.07 0.01 325 24 4 0.07 0.01 260 14 3 0.05 0.01 160 13 3 0.08 0.02 Searching Now that we have a giant array of contacts in JavaScript, we needed a way to search through them and select one. For this, we used YUI’s excellent AutoComplete widget . To get the data into the widget, we created a DataSource object that would execute a function to get results. This function simply looped through our contact array and matched the given query against four different properties of each contact, using a regular expression (RegExp objects turned out to be extremely well-suited for this, with the average search time for the 10,000 contacts case coming in under 38ms). After the results were collected, the AutoComplete widget took care of everything else, including caching the results. There was one optimization we made to our AutoComplete configuration that was particularly effective. Regardless of how much we optimized our search method, we could never get results to return in less than 200ms (even for trivially small numbers of contacts). After a lot of profiling and hair pulling, we found the queryDelay setting. This is set to 200ms by default, and artificially delays every search in order to reduce UI flicker for quick typists. After setting that to 0, we found our search times improved dramatically. The End Result Head over to your Contact List page and give it a whirl. We are also using the Bo Selecta with FlickrMail and the Share This widget on each photo page.", "date": "2009-03-18,"},
{"website": "Flickr", "title": "Tags in Space", "author": ["Neil Kandalgaonkar"], "link": "https://code.flickr.net/2009/03/20/tags-in-space/", "abstract": "A lot of you enjoyed our post ( “Found in Space” ) on the amazing astrometry.net project, and there have been some interesting followups. A mysterious figure known only as “jim” paired up astronomy photos from Flickr with Google Sky . (You’re going to need the Google Earth plug-in for your browser — just follow the instructions on that page if you don’t have it.) In his technical writeup , “jim” explains how he used the Yahoo Query Language (YQL) to fetch the data. YQL is similar to the existing Flickr APIs , but it’s a query language like SQL rather than a set of REST-ish APIs. And both of those are really just ways to get data out of Flickr’s machine tag system, specifically the astro:* namespace. It’s turtles all the way down. Who else is using astrotags? The British Royal Observatory in Greenwich is sponsoring a contest to determine the Astronomy Photographer of the Year and the whole thing is based on a Flickr group and extensive use of Flickr’s APIs. The integration is so seamless — galleries of photos and discussions are surfaced on their site as well as ours — you might as well consider Flickr to be their “backend” server. But they’ve also added much, such as great documentation about how to astrotag your photos as well as a concise explanation about how Astrometry.net identifies your photo, even among millions of known stars. (The sci-fi website io9 interviewed Fiona Romeo of the Royal Observatory about the contest; check it out.) It’s dizzying how many services have been combined here — Astrometry.net grew out of research at the University of Toronto, web mashups use Google Sky for visualization in context, Yahoo infrastructure delivers and transforms data, the Royal Observatory at Greenwich provides leadership and expertise, and then little old Flickr acts as a data repository and social hub. And let’s not forget you, the Flickr community, and your inexhaustible creativity — which is the reason why all this can even come together. All this was done with pretty light coordination and few people at Flickr were even aware what was going on until recently. I have no idea what the future is for APIs and a web of services loosely joined, but I hope we get to see more and more of this sort of thing.", "date": "2009-03-20,"},
{"website": "Flickr", "title": "How the contact cache was won", "author": ["Daniel Bogan"], "link": "https://code.flickr.net/2009/04/01/how-the-contact-cache-was-won/", "abstract": "You say ‘cash’, I say ‘kaysh’ Flickr has a lot of users. A lot . And most of those users have contacts, family, friends; somewhere between none and a bajillion. Or tens of thousands, anyhow. That’s a lot of relationships flying hither and yon, meaning we can’t just cache this stuff on the fly whenever the need strikes us. And strike it did. Thus, Bo Selecta. This project was designed to grab up a person’s contacts from anywhere in Flickrspace, and it had to be usable in bits of the site we hadn’t even designed yet. But it also had to not suck, and it had to be fast. Supafast . Luckily for us, we have at our disposal a shipping crate in the basement full of terribly clever little robots wearing suitable, fleshy attire and having names like Ross and Paul and Cal. Walking into the river As Rossbot has already covered , we spent a lot of time back-and-forthing on how we’d seed this aggregated cache all over the damned place without compromising on speed or our own general sexual attractiveness. Plus, I just wanted to use big words like ‘aggregated’ and ‘seed’. As I’ve already mentioned above, making this magic happen at request time was not an option, so we turned to our (somewhat) trusty offline tasks system . These tasks munge and purge and generally do all sorts of wonderful data manipulation on boxes separate to the main site, in a generally orderly fashion, and do it in the background. Offline tasks do it in the background First up, we needed to work out what data we’d actually want to cache, which ended up being a minimal chunk useful enough for Rossbot to do whatever it is he does with Javascript that makes the ladies throw their panties on stage, and not a single byte more. We ended up with something that looks like this: Oh, you’re a clever one. That’s actually a picture of a fish. We really ended up with something like this: NSIDaemail@address.comacharacter_nameareal nameaicon serveraicon farmapath aliasais_friendais_familyamagic_dust Thus, we’re generating a bunch of contact data separated by designated control characters, and ultimately stored in a TEXT field in a database. The first time your cache is built, we actually walk your entire contact list and generate one of these chunks for each person you’re affiliated with. On subsequent updates, we use a bit of regular expression hoohah pixie dust to only change the necessary details from individuals, and write those changes back to the DB. Big ups to Mylesbot for his help with making these tasks as efficient and as well-oiled as he is. Speaking of updates, clearly we have to make sure we catch any changes you or your contacts make, so we have various spots around the site that fire off these offline tasks – when you update your various profile details, when you pick a named URL on Flickr for the first time, or when change your relationship with someone. These updates have been carefully honed to work in the context of what’s changing – again, to squeeze out as much speed as we can. F’instance, there’s no need for us to tell all of your contacts that your relationship with SexyBabe43 has progressed to ‘Friend’. Unless that’s your sort of thing, but really, let’s leave that as an exercise for the reader. All of this attention to detail has ultimately helped us eck out as much speed as possible. Seeing a theme here? So any time you’re sending a Flickrmail, searching for a contact or sharing a photo, think of the robots, and smile that secret little smile of yours, knowingly.", "date": "2009-04-1,"},
{"website": "Flickr", "title": "The Only Question Left Is", "author": ["Aaron Straup Cope"], "link": "https://code.flickr.net/2009/04/07/the-only-question-left-is/", "abstract": "photo by Shawn Allen At the Emerging Technology conference this year Stamen Design’s Michal Migurski and Shawn Allen led an afternoon workshop called “ Maps from Scratch: Online Maps from the Ground Up ” where people made digital maps from, well… scratch. If you’ve never heard of Stamen they’ve been doing some of the most exciting work around the idea of “ custom cartography ” including: Cabspotting , Oakland Crimespotting and Old Oakland Maps , work for the London Olympics , and designing custom map tiles for CloudMade . (Stamen also built the recently launched Flickr Clock :-) All of this is interesting in its own right; proof that there is still a lot of room in which to imagine maps beyond so-called red-dot fever . All of this is extra interesting in light of Apple’s recent announcement to allow developers to define their own map tiles in the next iPhone OS release. All of this super-duper interesting because it is work produced by a team of less than 10 people . The tools , and increasingly the data , to build the maps we want are bubbling up and becoming easier and more accessible to more people every day. Easier, anyway. “One of the things that made this tutorial especially interesting for us was our use of Amazon’s EC2 service, the “Elastic Compute Cloud” that provides billed-by-the-hour virtual servers with speedy internet connections and a wide variety of operating system and configuration options. Each participant received a login to a freshly-made EC2 instance (a single server) with code and lesson data already in-place. We walked through the five stages of the tutorial with the group coding along and making their own maps, starting from incomplete initial files and progressing through added layers of complexity. “Probably the biggest hassle with open source geospatial software is getting the full stack installed and set up, so we’ve gone ahead and made the AMI (Amazon Machine Image, a template for a virtual server) available publicly for anyone to use, along with notes on the process we used to create it .” — Michal Migurski The Maps From Scratch (MFS) AMI may not be a Leveraged Turn Key Synergistic Do-What-I-Mean Solutions Platform but, really, anything that dulls the hassle and cost of setting up specialized software is a great big step in the right direction . I mention all of this because Clustr, the command-line application we use to derive shapefiles from geotagged photos , has recently been added to the list of tools bundled with the MFS AMI. Specifically: ami-4d769124 . We’re super excited about this because it means that Clustr is that much easier for people to use. We expressly chose to make Clustr an open-source project to share some of the tools we’ve developed with the community but it has also always had a relatively high barrier to entry. Building and configuring a Unix machine is often more that most people are interested in, let alone compiling big and complicated maths libraries from scratch. Clustr on EC2 is not a magic pony factory but hopefully it will make the application a little friendlier. Creating and configuring an EC2 account is too involved for this post but there are lots of good resources out there, starting with Amazon’s own documentation . When I’m stuck I usually refer back to Paul Stamatiou’s How To: Getting Started with Amazon EC2 . Assuming that you familiar using Unix command line tools, let’s also assume that you have gotten all your ducks in a row and are ready to fire up the MFS AMI: your-computer> ec2-run-instances ami-4d769124 -k example-keypair\n\nyour-computer> ec2-describe-instances At which point, you’ll see something like this: INSTANCE i-xxxxxxxx ami-4d769124 ec2-xxxxx.amazonaws.com blah blah blah i-xxxxxxxx is the unique identifier of your current EC2 session. You will need this to tell Amazon to shut down the server and stop billing you for its use. ec2-xxxxx.amazonaws.com is the address of your EC2 server on the Internets. Once you have that information, you can start using Clustr. First, log in and create a new folder where you’ll save your shapefile: your-computer> ssh -i example-rsa-key root@ec2-xxxxx.amazonaws.com\n\nec2-xxxxx.amazonaws.com> mkdir /root/clustr-test The MFS AMI comes complete with a series of sample “points” files to render. We’ll start with the list of all the geotagged photos uploaded to Flickr on March 24: ec2-xxxxx.amazonaws.com> /usr/bin/clustr -v -a 0.001 /root/clustr/start/points-2009-03-24.txt /root/clustr-test/clustr-test.shp By default Clustr generates a series of files named clustr (dot shp , dot dbf and dot shx because shapefiles are funny that way) in the current working directory. You can specify an alternate name by passing a fully qualified path as the last argument to Clustr. When run in verbose mode (that’s the -v flag) you’ll see something like this: Reading points from input.\nGot 44410 points for tag '20090324'.\n799 component(s) found for alpha value 0.001.\n- 23 vertices, area: 86.7491, perimeter: 71.9647\n- 32 vertices, area: 1171.51, perimeter: 41.3095\n- 8 vertices, area: 18.5112, perimeter: 0.529504\n- 12 vertices, area: 1484.81, perimeter: 10.8544\n...\nWriting 505 polygons to shapefile. Yay! ec2-xxxxx.amazonaws.com> ls -la /root/clustr-test\ntotal 172\ndrwxr-xr-x 2 root root  4096 2009-04-07 03:14 .\ndrwxr-xr-x 5 root root  4096 2009-04-07 02:22 ..\n-rw-r--r-- 1 root root 52208 2009-04-07 03:14 clustr-test.dbf\n-rw-r--r-- 1 root root 97388 2009-04-07 03:14 clustr-test.shp\n-rw-r--r-- 1 root root  4140 2009-04-07 03:14 clustr-test.shx Now copy the shapefiles back to your computer and terminate your EC2 instance (or you might be surprised when you get your next billing statement from Amazon). ec2-xxxxx.amazonaws.com> scp -r /root/clustr-test \n   you@your-computer:/path/to/your/desktop/\n\nec2-xxxxx.amazonaws.com> exit\n\nyour-computer> ec2-terminate-instances i-xxxxxxxxx I created this image (using the open source QGIS application) for all those points by running Clustr multiple times with alpha numbers ranging from 0.05 to 603 : Here’s another version rendered using the nik2img application and a custom style sheet, both included with the MFS distribution: Here’s one of all the geotagged photos tagged “ route66 ” (with alpha numbers ranging from 0.001 to 0.5): Apologies and big sloppy kisses to Stamen’s own Mappr (first released in 2005). Or tagged “ caltrain “, the commuter train that runs between San Francisco and San Jose: Meanwhile, Matt Biddulph at Dopplr has been generating a series of visualizations depicting the shape of where to eat, stay and explore for the cities in their Places database. This is what London looks like: Or: “ London dopplr places, filtered to only places my social network has been to, clustrd “. One of the things I like the most about Clustr is that it will generate shape(file)s for any old list of geographic coordinates. Now that most of the hassle of setting up Clustr has been (mostly) removed, the only question left is: What do you want to render? “They do not detail locations in space but histories of movement that constitute space.” — Rob Kitchin, Chris Perkins If you’re like me you’re probably thinking something like “Wouldn’t it be nice if I could just POST a points file to a webservice running on the AMI and have it return a compressed shapefile?” It sure would so I wrote a quick and dirty version (not included in the MFS AMI; you’ll need to do that yourself) in PHP but if there are any Apache hackers in the house who want to make a zippy C version that would be even Moar Awesome ™. If you don’t want to use the MFS AMI and would rather just install Clustr on your own machine instance, here are the steps I went through to get it work on a Debian 5.0 (Lenny) AMI; presumably the steps are basically the same for any Linux flavoured operating system: $> apt-get update\n$> apt-get install libcgal-dev\n$> apt-get install libgdal1-dev\n$> apt-get install subversion\n\n$> svn co http://code.flickr.com/svn/trunk/clustr/ $> cd clustr\n$> make\n$> cp clustr /usr/bin/\n\n$> clustr -h\n\nclustr 0.2 - construct polygons from tagged points\nwritten by Schuyler Erle\n\n(c) 2007-2008 Yahoo!, Inc.\n\nUsage: clustr [-a <n>] [-p] [-v] <input> <output>\n   -h, -?      this help message\n   -v          be verbose (default: off)\n   -a <n>      set alpha value (default: use \"optimal\" value)\n   -p          output points to shapefile, instead of polygons\n\nIf <input> is missing or given as \"-\", stdin is used.\nIf <output> is missing, output is written to clustr.shp.\nInput file should be formatted as: <tag> <lon> <lat>n\nTags must not contain spaces. Just like that! photo by Timo Arnall", "date": "2009-04-7,"},
{"website": "Flickr", "title": "Integrating Flickr into your Rails website", "author": ["Kellan Elliott-McCrea"], "link": "https://code.flickr.net/2009/04/09/integrating-flickr-into-your-rails-website/", "abstract": "Straightforward, and thorough tutorial on working with the Flickr API and Rails . Takes you from installing the libraries, to coding, to designing your layout, to caching to speed up your site (and keep Flickr happy with you).  Awesome.", "date": "2009-04-9,"},
{"website": "Flickr", "title": "I See Smart People, AKA: We Do Stuff …", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2009/04/09/i-see-smart-people-aka-we-do-stuff/", "abstract": "Over on our sister photo arty blog you could easily imagine reading phrases like “ One of the amazing things about working at Flickr is the vast amount of incredible photography it exposes you to “, or some such. Hah! Those arty types! Over here, I’d like to post the flip side … about how one of the amazing things about working at Flickr, is the awesome people I get to work with. Take for example … Ross Harmes … people often think I’m joking when we’re sitting in a meeting, discussing how we should standardise our front-end coding conventions or some such and I say we should just “ask Ross”. But! BUT!! but, Ross wrote a fricking book about JavaScript; Pro JavaScript Design Patterns and sits 3 desks away, it’s faster (and more amusing (to me)) to shout out a question than it is to flick through the index of the book. It’s like having the talking Kindle version , but with a much more natural voice! You can also find out more at jsdesignpatterns.com Anyway … The reason why I wanted to post this, is that recently an awful lot of my co-workers have been doing stuff! So with an eye to that … The Lovely Kellan Elliott-McCrea Flashing quickly across the radar last couple of weeks is/was a bunch of discussion about URL shorteners, sort of starting with Josh over here: on url shorteners , and David with The Security Implications of URL Shortening Services . With other discussions popping up here and there and sorta everywhere . And so, with this bit of philosophy from “Stinky” Willison (more on Simon later) … … Kellan built this ( maybe even during meetings); RevCanonical: url shortening that doesn’t hurt the internet , you can read more about it in his blog post URL Shortening Hinting [Note: includes mention of Flickr], where the comments are worth price of admission alone, also, features hamster photos. If you want to join in, you can read more over on the official RevCanonical blog and get, fork or whatever it is people do with code on github . And I’m sure we’ll have more news about RevCanonical here soon :) But not content with starting that wildfire, Kellan has also been does his bit to help OpenStreetMap , by walking around with a GPS unit and, I think this part is important, drinking beer … … shown here featuring our good friend Mikel Maron , remember we use OpenSteetMap on Flickr when our own maps are a little sparse. More on maps later! Meanwhile … The Mighty John Allspaw Mr Allspaw is our wonderful Ops guy, here he is … … smoldering. As well as smoldering he also gave a talk at the Web 2.0 Expo called Operational Efficiency Hacks the other week. If you’re into that type of thing and missed it, which you probably did, here are his slides … Operational Efficiency Hacks Web20 Expo2009 View more presentations from John Allspaw . … and his follow up post adds a little further reading. If you like this kinda of thing you should probably subscribe to his blog where he posts really interesting Flickr related stuff, and infuriatingly enough *not* here on this blog, /me sulk. On the subject of Allspaw (and as we’ve already mentioned one book), I was pretty sure I’d mentioned his book before: The Art of Capacity Planning: Scaling Web Resources … if building big things on the web is your kinda thing, but apparently I haven’t, so … According to a reviewer on Amazon “John’s examples are just like Charlie’s from the TV show Numb3rs” , having never watched Numb3rs I can only assume that’s a probably a bad thing (kinda like Scully from X-Files explaining science) but gave it 4 stars anyway :) on those grounds alone you should buy it … Oh and don’t forget the WebOps Visualizations Pool on flickr, that John often posts to when things suddenly get much better or worse ;) if you enjoy graphs like this … Getting back to the front-end for a second … Scott Schiller, Fish enthusiast! You’ve heard of Muxtape right? That website that plays MP3s, how’d you think it does that? Or the cool javascript/audio stuff that Jacob Seidelin does over at Nihilogic ; JavaScript + Canvas + SM2 + MilkDrop = JuicyDrop & Music visualization with Canvas and SoundManager2 ? Screenshot shown here (warning: link to noisy thing) … Well they both use SoundManager 2 written by our very own Scott Schiller . It’s an extensive and easy to use javascript thingy … wait, Scott says it better … “ SoundManager 2 wraps and extends the Flash Sound API, and exposes it to Javascript. (The flash portion is hidden, transparent to both developers and end users.) ” … which basically means that if you like JavaScript, messing with audio but hate , I mean, dislike working with Flash, it can save you a lot of pain. Here’s one of the demos Scott put together … And if you think that all looks awesome, remember that Scott is one of our fantastic front-end guys, bringing all that good js magic to Flickr! Apart from the music part, well unless we one day decide to add music and customisable backgrounds to flickr [ 1 ] . Aaron Straup Cope Aaron covered this only the other day: The Only Question Left Is , but has been doing an awful lot with generating shapefiles recently. I just wanted to add my take to it, because even I have trouble keeping up. Basically what I want, is to be able to send something-somewhere a list of latitudes and longitudes and it return me the “shape” that those points make. This could be anything, the locations of geotagged Squirrels , or even something useful, well kinda like this from Matt Biddulph (him wot of Dopplr ) … “ London dopplr places, filtered to only places my social network has been to, clustrd “ … which maps out something of interest to him, where his “social network” go/eat etc. in London. Which may be different from mine, or could even have some overlap, thus answering the time old question; which pub should we all go to for lunch? It’s not quite at the point where you can do it without having to put a little effort in, but I keep prodding Aaron because I want it now! But if you’re the type that does enjoy putting the effort in then you can again do the GitHub dance here: ws-clustr and py-wsclustr (Python bindings for spinning up and using an EC2 instance running ws-clustr). Once more, more on maps later. Daniel Bogan – Setup Man Bogan is virtual, and only exists in the internets, as can be seen here … … kind of like Max Headroom , but with worse resolution. Which I think makes Flickr the first interweb company you have a real AI working on the code, not that pretend AI stuff ! Last year he has a bash at helping to put a little context around us delicate flower developers, with a quick run-down of the setup we each used with Trickr, or Humanising the Developers (Part 1) & Trickr, or Humanising the Developers (Part 2) . Based on an old project of his called “The Setup” where he used to interview various Internet Famous people (when there wasn’t so damn many of them) about their Setup. Recently he’s reprised that task, with, wait for it … The Setup , where our very own Bogan asks such leading lights as John Gruber , Steph Thirion , Jonathan Coulton & Gina Trapani . I have no idea how he finds the time! In turn you can read a quick interview with Bogan over at indicommons . Rev Dan Catt — errr, me! Meanwhile, if you’re reading this blog, you’ve probably already seen this, I’ve been trying my hand at using Processing to visualize 24 hours of geotagged photos on Flickr … … which I managed by following the instructions here Processing, JSON & The New York Times to get Processing to consume our very own Flickr API in JSON format . Which in turn started me off prodding at the Processing group … and Visualization groups on Flickr … Pulling it all together So that’s what some of us are up-to, and going back to the start, I’m amazed and all the stuff that goes on, brilliant minds and all that. In my head, this is what ties it all together, hang on, here we go … Kellan’s been walking around with a GPS unit (along with 1000s of others), adding to the OpenStreetMap (OSM) dataset, we (Flickr) sometimes use that dataset, but also … Matt Jones (also him wot of Dopplr ) made this … … using Cloudmade , who in turn use OSM data to allow people to easily style up and use maps. Now, I’m sure Mr Jones, wont mind me saying that he’s not a coder, infact here he is; Matt Jones – Design Husband … … but a non-programmer can now easily make maps, as demonstrated above and described here: My first Cloudmade map style: “Lynchian_Mid” . Then using our wonderfully public Shape Files API ( flickr.places.getShapeHistory which yes you can get in JSON format for using with Processing or JavaScript) overlay boundaries. Even, when it’s easier (for the non-programmer) plot outlines and shapes, based on the code Aaron is working on, onto those maps. More on making maps here: maps from scratch . But where could you get such useful data for plotting or visualising, well obviously there’s our API , which is where the senseable team at MIT got the data for their Los ojos del mundo (the world’s eyes) project, again using Processing … (un)photographed Spain from senseablecity on Vimeo . But also, let us recall “Stinky” Willison , one time employee of Flickr, who now works at The Gruadian . They have a geocoding project, that allows you, if you so wished to place their stories on a map … http://guardian.apimaps.org/search.html … which uses Mapping from Cloudmade, map data from OpenStreetMap, location search from our very own API, and stories from their own API. Which in turn allows you to plot their stories on your own maps, phew! More about the Guardian Open Platform . You can also read about their Data Store , which gives you access to a load of easy to use data just ripe for visualizing… … be that with Processing, Flash or JavaScript (following the advice in Ross’s book), and even with photos from Flickr and Audio driven with Scott’s SoundManager2, in “Shapes” powered by Aaron, and preserved with short URLs that’ll stick around, thanks Kellan :) and you can scale it if you need to by following John’s insights. And that’s just what we do when we’re not working on Flickr. Photos by Ross Harmes , Kellan , jspaw , jesse robbins , Matt Biddulph , waferbaby , moleitau and dan taylor . [ 1 ] never going to happen.", "date": "2009-04-9,"},
{"website": "Flickr", "title": "Kittens vs Sunsets", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2009/04/15/kittens-vs-sunsets/", "abstract": "Speaking of things that Kellan has done recently, Jayel over in this thread: Flickr Trends Search kicked off a whole Flickr Central thread of This vs That … … including Girls vs Boys , Mayonnaise vs. Ketchup , Winter vs Summer and Varying vs Flatlined . Why not go and have a play? As the footer says “derived from the awesome NYT Trender and code by derekg adapted for Flickr API by kellan ”", "date": "2009-04-15,"},
{"website": "Flickr", "title": "[changelog] “Find my location” button", "author": ["Rev Dan Catt"], "link": "https://code.flickr.net/2009/04/16/changelog-find-my-location-button/", "abstract": "I just added a button to the Explore Map and the pop-up map you see when geotagging your own photos from the photo page (organizer support v.soon). Using my l33t Skitch skills I’ll attempt to highlight it … … but WAIT! You may not see it! It’s one of those “Power-User” type things… To get the button to show up you’ll need some form of geo-locating built-in/plug-in type thing, or maybe you’re all smarty-pants and running a cutting edge beta version of a browser with location finding built in already. Perhaps you’ve already installed Google Gears , in which case we’ll use that. Probably the easiest way of getting the button to appear is to pop over to the Loki site and click the “Try it Now” button, install the plug-in, then pop back to Flickr. Loki is from the SKYHOOK Wireless peeps, who all the cool kids seem to be using. You can also click over to the Mozilla Labs and read more about their Geode project, about how location stuff will soon be built into browsers and everything and install their geode plug-in from there. Either way, it’ll check all three “things” and show the button if it finds one, as more options come along I’ll add those too. As an aside … This is why you shouldn’t do graphical buttons and multi-language support at the same time. … nightmare!", "date": "2009-04-16,"},
{"website": "Flickr", "title": "What Would Brooklyn Do?", "author": ["Aaron Straup Cope"], "link": "https://code.flickr.net/2009/04/20/what-would-brooklyn-do/", "abstract": "photo by kfranzman The other day, Mike Ellis posted a really lovely interview with Shelley Bernstein and Paul Beaudoin about the release of the Brooklyn Museum’s Collections API . One passage that I thought was worth calling out, and which I’ve copied verbatim below, is Shelley’s answer to the question “Why did you decide to build an API?” First, practical… in the past we’d been asked to be a part of larger projects where institutions were trying to aggregate data across many collections (like d*hub). At the time, we couldn’t justify allocating the time to provide data sets which would become stale as fast as we could turn over the data. By developing the API, we can create this one thing that will work for many people so it no longer become a project every time we are asked to take part. Second, community… the developer community is not one we’d worked with before. We’d recently had exposure to the indicommons community at the Flickr Commons and had seen developers like David Wilkinson do some great things with our data there. It’s been a very positive experience and one we wanted to carry forward (emphasis mine) into our Collection, not just the materials we are posting to The Commons. Third, community+practical… I think we needed to recognize that ideas about our data can come from anywhere, and encourage outside partnerships. We should recognize that programmers from outside the organization will have skills and ideas that we don’t have internally and encourage everyone to use them with our data if they want to. When they do, we want to make sure we get them the credit they deserve by pointing our visitors to their sites so they get some exposure for their efforts. The only thing I would add is: What she said!", "date": "2009-04-20,"},
{"website": "Flickr", "title": "The Absence and the Anchor", "author": ["Aaron Straup Cope"], "link": "https://code.flickr.net/2009/05/06/the-absence-and-the-anchor/", "abstract": "photo by selflesh Back in January, I wrote a blog post about some experimental work that I’d been doing with the shapefile data we derive from geotagged photos. I was investigating the idea of generating shapefiles for a given location using not the photos associated with that place but, instead, from the photos associated with the children of that place. For example, London: The larger pink shape is what we (Flickr) think of as the “city” of London. The smaller white shapes are its neighbourhoods. The red shapes represent an entirely new shapefile that we created by collecting all the points for those neighbourhoods and running them through Clustr , the tool we use to generate shapes. For lack of any better name I called these shapes “donut holes” because, well, because that’s what they look like. The larger shape is a pretty accurate reflection of the greater metropolitain area of London, the place that has grown and evolved over the years out of the city center that most people would recognize in the smaller red shape. Our goal with the shapefiles has always been to use them to better reverse-geocode people’s geotagged photos so these sorts of variations on a theme can better help us understand where a place is . Like New York City. No one gets New York right including us try as we might (though, in fairness, it’s gotten better recently (no, really)) and even I am hard pressed to explain the giant pink blob, below, that is supposed to be New York City. On the other hand, the red donut hole shape even though (perhaps, because) it spills in to New Jersey a bit is actually a pretty good reflection of the way people move through the city as a whole. It could play New York on TV, I think. I’m not sure how to explain the outliers yet, either, other than to say the shapefiles for city-derived donut holes may contain up to 3 polygons (or “records” in proper Shapefile-speak ) compared to a single polygon for plain-old city shapes so if nothing else it’s an indicator of where people are taking photos. If the shapefiles themselves are uncharted territory, the donut holes are the fuzzy horizon even further off in the distance. We’re not really sure where this will take us but we’re pretty sure there’s something to it all so we’re eager to share it with people and see what they can make of it too. (This is probably still my favourite shapefile ever.) Starting today, the donut hole shapes are available for developers to use with their developer magic via the Flickr API . At the moment we are only rendering donut hole shapefiles for cities and countries. I suppose it might make sense to do the same for continents but we probably won’t render states (or provinces) simply because there is too much empty unphotographed space between the cities to make it very interesting. There are also relatively few donut holes compared to the corpus of all the available shapefiles so rather than create an entirely new API method we’ve included them in the flickr.places.getShapeHistory API method which returns all the shapefiles ever created for a place. Each shape element now contains an is_donuthole attribute. Here’s what it looks like for London : <shapes total=\"6\" woe_id=\"44418\" place_id=\".2P4je.dBZgMyQ\"\n\tplace_type=\"locality\" place_type_id=\"7\">\n\n\t<shape created=\"1241477118\" alpha=\"9.765625E-05\" count_points=\"275464\"\n\t\tcount_edges=\"333\" is_donuthole=\"1\" >\n\n\t\t<!-- shape data goes here... -->\n\n\t</shape>\n\n\t<!-- and so on ->\n\n</shapes> Meanwhile, the places.getInfo API method has been updated to included a has_donuthole attribute, to help people decide whether it’s worth calling the getShapeHistory method or not. Again, using London as the example: <place place_id=\".2P4je.dBZgMyQ\" woeid=\"44418\" latitude=\"51.506\"\n       longitude=\"-0.127\" place_url=\"/United+Kingdom/England/London\"\n       place_type=\"locality\" place_type_id=\"7\" timezone=\"Europe/London\"\n       name=\"London, England, United Kingdom\" has_shapedata=\"1\" >\n\n\t<shapedata created=\"1239037710\" alpha=\"0.00029296875\" count_points=\"406594\"\n                   count_edges=\"231\" has_donuthole=\"1\" is_donuthole=\"0\" >\n\n        <!-- and so on -->\n</place> Finally, here’s another picture by Shannon Rankin mostly just because I like her work so much. Enjoy! photo by selflesh", "date": "2009-05-6,"},
{"website": "Flickr", "title": "Flickr Shapefiles Public Dataset 1.0", "author": ["Aaron Straup Cope"], "link": "https://code.flickr.net/2009/05/21/flickr-shapefiles-public-dataset-10/", "abstract": "photo by dp The name sort of says it all, really, but here’s the short version: We are releasing all of the Flickr shapefiles as a single download, available for use under the Creative Commons Zero Waiver . That’s fancy-talk for “public domain”. The long version is: To the extent possible under law, Flickr has waived all copyright and related or neighboring rights to the “Flickr Shapefiles Public Dataset, Version 1.0”. This work is published from the United States. While you are under no obligation to do so, wherever possible it would be extra-super-duper-awesome if you would attribute flickr.com when using the dataset. Thanks! We are doing this for a few reasons. We want people (developers, researchers and anyone else who wants to play) to find new and interesting ways to use the shapefiles and we recognize that, in many cases, this means having access to the entire dataset. We want people to feel both comfortable and confident using this data in their projects and so we opted for a public domain license so no one would have to spend their time wondering about the issue of licensing. We also think the work that the Creative Commons crew is doing is valuable and important and so we chose to release the shapefiles under the CC0 waiver as a show of support. We want people to create their own shapefiles and to share them so that other people (including us!) can find interesting ways to use them. We’re pretty sure there’s something to this “shapefile stuff” even if we can’t always put our finger on it so if publishing the dataset will encourage others to do the same then we’re happy to do so. photo by mbkepp The dataset itself is pretty straightforward. It is a single 549MB XML file uncompressed (84MB when zipped). The data model is a simple, pared-down version of what you can already get via the Flickr API with an emphasis on the shape data. Everything lives under a single root places element. For example: <place woe_id=\"26\" place_id=\"BvYpo7abBw\" place_type=\"locality\" place_type_id=\"7\" label=\" Arvida, Quebec, Canada \">\n\t<shape created=\"1226804891\" alpha=\"0.00015\" points=\"45\" edges=\"15\" is_donuthole=\"0\">\n\t\t<polylines bbox=\"48.399932861328,-71.214576721191,48.444801330566,-71.157333374023\" >\n\t\t\t<polyline> <!-- points go here--> </polyline>\n\t\t</polylines>\n\t\t<shapefile url=\" http://farm4.static.flickr.com/3203/shapefiles/26_20081116_082a565562.tar.gz \" />\n\t</shape>\n\t\n\t<!-- and so on -->\n</place> Aside from the quirkiness of the shapes themselves, it is worth remembering that some of them may just be wrong. We work pretty hard to prevent Undue Wronginess ™ from occurring but we’ve seen it happen in the past and so it would be, well, wrong not to acknowledge the possibility. On the other hand we don’t think we would have gotten this far if it wasn’t mostly right but if you see something that looks weird, please let us know The dataset is available for download, today, from: http://www.flickr.com/services/shapefiles/1.0/ The other exciting piece of news is that the Yahoo! GeoPlanet team has also released a public dataset of all their WOE IDs that include parent IDs, adjacent IDs and aliases (that’s just more fancy-talk for “different names for the same place”) under the Creative Commons Attribution License . Which is pretty awesome, really. They’ve also released the GeoPlanet Placemaker API . You feed it a big old chunk of free-form text and then “the service identifies places mentioned in text, disambiguates those places, and returns unique identifiers (WOEIDs) for each, as well as information about how many times the place was found in the text, and where in the text it was found.” Again, Moar Awesome . And a bit dorky. It’s true. The data, all by itself, won’t tell a story. It needs people and history to make that possible but as you poke around all this stuff don’t forget the value of having a big giant, and now open, database of unique identifiers and what is possible when you use them as a bridge between other things. Without WOE IDs we wouldn’t have been able to generate the shapefiles or do the Places project or provide a way to search for photos by place, rather than location . Enjoy! Oh, and those “unidentified” outliers, in New York City, that I mentioned in the last post about the donut hole shapefiles : The Bronx Zoo, Coney Island and Shea Stadium . Of course! photos by ajagendorf25 , auggie tolosa and the sky", "date": "2009-05-21,"},
{"website": "Flickr", "title": "Every Step A Story", "author": ["Aaron Straup Cope"], "link": "https://code.flickr.net/2009/06/23/every-step-a-story/", "abstract": "photo by aakash nihalani Many of you will have already read on the sister Flickr.com blog that we added “nearby” pages to the m.flickr.com site , last week, for phones that support the W3C Geolocation API (that means the iPhone, or Gears if you’ve got an Android phone). Ross summed it up nicely, writing: Use this to explore your neighborhood, or find the best places to photograph local landmarks from. Reload the page as you walk around a city, and see the things that have happened there in the past. You’ll see a place through the eyes of the flickrverse. We’ve also updated the nearby pages on the main site so that when you go to… www.flickr.com/nearby …without a trailing latitude and longitude, we’ll see if you have any one of a variety of browser plugins that can tell us your location. This is similar to the Find My Location button on the site maps, that Dan described back in April , but for nearby! Like the iPhone’s Mobile Safari browser, the next version of  Firefox (version 3.5, currently being tested as a release candidate) will also support automagic geolocation so you won’t even need to install any plugins or other widgets. Just point your browser to www.flickr.com/nearby/ and away you go. photo by Candy Chang The other piece of nearby-related news is Tom Taylor’s fantastic FireEagle application for the Mac called Clarke . Clarke is a toolbar app that sits quietly in the background and scans the available wireless networks using the Skyhook APIs to triangulate your position and updates FireEagle with your current location. In addition to being an excellent FireEagle client, Clarke also supports Nearby-iness for a variety of services, including Flickr. I’m writing this post from FlickrHQ , in downtown San Francisco, so when I choose Flickr from Clarke’s Nearby menu it loads the following page in my web browser: http://www.flickr.com/nearby/37.794116,-122.402776 Which is kind of awesome! It means that you can travel to a brand new place, open up your laptop and just like magic (read: once you’ve connected to a wireless network) see pictures nearby. Woosh!", "date": "2009-06-23,"},
{"website": "Flickr", "title": "Twitter in the API", "author": ["Kellan Elliott-McCrea"], "link": "https://code.flickr.net/2009/06/30/twitter-in-the-api/", "abstract": "photo by mocachip Ever since we launched our Flickr2Twitter beta , developers have been requesting new API methods, so they can support Flickr as a photo sharing option in their Twitter clients. I’ve got good news, and bad news. The bad news is we don’t have any new APIs to offer you. The good news is we shipped our “Twitter APIs” nearly five years ago. Let me explain. Working with Blogs (including Twitter) For as long as anyone can remember, we’ve supported the option of posting to external blogs directly from Flickr. Once you’ve conﬁgured a blogging service it becomes available in the “Blog This” drop down, as an option for Upload by Email, and, of  course, in the API. You and I might have serious philosophical questions about whether Twitter is a blogging service, but our web servers are more pragmatic. To them, the Twitter integration is just a new blogging service. Configuring a blogging service The first step for a member wishing to blog (or tweet) via Flickr is to configure an external blog.  The only way to do this on flickr.com , generally from the Add a blog page. Twitter is a bit special (or rather a preview of things to come) as we’ve given it its own service page .  Directing users of your app to the Flickr2Twitter page is probably the best way get them “tweet ready”. All set? photo by RCKM ©®™ From here on out, you’ll need your user to have authorized you to access their Flickr account. ( Find out more about FlickrAuth ) With a signed call to flickr.blogs.getList() you can get a list of all the blogging services a member has configured.  Alternately you can pass in a service id (e.g. Twitter ) to scope the list of blogs to the service you’re interested in.  The response looks something like: <blogs>\n  <blog id=\"7214\" name=\"Code Flickr\" service=\"MetaWeblogAPI\" needspassword=\"0\" url=\"http://code.flickr.com/blog/\"/>\n  <blog id=\"7215\" name=\"Twitter: kellan\" service=\"Twitter\" needspassword=\"0\" url=\"http://twitter.com/kellan\"/>\n  <blog id=\"72157\" name=\"Twitter: Flickr\" service=\"Twitter\" needspassword=\"0\" url=\"http://twitter.com/flickr\"/>\n</blogs> This account has 3 blogs configured.  A WordPress blog, and two Twitter accounts.  Each one has a unique id.  Additionally needpassword=\"0\" means we have credentials for these blogs stored server side and you don’t need to prompt your user to log in to their blog. If you passed in Twitter as the service, and instead of the above you got something like: <blogs/> Then your user hasn’t configured any blogs for that service. The Easy Option: Upload a photo to Flickr, post to Twitter via Flickr If your application has been authorized to upload photos on your user’s behalf, and you’ve made sure they have a Twitter blog configured with Flickr, then the easiest solution is to use Flickr as a passthru service. Once you’ve successfully uploaded a photo you’ll get an API response like <photoid>1234</photoid> .  (Find out more about uploading and asynchronous uploading ). Pass the blog id from the <blogs> list above, and the photoid from the upload response to flickr.blogs.postPhoto() .  If you’re posting to Twitter the title argument is optional and the description argument is ignored. (By default the title of the photo is the body of the tweet, alternately pass a different status update in the title field) Or instead of passing a blog id, you can pass a service id (i.e. Twitter ) and the photo (and blog post) will be sent to the first matching blog of that service.  If we don’t find a blog matching that service, you’ll get a “ Blog not found. ” error. Assuming your API call to flickr.blogs.postPhoto() is well formed, Flickr will turn around and post your user’s tweet to Twitter , including a short flic.kr url linking back to their photo. photo by tonyadcockphotos The Established Option: Upload a photo Flickr, post to Twitter any which way you can If you’re looking to integrate Flickr photos into an existing Twitter application you might already have a preferred method for posting to Twitter. After you’ve successfully uploaded a photo and received the photoid follow these instructions for manufacturing a short url using the flic.kr domain. Unlike most URL shortening schemes, every photo on Flickr already has a short URL associated with it. The follow the form: http://flic.kr/p/{base58-photo-id} By the way, you shouldn’t feel constrained to only use short urls on Twitter.  They work equally well for a diverse range of applications including fortune cookies. Thumbnails If you want to display a thumbnail of a photo, you’ll need to make an API call to one of the methods that returns the photo’s secret.  Either flickr.photos.getSizes() or flickr.photos.getInfo() will do.  Read up on constructing Flickr URLs . Follow Along My favorite new game has been watching the flows of shared Flickr photos as they appear on Twitter. Happy photo sharing!", "date": "2009-06-30,"},
{"website": "Flickr", "title": "extra:extra=extra", "author": ["Aaron Straup Cope"], "link": "https://code.flickr.net/2009/07/06/extraextraextra/", "abstract": "This is Eric. We loves him! Internally, the nomenclature for tags goes something like this: There are “raw” tags (the actual tag you enter on a photo), “clean” tags (the tag that you see in a URL), “machine tags” (things like upcoming:event=2413636 ) and machine tag “extras”. Machine tag “extras” are what we call the entire process of using a machine tag as a kind of foreign key to access data stored on another website. Small pieces (of data) loosely joined (by the Internets). For example if you tagged a photo with upcoming:event=2413636 that would cause a robot squirrel on the Flickr servers to call the robot squirrels running the Upcoming API and ask for the name of the Upcoming event with ID 2413636 . Upcoming then answers back and says: That event was called “Flickr Turns 5.25” and we store the title in our database. The next time you load that photo we’ll show a little Upcoming icon and the name of the event in the sidebar. To date, we’ve only had machine tags “extras” available for upcoming:event= and lastfm:event= tags but starting today we’re adding support for three new projects: Dopplr , Open Plaques and the Open Library . dopplr:(eat|stay|explore)= Dopplr is a social travel site which recently launched a Social Atlas to allow their users to create and share lists of interesting places, in the cities they know about, of where to eat and stay and poke around during their visit. “Over time, we can anonymise and aggregate all the recommendations that have been added to Dopplr. This is the Social Atlas itself, something that’s greater than the sum of its parts: a kind of world map representing the combined wisdom of smart travellers. It’s early days still, but we are very excited by its potential.” Which is pretty exciting, especially when you think about how many pictures of delicious food people upload to Flickr! photo by moleitau You can add Social Atlas machine tags to your photos by tagging them with either \" dopplr:eat= \" , \" dopplr:stay= \" or \" dopplr:explore= \" followed by the short-code for that place. For example, dopplr:eat=tp71 . photo by moleitau As an added bonus every single page in the Dopplr Social Atlas displays the complete machine tag you need to tag your photos with so you can just copy and paste the tag from one page into the other and your photos will be updated like magic! openplaques:id= The Open Plaques website is a community-run website set up to catalogue and document the many blue plaques that are hung across the UK to commemorate persons and famous events. Frankie Roberto , one of the people behind the project has written often about the project, and the motivations behind it so rather than try to paraphrase I will just quote him (at length): “With these in mind, I was thinking how this kind of ‘mobile learning’ might apply to the heritage sector, and as you might have guessed from the title, thought of blue plaques. You see them everywhere — especially when sat on the top deck of a double decker bus in London — and yet the plaques themselves never seem that revealing. You’ve often never heard of the person named, or perhaps only vaguely, and the only clue you’re given is something like “scientist and electrical engineer” (Sir Ambrose Fleming) or “landscape gardener” (Charles Bridgeman). I always want to know more. Who are these people, what’s the story about them, and why are they considered important enough for their home to be commemorated?” — Getting information about blue plaques on your mobile phone… “The final step towards making this more compelling was to add some photographs. Here, Flickr came to our rescue. There was already a ‘blue plaques’ group , which contained hundreds of photos. To link them together, I used special tags called ‘machine tags’, which are like normal tags except that they contain some slightly more structured data. It’s very simple though — each plaque on the Open Plaques website has an ID number (which can be found at the end of the URL), and the corresponding machine tag for that plaque is openplaques:id=999 (where 999 is the ID number). Another script then uses the Flickr API to find all the photos tagged with a relevant machine tag, checks to see if they are Creative Commons licenced, and then to displays them on the Open Plaques website, with a credit and a link back to the Flickr photo page.” — Open Plaques project update So, we did the same! If you have an openplaques:id= machine tag on your photo then we’ll try to look up and display the inscription for that plaque. You can add Open Plaques machine tags to your photos by tagging them with \" openplaques:id= \" followed by the numeric ID for a specific plaque. For example, openplaques:id=1633 . openlibrary:id= The Open Library is a part of the Internet Archive whose mission is to create a “web page for every book ever published.” To do that they’re hoping that anyone and everyone will participate and help by adding information they have a published work or a particular edition. “After almost fifty years of computerizing everything, we’re realising now that the stories have gone, and we need them back — the handicraft, the boutique, the beauty, the dragons, the colour of stories. I’m reminded of the gorgeous mysterious early maps of the Australian coast. The explorer only got so far, and the cartographer could only draw so much. Much more exciting than boring old satellite, top-of-a-pin’s-head accuracy! I love the idea of trying to catch some of these dog-eared tales within Open Library.” — George Oates As it happens Flickr users have created over 900 groups about book covers and a casual search for the phrase (“book covers”) returns 98, 000 photos! Back in July of 2007 Johnson Cameraface uploaded a photo of the cover of ROBOTS Spaceships & Other Tin Toys” . Two years later, George asked if it would be alright to use the photo to update the Open Library record for the book , and added an openlibrary machine tag along the way. Now, starting today, the photo page now displays the title of the book and links back to the Open Library! This makes me happy. You can add Open Library machine tags to your photos by tagging them with \" openlibrary:id= \" followed by the unique identifier for that book. For example, openlibrary:id=OL5853184M . It’s worth noting that the unique identifiers for Open Library books are sometimes a bit of a treasure hunt; they are the letters and numbers that come after openlibrary.org/b/ and before the book title in the URL for that book. Like this: http://openlibrary.org/b/ OL5853184M /Soviet-science-fiction But wait! There’s more!! Did I mention that we have over one million photos tagged with Last.fm event machine tags? That makes it kind of hard to know when new machine tags have been added because lopping over all those tags just to find recent ones is expensive and time-consuming. To help address this problem we’ve add a shiney new API method called: flickr.machinetags.getRecentValues This does pretty much what it sounds like. Given a namespace or a predicate (or both) and a Unix timestamp, the method returns the values for those machine tags that have been added since the date specified. Enjoy!", "date": "2009-07-6,"},
{"website": "Flickr", "title": "Matthias Gansrigler: On flickery, and making your API apps fly", "author": ["Kellan Elliott-McCrea"], "link": "https://code.flickr.net/2009/07/16/matthias-gansrigler-on-flickery-and-making-your-api-apps-fly/", "abstract": "We meant to take a short break in our developer interview series.  Well we ended up taking a very long short break.  Sorry about that.  Thankfully we’re coming back with a really great interview from the developer behind flickery , a full featured, and fast Flickr desktop client.  And he is talking about super important topics: optimization, performance and caching. Part detective story, part day-in-the-life-of-a-working developer, Matthias’ nuts and bolts discussion of how to take full advantage of the Flickr API without slowing down your app is a must read. Hello there. My name is Matthias Gansrigler , I’m the founder of Eternal Storms Software , responsible for donationware like GimmeSomeTune or PresentYourApps for Mac. My newest creation and the reason I’m writing here, however – and first shareware application, might I add – is flickery , a Flickr desktop client for Mac OS X 10.5 Leopard! With flickery you can easily upload your photos to Flickr, manage your sets and favorites, view your contacts’ photos, search for photos in Flickr’s data, comment, view the most “interesting” pictures and much more. All in one tiny, yet powerful application. I was kindly invited by the great guys behind Flickr to write about my experiences optimizing calls to the Flickr API. But first, a little background story: I was born in 1986 in… well, uhm, that’s maybe a little too far back. Let’s start in 2008. That’s more like it. In February 2008, I began flirting with the idea of writing a full-featured desktop client for Flickr for Mac. But really, I just wanted to toy around with the Flickr API since I’d seen so many web-applications making use of it and had heard good things of the API in general. “like a kid in a toys-store” Soon thereafter, I had tremendous results in the shortest amount of time. I felt like a kid in a toys-store – completely overwhelmed (and way in over my head, as it later turned out). I did not worry very much about how many calls I made to the API. My main interest was in getting a product ready. So it took me about three months (until mid-May 2008) to ship my first public beta.  I had implemented lots of things, like commenting on photos, adding photos to favorites, different views of photos, etc. It was a great first public beta, feature-wise.  But under the hood, it wasn’t that great. It was sluggish, slow, leaked tons of memory and made an awful lot of calls to the Flickr API. And when I say an awful lot, I mean a hell of a lot. Flickr’s servers were smoking (well, I doubt that, because I think then I would have heard from their lawyers). “30 QPS” With just about 1,000 registered users, flickery made more than 30 QPS (queries per second) to the Flickr API. 30 QPS. Can you imagine? How come, you might ask. Just try it, it’s easy (no, seriously, don’t!). I made calls for everything. I even made calls in advance. But let me elaborate. Take flickery’s main screen. You can see 30 items in the thumbnail view. For every item in that list, I made a getInfo() and getSizes() call for later use (should the user want to view the photo bigger or view it’s description). Additionally, whenever a user selected one of the items, flickery would call out to the API again asking if it was allowed to download that photo. So that’s 1 call for the 30 photos, 30 calls for getInfo and 30 calls for getSizes . That alone is 61 calls for basically ONE user-interaction (clicking on Newest). That’s not right. Should a user go through the entire list and select every item, that’s 30 more calls (totalling at 91). That’s even not righter. If you want to get your API key shut down (which is what happened to that first public beta of flickery) – that’s absolutely the way to do it. No doubt about it. Obviously, the really hard work started here – retaining the same feature-set, the same interface and the same comfort, yet making far, far less calls to the API. What also starts here is the really technical gibberish. So get out your nerd-english dictionaries and let’s get to it. “really hard work” I basically had all the features in there for a 1.0 release. But I couldn’t release it until it made a proper amount of calls to the API. So where I started, obviously, was at removing the “in-advance” calls to the API, like getInfo and getSizes . They just don’t get called in advance anymore, only on demand. So if a user selects a photo and clicks on “More info”, then the getInfo call is made. No sooner. I chose a different approach for getSizes , which I eliminated entirely (except when downloading a photo). The photo gets loaded in thumbnail-size (which is always there). A double click on the photo loads the medium sized image, which is always there, as well. When you want to view the photo in fullscreen mode, there’s two possibilities. If the call to the API returned a o_dims attribute, we load the original size (since that’s returned in the one call to the API retrieving all the 30 photos). Should there be no such attribute, I just load the medium size and am done with it. So for viewing photos, whatever size, there’s no extra calls made. So from 61 calls, we trimmed that down to 1 call to retrieve the 30 photos. Also, when an item is selected, there is no call made anymore. Only when the user selects “Download”, flickery asks if it is allowed to download that photo and if so, makes a getSizes call to download the largest possible version. “retrieving the photolist” Now, for retrieving the photolist. In the first public beta, I made one call for every 30 images I wanted to retrieve. That’s kind of logical. However, that would mean that we would have to make 16 calls to view 480 items. We can trim that down. Flickr allows you to retrieve 500 items at once with each call. So that’s basically what I did. I wrote my own system that internally loaded 480 items, but only returned the 30 items of the page that were requested in the interface. So for viewing 480 photos, we now only need one call, which is like one 16th of the calls we made before. It has a little downside, though – the initial call takes a little longer for results to come back (since it is fetching more items at once from Flickr’s servers). But on the other hand, once those 480 items are retrieved, subsequent paging through those 16 pages is instant. Adding photos to favorites, sets and groups was the next step. Let’s stay at favorites, because basically, it’s the same procedure for all three of them. In the first public beta, a user could add one photo to their favorites over and over again, and flickery would dutifully send that call to the API. Of course, completely unnecessarily. So I just cached what was added to the favorites and just checked if we already added the photo to the favorites, which, as it turned out, saved lots of highly unnecessary calls. I also took the liberty to implement some of Flickr’s rules right into flickery, meaning the following: If you’re not the admin of a group, you can only remove your own photos from it. If you’re the admin, you can remove all of them. So instead of flickery making a call to the API when the user wants to remove a photo and they’re not the admin, the application itself tells them they can’t do that. A lot of calls are saved this way. “caching” Another big step in making less calls to the API was caching, naturally. In the first public beta, nothing except the frob for talking to the API was saved over application launches. So every time the user launched the application, the authorization token, the Newest photos, the user’s sets and contacts were loaded. All that has changed and they are now saved over restarts for different time intervals. Newest photos for one hour, sets and contacts forever (albeit, the user can reload photosets groups and contacts (once an hour) should they ever be out of sync). The same goes for the contents of photosets, groups, favorites and own photos. To remove some other calls, if there’s a cache already in place for, say, a photoset, I don’t delete that cache should a new photo be added to it, but update it. So, the call is made to the API to add the photo to the set, obviously, but to then view the set (with the newly added photo in it), I don’t reload the photoset but just update the existing cache. Another example here is comments. When adding your own comment to a photo, flickery doesn’t reload the entire comment-list, but just adds the new comment to the already existing cache. “extras” Specifying options in the “extras” parameter can save you lots of calls as well. ( e.d. see our post on the standard photos response for more on extras ) Instead of having to make another call to retrieve certain information about an item, you can have all kinds of information returned in the initial response. Say, you want the owner’s name of items. Instead of having to call flickr.photos.getInfo on every item, you just specify the owner_name option in the “extras” parameter and immediately have the owner’s name for every item of your query. This is really handy and saves lots of time for the users and lots of calls for you. As for the Flickr API – it’s really great and easy to use. However, there are some smaller things I’d love to see: Being able to specify more than 1 photo id for adding to / removing from favorites, photosets and groups – resulting in far less calls to the API, the upload response returning not only the photo id for the uploaded image, but also the farm and server, official support for collections, adding / removing contacts, support for retrieving videos, maybe even some calls for flickrMail. Some more “extras” options would be great, too, like can_download, for example, which is currently only retrievable through flickr.photos.getInfo . Other than that, the Flickr API let’s you really do everything you could imagine. There’s almost no restriction to what you can do with it! Closing, I’d just like to thank the wizards at Flickr for the opportunity to write about optimizing here and working so closely with me on flickery. Here’s to more great applications using the Flickr API!", "date": "2009-07-16,"},
{"website": "Flickr", "title": "horse=yes", "author": ["Aaron Straup Cope"], "link": "https://code.flickr.net/2009/07/22/horseyes/", "abstract": "Yes, I got a bit emotional at the third OpenStreetMap conference, held in the CCC, Amsterdam last weekend — mainly because this globe we are on is the only one we know — we really are mapping our universe, doing it our way. Creating the world we want to live in. I thought it worth while to say “Thanks” to some people. Being British, the feeling of being a bit foolish stopped me from being too effusive! — Tim Waters A couple weeks ago I had the pleasure of attending, and the privilege of speaking at, the State of the Map conference in Amsterdam. I told the story of how we came to use Open Street Maps (OSM), how it works on the backend and talked a little bit about what we’d like to do next: Moving beyond “bags of tiles”, a better way to keep up to date with changes to the OSM database and, for good measure, a little bit of tree-hugging at the end. Most of all, though, I wanted to take the opportunity to thank the OSM community. To thank them for making Flickr, the thing that we care about and work on all day, better. To thank them for proving the nay-sayers wrong. To say that OSM started with an audacious plan (to map the entire world by “hand” one neighbourhood and one person at a time) would be an understatement. You would have been forgiven, at the time, for laughing. And yet, in a few short years they are well on their way having nurtured both a community of users and an infrastructure of tools that makes it hard to ever imagine a world without Open Street Maps. In the U.K. alone, as Muki Haklay demonstrated , they have produced a free and open dataset whose coverage and fidelity rivals those created by the Ordinance Survey with its government funding and 250-year head start. That is really exciting both because of the opportunities that such a rich and comprehensive dataset provide but also because it proves what is possible . The Internets are still a pretty great place that way. photo by russelldavies There were too many excellent talks to list them all, but here’s a short (ish) list that betrays some of my interests and biases: Harry Wood’s talk on tagging in OSM . I actually missed this talk and after seeing the slides I am doubly disappointed. Open Street Map is not just the raw geographic  data that people collect but also all the metadata that is used to describe it. OSM uses a simple tagging system for recording “ map features ” and Harry’s talk on managing the chaos, navigating the disputes and juggling the possibilities looked like it was really interesting. (The title of this post is, in fact, a gentle poke at the black sheep of the OSM tagging world. There really are map features tagged “ horse=yes ” which is mostly hilarious until you remember how much has been accomplished with a framework that allows for tags like that.) The Sunday afternoon maps-and-history love-fest that included Frankie Roberto’s “ Mapping History on Open Street Map “, Tim Water’s “ Open Historical Maps ” and the Dutch Nationaal Archief’s presenting “ MapIt 1418 “, a project to allow users to add suggested locations for their photos in the Flickr Commons ! Tim’s been doing work for The New York Public Library , another Flickr Commons member, and MapWarper (the code that powers the NYPL’s historical map rectifier ) is an open source project and available on GitHub . Mikel Maron’s “ Free and Open Palestine ” (the slides are here but you should really watch the video) which is an amazing story of collecting map data in the West Bank and Gaza. Mikel was also instrumental in creating a scholarship program to pay the travel and lodging expenses for 15 members from the OSM community , from all over the world, to attend the conference. Because he’s kind of awesome, that way. But that’s just me. I’d encourage you to spend some time poking around all the other presentations that are available online: Presentations tagged “sotm09” on Slideshare. Videos tagged “sotm09” on Vimeo. Photos tagged “sotm09” on Flickr The complete State of the Map schedule on the OSM website Despite the “bag of tiles” approach for using OSM on Flickr getting a bit old it still works so as of right here, right now: In Vietnam , we’ve added OSM tiles for Ha Noi and Ho Chi Minh City (see also: The State of Vietnam ) . In Cuba , we’ve added OSM tiles for Havana (see also: The State of Cuba ) . In Chile , we’ve added OSM tiles for Santiago (if there was a State of Chile presentation I missed it and haven’t found any slides online so instead I’ll just link to this lovely localized version of OSM for Chilean users ) . We’ve also refreshed the tiles for Beijing and Tehran where, I’m told, the OSM community has added twice as much data since we first started showing (OSM) maps a month ago! If it sometimes seems like we’re doing all of this in a bit of an ad hoc fashion that’s because we (mostly) are. How and when and where are all details we need to work out going forward but, in the meantime, we have map tiles where there were none before so it can’t be all bad. Finally, because the actual decision to attend the conference was so last minute I did not get the memo to all presenters to include a funny picture of SteveC (one of the original founders of Open Street Maps) in their slides. To make up for that omission, I leave you now with the one-and-only Steve Coast . photo by Andy Hume This entry was posted in Uncategorized and tagged community , geo , maps , osm by Aaron Straup Cope . Bookmark the permalink . About Aaron Straup Cope Kay is a Community Manager for Flickr and passionate about extraordinary photography. As an editor on Flickr Blog he loves to showcase the beauty and diversity of Flickr in his posts. When he's not blogging or making Flickr more awesome (in front of and behind the scenes), you can find him taking pictures with his beloved Nikon and iPhone, listening to Hans Zimmer's music or playing board games. | On Flickr you can find him at https://flic.kr/quicksilver View all posts by Aaron Straup Cope →", "date": "2009-07-22,"},
{"website": "Flickr", "title": "5 questions for Matt Biddulph", "author": ["Kellan Elliott-McCrea"], "link": "https://code.flickr.net/2009/07/27/5-questions-for-matt-biddulph/", "abstract": "Matt Biddulph takes lovely photos.  And apparently when he isn’t taking photos he find times to be the founder and CTO of Dopplr .  Which makes us happy, because we’re big Dopplr fans.  If Flickr is the site where photos become social objects, Dopplr is that site for travel.  Except with personal informatics, and positionally dependent rainbows. (we have a well documented weakness for rainbows) Dopplr is consistently one of the most interesting projects tying together the loosely joined datastreams into a cohesive whole, and Matt’s meditations on how to do that are not to be missed. 1. What are you currently building that integrates with Flickr, or a past favorite that you think is cool, neat, popular and worth telling folks about? Or both. Matt: I’m the CTO at Dopplr , and we use Flickr all over our site to integrate our users’ travel pictures and illustrate places in our “Social Atlas”. We’re particularly happy with Flickr’s recent integration of Dopplr places into their automatic machine tag handling This year we’ve been radically upgrading our city information pages (e.g. http://dplr.it/san-francisco ) and Flickr photos have been an important part of that. Designer Matt Jones came up with a combination of rich information visualisation and beautiful images, combining sparklines with photos of cities that we source from Flickr’s geotagged Creative-Commons licensed contributions. The Flickr API is key to how these pages are made. Although ‘interestingness’ is a powerful sorting metric, we found that just asking for the most interesting picture of a city didn’t always give us the right aesthetic (just as Dan Catt’s pandas occasionally show a preference for bikinis ). Instead, our developer Tom Insam built a workflow tool that shows site admins a list of the top pictures per place. It displays a preview of a sample data visualisation overlay, and limits the pictures to those licensed for derivative commercial works. An admin can quickly work through our most popular cities, choosing pictures and a few design settings (such as whether to use black or white text for best contrast). Once a month, a script runs through our chosen images, regenerating this month’s data viz and removing any photos whose license has changed since our last check. This happens more often than we expected. 2. What are the best tricks or tips you’ve learned working with the Flickr API? Matt: To make our city page Flickr integration possible, we had to correlate our city database with Flickr’s geotagging data. Our data is sourced from GeoNames and augmented by us and our users. Luckily Flickr provides a handy reverse-geocoder that maps a latitude/longitude point to an identifier. As a bonus, Flickr now uses the same “WOE IDs” as Yahoo’s Geo APIs , meaning that this correlation between our IDs and theirs can be used to lookup other rich geo data elsewhere on our site. We also started supporting WOE IDs in our own API in return, and we were very happy when Flickr started linking to our city pages from their own place pages. Another tip for integrating Flickr API calls into web pages is to consider whether you can achieve a feature with purely client-side code. Rather than have our webserver tied up waiting for an HTTP request to return from Flickr, in certain places we serve up jQuery code to perform Flickr queries from the clientside, inserting the results into the page via ajax. 3. As a Flickr developer what would you like to see Flickr do more of and why? Matt: I’m in complete agreement with Kellan — that realtime APIs are going to be big . For example, I want the ability to register an interest in something (“the latest CC-licensed photos of London”) and get the data pushed to me whenever new results are ready. This might be implemented using Web Hooks , like when github makes an HTTP POST for you whenever you push into a repository. It might be over XMPP, as used by Fire Eagle , when services register for publish-subscribe notifications of new location pings. The choice of technology is less important than the new possibilities (and design challenges) of the realtime web. 4. What excites you about Flickr and hacking? What do you think you’ll build next or would like someone else to build so you don’t have to? Matt: The best thing about Flickr for hacking is the sheer amount of data that the community collects. The recent release of shapefiles derived from tags and geotagging is very exciting and has huge possibilities for geo mashups. I’m looking forward to firing up a Maps From Scratch instance and figuring out how to combine it with data from our service. There are some screenshots of my early experiments using Flickr’s clustr tool on our Social Atlas data at http://www.flickr.com/photos/mbiddulph/tags/clustr/ 5. Besides your own, what Flickr projects and hacks do you use on a regular basis? Who should we interview next? Matt: I nominate Dave Beckett, author of the C library and commandline tool Flickcurl because I use the commandline tool as a quick interface to Flickr data all the time. As a devotee of dynamic languages, I’m interested to hear about what it’s like to program the web in C.", "date": "2009-07-27,"},
{"website": "Flickr", "title": "“Introducing astrotags”", "author": ["Aaron Straup Cope"], "link": "https://code.flickr.net/2009/09/16/introducing-astrotags/", "abstract": "The Royal Observatory Greenwich has posted an absolutely lovely video about “ astrotags “, writing: “Astrotags are a new way to label your astronomy photos with their celestial subject and its location. This short film, made by Jim Le Fevre and Mike Paterson for the Royal Observatory’s Astronomy Photographer of the Year exhibition , shows you how. So have a watch, then astrotag your pictures at the Astronomy Photographer of the Year group on Flickr . If everyone joins in we can make a beautiful and accurate map of the night sky… so pass the word on.” We’ve written about astrotags before, in a couple of posts titled “ Found in Space ” and “ Tags in Space “, and earlier this year Fiona Romeo , Head of Digital Media at the National Maritime Museum , spoke about the Observatory’s astrotagging project asking the question “what’s the space equivalent of geotagging”? at Webstock09 . Tangentially related, we’ve also updated the wildcard machine tag pages to display related tags based on the current namespace or predicate. For example, if you go to /photos/tags/astro:name= you’ll see these other related tags in the sidebar on the left: Now we just need people to make some astrotagging galleries!", "date": "2009-09-16,"},
{"website": "Flickr", "title": "Flickr is hiring", "author": ["Jude Matsalla"], "link": "https://code.flickr.net/2009/09/18/flickr-is-hiring/", "abstract": "We’re looking for a few talented geeks to join our panda-wrangling, daily-deploying, shard-juggling squad, based in downtown San Francisco. Flickr has openings for PHP and Front-end Engineers, an Engineering Manager, a MySQL DBA, Product Managers and a Designer. If you think that you’ve got the chops, hop over to our jobs page for information on how to apply.", "date": "2009-09-18,"},
{"website": "Flickr", "title": "“That’s maybe a bit too dorky, even for us.”", "author": ["Aaron Straup Cope"], "link": "https://code.flickr.net/2009/09/28/thats-maybe-a-bit-too-dorky-even-for-us/", "abstract": "photo by rabbitier Around the time we added support for Open Plaques machine tags Frankie Roberto, the project lead, asked: “What about supporting Open Street Map (OSM) way machine tags?” My immediate response was something along the lines of “That’s maybe a bit too dorky, even for us.” Which meant that I kept thinking about it. And now we’re doing it. If you’re not sure way what a “way” is, it’s best to start with OpenStreetMap’s own description of how their metadata is structured : Our maps are made up of only a few simple elements, namely nodes, ways and relations. Each element may have an arbitrary number of properties (a.k.a. Tags) which are Key-Value pairs (e.g. highway=primary) … A node is the basic element of the OSM scheme. Nodes consist of latitude and longitude (a single geospacial point) … A way is an ordered interconnection of at least 2 and at most 2000 nodes that describe a linear feature such as a street, or similar. Should you reach the node limit simply split your way and group all ways in a relation if necessary. Nodes can be members of multiple ways. Frankie’s interest is principally in marking up buildings in and around Manchester, where he lives. When he tags one of his photos with osm:way=30089216 we can fetch the metadata (the key-value pairs) for that way using the OSM API and see that it has the following properties: <osm version=\"0.6\" generator=\"OpenStreetMap server\">\n\t<way id=\"30089216\" visible=\"true\" timestamp=\"2009-07-04T12:02:47Z\" version=\"2\" changeset=\"1728727\" user=\"Frankie Roberto\" uid=\"515\">\n\t\t<nd ref=\"331415447\"/>\n\t\t<nd ref=\"331415448\"/>\n\t\t<nd ref=\"331415449\"/>\n\t\t<nd ref=\"331415450\"/>\n\t\t<nd ref=\"331415447\"/>\n\t\t<tag k=\"architect\" v=\"Woodhouse, Corbett & Dean\"/> <tag k=\"building\" v=\"yes\"/> <tag k=\"created_by\" v=\"Potlatch 0.10f\"/> <tag k=\"name\" v=\"St George's House\"/> <tag k=\"old_name\" v=\"YMCA\"/>\n\t\t<tag k=\"start_date\" v=\"1911\"/>\n\t</way>\n</osm> That allows to us “expand” the original machine tag and display a short caption next to the photo, in this case: “St George’s House is a building in OpenStreetMap” with a link back to the web page for that way on the OSM site . photo by Matt Biddulph The technical terms for this process is “ Adding the machine tags extra love “. You may have noticed that there are a bunch of other key-value pairs in that example, like the name of the architect, that we don’t do anything with. Which attributes are we looking for, then? The short answer is: Not most of them. The complete list of map features in OSM is a bit daunting in scope and constantly changing. It would be nice to imagine that we could keep pace with the discussions and the churn but that’s just not going to happen. If nothing else, the translations alone would become unmanageable. Instead we’re going to start small and see where it takes us. Here are the list of tagged features in a way or node definition that we pay attention to, and how they’ll be displayed: k=name v={NAME} … is a feature in OpenStreetMap (If present, with another recognized tag we will display the name for the thing being described in place of the more generic “this is a…”) k=building v=yes … is a building in OpenStreetMap k=historic … is an historic site in OpenStreetMap k=cycleway … is a bicycle path in OpenStreetMap k=motorway (v=cycleway) … is a highway in OpenStreetMap (unless v is “cycleway” in which case it’s a bike path) k=railway v=subway (or tram or monorail or light_rail) … is a subway (or tram or monorail or light_rail) line in OpenStreetMap k=railway v=station … is a train station in OpenStreetMap; if the type of railway is also defined (above) then we’ll be specific about the type of station. I should mention that as of this writing we’re still waiting for the translations for “this is a train station” to come back because I, uh… anyway, real soon now. k=waterway v=stream (or canal or river) … this is a stream (or canal or river) in OpenStreetMap k=landuse v=farm (or forest) … this is a farm (or forest) in OpenStreetMap k=natural v=forest (or beach) … this is a forest (or beach) in OpenStreetMap Which means: We’ve almost certainly got at least some of it wrong. Anyone familiar with OSM features will probably be wondering why we haven’t included amentiy or shop tags since they contain a wealth of useful information. I hope we will, but it wasn’t clear how we should decide which features to support (more importantly, which to exclude) and the number of possible combinations were starting to get a bit out of hand and we have this little photo-sharing site to keep running. This is the part where I casually mention that we’ve also added machine tags extra love for Four Square venues IDs . I’m just saying… The features we’re starting with may seem a bit odd, with a heavy focus on natural land features (and train stations). Some of this is a by-product of the work we’ve been pursuing with the alpha shapes and “donut holes” , derived from geotagged photos, and some of it is just trying to shine the spotlight on places and environments that we take for granted. Like I said, we’ve almost certainly got at least some of it wrong but hopefully we got part of it right and can correct the rest as we go. This one is definitely a bit more of an experiment than some of the others. photo by artistofmimicry Finally, in the tangentially related department we finished wiring up the RSS/syndication feeds to work properly with wildcard machine tags. That means you can subscribe to a feed of all the (public) photos tagged with osm:way= or osm:node= or, if you’re like me, all the photos of places to eat in Dopplr with dopplr:eat= . Enjoy!", "date": "2009-09-28,"},
{"website": "Flickr", "title": "Small Bridges (to Proximate Spaces)", "author": ["Aaron Straup Cope"], "link": "https://code.flickr.net/2009/10/19/small-bridges-to-proximate-spaces/", "abstract": "photo by jordi ventura A couple months ago Tom Taylor and Tom Armitage launched a web-based game based around geotagged Flickr photos called noticin.gs . Noticings is a game about learning to look at the world around you. Cities are wonderful places, and everybody finds different things in them. Some of us like to take pictures of interesting, unusual, or beautiful things we see, but many of us are moving so fast through the urban landscape we don’t take in the things around us. Noticings is a game you play by going a bit slower, and having a look around you. It doesn’t require you change your behaviour significantly, or interrupt your routine: you just take photographs of things that you think are interesting, or things you see. You’ll get points for just noticing things, and you might get bonuses for interesting coincidences. Maybe it’s just me (I don’t think so) but this is precisely the sort of thing we always hoped people would build on top of the Flickr API . You “play” noticin.gs by uploading geotagged photos to Flickr and tagging them noticings . At the end of each day the noticin.gs robots query the Flickr API for new photos and assign points to each photo. Points are awarded for being the first noticing in a neighbourhood, because a player noticed something every day at lunchtime and so on ; the scorings change and adapt with the game itself . photo by Ben Terrett Tom Taylor and I started talking about adding the “ machine tags extras love ” (remember, that is actually now a technical term on the Flickr engineering team) a while back. One idea was use the photo’s (noticin.gs) score as the key back in to their world but that seemed like an odd fit. Knowing the score for a photo doesn’t tell us how those points were awarded which is, really, the interesting part and what would we link to? I’ll come back to the what-do-we-link-to part again later. As it happens, every single noticing has its own web page and a unique ID that, conveniently, is the same as the photo that was noticed so we settled on noticings:id=PHOTO_ID as the tag that will be “expanded”. If it’s present we’ll ask the noticin.gs servers for the list of reasons that photo was awarded points and display the one with the highest score linking back to the noticin.gs page for that ID. You can either add the special machine tag yourself or ask noticin.gs to do it for you automatically. To enable automagic machine tagging you’ll need to log in to noticin.gs and change the default settings. If you’re worried about creating yet another account for an yet another online service, don’t be: noticin.gs uses the Flickr Auth API itself to manage user accounts so “logging in” is as simple as authorizing noticin.gs to access your Flickr account (the way you would any other Flickr API application). This is what it looks like once you’ve logged in: Paul Mison has a lovely post about noticin.gs where he describes the game as “ the biggest change to the way I post photos ” to Flickr. That kind of thing makes all the bad days worth it. Don’t forget: You can subscribe to an RSS feed of all the new photos machine tagged with noticings:id= and since the photos should all be geotagged already you can also create a network link for new photos and hop around from noticing to noticing in Google Earth. Which is as good a segue as any to show a picture of a space ship. A “space ship.” I like this picture because it reminds me of machine tags. photo by mattcottam Which is as good a segue as any to talk about trains. But not just trains. Machine tags for trains. Actually, train stations. We have a lot of pictures of train and subway stations. A casual search for the words subway OR metro alone yields 1.5 million results. If you add the word train to the list you get 5.7 million. That’s just searching for stuff in English. Unfortunately, few transit systems have websites with pages dedicated to each station in their network (we’ll cut them some slack as they’re busy, you know, running the trains). A few do but none of them seem to have much in the way of a public API, even something as simple as a getInfo method. So, a couple weekends ago I created Fake Subway APIs , which is a plain-vanilla XML-over-HTTP API service for returning information about train and subway stations. It doesn’t do much right now except return the name and URI for a station given its short code. I did this because I wanted to make sure that the code we run to determine the “meaning” of a given machine tag always expects to be asking someone else for the answer. Even if a fake subway API is little more than a canned list of IDs and names it seemed important to go through the motions of treating machine tag extras as something external to Flickr. My hope is that Fake Subways APIs will become irrelevant sooner rather than later, as individual services start building this stuff themselves. For now, though, it works and it means we can enable the “ machine tags extras love ” for four transit systems: BART in the San Francisco Bay Area, the metro (STM) in Montréal; Transport for London (aka the “Tube”) in, well, London; the National Rail Service in the UK. The syntax is the same for all of them: service name + \":station=\" + station code Like this: For BART : bart:station=16th A complete list of BART station codes is available over here . For the STM (aka the “metro”) : stm:station=m48 A complete list of STM station codes is available over here . For TFL (aka the “Tube”) : tfl:station=LON-N TFL machine tags are a bit of a bear compared to the others. Specifically, you need to indicate both the station code and the line code. This is a consequence of the way the TFL website is set up. A complete list of TFL station codes is available over here . For the UK National Rail System : ukrail:station=HMN A complete list of National Rail station codes is available over here . We chose those four because they were the ones which I knew to have a webpage for each station that could be linked to (or in the case of London Underground could be teased out because, let’s be honest, it’s the Tube ) at the end of a machine tag. Since all of this has started, hooks for the MBTA in Boston and the TTC in Toronto have been added to Fake Subway APIs so it seems reasonable to expect that we’ll add support for them here too. Check it out, a train: If your subway system isn’t listed please don’t take it personally. I work on this in the mornings over coffee and weekends when I’m sick and should be resting. The entire project is open source and I’d welcome contributions. Munich , for example… One glaring omission is the New York City subway system, sometimes known as the Metropolitain Transportation Authority (MTA) , because they don’t have proper webpages for the stations they operate. Fake Subway APIs provides a (fake) MTA API and even has fake/place-holder subway pages for each of the stations but where’s the fun in that? One of the goals with the machine tags project has been to deliberately link outwards to the various sites, and services, rather than funnel everything through a single channel. A “small bridges” approach instead of an all roads lead to [INSERT BIG SITE HERE] model, so to speak. photo by antimega (Speaking of tubes…) We’ve certainly had discussions around the idea of using Wikipedia as a sort of universal content resolution system, for things or people otherwise “missing” from the Interwebs. Tim Bray wrote a really good piece about this called “ On Linking ” a couple years ago. It’s not that we don’t love Wikipedia or support what they’re doing and they almost certainly have the most comprehensive list of trains stations anywhere on the Internet. It’s just that we’d like to actively encourage as many people as possible to participate in what Tom Coates’ called the “ Web of Data “, in a presentation in 2006, making their data available to to both humans and machines but also maintaining authorship of all that crunchy goodness. Tom’s slides are a bit opaque on their own and to date the best telling of the presentation has been Simon Willison and company’s collaborative note-taking , which I’ve liberally excerpted here: Every new service that you create can potentially build on top of every other existing service. Every service and piece of data that’s added to the web makes every other service potentially more powerful. So the same things that keep the hippies happy keep the evil capitalists happy. They all have to play in the same ecosystem. If not, you end up in a backwater, disconnected from the cool stuff that’s happening. strength in sharing and participating. So far, it’s an idea that worked pretty well for us if all the amazing stuff people have built on to top of the API is any measure. If you look closely you’ll notice that I’ve had to link to an ( Internet Archive ) archived version of Simon’s site from 2006 since the notes are nowhere else to be found. There is still, obviously, lots of work left to be done no matter which road you prefer. Also: While we’re talking about Wikipedia,  Josh Clark’s Wikipedia Machine Tag Generator , which he built during the Yahoo! BBC Hack Day event in 2007, is just plain awesome. So, where are we going with all of this? It’s a bit too soon to tell but one of the things I like about all of the recent machine tag work is that they start to expose geographies outside of the traditional grid of latitudes and longitudes. If that sounds a bit wooly and hand-wavey that’s because it is. In concrete terms, one thing that’s pretty exciting is the ability to infer location for all those photos that aren’t geotagged yet but do have Upcoming , or foursquare , or OpenStreetMap machine tags or, yes, even train stations . All those services have their own APIs and aside from just pulling back coordinates you can use them to fill in simple, but important, details like whether a photo was taken indoors or outdoors . And if we’re lucky they’ll start to show us the donut holes and the “ place fields ” ( props to Matt Jones’ delicious links for that one ) that we walk through every day but don’t recognize or don’t have names for yet. photo by JLB", "date": "2009-10-19,"},
{"website": "Flickr", "title": "Introducing The App Garden", "author": ["Mike Panchenko"], "link": "https://code.flickr.net/2009/11/03/introducing-the-app-garden/", "abstract": "Flickr has long had an extensive, well-documented API. Over the years, thousands of developers have taken advantage of it, coming up with some awesome apps. We love that. We love it so much, we’ve revamped the /services/ section of Flickr, replacing it with The App Garden. What is it, you say? It’s a place for developers to promote their apps, right here on Flickr. We hope that it will make it easier for Flickr users to find the awesome apps that the Flickr API hackers have been building. You will see that The App Garden already has some apps in it, and you might think “OOOH SHINY!!” You might also wonder how to get your app into the App Garden. I will show you! Getting Started We’ve tried to make things as simple and straight-forward as possible. You will find all of your API Keys under the Apps By Me page, which replaces the old API Key list. You will notice that they are all labeled as “Private” – we leave it up to you to decide when your app page is ready to be made public. When you click on one of your apps, you will be taken to the owner view of your app page. This page is where you tell the world about your app – provide a description, link to a website, set screenshots, and add tags. When you’re ready, change the privacy setting to public. That will make your app visible to other users and allow it to show up in searches . Managing Your Apps Below the privacy settings, you will find the Admin section of the sidebar – your own little command center. You will find a link to a page with statistics for the app’s API Key (largely unchanged, though developers with higher user counts may notice a considerate speed up), as well as pages for disabling the key, editing the authentication flow for the key, and deleting the app altogether. We love our API hackers and are happy to embrace them in a whole new way. We hope you like it. More Info App Garden FAQ What is the App Garden?", "date": "2009-11-3,"},
{"website": "Flickr", "title": "In case you wanted to bake us a cake ….", "author": ["Kellan Elliott-McCrea"], "link": "https://code.flickr.net/2009/11/13/in-case-you-wanted-to-bake-us-a-cake/", "abstract": "Ian Sanchez has proposed a new geo data “test for freeness” in the spirit of the Debian project’s tests for free software. A set of geodata, or a map, is libre only if somebody can give you a cake with that map on top, as a present. – Ian Sanchez I mention this because the Flickr Shapefiles can be used unencumbered as a cake decoration.  And we like cake.  We like photos of cake as well.  But we prefer cake.", "date": "2009-11-13,"},
{"website": "Flickr", "title": "Building authorized Flickr apps for the iPhone", "author": ["Jerome Decq"], "link": "https://code.flickr.net/2009/11/19/building-authorized-flickr-apps-for-the-iphone/", "abstract": "You want to develop an iPhone app that interacts with Flickr content? This sounds pretty good. And the Flickr API provides you with an authorization workflow that is particularly adapted for this device.   And Flickr members love their iPhones . An authorization workflow you say, but why? Let me step back for a moment. First you may not need to authorize your application. This is needed only if your app needs to make authenticated API calls. That means if the users of your app access in a non anonymous way the Flickrverse: accessing non-public content, commenting, tagging, deleting, etc… In order for this to happen in a safe environment (for you and our users), a 3-way authorization needs to happen between Flickr, you and our mutual users, typical of social engineering interactions. So how does this work? Step 0 You need to setup your application. Get an API key that uniquely identifies your application. Configure your application to be web-based (yes, this can seem odd but this is the smoothest user experience on the iPhone) and specify the authorization callback URL (it will be used in Step 3.) I suggest not to use the http:// protocol reserved for the web but your own, like myapp:// There exists already a workflow explanation from developer stand point , but I’d like to add the specificities introduced by the use of a web-based authorization in an iPhone environment. In the app, for each user you want to authorize: Step 1 Create a login URL, specific to you application in the shape of http://flickr.com/services/auth/?api_key=%5Bapi_key%5D&perms=%5Bperms%5D&api_sig=%5Bapi_sig%5D and launch a web browser with this URL. Step 2 Flickr will ask the user to sign-in into their account and present them with a page prompting them to authorize your application. Step 3 If the user decides to authorize your application, Flickr will call the callback URL specified in Step 0. Here is the nice trick! This can actually launch back your application. All you have to do is to add a new entry for CFBundleURLTypes in your Info.plist: <key>CFBundleURLTypes</key>\n <array>\n   <dict>\n     <key>CFBundleURLName</key>\n     <string>MyApp's URL</string>\n     <key>CFBundleURLSchemes</key>\n     <array>\n       <string>myapp</string>\n     </array>\n   </dict>\n </array> See Apple’s documentation on Implementing Custom URL Schemes for more details. Step 4 Your application is launched back by Flickr (through the browser and the iPhone OS) with a frob as one of the argument of the URL. The app is effectively a Flickr auth handler. You can implement application:handleOpenURL: in a similar way as: - (BOOL)application:(UIApplication *)application handleOpenURL:(NSURL *)url\n {\n        if (NSOrderedSame == [[url scheme] caseInsensitiveCompare:@”flickrApp”]) {\n                // query has the form of \"&frob=\", the rest is the frob\n                NSString *frob = [[url query] substringFromIndex:6];\n\n                // Keep the frob for Step 5 and schedule Step 5\n\n               return YES;\n        } else {\n               Return NO;\n        }       \n  } Step 5 Your app makes an API call to convert this frob into a token . The frob is valid only for a certain time. The token will be used for the API calls that require authentication. This token is what uniquely identifies the use of your API key for a specific user on Flickr. You can save it using NSUserDefaults for next time the user uses the application without having to reauthorize the application. Even better to use KeyChain. Note that you should use checkToken to make sure the user has not de-authorized the application otherwise your authenticated call may fail for no apparent reasons. I would like to take the opportunity of this blog post to recommend an excellent library to develop iPhone apps interacting with flickr: ObjectiveFlickr . Have a good hack! Jérôme Decq , from his home outside of Paris, singled handedly runs the Flickr Desktop Uploadr development , as well as hacking on making Flickr avaiable on a wide range of platforms, and photographing purple ducks .", "date": "2009-11-19,"},
{"website": "Flickr", "title": "On The Importance of Fun (And Some Holiday Snow)", "author": ["Scott Schiller"], "link": "https://code.flickr.net/2009/11/26/on-the-importance-of-fun-and-some-holiday-snow/", "abstract": "Since we’re based in (mostly-) sunny San Francisco, Flickr’s code monkeys will be away for a few days eating excessive amounts of turkey and being appropriately thankful. Before heading off with friends and family, I thought I’d share a bit of “seasonal” JavaScript we have on the site, and a few notes about its inner workings. Fun is Good We build a lot of “serious” stuff at Flickr HQ, but we also recognize why it’s crucial that we save some time for the small things, the goofy and occasionally-irreverent parts of the site that remind people of a playful spirit. Outside of our daily work, many of us have our own personal geekery going on. It’s no coincidence that shiny, and even silly things people tinker with and build in their own time sometimes bleed over and become features or elements on Flickr; ideally, great ideas come from all sorts of places. Let It Snow Lights in the Snow (with ?snow=1), by mezzoblue Last holiday season, we got around to making it snow on photo pages – just for the fun of it. If you happen to be on a photo.gne page and add ?snow=1, it might still even work.. We’ve also used a variant of the effect in the past for beta feature sign-up sequences , complete with cheesy MIDI renderings of “The Girl From Ipanema.” Again, this was “just because”; sometimes the web is most entertaining when it’s most unexpected. If you can occasionally make your users smile, giggle and laugh when using your site, chances are you’re doing something right – or, you’re running a comedy site that’s going downhill. Web Hold Muzak by .Schill Wait, Snow is Hard The snow effect was a JavaScript experiment made strictly as a test of DOM-2 event handlers, PNG and animation performance. Or, viewed in a more-cynical light, it was an evil plot to pollute websites with falling animated .GIFs and light CPUs on fire world web-wide. To achieve a realistic effect, each snowflake has its own size, “weight” and x/y velocity properties which can be affected by “wind”. A slight parallax effect is achieved by the notion that larger flakes are closer to the screen and move faster, and so on. It may not be surprising to note that it’s still expensive CPU-wise to animate many individual elements simultaneously. The effect is much smoother and has higher frame-rates these days when compared to early versions from 2003, but even modern browsers will readily eat up 50% CPU while running the effect. That said, the animation is keyed off of a single setInterval()-style loop with a low interval, so it will effectively try to run as fast as it can. Revisiting Performance “In theory there is no difference between theory and practice. But, in practice, there is.” — Yogi Berra I had a theory that using text elements might be more efficient than image-based snow to animate, so I made the switch to using elements with the bullet entity • instead of PNGs with alpha transparency. No noticeable improvement was actually seen, despite my theories about drawing and moving images; HTTP requests were being saved, but the browser was still doing a lot of redraw or reflow work despite the elements using either absolute or fixed positioning. On my netbook-esque laptop with Firefox 3.5 under WinXP, animation would “freeze” when scrolling the window with position:fixed elements – presumably because my single interval-based JavaScript timer was blocked from running while scrolling was active. In retrospect, what might be more efficient for overall CPU use is a number of absolutely-positioned “sheets” using a tiling background image with a pattern of snow. Each sheet of snow would move at the same speed and angle, but the number of unique elements being animated would be drastically reduced. JavaScript-based animation is not a science in any case, and having a large number of elements actively moving can significantly impact browser responsiveness. While this is not a common web use case for animation, it is interesting to note how the different browsers handle it. Make Fun Stuff, and Learn! Much of what I enjoy about front-end engineering is the challenge. I’ve learned a lot about browser performance and quirks just from prototypes and experiments, which can then be used in real production work. While cross-browser layout and performance varies, it’s also rewarding to work on the occasional crazy idea or silly prototype and then refine it, features and quality-wise, to a point where it’s ready for a production site somewhere. JavaScript-based snow is at best a “perennial” feature, but the process of thinking about how to make something different and new work, and work well (theory) – and then researching, testing and hacking away to actually do it (practice) – is where the learning comes in.", "date": "2009-11-26,"},
{"website": "Flickr", "title": "Flipping Out", "author": ["Ross Harmes"], "link": "https://code.flickr.net/2009/12/02/flipping-out/", "abstract": "Flickr is somewhat unique in that it uses a code repository with no branches; everything is checked into head, and head is pushed to production several times a day. This works well for bug fixes that we want to go out immediately, but presents a problem when we’re working on a new feature that takes several months to complete. How do we solve that problem? With flags and flippers! Feature Flags Flags allow us to restrict features to certain environments, while still using the same code base on all servers. In our code, we’ll write something like this: if ($cfg.enable_unicorn_polo) {\n    // do something new and amazing here.\n}\nelse {\n    // do the current boring stuff.\n} We can then set the value of $cfg.enable_unicorn_polo on each environment ( false for production, true for dev, etc.). This has the additional benefit of making new feature launches extremely easy: Instead of having to copy a bunch of new code over to the production servers, we simply change a single false to true , which enables the code that has already been on the servers for months. Feature Flippers Flags allows us to enable features on a per environment basis, but what if we wanted to be more granular and enable features on a per user basis? For that we use feature flippers. These allow us to turn on features that we are actively developing without being affected by the changes other developers are making. It also lets us turn individual features on and off for testing. Here is what the Feature Flip page looks like: Continuously Integrating Feature flags and flippers mean we don’t have to do merges, and that all code (no matter how far it is from being released) is integrated as soon as it is committed. Deploys become smaller and more frequent; this leads to bugs that are easier to fix, since we can catch them earlier and the amount of changed code is minimized. This style of development isn’t all rainbows and sunshine. We have to restrict it to the development team because occasionally things go horribly wrong; it’s easy to imagine code that’s in development going awry and corrupting all your data. Also, after launching a feature, we have to go back in the code base and remove the old version (maintaining separate versions of all features on Flickr would be a nightmare). But overall, we find it helps us develop new features faster and with fewer bugs.", "date": "2009-12-2,"},
{"website": "Flickr", "title": "Language Detection: A Witch’s Brew?", "author": ["Simon Batistoni"], "link": "https://code.flickr.net/2009/12/04/language-detection-a-witchs-brew/", "abstract": "photo by Arbron Software developers sometimes have a tendency to cling to bits of ancient lore, even after they’ve long-since become irrelevant. Globalization, and specifically language detection, raises an interesting case-in-point. These days, language detection is really simple in most cases – just use the “ Accept-Language ” HTTP header, which pretty much every client on the planet passes to you. “But wait!” I hear you cry. “I thought the Accept-Language header wasn’t to be trusted?” “Ancient and outdated lore, my friend” I will reply. “Ancient and outdated lore.” It’s true that the Accept-Language header has a troubled history. Because of this, many developers regard it the way medieval villagers might have regarded a woman with a warty nose and a pet cat – it should be shunned, avoided and possibly burned at the stake. In the early days of the web, Accept-Language really couldn’t be trusted – many browsers made no real effort to send a meaningful value, often sending nothing (or worse, defaulting to requesting English) unless the user worked their way through a complicated configuration process buried 3 panes back in a labyrinthine dialog box. I’m rarely comfortable being categorical when talking about software engineering – there are usually too many variables, and too many specific circumstances for blanket assertions to be of use. But I can state with certainty that the “Accept-Language” header works these days, and any case where it doesn’t can (and will) be considered an error on the part of the browser vendor. In two and a half years of running as an international site, we’ve only ever had one case where it didn’t work. Helio, a cellphone company, had a browser which was custom-built for them in Korea, and had its “Accept-Language” header hard-coded to always request Korean, something which led to much confusion for the Flickr users amongst their American customers. When we alerted Helio to the problem, however, they were highly responsive – the next release of their software returned the correct value. Location != Language Perhaps driven by their superstitious fear of Accept-Language, many developers fall back on other means of determining their visitors’ preferred language. Top of the list is IP-detection – pinpointing the visitor’s current location using a lookup database. This isn’t necessarily a terrible solution, but it’s prone to a couple of problems. Firstly, even the best IP location databases contain mistakes, or outdated information where netblocks have been reassigned over time. Secondly, if you only rely on IP detection in order to provide language, travelers will often be highly confused when they reach their destination, connect to your site and are greeted with a language they don’t speak. This can be especially disconcerting if you’ve been using a site regularly for years. This error is easily avoided, simply by applying a cookie the first time you pick a language for a user. By using a cookie, you can guarantee a consistent experience, regardless of where in the world a laptop happens to fly. None of this is to say that IP detection doesn’t have its place. It’s a useful fallback if you come across a user-agent that doesn’t send Accept-Language, or you have a specific case where you believe you can’t trust the header. But it general it should be just that – a fallback. Always an option… To guard against any possibility that you’ll detect the wrong language for a user, it doesn’t hurt to always provide language-switching as an option on every page. You’ll see that we do that on Flickr (with the exception of Organizr)… You’ll notice that we always render the language in its native name, so that people can find their preferred language, even if they understand nothing else on the page. Rights and Wrongs “But!” some of you will cry. “We are a site that deals with media content. There are rights issues and legal constraints – we have to send people in France to our French site”… …which nicely brings me to the last point of this post. Many global sites will find themselves dealing with various jurisdictional concerns. There’s no reason, however, that the “legal” logic in your code needs to be tied to the interface presentation. Whilst certain legal texts (Terms and Conditions, etc) can provide interesting challenges, if you keep presentation as an entirely separate consideration from jurisdiction, it’s much easier to provide everyone with the best possible experience, regardless of where they are and what language they speak. In Summary All the above can be reduced to a very simple set of points to cut-out-and-keep for the next time you’re asked to create a Global version of a website. Use Accept-Language – it just works Use IP detection as a fallback for language, or a separate test to determine jurisdiction Cookie your visitors’ language preferences, so you are consistent Provide a simple way to switch language, everywhere Treat language and compliance issues as two entirely separate problems …at least, that’s how we’ve been doing it on Flickr for two and a half years, and it’s worked out pretty well for us so far.", "date": "2009-12-4,"},
{"website": "Flickr", "title": "A Chinese puzzle: Unicode and EXIF metadata parsing", "author": ["Jerome Decq"], "link": "https://code.flickr.net/2010/01/08/a-chinese-puzzle-unicode-and-exif-metadata-parsing/", "abstract": "At flickr, there are quite a few photos and you can browse the site in 8 different languages, including Korean and Chinese. Common metadata such as title, description and tags can be pre-populated based on information contained in the image itself, using what is commonly called EXIF information. So, it makes sense to implement this with respect to language and, above all, alphabet/character encodings. Well, what made sense did not make so much sense anymore when using existing specifications. Here is how we coped with them. The standards To begin with, there is not just EXIF. Metadata can actually be written within a picture file in at least 3 different formats: – EXIF itself. – IPTC-IIM headers. – XMP by Adobe. Of course, these formats are neither mutually exclusive nor completely redundant and this is where this can get tricky. But let’s step back a moment to describe the specifics of these formats, not in details, but with regards to our need, which is to extract information in a reliable way, independently from how/when/where the image was created. The EXIF is the oldest form Hence, with the most limitations yet the most widespread, as this is often the case in our industry. Thus, we need to deal with it, even though it is radically flawed from an internationalization stand point: text fields are not stored using UTF and most of the time there is no indication of the character set encoding. IPTC-IIM In its later versions, it added the optional (Grr!!!) support for a field indicating the encoding of all the string properties in the container. XMP At last, text from XMP format is stored in UTF-8. On a side note, the XML based openness of this format is not making things easier, for each application can come up with its own set of metadata. Nevertheless, from an internationalization and structural standpoint, this format is modernly adequate: finally! So, now that we know what hides behind each of these standards we can start tackling our problem. A solution ? Rely on existing libraries, when performant For flickr Desktop client ( Windows , Mac ), we are using Exiv2 Image metadata library, which helps the initial reconciliation between all fields (especially with EXIF and IPTC-IIM contained within XMP). The “guesstimation” of character set from EXIF We first scan the string to see if all bytes are in the range: 0 to 127. If so, we treat the string as ASCII . If not, we scan the string to see if it is consistent with valid UTF-8. If so, we treat the string as UTF-8. Checking against UTF8 validity is not a bullet-proof test. But statistically, this is better than any other scenario. At the last resort, we pick a “reasonable” fallback encoding. For the desktop application, we use hints from the user system. On windows, the GetLocaleInfo function provides with the user default ANSI code page ( LOCALE_IDEFAULTANSICODEPAGE ), which can be used to specify an appropriate locale for string conversion methods. On Mac OS X, CFStringGetSystemEncoding is our ally. In our case, there is no point to use the characterset of our own application, which we control and is not linked to the characterset of the application that “wrote” the EXIF. Consolidation: the easy case The workflow followed by the image we handle is unknown. We can have all 3 formats filled-in but not necessarily by the same application. So, we need a mechanism to consolidate the data. The easy case is for single field such as title and description. We follow the obvious quality hierarchy of the different formats. To extract the description for instance, we first look for the XMP.dc.description field, then XMP.photoshop.Headline (to support the extensibility mentioned before as a side note), then IPTC.Application2.Caption and finally the Exif.Image.ImageDescription. We only keep the first data found and ignore the others. There is only one title and one description per image: might as well take the one we are sure about. Consolidation: it becomes even trickier for tags. The singleness of the final result disappearing (we deal with a list of tags, not just one single tag), we cannot ignore the “EXIF” tags as easily as for the title and the description case. Fortunately, IPTC Keywords are supposed to be mapped to XMP (dc:subject). Therefore we can take into account the number of keywords that would be extracted from EXIF and the number that would be extracted from XMP. If those equal, we plainly ignore the EXIF. If they don’t, we try to match each guestimation of EXIF keyword against the XMP keywords to avoid duplicates . All in one, quite an interesting issue where, per design, the final solution is going to be an approximation with different results depending on the context . Computers: an exact science? For more information and main reference: http://www.metadataworkinggroup.com/ Photos by Groume , Raïssa Bandou , seanmcgrath , tcp909 , aftab. , 姒儿喵喵 and Laughing Squid", "date": "2010-01-8,"},
{"website": "Flickr", "title": "People in Photos: The API Methods", "author": ["Simon Batistoni"], "link": "https://code.flickr.net/2010/01/21/people-in-photos-the-api-methods/", "abstract": "I’ve been racking my brains for a while trying to think of something informative to tell you about the API methods we’re releasing today, but all I keep coming back to is a venerable British advertising slogan for a brand of woodstain: “ It does exactly what it says on the tin “. What does the tin say, then? photo by 5500 First off, we have a simple accessor method which will return you a list of people for a given photo. It’s called flickr.photos.people.getList , and it takes a photo ID as its sole argument. But what about the reverse – finding all the photos of a given person? Fret not, that’s why we have flickr.people.getPhotosOf . This method takes a user ID, and since it returns a Standard Photos Response , you can also specify any extra data you want through the extras parameter. Sometimes, simply consuming data isn’t enough – you will feel a need to create some. If you want to add a person to a photo, simply use flickr.photos.people.add . This method takes a photo_id and a user_id, and can optionally take another 4 arguments ( person_x , person_y , person_w , person_h ) to specify a “face boundary” for that person. If you decide you don’t like the face boundary later, there’s always flickr.photos.people.deleteCoords to remove it entirely, or flickr.photos.people.editCoords to update. Last, but not least, you can remove someone from a photo with flickr.photos.people.delete . Obviously, all of the above methods require that the calling user is permitted to perform the action in question. Why so late? We’re aware that some people have been clamoring for access to these methods for a little while now. And while I’m not generally one for making excuses, I do have a pretty good reason for the delay this time – I broke my leg snowboarding a month ago. What’s more, I have evidence, in the form of a photo which I marked as containing me, using the Flickr API… So, yeah, sorry about that.", "date": "2010-01-21,"},
{"website": "Flickr", "title": "Using, Abusing and Scaling MySQL at Flickr", "author": ["Kellan Elliott-McCrea"], "link": "https://code.flickr.net/2010/02/08/using-abusing-and-scaling-mysql-at-flickr/", "abstract": "I like “NoSQL” .  But at Flickr, MySQL is our hammer, and we use it for nearly everything.  It’s our federated data store, our key-value store, and our document store.  We’ve built an event queue, and a job server on top of it, a stats feature, and a data warehouse. We’ve spent the last several years abusing, twisting, and generally mis-using MySQL in ways that could only be called “post relational”.  Our founding architect is famously in print saying , “Normalization is for sissies.” So while it’s great to see folks going back to basics — instead of assuming a complex and historically dictated series of interfaces, assuming just disks, RAM, data, and problem to solve — I think it’s also worth looking a bit harder at what you can do with MySQL.  Because frankly MySQL brings some difficult to beat advantages. it is a very well known component.  When you’re scaling a complex app everything that can go wrong, will.  Anything which cuts down on your debugging time is gold.  All of MySQL’s flags and stats can be a bit overwhelming at times, but they’ve accumulated over time to solve real problems. it’s pretty darn fast and stable.  Speed is usually one of the key appeals of the new NoSQL architectures, but MySQL isn’t exactly slow (if you’re doing it right).  I’ve seen two large, commercial “NoSQL” services flounder, stall and eventually get rewritten on top of MySQL. (and you’ve used services backed by both of them) Over the next bit I’ll be writing a series of blog posts looking into how Flickr scales MySQL to do all sorts of things it really wasn’t intended for.  I can’t promise you these are the best techniques, they are merely our techniques, there are others, but these are ours.  They’re in production, and they work.  I was tempted to call the series “YesSQL”, but that really doesn’t capture the spirit, so instead I’m calling it “Using and Abusing MySQL”. And the first article is on ticket servers .", "date": "2010-02-8,"},
{"website": "Flickr", "title": "Ticket Servers: Distributed Unique Primary Keys on the Cheap", "author": ["Kellan Elliott-McCrea"], "link": "https://code.flickr.net/2010/02/08/ticket-servers-distributed-unique-primary-keys-on-the-cheap/", "abstract": "This is the first post in the Using, Abusing and Scaling MySQL at Flickr series. Ticket servers aren’t inherently interesting, but they’re an important building block at Flickr.  They are core to topics we’ll be talking about later, like sharding and master-master.  Ticket servers give us globally (Flickr-wide) unique integers to serve as primary keys in our distributed setup. Why? Sharding (aka data partioning ) is how we scale Flickr’s datastore.  Instead of storing all our data on one really big database, we have lots of databases, each with some of the data, and spread the load between them. Sometimes we need to migrate data between databases, so we need our primary keys to be globally unique.  Additionally our MySQL shards are built as master-master replicant pairs for resiliency. This means we need to be able to guarantee uniqueness within a shard in order to avoid key collisions. We’d love to go on using MySQL auto-incrementing columns for primary keys like everyone else, but MySQL can’t guarantee uniqueness across physical and logical databases. GUIDs? Given the need for globally unique ids the obvious question is, why not use GUIDs?  Mostly because GUIDs are big, and they index badly in MySQL.  One of the ways we keep MySQL fast is we index everything we want to query on, and we only query on indexes.  So index size is a key consideration. If you can’t keep your indexes in memory, you can’t keep your database fast.  Additionally ticket servers give us sequentiality which has some really nice properties including making reporting and debugging more straightforward, and enabling some caching hacks. Consistent Hashing? Some projects like Amazon’s Dynamo provide a consistent hashing ring on top of the datastore to handle the GUID/sharding issue.  This is better suited for write-cheap environments (e.g. LSMTs ), while MySQL is optimized for fast random reads. Centralizing Auto-Increments If we can’t make MySQL auto-increments work across multiple databases, what if we just used one database?  If we inserted a new row into this one database every time someone uploaded a photo we could then just use the auto-incrementing ID from that table as the primary key for all of our databases. Of course at 60+ photos a second that table is going to get pretty big.  We can get rid of all the extra data about the photo, and just have the ID in the centralized database.  Even then the table gets unmanageably big quickly. And there are comments, and favorites, and group postings, and tags, and so on, and those all need IDs too. REPLACE INTO A little over a decade ago MySQL shipped with a non-standard extension to the ANSI SQL spec, “REPLACE INTO” .  Later “INSERT ON DUPLICATE KEY UPDATE” came along and solved the original problem much better.  However REPLACE INTO is still supported. REPLACE works exactly like INSERT, except that if an old row in the table has the same value as a new row for a PRIMARY KEY or a UNIQUE index, the old row is deleted before the new row is inserted. This allows us to atomically update in a place a single row in a database, and get a new auto-incremented primary ID. Putting It All Together A Flickr ticket server is a dedicated database server, with a single database on it, and in that database there are tables like Tickets32 for 32-bit IDs, and Tickets64 for 64-bit IDs. The Tickets64 schema looks like: CREATE TABLE `Tickets64` (\n  `id` bigint(20) unsigned NOT NULL auto_increment,\n  `stub` char(1) NOT NULL default '',\n  PRIMARY KEY  (`id`),\n  UNIQUE KEY `stub` (`stub`)\n) ENGINE=InnoDB SELECT * from Tickets64 returns a single row that looks something like: +-------------------+------+\n| id                | stub |\n+-------------------+------+\n| 72157623227190423 |    a |\n+-------------------+------+ When I need a new globally unique 64-bit ID I issue the following SQL: REPLACE INTO Tickets64 (stub) VALUES ('a');\nSELECT LAST_INSERT_ID(); SPOFs You really really don’t know want provisioning your IDs to be a single point of failure.  We achieve “high availability” by running two ticket servers.  At this write/update volume replicating between the boxes would be problematic, and locking would kill the performance of the site.  We divide responsibility between the two boxes by dividing the ID space down the middle, evens and odds, using: TicketServer1:\nauto-increment-increment = 2\nauto-increment-offset = 1\n\nTicketServer2:\nauto-increment-increment = 2\nauto-increment-offset = 2 We round robin between the two servers to load balance and deal with down time. The sides do drift a bit out of sync, I think we have a few hundred thousand more odd number objects then evenly numbered objects at the moment, but this hurts no one. More Sequences We actually have more tables then just Tickets32 and Tickets64 on the ticket servers.  We have a sequences for Photos, for Accounts, for OfflineTasks , and for Groups, etc.  OfflineTasks get their own sequence because we burn through so many of them we don’t want to unnecessarily run up the counts on other things.  Groups, and Accounts get their own sequence because we get comparatively so few of them.  Photos have their own sequence that we made sure to sync to our old auto-increment table when we cut over because its nice to know how many photos we’ve had uploaded, and we use the ID as a short hand for keeping track. So There’s That It’s not particularly elegant, but it works shockingly well for us having been in production since Friday the 13th, January 2006, and is a great example of the Flickr engineering dumbest possible thing that will work design principle. More soon. Belorussian translation provided by PC .", "date": "2010-02-8,"},
{"website": "Flickr", "title": "5 Questions for Simon Willison", "author": ["Kellan Elliott-McCrea"], "link": "https://code.flickr.net/2010/02/10/5-questions-for-simon-willison/", "abstract": "Simon Willison kindly took time out of his llama-spotting , Python wrangling (Django co-creating), MP expenses crowdsourcing day, to answer a few questions about he and his co-conspirator Natalie Downe ‘s latest journalistic foray. 1. What are you currently building that integrates with Flickr, or a past favorite that you think is cool, neat, popular and worth telling folks about? Or both. Simon: Our main project at the moment is WildlifeNearYou.com . It started life as a holiday hacking project with twelve geeks, on a Napoleonic Sea Fort , in the channel islands, half way between England and France, with no internet connection (see devfort.com for background info). Natalie and I continued to work on it after we returned to civilisation and we finally put it live last month. 170 people have imported more than 6,500 photos from Flickr in just the past few weeks, so people seem to like it! The site’s principle purpose is to help people see wildlife – both in the wild and in places like nature reserves or zoos. We ask people to report wildlife they have seen by adding trips, sightings and photos – they get to build up their own profile showing everything they’ve spotted, and we get to build a search engine over the top that can answer queries like llamas near brighton or otters near san francisco . In addition to that core functionality, we have a couple of fun extra features based around people’s photos. Our users can import their pictures from Flickr, and use WildlifeNearYou’s species database (actually sourced from Freebase.com ) to tag those photos. We then push the tags back to their Flickr account in the form of text tags and machine tags – if they tell us the location (e.g. London Zoo) we’ll geotag their photo on Flickr as well. If they don’t know what the animal in the photograph is, other users of the site can suggest a species. If the owner of the photograph agrees with the suggestion the species will be added to their list of sightings and the correct tags applied (and pushed through to Flickr). Finally, we have a really fun crowdsourcing system for identifying and rewarding the best photos. If you go to http://www.wildlifenearyou.com/best/ we’ll show you two photos of the same species and ask you to pick the best one. Once we’ve collected a few opinions, we award gold, silver and bronze medals to the top three photographs for each species. The best photo is then used as the thumbnail for that species all over our site. Here are our best giraffe photos, as voted by the community: http://www.wildlifenearyou.com/animals/giraffe/photos/best/ 2. What are the best tricks or tips you’ve learned working with the > Flickr API? Simon: It really is amazing how much benefit we got out of pushing machine tags over to Flickr. One feature we got for free was slideshows – once you have a machine tag, it’s easy to compose a URL which will present all of the photos tagged with that machine tag as a slideshow on Flickr. Here’s one for all of our photos of London Zoo: http://www.flickr.com/photos/tags/wlny:place%3Dp1/show/ Even better, our site can now be queried using the Flickr API! Here’s a fun example: finding all of the Red Pandas that have been spotted in Europe. WildlifeNearYou doesn’t yet have a concept of Europe, but Flickr’s API can search using Yahoo! WhereOnEarth IDs. We can find the WOEID for Europe using the GeoPlanet API : http://where.yahooapis.com/v1/places.q(‘europe&#8217 ;)?appid=[yourappidhere] The WOEID for Europe is 24865675 . Next we need the WildlifeNearYou identifier for red pandas, so we can figure out the correct machine tag to search for. We re-use the codes from our custom URL shortener for our machine tags, so we can find that ID by looking for the “Short URL” link on http://www.wildlifenearyou.com/animals/red-panda/ (at the bottom of the right hand column). The short URL is http://wlny.eu/s2f which means the machine tag we need is wlny:species=s2f Armed with the WOEID and the machine tag, we can compose a Flickr search API call: http://api.flickr.com/services/rest/?method=flickr.photos.search&machine_tags=wlny:species=s2f&woe_id=24865675&api_key=… That gives us back a list of photos of Red Pandas taken in places that are within Europe. Add &extras=geo,url_s,tags to get back the tags, latitude/longitude and photo URL at the same time. The wlny: machine tags that come back can be used to link back to the place, species and trip pages – for example, wlny:place=p6p means the photo was taken at the place linked to by http://wlny.eu/p6p This is pretty powerful stuff, and it’s all a natural consequence of writing machine tags back to Flickr. ( editors note: or even a slideshow of European red pandas ) 3. As a Flickr developer what would you like to see Flickr do more of and why? Simon: One thing that would make my life an enormous amount easier would be a Flickr-hosted photo picking application. For WildlifeNearYou I had to build a full interface for selecting photos from scratch, with options to search your photos, browse your sets, browse photos by place and so forth. The Flick API makes this pretty easy to do from a back end code perspective, but designing and implementing a pleasant front end is a pretty major job. I’ve wanted to implement simple photo picking from Flickr on various other projects, but have been put off by the effort involved. WildlifeNearYou is the first time I’ve actually taken the challenge on properly. What I’d love to see instead is an OAuth-style flow for selecting photos. I’d like to (for example) redirect my users to somewhere like http://www.flickr.com/pickr/?return_to=http://www.wildlifenearyou.com/selected/ and have Flickr present them with a full UI for searching and selecting from their photos. Once they had selected some photos, Flickr could redirect them back to http://www.wildlifenearyou.com/selected/?photo_ids=4303651932,4282571384,4282571396 and my application would know which photos they had selected. This would make integrating “pick a photo / some photos from Flickr” in to any application much, much easier. On a less exotic note, we have to do quite a few bulk operations against Flickr and having a bulk version of the flickr.photos.getInfo call would make this a lot faster – just the ability to pass up to 10 photo IDs at a time would reduce the number of HTTP calls we have to make by a huge amount. 4. What excites you about Flick and hacking? What do you think you’ll > build next or would like someone else to build so you don’t have to? Simon: We’re going to be working on WildlifeNearYou for quite a while – I certainly don’t expect to get tired of looking at people’s photos of wildlife for a long time. We have a bunch of improvements planned for the “best photo” feature – we want to start showing your medals on your profile, and maybe have a league table for the best photographers based on who has won the most “best picture of X” awards. Once we’ve improved our species and location data we should be able to break that down in to best photographer for a certain area, or even for a category of animal (best owl photographer is sure to be hotly contested). We also want to streamline our “add trip” flow based on Flickr metadata. If you import a bunch of geotagged photos, we can guess that they were probably taken on a trip to London Zoo an the 5th of February based on the location and date information from the pictures. As for other projects… I’d love it if someone else would build the general purpose photo picker idea above – it doesn’t necessarily have to be Flickr, anyone could provide it as a service. 5. Besides your own, what Flickr projects and hacks do you use on a regular basis? Who should we interview next? Simon: Matthew Somerville’s work is always interesting, and his current project, Theatricalia , is a single-handed attempt to create an IMDB for theatre productions. Naturally, he’s pulling in photos from Flickr based on his own machine tags.", "date": "2010-02-10,"},
{"website": "Flickr", "title": "Galleries APIs", "author": ["Kellan Elliott-McCrea"], "link": "https://code.flickr.net/2010/04/08/galleries-apis/", "abstract": "We love galleries .  After all, without galleries how would you find your giant sea bugs ? This post is to quickly announce we’ve added galleries to the API. A Rose+GUID by Any Other Name …. Galleries in the API use “compound-ids”.  Like tags.  An example gallery compound id might look like 9634-72157621980433950 .  Unlike photos you can’t simply grab the last number off a gallery url and stick it into the API.  Yeah, I’m not thrilled about it either, but there are good (read boring) reasons why it works that way. So when an API method says it takes a gallery_id , we’re talking about the compound-id. You can however use the flickr.urls.lookupGallery method to go from gallery url to gallery_id .  Pass the method the URL for the gallery, and we’ll give you back the gallery info blob. You can also get gallery IDs from flickr.galleries.getList and flickr.galleries.getInfo . Anatomy of a Gallery Behold, a gallery info blob: <gallery id=\"6065-72157617483228192\" \n    url=\"http://www.flickr.com/photos/straup/galleries/72157617483228192\" \n    owner=\"35034348999@N01\" \n    primary_photo_id=\"292882708\" primary_photo_server=\"112\" primary_photo_farm=\"1\" primary_photo_secret=\"7f29861bc4\" \n    date_create=\"1241028772\" date_update=\"1270111667\" count_photos=\"17\" count_videos=\"0\" >\n <title>Cat Pictures I've Sent To Kevin Collins</title>\n <description>dive dive dive</description>\n </gallery> The primary_photo_* attributes refer to the “cover photo” for the gallery.  The owner is the Flickr user_id (aka NSID ) of the member who created the gallery.  The id is that compound-id we talked about. Lists of Galleries You can fetch all of a member’s galleries using flickr.galleries.getList , sorted from newest to oldest, returning a list of gallery info blobs. Or you can fetch all the galleries a given photo is in with flickr.galleries.getListForPhoto . A Bag of Photos Perhaps most interesting, flickr.galleries.getPhotos will return a list of all the photos for a given gallery .  It’s a standard photo response , with a twist. <photos page=\"1\" pages=\"1\" perpage=\"500\" total=\"15\"> \n   <photo id=\"2935475111\" owner=\"8147452@N05\" secret=\"e20746148b\" server=\"3068\" farm=\"4\" title=\"Day off from the Death Star.\" ispublic=\"1\" isfriend=\"0\" isfamily=\"0\" is_primary=\"1\" has_comment=\"1\">\n        <comment>best cat picture ever!</comment>\n   </photo>\n   <photo id=\"3078977730\" owner=\"68779755@N00\" secret=\"dba9d8105e\" server=\"3229\" farm=\"4\" title=\"&quot;We could stuff it with Kleenex...&quot;\" ispublic=\"1\" isfriend=\"0\" isfamily=\"0\" is_primary=\"0\" has_comment=\"0\" /> \n   <photo id=\"3212123792\" owner=\"10983978@N03\" secret=\"4231501383\" server=\"3391\" farm=\"4\" title=\"1-19-09: Some People Just Don't Get It\" ispublic=\"1\" isfriend=\"0\" isfamily=\"0\" is_primary=\"0\" has_comment=\"0\" /> \n     ....\n</photos> In addition to standard photo response attributes, there is also a has_comment attribute which signals whether the gallery creator added a comment about why she included the photo, and whether the child comment element is present.  Also is_primary , when set to 1, indicates this is the gallery’s “cover photo”. CRUD flickr.galleries.create creates a gallery, with a title, description, and optional primary photo, and will return a gallery element with the compound-id and the URL of the gallery. <gallery id=\"50736-72157623680420409\" url=\"http://www.flickr.com/photos/kellan/galleries/72157623680420409\" /> flickr.galleries.editMeta is simply for updating the title and description. flickr.galleries.editPhoto confusingly doesn’t edit a photo, but rather the comment about a photo in a gallery. Of course the money is all in flickr.galleries.addPhoto which allows you to actually build a gallery of photos. Nota bene:  Remember only public-safe can be added to galleries. The Curated Life We’ve also added the ability to restrict searches to only photos in galleries, with the in_gallery argument to flickr.photos.search So whether you’re interested in kittens deemed cute enough for galleries , or hand selected pink photos , or Flickr Commons photos in galleries , or simply photos taken near you (assuming you’re in Brooklyn), in galleries , that’s available.", "date": "2010-04-8,"},
{"website": "Flickr", "title": "Ishmael – a UI for mk-query-digest", "author": ["Mike Panchenko"], "link": "https://code.flickr.net/2010/04/13/ishmael-a-ui-for-mk-query-digest/", "abstract": "Ever since Peter Z from Percona showed us the awesome that is the mk-query-digest tool, we’ve been using it quite a bit to help identify costly queries. It analyzes the MySQL slow query log and compiles a detailed report which includes tons of useful data – how many times was a particular query called? how much time did MySQL spend executing a particular query during the given timeframe? how long did the query take to execute on average? what about the worst case? The report is output as a long long text file that you can page through and look at all the queries. All the data you need is there, but it’s a bit difficult to get to, especially if you want to compare multiple reports or if you want to aggregate over a period of time longer than the interval at which you run the reports (we’ve been running it every 15 minutes on one of our shards and are in the process of adding it to all servers). Enter ishmael Photo from raphie . Luckily, the tool can be configured to write most of the data in the report to a database table. Where there’s a database, there’s a former tools developer eager to write a UI on top of it (me). After Timmy, our DBA, showed me a quick prototype of a page he put together for displaying the data, I took it and ran with it. The result is ishmael – a UI on top of mk-query-digest. The name comes from the tools purpose – to help hunt down “whale” queries. For now, ishmael lets you sort the queries by 3 characteristics – the total amount of time MySQL spent executing the query, the number of times the query was actually executed, and the ratio between the two. It also displays the queries with some highlighting (done using a brutal regex) and lets you click through to see historical data (assuming you’ve run the report more than once) as well as a page that shows the EXPLAIN output for the query and the SHOW CREATE TABLE output for the tables involved (once again, traced back from their aliases using brutal regex hacks). There are already a bunch of additional features in the pipeline – being able to configure ishmael to switch between different databases, better handling of historical data, etc. We hope other mk-query-digest users find the tool useful and can let us know how the tool can be made better. Patches are welcome. The docs for mk-query-digest are on the maatkit toolkit website: http://www.maatkit.org/doc/mk-query-digest.html . The source for ishmael, is on github: http://github.com/mihasya/ishmael (click “Issues” to view the list of open tickets)", "date": "2010-04-13,"},
{"website": "Flickr", "title": "Stats API Redux", "author": ["Kellan Elliott-McCrea"], "link": "https://code.flickr.net/2010/05/13/stats-api-redux/", "abstract": "We’ve seen great uptake by library developers on supporting the new Stats APIs .  Now we’d love to see what you’re building with those APIs. PHPFlickr , Flickr.Net , flickrapi.py , CFlickr , and flickcurl all have let us know they support the new APIs . Poking around the App Garden I found flickrstats which provides a very interactive, and lovely timeline based visualization. (in the style of Google Finance).  Are there other great examples?  Let us know by adding them to the App Garden or telling us about them in the API Group . June 1st. And a reminder. Historical stats data is only available until June 1st .  After June 1st only the last 28 days of data will be available.  If you want to archive that data we have the stats download page , and a new utility method flickr.stats.getCSVFiles (because I was too lazy to manually download those files, see programmer virtues ). statsdumper provides a couple of different methods to facilitate backing up all your stats data, but I’d love to hear about others.", "date": "2010-05-13,"},
{"website": "Flickr", "title": "A lil’ time with… Nolan", "author": ["Daniel Bogan"], "link": "https://code.flickr.net/2010/07/30/a-lil-time-with-nolan/", "abstract": "This is the first in a new series of interviews with the Flickr staff, asking what tools (hardware, software) they use to get the job done. Our first volunteer is Nolan Caudill, PHP engineer and wearer of beards. Who are you, and what do you? I’m Nolan Caudill, a software engineer at Flickr (obviously). I’m a recent addition to the company and a new transplant to San Francisco. At Flickr, I work on i18n/l10n, though I like to keep a variety of projects on my plate to keep things interesting. I’m originally from Wilkes County, North Carolina which is the moonshine capital of the world and the birthplace of NASCAR (and those two facts are indeed related). I moved here from the Research Triangle in North Carolina, going to school in Chapel Hill and residing in Durham. My wife and our cat moved here in May and are both loving this chilly, foggy city with good beer and food on every corner. What hardware are you using? I’m currently typing this on my work-issued MacBook Pro hooked up to a 24″ HP monitor, fairly par for the course at Flickr. This is the first Mac I’ve worked on in several years and I’m a big fan. The consistency of all the software is nice to use and things just seem to work. At home, I use a Lenovo Thinkpad R61i with Ubuntu 10.04 installed. I’ve ran Ubuntu since version 4.10 and really enjoy being able to install about any developer tool under the sun with a one-line command. It’s hard to beat the big Debian/Ubuntu repositories. This is also about my third or fourth Thinkpad laptop. They are built like tanks and are nicely supported out of the box by most Linux distributions. I use a iPhone 3G and it gets along just fine. And what software? As far as software goes, my programming life revolves around programming languages, browsers, and text editors. At work, I do almost everything in PHP and at home I’m always playing with new languages. I’m currently in the process of learning Haskell and learning how compilers work so I can one day build my own toy language. I’ve also done professional work in Python and that is still my favorite multi-purpose language where I can just get stuff done. My main browser at work is whatever the stable version of Firefox is. Like most web developers, I use Firefox for extensions like Firebug, Web Developer, and Y!Slow. I use Chrome at home, mainly just because I don’t need a lot of bells and whistles when I’m browsing and with a smaller laptop screen, the minimal chrome really frees up some real estate. At work, I use TextMate to write code. This was a big departure from what I normally used. I’ve always used vim and have those keyboard bindings hard-wired into my muscle memory. I don’t think TextMate is as powerful or as quick to use as vim (at least for me), but that’s not necessarily a bad thing. TextMate shines at its intended purpose, which is to just get some code out. The project navigation and function browse is really nice and having it integrated in the Mac style is a plus. I still use vim for my off-hours hacking projects though. Other random pieces of software include: iTunes, Adium for both IM and IRC, Thunderbird for email, and Tweetie for keeping up with Twitter. What would be your dream setup? As far as dream setup goes, I’ve never been that picky and happy with a minimal setup. As long as I have a PC that keeps up with my work, a good keyboard, a mouse that glides, a good-sized second monitor, and a comfortable chair, I’m golden.", "date": "2010-07-30,"},
{"website": "Flickr", "title": "A lil’ time with… Timoni", "author": ["Daniel Bogan"], "link": "https://code.flickr.net/2010/08/06/a-lil-time-with-timoni/", "abstract": "Who are you, and what do you? I am Timoni Grone, former-Nebraska-farmgirl-now-Californian-city-dweller, and I design web interfaces. I’m currently a UI/visual designer at Flickr, which is a totally rad gig. What hardware are you using? At work, I have a  2.15Ghz Core 2 Duo 15″ MBP, a 24″ external monitor, the short Mac keyboard and a Mighty Mouse. At home, I have a 2.67GHz 15″  MBP and a Magic Mouse. I forget how much RAM I have, but both computers are fast enough. I also have an old Mac Mini working as a media center (hooked up to my absurdly ginormous television), a 2TB Lacie Quadra for backups, an Airport for network stuff, a 16GB wifi iPad and an iPhone 4. I briefly had a Wacom Cintiq 12WX earlier this year. I excitedly used it for a few weeks, then left it untouched for months, and finally sold it.  It was too much of a pain to switch back and forth between the tablet and my laptop. And what software? For web dev, I like TextMate, though I was on a strict Coda diet for a while when it made sense to have seamless FTP integration. If I’m using TextMate, I use Transmit. Current browser of choice is Chrome, cause it’s super-fast, though I prefer Firebug to Web Inspector for development. Day to day, I use Photoshop and Illustrator all the time. I’ve been trying to switch to comping in Fireworks for a while, but for some reason it was never installed on my work computer, so I default back to Photoshop. The way it handles smart objects is a pain, but Save For Web in Illustrator can be effing tricky sometimes (also, the way Illustrator renders text below a certain size makes my eyes hurt). I have Skitch, the Last.fm scrobble app, and Dropbox running at all times on both of my MBPs. I use Dropbox to sync my work folder and one of my iTunes libraries. Works like a charm and it’s only ten bucks a month. For print work, I use InDesign. For small things like bills & letterhead, I use TextEdit. I try to avoid halfway-done word processors like Pages and Word; the way they handle style sheets drives me batty. I just started using Notational Velocity for my to-do lists (thanks Daniel!).  I use Google Calendar for scheduling, and have SMS reminders sent to my phone (a total lifesaver). When I want to write, I use Ommwriter, the nicest little text editor out there. On my iPad, I most often use NetNewsWire, Instapaper, the Kindle app, and Autodesk’s Sketchbook Pro. What would be your dream setup? We’re at a really fascinating point in hardware development right now, which makes it difficult to answer this question. My knee-jerk answer is that I want the Young Lady’s Illustrated Primer combined with an iPad combined with the Cintiq combined with, you know, a Cray supercomputer or something else equally powerful. The problem is, really, handwriting recognition; if you’ve ever tried to use the iPad with an external keyboard, you’ll know exactly what I mean. Switching from typing to writing or drawing and back is a fucking pain.  Regular notebooks allow you to draw and write without changing your hand position, which doesn’t seem like a luxury until you try actually working on a tablet and then find you need to input text. SJ may think that styli are inelegant, but the fact is, using a pen to write or draw on paper is both comfortable and easy; it’s just not as fast as typing. Most people are content with inputting data via a keyboard, and this makes sense for a lot of jobs: marketing, business development, finance, and programming, for example. But for the designers, there’s a big gap between starting the creative process and executing the product design *because* it’s much easier to sketch out your ideas on paper, with a pen, than a computer. And this is unfortunate; in the future, we should have computers that allow us to keep contexts for different stages of product development. The iPad and ThinkPads are steps in the right direction, but they’re still awfully clumsy, which is why, in part, people criticize the iPad as a product for mere consumption. I want a Moleskine that is a blindingly superfast computer. That’s my dream setup.", "date": "2010-08-6,"},
{"website": "Flickr", "title": "Now in Belorussian…", "author": ["Simon Batistoni"], "link": "https://code.flickr.net/2010/08/16/now-in-belorussian/", "abstract": "As compliments to writers go, having your work translated into another language comes pretty high on the list. That said, I’m not sure I ever expected to see one of my code.flickr blog posts re-interpreted in Belorussian until this weekend when I was contacted by a translator by the name of Patricia Clausnitzer. Patricia has provided a Belorussian rendering of my post (complete with pictures of paint tins and me in a stretcher) on a site called pc.de. So if you read Belorussian, you can now get the skinny on our “People in Photos” API methods in your native tongue. And if you don’t speak Belorussian but want to code up an app that takes advantage of our people-annotating features, you can revisit the original post about the API methods here .", "date": "2010-08-16,"},
{"website": "Flickr", "title": "Creating a dashboard for the help team", "author": ["Zack Shepard"], "link": "https://code.flickr.net/2010/08/20/creating-a-dashboard-for-the-help-team/", "abstract": "When creating a tool for the help team, one of the main things we wanted to do was find a good way to give them updates on new features and site issues. For any of you that have ever been on a help team you know that no matter how much your boss tells you it’s very important that you check your email or look at X web page for updates before each case, that’s probably not going to happen. When you are trying to get through help cases every click and keystroke counts. So if you are supposed to check some page that only changes 1 out of 100 times you check it, it naturally falls into the list of things that you probably don’t 100% have to check. You don’t have time for that, you’ve got people to help and the queue keeps growing! So how do you get people to look at those updates? Make it useful! To make the page useful we tried to solve one of the other frustrations common to most help teams, the tools you need are all on different pages (maybe even managed by different teams). Go here to search for accounts, over there to search for a photo, another place to look up an ID, etc. Any search that might be needed to research a question we put all on one page. The actual tools may still reside somewhere else, but a search box is also included here so you can get to any tool you need, even the flickr.com searches for pictures and people. Here is an example of what it looks like with a few parts and dates changed for security reasons. Directly below the searches is “Current Issues” and new FAQs. Now that this is the page you will start at for every case, you’ll always see these updates. But is that enough? At each stage we tried to think of our audience. If you are trying to get through cases, when you go to a page over and over you start to tune out what you don’t use. To combat that tunnel vision we rotate the c o l o r of the issue title and FAQs so it’s easier to notice that something changed.  (I actually usually think of that T-Rex in Jurassic park that only sees you if you move. But don’t tell the help team that’s what I was thinking. They’re actually very nice.) When we released it to the help team, everyone made it their homepage without the boss man having to go around and make them. Success!", "date": "2010-08-20,"},
{"website": "Flickr", "title": "A lil’ time with… Chris", "author": ["Daniel Bogan"], "link": "https://code.flickr.net/2010/09/08/a-lil-time-with-chris/", "abstract": "Who are you, and what do you? Hello, I’m Chris Martin, one of the resident nerds here at Flickr who keep everything running smoothly. I’m from Atlanta, Georgia but I don’t have a southern accent and this seems to disappoint people pretty regularly, I apologize for that. My focus at Flickr is on mobile engineering and making sure people can get their lovely photos out of their pockets and on to Flickr; and vice versa. In real life I’m a professional travel writer/photographer. I’ve never written anything and my photos are sub-par, but I have to justify traveling around the world somehow. What hardware are you using? At work I use a 15″ MacBook Pro hooked up to two 24″ Dell displays, one for code and one for browser windows etc., they are both quite useful for creating a barrier to hide behind during the regular FlickrHQ foam dart wars. I also have a variety of mobile devices that we use to test our own code, 3rd party integrations, and API apps. My Flickr Moleskine and stack of post-it notes are also integral parts of my work hardware setup. At home I have a pretty assorted collection of hardware, the most interesting of which is probably a 450MHz G4 Cube that sits quietly on my desk and still does a great job as a file server. Rounding out my fanboy worthy collection of Apple products are a unibody 15″ MacBook Pro, an iPad, and an iPhone 4 that replaced my 1st gen iPhone which I’ve used for the past 3 years (and still do for testing). I have two “hackintosh” machines; one desktop with a couple terabyte drives hooked up to my Samsung TV as a media center, and a 12″ ASUS 1201n which I find myself using more than my MacBook Pro because I like it’s small size when I’m out, and I hook up to a 24″ display when I’m at my desk at home. In my “retired” collection are a linux PC that used to be a MythTV media center, an original eepc 701 that served me well on a 5 month backpacking trip, an ancient original iMac, and an assorted collection of iPods. I have a problem saying goodbye. It’s probably appropriate to include camera gear as well… I have a Canon 350D which usually wears a 50mm f1.4 lens; it’s starting to show it’s age but has seen and faithfully captured many great trips over the years, a Kodak M1033 which isn’t the greatest camera in the world but it’s built like a tank and was the least obscenely expensive thing I could find in India after leaving a Canon on a train, and a Panasonic Lumix ZS3 that takes excellent video. However, I still take most of my photos with my iPhone. And what software? Obviously I’m an OSX guy, ever since I moved from Gentoo linux to a powerbook in college. I love the polish of the Mac GUI, and the raw power of Unix that’s just below the pretty surface. I use the assortment of standard apps (Safari, Mail.app, iCal, and address book) as my primary browser, mail, and calendar applications. I’ve moved around a bunch in the past in this area, but the apple tools have finally started playing nicely with the google suite of services and they work well enough to keep me happy and in sync with my iPhone. For development I primarily use TextMate (web-dev) and xCode (Cocoa), with the occasional trip over to Coda when I need to quickly touch something I’m doing any front end development on. I don’t have any special tools I use for source control or file transfer, that’s all svn, git, scp, etc. in terminal. A few other apps like Linkinus, Adium, and Echofon live in my dock all the time, and of course I pull out photoshop when I need to mock something up. As for utilities, I absolutely can’t live without Visor, a SIMBL plugin that makes my terminal appear on command from the top of the screen, teleport is a nifty tool for sharing one keyboard and mouse between multiple computers, DeskLickr keeps my desktop beautiful with photos from Flickr, and MenuMeters gives me a quick view of how the computer is doing from the menu bar. On my phone, I use Echofon for twitter, Reeder is my absolute favorite RSS reader of all time (I wish there were a desktop version), Instapaper, AutoStitch to make panorama photos, iTimelapse for time-lapse video, and of course m.flickr.com and the Flickr app for browsing Flickr. What would be your dream setup? As much as I love laptop computers and the amazing mobile devices we have today, I can’t wait until our computing experiences are more thoroughly integrated into all of the other objects we use in daily life. I don’t just mean “connected things”, but more along the lines of augmented reality. I guess my dream setup would be a pair of contact lenses that I could put in in the morning and immediately start seeing extra information in every day life. I’m sure it will be done, but it will be a fine balance between adding to reality, and completely removing ourselves from it; I hope we do it well.", "date": "2010-09-8,"},
{"website": "Flickr", "title": "A lil’ time with… mroth", "author": ["Daniel Bogan"], "link": "https://code.flickr.net/2010/09/29/a-lil-time-with-mroth/", "abstract": "Who are you, and what do you? Ahoy!  I’m Matthew Rothenberg.  Most people here call me mroth. I’m the head of product strategy / management at Flickr, which means I’m ultimately responsible for figuring out what Flickr needs to build and why .  I work with all of our teams during the development of Flickr features to help make sure they meet our goals and vision, as well as do boring manager-y things.  In reality, that means I spend most of my life in meetings (formal or informal). In a broader sense, I’m responsible for trying to make Flickr the world’s best place to share your photos with the people who matter to you. What hardware are you using? I have a 15” MacBook Pro with 4GB of RAM (which I insist on still calling a PowerBook).  I have a 24” Dell display both at work and at home, but I almost never bother to plug into them anymore. Mobile, oy–I have way too many phones.  My primary phones that I carry every day are a BlackBerry 9650 (writing email, making phone calls I want to stay connected for more than 60 seconds), and an iPhone 4 (browsing the web and my primary camera).  I also have a Palm Pre (used to be my main phone, now I use it mostly for development), HTC Evo 4G (development and testing) and a Nokia N95-3 (which I use as the world’s most expensive alarm clock).  With all those phones, I’m a heavy Google Voice user for bouncing calls back and forth. For running all my side projects and home backup, I have a homebuilt Linux (Ubuntu Server 10.04.1 LTS) server with an Atom chipset sitting in a MSI Wind enclosure, and a Seagate “Green” hard drive that spins down to lower speeds to conserve power. My loft’s stereo is wired up for network play via AirTunes and Airport Express, but unfortunately the software for it is so terrible (no streaming from iOS) it doesn’t get much use currently — I’m hoping AirPlay in iOS 4.2 will finally resolve this.  More successfully I do stream video via the 802.11n network from the Linux server to my Sony PS3 which is hooked up to the television (Sony 40” flatscreen of some sort, if anyone actually cares). The most transformative tech hardware I’ve purchased in years has been the Kindle (2nd gen), which completely changed my relationship with books. It perfectly embodies the design principle of doing one thing exceptionally well , and has resulted in me reading approximately 350% more books since I purchased it. Finally, and most importantly, the best productivity-enhancing hardware I own is a La Marzocco GS/3 espresso machine. And what software? I live in email — I currently use Mail.app for its live search folders but I’m not entirely satisfied with it.  There is a definite market for a real “power user” email app that hasn’t quite been filled yet. Skitch is another “couldn’t live without it” application for me, since I spend a lot of time reviewing not-quite-ready-for-primetime features, it lets me super quickly capture and annotate notes for the team. I’ve been a UNIX nerd for a long long time, so I spend a lot of time in Terminal.app, and sadly even do most of my basic navigation file system operations in there.  For actually hacking on code, I use TextMate. Most of my side projects are coded in Python, but lately I’ve been dabbling in Ruby/Rails. Github completely changed the way I hack on side projects.  If you haven’t read Anil Dash’s recent blog post on it , you should.  If you ever develop software for fun and you aren’t using it, you’re sorely missing out. I occasionally have to build presentations for my job, so when I do, I insist on using Keynote. What would be your dream setup? A mythical iPhone with a physical QWERTY keyboard (jailbroken, of course).", "date": "2010-09-29,"},
{"website": "Flickr", "title": "A lil’ time with… Stephen", "author": ["Daniel Bogan"], "link": "https://code.flickr.net/2010/10/13/a-lil-time-with-stephen/", "abstract": "Who are you, and what do you? I’m Stephen Woods and I’m a frontend engineer at Flickr. Along with the rest of our team I am responsible for the bits that make the UI work, which mostly means html, css and javascript. So I write code, but I also make sure the design of the site looks and works the way the designers want it to. What hardware are you using? In terms of computers I have 15″ MacBook Pro with a 24″ monitor, as well as the standard ipad and iphone. I don’t have a desktop anymore and I do any hacking or side projects on a slicehost image. I used to have a pretty large collection of random computers and desktops, but over the years I have really cut back on hardware, it just doesn’t seem necessary anymore. Like a lot of people here I also enjoy photography. I have a digital rebel xsi and a someone worse for wear 4×5 view camera, neither of which I use as much as I would like. And what software? I use TextMate (and sometimes vim) for coding. My development browser is mostly Safari, with Firefox/firebox for some things. I use Charles proxy a lot for HTTP debugging. VMWare fusion is indispensable. I have a portable hard drive with a little collection of VM images to test all the various browsers out there. I am also completely dependent on Evernote. I use it for everything. Frequently I remember something I need to do or figure out a problem while lying in bed, so I just grab my ipad, write down my notes and they are on my laptop when I need them. What would be your dream setup? My dream setup would be the MacBook Pro with the good video so I could actually play games on it. Otherwise I am pretty much living the dream.", "date": "2010-10-13,"},
{"website": "Flickr", "title": "The Not-So-New Image Size: Medium 640", "author": ["Henry Lyne"], "link": "https://code.flickr.net/2010/10/26/the-not-so-new-image-size-medium-640/", "abstract": "In July 2010 we launched a new photo page that displays photos at up to 640 pixels on the largest side. In the months leading up to this launch we did a lot of work to support generating this new size, along with making our support of the Large image size more consistent. Medium 640 We started adding support for the new Medium 640 size(_z suffix) on March 31st 2010. We generated a Medium 640 size for all photos uploaded after that date. Over the next couple months after that we found and corrected instances where we were not fully supporting this size. We display the Medium 640 size on the Photo, Photostream and All Sizes pages. Dynamically Generated Images If a photo was updated/uploaded before March 31st 2010 then we do not have a Medium 640 size on file and will try to dynamically generate it. We can do this if a large version of the photo exists, or if the original is a jpeg that has not been rotated(when you see an image URL that ends in _z.jpg?zz=1 , the zz=1 indicates we can generate from an original). By doing this we make a lot of old photos display at 640 on the new Photo, All Sizes and Photostream pages. And every time we generate a Medium 640 image it gets cached, so we don’t have to generate it again for a while. If we can’t generate the image dynamically, we fall back to displaying the Medium 500. Generating photos dynamically needs to be happen fast, so we don’t try to generate a Medium 640 from the original if the photo has been rotated or needs to be converted from a non-jpeg format. Larges For All We wished we had generated a Large size in the past for all photos uploaded, it would have made it simpler to dynamically generate the Medium 640 size. Before May 25th 2010 we only generated Larges for photos uploaded greater than 1280 in size, now we generate a Large no matter what size is uploaded. This is great for certain types of photos, for example we can now display 800×600 photos(common to some camera phones) as a Large in Lightbox or on the All Sizes page. So, a Large photo will be 1024 pixels in size on the longest side, or the same size as the original if something smaller than 1024 pixels was uploaded(we never scale a photo larger, an original photo of 800×600 will have a Large that is 800×600). You can expect the Large to always exist for newly uploaded photos. Using the API The best way to get the URL for a Medium 640 is to include url_z in the extras field when calling the flickr API. When photo data is returned, it will also include the full Medium 640 URL in url_z , along with height_z and width_z . Constructing the URL from the photo data returned will also work, but less reliably for photos before March 31st 2010. http://farm {farm-id}.static.flickr.com/{server-id}/{id}_{secret}_z.jpg See: http://www.flickr.com/services/api/misc.urls.html", "date": "2010-10-26,"},
{"website": "Flickr", "title": "The Flickr Developer Guide", "author": ["Ross Harmes"], "link": "https://code.flickr.net/2010/11/29/the-flickr-developer-guide/", "abstract": "We’re happy to announce today the launch of our Flickr API Developer Guide . Whether you’re just getting started or you have worked with our API for a long time, check it out for some tips on how to best (and respectfully) use the API. All feedback is welcome via the discussion board for the API Mailing List , and you can also check out the API Changelog for any updates, via @flickrAPI , the Flickr API Twitter feed.", "date": "2010-11-29,"},
{"website": "Flickr", "title": "Farewell FlickrAuth", "author": ["jfanaian"], "link": "https://code.flickr.net/2012/01/13/farewell-flickrauth/", "abstract": "Last year, we added support for OAuth 1.0 – a much better way to have your users authenticating against Flickr. More information on Flickr user authentication via this method is available here in our Developer Guide and specifically here . If you are app already uses OAuth, then you can skip this post and look at some gorgeous photos instead. However – if your app still uses the old Authentication API, you will need to update it to OAuth by July 31st this year. Updating to OAuth is easy and you don’t worry about any user impact. You can exchange an old auth token from the old Authentication API, to an OAuth access token. The process simply requires that you make an authenticated request to the flickr.auth.oauth.getAccessToken API, which will exchange the old token used to make the request, with a new OAuth access token. Again, everything is documented right here . Flickr member Jef Poskanzer has also written an overview and comparison between the two auth methods: http://acme.com/flickr/authmap.html. After July 31st, we will no longer support the old Authentication API. More information is available in the Flickr API FAQ’s , and in the Flickr Developer guide , but if you have specific questions about updating your app, you may file a help ticket here . Thanks for making Flickr more fun by contributing to our growing collection of apps! Your Engineering Team at Flickr ( @FlickrAPI )", "date": "2012-01-13,"},
{"website": "Flickr", "title": "Scott Schiller on Web Audio", "author": ["Ross Harmes"], "link": "https://code.flickr.net/2012/02/28/scott-schiller-on-web-audio/", "abstract": "We recently had a Flickr Frontend Night at BayJax , the Bay Area JavaScript group. We’ll be posting the videos from those talks over the next couple of weeks. First up! Frontend Engineer, DJ, and all-around nice guy Scott Schiller , with his great talk on Web Audio. Scott used impress.js to make his awesome slides (using HTML and CSS Transitions). If you want to dig deeper into Scott’s talk, there is the HD version on YouTube , slides , the Wheels of Steel demo , and the HTML5 game he created, SURVIVOR . Big thanks to Gonzalo Cordero for organizing the event, and to Ryan Grove and Allen Rabinovich for their great work filming it. On a side note, if you’re in Austin for SxSW next week, be sure to check out talks by Flickr’s own Eric Gelinas ( Geo Interfaces for Actual Humans ) and Stephen Woods ( Creating Responsive HTML5 Touch Interfaces ). Thanks to softdroid.net for creating a Ukrainian translation of this post .", "date": "2012-02-28,"},
{"website": "Flickr", "title": "Liquid Photo Page Layout", "author": ["Ross Harmes"], "link": "https://code.flickr.net/2012/05/15/liquid-photo-page-layout/", "abstract": "The Flickr photo page has gone through several revisions over the years. It was initially designed for 800×600 pixel displays, with a 500 pixel wide photo and a 250 pixel wide sidebar. The 500×375 photo takes up 9.1% of the 1905×1079 pixels available in my viewport By 2010, display resolutions had increased significantly, and 1024×768 became the new standard for our smallest supported resolution. We launched a re-designed photo page, designed for a width of 960. It featured a 640 pixel wide photo and a sidebar of 300 pixels. The 640×480 photo takes up 14.9% of the 1905×1079 pixels available in my viewport Since then the number of different display resolutions has increased and larger sizes have become more popular, but the number of users still on 1024×768 displays have made it hard to increase the width of the page beyond 960. We realized that we would always have to support smaller monitors, but that there was no reason not to give bigger photos to those with larger monitors. The recent launch of the 800, 1600, and 2048 photo sizes gave us a lot of different options for showing big, beautiful photos to members, and we wanted to take advantage of that. Starting today, we will display the biggest photo that we can on the photo page for your monitor . The 1213×910 photo takes up 53.7% of the 1905×1079 pixels available in my viewport Algorithmic As you use the new liquid photo page, you may notice that the page content doesn’t always fill the entire viewport. This is because we created an algorithm for taking the width and height into account that will display content at a width that will best showcase the most common photo ratio, the 4:3. Here are the goals of that algorithm: Show the biggest photo the window allows Ensure the title and the sidebar are visible Keep the width of the page consistent across all photo pages, regardless of the individual photo dimensions Whenever possible, prefer native dimensions of a photo size (i.e., resist downsampling and never upsample) Going Big Big photos are really compelling. We knew from using the Flickr Light Box that our members’ photos look amazing at full screen, and we wanted to give the same experience on the photo page. This part of the algorithm was easy; as soon as the page starts loading, we read the innerWidth and innerHeight of the viewport (or the browser’s equivalent), and then go through the photo sizes that the photo owner allows us to display to find the best fit. If the photo is a little too big for the space we have to work with, we scale it down in the browser. Providing Context As great as a giant photo is, a photo is more than just its pixels. The context and story around a photo is just as important. Imagine a photo of a tiger; it’s impressive in its own right, but throw in a map showing that the tiger is in a public park, and a title stating, “A Tiger Escaped From the Zoo!” and then you really have something.</> We decided that the title and the sidebar are important enough to make it worth showing a slightly smaller photo on the page. We adjusted the algorithm to take into account the width of the sidebar and its gutter (335 pixels) and the height of the first line of the title (45 pixels) when calculating how much available space there is for a photo. Site Consistency So far, so good. However, as we used the liquid photo page we noticed that it had one fatal flaw: Since the algorithm uses the dimensions of the photo that you are viewing to adjust the page width, it changes from photo to photo. This mean that if you’re browsing through some photos, the elements of the page are moving around from page to page. This is especially problematic with the header and the Next / Previous buttons; It’s incredibly difficult to navigate around if you always have to hunt around to find them first. To fix this problem, we decided to make the algorithm ignore the dimensions of the currently displayed photo when calculating page width, and instead to always use the dimensions of an imaginary 4:3 photo. This means that the page width will always be the same for any given combination of viewport width and viewport height, and that the UI elements will be in the same places for each page. The downsides of this are that photos that aren’t 4:3 will have more whitespace around them and even potentially be cut off by the bottom of the page, forcing the viewer to scroll. Using a consistent width is definitely the lesser of the two evils, though. The current photo page has the same problem with photos that are taller than they are wide being below the fold, and we’ve been happily viewing them for years. Going Native These days, browsers do a pretty good job scaling a photo down. By default, most browsers err on the side of quality rather than speed, so the resulting photo should look good regardless of the size it is displayed. That being said, if we ever downsample a photo, then we are downloading more pixels than we need and throwing them away. This isn’t good for performance. We adjusted the algorithm to favor native sizes, even if that means a slightly smaller photo is shown. We coded in detents , so that if a photo size is within 60 pixels of a native size, we will just use that size instead of downsampling a larger one. This means the page loads faster and that most common monitor resolutions will see photos at the native size, as this table illustrates (percentage use data from StatCounter ): Resolution Use % Page width Image size Image width Efficiency 1366 x 768 19.28% 975px Medium 640 640px 100.0% 1024 x 768 18.60% 975px Medium 640 640px 100.0% 1280 x 800 12.95% 1044px Medium 800 709px 88.6% 1280 x 1024 7.48% 1216px Large 1024 881px 86.0% 1440 x 900 6.60% 1135px Medium 800 800px 100.0% 1920 x 1080 5.09% 1359px Large 1024 1024px 100.0% 1600 x 900 3.83% 1135px Medium 800 800px 100.0% 1680 x 1050 3.63% 1359px Large 1024 1024px 100.0% 1360 x 768 2.32% 975px Medium 640 640px 100.0% Titles Are for Squares, Man Square photos are an interesting loophole in the way we size photos. Because we’re targeting an imaginary 4:3 photo, square photos will be displayed with more actual pixels than any other size, taking up the full width and height allotted. While browsing the site we noticed this, as well as the fact that the title is never visible. In order to bring the overall pixel count more in line with landscape and portrait photos, we reduce the size of square photos a bit more than the others. This helps ensure that the titles are always visible as well. Making it Fast Now that the algorithm is complete, we need to work on the performance. We noticed that reading the viewport dimensions and resizing the page every single time you go to a photo is unnecessary and distracting (since the page loads with a width of 960 and must be adjusted after the JavaScript loads on the page). To fix this, we cache the viewport dimensions in a cookie that can be read by the PHP code that generates the page. The first time you go to a liquid photo page, we have no choice but to adjust the page width on the fly. But every other photo page you visit will have the dimensions stored from the last page, and the page will be rendered with the correct width from the start. More to Come We have a lot more changes in store for this year. Stay tuned!", "date": "2012-05-15,"},
{"website": "Flickr", "title": "The great map update of 2012", "author": ["Eric Gelinas"], "link": "https://code.flickr.net/2012/06/29/the-great-map-update-of-2012/", "abstract": "Today we are announcing an update to the map tiles which we use site wide. A very high majority of the globe will be represented by Nokia’s clever looking tiles. We are not stopping there. As some of you may know, Flickr has been using Open Street Maps (OSM) data to make map tiles for some places. We started with Beijing and the list has grown to twenty one additional places: Mogadishu Cairo Algiers Kiev Tokyo Tehran Hanoi Ho Chi Minh City Manila Davao Cebu Baghdad Kabul Accra Hispaniola Havana Kinshasa Harare Nairobi Buenos aires Santiago It has been a while since we last updated our OSM tiles. Since 2009, the OSM community has advanced quite a bit in the tools they provide and data quality. I went into a little detail about this in a talk I gave last year . Introducing Pandonia Today we are launching Buenos Aires and Santiago in a new style. We will be launching more cities in this new style in the near future. They are built from more recent OSM data and they will also have an entirely new style which we call Pandonia . Our new style was designed in TileMill from the osm-bright template, both created by the rad team at MapBox. TileMill changes the game when it comes to styling map tiles. The interface is developed to let you quickly iterate style changes to tiles and see the changes immediately. Ross Harmes will be writing a more detailed account of the work he did to create the Pandonia style. We appreciate the tips and guidance from Eric Gunderson, Tom MacWright, and the rest of the team at MapBox We are looking forward to updating all of our OSM places with the Pandonia style in the near future and growing to more places after that… Antarctica? Null Island? The Moon? Stay tuned and see… Changing our Javascript API To host all of these new tiles we needed to find a flexible javascript api. Cloudmade’s Leaflet is a simple and open source tile serving javascript library. The events and methods map well to our previous JS API, which made upgrading simple for us. All of our existing map interfaces will stay the same with the addition of modern map tiles. They will also support touch screen devices better than ever. Leaflet’s layers mechanism will make it easier for us to blend different tile sources together seamlessly. We have a fork on GitHub which we plan to contribute to as time goes on. We’ll keep you posted.", "date": "2012-06-29,"},
{"website": "Flickr", "title": "Raising the bar on web uploads", "author": ["Scott Schiller"], "link": "https://code.flickr.net/2012/04/25/raising-the-bar-on-web-uploads/", "abstract": "With over seven billion photos uploaded since day one, it’s safe to say that uploading is an important part of the Flickr experience. There are numerous ways to get photos onto Flickr, but the native web-based one at flickr.com/photos/upload/ is especially important as it typically accounts for a majority of uploads to the site. A brief history of Flickr “Web Uploadrs” Flickr “Flashy” Uploadr UI (2008) vs. Basic Uploadr UI Earlier versions of Flickr’s web-based upload UI used a simple <form> with six file inputs, and no more. As the site grew in scale, the native web upload experience had to scale to match. In early 2008, an HTML/Flash hybrid upgrade added support for batch file selection, allowing up to several gigabytes of files to be uploaded in one session. This was a much-needed step in the right direction. The “flashy” uploader does one thing – sending lots of files – fast, and reliably. However, it was not designed to tackle the other tasks one often performs on photos including adding and editing of metadata, sorting and organizing. As a result, “upload and organize” has traditionally been reinforced as two separate actions on Flickr when using the web-based UI. The new (mostly-HTML5-based) shiny Thanks to HTML5-based features in newer browsers, we have been able to build a new uploader that’s pretty slick, and is more desktop application-like than ever before; it brings us closer to the idea of a one-stop “upload and organize” experience. At the same time, the UI also retains common web conventions and has a distinct Flickr feel to it. We think the result is a pretty good mix, combining some of the best parts of both. As feedback from a group of beta testers have confirmed, it can also be deceivingly fast. The new Flickr Web Uploader. It’s powerful, it’s got a dark background, and it’s fast . Features: An Overview Here are a few fun things the new uploader does: Drag and drop batches of files from your OS. Where present and supported, EXIF thumbnails are shown in the UI almost immediately. Fluid photo “grid” shows photo thumbnails, allows larger, lightbox-style previews, inline editing of description/title and rotation. Mouse and keyboard-based grid selection and rearrange functionality similar to that of desktops. “Editor panel” shows state of current selection, provides powerful batch editing features (title + description, adding of tags, people, sets, license, privacy etc.) “Info” mode shows overlay icons on grid items, allowing for a quick overview of pending edits (privacy, people, tags etc.) Auto-retry and recovery cases for dropped / lost connection cases Technical Bits A small book could probably be written on the process, prototypes and technology decisions made during the development of this uploader, but we’ll save the gory details for a couple of in-depth blog posts which will highlight specific parts of the UI. In the meantime, here are some notes on the tech used: HTML5 File APIs Modern browser file APIs make up the core of file handling functionality, including drag-and-dropping of files right into the browser. FileReader -type APIs allow access to data from disk, enabling things like EXIF thumbnail parsing and retrieval where supported. EXIF parsing is almost instantaneous and thumbnails are hugely valuable, of course, in prompting users’ editing decisions. (For browsers without the relevant file APIs, a Flash-based fallback is used in which case file drag-and-drop is not supported, and EXIF thumb previews are not implemented.) CSS3 Thanks to growing support across newer browsers, we’ve been able to produce a modern design that takes advantage of CSS-based gradients to achieve visual goals that would have traditionally required external images, and occasionally, hacks or shims in our HTML and JavaScript. CSS3’s border-radius , text-shadow and box-shadow are also featured nicely in this new design, alongside visual transform effects such as rotate , zoom and scale . Eagle-eyed users of newer Webkit builds such as Chrome Canary may even see a little use of filter with blur here and there. CSS transitions are also featured extensively in the new uploader, a notable shift away from animation sequences which would traditionally have been calculated and rendered by JavaScript. Good candidates for transitions include the expanding or collapsing of a menu section, or a background color fade when a text area is focused, for example. While triggering transitions and/or transforms can be a little quirky depending on the current “state” of the element (for example, an element just added to the DOM may need a moment to settle and be rendered before transitioning,) the advantage of using CSS vs. JS for “enhancement”-style UI effects like these is absolutely clear. YUI3 Thanks to YUI3 , the new Flickr Uploader is a highly-modularized, component-based application. The editr module itself is comprised of about 35 sub-modules, following YUI’s standard module pattern. In Flickr’s case, modules are defined as being JavaScript, CSS or string (i.e., language translation) components. This compartmentalization approach reduces the overall complexity of code, encourages extensibility and allows developers to work on features within a specific scope. A sneak peek: Screencast (Beta Version) At time of writing, the new uploader is being gradually rolled out to the masses. For those who haven’t seen it yet, here’s a demo screencast of an earlier beta version showing some of the interactions for common upload and editing use cases. (Best viewed full-screen, and with “HD” on.) The video gives an idea of what the experience is like, but it’s best seen in person. We’ve really had a lot of fun building this one. [flickr video=6928227556 show_info=true secret=11b73352d1 w=500 h=281]", "date": "2012-04-25,"},
{"website": "Flickr", "title": "Building an HTML5 Photo Editor", "author": ["Eric Gelinas"], "link": "https://code.flickr.net/2012/04/18/building-an-html5-photo-editor/", "abstract": "Introducing guest blogger, Ari Fuchs . He is a Lead API Engineer and Developer Evangelist at Aviary . He has spent the last 3 years building out Aviary’s internal and external facing APIs, and is now working with partners to bring Aviary’s tools to the masses. He also did a lot of work to bring the Aviary editor to Flickr. Follow him on Twitter and send him a nice message to make him feel better about his stolen bike . Now, on to his post… At Aviary, we’ve been passionate about photos since day one. It’s been five years since we released our first creative tool, Phoenix, a powerful, free Flash-based photo editor. Phoenix offered functionality on par with Adobe Photoshop 5 and a price point that opened its usage to anyone with an internet connection. As amateur photographers worldwide began trying their hand at editing, we watched our product join the ranks of a small number of companies working to democratize the process of photo editing for the first time. Around two years ago we began rethinking the future of our tool set. While our original tools offered incredible functionality, they did have a learning curve which meant that the average person couldn’t just sit down and begin editing without investing time to become familiar with the tools. We wanted to build a powerful editor that anyone could use. Because we were rebuilding the editor from the ground up, we took the opportunity to switch from a Flash based solution to one built using HTML5 technologies. We saw this as an opportunity to build on a growing standard, and to support the most platforms. In fall of 2010 we released our HTML5 photo editor which has evolved into the product we’re proud to share with you today. Widget Encapsulation During our initial foray into the online editor space, we took a straightforward approach by having API users launch our editor in a new page or window. This simplified integrations and allowed us to own the editing experience. When we rebuilt our editor in JavaScript, we took the opportunity to re-architect our API as well. Our first big change was making the editor embeddable. This meant that third party developers could load the editor on their own sites, maintaining user engagement while controlling their experience. We built out customization options that allowed the site owner to decide which tools appeared in the editor. A real estate site, for example, might not want its users adding mustache stickers to appliances in photos. Our editor, unlike many rich HTML widgets, does not require an iframe and is truly embedded into a hosting webpage. This posed many challenges during development, but the result is a more seamless, lightweight integration. Constructor API When we rebuilt our API, we took a leap by assuming that web developers integrating our editor would have experience with other JavaScript libraries and plugins. We built our API to use a Constructor method that accepts a configuration object to allow for the aforementioned tool customization. The configuration object is also used to configure callbacks, image URLs, language settings, etc., and allows us to continue building out our API without losing backwards compatibility. Simplifying the Save Process Saving image data is always a challenge in the browser, and can require various cross-browser workarounds. An obvious method would be to initiate a form post to the server and include the base64 image data in a hidden field. This breaks in Safari, where form fields have an undocumented value length limit. We worked around this by switching to an ajax post with the appropriate CORS headers to get around cross domain issues. In browsers that don’t support CORS, we fall back to the form post method. To hide this complexity from the developer, we’ve abstracted the save process completely. When a user saves an edited image, we temporarily save the image data to our own servers and return a public URL so the host application can download the image to their own. High Resolution Photos One of the coolest features of our editor is the high resolution image support — that being said, it certainly has a number of challenges. There’s the practical issue of limited real estate in the browser (keep an eye out for updates addressing this in the near future), as well as performance issues that are harder to quantify. Even in Flash based tools, the size of the image you can edit in the browser is limited by a number of gating factors: hardware specs, number of running processes, etc. To get around these client limitations, we’ve set a configurable maxSize on the editor and added a configuration field for an original-resolution version of the image to be edited: hiresUrl. When a hiresUrl is supplied, every user edit action is logged. On save, the aptly named “actionlist” is sent to our server along with the hiresUrl. When it hits our render farm, the actionlist is replayed on the high resolution image, and the final results are returned to the host site via a new hiresUrl. {\r\n    \"metadata\": {\r\n        \"imageorigsize\": [\r\n            800,\r\n            530\r\n        ]\r\n    },\r\n    \"actionlist\": [\r\n        {\r\n            \"action\": \"setfeathereditsize\",\r\n            \"width\": 800,\r\n            \"height\": 530\r\n        },\r\n        {\r\n            \"action\": \"flatten\"\r\n        },\r\n        {\r\n            \"action\": \"redeye\",\r\n            \"radius\": 5,\r\n            \"pointlist\": [\r\n                [545, 183], [546,183], [547,182], [548,181], [548,179], [548,177], [547,177], [545,177], [544,177], [543,177], [542,177], [541,179], [541,181], [541,183], [542,184]\r\n            ]\r\n        },\r\n        {\r\n            \"action\": \"redeye\",\r\n            \"radius\": 5,\r\n            \"pointlist\": [\r\n                [481, 191], [481,193], [481,195], [482,196], [483,197], [484,198], [485,197], [485,196], [485,193], [485,190], [485,189], [485,188], [484,188], [482,188], [480,189], [480,190], [480, 191]\r\n            ]\r\n        },\r\n        {\r\n            \"action\": \"sharpen\",\r\n            \"value\": 21.69312,\r\n            \"flatten\": true\r\n        }\r\n    ]\r\n} As a side note, we maintain feature parity across all of our platforms (mobile included) by prototyping new tools and filters in the JavaScript first, and then porting them to C for our render farm and Android, and then to Objective-C for our iPhone SDK. By maintaining feature parity and synchronizing output across platforms, we’re able to ensure that users get the edits they expect on their high resolution photos, and we keep the door open for future server-side support for our mobile SDKs where the original photo might not be stored on the device. Tools and Libraries We use some pretty awesome tools to help us maintain cross-browser compatibility. LESS CSS We moved a lot of the cross-browser concerns to build-time with LESS and a library of mix-ins inspired initially by Twitter Bootstrap, though the final result is wholly our own. LESS’s color math and variables let us achieve a textured and rounded look and feel while minimizing complexity during development. /* LESS */\r\n.avpw_inset_button_group {\r\n#gradient > .vertical(lighten(@conveyorBelt, 4%), darken(@conveyorBelt, 1%));\r\n.box-shadow(inset 0 0 4px darken(@conveyorBelt, 20%));\r\n.border-radius(8px);\r\n}\r\n\r\n/* EXPANDED */\r\n.avpw_inset_button_group {\r\n  background-color: #2a2a2a;\r\n  background-repeat: repeat-x;\r\n  background-image: -khtml-gradient(linear, left top, left bottom, from(#383838), to(#2a2a2a));\r\n  background-image: -moz-linear-gradient(top, #383838, #2a2a2a);\r\n  background-image: -ms-linear-gradient(top, #383838, #2a2a2a);\r\n  background-image: -webkit-gradient(linear, left top, left bottom, color-stop(0%, #383838), color-stop(100%, #2a2a2a));\r\n  background-image: -webkit-linear-gradient(top, #383838, #2a2a2a);\r\n  background-image: -o-linear-gradient(top, #383838, #2a2a2a);\r\n  background-image: linear-gradient(top, #383838, #2a2a2a);\r\n  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#383838', endColorstr='#2a2a2a', GradientType=0);\r\n  -webkit-box-shadow: inset 0 0 4px #000000;\r\n  -moz-box-shadow: inset 0 0 4px #000000;\r\n  box-shadow: inset 0 0 4px #000000;\r\n  -webkit-border-radius: 8px;\r\n  -moz-border-radius: 8px;\r\n  border-radius: 8px;\r\n} CSS3 With CSS3, we’ve just about managed a complete break from the DHTML effects of the past. The new UI uses CSS3 transitions and transforms wherever possible to remain future-proof. Flash Yes, our editor does indeed have a Flash fallback for browsers that lack certain HTML5 features (namely canvas ). We initially built the editor as a move away from Flash, but because of the legacy IE7 and IE8 userbases on our larger partner sites, we had to go back and rebuild certain components in Flash to support those browsers. We’ve architected the editor so that Flash is only being used where necessary. Some tools, such as draw, have been completely rebuilt in Flash; for others, like effects, the bitmap data is being exported and manipulated in JavaScript (using a reverse implementation of pibeca). This allows for code reuse, and enables us to build new features faster with more backwards compatibility. Future While the feedback for our editor has been overwhelmingly fantastic, we’re continuing to work hard building out new tools and features, and performance enhancements to our existing set.", "date": "2012-04-18,"},
{"website": "Flickr", "title": "Designing an OSM Map Style", "author": ["Ross Harmes"], "link": "https://code.flickr.net/2012/07/11/designing-an-osm-map-style/", "abstract": "With the recent change to our map system , we introduced a new map style for our OSM tiles. Since 2008, we’ve used the default OSM styles, which produces map tiles like this: This style is extremely good at putting a lot of information in front of you. OSM doesn’t know your intended purpose for the maps (navigation, orientation, exploration, city planning, disaster response, etc.), so they err on the side of lots of information. This is good, but with the introduction of TileMill , non-professional cartographers (like myself) can now easily change map styles to better suit our needs. Using TileMill, we decided to take a crack at designing a map that is better suited to Flickr. On Flickr, we use maps for a very specific purpose: to provide context for a photo. This means there are a lot of map features that we can leave out entirely. We can choose to hide features that are primarily used for navigation (ferry and train routes, bus stops) or for demarcation (city and county boundaries). Roads are useful as orientation tools, but certain road features (like exit numbers on highways) aren’t needed. In the end, we can reduce the data that the map shows to much smaller and more useful subset: This is the style provided by MapBox’s excellent OSM Bright . As a starting point, this gets us a long way towards our goal of an unobtrusive yet still useful map. We made a few changes to OSM Bright and released them on GitHub as our Pandonia map style. Here are a few examples of the changes we made: Toned down the road, land, and water colors, to allow greater contrast with the pink and blue dots that we use as markers Reduced the density of road and highway names, as well as city, town and state names Removed underground tram and rail line Removed land use overlays for residential, commercial, and industrial zones, as well as parking lots Removed state park overlays that overlapped the water This is how it looks: We tried a lot of different color combinations on the road to this style. Here is an animation of the different styles we tried, starting with OSM Bright. Here it is zoomed in a bit more: Over the next couple of weeks, we’ll be rolling out this style to all of the places where we use OSM tiles. These maps are still a work in progress. The world is a big place, and creating a unified style that works well for every single location is challenging. If you notice problems with our new map styles, please let us know !", "date": "2012-07-11,"},
{"website": "Flickr", "title": "Web workers and YUI", "author": ["Trevor Hartsell"], "link": "https://code.flickr.net/2012/06/06/web-workers-and-yui/", "abstract": "(Flickr is hiring! Check out our open job postings and what it’s like to work at Flickr .) Web workers are awesome. They’ll change the way you think about JavaScript. Chris posted an excellent writeup on how we do client-side Exif parsing in the new Uploader, which is how we can display thumbnails before uploading your photos to the Flickr servers. But parsing metadata from hundreds of files can be a little expensive. In the old days, we’d attempt to divide our expensive JS into smaller parts, using setTimeout to yield to the UI thread, crossing our fingers, and hoping that the user could still scroll and click when they wanted to. If that didn’t work, then the feature was simply too fancy for the web. Since then, a lot has happened. People started using better browsers. HTML got an orange logo. Web workers were discovered. So now we can run JavaScript in separate threads (“parallel execution environments”), without interrupting the standard UI stuff the browser is always working on. We just need to put our job code in a separate file, and instantiate a web worker. Without YUI For simple, one-off tasks, you can just write some JavaScript in a new file and upload it to your server. Then create a worker like this: var worker = new Worker('my_file.js');\r\n\r\nworker.addEventListener('message', function (e) {\r\n\t// do something with the message from the worker\r\n});\r\n\r\n// pass some data into the worker\r\nworker.postMessage({\r\n\tfoo: bar\r\n}); Of course, the worker thread won’t have access to anything in the main thread. You can post messages containing anything that’s JSON compatible, but not functions, cyclical references, or special objects like File references. That means any modules or helper functions you’ve defined in your main thread are out of bounds, unless you’ve also included them in your worker file. That can be a drag if you’re accustomed to working in a framework. With YUI Practically speaking, a worker thread isn’t very different from the main thread. Workers can’t access the DOM, and they have a top-level self object instead of window . But plenty of our existing JavaScript modules and helper functions would be very useful in a worker thread. Flickr is built on YUI. Its modular architecture is powerful and encourages clean, reusable code. We have a ton of small JS files—one per module—and the YUI Loader figures out how to put them all together into a single URL. If we want to write our worker code like we write our normal code, our worker file can’t be just my_file.js . It needs to be a full combo URL, with YUI running inside it. An aside for the brogrammers who have never seen modular JS in practice Loader dynamically loads script and css files for YUI modules as well as external modules. It includes the dependency information for the version of the library in use, and will automatically pull in dependencies for the modules requested. In development, we have one JS file per module. Let’s say photo.js , kitten.js , and puppy.js . A page full of kitten photos might require two of those modules. So we tell YUI that we want to use photo.js and kitten.js , and the YUI Loader appends a script node with a combo URL that looks something like this: <script src=\"/combo.php?photo.js&kitten.js\"> . On our server, combo.php finds the two files on disk and prints out the contents, which are immediately executed inside the script node. C-c-c-combo Of course, the main thread is already running YUI, which we can use to generate the combo URL required to create a worker. That URL needs to return the following: YUI.add() statements for any required modules. (Don’t forget yui-base) YUI.add() statement for the primary module with the expensive code. YUI.add() statement to execute the primary module. Ok, so how do we generate this combo URL? Like so: //\r\n// Make a reference to our original YUI configuration object,\r\n// with all of our module definitions and combo handler options.\r\n//\r\n// To make sure it's as clean as possible, we use a clone of the\r\n// object from before we passed it into YUI.\r\n//\r\n\r\nvar yconf = window.yconf; // global for demo purposes\r\n\r\n//\r\n// Y.Loader.resolve can be used to generate a combo URL with all\r\n// the YUI modules needed within the web worker. (YUI 3.5 or later)\r\n//\r\n// The YUI Loader will bypass any required modules that have\r\n// already been loaded in this instance, so in addition to the\r\n// clean configuration object, we use a new YUI instance.\r\n//\r\n\r\nvar Y2 = YUI(Y.merge(yconf));\r\n\r\nvar loader = new Y2.Loader({\r\n\t// comboBase must be on the same domain as the main thread\r\n\tcomboBase: '/local/combo/path/',\r\n\tcombine: true,\r\n\tignoreRegistered: true,\r\n\tmaxURLLength: 2048,\r\n\trequire: ['my_worker_module']\r\n});\r\n\r\nvar out = loader.resolve(true);\r\n\r\nvar combo_url = out.js[0]; Then, also in the main thread, we can start the worker instance: //\r\n// Use the combo URL to create a web worker.\r\n// This is when the combo URL is downloaded, parsed, \r\n// and executed.\r\n//\r\n\r\nvar worker = new window.Worker(combo_url); To start using YUI, we need to pass our YUI config object into the worker thread. That could have been part of the combo URL, but our YUI config is pretty specific to the particular page you’re on, so we need to reuse the same object we started with in the main thread. So we use postMessage to pass it from the main thread to the worker: //\r\n// Post the YUI config into the worker.\r\n// This is when the worker actually starts its work.\r\n//\r\n\r\nworker.postMessage({\r\n\tyconf: yconf\r\n}); Now we’re almost done. We just need to write the worker code that waits for our YUI config before using the module. So, at the bottom of the combo response, in the worker thread: self.addEventListener('message', function (e) {\r\n\r\n\tif (e.data.yconf) {\r\n\r\n\t\t//\r\n\t\t// make sure bootstrapping is disabled\r\n\t\t//\r\n\t\t\r\n\t\te.data.yconf.bootstrap = false;\r\n\r\n\t\t//\r\n\t\t// instantiate YUI and use it to execute the callback\r\n\t\t//\r\n\t\t\r\n\t\tYUI(e.data.yconf).use('my_worker_module', function (Y) {\r\n\r\n\t\t\t// do some hard work!\r\n\r\n\t\t});\r\n\r\n\t}\r\n\r\n}, false); Yeah, I know the back-and-forth between the main thread and the worker makes that look complicated. But it’s actually just a few steps: Main thread generates a combo URL and instantiates a Web Worker. Worker thread parses and executes the JS returned by that URL. Main thread posts the page’s YUI config into the worker thread. Worker thread uses the config to instantiate YUI and “use” the worker module. That’s it. Now get to work!", "date": "2012-06-6,"},
{"website": "Flickr", "title": "Group APIs", "author": ["jfanaian"], "link": "https://code.flickr.net/2012/05/24/group-apis/", "abstract": "With over 1.5 million groups , it’s no doubt that they are an important part of Flickr. Today, we’re releasing a few new ways to interact with groups using our API. Group Membership We are adding two new methods to manage group membership through the API. flickr.groups.join to join a group. Before calling this method, check if the group has rules using flickr.groups.getInfo . The user needs to agree to the rules before being able to join the group. Pass the accept_rules argument if the user accepted the rules. flickr.groups.leave to leave a group. The user’s photos can also be deleted when leaving the group by passing the delete_photos argument. Group Discussions We are also opening up group discussions in the API. You can now fetch a list of discussion topics for a group using flickr.groups.discuss.topics.getList , with sticky topics first, then regular topics sorted from newest to oldest. &lt;rsp stat=&quot;ok&quot;&gt;\r\n    &lt;topics group_id=&quot;46744914@N00&quot; iconserver=&quot;1&quot; iconfarm=&quot;1&quot; name=&quot;Tell a story in 5 frames (Visual story telling)&quot; members=&quot;12428&quot; privacy=&quot;3&quot; lang=&quot;en-us&quot; ispoolmoderated=&quot;1&quot; total=&quot;4621&quot; page=&quot;1&quot; per_page=&quot;2&quot; pages=&quot;2310&quot;&gt;\r\n        &lt;topic id=&quot;72157625038324579&quot; subject=&quot;A long time ago in a galaxy far, far away...&quot; author=&quot;53930889@N04&quot; authorname=&quot;Smallportfolio_jm08&quot; role=&quot;member&quot; iconserver=&quot;5169&quot; iconfarm=&quot;6&quot; count_replies=&quot;8&quot; can_edit=&quot;0&quot; can_delete=&quot;0&quot; can_reply=&quot;0&quot; is_sticky=&quot;0&quot; is_locked=&quot;&quot; datecreate=&quot;1287070965&quot; datelastpost=&quot;1336905518&quot;&gt;\r\n            &lt;message&gt; ... &lt;/message&gt;\r\n        &lt;/topic&gt;\r\n    &lt;/topics&gt;\r\n&lt;/rsp&gt; flickr.groups.discuss.topics.add to post a new topic to a group, passing a subject and the message content. Additionally, you can fetch a list of replies for a topic using flickr.groups.discuss.replies.getList , which includes the information for the topic along with all the replies, sorted from oldest to newest. &lt;rsp stat=&quot;ok&quot;&gt;\r\n    &lt;replies&gt;\r\n        &lt;topic topic_id=&quot;72157625038324579&quot; subject=&quot;A long time ago in a galaxy far, far away...&quot; group_id=&quot;46744914@N00&quot; iconserver=&quot;1&quot; iconfarm=&quot;1&quot; name=&quot;Tell a story in 5 frames (Visual story telling)&quot; author=&quot;53930889@N04&quot; authorname=&quot;Smallportfolio_jm08&quot; role=&quot;member&quot; author_iconserver=&quot;5169&quot; author_iconfarm=&quot;6&quot; can_edit=&quot;0&quot; can_delete=&quot;0&quot; can_reply=&quot;0&quot; is_sticky=&quot;0&quot; is_locked=&quot;&quot; datecreate=&quot;1287070965&quot; datelastpost=&quot;1336905518&quot; total=&quot;8&quot; page=&quot;1&quot; per_page=&quot;3&quot; pages=&quot;2&quot;&gt;\r\n            &lt;message&gt; ... &lt;/message&gt;\r\n        &lt;/topic&gt;\r\n        &lt;reply id=&quot;72157625163054214&quot; author=&quot;41380738@N05&quot; authorname=&quot;BlueRidgeKitties&quot; role=&quot;member&quot; iconserver=&quot;2459&quot; iconfarm=&quot;3&quot; can_edit=&quot;0&quot; can_delete=&quot;0&quot; datecreate=&quot;1287071539&quot; lastedit=&quot;0&quot;&gt;\r\n            &lt;message&gt; ... &lt;/message&gt;\r\n        &lt;/reply&gt;\r\n    &lt;/replies&gt;\r\n&lt;/rsp&gt; flickr.groups.discuss.replies.add to post a reply to a topic, passing the message content. flickr.groups.discuss.replies.edit to edit a reply, passing the updated message. flickr.groups.discuss.replies.delete to delete a reply. You can only edit and delete replies when authorized as the owner of the reply. For now, it is not possible to edit or delete a topic through the API. If you have any questions, comments, concerns, or just want to chat about these methods or anything else related to the API, please join the Flickr Developer mailing list . Photos from fofurasfelinas and larissa_allen .", "date": "2012-05-24,"},
{"website": "Flickr", "title": "We saved you a step…", "author": ["Eric Gelinas"], "link": "https://code.flickr.net/2012/10/24/2273/", "abstract": "It seems when we launched version 2.0 of our Flickr shapes , we posted them with a flaw which made them useless to most popular geo applications. Awwwww… Luckily, Christopher Manning wrote a python script which makes them useful. Yaaaayyyyy! The least we can do is post an update which has already been christopher-manning-ified, So, we are very happy to announce version 2.0.1 of the Flickr shape files which can be downloaded here: http://www.flickr.com/services/shapefiles/2.0.1/ Look, it works: Flickr Shapes 2.0.1 in TileMill A very hearty THANKS! from your friends at Flickr, Christopher.", "date": "2012-10-24,"},
{"website": "Flickr", "title": "Join the Flickr Frontend team tonight at the SF Web Performance meet up!", "author": ["Ross Harmes"], "link": "https://code.flickr.net/2012/10/25/join-the-flickr-frontend-team-tonight-at-the-sf-web-performance-meet-up/", "abstract": "Team Tinfoil by waferbaby We will be hosting the SF Web Performance meet up tonight at 7pm at Citizen Space . Come join us for pizza, drinks, and these great talks: Using Web Workers for fun and profit: Parsing Exif in the client, by Chris Berry Exif, exchangeable image file format, describes various sets of metadata stored in a photo. Really interesting metadata, like image titles, descriptions, lens focal lengths, camera types, image orientation, even GPS data! I’ll go over the methods to extracting this data on the front-end, in real-time, using web workers. The Grid: How we show 10,000 photos on a page without crashing your browser, by Scott Schiller Flickr’s latest Web-based Uploadr interface uses HTML5 APIs to push bytes en masse. Its real power, however, is the UI which enables users to add and edit the metadata of hundreds of photos while they are uploading in the background. Handling the selection, display and management of large numbers of photos in a browser UI meant that the Uploadr project needed to be designed for scalability from the ground up. This talk will go into some of the details of the Uploadr “Grid” UI, technical notes and performance findings made during its development. Optimizing Touch Performance, by Stephen Woods Touch interfaces are amazing. Touch devices are amazingly slow.  Stephen Woods will share hard-won advice for building responsive touch-based interfaces using HTML5, CSS, and JavaScript. He also reveals how Star Trek: The Next Generation predicted the need for instant user feedback in a touch-based UI and how Tivos slow UI was made bearable by a simple “bloop” sound. See you there!", "date": "2012-10-25,"},
{"website": "Flickr", "title": "Lessons Learned from the Flickr Touch Lightbox", "author": ["ysaw"], "link": "https://code.flickr.net/2011/07/20/lessons-learned-from-the-flickr-touch-lightbox/", "abstract": "Bye bye Kodachrome by e_pics Recently we released the Flickr Lightbox for iPad, iPhone and Android. We managed to create a pretty responsive interface. It took us a while to get there, and we learned a lot doing it. In this post we’d like to share a few useful key lessons we took away from the project. Get Your Viewport Tag Right The viewport tag is actually not that well understood, but you have to get it right, or everything is just going to be completely confusing. For the lightbox it looks like this: &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width; initial-scale=1.0; maximum-scale=1.0; user-scalable=0;&quot;&gt; This is the way we made sure that we could reliably position elements by pixel, and properly handle device rotation without fear. Other places on the web describe the details of how this tag works, but for us the most important was “maximum-scale=1”. This is because when you rotate the iphone it scales the content normally, unless you specify a max scale. Positioning becomes very difficult when the phone is scaling the interface you carefully crafted to fit on the device. Simplify the DOM On mobile WebKit you can use CSS for much, much more than you can on the desktop where you need to support bad browsers. This makes it very easy to clean the DOM down to the minimum. We did this on the lightbox by starting from scratch (the DOM for the desktop lightbox is quite complex to support a cross-browser layout.) We also actually nuke the existing DOM when the lightbox opens. Keep It Responsive I can not emphasize this enough. Of course, any UI must be responsive. But a touch UI doubly so. Apple clearly “got” this in the development of the first iPhone. Possibly because there is no “click” sound when you interact with the device it is very hard to tell if the device has registered your interaction without immediate feedback. Further, if there is even a tiny delay in how the device responds to touch interaction it feels clunky. Much clunkier than a slightly glitchy desktop UI. One of the things keep us from supporting the desktop lightbox on touch devices was that it felt very slow and clunky. Once we figured out that this was a matter of percieved responsiveness it was pretty clear where we needed to focus: percieved performance. From a user experience standpoint this means that any interaction needs to give the user feedback. In the lightbox this means that when the user swipes the photo always moves with their finger. When the user hits the end of the photos, rather than not responding anymore, the photo continues to move but snaps back (with CSS transitions) to the last point. Of course, for this interaction to work it needs to be very fast, any delay feels very awkward. So from an optimization standpoint we went after the performance of the swipe animation above everything else. The first thing to do is to use 3d CSS transforms. All the touch devices have 3d acceleration hardware which makes it possible to move the photo much faster than with just the CPU. The additional benefit of course is that when using transforms the animation does not block JS execution at all. The code looks something like this: distance = e.touches[0].pageX - startX;\r\n    absDistance = Math.abs(distance);\r\n    direction = (absDistance === distance) ? 1 : -1;\r\n\r\n    if (absDistance &amp;gt; 2) {\r\n        thisPosition.setStyle('transform', 'translate3d('+distance+'px, 0px, 0px)');\r\n    } When we first tested this, however, we found performance quite disappointing. Another team pointed us to a trick: don’t use <img> tags. We got a huge performance boost when using <div> tags with the photo as a background image. The next thing we noticed was a slight but perceptible delay between the touch event and the movement of the photo element. After some profiling we found that the YUI event abstraction was actually taking enough time to be perceptible, so we switched to native event handling. Which lead us to further optimization along the same lines: do as little as possible. Most things you do in JS (with some special exceptions) are blocking. So any work you do while touch events are being handled necessarily delays the feedback the user needs to know that their touches are being registered. We went through the code path that happened during touch events. Anything that could wait until “touchend” was deferred there. The last problem to solve was that the browser would crash with more than twenty or so slides loaded. It seems that the iPhone browser dies very quickly when it runs out of memory, especially when using 3d acceleration. So we implemented a simple garbage collector for the slide nodes: //remove all slides more than 10 positions away\r\n    function pruneSlideNodes() {\r\n        if (inTransition || moving) {\r\n            if (pruneHandle &amp;amp;&amp;amp; pruneHandle.cancel) {\r\n                pruneHandle.cancel();\r\n            }\r\n            pruneHandle = Y.later(500, this, pruneSlideNodes); //wait\r\n            return;\r\n        }\r\n        \r\n        positionManager.each(function (value, key) {\r\n            if ((Math.abs(key - parseInt(currentPosition,10)) &amp;gt; 10) &amp;amp;&amp;amp; value.id) {\r\n                \r\n                Y.one('#' + value.id).remove();\r\n                delete(value.id);\r\n            }\r\n            \r\n        }, this);\r\n    } Final note: Moving to YUI 3 has been huge for us, even on mobile tasks. The mobile lightbox takes advantage of several modules created for the desktop, most importantly a “model” module we created to manage what we call photo “contexts”. This meant that the logic of displaying slides is the same in all places, the view/controller code was all that we needed to create for this. Which also means that this logic exists in just one file. Like this post? Have a love of online photography? Want to work with us? Flickr is hiring engineers , designers and product managers in our San Francisco office. Find out more at flickr.com/jobs .", "date": "2011-07-20,"},
{"website": "Flickr", "title": "Parsing Exif client-side using JavaScript", "author": ["cberry"], "link": "https://code.flickr.net/2012/06/01/parsing-exif-client-side-using-javascript-2/", "abstract": "What is Exif? A short primer. Exif is short for Exchangeable image file format. A standard that specifies the formats to be used in images, sounds, and tags used by digital still cameras. In this case we are concerned with the tags standard and how it is used in still images. How Flickr currently parses Exif data. Currently we parse an image’s Exif data after it is uploaded to the Flickr servers and then expose that data on the photo’s metadata page ( http://www.flickr.com/photos/rubixdead/7192796744/meta/in/photostream ). This page will show you all the data recorded from your camera when a photo was taken, the camera type, lens, aperture, exposure settings, etc. We currently use ExifTool ( http://www.sno.phy.queensu.ca/~phil/exiftool/ ) to parse all of this data, which is a robust, albeit server side only, solution. An opportunity to parse Exif data on the client-side Sometime in the beginning phases of spec’ing out the Uploadr project we realized modern browsers can read an image’s data directly from the disk, using the FileReader API ( http://www.w3.org/TR/FileAPI/#FileReader-interface ). This lead to the realization that we could parse Exif data while the photo is being uploaded, then expose this to the user while they are editing their photos in the Uploadr before they even hit the Upload button. Why client-side Exif? Why would we need to parse Exif on the client-side, if we are parsing it already on the server-side? Parsing Exif on the client-side is both fast and efficient. It allows us to show the user a thumbnail without putting the entire image in the DOM, which uses a lot of memory and kills performance. Users can also add titles, descriptions, and tags in a third-party image editing program saving the metadata into the photo’s Exif. When they drag those photos into the Uploadr, BOOM, we show them the data they have already entered and organized, eliminating the need to enter it twice. Using Web Workers We started doing some testing and research around parsing Exif data by reading a file’s bytes in JavaScript. We found a few people had accomplished this already, it’s not a difficult feat, but a messy one. We then quickly realized that making a user’s browser run through 10 megabytes of data can be a heavy operation. Web workers allow us to offload the parsing of byte data into a separate cpu thread. Therefore freeing up the user’s browser, so they can continue using Uploadr while Exif is being parsed. Exif Processing Flow Once we had a web worker prototype setup, we next had to write to code that would parse the actual bytes. The first thing we do is pre-fetch the JavaScript used in the web worker thread. Then when a user adds an image to the Uploadr we create event handlers for the worker. When a web worker calls postMessage() we capture that, check for Exif data and then display it on the page. Any additional processing is also done at this time. Parsing XMP data, for example, is done outside of the worker because the DOM isn’t available in worker threads. Using Blob.slice() we pull out the first 128kb of the image to limit load on the worker and speed things up. The Exif specification states that all of the data should exist in the first 64kb, but IPTC sometimes goes beyond that, especially when formatted as XMP. if (file.slice) {\r\n\tfilePart = file.slice(0, 131072);\r\n} else if (file.webkitSlice) {\r\n\tfilePart = file.webkitSlice(0, 131072);\r\n} else if (file.mozSlice) {\r\n\tfilePart = file.mozSlice(0, 131072);\r\n} else {\r\n\tfilePart = file;\r\n} We create a new FileReader object and pass in the Blob slice to be read. An event handler is created at this point to handle the reading of the Blob data and pass it into the worker. FileReader.readAsBinaryString() is called, passing in the blob slice, to read it as a binary string into the worker. binaryReader = new FileReader();\r\n\r\nbinaryReader.onload = function () {\r\n\r\n\tworker.postMessage({\r\n\t\tguid: guid,\r\n\t\tbinary_string: binaryReader.result\r\n\t});\r\n\r\n};\r\n\r\nbinaryReader.readAsBinaryString(filePart); The worker receives the binary string and passes it through multiple Exif processors in succession. One for Exif data, one for XMP formatted IPTC data and one for unformatted IPTC data. Each of the processors uses postMessage() to post the Exif data back out and is caught by the module. The data is displayed in the uploadr, which is later sent along to the API with the uploaded batch. On asynchronous Exif parsing When reading in Exif data asynchronously we ran into a few problems, because processing does not happen immediately. We had to prevent the user from sorting their photos until all the Exif data was parsed, namely the date and time for “order by” sorting. We also ran into a race condition when getting tags out of the Exif data. If a user had already entered tags we want to amend those tags with what was possibly saved in their photo. We also update the Uploadr with data from Exiftool once it is processed on the back-end. The Nitty Gritty: Creating EXIF Parsers and dealing with typed arrays support pre-electronic binary code by dret Creating an Exif parser is no simple task, but there are a few things to consider: What specification of Exif are we dealing with? (Exif, XMP, IPTC, any and all of the above?) When processing the binary string data, is it big or little endian? How do we read binary data in a browser? Do we have typed arrays support or do we need to create our own data view? First things first, how do we read binary data? As we saw above our worker is fed a binary string, meaning this is a stream of ASCII characters representing values from 0-255. We need to create a way to access and parse this data. The Exif specification defines a few different data value types we will encounter: 1 = BYTE An 8-bit unsigned integer 2 = ASCII An 8-bit byte containing one 7-bit ASCII code. The final byte is terminated with NULL. 3 = SHORT A 16-bit (2-byte) unsigned integer 4 = LONG A 32-bit (4-byte) unsigned integer 5 = RATIONAL Two LONGs. The first LONG is the numerator and the second LONG expresses the denominator. 7 = UNDEFINED An 8-bit byte that can take any value depending on the field definition 9 = SLONG A 32-bit (4-byte) signed integer (2’s complement notation) 10 = SRATIONAL Two SLONGs. The first SLONG is the numerator and the second SLONG is the denominator So, we need to be able to read an unsigned int (1 byte), an unsigned short (2 bytes), an unsigned long (4 bytes), an slong (4 bytes signed), and an ASCII string. Since the we read the stream as a binary string it is already in ASCII, that one is done for us. The others can be accomplished by using typed arrays, if supported, or some fun binary math. Typed Array Support Now that we know what types of data we are expecting, we just need a way to translate the binary string we have into useful values. The easiest approach would be typed arrays ( https://developer.mozilla.org/en/JavaScript_typed_arrays ), meaning we can create an ArrayBuffer using the string we received from from the FileReader, and then create typed arrays, or views, as needed to read values from the string. Unfortunately array buffer views do not support endianness, so the preferred method is to use DataView ( http://www.khronos.org/registry/typedarray/specs/latest/#8 ), which essentially creates a view to read into the buffer and spit out various integer types. Due to lack of great support, Firefox does not support DataView and Safari’s typed array support can be slow, we are currently using a combination of manual byte conversion and ArrayBuffer views. var arrayBuffer = new ArrayBuffer(this.data.length);\r\nvar int8View = new Int8Array(arrayBuffer);\r\n\r\nfor (var i = 0; i &amp;lt; this.data.length; i++) {\r\n\tint8View[i] = this.data[i].charCodeAt(0);\r\n}\r\n\r\nthis.buffer = arrayBuffer;\r\n\r\nthis.getUint8 = function(offset) {\r\n\tif (compatibility.ArrayBuffer) {\r\n\r\n\treturn new Uint8Array(this.buffer, offset, 1)[0];\r\n\t}\r\n\telse {\r\n\t\treturn this.data.charCodeAt(offset) &amp;amp; 0xff;\r\n\t}\r\n} Above we are creating an ArrayBuffer of length to match the data being passed in, and then creating a view consisting of 8-bit signed integers which allows us to store data into the ArrayBuffer from the data passed in. We then process the charCode() at each location in the data string passed in and store it in the array buffer via the int8View. Next you can see an example function, getUint8(), where we get an unsigned 8-bit value at a specified offset. If typed arrays are supported we use a Uint8Array view to access data from the buffer at an offset, otherwise we just get the character code at an offset and then mask the least significant 8 bits. To read a short or long value we can do the following: this.getLongAt = function(offset,littleEndian) {\r\n\r\n\t//DataView method\r\n\treturn new DataView(this.buffer).getUint32(offset, littleEndian);\r\n\r\n\t//ArrayBufferView method always littleEndian\r\n\tvar uint32Array = new Uint32Array(this.buffer);\r\n\treturn uint32Array[offset];\r\n\r\n\t//The method we are currently using\r\n\tvar b3 = this.getUint8(this.endianness(offset, 0, 4, littleEndian)),\r\n\tb2 = this.getUint8(this.endianness(offset, 1, 4, littleEndian)),\r\n\tb1 = this.getUint8(this.endianness(offset, 2, 4, littleEndian)),\r\n\tb0 = this.getUint8(this.endianness(offset, 3, 4, littleEndian));\r\n\r\n\treturn (b3 * Math.pow(2, 24)) + (b2 &amp;lt;&amp;lt; 16) + (b1 &amp;lt;&amp;lt; 8) + b0;\r\n\r\n} The DataView method is pretty straight forward, as is the ArrayBufferView method, but without concern for endianness. The last method above, the one we are currently using, gets the unsigned int at each byte location for the 4 bytes. Transposes them based on endianness and then creates a long integer value out of it. This is an example of the custom binary math needed to support data view in Firefox. When originally beginning to build out the Exif parser I found this jDataView ( https://github.com/vjeux/jDataView ) library written by Christopher Chedeau aka Vjeux ( http://blog.vjeux.com/ ). Inspired by Christopher’s jDataView module we created a DataView module for YUI. Translating all of this into useful data There are a few documents you should become familiar with if you are considering writing your own Exif parser: The Exif specification in its entirety: http://www.kodak.com/global/plugins/acrobat/en/service/digCam/exifStandard2.pdf The TIFF 6.0 specification which has a good breakdown of the tags and Tiff header: http://partners.adobe.com/public/developer/en/tiff/TIFF6.pdf A breakdown of the Exif specification originally put together by TsuruZoh Tachibanaya, but hosted now at MIT: http://www.media.mit.edu/pia/Research/deepview/exif.html The diagram above is taken straight from the Exif specification section 4.5.4, it describes the basic structure for Exif data in compressed JPEG images. Exif data is broken up into application segments (APP0, APP1, etc.). Each application segment contains a maker, length, Exif identification code, TIFF header, and usually 2 image file directories (IFDs). These IFD subdirectories contain a series of tags, of which each contains the tag number, type, count or length, and the data itself or offset to the data. These tags are described in Appendix A of the TIFF6 Spec , or at Table 41 JPEG Compressed (4:2:0) File APP1 Description Sample in the Exif spec and also broken down on the Exif spec page created by TsuruZoh Tachibanaya. Finding APP1 The first thing we want to find is the APP1 marker, so we know we are in the right place. For APP1, this is always the 2 bytes 0xFFE1, We usually check the last byte of this for the value 0xE1, or 225 in decimal, to prevent any endianness problems. The next thing we want to know is the size of the APP1 data, we can use this to optimize and know when to stop reading, which is also 2 bytes. Next up is the Exif header, which is always the 4 bytes 0x45, 0x78, 0x69, 0x66, or “Exif” in ASCII, which makes it easy. This is always followed up 2 null bytes 0x0000. Then begins the TIFF header and then the 0th IFD, where our Exif is stored, followed by the 1st IFD, which usually contains a thumbnail of the image. We are concerned with application segment 1 (APP1). APP2 and others can contain other metadata about this compressed image, but we are interested in the Exif attribute information. Wherefore art thou, TIFF header? Once we know we are at APP1 we can move on to the TIFF header which starts with the byte alignment, 0x4949 (II, Intel) or 0x4D4D (MM, Motorola), Intel being little endian and Motorola being big endian. Then we have the tag marker, which is always 0x2A00 (or 0x002A for big endian): “an arbitrary but carefully chosen number (42) that further identifies the file as a TIFF file”. Next we have the offset to the first IFD, which is usually 0x08000000, or 8 bytes from the beginning of the TIFF header (The 8 bytes: 0x49 0x49 0x2A 0x00 0x08 0x00 0x00 0x00). Now we can begin parsing the 0th IFD! The diagram above (taken from the TIFF6.0 specification found here: http://partners.adobe.com/public/developer/en/tiff/TIFF6.pdf ), shows the structure of the TIFF header, the following IFD and a directory entry contained within the IFD. The IFD starts off with the number of directory entries in the IFD, 2 bytes, then follows with all of the directory entries and ends with the offset to the next IFD if there is one. Each directory entry is 12 bytes long and comprised of 4 parts: the tag number, the data format, the number of components, and the data itself or an offset to the data value in the file. Then follows the offset to the next IFD which is again 8 bytes. Example: Processing some real world bytes Let’s run through an example below! I took a screen shot from hexfiend ( http://ridiculousfish.com/hexfiend/ , which is an awesome little program for looking at raw hex data from any file, I highly recommend it) and highlighted the appropriate bytes from start of image (SOI) to some tag examples. This is the first 48 bytes of the image file. I’ve grouped everything into 2 byte groups and 12 byte columns, because IFD entries are 12 bytes it makes it easier to read. You can see the start of image marker (SOI), APP1 mark and size, “Exif” mark and null bytes. Next is the beginning of the TIFF header including byte align, the 42 TIFF verification mark, the offset to the 0th IFD, the number of directory entries, and then the first 2 directory entries. These entries are in little endian and I wrote them out as big endian to make them easier to read. Both of these first entries are of ASCII type, which always point to an offset in the file and ends with a null terminator byte. Writing code to parse Exif Now that we understand the tag structure and what we are looking for in our 128k of data we sliced from the beginning of the image, we can write some code to do just that. A lot of insipration for this code comes from an exif parser written by Jacob Seidelin, http://blog.nihilogic.dk , the original you can find here: http://www.nihilogic.dk/labs/exif/exif.js . We used a lot of his tag mapping objects to translate the Exif tag number values into tag names as well as his logic that applies to reading and finding Exif data in a binary string. First we start looking for the APP1 marker, by looping through the binary string recording our offset and moving it up as we go along. if (dataview.getByteAt(0) != 0xFF || dataview.getByteAt(1) != 0xD8) {\r\n\treturn;\r\n}\r\nelse {\r\n\toffset = 2;\r\n\tlength = dataview.length;\r\n\t\r\n\twhile (offset &amp;lt; length) {\r\n\t\tmarker = dataview.getByteAt(offset+1);\r\n\t\tif (marker == 225) {\r\n\t\t\treadExifData(dataview, offset + 4, dataview.getShortAt(offset+2, true)-2);\r\n\t\t\tbreak;\r\n\t\t}\r\n\t\telse if(marker == 224) {\r\n\t\t\toffset = 20;\r\n\t\t}\r\n\t\telse {\r\n\t\t\toffset += 2 + dataview.getShortAt(offset+2, true);\r\n\t\t}\r\n\t}\r\n} We check for a valid SOI marker (0xFFD8) and then loop through the string we passed in. If we find the APP1 marker (225) we start reading Exif data, if we find a APP0 marker (224) we move the offset up by 20 and continue reading, otherwise we move the offset up by 2 plus the length of the APP data segment we are at, because it is not APP1, we are not interested. Once we find what we are looking for we can look for the Exif header, endianness, the TIFF header, and look for IFD0. function readExifData(dataview, start, length) {\r\n\r\n\tvar littleEndian;\r\n\tvar TIFFOffset = start + 6;\r\n\r\n\tif (dataview.getStringAt(iStart, 4) != &quot;Exif&quot;) {\r\n\t\treturn false;\r\n\t}\r\n\r\n\tif (dataview.getShortAt(TIFFOffset) == 0x4949) {\r\n\t\tlittleEndian = true;\r\n\t\tself.postMessage({msg:&quot;----Yes Little Endian&quot;});\r\n\t}\r\n\telse if (dataview.getShortAt(TIFFOffset) == 0x4D4D) {\r\n\t\tlittleEndian = false;\r\n\t\tself.postMessage({msg:&quot;----Not Little Endian&quot;});\r\n\t}\r\n\telse {\r\n\t\treturn false;\r\n\t}\r\n\r\n\tif (dataview.getShortAt(TIFFOffset+2, littleEndian) != 0x002A) {\r\n\t\treturn false;\r\n\t}\r\n\r\n\tif (dataview.getLongAt(TIFFOffset+4, littleEndian) != 0x00000008) {\r\n\t\treturn false;\r\n\t}\r\n\r\n\tvar tags = ExifExtractorTags.readTags(dataview, TIFFOffset, TIFFOffset+8, ExifExtractorTags.Exif.TiffTags, littleEndian); This is the first part of the readExifData function that is called once we find our APP1 segment marker. We start by verifying the Exif marker, then figuring out endianness, then checking if our TIFF header verification marker exists (42), and then getting our tags and values by calling ExifExtractorTags.readTags. We pass in the dataview to our binary string, the offset, the offset plus 8, which bypasses the TIFF header, the tags mapping object, and the endianness. Next we pass that data into a function that creates an object which maps all of the tag numbers to real world descriptions, and includes maps for tags that have mappable values. this.readTags = function(dataview, TIFFStart, dirStart, strings, littleEndian) {\r\n\tvar entries = dataview.getShortAt(dirStart, littleEndian);\r\n\tvar tags = {};\r\n\tvar i;\r\n\r\n\tfor (i = 0; i &amp;lt; entries; i++) {\r\n\t\tvar entryOffset = dirStart + i*12 + 2;\r\n\t\tvar tag = strings[dataview.getShortAt(entryOffset, littleEndian)];\r\n\r\n\t\ttags[tag] = this.readTagValue(dataview, entryOffset, TIFFStart, dirStart, littleEndian);\r\n\t}\r\n\r\n\tif(tags.ExifIFDPointer) {\r\n\t\tvar entryOffset = dirStart + i*12 + 2;\r\n\t\tvar IFD1Offset = dataview.getLongAt(entryOffset,littleEndian);\r\n\r\n\t\ttags.IFD1Offset = IFD1Offset;\r\n\t}\r\n\r\n\treturn tags;\r\n} This function is quite simple, once we know where we are at of course. For each entry we get the tag name from our tag strings and create a key on a tags object with a value of the tag. If there is an IFD1, we store that offset in the tags object as well. The readTagValue function takes the dataview object, the entry’s offset, the TIFF starting point, the directory starting point (TIFFStart + 8), and then endianness. It returns the tag’s value based on the data type (byte, short, long, ASCII). We return a tags object which has keys and values for various Exif tags that were found in the IFD. We check if ExifIFDPointer exists on this object, if so, we have IFD entries to pass back out of the worker and show the user. We also check for GPS data and an offset to the next IFD, IFD1Offset, if that exists we know we have another IFD, which is usually a thumbnail image. if (tags.ExifIFDPointer) {\r\n\r\n\tvar ExifTags = ExifExtractorTags.readTags(dataview, TIFFOffset, TIFFOffset + tags.ExifIFDPointer, ExifExtractorTags.Exif.Tags, littleEndian);\r\n\r\n\tfor (var tag in ExifTags) {\r\n\t\tswitch (tag) {\r\n\t\t\tcase &quot;LightSource&quot; :\r\n\t\t\tcase &quot;Flash&quot; :\r\n\t\t\tcase &quot;MeteringMode&quot; :\r\n\t\t\tcase &quot;ExposureProgram&quot; :\r\n\t\t\tcase &quot;SensingMethod&quot; :\r\n\t\t\tcase &quot;SceneCaptureType&quot; :\r\n\t\t\tcase &quot;SceneType&quot; :\r\n\t\t\tcase &quot;CustomRendered&quot; :\r\n\t\t\tcase &quot;WhiteBalance&quot; :\r\n\t\t\tcase &quot;GainControl&quot; :\r\n\t\t\tcase &quot;Contrast&quot; :\r\n\t\t\tcase &quot;Saturation&quot; :\r\n\t\t\tcase &quot;Sharpness&quot; :\r\n\t\t\tcase &quot;SubjectDistanceRange&quot; :\r\n\t\t\tcase &quot;FileSource&quot; :\r\n\t\t\t\tExifTags[tag] = ExifExtractorTags.Exif.StringValues[tag][ExifTags[tag]];\r\n\t\t\t\tbreak;\r\n\t\t\tcase &quot;ExifVersion&quot; :\r\n\t\t\tcase &quot;FlashpixVersion&quot; :\r\n\t\t\t\tExifTags[tag] = String.fromCharCode(ExifTags[tag][0], ExifTags[tag][1], ExifTags[tag][2], ExifTags[tag][3]);\r\n\t\t\t\tbreak;\r\n\t\t\tcase &quot;ComponentsConfiguration&quot; :\r\n\t\t\t\tExifTags[tag] =\r\n\t\t\t\t\tExifExtractorTags.Exif.StringValues.Components[ExifTags[tag][0]]\r\n\t\t\t\t\t+ ExifExtractorTags.Exif.StringValues.Components[ExifTags[tag][1]]\r\n\t\t\t\t\t+ ExifExtractorTags.Exif.StringValues.Components[ExifTags[tag][2]]\r\n\t\t\t\t\t+ ExifExtractorTags.Exif.StringValues.Components[ExifTags[tag][3]];\r\n\t\t\t\tbreak;\r\n\t\t}\r\n\t\t\r\n\t\ttags[tag] = ExifTags[tag];\r\n\t}\r\n} This is the rest of the readTags function, basically we are checking if ExifIFDPointer exists and then reading tags again at that offset pointer. Once we get another tags object back, we check to see if that tag has a value that needs to be mapped to a readable value. For example if the Flash Exif tag returns 0x0019 we can map that to “Flash fired, auto mode”. if(tags.IFD1Offset) {\r\n\tIFD1Tags = ExifExtractorTags.readTags(dataview, TIFFOffset, tags.IFD1Offset + TIFFOffset, ExifExtractorTags.Exif.TiffTags, littleEndian);\r\n\t\r\n\tif(IFD1Tags.JPEGInterchangeFormat) {\r\n\t\treadThumbnailData(dataview, IFD1Tags.JPEGInterchangeFormat, IFD1Tags.JPEGInterchangeFormatLength, TIFFOffset, littleEndian);\r\n\t}\r\n}\r\n\r\nfunction readThumbnailData(dataview, ThumbStart, ThumbLength, TIFFOffset, littleEndian) {\r\n\r\n\tif (dataview.length &amp;lt; ThumbStart+TIFFOffset+ThumbLength) {\r\n\t\treturn;\r\n\t}\r\n\r\n\tvar data = dataview.getBytesAt(ThumbStart+TIFFOffset,ThumbLength);\r\n\tvar hexData = new Array();\r\n\tvar i;\r\n\r\n\tfor(i in data) {\r\n\t\tif (data[i] &amp;lt; 16) {\r\n\t\t\thexData[i] = &quot;0&quot;+data[i].toString(16);\r\n\t\t}\r\n\t\telse {\r\n\t\t\thexData[i] = data[i].toString(16);\r\n\t\t}\r\n\t}\r\n\r\n\tself.postMessage({guid:dataview.guid, thumb_src:&quot;data:image/jpeg,%&quot;+hexData.join('%')});\r\n} The directory entry for the thumbnail image is just like the others. If we find the IFD1 offset at the end of IFD0, we pass the data back into the readTags function looking for two specific tags: JPEGInterchangeFormat (the offset to the thumbnail) and JPEGInterchangeFormatLength (the size of the thumbnail in bytes). We read in the correct amount of raw bytes at the appropriate offset, convert each byte into hex, and pass it back as a data URI to be inserted into the DOM showing the user a thumbnail while their photo is being uploaded. As we get data back from the readTags function, we post a message out of the worker with the tags as an object. Which will be caught caught by our event handlers from earlier, shown the user, and stored as necessary to be uploaded when the user is ready. We use this same process to parse older IPTC data. Essentially we look for an APP14 marker, a Photoshop 3.0 marker, a “8BIM” marker, and then begin running through the bytes looking for segment type, size, and data. We map the segment type against a lookup table to get the segment name and get size number of bytes at the offset to get the segment data. This is all stored in a tags object and passed out of the worker. XMP data is a little different, even easier. Basically we look for the slice of data surrounded by the values “<x:xmpmeta” to “</x:xmpmeta>” in the binary string, then pass that out of the worker to be parsed via Y.DataType.XML.parse(). Conclusion In conclusion the major steps we take to process an image’s Exif are: Initialize a web worker Get a file reference Get a slice of the file’s data Read a byte string Look for APP1/APP0 markers Look for Exif and TIFF header markers Look for IFD0 and IFD1 Process entries from IFD0 and IFD1 Pass data back out of the worker That is pretty much all there is to reading Exif! The key is to be very forgiving in the parsing of Exif data, because there are a lot of different cameras out there and the format has changed over the years. One final note: Web workers have made client-side Exif processing feasible at scale. Tasks like this can be performed without web workers, but run the risk of locking the UI thread – certainly not ideal for a web app that begs for user interaction. Like this post? Have a love of online photography? Want to work with us? Flickr is hiring engineers , designers and product managers in our San Francisco office. Find out more at flickr.com/jobs .", "date": "2012-06-1,"},
{"website": "Flickr", "title": "Building The Flickr Web Uploadr: The Grid", "author": ["Scott Schiller"], "link": "https://code.flickr.net/2012/05/11/building-the-flickr-web-uploadr-the-grid/", "abstract": "The new Flickr Web Uploadr is the result of a good amount of prototyping, research and good old-fashioned testing across the team that built it. This article goes into some of the details behind the “grid” – the area where photo thumbnails are shown – and sheds a little light on some of the thinking and logic behind the scenes. It’s a little lengthy, but don’t worry, there are pictures! In April 2012 , Flickr started rolling out its new web-based upload UI to the masses. We’re stoked to see it out there, and user feedback has been overwhelmingly positive. The product is an ongoing work in progress and enhancements are still being added, but the core is quite well-established and the experience is a significant upgrade over the one provided by the previous web-based uploadr. The new Flickr Web Uploadr. It’s powerful, it’s got a dark background, and it’s fast . The new uploadr has also simply been fun to work on; there are numerous interesting challenges in terms of UI, interactions, performance and sheer scale on the front-end that we had to feel confident in tackling before we were able to commit to moving forward with the project. Building The Grid: Prototypes Initial discussions about the new Flickr uploadr weren’t too detailed, because I think everyone already had a pretty good idea of what we wanted to see in a browser: Something more desktop-like, feature-wise (like our older XUL-based Flickr Uploadr application) that would load and show photo thumbnails in a grid arrangement, with a desktop-like selection and batch editing model. The next step was to start building a prototype in plain old HTML, CSS and JavaScript, and then figure out how many photos we could potentially get into the thing before it broke down. Could the grid handle selection and editing of 1,000 items? 10,000 items? I was cautiously optimistic. A continuous joke I had with the team was that I had built this before, in 2005: The project was an adventurous redesign of Yahoo! Photos , and joking aside, it actually did share a lot of design and interaction elements in common with what we were about to build. In 2005, we were targeting IE 6 and Firefox 1.5, so the landscape has changed a lot in terms of support and performance. Seven years later, it was fun to review some of the lessons and fun bits from the Y! Photos redesign as applicable to Flickr. Prototype: Fluid Grid Layout Some of the first prototypes involved building a grid layout, forming a two-column page that would be fluid to the browser width. We wanted to guarantee at least three photos per row would show in the grid, so the thumbnails could scale themselves relative to the browser size in order to fit in the space – easily done via CSS’ min-width and max-width attributes. A very early version of the uploadr UI. The earliest prototypes simply populated the DOM with a few hundred copies of a cloned photo item “template”, to give the idea of what a busy UI might look like. It was mostly just HTML and CSS at this point. With the grid rendering in fluid form as a series of inline-block <li> elements, the next thing to start was the selection model. Selection and Drag Events Building a desktop-like selection and drag-and-drop model can be a technical challenge, given the underlying complexity. As anyone who’s built one of these will understand, there are a whole ton of interactions one must consider and account for between event monitoring, coordinate tracking, drag-to-select vs. rearrange intents, event cancellation, handling of invalid actions and so on. Selection In general, all user interactions start with watching mousedown() events inside the grid area. If mousedown() fires within “whitespace”, any existing selection is reset and mousemove() events are then used to draw a selection marquee which compares coordinates to the grid, highlighting items based on basic region intersection logic (for example, xyToRowCol() , points can be checked to see what grid row/column they fall within and thus “from/to” ranges can be established for a given marquee box.) Once a mouseup() event fires, selection can be completed and the mousemove() and mouseup() handlers released. Testing the selection UI at various grid sizes. The above marquee drawing and intersection logic is not terribly fancy, but things start to get interesting when you throw in additional positioning considerations like vertical offset from window scrolling (and drag-initiated window scrolling), browser window resizing affecting layout, positioning of the marquee UI vs. coordinates of the underlying grid items and so forth. Keyboard modifiers can also affect selection mode – whether selection is exclusive, additive or toggle-based – so an intersect does not also always mean “select this item”, too. Marquee selection mode in action. Dragging + Rearrange When mousedown() fires on an unselected grid item, selection can immediately change to only that item (unless selection mode is additive or toggle-based via a modifier key.) If firing on an already-selected item, mousemove() is watched for a “threshold” of perhaps 4+ pixels of movement from the original coordinates, at which point “dragging” becomes active. Once dragging has begun, the selected grid DOM elements are marked with a “disabled” CSS class, greying them out somewhat to indicate drag state, and mousemove() now moves around a cursor trailer that shows the count of items being rearranged. Rearrange mode, once entered, is similar to the marquee selection mode except that now only a single mouse coordinate is checked in order to determine what row and column is the current “target” for rearrange – that is, what position the user intends to drag the selected photo(s) to. The logic here can get interesting in edge cases, because the user is able to insert both “before” and “after” a given target point based on whether the cursor is on the left side, or the right side of the target. In terms of the UI, the current drag target simply has an “insert-before” or “insert-after” CSS class appended to it which results in the appropriate “insert point” marker (a CSS border) being applied to it. Rearrange mode in action. Once mouseup() fires on a valid rearrange target, the actual rearrange action is applied to both the UI and data model. The underlying JavaScript re-appends the dragged DOM nodes next to their new target sibling node and then splices the photo item array, matching the order of the array to the new layout shown in the UI. Additional Selection Interactions A few other use cases to consider: Clicking an item, then shift + clicking another should have the effect of setting an “anchor point”, and selecting a range of items from X-Y within the grid. The user should be able, once setting an anchor point, to “pivot” from that point by clicking while continuing to hold the shift key. (Put another way, holding shift should not set the anchor point when clicking.) By holding CTRL (or the Command/Apple key on OS X), selection should be additive and toggle-based. My approach to this meant taking a “snapshot” of the selection when marquee drawing begins, and then applying the logic based on mouse coordinates and keypresses with each draw action. This way, you can draw a marquee over and out of an existing selection, causing it to “toggle” and reset accordingly without losing your original state. A new snapshot is only taken once the selection is finalized at mouseup() time. Demo video: Uploadr Prototype UI Here is a screencast of a very early version of the Uploadr grid UI, showing the basics of mouse-based selection interactions, scrolling and resizing. By this time, selection events were also firing and updating the “editr panel” area as well. [flickr video=7177694856 secret=3fb0a325e4 w=500 h=398] Enter The Keyboard With mouse events working, additional consideration was given to keyboard shortcuts. We intended to have a UI that supported most if not all of the same selection, editing and rearrange actions that could be achieved via the mouse. An important part to making this work involved watching focus inside the grid, tracking the last-known selected item, and supporting the use of the arrow keys as a means of changing focus between grid items. Focus-based navigation in the grid is interesting, more akin to mouse movement and hover behaviour. It is intentionally separate from keyboard-based selection (which is invoked with a toggle behaviour via the spacebar, or selection and editing of a single item via the return key.) Using this approach, it is relatively easy to navigate and build up a selection of items via the arrow keys and spacebar. For rearrange, a cut-and-paste approach was used; CTRL or Command/Apple + X (“cut”) are used to begin rearrange, arrow keys set the target rearrange point, and CTRL + V or return will apply the rearrange at the given target. If active, pressing escape will exit rearrange mode. Performance: Scaling The Front-End An important step in the grid prototype, once it was rendering in a fluid fashion, was to see find all the ways in which we could get it to break down. Which browsers were first to choke under the DOM load as more nodes were written out? Was layout and rendering the bottleneck? Were too many events firing? Was the JS engine spending too much time updating the DOM? After rendering several hundred photos in the UI, we started to see evidence of browsers getting laggy in terms of responsiveness, and CPU + RAM use trending upward. With plans to extend this UI to handle numbers of photos in the thousands, a number of optimizations were made up front including aggressive pruning of the DOM as the user scrolled the page. In brief, the trick is to create a large page with no content and only generate the DOM to reflect the slice of the whole view being shown. Given events like window scrolling and resize affecting browser coordinates and DOM layout, we are easily able to calculate and cache the changes as they happen, making quick lookups to determine precisely what range of grid items are in view for the user. A single “page” of grid items can then be generated on the fly, appended to the DOM and shown to the browser. Events like browser resize invalidates the coordinate cache, so the DOM reflows and the grid refresh / display process repeats itself in a throttled fashion when this happens. Event Throttling: Responsiveness’ Dirty Little Secret Native DOM events are useful, but they can fire quite aggressively and left unchecked, can really hurt the performance of your application. Scrolling and resize are good examples for the grid case, as we want the UI to respond with an updated display pretty quickly when scrolling – but we know that we only have to show new items when a new row comes into view, which is typically only every 200 vertical pixels. With resizing, we only need to reflow the grid when resizing has added or removed enough horizontal room that we’ve lost or gained a new column. In short, if you know events will fire often, subscribe to all of them but only do expensive work if there are real changes to apply. Alternately, you could only let resize handlers (for example) fire once every 500 milliseconds and do the work every time, so your handler only fires twice a second in the worst-case scenario. Cache The Hell Out Of The DOM This was hinted at previously, but is worth repeating: Get references and read values once, particularly from the DOM, and cache them when initially retrieving and updating them in response to events. If you know what a value is going to be, don’t query for it. In JavaScript, an internal lookup is far faster than reaching out to query the DOM for attributes like offsetWidth, for example. Simply reading certain attributes of DOM nodes can cause layout and reflow to happen in the browser, which means you’re making the browser do more work for information that is likely unchanged. Thrown into a loop mixed with DOM writes, this makes for pretty disastrous browser performance. JavaScript frameworks like YUI et al should do their own caching of this data, but I see no downside in grabbing and storing this stuff locally yourself; as the implementer, you have the best idea of what data is most static and what is not. Additionally, try to read at once and write at once to the DOM; don’t have loops that do a write and then a read, for example. Try to write DOM interactions that follow the browser’s rendering model, minimizing the back-and-forth of layout/reflow/display calculations. Use document fragments to build up collections of DOM nodes, and append them once to the DOM vs. using innerHTML , or – worse – multiple appendChild() calls. Don’t query className when you likely know what it’s going to be; track that state internally in JS, instead, and only write changes out to the DOM. “Stateful” CSS Class Names I’ve been a fan of the concept of “stateful” CSS – eg., .is_selected { border: red; } for years. Not only is state consistent, but using CSS in this way also encourages better separation of concerns (and less temptation to add or remove DOM nodes via JS when making changes.) When you want to grey something out, for example, you may set a disabled property to true on a JS object. That easily translates to a CSS class name change including .disabled {} applied to the relevant DOM node. As a result, your DOM is logically reflecting your JS state. It’s also helpful when troubleshooting, because you can add the class name to nodes ad-hoc when testing UI features. For the grid’s purposes, every grid item contains all relevant “states” and the markup for those states – selection, thumbnail, progress, overlay icons, messages, errors and so forth. This makes it very easy to change the item’s display with a single, or few additional CSS class names, and minimizes the amount of work JS has to do to update the DOM. It is also trivial to combine states this way, also – e.g., a photo upload that has a thumbnail, but is in a “failed” state because it’s over-size. While uploading, for example, a grid item may have class=\"has-thumbnail working selected\" , then completes with class=\"has-thumbnail has-fullsize-thumbnail complete\" when the upload has finished. All JS did here was update the class name (and while actively uploading, redraw a small progress meter on the item.) Thus, JS/DOM interaction is fairly minimal. A single CSS change can also completely change the display of the grid, also. “Info view” is one example of this. When enabled, a single additional class on the grid container causes all photo items to show overlay icons with their privacy state, and additional icons if they have tags, are in a set and so on. “Info” view, showing overlays with privacy, state and other information. Broadcast Events FTW Events are a great way for modular bits of code, written by the same or separate people, to work on separate problems independently. Among other things, the grid listens for events regarding file addition, removal, progress and success / failure states from the upload queue module. The grid generates and fires events itself reflecting changes around selection, editing and arrangement as the user is doing their work, which are picked up by the “editr panel” at left that updates to reflect the selection state. Provided that events are kept as simple notifications and relatively one-way, there is little risk of complex event-related tracing in the unlikely, er – event – that something that goes wrong. Flickr uses YUI 3 extensively, and we write and plug our application code into the system as YUI 3 modules. In addition to the excellent modular framework approach, we take advantage of the DOM and Event functionality in particular. In Summary The grid is only one of several modules that make up the new Flickr Web Uploadr, and is primarily responsible for the display and updating of photo thumbnails, selection, arrangement and basic metadata. There is a lot more going on in terms of JavaScript and network state under the hood, including API calls and permissions; posts highlighting some of the other fun areas are forthcoming. As it turns out, building a feature-rich browser-based application for millions of people that looks good, is fast and supports many use cases including constraints and unexpected error conditions, can be a challenge. It’s also part of the fun. Like this post? Have a love of online photography? Want to work with us? Flickr is hiring engineers , designers and product managers in our San Francisco office. Find out more at flickr.com/jobs .", "date": "2012-05-11,"},
{"website": "Flickr", "title": "Highly Available Real Time Push Notifications and You", "author": ["Joshua Cohen"], "link": "https://code.flickr.net/2012/12/12/highly-available-real-time-notifications/", "abstract": "One of the goals of our recently launched (and awesome!) new Flickr iPhone app was to further increase user engagement on Flickr. One of the best ways to drive engagement is to make sure Flickr users know what’s happening on Flickr in as near-real time as possible. We already have email notifications, but email is no longer a good mechanism for real-time updates. Users may have many email accounts and may not check in frequently causing timeliness to go right out the window. Clearly this called for… PUSH NOTIFICATIONS! Motor bike racer getting a push start at the track, Brisbane by State Library of Queensland, Australia I know, you’re thinking, “anyone can build push notifications, we’ve been doing it since 2009!” Which is, of course, absolutely true. The process for delivering push notifications is well trod territory by this point. So… let’s just skip all that boring stuff and focus on how we decided on the underlying architecture for our implementation. Our decisions focused on four major factors: Impact to normal page serving times should be minimal Delivery should be in near-real time Handle thousands of notifications per second The underlying services should be highly available Baby Steps Given these goals, we started by looking at systems we already have in place. Everyone loves not writing new code, right? Our thoughts immediately went to Flickr’s existing PuSH infrastructure . Our PuSH implementation is a great way to get an overview of relevant activity on Flickr, but it has limitations that made it unsuitable for powering mobile push notifications. The primary concern is that it’s less-near-real time than we’d like it to be. On average, activities occurring on Flickr will be delivered to a subscribed PuSH endpoint within one minute. That’s certainly better than waiting for an email to arrive or waiting until the next time you log in to the site and see your activity feed, but it’s not good enough for mobile notifications! This delay is due to some design decisions at the core of the PuSH system. PuSH is designed to aggregate activity and deliver a periodic digest and, because of this, it has a built in window to allow multiple changes to the same photo to be accumulated. PuSH is also focused on ensured delivery, so it maintains an up to date list of all subscribers. These features, which make PuSH great for the purpose it was designed, make it not-so-great for real time notifications. So, repurposing the PuSH code for reuse in a more real time fashion proved to be untenable. Tentative Plans So, what to do? In the end we wound up building a new lightweight event system that is broken up into three phases: Event Generation Event Targeting Message Delivery Event Generation The event generation phase happens while processing the response to a user request. As such, we wanted to ensure that there was little to no impact on the response times as a result. To ensure this was the case, all we do here is a lightweight write into a global Redis queue. We store the minimum amount of data possible, just a few identifiers, so we don’t have to make any extra DB calls and slow down the response just to (potentially) kick off a push notification. Everything after this initial Redis action is processed out of band by our deferred task system and has no impact on site performance. Event Targeting Next in the process is the event targeting phase. Here we have many workers reading from the global Redis queue. When a worker receives an event from the queue it rehydrates the data and loads up any additional information necessary to act on the notification. This includes checking to see what users should be notified, whether those users have devices that are registered to receive notifications, if they’ve opted out of notifications of this type, and finally if they’ve muted activity for the object in question. Message Delivery Flickr’s web-serving stack is PHP, and, up until now, everything described has been processed by PHP. Unfortunately, one area where PHP does not excel is long-lived processes or network connections, both of which make delivering push notifications in real time much easier. Because of this we decided to build the final phase, message delivery, as a separate endpoint in Node.js . So, the question arose: how do we get messages pending delivery from these PHP workers over to the Node.js endpoints that will actually deliver them? For this, we again turned to Redis, this time using its built in pub/sub functionality. The PHP workers simply publish a message to a Redis channel with the assumption that there’s a Node.js process subscribed to that channel eagerly awaiting some data on which it can act. After that the Node process delivers the notification to Apple’s APNS push notification system. Communicating with APNS is a well-documented topic, and not one that’s particularly interesting. In fact, I can sum it up with a single link: https://github.com/argon/node-apn , a great npm package for talking to APNS. The Real Challenge There is, however, a much more interesting problem to discuss at this point: how do we ensure that delivery to APNS is both scalable and highly available? At first blush, this seems like it could be problematic. What if the Node.js worker has crashed? The message will just be lost to the ether! Solving this problem turned out to be the majority of the work involved in implementing push notifications. Scalability The first step to ensuring a service is scalable is to divide the workload. Since Node.js is single threaded, we would already be dividing the workload across individual Node.js processes anyway, so this works out well! When we publish messages to the Redis pub/sub channel, we simply publish to a sharded channel. Each Node.js process subscribes to some subset of those sharded channels, and so will only act on that subset of messages. Configuring our Node.js processes in this way makes it easy to scale horizontally. Whenever we need to add more processing power to the cluster, we can just add more servers and more shards. This also makes it easy to pull hosts out of rotation for maintenance without impacting message delivery: we simply reconfigure the remaining processes to subscribe to additional channels to pick up the slack. Availability Designing for high availability proved to be somewhat more challenging. We needed to ensure that we could lose individual Node processes, a whole server or even an entire data center without degrading our ability to deliver messages. And we wanted to avoid the need for a human in the loop — automatic failover. We already knew that we’d have multiple hosts running in multiple data centers, so the main question was how to get them coordinating with each other so that we would not lose messages in the event of an outage while also ensuring we would not deliver the same message multiple times. Our first thought experiment along these lines was to implement a relatively complex message passing scheme, where two hosts would subscribe to a given channel, one as the primary and one as the backup. The primary would pass a message to the backup saying that it was starting to process a message, and another when it completed. The backup would wait a certain amount of time to receive the first and then the second message from the primary. If a message failed to arrive, it would assume something had gone wrong with the primary and attempt to complete delivery to Apple’s push notification gateway. This plan had two major problems: hosts had to be aware of each other and increasing the number of hosts working in conjunction raised the complexity of ensuring reliable delivery. We liked the idea of having one host serve as a backup for another, but we didn’t like having to coordinate the interaction between so many moving pieces. To solve this issue we went with a convention based approach. Instead of each host having to maintain a list of its partners, we just use Redis to maintain a global lock. Easy enough, right? Perhaps some code is in order! Finally, some code! First we create our Redis clients. We need one client for regular Redis commands we use to maintain the lock, and a separate client for Redis pub/sub commands. var redis = require(&quot;redis&quot;);\r\nvar client = redis.createClient(config.port, config.host);\r\nvar pubsubClient = redis.createClient(config.port, config.host); Next, subscribe to the sharded channel and set up a message handler: // We could be subscribing to multiple shards, but for the sake of simplicity we’ll just subscribe to one here\r\npubsubClient.subscribe(&quot;notification_&quot; + shard);\r\npubsubClient.on(&quot;message&quot;, handleMessage); Now, the interesting part. We have multiple Node.js processes subscribed to the same Redis pub/sub channel, and each process is in a different data center. Whenever any of them receive a message, they attempt to acquire a lock for that message: function handleMessage(channel, message) {\r\n    // Error handling elided for brevity\r\n    var payload = JSON.parse(message);\r\n\r\n    acquireLock(payload, 1, lockCallback);\r\n} Managing locks with Redis is made easy using the SETNX command. SETNX is a “set if not exists” primitive. From the Redis docs: Set key to hold string value if key does not exist. In that case, it is equal to SET. When key already holds a value, no operation is performed. If we have multiple processes calling SETNX on the same key, the command will only succeed for the process that first makes the call, and in that case the response from Redis will be 1. For subsequent SETNX commands, the key will already exist, and the response from Redis will be 0. The value we try to set with SETNX keeps track of how many attempts have been made to deliver the message, initially set to one, this allows us to retry failed messages a predefined number of times before giving up entirely. function acquireLock(payload, attempt, callback) {\r\n    var lockIdentifier = &quot;lock.&quot; + payload.identifier;\r\n\r\n    function dataForCallback(acquired) {\r\n        return {\r\n            &quot;acquired&quot; : acquired,\r\n            &quot;lockIdentifier&quot; : lockIdentifier,\r\n            &quot;payload&quot; : payload,\r\n            &quot;attempt&quot; : attempt\r\n        };\r\n    }\r\n\r\n    // The value of the lock key indicates how many lock attempts have been made\r\n    client.setnx(lockIdentifier, attempt, function(error, data) {\r\n        if (error) {\r\n            logger.error(&quot;Error trying to acquire redis lock for: %s&quot;, lockIdentifier);\r\n            return callback(error, dataForCallback(false));\r\n        }\r\n\r\n        return callback(null, dataForCallback(data === 1));\r\n    });\r\n} At this point our attempt to acquire the lock has either succeeded or failed, and our callback is invoked. What we do next depends on whether we managed to acquire the lock. If we did acquire the lock, we simply attempt to send the message. If we did not acquire the lock, then we will check back later to see if the message was sent successfully (more on this later): function lockCallback(error, data) {\r\n    // Again, error handling elided for brevity\r\n    if (data &amp;&amp; data.acquired) {\r\n        return sendMessage(data.payload, data.lockIdentifier, data.attempt === MAX_ATTEMPTS);\r\n    } else if (data &amp;&amp; !data.acquired) {\r\n        return setTimeout(checkLock, LOCK_EXPIRY, data.payload, data.lockIdentifier);\r\n    }\r\n} Finally, it’s time to actually send the message! We do some work to process the payload into a form we can use to pass to APNS and send it off. If all goes well, we do one of two things: If this was our first attempt to send the message, we update the lock key in Redis to a sentinel value indicating we were successful. This is the value the backup processes will check for to determine whether or not sending succeeded. If this was our last attempt to send the message (i.e. the primary process failed to deliver and now a backup process is handling delivery), we simply delete the lock key. function sendMessage(payload, lockIdentifier, lastAttempt) {\r\n    // Does some work to process the payload and generate an APNS notification object\r\n    var notification = generateApnsNotification(payload);\r\n\r\n    if (notification) {\r\n        // The APNS connection is defined/initialized elsewhere\r\n        apnsConnection.sendNotification(notification);\r\n\r\n        if (lastAttempt) {\r\n            client.del(lockIdentifier);\r\n        } else {\r\n            client.set(lockIdentifier, DONE_VALUE);\r\n        }\r\n    }\r\n} There’s one final piece of the puzzle: checking the lock in the process that did not acquire it initially. Here we issue a Redis GET to retrieve the current value of the lock key. If the process that won the lock managed to send the message, this key should be set to a well known sentinel value. If so, we don’t have any work to do, and we can simply delete the lock. However, if this value is not set to that sentinel value, then something went wrong with delivery in the process that originally acquired the lock and we should step up and try to deliver the message from this backup process: function checkLock(payload, lockIdentifier) {\r\n    client.get(lockIdentifier, function(error, data) {\r\n        // Error handling elided for brevity\r\n        if (data !== DONE_VALUE) {\r\n            acquireLock(payload, data + 1, lockCallback);\r\n        } else {\r\n            client.del(lockIdentifier);\r\n        }\r\n    });\r\n} Summing Up So, there you have it in a nutshell. This method of coordinating between processes makes it very easy to adjust the number of processes subscribing to a given shard’s channels. There’s no need for any process subscribed to a channel to be aware of how many other processes are also subscribed. As long as we have at least two processes in separate data centers subscribing to each shard we are protected from all of the from the following scenarios: The crash of any individual Node.js process The loss of a single host running the Node.js processes The loss of an entire data center containing many hosts running the Node.js processes Let’s go back over our initial goals and see how we fared: Impact to normal page serving times should be minimal We accomplish this by minimizing the workload done as part of the normal browser-driven request/response processing. The deferred task system picks up from there, out of band. Delivery should be in near-real time Processing stats from our implementation show that time from user actions leading to event generation to message delivery averages about 400ms and is completely event driven (no polling). Handle thousands of notifications per second In stress tests of our system, we were able to process more than 2,000 notifications per second on a single host (8 Node.js workers, each subscribing to multiple shards). The underlying services should be highly available The availability design is resilient to a variety of failure scenarios, and failover is automatic. We hope you’re enjoying push notifications in the new Flickr iPhone app . Addendum! There was a minor problem with the code in this post when supporting more than two workers. For a full explanation of the problem and the solution, check out Global Redis Locks Redux . Like this post? Have a love of online photography? Want to work with us? Flickr is hiring engineers , designers and product managers in our San Francisco office. Find out more at flickr.com/jobs .", "date": "2012-12-12,"},
{"website": "Flickr", "title": "Redis Global Locks Redux", "author": ["Joshua Cohen"], "link": "https://code.flickr.net/2012/12/21/redis-global-locks-redux/", "abstract": "In my last post I described how we use Redis to manage a global lock that allows us to automatically failover to a backup process if there was a problem in the primary process. The method described allegedly allowed for any number of backup processes to work in conjunction to pick up on primary failures and take over processing. Locks #1 by Christoph Kummer Thanks to an astute reader, it was pointed out that the code in the blog wouldn’t actually work as advertised: @heyjoshua I might be missing something but the code looks like it'll keep trying to acquire the lock, which it'll can't, due to the SETNX. — Nolan Caudill (@nolancaudill) December 15, 2012 The Problem Nolan correctly noticed that when the backup processes attempts to acquire the lock via SETNX, that lock key will already exist from when it was acquired by the primary, and thus all subsequent attempts to acquire locks will simply end up constantly trying to acquire a lock that can never be acquired. As a reminder, here’s what we do when we check back on the status of a lock: function checkLock(payload, lockIdentifier) {\n    client.get(lockIdentifier, function(error, data) {\n        // Error handling elided for brevity\n        if (data !== DONE_VALUE) {\n            acquireLock(payload, data + 1, lockCallback);\n        } else {\n            client.del(lockIdentifier);\n        }\n    });\n} And here’s the relevant bit from acquireLock that calls SETNX: client.setnx(lockIdentifier, attempt, function(error, data) {\n        if (error) {\n            logger.error(&amp;quot;Error trying to acquire redis lock for: %s&amp;quot;, lockIdentifier);\n            return callback(error, dataForCallback(false));\n        }\n\n        return callback(null, dataForCallback(data === 1));\n    }); So, you’re thinking, how could this vaunted failover process ever actually work? The answer is simple: the code from that post isn’t what we actually run. The actual production code has a single backup process, so it doesn’t try to re-acquire the lock in the event of failure, it just skips right to trying to send the message itself. In the previous post, I described a more general solution that would work for any number of backup processes, but I missed this one important detail. That being said, with some relatively minor changes, it’s absolutely possible to support an arbitrary number of backup processes and still maintain the use of the global lock. The trivial solution is to simply have the backup process delete the key before trying to re-acquire the lock (or, technically acquire it anew). However, the problem with that becomes apparent pretty quickly. If there are multiple backup processes all deleting the lock and trying to SETNX a new lock again, there’s a good chance that a race condition could arise wherein one of backups deletes a lock that was acquired by another backup process, rather than the failed lock from the primary. The Solution Thankfully, Redis has a solution to help us out here: transactions . By using a combination of WATCH , MULTI , and EXEC , we can perform actions on the lock key and be confident that no one has modified it before our actions can complete. The process to acquire a lock remains the same: many processes will issue a SETNX and only one will win. The changes come into play when the processes that didn’t acquire the lock check back on its status. Whereas before, we simply checked the current value of the lock key, now we must go through the above described Redis transaction process. First we watch the key, then we do what amounts to a check and set (albeit with a few different actions to perform based on the outcome of the check): function checkLock(payload, lockIdentifier, lastCount) {\n    client.watch(lockIdentifier);\n    client.multi()\n        .get(lockIdentifier)\n        .exec(function(error, replies) {\n            if (!replies) {\n                // Lock value changed while we were checking it, someone else got the lock\n                client.get(lockIdentifier, function(error, newCount) {\n                    setTimeout(checkLock, LOCK_EXPIRY, payload, lockIdentifier, newCount);\n                });\n\n                return;\n            }\n\n            var currentCount = replies[0];\n            if (currentCount === null) {\n                // No lock means someone else completed the work while we were checking on its status and the key has already been deleted\n                return;\n            } else if (currentCount === DONE_VALUE) {\n                // Another process completed the work, let’s delete the lock key\n                client.del(lockIdentifier);\n            } else if (currentCount == lastCount) {\n                // Key still exists, and no one has incremented the lock count, let’s try to reacquire the lock\n                reacquireLock(payload, lockIdentifier, currentCount, doWork);\n            } else {\n                // Key still exists, but the value does not match what we expected, someone else has reacquired the lock, check back later to see how they fared\n                setTimeout(checkLock, LOCK_EXPIRY, payload, lockIdentifier, currentCount);\n            }\n        });\n} As you can see, there are five basic cases we need to deal with after we get the value of the lock key: If we got a null reply back from Redis, that means that something else changed the value of our key, and our exec was aborted; i.e. someone else got the lock and changed its value before we could do anything. We just treat it as a failure to acquire the lock and check back again later. If we get back a reply from Redis, but the value for the key is null, that means that the work was actually completed and the key was deleted before we could do anything. In this case there’s nothing for us to do at all, so we can stop right away. If we get back a value for the lock key that is equal to our sentinel value, then someone else completed the work, but it’s up to us to clean up the lock key, so we issue a Redis DEL and call our job done. Here’s where things get interesting: if the key still exists, and its value (the number of attempts that have been made) is equal to our last attempt count, then we should try and reacquire the lock. The last scenario is where the key exists but its value (again, the number of attempts that have been made) does not equal our last attempt count. In this case, someone else has already tried to reacquire the lock and failed. We treat this as a failure to acquire the lock and schedule a timeout to check back later to see how whoever did acquire the lock got on. The appropriate action here is debatable. Depending on how long your underlying work takes, it may be better to actually try and reacquire the lock here as well, since whoever acquired the lock may have already failed. This can, however, lead to premature exhaustion of your attempt allotment, so to be safe, we just wait. So, we’ve checked on our lock, and, since the previous process with the lock failed to complete its work, it’s time to actually try and reacquire the lock. The process in this case is similar to the above inasmuch as we must use Redis transactions to manage the reacquisition process, thankfully however, the steps are (somewhat) simpler: function reacquireLock(payload, lockIdentifier, attemptCount, callback) {\n    client.watch(lockIdentifier);\n    client.get(lockIdentifier, function(error, data) {\n        if (!data) {\n            // Lock is gone, someone else completed the work and deleted the lock, nothing to do here, stop watching and carry on\n            client.unwatch();\n            return;\n        }\n\n        var attempts = parseInt(data, 10) + 1;\n\n        if (attempts &amp;gt; MAX_ATTEMPTS) {\n            // Our allotment has been exceeded by another process, unwatch and expire the key\n            client.unwatch();\n            client.expire(lockIdentifier, ((LOCK_EXPIRY / 1000) * 2));\n            return;\n        }\n\n        client.multi()\n            .set(lockIdentifier, attempts)\n            .exec(function(error, replies) {\n                if (!replies) {\n                    // The value changed out from under us, we didn't get the lock!\n                    client.get(lockIdentifier, function(error, currentAttemptCount) {\n                        setTimeout(checkLock, LOCK_TIMEOUT, payload, lockIdentifier, currentAttemptCount);\n                    });\n                } else {\n                    // Hooray, we acquired the lock!\n                    callback(null, {\n                        &amp;quot;acquired&amp;quot; : true,\n                        &amp;quot;lockIdentifier&amp;quot; : lockIdentifier,\n                        &amp;quot;payload&amp;quot; : payload\n                    });\n                }\n            });\n    });\n} As with checkLock we start out by watching the lock key, and proceed do a (comparitively) simplified check and set. In this case, we’ve “only” got three scenarios to deal with: If we’ve already exceeded our allotment of attempts, it’s time to give up. In this case, the allotment was actually exceeded in another worker, so we can just stop right away. We make sure to unwatch the key, and set it expire at some point far enough in the future that any remaining processes attempting to acquire locks will also see that it’s time to give up. Assuming we’re still good to keep working, we try and update the lock key within a MULTI/EXEC block, where we have our remaining two scenarios: If we get no replies back, that again means that something changed the value of the lock key during our transaction and the EXEC was aborted. Since we failed to acquire the lock we just check back later to see what happened to whoever did acquire the lock. The last scenario is the one in which we managed to acquire the lock. In this case we just go ahead and do our work and hopefully complete it! Bonus! To make managing global locks even easier, I’ve gone ahead and generalized all the code mentioned in both this and the previous post on the subject into a tidy little event based npm package: https://github.com/yahoo/redis-locking-worker . Here’s a quick snippet of how to implement global locks using this new package: var RedisLockingWorker = require(&amp;quot;redis-locking-worker”);\n\nvar SUCCESS_CHANCE = 0.15;\n\nvar lock = new RedisLockingWorker({\n    &amp;quot;lockKey&amp;quot; : &amp;quot;mylock&amp;quot;,\n    &amp;quot;statusLevel&amp;quot; : RedisLockingWorker.StatusLevels.Verbose,\n    &amp;quot;lockTimeout&amp;quot; : 5000,\n    &amp;quot;maxAttempts&amp;quot; : 5\n});\n\nlock.on(&amp;quot;acquired&amp;quot;, function(lastAttempt) {\n    if (Math.random() &amp;lt;= SUCCESS_CHANCE) {\n        console.log(&amp;quot;Completed work successfully!&amp;quot;, lastAttempt);\n        lock.done(lastAttempt);\n    } else {\n        // oh no, we failed to do work!\n        console.log(&amp;quot;Failed to do work&amp;quot;);\n    }\n});\nlock.acquire(); There’s also a few other events you can use to track the lock status: lock.on(&amp;quot;locked&amp;quot;, function() {\n    console.log(&amp;quot;Did not acquire lock, someone beat us to it&amp;quot;);\n});\n\nlock.on(&amp;quot;error&amp;quot;, function(error) {\n    console.error(&amp;quot;Error from lock: %j&amp;quot;, error);\n});\n\nlock.on(&amp;quot;status&amp;quot;, function(message) {\n    console.log(&amp;quot;Status message from lock: %s&amp;quot;, message);\n}); More Bonus! If you don’t need the added complexity if multiple backup processes, I also want to give credit to npm user pokehanai who took the methodology described in the original post and created a generalized version of the two-worker solution: https://npmjs.org/package/redis-paired-worker . Wrapping Up So there you have it! Coordinating work on any number of processes across any number of hosts couldn’t be easier! If you have any questions or comments on this, please feel free to follow up on Twitter . Like this post? Have a love of online photography? Want to work with us? Flickr is hiring engineers , designers and product managers in our San Francisco office. Find out more at flickr.com/jobs .", "date": "2012-12-21,"},
{"website": "Flickr", "title": "Using Redis as a Secondary Index for MySQL", "author": ["Joshua Cohen"], "link": "https://code.flickr.net/2013/03/26/using-redis-as-a-secondary-index-for-mysql/", "abstract": "Hey, did you notice, on the brand-spanking-new Yahoo homepage , right there on the side of the page, it’s photos from your Flickr contacts (or maybe your groups)! No? Go check it out, I’ll wait. Ok, great, you’re back! What you should have seen, assuming you have Flickr contacts (or are a member of some groups), is photos from your most recently active contact (or group!). Something like… this: (thanks schill !) What you see above is the 10 most recent photos from my contact who most recently uploaded any photos. The homepage retrieves this data by making a call to a specially tailored method from the Flickr API. The Latency Problem In order to ensure performance for Yahoo.com , this API method had very tight SLAs; we can’t slow down the page for the millions of homepage visitors to pull in Flickr content, after all. While the method can return different data sets depending on the most recent activity from your Flickr contacts and groups, for the sake of this post we’re going to focus exclusively on the contacts case. The first step to returning data is to get your most recently active contact, and to do that we need a list of all of your contacts sorted by how recently they’ve uploaded a photo. In an ideal world, the SQL query would be something along the lines of: SELECT contact_id FROM Contacts WHERE user_id = ? ORDER BY date_upload LIMIT 1; Easy, right? Sadly, no. Due to the way our contact relationships are stored, the query we need to run is more complex than the above. Still, for the vast majority of Flickr users, the *actual* SQL query performed just fine, usually in less than 1ms. However, Flickr users come in all shapes and sizes. Some users have a few contacts, some have hundreds, and some… have tens of thousands. Unfortunately, the runtime for the query scaled proportionally to number of contacts the user has. I won’t delve into the specifics of the query or how we store contact relationships, but suffice it to say we investigated possible changes to both the query and to the indexes in MySQL, but neither was sufficient to give us the performance we needed across all possible use cases. With that in mind, we had to look elsewhere for optimizations. The First Attempt With a pure MySQL solution off the table, our first thoughts for optimization avenues turned to the obvious: denormalization. If we stored the 10 most recent photos from your most recently active contact ahead of time, then getting that list when the homepage was rendered would be trivial regardless of how many contacts you have. At Flickr we’re big fans of Redis , so our thoughts immediately turned to using it as a store for the denormalized contact data. In order to denormalize the data, we need to constantly process six different user actions: Add a contact Remove a contact Change a contact relationship User uploads a photo User deletes a photo User changes a photo’s privacy Each one of these actions can impact what photos you should see on the Yahoo homepage, and therefore must update the denormalized data appropriately. Because the photo related actions can potentially impact thousands of users, if not tens or hundreds of thousands (everyone who calls the photo owner a contact would need to have their denormalized data updated on upload), they must be processed by our offline task system to ensure site performance is not adversely impacted. Unfortunately, this is where we ran into a snag. The nature of Flickr’s offline task system is such that the order of processing is not guaranteed. In most cases this is not a problem, however when it comes to maintaining an accurate list of denormalized data not being able to predict the outcome of running multiple tasks is problematic. Out of Order Imagine the following scenario: you add a contact then quickly remove them. This results in two tasks being added, one to add and one to remove. If they’re processed in the order in which the actions were taken, everything is fine. However, if they’re processed out of order then when everything is said and done the denormalized data no longer accurately reflects your actual contact relationships. Maybe the next action that updates your data will correct this problem, or maybe it will introduce another problem. Over time, it’s likely that a not-insignificant number of users will have denormalized data that is out of sync with reality. Out of order by Foomandoonian Ok, let’s try this another way Looking back at our original problem, the bottleneck was not in generating the list of photos for your most recently active contact, it was just in finding who your most recently active contact was (specifically if you have thousands or tens of thousands of contacts). What if, instead of fully denormalizing, we just maintain a list of your recently active contacts? That would allow us to optimize the slow query, much like a native MySQL index would; instead of needing to look through a list of 20,000 contacts to see which one has uploaded a photo recently, we only need to look at your most recent 5 or 10 (regardless of your total contacts count)! In the end, this is exactly what we ended up doing. We still process offline tasks to keep track of your contacts’ activity, but now the set of actions we need to track is smaller: Add a new contact Remove a contact User uploads a photo Each task maintains a per-user sorted set where the set member is the contact id, and the score is the timestamp of when that contact last uploaded a photo. So, for example, if a user (user id 12345) adds a new contact (user id 98765) we simply do: ZADD user_12345 1363665432 98765 Removing a contact is the opposite, using ZREM as the yin to ZADD ‘s yang: ZREM user_12345 98765 When a user uploads a new photo it’s once again a ZADD (though it’s a ZADD against the set for every user that calls the uploading user a contact). If the uploader is already in a given user’s set, this will simply update the score, if not then the uploader will be added to the set. As things stand right now, the set will grow without bound, which is obviously not much help if our goal is to limit the number of contacts we need to check against when querying the DB. The solution to this is to cap the set. After each ZADD we check to see if the size of the set has exceeded a threshold (generally we store an additional 20% on top of the data we absolutely need), and if so, we remove all of the extra records using ZREMRANGEBYRANK : ZREMRANGEBYRANK user_12345 0 ($collection_size - $max_size) - 1 Where $collection_size is the current number of members in the set and $max_size is the maximum number of members we want to store. Note that we’re removing from the head of the set. Redis stores data in sorted sets in ascending order, so the least recently active contacts are at the beginning of the set. Akin to how we must cap the set to keep it below a maximum size, we also have a threshold on the other end of the spectrum to keep the set above a minimum size. If a user happens to remove their 10 most recently active contacts then there would be no data in their set, and the Redis index would be of little value. With that in mind, any time the user removes a contact, we check to see if the size of the set has dropped under the minimum threshold, and if so we repopulate the data based on their remaining contacts. This is slightly more complex than a simple Redis command, so we’ll use actual PHP to explain: //\r\n// $key is the redis ZSET key, e.g. user_12345\r\n//\r\n$count = redis_zset_zcard($key);\r\nif ($count &lt; $MIN_SET_SIZE) {\r\n      if ($count &gt; 0) {\r\n        $current_contacts = redis_zset_zrange($key, 0, -1);\r\n    } else {\r\n        $current_contacts = array();\r\n    }\r\n\r\n    //\r\n    // In this call, the second parameter is the number of contacts\r\n    // to return and the third parameter is a list of users to\r\n    // exclude from the response. Since they’re already in the\r\n    // set, there’s no need to add them again\r\n    //\r\n    $contacts = contacts_most_recently_uploaded_list($user, $MAX_SET_SIZE - $count,\r\n        $current_contacts);\r\n\r\n    foreach ($contacts as $contact) {\r\n        redis_zset_zadd($key, $contact['last_upload'], $contact['id']);\r\n    }\r\n} Remember, this is all being done outside of the context of a page request, so there’s no harm in spending a little bit of extra time to ensure when the API is called we have a reliable index that we can use to optimize the DB query. The final action we need to take is to actually query the set to get the list of contacts so we can actually do said DB query optimization. This is done with a standard ZREVRANGE : ZREVRANGE user_12345 0 10 Similar to how we cap the set by removing members from the beginning because those are the least recently active, when we want the most recently active we use ZREVRANGE to get members at the end of the set. You’re probably wondering, what about the other three events that generated tasks for the fully denormalized solution? How are we able to get by without them? Well, because we’ll just be using the list of contacts to optimize a live DB query, we can take some liberties with data purity in Redis. Because we store multiple recently active contacts, it doesn’t matter if, for example, the most recent contact in your Redis set has deleted his most recent photos or made them all private. When we query the DB, we further restrict the list based on your current relationship with a contact and photo-visibility, so any issues with Redis being out of sync sort themselves out automatically. Take the above scenario where a user adds and quickly removes a contact. If the remove is processed first and the contact remains in the user’s Redis set, when we go to query the live contacts DB, we’ll get no results for that relationship and move on to the next contact in the set—crisis averted. There is a slight problem with the reverse scenario wherein a user removes a contact and then adds them back. If the tasks are processed out of order, there’s a chance that the add may not end up being reflected in the Redis set. This is the only chance for corruption under this system (as opposed to the fully denormalized solution where many tasks could interact and corrupt one another), and it’s likely to be fixed the next time any of the other actions occur. Furthermore, the only downside of this corruption is a user seeing photos from their second most recently active contact; this is a condition we’re willing to live with given the overall gains provided by the solution as a whole. Not All Wine and Roses The dataset involved in this solution is one of the largest we’ve pushed at Redis so far, and it’s not without its pitfalls. Namely, as the size of the dataset increases, the amount of time spent doing RDB saves also increases. This can introduce latency into Redis commands while the save is in progress. From Redis Persistence : RDB needs to fork() often in order to persist on disk using a child process. Fork() can be time consuming if the dataset is big, and may result in Redis to stop serving clients for some millisecond or even for one second if the dataset is very big and the CPU performance not great. AOF also needs to fork() but you can tune how often you want to rewrite your logs without any trade-off on durability. This is a key factor to be aware of when optimizing Redis performance. The overall speed of the system can be adversely impacted by RDB saves, therefore taking steps to minimize the time spent saving is critical. We’ve solved this problem by isolating these writes to their own Redis instance, thereby limiting the size of the dataset to only keys related to contacts activity. In the long term, as activity increases, it’s likely that we’ll need to further reduce the number of writes-per-instance by sharding this contact activity to a number of Redis instances. In some cases, multiple small Redis instances running on a single host can be preferable to one large Redis instance. As mentioned in the Redis Persistence guide, RDB saves can suffer with a slow CPU; if upgrading hardware (mostly faster CPUs to improve fork() performance), is possible, that’s certainly another option to investigate. Like this post? Have a love of online photography? Want to work with us? Flickr is hiring engineers , designers and product managers in our San Francisco office. Find out more at flickr.com/jobs .", "date": "2013-03-26,"},
{"website": "Flickr", "title": "Flickr Stats API", "author": ["Paul Hammond"], "link": "https://code.flickr.net/2010/03/03/flickr-stats-api/", "abstract": "photo by xgray After collections , the most frequently requested addition to the Flickr API has been Stats. Today, alongside access to some archived older data , we’re announcing lots of new API methods that give you access to all the stats data we’re logging. First up is flickr.stats.getTotalViews . If we call this with no arguments, it will give you the all time view counts for a user’s photos, photostream, sets and collections: # ?method=flickr.stats.getTotalViews\r\n\r\n<stats>\r\n\t<total views=\"584957\"/>\r\n\t<photos views=\"391325\"/>\r\n\t<photostream views=\"175691\"/>\r\n\t<sets views=\"17856\"/>\r\n\t<collections views=\"85\"/>\r\n</stats> Sounds simple right? There’s a couple of details that are worth highlighting: Not all Flickr members have stats enabled. If you get back an error, then someone needs to visit www.flickr.com/photos/me/stats/ and press the big pink button. “All time” is a surprisingly vague concept. We started recording “All-time” view counts for photos, photostreams and sets sometime back in 2006, and started recording collection counts on October 21, 2008 . Now we know how many views someone got on their account, you’ll probably want to drill down into when the views happened, where those views came from, and what photos were popular. When Getting details on when the views happened is simple, we can pass a date to flickr.stats.getTotalViews , and get the counts for just that that day. The date can be a unix timestamp, or something like \"2010-03-24\", but it has to be in the last 28 days as we don’t keep detailed data around longer than that. What Getting an idea of exactly what photos have been viewed is also simple – you can call flickr.stats.getPopularPhotos . This returns a standard photos response , with an extra <stats> element giving the view, comment and favorite counts. As with flickr.stats.getTotalViews, this can be called with a date if you’d like to narrow down your results to just one day. Where Lastly, if you want to know where the views came from, you can call flickr.stats.getPhotoDomains . This gives a list of all of the domains that sent traffic to the photo. If you want even more detail on a domain, flickr.stats.getPhotoReferrers will tell you exactly what pages we’ve seen as referrers. Both methods can be narrowed down by both date or photo id if you want to filter the results more. But wait! There’s more! So far we’ve only talked about detailed data for photos, because that’s all we’ve ever shown on the flickr stats pages. But, behind the scenes we’re also recording data for sets, collections and photostreams, so we’ve added API methods to get at this data too. Full details are available as part of the full API documentation . Between them these new API methods give you programatic access to all of the data we’re recording as part of the stats system. So go ahead and make awesome things with them!", "date": "2010-03-3,"},
{"website": "Flickr", "title": "Slides from Velocity 2009", "author": ["Paul Hammond"], "link": "https://code.flickr.net/2009/06/26/slides-from-velocity-2009/", "abstract": "At Flickr Engineering our favorite thing is rainbows . But fast stable websites come a pretty close second. So last week some of us drove down to San Jose to take part in the 2009 O’Reilly Velocity conference . photo by NathanFromDeVryEET The conference consists of two tracks, performance and operations. The attendance was a great mix of people working on optimizing the whole web stack – everything from the download size of CSS to the PUE of data centers. There were talks on automated infrastructure, Javascript, MySQL, front-end performance recommendations and CDNs, but the most significant theme was the direct business benefits that companies like Google and Microsoft are seeing from microsecond speed improvements on their sites. Some of the most interesting sessions for for us were: John Adams talking about Twitter operations ( details , slides , video ) Eric Schurman and Jake Brutlag’s joint presentation about performance benefits seen at Google and Bing ( details , slides , video ) Andrew Shafer’s discussion of Agile Infrastructure ( details ) Adam Jacobs and Ezra Zygmuntowicz’s talk about Cloud Infrastructure ( details ) John and I also gave a talk on how Flickr’s engineering and operations team work together to allow us to iterate quickly without causing stability problems. The full video is available , and here’s the slides: 10+ Deploys Per Day: Dev and Ops Cooperation at Flickr View more presentations from John Allspaw . Thank you to Jesse and Steve for putting together a great conference. We’re already looking forward to next year.", "date": "2009-06-26,"},
{"website": "Flickr", "title": "Flickr at SF Web Performance", "author": ["Ross Harmes"], "link": "https://code.flickr.net/2012/10/26/flickr-at-sf-web-performance/", "abstract": "Wait! Did you say they all run Webkit? by Schill Thanks to everyone that came out to the SF Web Performance meet up last night! For those of you that missed it, JP and Aaron were kind enough to record the entire event on Ustream . You can also view the slides and associated blog posts for each of the presentations: Optimizing Touch Performance , by Stephen Woods: slides and blog post Using Web Workers for fun and profit: Parsing Exif in the client , by Chris Berry: slides and blog post The Grid: How we show 10,000 photos on a page without crashing your browser , by Scott Schiller: slides and blog post Big thanks to JP and Aaron for setting it up and running the event so well!", "date": "2012-10-26,"},
{"website": "Flickr", "title": "Pre-generating Justified Views", "author": ["Ross Harmes"], "link": "https://code.flickr.net/2013/06/14/pre-generating-justified-views/", "abstract": "On May 20th, we introduced our Justified layout to the Photostream page. Ever since launch, we’ve been working hard to improve the performance of this page, and this week we’ve deployed a change that dramatically reduces the time it takes to display photos. The secret? Cookies. At a high level, our Justified algorithm works like this: Take, as input, the browser viewport width and a list of photos Begin to lay those photos out in a row sequentially, using the maximum allowed height and scaling the width proportionately If a row becomes longer than the viewport width, reduce the height of that row and all the photos in it until the width is correct Because we need the viewport width, we have to run this algorithm entirely in the browser. And because we won’t know which particular photo size to request until we’ve run the algorithm, we can’t start downloading the photos until very late in the process. This is why, up until Friday, when you loaded a photostream page, you saw the spinning blue and pink balls before the photos loaded. Last week we were able to make one key change: we now pre-generate the layout on the server. This means that we know exactly which image sizes we need at the very top of the page, and can start downloading them immediately. It also means the spinning balls aren’t needed anymore. The end result is that the first photo on the page now loads seven times faster than on May 20th. “Time to First Photo” on the Photostream page One question remains: we need client viewport width in order to generate the layout, so how are we able to pre-generate it on the server? The first time you come to any Flickr page, we store the width of your browser window in a cookie. We can then read that cookie on the server on subsequent page loads. This means we aren’t able to pre-generate the photostream layout the very first time you come to the site. It also means that the layout will occasionally be incorrect, if you have resized the browser window since the last time you visited Flickr; we deal with this by always correcting the layout on the client, if a mismatch is detected. This is one of many performance improvements we’re working on after our 5/20 release (we’ve also deployed some improvements to the homepage activity feed). Expect to see the performance continue to improve on the redesigned pages in the coming weeks and months.", "date": "2013-06-14,"},
{"website": "Flickr", "title": "A Summer at Flickr", "author": ["Joseph Baena"], "link": "https://code.flickr.net/2013/09/04/a-summer-at-flickr/", "abstract": "This summer I had the unforgettable opportunity to work side-by-side with the some of the smartest, photography-loving software engineers I’ve ever met. Looking back on my first day at Flickr HQ – beginning with a harmonious welcome during Flickr Engineering’s weekly meeting – I can confidently say that over the past ten weeks I have become a much better software engineer. One of my projects for the summer was to build a new and improved locking manager that controls the distribution of locks for offline tasks (or OLT s for short). Flickr uses OLTs all the time for data migration, uploading photos, updates to accounts, and more. An OLT needs to acquire a lock on a shared resource, such as a group or an account, to prevent other OLTs from accessing the same resource at the same time. The OLT will then release the lock when it’s done modifying the shared data. Myles wrote an excellent blog post on how Flickr uses offline tasks here . When building a distributed lock system, we need to take into account a couple of important details. First, we need to make sure that all the lock servers are consistent. One way to maintain consistency is to elect one server to act as a master and the remaining servers as slaves , where the master server is responsible for data replication among the slave servers. Second, we need to account for network and hardware failures – for instance, if the master server goes down for some reason, we need to quickly elect a new master server from one of the slave servers. The good news is, Apache ZooKeeper is an open-source implementation of master-slave data replication, automatic leader election, and atomic distributed data reads and writes. Offline tasks send lock acquire and release requests through ZooLocker. ZooLocker in turn interfaces with the ZooKeeper cluster to create and delete znodes that correspond to the individual locks. In the new locking system (dubbed “ZooLocker”), each lock is stored as a unique data node (or znode ) on the ZooKeeper servers. When a client acquires a lock, ZooLocker creates a znode that corresponds to the lock. If the znode already exists, ZooLocker will tell the client that the lock is currently in use. When a client releases the lock, ZooLocker deletes the corresponding znode from memory. ZooLocker stores helpful debugging information, such as the owner of the lock, the host it was created on, and the maximum amount of time to hold on to the lock, in a JSON-serialized format in the znode. ZooLocker also periodically scans through each znode in the ZooKeeper ensemble to release locks that are past their expiration time. My locking manager is already serving locks in production. In spite of sudden spikes in lock acquire and release requests by clients, the system holds up pretty well. A graph of the number of lock acquire requests in ZooLocker per second My summer internship at Flickr has been an incredibly valuable experience for me. I have demystified the process of writing, testing, and integrating code into a running system that millions of people around the world use each and every day. I have also learned about the amazing work going on in the engineering team, the ups and downs the code deploy process, and how to dodge the incoming flying finger rockets that the Flickr team members fling at each other.  My internship at Flickr is an experience I will never forget, and I am very grateful to the entire Flickr team for giving me the opportunity to work and learn from them this summer.", "date": "2013-09-4,"},
{"website": "Flickr", "title": "New SSL Endpoints for the Flickr API", "author": ["Chris Martin"], "link": "https://code.flickr.net/2014/02/24/new-ssl-endpoints-for-the-flickr-api/", "abstract": "Sometime in the last few months, we went and updated our API SSL endpoints. Shame on us for not making a bigger deal about it! In the past, to access the Flickr API via SSL you needed to use the “secure.flickr.com” subdomain… Not anymore! Now calling the API via SSL is as easy as updating your code to call: https://api.flickr.com/ In fact, it’s so easy that we want everyone to use it. You’ll notice in the API documentation that all of the endpoints have been updated to https. While OAuth adds security by removing the need to authenticate by username and password, sending all traffic over SSL will further protect our users’ data while in transit. The SSL endpoints for the Flickr API are: https://api.flickr.com/services/rest/\r\nhttps://api.flickr.com/services/soap/\r\nhttps://api.flickr.com/services/xmlrpc/ And for uploads: https://up.flickr.com/services/upload/\r\nhttps://up.flickr.com/services/replace/ Later this year we will be migrating the Flickr API to SSL access only. We’ll let you know the exact date in advance, and will run a blackout test before the big day. For applications that use well established HTTP client libraries, this should only require updating the protocol and (maybe) some updated configuration. We’ll also be working with API kit developers, so updating many apps will be a git pull away. Of course we realize that this change might be more difficult for some. We’ll be following the Developer Support Group closely, so please let us hear your questions, comments, and concerns. This is the first step of many improvements that we’ll be making to the API and our developer tools this year, and we’ll post additional details and timelines as we go. Want to help? We’re hiring!", "date": "2014-02-24,"},
{"website": "Flickr", "title": "Building Flickr’s new Hybrid Signed-Out Homepage", "author": ["Scott Schiller"], "link": "https://code.flickr.net/2014/04/23/building-flickrs-new-hybrid-signed-out-homepage/", "abstract": "Adventures in Frontend-Landia tl;dr: Chrome’s DevTools: still awesome. Test carefully on small screens, mobile/tablets. Progressively enhance “extraneous”, but shiny, features where appropriate. Building a fast, fun Slideshow / Web Page Hybrid Every so often, dear reader, you may find yourself with a unique opportunity. Sometimes it’s a chance to take on some crazy ideas, break the rules and perhaps get away with some front-end skullduggery that wouldn’t be allowed, nor encouraged under normal circumstances. In this instance, Flickr’s newest Signed-Out Homepage turned out to be just that sort of thing. The 2014 signed-out flickr.com experience ( flickr.com/new/ ) is a hybrid, interactive blend of slideshow and web page combining scroll and scaling tricks, all the while highlighting the lovely new Flickr mobile apps for Android and iPhone with UI demos shown via inline HTML5 video and JS/CSS-based effects. Flickr.com scroll-through demo Features In 2013, we covered performance details of developing a vertical-scrolling page using some parallax effects, targeting and optimizing for a smooth experience. In 2014, we are using some of the same techniques, but have added some new twists and tricks. In addition, there is more consideration for some smaller screens this year, given the popularity of tablet and other portable devices. Briefly: Fluid slideshow-like UI, scale3d() and zoom -based scaling of content for larger screens Inline HTML5 <video> , “retina” / hi-DPI scale (with fallback considerations) Timeline-based HTML transition effects, synced to HTML5 video “Hijacking” of touch/mouse/keyboard scroll actions, where appropriate to experience Background parallax, scale/zoom and blur effects (where supported) Usability Considerations: Scrolling In line with current trends, our designers intended to have a slideshow-like experience. The page was to be split into multiple “slides” of a larger presentation, with perhaps some additional navigation elements and cues to help the user move between slides. Out in the wild, implementations of the slideshow-style web page widely in their flexibility. Controlling the presentation like this is challenging and dangerous from a technical perspective, as the first thing you are doing is trying to prevent the browser from doing what it does well (arbitrary bi-directional scrolling, in either staggered steps or smooth inertia-based increments depending on the method used) in favour of your own method which is more likely to have holes in its implementation. If you’re going to hijack a basic interaction like scrolling, attention to detail is critical. Because you’ve built something non-standard, even in the best case the user may notice and think, “That’s not how it normally scrolls, but it responded and now I’m seeing the next page.” If you’re lucky, they could be using a touchpad to scroll and may barely notice the difference. By carefully managing the display of content to fit the screen and accounting for common scroll actions, we are able to confidently override the browser’s default scroll behaviour in most cases to present a unique experience that’s a hybrid of web page and slideshow. The implementation itself is fairly straightforward; you can listen to the mouse wheel event (triggered both by physical wheels and touchpads), determine which direction the user is moving in, debounce further wheel events and then run an animation to transition to the next slide. It’s imperfect and subject to double-scrolling, but most users will not “throw” the scroll so hard that it retains enough inertia and continues to fire after your animation ends. Additionally, if the user is on an OS that shows a scrollbar (i.e., non-OS X or OS X with a mouse plugged in), they should be able to grab and drag the scrollbar and navigate through the page that way. Don’t even try messing with that stuff – your users will kill you with pitchforks, ensuring you will be sent to Web Developer Usability Anti-Pattern Hell. You will not pass Go, and will not collect $200. Content Sizing In order to get a slideshow-like experience, each “slide” had to be designed to fit within common viewport dimensions. We assumed roughly 1024×768, but ended up targeting a minimum viewport height of around 600px – roughly what you’d get on a typical 13″ MacBook laptop with a maximized window and a visible dock. In retrospect, that doesn’t feel like a whole lot of space; it’s important to consider if you’re also aiming to display your work on mobile screens, as well. Once each slide fit within our target dimensions, the positioning of each slide’s content could be tightly controlled. Each is in a relatively-positioned container so they stack vertically as normal, and the height is at minimum, the height of the viewport or the natural offsetHeight dictated by the content itself. Reasonable defaults are first assigned by CSS, and future updates are done via JS at initial render and on window.resize() . With each slide being one viewport high, one might assume we could then let the user scroll freely through the content, perusing at will. We decided to go against this and control the scrolling for a few reasons. Web browsers’ default “page down” (spacebar or page up/down keys, etc.) does not scroll through 100% of the viewport, as we would want in this case; there is always some overlap from the previous page. While this is completely logical considering the context of reading a document, etc., we want to scroll precisely to the beginning of the next frame. Thus, we use JS to animate and set scrollTop . Content does not normally shift vertically when the user resizes their browser, but will now due to JS adjusting each slide’s height to fit the viewport as mentioned. Thus, we must also adjust scrollTop to re-align to the current slide, preventing the content from shifting as the user resizes the window. Sneaky. We want to know when a user enters and leaves a slide, so we can play or reset HTML5 <video> elements and related animations as appropriate. By controlling scroll, we have discrete events for both. Content Scaling Given that we know the dimensions of our content and the dimensions of the browser viewport, we are able to “zoom” each slide’s absolutely-positioned content to fit nicely within the viewport of larger screens. This is a potential minefield-type feature, but can be applied selectively after careful testing. Just like min and max-width , you can implement your own form of min-scale and max-scale . Content Scaling demo Avoiding Pixelation Scaling raster-based content, of course, is subject to degrading pretty quickly in terms of visual quality. To help combat pixelation, scaling is limited to a reasonable maximum – i.e., 150% – and where practical, retina/hi-DPI (@2x) assets are used for elements like icons, logos and so forth, regardless of screen type. This works rather well on standard LCDs. On the hi-DPI side, thankfully, huge retina screens are not common and there is less potential for scaling. Depending on browser, content scaling can be done via scale3d() or the old DOM .style.zoom property (yes, it wasn’t just meant for triggering layout in old IE.) From my findings, Webkit appears to rasterize all content before scaling it. As a result, vector-based content like text is blurred in Webkit when using scale3d() . Thus, Wekbit gets the older .style.zoom approach. Firefox doesn’t support .style.zoom , but does render crisp text when using scale3d() . There are few tricks to getting scaling to work, short of updating it alongside initial render and window.resize() events. overflow: hidden may need to be applied to the frame container, in the scale3d() case. JS Performance: window.onscroll() and window.onresize() It’s no secret: scroll and resize are two popular JavaScript events that can cause a lot of layout thrashing. Some cost is incurred by the browser’s own layout, decoding of images, compositing and painting, but most notable thrashing is caused by developers attaching expensive UI refresh-related functions to these events. Parallax effects on scrolling is a popular example, but resize can trigger it as well. In this case, synchronous code fires on resize so that the frames immediately resize themselves to fit the new window dimensions, and the window’s scrollTop property is adjusted to prevent any vertical shift of content. This is expensive, but is justified in keeping the view consistent with what the user would expect during resize. Scroll events on this page are throttled (that is, there is not a 1:1 event-firing-to-code-running ratio) so that the parallax, zoom and blur effects on the page – which can be expensive when combined – are updated at a lower, yet still responsive interval, thus lowering the load on rendering during scroll. Fun stuff: Background sizing, Parallax, Scale-based Motion, Blur Effects via Opacity, Video/HTML Timelines The parallax thing has been done before, by Flickr and countless other web sites. This year, some twists on the style included a gradual blur effect introduced as the user scrolls down the page, and in some cases, a slight motion effect via scaling. Backgrounds and Overlays For this fluid layout, the design needed to be flexible enough that exact background positioning was not a requirement. We wanted to retain scale, and also cover the browser window. A fixed-position element is used in this case, width/height: 100% , background-size: cover and background-position: 50% 0px , which works nicely for the main background and additional image-based overlays that are sometimes shown. The background tree scene becomes increasingly blurry as the user scrolls through the page. CSS-based filters and canvas were options, but it was simpler to apply these as background images with identical scaling and positioning, and overlay them on top of the existing tree image. As the user scrolls through the top half of the page, a “semi-blur” image is gradually made visible by adjusting opacity. For the latter half, the semi-blur is at 100% and a third “full-blur” image is faded in using the same opacity approach. Where supported, the background also also scales up somewhat as the user scrolls through the page, giving the effect of forward motion toward the trees. It is subtle when masked by the foreground content, but still noticeable. Here is an example with the content hidden, showing how the background moves during scroll. Background parallax/blur/zoom demo Parallax + Scaling In terms of parallax, a little extra image is needed for the background to be able to move. Thus, the element containing the background images is width: 100% and height: 110% . The background is scaled by the browser to fit the container as previously described, and the additional 10% height is off-screen “parallax buffering” content. This way, the motion is always relative in scale and consistent with the background. HTML5 Video and “Timelines” in JS One of the UI videos in this page shows live filters being applied – “Iced Tea”, “Throwback” and so on, and we wanted to have those filters showing outside the video area also if possible. Full-screen video was considered briefly, but wasn’t appropriate for this design. Thus, it was JS to the rescue. By listening to a video’s timeupdate event and watching the currentTime attribute, events could be queued in JS with an associated time, and subsequently fired roughly in sync with effects in the video. Filter UI demo In this case, the HTML-based effects were simple CSS opacity transitions triggered by changing className values on a parent element. When a user leaves a slide, the video can be reset when the scroll animation completes, and any filter / transition-based effects can also be faded out. If the user returns to the slide, the video and effects seamlessly restart from their original position. HTML5 Video Fallbacks Some clients treat inline HTML5 video specially, or may lack support for the video formats you provide. Both MP4 (H.264) and WebM are used in this case, but there’s still no guarantee of support. Tablet and mobile devices are unlikely to allow auto-play of video, may show a play arrow-style overlay, or may only play video in full-screen mode. It’s good to keep these factors in mind when developing a multimedia-rich page; many users are on smaller screens – tablets, phones and the like – which need to be given consideration in terms of their features and support. Some clients also support a poster attribute on the video element, which takes a URL to a static poster frame image. This can sometimes be a good fallback, where a device may have video support but fails to decode or play the provided video assets. Some browsers don’t support the poster attribute, so in those instances you may want to listen for error events thrown from the video element. If it looks like the video can’t be played, you can use this event as a signal to hide the video element with an image of the poster frame URL. Considerations for Tablets and Smaller Screens The tl;dr of this section: Start with a simple CSS-only layout, and (carefully) progressively enhance your effects via JS depending on the type of device. ALL THE SCREENS Smaller devices don’t have the bandwidth, CPU or GPU of their laptop and desktop counterparts. Additionally, they typically do not fire resize and scroll events with the same rapid interval because they are optimized for touch and inertia-based scrolling. Therefore, it is best to avoid “scroll hijacking” entirely; instead, allow users to swipe or otherwise scroll through the page as they normally would. Given the points about video support and auto-play not being allowed, the benefits offered by controlled scrolling are largely moot on smaller devices. Users who tap on videos will find that they do play where supported, in line with their experience on other web sites. The iPad with iOS 7 and some Samsung tablets, for example, are capable of playing inline video, but the iPhone will go to a full-screen view and then return to the web page when “done” is tapped. Without controlled scrolling and regular scroll events being fired, the parallax, blur and zoom effects are also not appropriate to use on smaller screens. Even if scroll events were fired or a timer were used to force regular updates at a similar interval, the effects would be too heavy for most devices to draw at any reasonable frame rate. The images for these effects are also fairly large, contributing to page weight. Rendering Performance Much of what helped for this page was covered in the 2013 article , but is worth a re-tread. Do as little DOM “I/O” as possible. Cache DOM attributes that are expensive (cause layout) to read. Possible candidates include offsetWidth , offsetHeight , scrollTop , innerWidth , innerHeight etc. Throttle your function calls, particularly layout-causing work, for listeners attached to window scroll and resize events as appropriate. Use translate3d() for moving elements (i.e., fast parallax), and for promoting selected elements to layers for GPU-accelerated rendering. It’s helpful to look at measured performance in Chrome’s DevTools “Timeline” / frames view, and the performance pane of IE 11’s “F12 Developer Tools” during development to see if there are any hotspots in your CSS or JS in particular. It can also be helpful to have a quick way to disable JS, to see if there are any expensive bits present just when scrolling natively and without regular events firing. JS aside, browsers still have to do layout, decode, resize and compositing of images for display, for example. Chrome DevTools: Initial page load, and scroll-through. There are a few expensive image decode and resize operations, but overall the performance is quite smooth. IE 11 + Windows 8.1, F12 Developer Tools: “UI Responsiveness” panel. Again, largely smooth with a few expensive frames here and there. The teal-coloured frames toward the middle are related to image decoding. For the record, I found that Safari 7.0.3 on OS X (10.9.2) renders this page incredibly smoothly when scrolling, as seen in the demo videos. I suspect some of the overhead may stem from JS animating scrollTop . If I were to do this again, I might look at using a transition and applying something sneaky like translate3d() to move the whole page, effectively bypassing scrolling entirely. However, that would require eliminating the scrollbar altogether for usability. What’s Next? While a good number of Flickr users are on desktop or laptop browsers, tablets and mobile devices are here to stay. With a growing number of users on various forms of portable web browsers, designers and developers will have to work closely together to build pages that are increasingly fluid, responsive and performant across a variety of screens, platforms and device capabilities. Did I mention we’re hiring? We have openings in our San Francisco office. Find out more at flickr.com/jobs .", "date": "2014-04-23,"},
{"website": "Flickr", "title": "Adventures in Jank Busting: Parallax, performance, and the new Flickr Home Page", "author": ["Scott Schiller"], "link": "https://code.flickr.net/2013/06/04/adventures-in-jank-busting-parallax-performance-and-the-new-flickr-home-page/", "abstract": "tl;dr version: transform3d() is your friend, but use sparingly. Chrome Dev Tools are awesome. What’s old is new again: Stealing from the arcade Back in 1985, games like Super Mario Bros. popularized the effect of horizontal parallax scrolling – a technique wherein the background moves at a slower speed relative to the foreground, giving the impression of depth. In 2013, the web is seeing a trend of vertical parallax effects as developers look to make pages feel more interactive and responsive. Your author’s $0.25, for the record, is that we’ll continue to see arcade and demoscene-era effects being ported over to the mainstream web in creative ways as client-side performance improves. While the effect can be aesthetically pleasing when executed correctly, parallax motion tends to be expensive to render – and when performance is lacking, the user’s impression of your site may follow suit. Vertical parallaxing is pretty straightforward in behaviour: For every Y pixels of vertical axis scrolling, move an absolutely-positioned image in the same direction at scale. There are some additional considerations for the offset of the element on the page and how far it can move, but the implementation remains quite simple. The following are some findings made while working on Flickr’s redesigned signed-out homepage at flickr.com/new/ , specifically related to rendering and scrolling performance. Events and DOM performance To optimize performance in the browser environment, it’s important to consider the expensive parts of DOM “I/O”. You ideally want a minimal amount of both, particularly since this is work being done during scrolling. Executing JavaScript on scroll is one of the worst ways to interrupt the browser, typically because it’s done to introduce reflow/layout and painting – thus, denying the browser the chance to use GPU/hardware-accelerated scrolling. window.onscroll() can also fire very rapidly on desktops, making way for a veritable flood of expensive scroll → reflow → paint operations if your “paths” are not fast. A typical parallax implementation will hook into window.onscroll() , and will update the backgroundPosition or marginTop of an element with a background image attached in order to make it move. An <img> could be used here, but backgrounds are convenient because they allow for positioning in relative units, tiling and so on. A minimal parallax example, just the script portion: window.onscroll = function(e) {\r\n\r\n  var parallax = document.getElementById('parallax-background');\r\n\r\n  parallax.style.marginTop = (window.scrollY/2) + 'px';\r\n\r\n} This could work for a single element, but quickly breaks down if multiple elements are to be updated. In any case, references to the DOM should be cached for faster look-ups; reading window.scrollY and other DOM attributes can be expensive due to potential to cause layout/reflow themselves, and thus should also be stored in a local variable for each onscroll() event to minimize thrashing. An additional performance consideration: Should all parallax elements always be moved, even those which are outside of the viewport? Quick tests suggested savings were negligible in this case at best. Even if the additional work to determine in-view elements were free, moving only one element did not notably improve performance relative to moving only three at a time. It appears that Webkit’s rendering engine is smart about this, as the only expensive operations seen here involve painting things within the viewport. In any event, using marginTop or backgroundPosition alone will not perform well as neither take advantage of hardware-accelerated compositing. And now, VOIDH (Video Or It Didn’t Happen) of marginTop -based parallax performing terribly: [flickr video=8859509880 secret=412e7dafff w=639 h=364] Look at that: Terrible. Jank city! You can try it for yourself in your browser of choice, via via ?notransform=1 . Enter the GPU: Hardware Acceleration To The Rescue Despite caching our DOM references and scroll properties, the cost of drawing all these pixels in software is very high. The trick to performance here is to have the GPU (hardware) take on the job of accelerating the compositing of the expensive bits, which can be done much faster than in software rendering. Elements can be promoted to a “layer” in rendering terms, via CSS transforms like translate3d() . When this and other translateZ() -style properties are applied, the element will then be composited by the GPU, avoiding expensive repaints. In this case, we are interested in having fast-moving parallax backgrounds. Thus, translate3d() can be applied directly to the parallax element. window.onscroll = function(e) {\r\n\r\n  // note: most browsers presently use prefixes: webkitTransform, mozTransform etc.\r\n\r\n  var parallax = document.getElementById('parallax-background');\r\n\r\n  parallax.style.transform = 'translate3d(0px,' + (window.scrollY/2) + 'px, 0px)';\r\n\r\n} In webkit-based browsers like Safari and Chrome, the results speak for themselves. [flickr video=8758952624 secret=f414d2f2fd w=640 h=448] Look, ma! Way less jank! The performance here is comparable to the same page with parallax disabled (i.e., regular browser scrolling/drawing.) GPU acceleration sounds like a magic bullet, but it comes at the cost of both video memory and cache. If you create too many layers, you may see rendering problems and even degraded performance – so use them selectively. It’s also worth noting that browser-based hardware acceleration and performance rests on having agreement between the browser, drivers, OS, and hardware. Firefox might be sluggish on one machine, and butter-smooth on another. High-density vs. standard-resolution screens make a big difference in paint cost. All GPUs are made equal, but some GPUs are made more equal than others. The videos and screenshots for this post were made on my work laptop, which may perform quite differently than your hardware. Ultimately, you need to test your work on a variety of devices to see what real-world performance is like – and this is where Chrome’s dev tools come in handy. Debugging Render Performance In brief, Chrome’s Developer Tools are awesome. Chrome Canary typically has the freshest features in regards to profiling, and Safari also has many of the same. The features most of interest to this entry are the Timeline → Frames view, and the gear icon’s “Show Paint rectangles” and “Show composited layer borders” options. Timeline → Frames view: Helpful in identifying expensive painting operations. Paint rectangles + composited layer borders, AKA “Plaid mode.” Visually identify layers. Timeline → Frames view Timeline’s “Frames” view allows you to see how much time is spent calculating and drawing each frame during runtime, which gives a good idea of rendering performance. To maintain a refresh rate of 60 frames per second, each frame must take no longer than 16 milliseconds to draw. If you have JS doing expensive things during scroll events, this can be particularly challenging. Expensive frames in Flickr’s case stem primarily from occasional decoding of JPEGs and non-cached image resizes, and more frequently, compositing and painting. The less of each that your page requires, the better. Paint rectangles It is interesting to see what content is being painted (and re-painted) by the browser, particularly during scroll events. Enabling paint rectangles in Chrome’s dev tools results in red highlights on paint areas. In the ideal case when scrolling, you should see red only around the scrollbar area; in the best scenario, the browser is able to efficiently move the rest of the HTML content in hardware-accelerated fashion, effectively sliding it vertically on the screen without having to perform expensive paint operations. Script-based DOM updates, CSS :hover and transition effects can all cause painting to happen when scrolling, so keep these things in mind as well. Composited layer borders As mentioned previously, layers are elements that have been promoted and are to be composited by the GPU, via CSS properties like translate3d() , translateZ() and so on. With layer borders enabled, you can get a visual representation of promoted elements and review whether your CSS is too broad or too specific in creating these layers. The browser itself may also create additional layers based on a number of scenarios such as the presence of child elements, siblings, or elements that overlap an existing layer. Composited borders are shown in brown. Cyan borders indicate a tile, which combines with other tiles to form a larger composited layer. Other notes Image rendering costs When using parallax effects, “full-bleed” images that cover the entire page width are also popular. One approach is to simply use a large centered background image for all clients, regardless of screen size; alternately, a responsive @media query-style approach can be taken where separate images are cut to fit within common screen widths like 1024, 1280, 1600, 2048 etc. In some cases, however, the single-size approach can work quite nicely. In the case of the Flickr homepage, the performance cost in using 2048-pixel-wide background images for all screens seemed to be negligible – even in spite of “wasted” pixels for those browsing at 1024×768. The approach we took uses clip-friendly content, typically a centered “hero” element with shading and color that extends to the far edges. Using this approach, the images are quite width-agnostic. The hero-style images also compress quite nicely as JPEGs thanks to their soft gradients and lighting; as one example, we got a 2048×950-pixel image of a flower down to 68 KB with little effort. Bandwidth aside, the 2048-pixel-wide images clip nicely on screens down to 1024 pixels in width and with no obvious flaws. However, Chrome’s dev tools also show that there are costs associated with decoding, compositing, re-sizing and painting images which should be considered. Testing on my work laptop*, “Image resized (non-cached)” is occasionally shown in Chrome’s timeline → frames view after an Image Decode (JPEG) operation – both of which appear to be expensive, contributing to a frame that took 60 msec in one case. It appears that this happens the first time a large parallax image is scrolled into the viewport. It is unclear why there is a resize (and whether it can be avoided), but I suspect it’s due to the retina display on this particular laptop. I’m not using background-size or otherwise applying scaling/sizing in CSS, merely positioning eg., background-position:50% 50%; and background-repeat: no-repeat; . As curiosity sets in, this author will readily admit he has some more research to do on this front. ;) There are also aspects to RAM and caching that can affect GPU performance. I did not dig deeply into this specifically for Flickr’s new homepage, but it is worth considering the impact of the complexity and number of layers present and active at any given time. If regular scrolling triggers non-cached image resizes each time an asset is scrolled into view, there may be a cache eviction problem stemming from having too many layers active or present at once. * Work laptop: Mid-2012 15″ Retina MBP, 16 GB RAM, 2.6 Ghz Intel i7, NVIDIA GeForce GT 650M/1024 MB, OS X 10.8.3. Debugging in action Here are two videos showing performance of flickr.com/new/ with transforms disabled and enabled, respectively. Transforms off (marginTop-based parallax) [flickr video=8845365987 secret=cfe2155de7 w=640 h=361] Notice the huge spikes on the timeline with transforms disabled, indicating many frames that are taking up to 80 msec to draw; this is terrible for performance, “blowing the frame budget” of 16 ms, and lowers UI responsiveness significantly. Red paint rectangles indicate that the whole viewport is being repainted on scroll, a major contributor to performance overhead. With compositing borders, you see that every “strip” of the page – each parallax background, in effect – is rendered as a single layer. A quick check of the FPS meter and “continuous repaint” graphs does not look great. Side note: Continuous repaint is most useful when not scrolling. The feature causes repeated painting of the viewport, and displays an FPS graph with real-time performance numbers. You can go into the style editor while continuous repaint is on and flip things off, e.g., disabling box-shadow , border-radius or hiding expensive elements via display to see if the frame rate improves. Transforms on ( translate3d() -based parallax) [flickr video=8845359795 secret=5fa8467374 w=639 h=364] With GPU acceleration, you see much-improved frame times, thus a higher framerate and a smoother, more-responsive UI. There is still the occasional spike when a new image scrolls into view, but there is much less jank overall than before. Paint rectangles are much less common thanks to the GPU-accelerated compositing, and layer borders now indicate that more individual elements are being promoted. The FPS meter and continuous repaint mode does have a few dips and spikes, but performance is notably improved. You may notice that I intentionally trigger video playback in this case, to see how it performs. The flashing red is the result of repainting as the video plays back – and in spite of overflow: hidden -based clipping we apply for the parallax effect on the video, it’s interesting to notice that the overflowed content, while not visible, is also being painted. Miscellany Random bits: HTML5 <video> A frame from a .webm and H.264-encoded video, shown in the mobile portion of Flickr’s redesigned home page. We wanted the signed-out Flickr homepage to highlight our mobile offerings, including an animation or video showing a subtly-rotating iPhone demoing the Flickr iOS app on a static background. Instead of an inline video box, it felt appropriate to have a full-width video following the pattern used for the parallax images. The implementation is nearly the same, and simply uses a <video> element instead of a CSS background. With video, the usual questions came up around performance and file size: Would a 2048-pixel-wide video be too heavy for older computers to play back? What if we cropped the video only to be as wide as needed to cover the area being animated (eg., 500 pixels), and used a static JPEG background to fill in the remainder of the space? As it turned out, encoding a 2048×700 video and positioning it like a background element – even including a slight parallax effect – was quite reasonable. Playback was flawless on modern laptops and desktops, and even a 2006-era 1.2 GHz Fujitsu laptop running WinXP was able to run the video at reasonable speed. Per rendering documentation from the Chrome team, <video> elements are automatically promoted to layers for the GPU where applicable and thus benefit from accelerated rendering. Due to the inline nature of the video, we excluded it from display on mobile devices, and show a static image to clients that don’t support HTML5 video. Perhaps the most interesting aspect of the video was file size. In our case, the WebM-encoded video (supported natively in Chrome, Firefox, and Opera) was clearly able to optimize for the low amount of motion within the wide frame, as eight seconds of 2048×700 video at 24 fps resulted in a .webm file of only 900 KB . Impressive! By comparison, the H.264-encoded equivalent ended up being about 3.8 MB, with a matching data rate of ~3.8 mbps. The “Justified” View It’s worth mentioning that the Justified photos at the bottom of the page lazy-load in, and have been excluded from any additional display optimizations in this case. There is an initial spike with the render and subsequent loading of images, but things settle down pretty quickly. Blindly assigning translate-type transforms to the Justified photo container – a complex beast in and of itself – causes all sorts of rendering hell to break loose. In Review This article represents my findings and approach to getting GPU-accelerated compositing working for background images in the Webkit-based Chrome and Safari browsers, in May 2013. With ever-changing rendering engines getting smarter over time, your mileage may vary as the best route to the “fast path” changes. As numerous other articles have said regarding performance, “don’t guess it, test it.” To recap: Painting : Expensive. Repaints should be minimal, and limited to small areas. Reduce by carefully choosing layers. Compositing : Good! Fast when done by the GPU. Layers : The secret to speed, when done correctly. Apply sparingly. References / further reading: Google I/O 2013: Web Page Design with the GPU in Mind (YouTube) GPU Accelerated Compositing in Chrome (Chromium) Accelerated Compositing in Chrome: The Layer Model (HTML5Rocks) Scrolling (HTML5Rocks) Parallaxin’ (HTML5Rocks) jankfree.org … And finally, did I mention we’re hiring ? (hint: view-source :))", "date": "2013-06-4,"},
{"website": "Flickr", "title": "Flickr API Going SSL-Only on June 27th, 2014", "author": ["Chris Martin"], "link": "https://code.flickr.net/2014/04/30/flickr-api-going-ssl-only-on-june-27th-2014/", "abstract": "If you read my recent post , you know that the Flickr API fully supports SSL. We’ve already updated our web and mobile apps to use HTTPS, and you no longer need to use “secure.flickr.com” to access the Flickr API via SSL. Simply update your code to call: https://api.flickr.com/ We want communication with Flickr to be secure, all the time. So, we are tightening things up. Effective this week, all new API keys will work via HTTPS only. On June 27th , we will deprecate non-SSL access to the API. If you haven’t already made the change to HTTPS, now is the time! Blackout Tests In preparation for the June 27th cut-off date, we will run two “blackout” tests, each for 2 hours, so that you can ensure that API calls in your app no longer use HTTP. If you have changed your code to use HTTPS, your app should function normally during the blackout window. If you have not changed your code to use HTTPS, then during the 2-hr blackout window all API calls from your application will fail. The API will return a 403 status code for non-SSL requests. Important Dates and Times Change in new API keys :  6 May 2014 (If you request a new API key after 6 May, it will be issued for HTTPS only) First blackout window : 3 June 2014, 10:00-12:00 Pacific Daylight Time (PDT) / 17:00-19:00 GMT Second blackout window : 17 June 2014, 18:00-20:00 (PDT) / 18 June 2014, 01:00-03:00 GMT Non-SSL calls deprecated : 27 June 2014, 10:00 (PDT) / 17:00 GMT More About the Flickr API Endpoints In the API documentation, all of the endpoints have been updated to HTTPS. While OAuth adds security by removing the need to authenticate by username and password, sending all traffic over SSL further protects our users’ data while in transit. The SSL endpoints for the Flickr API are: https://api.flickr.com/services/rest/\r\n\r\nhttps://api.flickr.com/services/soap/\r\n\r\nhttps://api.flickr.com/services/xmlrpc/ And for uploads: https://up.flickr.com/services/upload/\r\n\r\nhttps://up.flickr.com/services/replace/ For applications that use well-established HTTP client libraries, this switch should only require updating the protocol and (maybe) some updated configuration. We realize that this change might be more difficult for some. We will follow the Developer Support Group closely, so please let us hear your questions. We will respond to them there, and will collect questions of general interest in the below FAQ. FAQs About the Transition to SSL-Only for the Flickr API Question :  I only have a Flickr API key because I use an application or plugin that calls the Flickr API.  Will that application or plugin continue to work after June 27th?  Do I need to do something? Answer :  An application or plugin that calls the Flickr API will stop working on June 27th if its owner does not make the changes we’ve described above.  There are many, many providers of such services and plugins.  We have notified them about this transition via email, blog post, developer lists, and on Twitter.  As a user of such a service or plugin, you have no action to do for the transition unless the application or plugin owner asks you to upgrade to a new version.  You also have the option to reach out to the application or plugin owner to assure yourself of their plans to handle this transition. Question: Are all http://www.flickr.com urls going to be HTTPS from now on? Answer: Yes, all http://www.flickr.com urls returned by the API are now HTTPS, and all requests to HTTP in the browser are redirected to HTTPS. Question: Should I switch my code to HTTPS right away or should I wait a bit? Answer: Switch now. The important thing is for your app to be changed to HTTPS before the first blackout on 3 June 2014. In fact, if your app is a mobile app, the earlier the better, so that your users will be more likely to upgrade before the first blackout. Question :  Do I need a new API key to replace my old one for this transition? Answer :  No, you don’t need a new API key for this transition.  You keep your existing key, and you change the code where you call the Flickr API, so that you call it with the HTTPS protocol, instead of HTTP. Change it to this: https://api.flickr.com/ Question :  What do I do with the new API key that is being issued by Flickr as of May 6? Answer :  We are not automatically issuing a new key to you.  What happened on 6 May was a change to how we handle new API keys.  From now on, if you submit a request for a new key, that new key will only support calls to the Flickr API over HTTPS; it will not support calls to the API over HTTP.  You do not need to request a new API key for this transition. Question :  I received your email about the API going SSL-only on June 27th.  Do I need to do something? Answer :  Maybe not.  If you already call the API over HTTPS, then you’re good.  No action needed.  But if your code currently calls the API over HTTP, then, YES, you do need to do something.  In your code you need to change the protocol to HTTPS.  Like this: https://api.flickr.com/ Question: If I use a protocol-less call to the API or match the protocol of the page that is making the call, do I need to change anything? Answer: Yes, you should change your calls to specifically use HTTPS. During the blackout period and starting on June 27th, protocol-less calls to the Flickr API from non-SSL pages will fail. Question: Will the old https://secure.flickr.com endpoints continue to be supported in addition to the new https://api.flickr.com endpoints, or will only the latter be supported? Answer: Yes, https://secure.flickr.com endpoints will still be supported.  If you use that today, your application will continue to work during the blackout windows and after 27 June.", "date": "2014-04-30,"},
{"website": "Flickr", "title": "Computer vision at scale with Hadoop and Storm", "author": ["Tim A. Miller"], "link": "https://code.flickr.net/2014/05/20/computer-vision-at-scale-with-hadoop-and-storm/", "abstract": "Recently, the team at Flickr has been working to improve photo search. Before our work began, Flickr only knew about photo metadata — information about the photo included in camera-generated EXIF data, plus any labels the photo owner added manually like tags, titles, and descriptions. Ironically, Flickr has never before been able to “see” what’s in the photograph itself. Over time, many of us have started taking more photos, and it has become routine — especially with the launch last year of our free terabyte* — for users to have many un-curated photos with little or no metadata. This has made it difficult in some cases to find photos, either your own or from others. So for the first time, Flickr has started looking at the photo itself**. Last week, the Flickr team presented this technology at the May meeting of the San Francisco Hadoop User’s Group at our new offices in San Francisco. The presentation focuses on how we scaled computer vision and deep learning algorithms to Flickr’s multi-billion image collection using technologies like Apache Hadoop and Storm . (In a future post here, we’ll describe the learning and vision systems themselves in more detail.) [youtube=http://www.youtube.com/watch?v=OrhAbZGkW8k&w=640&h=360] Slides available here: Flickr: Computer vision at scale with Hadoop and Storm . Thanks very much to Amit Nithian and Krista Wiederhold (organizers of the SFHUG meetup) for giving us a chance to share our work. If you’d like to work on interesting challenges like this at Flickr in San Francisco, we’d like to talk to you! Please look here for more information: http://www.flickr.com/jobs * Today is the first anniversary of the terabyte ! ** Your photos are processed by computers – no humans look at them. The automatic tagging data is also protected by your privacy settings.", "date": "2014-05-20,"},
{"website": "Flickr", "title": "Redis Sentinel at Flickr", "author": ["by Richard “Hammertime” Thorn and Shawn \"The F-Train\" Cook"], "link": "https://code.flickr.net/2014/07/31/redis-sentinel-at-flickr/", "abstract": "We recently implemented Redis Sentinel at Flickr to provide automated Redis master failover for an important subsystem and we wanted to share our experience with it. Hopefully, we can provide insight into our experience adopting this relatively new technology and some of the nuances we encountered getting it up and running. Although we try to provide a basic explanation of what Sentinel is and how it works, anyone who is new to Redis or Sentinel should start with the excellent Redis and Sentinel documentation. At Flickr we use an offline task processing system that allows us to execute heavyweight operations asynchronously from our API and web pages. This prevents these operations from making users wait needlessly for pages to render or API methods to return. Our task system handles millions of tasks per day which includes operations like photo uploads , user notifications and metadata edits. In this system, code can push a task onto one of several Redis-backed queues based on priority and operation, then forget about the task. Many of these operations are critical and we need to make sure we process at least 99.9999% of them (less than 1 in 1 million dropped). Additionally, we need to make sure this system is available to insert and process tasks at least 99.995% of the time – no more than about 2 minutes a month downtime. Until a few months ago our Redis BCP consisted of: Redis append-only-files (AOF) Warm-swappable slaves Logging task insert failures Upon master failure, the recovery plan included several manual steps: reconfiguring code to take the Redis master(s) offline and manually promoting a Redis slave (a mildly time consuming activity). Then we would rebuild and backfill unprocessed data from AOF files and error logs — a very time consuming activity. We knew if we lost a master we would have hours and hours of less-than-satisfying work to run the recovery plan, and there was potential for user impact and even a small amount of data loss. We had never experienced a Redis master failure, but we all know that such events are simply a matter of time. Overall, this fell far short of our durability and availability goals. Configuring Sentinel We started by installing and testing Sentinel in a development environment and the first thing we noticed was how simple Sentinel is to use and how similar the syntax is to Redis. We read Aphyr’s article and his back-and-forth blog duel with Salvatore and verified Aphyr’s warning about the “split brain” scenario. Eventually we decided the benefits outweighed the risks in our specific use case. During testing we learned about some Sentinel nuances and got a better feel for appropriate configuration values, many of which have little or no community guidance yet. One such example was choosing a good value for the level-of-agreement setting, which is the number of Sentinels simultaneously reporting a host outage before automatic failover starts. If this value is too high then you’ll miss real failures and if it’s too low you are more susceptible to false alarms.  (*thanks to Aleksey Asiutin(@aasiutin) for the edit!)  In the end, we looked at the physical topology of our hosts over rack and switches and chose to run a relatively large number of Sentinel instances to ensure good coverage. Based on tuning in production we chose a value for level-of-agreement equal to about 80% of the Sentinel instances. The down-after-milliseconds configuration setting is the time the Sentinels will wait with no response to their ping requests before declaring a host outage. Sentinels ping the hosts they monitor approximately every second, so by choosing a value of 3,100 we expect Sentinels to miss 3 pings before declaring host outage. Interestingly, because of Sentinel’s ping frequency we found that setting this value to less than 1,000 results in an endless stream of host outage notifications from the Sentinels, so don’t do that.  We also added an extra 100 milliseconds (3,100ms rather than 3,000ms) to allow for some variation in Redis response time. We chose a parallel-syncs value of 1.  This item dictates the number of slaves that are reconfigured simultaneously after a failover event.  If you serve queries from the read-only slaves you’ll want to keep this value low. For an explanation of the other values we refer you to the self-documented default sentinel.conf file. An example of the Sentinel configuration we use: sentinel monitor cluster_name_1 redis_host_1 6390 35\r\nsentinel down-after-milliseconds cluster_name_1 3100\r\nsentinel parallel-syncs cluster_name_1 1\r\n\r\nsentinel monitor cluster_name_2 redis_host_2 6391 35\r\nsentinel down-after-milliseconds cluster_name_2 3100\r\nsentinel parallel-syncs cluster_name_2 1\r\n\r\nport 26379\r\n\r\npidfile [path]/redis-sentinel.pid\r\nlogfile [path]logs/redis/redis-sentinel.log\r\ndaemonize yes An interesting nuance of Sentinels is that they write state to their configuration file. This presented a challenge for us because it conflicted with our change management procedures. How do we maintain a dependably consistent startup configuration if the Sentinels are modifying the config files at runtime? Our solution was to create two Sentinel config files. One is strictly maintained in Git and not modified by Sentinel. This “permanent” config file is part of our deployment process and is installed whenever we update our Sentinel system configuration (i.e.: “rarely”). We then wrote a startup script that first duplicates the “permanent” config file to a writable “temporary” config file, then starts Sentinel and passes it the “temporary” file via command-line params. Sentinels are allowed to modify the “temporary” files as they please. Interfacing with Sentinel A common misconception about Sentinel is that it resides in-band between Redis and Redis clients. In fact, Sentinel is out-of-band and is only contacted by your services on startup. Sentinel then publishes notifications when it detects a Redis outage. Your services subscribe to Sentinel, receive the initial Redis host list, and then carry on normal communication directly with the Redis host. The Sentinel command syntax is very similar to Redis command syntax. Since Flickr has been using Redis for a long time the adaptation of pre-existing code was pretty straightforward for us. Code modifications consisted of adding a few Java classes and modifying our configuration syntax. For Java-to-Redis interaction we use Jedis , and for PHP we use Predis and libredis . Using Sentinel from Jedis is not documented as well as it could be. Here’s some code that we hope will save you some time: // Verify that at least one Sentinel instance in the Set is available and responding.\r\n// sentinelHostPorts: String format: [hostname]:[port]\r\nprivate boolean jedisSentinelPoolAvailable(Set&lt;String&gt; sentinelHostPorts, String clusterName){\r\n   log.info(&quot;Trying to find master from available Sentinels...&quot;);\r\n   for ( String sentinelHostPort : sentinelHostPorts ) {\r\n      List&lt;String&gt; hostPort = Arrays.asList( sentinelHostPort.split(&quot;:&quot;) );\r\n      String hostname = hostPort.get(0);\r\n      int port = Integer.parseInt( hostPort.get(1) );\r\n      try {\r\n         Jedis jedis = new Jedis( hostname, port );\r\n         jedis.sentinelGetMasterAddrByName( clusterName );\r\n         jedis.disconnect();\r\n         log.info(&quot;Connected to Sentinel host:%s port:%d&quot;, hostname, port);\r\n         return true;\r\n      } catch (JedisConnectionException e) {\r\n         log.warn(&quot;Cannot connect to Sentinel host:%s port:%d”, hostname, port);\r\n      }\r\n   }\r\n   return false;\r\n}\r\n\r\nprivate Pool&lt;Jedis&gt; getDefaultJedisPool() {\r\n   // Create and return a default Jedis Pool object…\r\n   // ...\r\n}\r\n\r\n// ConfigurationMgr configMgr ⇐ your favorite way of managing system configuration (up to you)\r\npublic Pool&lt;Jedis&gt; getPool(ConfigurationMgr configMgr) {\r\n   String clusterName = configMgr.getRedisClusterName();\r\n   Set&lt;String&gt; sentinelHostPorts = configMgr.getSentinelHostPorts();\r\n\r\n   if(sentinels.size()&gt;0) {\r\n      if(jedisSentinelPoolAvailable( sentinelHostPorts, clusterName )) {\r\n         return new JedisSentinelPool(clusterName, sentinelHostPorts);\r\n      } else {\r\n         log.warn(“All Sentinels unreachable.  Using default Redis hosts.”);\r\n         return getDefaultJedisPool();\r\n      }\r\n   } else {\r\n      log.warn(“Sentinel config empty.  Using default Redis hosts.”);\r\n      return getDefaultJedisPool();\r\n   }\r\n} Testing Sentinel at Flickr Before deploying Sentinel to our production system we had several questions and concerns: How will the system react to host outages? How long does a failover event last? How much of a threat is the split-brain scenario? How much data loss can we expect from a failover? We commandeered several development machines and installed a few Redis and Sentinel instances. Then we wrote some scripts that insert or remove data from Redis to simulate production usage. We ran a series of tests on this setup, simulating a variety of Redis host failures with some combination of the commands: kill -9 , the Sentinel failover command , and Linux iptables . This resulted in “breaking” the system in various ways. Figure: Redis master failure Figure: Network partition producing a ‘split-brain’ scenario How will the system react to host outages? For the most part we found Sentinel to behave exactly as expected and described in the Sentinel docs . The Sentinels detect host outages within the configured down-after-milliseconds duration, then send “subjective down” notifications, then send “objective down” notifications if the level-of-agreement threshold is reached. In this environment we were able to quickly and accurately test our response to failover events. We began with small test scripts, but eventually were able to run repeatable integration tests on our production software. Adding Redis to a Maven test phase for automated integration testing is a backlog item that we haven’t implemented yet. How long does a failover event last? The Sentinel test environment was configured with a down-after-milliseconds value of 3,100ms (just like production, see above). With this value Sentinels would produce a host outage notification after approximately 3 unsuccessful pings (one ping per second). In addition to the 3,100ms delay, we found there were 1-3 seconds in overhead for processing the failover event and electing a new Redis master, resulting in 4-6 seconds of total downtime. We are pretty confident we’ll see the same behavior in production (verified — see below). How much of a threat is the “split-brain” scenario? We carefully read Aphyr and Salvatore’s blog articles debating the threat of a “split brain scenario.” To summarize: this is a situation in which network connectivity is split, with some nodes still functioning on one side and other nodes continuing to function independently on the other side. The concern is the potential for the data set to diverge with different data being written to masters on both sides of the partition. This could easily create data that is either impossible or very difficult to reconcile. We recreated this situation and verified that a network partition could create disjoint concurrent data sets. Removing the partition resulted in Sentinel arbitrarily (from our perspective) choosing a new master and losing all data written (post-partitioning) to the other master. So the question is: given our production architecture, what is the probability of this happening and is it acceptable given the significant benefit of automatic failover? We looked at this scenario in detail considering all the potential failure modes in our deployment. Although we believe our production environment is not immune from split-brain, we are pretty sure that the benefits outweigh the risks. How much data loss can we expect from a failover event? After testing we were confident that Redis host outages could produce 4-6 seconds of downtime in this system. Rapid Sentinel automated failover events combined with reasonable backoff and retry techniques in the code logic were expected to further reduce data loss during a failover event. With Sentinel deployed and considering a long history of a highly stable Redis operation, we believed we could achieve 99.995% or more production availability – a few minutes of downtime per year. Sentinel in Production So how has Sentinel performed in production? Mostly it has been silent, which is a good thing. A month after finishing our deployment we had a hardware failure in a network switch that had some of our Redis masters behind it. Instead of having a potential scenario involving  tens of minutes of user impact with human-in-the-loop actions to restore service, automatic failover allowed us to limit impact to just seconds with no human intervention. Due to the quick master failover and other reliability features in the code, only 270 tasks failed to insert due to the outage — all of which were captured by logging. Based on the volume of tasks in the system, this met our 99.9999% task durability goal. We did however decide to re-run a couple tasks manually and for certain critical and low-volume tasks we’re looking at providing even more reliability. One more note from production experience. We occasionally see Sentinels reporting false “subjective down” events. Our Sentinel instances cohabitate host machines with other services. Occasionally these hosts get busy and we suspect these occasional load spikes affect the Sentinels’ ability to send and receive ping requests. But because our level-of-agreement is high, these false alarms do not trigger objective down events and are relatively harmless. If you’re deploying Sentinel on hosts that share other workloads, make sure that you consider potential impact of load patterns on those hosts and make sure you take some time to tune your level-of-agreement. Conclusion We have been very happy with Sentinel’s ease of use, relatively simple learning curve and brilliant production execution. So far the Redis/Sentinel combination is working great for us. Like this post? Have a love of online photography? Want to work with us? Flickr is hiring engineers , designers and product managers in our San Francisco office. Find out more at flickr.com/jobs . References Redis Sentinel Documentation – http://redis.io/topics/sentinel Redis Command Reference – http://redis.io/commands Aphyr: Call me maybe: Redis – http://aphyr.com/posts/283-call-me-maybe-redis Antirez: Reply to Aphyr Attack on Sentinel – http://antirez.com/news/55 Jedis Project Page – https://github.com/xetorthio/jedis", "date": "2014-07-31,"},
{"website": "Flickr", "title": "Exploring Life Without Compass", "author": ["Andy Locascio"], "link": "https://code.flickr.net/2014/08/18/exploring-life-without-compass/", "abstract": "Compass is a great thing. At Flickr, we’re actually quite smitten with it. But being conscious of your friends’ friends is important (you never know who they’ll invite to your barbecue), and we’re not so sure about this “Ruby” that Compass is always hanging out with. Then there’s Ruby’s friend Bundler who, every year at the Christmas Party, tells the same stupid story about the time the police confused him with a jewelry thief. Enough is enough! We’ve got history, Compass, but we just feel it might be time to try seeing other people. Solving for Sprites In order to find a suitable replacement (and for a bit of closure), we had to find out what kept us relying on Compass for so long. We knew the big one coming in to this great experiment: sprites. Flickr is a huge site with many different pages, all of which have their own image folders that need to be sprited together. There are a few different options for compiling sprites on your own, but we liked spritesmith for its multiple image rendering engines. This gives us some flexibility in dependencies. A grunt task is available for spritesmith, but it assumes you are generating only one sprite. Our setup is a bit more complex and we’d like to keep our own sprite mixin intact so we don’t actually have to change a line of code. With spritesmith and our own runner to iterate over our sprite directories, we can easily create the sprites and output the dimensions and urls via a simple Handlebars template to a Sass file. {{#each sprites}}\r\n    {{#each images}}\r\n        %{{../dir}}-{{name}}-dimensions {\r\n            width: {{coords.width}}px;\r\n            height: {{coords.height}}px;\r\n        }\r\n        %{{../dir}}-{{name}}-background {\r\n            background: image-url('{{../url}}') -{{coords.x}}px -{{coords.y}}px no-repeat;\r\n        }\r\n    {{/each}}\r\n{{/each}} You could easily put all three of these rules in the same declaration, but we have some added flexibility in mind for our mixin. It’s important to note that, because we’re using placeholders (the % syntax in Sass), nothing is actually written out unless we use it. This keeps our compiled CSS nice and clean (just like Compass)! @import 'path/to/generated/sprite/file'\r\n\r\n@mixin background-sprite($icon, $set-dimensions: false) {\r\n    @extend %#{$spritePath}-#{$icon}-background;\r\n\r\n    @if $set-dimensions == true {\r\n        @extend %#{$spritePath}-#{$icon}-dimensions;\r\n    }\r\n} Here, our mixin uses the Sass file we generated to provide powerful and flexible sprites. Note: Although retina isn’t shown here, adding support is as simple as extending the Sass mixin with appropriate media queries. We wanted to keep the example simple for this post, but it gives you an idea of just how extensible this setup is! Now that the big problem is solved, what about the rest of Compass’s functionality? Completing the Package How do we account for the remaining items in the Compass toolbox? First, it’s important to find out just how many mixins, functions, and variables are used. An easy way to find out is to compile with Sass and see how much it complains! sass --update assets/sass:some-temp-dir Depending on the complexity of your app, you may see quite a lot of these errors. error assets/css/base.scss (Line 3: Undefined mixin 'font-face'.) In total, we’re missing 16 mixins provided by Compass (and a host of variables). How do we replace all the great mixin functionality of Compass? With mixins of the same name, node-bourbon is a nice drop-in replacement. What is the point of all this work again? The Big Reveal Now that we’re comfortably off Compass, how exactly are we going to compile our Sass? Well try not to blink, because this is the part that makes it all worthwhile. Libsass is a blazing-fast C port of the Sass compiler that exposes bindings to modules like node-sass . Just how fast? With Compass, our compile times were consistently around a minute and a half to two minutes. Taking care of spriting ourselves and using libsass for Sass compilation, we’re down to 5 seconds. When you deploy as often as we do at Flickr (in excess of 10 times a day), that adds up and turns into some huge savings! What’s the Catch? There isn’t one! Oh, okay. Maybe there are a few little ones. We’re pretty willing to swallow them though. Did you see that compile time?! There are some differences, particularly with the @extend directive , between Ruby Sass and libsass. We’re anticipating that these small kinks will continue to be ironed out as the port matures. Additionally, custom functions aren’t supported yet, so some extensibility is lost in coming from Ruby (although node-sass does have support for the image-url built-in which is the only one we use, anyway). With everything taken into account, we’re counting down the days until we make this dream a reality and turn it on for our production builds. Like what you’ve read and want to make the jump with us? We’re hiring engineers , designers and product managers in our San Francisco office. Find out more at flickr.com/jobs .", "date": "2014-08-18,"},
{"website": "Flickr", "title": "Performance improvements for photo serving", "author": ["Archie Russell, Bei Wu and Peter Norby"], "link": "https://code.flickr.net/2014/08/26/performance-improvements-for-photo-serving/", "abstract": "We’ve been working to make Flickr faster for our users around the world. Since the primary photo storage locations are in the US, and information on the internet travels at a finite speed, the farther away a Flickr user is located from the US, the slower Flickr’s response time will be. Recently, we looked at opportunities to improve this situation. One of the improvements involves keeping temporary copies of recently viewed photos in locations nearer to users.  The other improvement aims to get a benefit from these caches even when a user views a photo that is not already in the cache. Regional Photo Caches For a few years, we’ve deployed regional photo caches located in Switzerland and Singapore. Here’s how this works. When one of our users in Vietnam requests a photo, we copy it temporarily to Singapore. When a second user requests the same photo, from, say, Kuala Lumpur, the photo is already present in Singapore. Flickr can respond much faster using this copy (only a few hundred kilometers away) instead of using the original file back in the US (over 8,000 km away). The first piece of our solution has been to create additional caches closer to our users. We expanded our regional cache footprint around two months ago. Our Australian users, among others, should now see dramatically faster load times. Australian users will now see the average image load about twice as fast as it did in March. We’re happy with this improvement and we’re planning to add more regional caches over the next several months to help users in other regions. Cache Prefetch When users in locations far from the US view photos that are already in the cache, the speedup can be up to 10x, but only for the second and subsequent viewers. The first viewer still has to wait for the file to travel all the way from the US. This is important because there are so many photos on Flickr that are viewed infrequently. It’s likely that a given photo will not be present in the cache. One example is a user looking at their Auto Upload album. Auto uploaded photos are all private initially. Scrolling through this album, it’s likely that very few of the photos will be in their regional cache, since no other users would have been able to see them yet. It turns out that we can even help the first viewer of a photo using a trick called cache warming. To understand how caching warming works, you need to understand a bit about how we serve images. For example, say that I’m a user in Spain trying to access the photostream of a user, Martin Brock, in the US. When my request for Martin Brock’s Photostream at https://www.flickr.com/photos/martinbrock/ hits our backend servers, our code quickly determines the most recent photos Martin has uploaded that are visible to me, which sizes will fit best in my browser, and the URLs of those images. It then sends me the list of those URLs in an HTML response. The user’s web browser reads the HTML, finds the image URLs and starts loading them from the closest regional cache. Standard image fetch So you’re probably already guessing how to speed things up.  The trick is to take advantage of the time in between when the server knows which images will be needed and the time when the browser starts loading them from the closest cache. This period of time can be in the range of hundreds of milliseconds. We saw an opportunity during this time to send the needed images over to the viewer’s regional cache in advance of their browser requesting the images. If we can “win the race” to do this, the viewer’s experience will be much faster, since images will load from the local cache instead of loading from the US. To take advantage of this opportunity, we created a new “cache warming” process called The Warmer. Once we’ve determined which images will be requested (the first few photos in Martin’s photostream) we send  a message from the API servers to The Warmer. The Warmer listens for messages and, based on the user’s location, it determines from which of the Flickr regional caches the user will likely request the image. It then pushes the image out to this cache. Optimized image fetch, with cache warming path indicated in red Getting this to work well required a few optimizations. Persistent connections Yahoo encrypts all traffic between our data centers. This is great for security, but the time to set up a secure connection can be considerable. In our first iteration of The Warmer, this set up time was so long that we rarely got the photo to the cache in time to benefit a user. To eliminate this cost, we used an Nginx proxy which maintains persistent connections to our remote data centers. When we need to push an image out – a secure connection is already set up and waiting to be used. Transport layer The next optimization we made helped us reduce the cost of sending messages to The Warmer.  Since the data we’re sending always fits in one datagram, and we also don’t care too much if a small percentage of these messages are never received, we don’t need any of the socket and connection features of TCP. So instead of using HTTP, we created a simple JSON format for sending messages using UDP datagrams. Another reason we chose to use UDP is that if The Warmer is not available or is reacting slowly, we don’t want that to cause slowdowns in the API. Queue management Naturally, some images are quite popular and it would waste resources to push them to the same cache repeatedly. So, the third optimization we applied was to maintain a list of recently pushed images in The Warmer. This simple “de-deduplication” cut the number of requests made by The Warmer by 60%. Similarly, The Warmer drops any incoming requests that are more than fifty milliseconds old. This “time-to-live” provides a safety valve in case The Warmer has fallen behind and can’t catch up. def warm_up_url(params):\r\n  requested_jpg = params['jpg']\r\n\r\n  colo_to_warm = params['colo_to_warm']\r\n  curl = &amp;quot;curl -H 'Host: &amp;quot; + colo_to_warm + &amp;quot;' '&amp;quot; + keepalive_proxy + &amp;quot;/&amp;quot; + requested_jpg + &amp;quot;'&amp;quot;\r\n  os.system(curl)\r\n\r\nif __name__ == '__main__':\r\n\r\n# create the worker pool\r\n\r\n  from multiprocessing.pool import ThreadPool\r\n  worker_pool = ThreadPool(processes=100)\r\n\r\n  while True:\r\n\r\n    # receive requests\r\n    json_data, addr = sock.recvfrom(2048)\r\n\r\n    params = json.loads(json_data)\r\n\r\n    requested_jpg = warm_params['jpg']\r\n    colo_to_warm =\r\n      determine_colo_to_warm(params['http_endpoint'])\r\n\r\n    if recently_warmed(colo_to_warm, requested_jpg) :\r\n      continue\r\n\r\n    if request_too_old(params) :\r\n      continue\r\n\r\n    # warm up urls\r\n    params['colo_to_warm'] = colo_to_warm\r\n\r\n    warm_result = worker_pool.apply_async(warm_up_url,(params,)) Cache Warmer pseudocode Java Our initial implementation of the Warmer was in Python, using a ThreadPool. This allowed very rapid prototyping and worked great — up to a point. Profiling the Python code, we found a large portion of time spent in socket calls. Since there is so little code in The Warmer, we tried porting to Java. A nearly line-for-line translation resulted in a greater than 10x increase in capacity. Results When we began this process, we weren’t sure whether The Warmer would be able to populate caches before the user requests came in. We were pleasantly surprised when we first enabled it at scale. In the first region where we’ve deployed The Warmer (Western Europe), we observed a reduced median latency of more than 200 ms, 95% of photos requests sped up by at least 100 ms, and for a small percentage of photos we see over 400 ms reduction in latency. As we continue to deploy The Warmer in additional regions, we expect to see similar improvements. Next Steps In addition to deploying more regional photo caches and continuing to improve prefetching performance, we’re looking at a few more techniques to make photos load faster. Compression Overall Flickr uses a light touch on compression. This results in excellent image quality at the cost of relatively large file sizes. This translates directly into longer load times for users. With a growing number of our users connecting to Flickr with wireless devices, we want to make sure we can give users a good experience regardless of whether they have a high-speed LTE connection or two-bars of 3G in the countryside. An important goal will be to make these changes with little or no loss in image quality. We are also testing alternative image encoding formats (like WebP ). Under certain conditions WebP compression may offer better image quality at the same compression ratio than JPEG can achieve. Geolocation and routing It turns out it’s not straightforward to know which photo cache is going to give the best performance for a user. It depends on a lot of factors, many of which change over time — sometimes suddenly. We think the best way to do this is with a system that adapts dynamically to “Internet weather.” Cache intelligence Today, if a user needs to see a medium sized version of an image, and that version is not already present in the cache, the user will need to wait to retrieve the image from the US, even if a larger version of the image is already in the cache. In this case, there is an opportunity to create the smaller version at the cache layer and avoid the round-trip to the US. Overall we’re happy with these improvements and we’re excited about the additional opportunities we have to continue to make the Flickr experience super fast for our users. Thanks for following along. Like what you’ve read and want to make the jump with us? We’re hiring engineers , designers and product managers in our San Francisco office. Find out more at flickr.com/jobs .", "date": "2014-08-26,"},
{"website": "Flickr", "title": "The Ins and Outs of the Yahoo Flickr Creative Commons 100 Million Dataset", "author": ["Bart Thomée and David A. Shamma"], "link": "https://code.flickr.net/2014/10/15/the-ins-and-outs-of-the-yahoo-flickr-100-million-creative-commons-dataset/", "abstract": "This past summer we (Yahoo Labs and Flickr) released the YFCC100M dataset that is the largest and most ambitious collection of Flickr photos and videos ever, containing 99,206,564 photos and 793,436 videos from 581,099 different photographers. We’re super excited about the dataset, because it is a reflection of how Flickr and photography have evolved over the past 10 years. And it contains photos and videos of almost everything under the sun (and yes, loads of cats). We’ve received a lot of emails and tweets asking for more details about the dataset, so in this blog post, we’ll gladly tell you. Each of the 100 million photos and videos is associated with a Creative Commons license that indicates how it may be used by others. The table below shows the complete breakdown of licenses in our dataset. Approximately 31.8% is marked for commercial use, while 17.3% has the most liberal license, which only requires attribution to the photographer. License Photos Videos 17,210,144 137,503 9,408,154 72,116 4,910,766 37,542 12,674,885 102,288 28,776,835 235,319 26,225,780 208,668 The photos and videos themselves are very diverse. We’ve found photos showing street scenes captured as part of photographer Andy Nystrom ‘s life-logging activities, photos of real-world events like protests and rallies, as well as photos of natural phenomena. Steve Rhodes Andy Nystrom BJ Graf To understand more about the visual content of the photos in the dataset, the Flickr Vision team used a deep-learning approach to find the presence of visual concepts, such as people, animals, objects, events, architecture, and scenery across a large sample of the corpus. There’s a diverse collection of visual concepts present in the photos and videos, ranging from indoor to outdoor images, faces to food, nature to automobiles. Concept Count outdoor 32,968,167 indoor 12,522,140 face 8,462,783 people 8,462,783 building 4,714,916 animal 3,515,971 nature 3,281,513 landscape 3,080,696 tree 2,885,045 sports 2,817,425 architecture 2,539,511 plant 2,533,575 house 2,258,396 groupshot 2,249,707 vehicle 2,064,329 water 2,040,048 mountain 2,017,749 automobile 1,351,444 car 1,340,751 food 1,218,207 concert 1,174,346 flower 1,164,607 game 1,110,219 text 1,105,763 night 1,105,296 There are 68,971,123 photos and videos in the set that have user-annotated tags. If we look at specific tags used, we see it is very common for people to use the year of capture, the camera brand, place names, scenery, and activities as tags. The top 25 tags (excluding the years of capture) and how often they were used are listed below, as well as the tag frequency distribution for the 100 most-frequently used tags. User Tag Count nikon 1,195,576 travel 1,195,467 usa 1,188,344 canon 1,101,769 london 996,166 japan 932,294 france 917,578 nature 872,029 art 854,669 music 826,692 europe 782,932 beach 758,799 united states 743,470 england 739,346 wedding 728,240 city 689,518 italy 688,743 canada 686,254 new york 685,311 vacation 680,142 germany 672,819 party 663,968 park 651,717 people 641,285 water 640,234 Some photos and videos (3,350,768 to be exact) carry machine tags . Noteworthy machine tags are those having the “siwild” namespace, referring to photos uploaded by scientists of the Smithsonian , and the “taxonomy” namespace, which refers to photos in which flora and fauna have been carefully classified. The most frequently occurring namespace, “uploaded,” refers to the applications used to share the photos on Flickr, which are principally the Flickr and Instagram iOS apps. Other interesting machine tags are those referring to the different filters that can be applied to a photo, or roughly 750,000 photos. Overall, most machine tags are related to food and drink, events, camera and application metadata, as well as locations. Machine Tag Count uploaded 1,917,650 siwild 1,169,957 taxonomy 1,067,857 foursquare 894,265 exif 617,287 flickriosapp 538,829 geo 443,762 sequence 429,948 lastfm 313,379 flickrandroidapp 222,238 In terms of locations, the photos and videos in the dataset have been taken all over the world. In total, 48,366,323 photos and 103,506 videos were geotagged. The most popular cities where photos and videos were shot are concentrated in the United States, principally New York City, San Francisco, Los Angeles, Chicago, and Seattle; in Europe, they were principally London, Berlin, Barcelona, Rome and Amsterdam. There are also photos that have been taken in remote locations like Kiribati , icy places like Svalbard , and exotic places like Comoros . In fact, photos and videos from this dataset have been taken in 249 different territories (countries, islands, etc) around the world, and even in international waters or airspace. Our dataset further reveals that there are many different cameras in use within the Flickr community. The Canon EOS 400D and 350D have a lead over the Nikon D90 (calm down…we’re not starting anything by saying that). Apple’s iPhones form the most popular type of cameraphone. Make Camera Count Canon EOS 400D 2,539,571 Canon EOS 350D 2,140,722 Nikon D90 1,998,637 Canon EOS 5D Mark II 1,896,219 Nikon D80 1,719,045 Canon EOS 7D 1,526,158 Canon EOS 450D 1,509,334 Nikon D40 1,358,791 Canon EOS 40D 1,334,891 Canon EOS 550D 1,175,229 Nikon D7000 1,068,591 Nikon D300 1,053,745 Nikon D50 1,032,019 Canon EOS 500D 1,031,044 Nikon D700 942,806 Apple iPhone 4 922,675 Nikon D200 919,688 Canon EOS 20D 843,133 Canon EOS 50D 831,570 Canon EOS 30D 820,838 Canon EOS 60D 772,700 Apple iPhone 4S 761,231 Apple iPhone 743,735 Nikon D70 742,591 Canon EOS 5D 699,381 Our collection of 100 million photos and videos marks a new milestone in the history of datasets. The collection is one of the largest released for academic use, and it’s incredibly varied—not just in terms of the content shown in the photos and videos, but also the locations where they were taken, the photographers who took them, the tags that were applied, the cameras that were used, etc. The best thing about the dataset is that it is completely free to download by anyone, given that all photos and videos have a Creative Commons license. Whether you are a researcher, a developer, a hobbyist or just plain curious about online photography, the dataset is the best way to study and explore a wide sample of Flickr photos and videos.  Happy researching and happy hacking!", "date": "2014-10-15,"},
{"website": "Flickr", "title": "Introducing: Flickr PARK or BIRD", "author": ["Rob Hess, Clayton Mellina, and Friends"], "link": "https://code.flickr.net/2014/10/20/introducing-flickr-park-or-bird/", "abstract": "OR Zion National Park Utah by Les Haines Secretary Bird by Bill Gracey tl;dr: Check it out at parkorbird.flickr.com ! We at Flickr are not ones to back down from a challenge. Especially when that challenge comes in webcomic form. And especially when that webcomic is xkcd . So, when we saw this xkcd comic we thought, “we’ve got to do that”: In fact, we already had the technology in place to do these things.  Like the woman in the comic says, determining whether a photo with GPS info embedded into it was taken in a national park is pretty straightforward. And, the Flickr Vision team has been working for the last year or so to be able to recognize more than 1000 things in images using deep convolutional neural nets. Incidentally, one of the things we’re pretty good at recognizing is birds! We put those things together, and thus was born parkorbird.flickr.com ! Recognizing Stuff in Images with Deep Networks The thing we’re really excited to show off with PARK or BIRD is our image recognition technology. To recognize 1000+ things, we employ a deep convolutional neural network similar to the one depicted below. This model transforms an input image into a representation in which different objects and scenes are easily distinguishable by a simple binary classification algorithm, like an SVM . It does this by passing the image through a series of layers, where each layer computes a function of the output of the layer below it. Each successive one of these layers, after training on millions of images, has learned to recognize higher- and higher-level features of images and the ways these features go together to form different objects and scenes. For example, the first layer might recognize the most basic image features, such as short straight lines, corners, and small circular arcs. The next layer might recognize higher level combinations of those features, such as circles or other basic shapes. Further layers might recognize higher-level concepts, like eyes and beaks, and even further ones might recognize heads and wings. For an example of what this looks like, check out Figure 2 in this paper by Matt Zeiler and Rob Fergus . As the image passes through these layers, they are “activated” in different ways depending on the features they’ve seen in the input image, and at the top of this network—after the image is transformed by the bottom layer, and that transformation of the image is transformed by the next layer, and that transformation of the transformation of the image is transformed by the next layer, and so on— a short floating-point vector summarizing all of the various activations at each layer is output. We pass this floating-point vector into more than 1000 binary classifiers, each of which is trained to give us a yes/no answer to identify a specific object/scene class. And, of course, one of those classes is birds! The Flickr Vision team is already applying this deep network to Flickr photos to help people more more easily find what they’re looking for via Flickr search, and we plan to integrate it into Flickr in other cool ways in the future. We’re also working on other innovative computer vision and image recognition technologies that will make it easier for Flickr members to find and organize their photos. Acknowledgements The Flickr Vision and Search team is awesome and PARK or BIRD is built upon technologies that we all pitched in on. Here we all are (at least most of us), in all our beautiful glory. Thanks Vision/Search! Thanks also to Stephen Woods, Bart Thomee, John Ko, Mike Shema, and Sean Perkins, all of whom provided a lot of help getting PARK or BIRD off the ground. If this all sounds like a challenge you’re interested in helping out with, you should join us! Flickr is hiring engineers , designers and product managers in our San Francisco office. Find out more at flickr.com/jobs .", "date": "2014-10-20,"},
{"website": "Flickr", "title": "33 Browser Stats You Just Might Believe", "author": ["Phil Dokas"], "link": "https://code.flickr.net/2015/03/04/browsers-in-2014/", "abstract": "We care an awful lot about the kinds of browsers and computers visiting Flickr. As people update to the latest versions of their browsers, the capabilities we can build against improve, which lets us build cool new things. At the same time, if lots of people continue using older browsers then we have to do extra work to gracefully support them. These days we not only have incredibly capable browsers, but thanks to the transparent and rapid update process of Chrome, Firefox, and soon Internet Explorer (hooray!), we can rely on new features rapidly showing up en masse. This is crazy great, but it doesn’t mean that we can stop paying attention to our usage statistics. In fact, as people spend more time on their phones, there’s as much of a need for a watchful eye as ever. We’ve never really shared our internal numbers, but we thought it would be interesting to take a look at the browsers Flickr visitors used in 2014. We use these numbers constantly to inform our project planning. Since limitations in older browsers take time to support we have to be judicious in picking which battles to fight. As you’ll see below, these numbers can be quite dynamic with a popular browser dropping to nearly 0% market-share in just a year. Let’s dive in and see some specifics. Fort Vancouver by Kate Dickerson Top level OSes and browsers At the highest level we learn a lot by looking at our OS family data. Probably the most notable thing here is how much of our traffic is coming from mobile devices. Moreover, the rate of growth is eye-popping. And this is just our website – this data doesn’t include our iOS or Android clients at all. A quarter of our traffic is from mobile devices. OSes in use on Flickr.com 2013 Q4 2014 Q4 Y/Y Windows 56.55% 50.61% -5.94 Macintosh 21.49% 21.42% -0.07 iOS 11.09% 17.61% 6.52 Android 5.39% 7.82% 2.43 Other 5.48% 2.54% -2.94 Let’s slice things slightly differently and look at browser families. We greatly differ from internet-wide traffic in that IE isn’t the outright majority browser. In fact, it clocks in at only the #4 position. More than half of Flickr visitors use a Webkit/Webkit-heritage browser (Safari and Chrome, respectively). Chrome rapidly climbed into its leadership position over the last few years and it’s stabilized there. Safari is hugely buoyed by iOS’s incredible growth numbers, while IE has been punished by Windows’s Flickr market-share decline. Browsers in use on Flickr.com 2013 Q4 2014 Q4 Y/Y Chrome 35.71% 35.42% -0.29 Safari 24.11% 27.50% 3.39 Firefox 17.94% 18.29% 0.35 Internet Explorer 13.98% 10.31% -3.67 Other 8.26% 8.48% 0.22 Fine-grained details We can go a step further and see many details in the individual versions of OSes and browsers out there. It’s one thing to say “Windows is down 6% over the year” but another to say “the growth rate for the latest version of Windows is 350% year over year.” When we look at the individual versions we can infer quite a bit of detail around update rates and changes in the landscape. OS version details A few highlights: Windows 7 is on the decline, XP and Vista fell by roughly 50% each, and Windows 8 and 8.1 are surging ahead. iOS 8.1 and Android 5.0 don’t appear in the list due to their late appearance in Q4. Our current monthly numbers have iOS 8.1 far outpacing every other iOS version. OS X 10.10 has accelerated Mac user upgrades; since its launch 10.9 has shed over a percent per month, and the legacy versions have sharply accelerated their decline. OS versions in use on Flickr.com 2013 Q4 2014 Q4 Y/Y Windows NT 3.39% 0% -3.39 Windows XP 10.12% 4.49% -5.63 Windows Vista 3.56% 2.41% -1.15 Windows 7 36.29% 33.14% -3.15 Windows 8 2.01% 2.31% 0.30 Windows 8.1 1.06% 8.22% 7.16 Macintosh OS X 10.5* – 0.65% 0.65 Macintosh OS X 10.6* – 2.90% 2.90 Macintosh OS X 10.7* – 1.91% 1.91 Macintosh OS X 10.8* – 1.83% 1.83 Macintosh OS X 10.9* – 8.26% 8.26 Macintosh OS X 10.10 0% 5.69% 5.69 iOS 4.3 0.19% 0% -0.19 iOS 5.0 0.12% 0% -0.12 iOS 5.1 0.59% 0% -0.59 iOS 6.0 0.42% 0% -0.42 iOS 6.1 2.02% 0.61% -1.41 iOS 7.0 7.36% 1.54% -5.82 iOS 7.1 0% 5.76% 5.76 iOS 8.0 0% 3.27% 3.27 Android 2.3 0.77% 0% -0.77 Android 4.0 0.82% 0% -0.82 Android 4.1 2.11% 1.22% -0.89 Android 4.2 0.84% 1.16% 0.32 Android 4.3 0.39% 0.56% 0.17 Android 4.4 0% 3.80% 3.80 Linux 4.37% 1.94% -2.43 * We didn’t start breaking out individual versions of OS X until Q1 2014. So unfortunately for this post we don’t have great info breaking down the versions of OS X, but we will in the future. OS X 10.10 did not exist in Q1 2014 so it’s counted as a natural 0% in our Q1 data. Browser version details These are the most dynamic numbers of the bunch. If there’s one thing they prove, it’s how incredibly effective the upgrade policies of Chrome and Firefox are. Where Safari and IE have years-old versions still hanging on (I’m looking at you Safari 5.1 and IE 8.0), virtually every Chrome and Firefox user is using a browser released within the last six weeks. That’s a hugel powerful thing. The IE team has suggested that Windows 10’s Project Spartan will adopt this policy , which is absolutely fantastic news. A few highlights: Despite not being on a continuous upgrade cycle, Safari and IE were able to piggyback on successful OS launches to consolidate their users on their latest releases. IE 8.0 is the only non-latest version of IE still holding on, thanks to its status as the latest version available for the still somewhat popular Windows XP. OS versions in use on Flickr.com 2013 Q4 2014 Q4 Y/Y Chrome 22.0.1229 1.67% 0% -1.67 Chrome 29.0.1547.76 1.39% 0% -1.39 Chrome 30.0.1599.101 8.94% 0% -8.94 Chrome 30.0.1599.69 3.74% 0% -3.74 Chrome 31.0.1650.57 6.08% 0% -6.08 Chrome 31.0.1650.63 6.91% 0% -6.91 Chrome 37.0.2062.124 0% 4.59% 4.59 Chrome 38.0.2125.104 0% 3.05% 3.05 Chrome 38.0.2125.111 0% 6.65% 6.65 Chrome 39.0.2171.71 0% 4.09% 4.09 Chrome 39.0.2171.95 0% 4.51% 4.51 Safari 5.0 1.96% 0% -1.96 Safari 5.1 5.60% 2.50% -3.10 Safari 6.0 6.21% 0.86% -5.35 Safari 7.0 7.29% 7.25% -0.04 Safari 7.1 0% 3.12% 3.12 Safari 8.0 0% 10.10% 10.10 Firefox 22.0 1.62% 0% -1.62 Firefox 24.0 5.50% 0% -5.50 Firefox 25.0 6.46% 0% -6.46 Firefox 26.0 1.90% 0% -1.90 Firefox 32.0 0% 4.92% 4.92 Firefox 33.0 0% 7.10% 7.10 Firefox 34.0 0% 3.52% 3.52 MSIE 8.0 3.69% 1.00% -2.69 MSIE 9.0 3.04% 1.22% -1.82 MSIE 10.0 5.94% 0% -5.94 MSIE 11.0 0% 6.69% 6.69 Generic WebKit 4.0* 3.18% 2.46% -0.72 Mozilla 5.0* 3.18% 4.80% 1.62 Opera 9.80 1.46% 0% -1.46 * These are catch-all versions of Mozilla-based and Webkit-based browsers that aren’t themselves Firefox, Safari, or Chrome. A word on methodology These numbers were anonymously collected using Yahoo’s in-house metrics libraries. The numbers here are aggregated over the course of three months each, making these numbers lagging indicators. This is why the latest releases, like Android 5.0 and iOS 8.1, are under-represented – they hadn’t yet enjoyed one full quarter when 2014 came to a close. Further reading There are a number of excellent sites out there watching similar browser statistics on a continuing basis. A few of them are: Ars Technica – on a monthly basis they analyze raw data from Net Market Share with insightful commentary. Net Market Share – while Ars does a bang-up job, it’s helpful to sift the data yourself to find the answers to your questions. Peter-Paul Koch – No one shines a sharper light on the state of browsers than PPK, with just one example being his attention to disambiguating the various versions of Chromium out there ( part two ). Like this post? Have a love of online photography? Want to work with us? Flickr is hiring engineers , designers and product managers in our San Francisco office. Find out more at flickr.com/jobs .", "date": "2015-03-4,"},
{"website": "Flickr", "title": "Much Photos!", "author": ["ericsoco"], "link": "https://code.flickr.net/2015/03/24/much-photos/", "abstract": "Introducing the New! Shiny! Photolist framework Blue skies. Mostly. Here at Flickr, we have photos. Lots of photos. Like, billions and billions of photos. So, it’s pretty important for us to be able to show you more than one at once. We have used what we call the “justified algorithm” to lay out photos for a while now , but as we move more and more pages onto our new-ish isomorphic node.js stack, we determined it was time to revisit the algorithm and create an updated implementation. A few of us here in Frontend-landia got together to figure out all the things this new shiny should be able to do. With a lot of projects in full swing and on the near horizon, we came up with a pretty significant list, including but not limited to: – Easy for developers to use – Fit into any kind of container – Support pagination (in both directions!) and infinite scroll – Jank-free, butter-silky-baby-smooth scrolling – Support layouts other than justified, like square thumbnails and grid layout with native aspect ratio After some brainstorming, drawing of diagrams, and gummi bear consumption, we got to work building out the framework and the underlying algorithm. Rejustification Drawing of diagrams The basics of the justified algorithm aren’t too complex. The goal is for the layout module to accept a list of photo aspect ratios, and return a list of rectangles. A layout consists of a number of rows of items (photos), each with a target height and allowable height deviation above and below. This, along with the container width, gives us a minimum and maximum row aspect ratio. Fig. 1: The justified algorithm: dimensions We push each photo into a row; once the row is filled up, we move on to the next. It goes a little something like this: Iterate over each photo in the list to display Create a new row if there’s not currently an open row Attempt to add the photo to the current row at its native aspect ratio and at the target row height If the new row aspect ratio is less than the minimum row aspect ratio, continue adding photos until the aspect ratio is greater than the maximum aspect ratio Either keep or drop the last added photo, depending on which generates a row aspect ratio closer to the target row aspect ratio; adjust the row height as needed, and seal the row Repeat until all the photos have been laid out. Fig. 2: The justified algorithm: row filling It’s Never That Easy… The justified algorithm described above is the primary responsibility of the layout module. In practice, however, there are a number of other things the layout must handle to get good results for all use cases, and to communicate the results to other parts of the framework. Diffs One key feature of the layout module is how it organizes its results. To minimize the amount of processing required to update photos as the layout changes, the layout module returns pre-sorted diffs, each with a specific purpose: new items, used to create new photos and put them in place layout-changed items, used to resize/reposition existing photos visibility-changed items, used to wake/sleep existing photos widows and orphans (leading and trailing items) (read on!) The container view can process only the parts of the layout response that are necessary, given the current state of the whole framework, to keep processing time down and keep performance up. Widows and orphans Annie the Musical, by Eva Rinaldi Some photolist pages on Flickr use infinite scrolling, and some display results one page at a time. Regardless of how a page shows its photos, it starts to feel messy when there is an incomplete row of photos hanging off the end of the page. If there is more content in the set, the last row should be full. However, since we fetch photos from the API in fixed batch sizes, things don’t always work out so nicely, leaving “leftovers” in the bottom row. Borrowing from typesetting terminology , we call these leftover photos orphans. (We can also paginate backwards; leftovers at the top are technically widows but we’ll just keep using the term orphans for simplicity.) The layout notes these incomplete rows and hides them from the rest of the framework until the next page of content loads in. (This led to frequent and questionable metaphors about “orphan suppression” and “orphan rehydration.”) When orphans are to be hidden, the layout simply keeps them out of the diff. When the orphans are brought back in as the next page loads, the layout prepends them to the next diff. The container view is none the wiser. This logic gets even more fun when you consider that it must perform in all of these use cases: fixed page size (book-style) pagination downward-scrolling infinite pagination upward-scrolling infinite pagination (enter into an “infinite” content set somewhere other than the beginning); this requires right-to-left layout! There’s also the case of the end of an “infinite” content set (scrolling down to the end or up to the beginning); in these cases, we still want the row to appear complete, and still must maintain the native aspect ratio of the photo. Therefore, we allow the row height to grow as much as it needs in this case only. Bonus Round! You might have noticed that Flickr is kind of a big site with, like, lots of photos. And we display photos in lots of different ways, with lots of different use cases. The photolist framework bends over backwards to support all of those, including: forward and backward pagination infinite scroll and fixed page-size pagination specific aspect ratios (e.g. squares) fixed number of rows fast relayout (only a few milliseconds for thousands of photos) Going into detail on each of those features is way beyond the scope of this blog post, but suffice it to say the framework is built to handle just about anything Flickr can throw at it. The one exception is the upcoming Camera Roll (coming soon to those of you who don’t yet have it!), which is Too Extreme for this framework, so we devised something special just for that page. The whole enchilada Mmm… enchiladas, by jeffreyww The layout is at the heart of the photolist framework, but wait — there’s more! The main components of the framework are the layout (dissected above), the container view / controller, and the subviews (usually containing photos). Fig. 3: Relationship of view/controller, layout, and subviews, and changing subview states during downward scroll The container view does a lot of fancy things, like: loading in photos as you scroll down or paginate triggering a relayout when the container size changes (i.e. when you resize your window) matching up server-rendered HTML with clientside JavaScript objects (see isomorphic JavaScript , and an upcoming blog post about the Hermes stack at Flickr). Its primary job, though, is to act as the conduit between the layout module and the individual subviews. Every time a layout is processed or changes, it returns a “layout response” to the container view. The layout response contains a list of rectangles and wake/sleep flags (actually, a list of lists; see Diffs above); the container view relays that new information on to each individual subview to determine position and visibility. The container view doesn’t even need to know about the layout details — each subview adjusts itself to its layout data all on its own. The subviews each have a decent amount of intelligence of their own, performing such tasks as: choosing the most appropriate photo file size to fit the layout rectangle adding/removing itself to/from the DOM as instructed by the layout to maintain good scroll performance providing an annotation and interaction layer for titles, faves, comments, etc. Coming soon to a webpage near you The new photolist framework is certainly not a one-size-fits-all solution; it’s tailored for Flickr’s specific use cases. However, we tried to design and build it to be as broadly useful for Flickr as possible; as we continue to move parts of the site onto the new frontend stack and innovate new features, it’s critical to have solid components upon which we can build Flickr’s future. The layout algorithm is probably useful for many applications though, and we hope you gained some insight into how you might implement your own. The photolist framework is already live in a number of places on the site, including the new Unified Search pages ( currently in Beta ), the Create / Wall art pages, the Group pool preview , and is coming soon to a number of other pages. As always, if you’re interested in helping with that “and more” part, we’d love to have you! Stop by our jobs page and drop us a line.", "date": "2015-03-24,"},
{"website": "Flickr", "title": "Avoiding Dragons: A Practical Guide to Drag ’n’ Drop", "author": ["Phil Dokas"], "link": "https://code.flickr.net/2012/12/10/drag-n-drop/", "abstract": "You, the enterprising programmer, know about parsing EXIF from photos in the browser and even how and why to power this parsing with web workers . “But,” you ask yourself, “how do I get those photos into the browser in the first place?” The oldest and most low-tech solution is the venerable <input type=\"file\" name=\"foo\"> . This plops the old standby file button on your page and POST s the file’s contents to your server upon form submission. To address many of this simple control’s limitations we debuted a Flash-based file uploader in 2008. This workhorse has been providing per-file upload statuses, batch file selection, and robust error handling for the last four years through Flash’s file system APIs. These days we can thankfully do this work without plugins . Not only can we use XHR to POST files and provide all the other fancy info we’ve long needed Flash for, but now we can pair this with something much better than an <input> : drag and drop. This allows people drag files directly into a browser window from the iPhotos, Lightrooms, and Windows Explorers of the world. Let’s take a look at how this works. Foundations first Workmen laying the cornerstone, construction of the McKim Building by Boston Public Library Let’s begin with our simple fallback, a – yes – <input type=\"file\"> . HTML : <input type=\"file\" multiple accept=\"image/*,video/*\"> JS : Y.all('input[type=file]').on('change', handleBrowse); Here we start with an <input> that accepts multiple files and knows it only accepts images and videos. Then, we bind an event handler to its change event. That handler can be very simple: function handleBrowse(e) {\r\n\t// get the raw event from YUI\r\n\tvar rawEvt = e._event;\r\n\t\r\n\t// pass the files handler into the loadFiles function\r\n\tif (rawEvt.target &amp;&amp; rawEvt.target.files) {\r\n\t\tloadFiles(rawEvt.target.files);\r\n\t}\r\n} A simple matter of handing the event object’s file array off to our universal function that adds files to our upload queue. Let’s take a look at this file loader: function loadFiles(files) {\r\n\tupdateQueueLength(count);\r\n\t\r\n\tfor (var i = 0; i &lt; files.length; i++) {\r\n\t\tvar file = files[i];\r\n\t\t\r\n\t\tif (File &amp;&amp; file instanceof File) {\r\n\t\t\tenqueueFileAddition(file);\r\n\t\t}\r\n\t}\r\n} Looks clear – it’s just going over the file list and adding them to a queue. “But wait,” you wonder, “why all this queue nonsense? Why not just kick off an XHR for the file right now?” Indeed, we’ve stuck in a layer of abstraction here that seems unnecessary. And for now it is. But suppose our pretty synchronous world were soon to become a whole lot less synchronous – that could get real fun in a hurry. For now, we’ll put that idea aside and take a look at these two queue functions themselves: function updateQueueLength(quantity) {\r\n\tstate.files.total += quantity;\r\n}\r\n\r\nfunction enqueueFileAddition(file) {\r\n\tstate.files.handles.push(file);\r\n\t\r\n\t// If all the files we expect have shown up, then flush the queue.\r\n\tif (state.files.handles.length === state.files.total) {\r\n\t\tfor (var i = 0, len = state.files.total; i &lt; len; i++) {\r\n\t\t\taddFile(state.files.handles[i]);\r\n\t\t}\r\n\t\t\r\n\t\t// reset the state of the world\r\n\t\tstate.files.handles = [];\r\n\t\tstate.files.total = 0;\r\n\t}\r\n} Pretty straightforward. One function for leaving a note of how many files we expect, one function to add files and see if we have all the files we expect. If so, pass along everything we have to addFile() which sends the file into our whirlwind of XHR s heading off to the great pandas in the sky . Droppin’ dragons Droppin’ dragons by Phil Dokas While all of that is well and good, it was all for a ho-hum <input> element! Let’s hook a modern browser’s drag and drop events into this system: document.addEventListener('drop', function(e) {\r\n\tif (e.dataTransfer &amp;&amp; e.dataTransfer.files) {\r\n\t\tloadFiles(e.dataTransfer.files);\r\n\t}\r\n}); The drag and drop API is a fairly complicated one, but it thankfully makes the task of reading files out of a drop event easy. Every drop will have a dataTransfer attribute and when there’s at least one file in the drag that member will itself have a files attribute. In fact, when you’re only concerned about handling files dragged directly into the browser you could call it a day right here. The loadFiles() function we wrote earlier knows how to handle instances of the File class and that’s exactly what dataTransfer.files stores. Easy! Put it up to eleven While easy is a good thing, awesome is awesome. How could we make dragging files into a browser even better? Well, how about cutting down on the trouble of finding the folder with your photos somewhere on your desktop, opening it, and then dragging those files into the browser? What if we could just drag the folder in and call it a day? goes to 11 by Rick Kimpel Try to drag a folder into the browser with the current state of our code; what happens? Our code tells the browser to treat all dropped file system objects as files. So what ultimately happens for folders is a very elaborate “nothing”. To fix this, we need to tell the browser how to handle directories. In our case, we want it to recursively walk every directory it sees and pick out the photos from each. From here on out we’re going to be treading over tumultuous land, rife with rapidly changing specs and swiftly updating browsers. This becomes immediately apparent in how we begin to add support for directories. We need to update our drop event handler like this: document.addEventListener('drop', function(e) {\r\n\tif (e.dataTransfer &amp;&amp; e.dataTransfer.items) {\r\n\t\tloadFiles(e.dataTransfer.items);\r\n\t}\r\n\telse if (e.dataTransfer &amp;&amp; e.dataTransfer.files) {\r\n\t\tloadFiles(e.dataTransfer.files);\r\n\t}\r\n}); Items? Files? The difference is purely a matter of one being the newer interface where development happens and the other being the legacy interface. This is spelled out a bit in the spec , but the short of it is that the files member will be kept around for backwards compatibility while newer abilities will be built in the items namespace. Our code above prefers to use the items attribute if available, while falling back to files for compatibility. The real fun is what comes next. You see, the items namespace deals with Items , not Files . Items can be thought of as pointers to things in the file system. Thankfully, that includes the directories we’re after. But unfortunately, this is the file system and the file system is slow. And JavaScript is single-threaded. These two facts together are a recipe for latency. The File System API tackles this problem with the same solution as Node.js: asynchronicity. Most of the functions in the API accept a callback that will be invoked when the disk gets around to providing the requested files. So we’ll have to update our code to do two new things: 1) translate items into files and 2) handle synchronous and asynchronous APIs. So what do these changes look like? Let’s turn back to loadFiles() and teach it how to handle these new types of files. Taking a look at the spec for the Item class , there appears to be a getAsFile() function and that sounds perfect. function loadFiles(files) {\r\n\tupdateQueueLength(count);\r\n\t\r\n\tfor (var i = 0; i &lt; files.length; i++) {\r\n\t\tvar file = files[i];\r\n\t\t\r\n\t\tif (typeof file.getAsFile === 'function') {\r\n\t\t\tenqueueFileAddition(file.getAsFile());\r\n\t\t}\r\n\t\telse if (File &amp;&amp; file instanceof File) {\r\n\t\t\tenqueueFileAddition(file);\r\n\t\t}\r\n\t}\r\n} Easy – but, there’s a problem. The getAsFile() function is very literal. It assumes the Item points to a file. But directories aren’t files and that means this method won’t meet our needs. Fortunately, there is a solution and that’s through yet another data type, the Entry . An Entry is much like a File, but it can also represent directories. As mentioned in this WHATWG wiki document , there is a proposed method, getAsEntry() , in the Item interface that allows you to grab an Entry for its file system object. It’s browser prefixed for now, so let’s add that in as well. function loadFiles(files) {\r\n\tupdateQueueLength(count);\r\n\t\r\n\tfor (var i = 0; i &lt; files.length; i++) {\r\n\t\tvar file = files[i];\r\n\t\tvar entry;\r\n\t\t\r\n\t\tif (file.getAsEntry) {\r\n\t\t\tentry = file.getAsEntry();\r\n\t\t}\r\n\t\telse if (file.webkitGetAsEntry) {\r\n\t\t\tentry = file.webkitGetAsEntry();\r\n\t\t}\r\n\t\telse if (typeof file.getAsFile === 'function') {\r\n\t\t\tenqueueFileAddition(file.getAsFile());\r\n\t\t}\r\n\t\telse if (File &amp;&amp; file instanceof File) {\r\n\t\t\tenqueueFileAddition(file);\r\n\t\t}\r\n\t}\r\n} So what we have now is a way of handling native files and a way of turning Items into Entries. Now we need to figure out if the Entry is a file or a directory and then handle that appropriately. What we’ll do is queue up any File objects we run across and skip the loop ahead to the next object. But if we have an Item and successfully turn it into an Entry then we’ll try to resolve this down to a file or a directory. function loadFiles(files) {\r\n\tupdateQueueLength(count);\r\n\t\r\n\tfor (var i = 0; i &lt; files.length; i++) {\r\n\t\tvar file = files[i];\r\n\t\tvar entry, reader;\r\n\t\t\r\n\t\tif (file.getAsEntry) {\r\n\t\t\tentry = file.getAsEntry();\r\n\t\t}\r\n\t\telse if (file.webkitGetAsEntry) {\r\n\t\t\tentry = file.webkitGetAsEntry();\r\n\t\t}\r\n\t\telse if (typeof file.getAsFile === 'function') {\r\n\t\t\tenqueueFileAddition(file.getAsFile());\r\n\t\t\tcontinue;\r\n\t\t}\r\n\t\telse if (File &amp;&amp; file instanceof File) {\r\n\t\t\tenqueueFileAddition(file);\r\n\t\t\tcontinue;\r\n\t\t}\r\n\t\t\r\n\t\tif (!entry) {\r\n\t\t\tupdateQueueLength(-1);\r\n\t\t}\r\n\t\telse if (entry.isFile) {\r\n\t\t\tentry.file(function(file) {\r\n\t\t\t\tenqueueFileAddition(file);\r\n\t\t\t}, function(err) {\r\n\t\t\t\tconsole.warn(err);\r\n\t\t\t});\r\n\t\t}\r\n\t\telse if (entry.isDirectory) {\r\n\t\t\treader = entry.createReader();\r\n\t\t\t\r\n\t\t\treader.readEntries(function(entries) {\r\n\t\t\t\tloadFiles(entries);\r\n\t\t\t\tupdateQueueLength(-1);\r\n\t\t\t}, function(err) {\r\n\t\t\t\tconsole.warn(err);\r\n\t\t\t});\r\n\t\t}\r\n\t}\r\n} The code is getting long, but we’re almost done. Let’s unpack this. The first branch of our new Entry logic ensures that what was returned by webkitGetAsEntry() / getAsEntry() is something useful. When they error they return null and this will happen if an application provides data in the drop event that isn’t a file. To see this in action try dragging a few files in from Preview in Mac OS X – it’s odd behavior, but this adequately cleans it up. Next we handle files. The Entry spec provides the brilliantly simple isFile and isDirectory attributes. These guarantee whether you have a FileEntry or a DirectoryEntry on your hands. These classes have useful – though as promised, asynchronous – methods and here we use FileEntry’s file() method and enqueue its returned file. Finally, the unicorn we’re chasing – handling directories. This is a tad more complicated, but the idea is straightforward. We create a DirectoryReader which lets us read its contents through its readEntries() method which provides an array of Entries. And what do we do with these Entries? We recursively call our loadFiles() function with them! In this step we achieve recursively walking a branch of the file system and rooting out every available image. Finally, we decrement the count of expected files by 1 to indicate that this was a directory and it has now been suitably handled. But there is one more thing. In that final directory reading step we recursively called loadFiles() with an array of Entries . As of right now, this function only expects to handle Files and Items. Let’s patch up this oversight, add a final bit of error handling, and call it a day. function loadFiles(files) {\r\n\tupdateQueueLength(count);\r\n\t\r\n\tfor (var i = 0; i &lt; files.length; i++) {\r\n\t\tvar file = files[i];\r\n\t\tvar entry, reader;\r\n\t\t\r\n\t\tif (file.isFile || file.isDirectory) {\r\n\t\t\tentry = file;\r\n\t\t}\r\n\t\telse if (file.getAsEntry) {\r\n\t\t\tentry = file.getAsEntry();\r\n\t\t}\r\n\t\telse if (file.webkitGetAsEntry) {\r\n\t\t\tentry = file.webkitGetAsEntry();\r\n\t\t}\r\n\t\telse if (typeof file.getAsFile === 'function') {\r\n\t\t\tenqueueFileAddition(file.getAsFile());\r\n\t\t\tcontinue;\r\n\t\t}\r\n\t\telse if (File &amp;&amp; file instanceof File) {\r\n\t\t\tenqueueFileAddition(file);\r\n\t\t\tcontinue;\r\n\t\t}\r\n\t\telse {\r\n\t\t\tupdateQueueLength(-1);\r\n\t\t\tcontinue;\r\n\t\t}\r\n\t\t\r\n\t\tif (!entry) {\r\n\t\t\tupdateQueueLength(-1);\r\n\t\t}\r\n\t\telse if (entry.isFile) {\r\n\t\t\tentry.file(function(file) {\r\n\t\t\t\tenqueueFileAddition(file);\r\n\t\t\t}, function(err) {\r\n\t\t\t\tconsole.warn(err);\r\n\t\t\t});\r\n\t\t}\r\n\t\telse if (entry.isDirectory) {\r\n\t\t\treader = entry.createReader();\r\n\t\t\t\r\n\t\t\treader.readEntries(function(entries) {\r\n\t\t\t\tloadFiles(entries);\r\n\t\t\t\tupdateQueueLength(-1);\r\n\t\t\t}, function(err) {\r\n\t\t\t\tconsole.warn(err);\r\n\t\t\t});\r\n\t\t}\r\n\t}\r\n} All we need to do to handle an Entry is to rely on the fact that Entries have those oh-so-helpful isFile and isDirectory attributes. If we see those we know we have an Entry of one type or another and we know how to work with them, so just skip on down to the FileEntry and DirectoryEntry handling code. And that, finally, is it. There are many specs with very new data types at play here, but through this turmoil we can achieve some very nice results never before possible in browsers. Further reading Exploring the Filesystem APIs , by Eric Bidelman, Google Drag and Drop a Folder onto Chrome Now Available , by Eiji Kitamura, Google WHATWG’s HTML Drag and Drop spec W3C’s File API spec W3C’s File API: Directories and System spec Like this post? Have a love of online photography? Want to work with us? Flickr is hiring engineers , designers and product managers in our San Francisco office. Find out more at flickr.com/jobs .", "date": "2012-12-10,"},
{"website": "Flickr", "title": "Real-time Resizing of Flickr Images Using GPUs", "author": ["Archie Russell"], "link": "https://code.flickr.net/2015/06/25/real-time-resizing-of-flickr-images-using-gpus/", "abstract": "At Flickr we work with a huge number of photos. Our users upload over 27 million photos a day, and our total collection has over 12 billion photos. This is fantastic! As usage grows, we are always looking for ways to use our storage more efficiently. Recently our storage team wrote about some new commodity storage technology now in use at Flickr which increases efficiency. But we also looked into how much data we store for each photo. In the past we stored many sizes of every photo to make serving fast. We wanted to challenge that model and find the minimal set of data to store. Thumbnail Footprint Reduction One of our biggest opportunities for byte per photo improvement is through reduction in the footprint of Flickr’s “thumbnails”. Thumbnail is a bit of a misnomer at Flickr; our thumbnails are as large as 2048 pixels on their longest side, so at Flickr we usually refer to these as resizes .  We create these resizes in order to provide a consistent,  fast experience for our users over a variety of use cases. Different sizes used in different contexts. From left to right: Cameraroll uses small thumbnails, to enable fast navigation through many sizes. Our Photo Page uses our largest, most detailed sizes. Search uses sizes in between these two extremes. Red panda photos by Mathias Appel . The selection of sizes has grown semi-organically over the years, and all told, we serve eleven different resizes per photo which, in sum, use nearly as much storage as the original photo. Almost 90% of this storage is held in the handful of resizes 640px and larger, so we targeted our efforts at eliminating some of these sizes. Left: Distribution of byte size by resize dimension. Storage is concentrated in images with largest dimensions. Right: size distribution after largest sizes eliminated. A Few Approaches A simple approach to this problem would be just to cease offering some of the larger sizes.    For instance, we could drop the 1600px image from our API and require the design to adjust.   However, this requires compromises that we didn’t want to take on. Instead we took on a pretty ambitious goal: maintain our largest resize, usually 2048px wide, as a source image and create any other moderate or large-sized resizes on-the-fly from this source, without sacrificing image quality or significantly affecting performance. Using the original uploaded photo as a resize source image was impractical, as these can be very large and exist in a variety of formats. Sounds easy, right? We already resize images when users upload, so why not just use that same technology on serving. Well, almost. The problem with the naive approach is that high-quality resizing of JPEGs is a lot slower than is widely known. A tool we use frequently, GraphicsMagick , produces beautiful images but takes over 225ms to resize a 2048px JPEG down to 1600px, depending on quality settings. This is slow enough that this method would impact user experience, and would require many CPUs to handle our load. Ymagine ,  a high-performance CPU-based tool we’ve open sourced,  is twice as fast as GraphicsMagick(!). We use Ymagine extensively on smaller images, but for the large sizes we’re targeting we needed even more performance. A GPU-based solution ultimately filled our needs. Our GPU-based Solution We created a tier of dedicated resize servers , each with an GPU co-processor. Each of these boards has two GPUs, each with 1500+ “cores”, running at just under 1GHz. These cores aren’t anywhere near as performant as a CPU core, but there are many of them. We tested a range of server-grade boards to find the best performing type for our workload. Many manufacturers offer consumer-grade boards with incredible specifications and lower price points, but these lack server-grade cooling and other features such as ECC RAM. One member of our team had experience using these lower grade boards in a previous application and recommended against it. Resize system architecture On these resize servers we run a fairly vanilla Apache with a plugin written in C++.  This server responds to resize requests, reads our source image from disk into shared memory,  and hands off requests off to persistent resize daemons that do all communication with our  GPUs.  A daemon-type approach is necessary due to a somewhat lengthy initialization process with our GPUs. Our resize daemons transfer JPEGs from shared memory to GPU device memory. Once here,  the real image processing takes place. The JPEGs are decoded, cropped, sharpened, resized, re-sharpened as needed, re-encoded as JPEGs, and finally transferred back to shared memory.    From shared memory, our Apache module returns the resized JPEG to the caller. A simple resize pipeline. Post-sharpening overcomes fuzziness introduced when downscaling. There are several accepted resize algorithms, but to retain the Flickr “look”, we implemented the same Lanczos resize and kernel sharpening algorithms that we’ve used for years in CUDA.     This had the added benefit of being able to directly compare images generated through GraphicsMagick and our GPU-based code. Performance With significant optimization, this code is able to resize our 2048px JPEGs to 1600px in under 16ms. This is more than 15x faster than GraphicsMagick and nearly 10x faster than Ymagine.  Resizes from 2048px to 640px take under 10ms. Equally noteworthy,  at peak load,  each resize server can perform over 300 resizes per second. Performance of different resize approaches. Although these timings are quite fast,  the source image for our resizes  is larger, byte-wise, than the images it is resizing to,  requiring additional I/O. For example,  a typical 2048px source JPEG is roughly 600kB and our typical 1024px JPEGs are just under 200kB. This difference in size leads to roughly 35ms additional I/O time per resize. Taking it slow As our GPU code is new and images are our most important product, this change carries some risk. We’ve addressed this with extensive testing, progressive rollout and provisions for rollback. We also used some insights into our user behavior to roll this solution out in a very controlled manner. Conclusion This system is currently in production and as we roll it out more fully, has the potential to cut the resize footprint of the majority of our photos by 50%, with negligible impact on performance and image appearance. We also have the ability to apply this same footprint reduction technique to images uploaded in the past, which has the potential to reduce our storage growth to zero for a significant period of time. Credits This project would not have been possible with hard work of Peter Norby, Tague Griffith, John Ko and many others.", "date": "2015-06-25,"},
{"website": "Flickr", "title": "Optimizing Caching: Twemproxy and Memcached at Flickr", "author": ["Tague Griffith and John Troxel"], "link": "https://code.flickr.net/2015/07/10/optimizing-caching-twemproxy-and-memcached-at-flickr/", "abstract": "Introduction Flickr places a high priority on our users’ experience, and a critical part of that experience is the speed of the interface.  Regardless of the client you use to access Flickr, caching the proper data and the speed at which our servers can access cached data is critical to delivering on that quality user experience.  The more effective our caching strategy is, the better the Flickr experience will be for our users.  This is true for all the layers of caching we deploy at Flickr from the photo caches to the process data caches buried deep in the system. In a previous post , we looked at how regional photo caching improved photo serving time in other countries, in this post we’re going to dive down into the innards of Flickr’s software stack and take a look at how we improved Memcached performance for our backend systems. Back in the olden days (pre-2014), we accessed our Memcached systems through a mix of direct reads from our web servers and writes through a Flickr-developed proxy.  Our proprietary proxy system, Cerberus, handled a whole host of responsibilities. In addition to Memcached set operations, Cerberus managed database updates, the bulk of our Redis accesses, cache consistency (which is why we directed our writes through Cerberus), and a few other miscellaneous transactions.   As Flickr’s traffic and functionality had grown, Memcached set operation performance wasn’t keeping up, so we needed to consider how to address the gap. Since the development of Cerberus, the software landscape had drastically changed.  When we developed Cerberus, there was no comparable software available, but now several open source projects exist that provide similar proxy services.  On top of the availability of open source tools, Flickr’s traffic and usage patterns have changed over the course of a decade, changing the requirements we had for a proxy system.  Needless to say we had a lot of questions to ask ourselves before we dived into revising the caching architecture. After years of operation, we had a good picture into the strengths and weaknesses of our current architecture, so when we started thinking about revising it, a few lessons from the past years really stood out.  One thing that we learned over the years was that Cerberus’ lack of a single purpose made it difficult to troubleshoot operational issues with Memcached.  Due to the lack of isolation, a downstream issue with a user database, could impact Memcached access time.  Whatever came next had to isolate cache requests from other data accesses. Experience had shown us that Memcached get operation latency was a key performance metric, and we learned through trial and error that placing our current Cerberus proxy between the web servers and the Memcached hosts added more network latency than we were willing to tolerate.  Unfortunately, our options for connection pooling and more efficient use of connections were limited in PHP, so we had little recourse than to suffer with a high connection load and fluctuating connections against our Memcached servers.  The next generation system would have to carefully monitor get operation timings and ensure we didn’t introduce more latency into the process. So as 2014 rolled around, we started to look into an alternative to Cerberus for accessing the Memcached systems.  Should we build a Cerberus 2.0?  Should we look at an open source alternative?  As we weighed our options, one alternative that stood out because it was quite successful in other parts of Yahoo and throughout the industry was Twitter’s Twemproxy. Introducing Twemproxy Twemproxy is a lightweight daemon that proxies requests to a pool of Memcached instances.  It provides the following features that we believed would improve our caching infrastructure: Consistent hashing:  Figuring out what hosts in the ring on which to store and retrieve cache data.  While we had implemented consistent hashing before Twemproxy, it was previously left to the client libraries to sort out. Connection pooling:  Reuse of network connections to the Memcached instances, cutting down significantly on the connection load to the Memcached daemons. Command Pipelining:  Accumulating requests destined for the same host and sending as a combined payload.  This feature further reduces connection load and network overhead to the cache processes. Resulting Architecture We implemented a solution where each web server host had two local Twemproxies, forwarding to the Memcached rings. In the resulting architecture, all Memcached operations go through twemproxy.  The change accomplished many goals, including: Providing a dedicated system for Memcached requests that was isolated from other systems Reducing the connection load on our Memcached servers through Twemproxy’s connection pooling.  We experienced a 75% overall reduction of TCP connections to Memcached nodes Improved overall caching latency.  This was a benefit that we didn’t necessarily expect.  With Twemproxy, we found that get operations had a 5% reduction in mean processing time and set operations had a 40% in mean processing time The Road to Twemproxy As nice as it would be to say we dropped in Twemproxy, declared victory and went for ice cream, we still had to solve a few interesting challenges along the way: maintaining availability, dealing with disparate consistent hashing schemes, and re-implementing cache coherency. If One is Good, Two is Even Better From the start, we recognized that the simple daemon model of Twemproxy would need to be managed carefully.  Each deploy through our continuous deployment system could result in changes to the Memcached hosts configuration.  And unfortunately, configuration changes for Twemproxy require the process to be restarted to take effect.  We measured the time to restart Twemproxy in the 1-2 second range, but even for these necessary restarts, it was too much of an interruption for the clients. Our solution was to run two instances of the daemon on every host that needed the service and manage a careful synchronization between the restarts.  This restart “dance” was wired into the process that deploys the configuration changes to all the Memcached clients.  A couple of patches have been proposed to allow configuring Twemproxy without restarting it, but none of them have yet made the master branch. When is Ketama not Ketama? Ketama is a popular consistent hashing algorithm used by many systems to determine where to place a particular key in a multi-node caching system.  Out of the box, Twemproxy uses an implementation of the Ketama algorithm that is compatible with libketama, the C library which is the most commonly used implementation of the Ketama algorithm. Our initial implementation of consistent hashing was done using Ketama, but with the Spymemcached Java library.  It turns out that Spymemcached has a slight variation in the implementation that makes it incompatible with Twemproxy. Our transition from our current system to Twemproxy had to happen live, and a sudden change in the cache algorithm would have a painful (and unacceptable) impact on our database systems.  How could we get across this bridge?  Ultimately, we had to patch Twemproxy’s implementation of Ketama to match Spymemcached to maintain a consistent implementation of the Ketama algorithm. Redis latency in propagating cache clears Until we figure out how to change the speed of light, the only way we are going to make Flickr fast across the world, is through multiple data centers conveniently located near our users.  While this is way easier than changing the speed of light, it’s not without its complications. Caches between the data centers have to be kept consistent.  Some caches, like photo caches, deal with immutable data and are easy to keep in synch, others like Memcached systems have read-write data which is harder.  Our approach to handling cache consistency in our Memcached systems was to invalidate stale keys in other colo facilities whenever a process updated a value.  As we mentioned previously, Memcache write operations were directly through our Cerberus proxy specifically so Cerberus could dispatch a cache invalidation event to other colo facilities.  The migration to Twemproxy would not be complete, until we implemented a new solution for cache invalidation. In our Twemproxy-based architecture, we decided to take the responsibility for cache invalidation our of the hands of the data proxy and push it into the client libraries we used to access Memcached.  Whenever a client updates a Memcache key, it enqueues a corresponding cache invalidation event into a distributed Redis queue.  We then deployed simple, single-purpose Java daemons to process the cache invalidation events from the Redis queue and delete the corresponding keys in their local Memcached systems.  A diagram of the system, appears below: The wrinkle with this approach was that the enqueuing of clear keys would occasionally take 20 times longer than the normal mean time, pushing cache sets up to 40ms. After much digging, we found that the spikes were happening when the clearing daemons dequeued a batch of keys.  The dequeuing daemons were performing operations across a WAN. Due to the single-threaded nature of Redis, it would periodically block the queue for adding keys for 10s of milliseconds.  Once we figured that out, the fix was a matter of keeping separate in- and out-queues, and moving the keys from in to out with a local daemon, which significantly reduced the blocked time for writing keys. Conclusion Caching is crucial to a high-traffic site like Flickr, and we have taken a big stride in making our Memcached utilization more effective.  Using Twemproxy, we were able to clean up an internal system, reduce the connection load on our caching daemons, and even make modest improvements to caching latency for all clients.  Although we faced some technical challenges in implementing twemproxy for Memcached, particularly in propagating cache clear events, it was ultimately well worth the engineering investment.  After several months, our implementation of Twemproxy has proven to make a positive contribution to caching speed and ultimately the experience of a responsive site for our users. If you dream in low latency and love to rip that extra 10 microseconds of overhead out of an operation, we’d love to have you! Stop by our Jobs page and tell us how awesome you are.", "date": "2015-07-10,"},
{"website": "Flickr", "title": "The Data Freshener", "author": ["ericsoco"], "link": "https://code.flickr.net/2015/09/01/the-data-freshener/", "abstract": "https://www.flickr.com/photos/hellomokona/956099245/ So fresh Change You may have noticed some changes in Flickr a couple months back. Like, half the site changed. 95% even, by some metrics. Some say CHANGE IT BACK! while others welcome change. Whatever your thoughts, the changes are here, and they mean things. For example, they mean new visual design and better usability. They mean a faster site. Unfortunately, up until recently, they also meant more stale data. Yuck. Change Why? What? Well…here’s the deal. We have a new-ish frontend stack we’ve been using for the past couple years now. It’s an isomorphic single-page application , runs on node.js , and is generally awesome. We call it Reboot. Reboot In the World of Reboot, we treat data with kid gloves. We <3 data. We never want to give it up, never want to let it down. Once we pull data from our APIs, we store the fetched data in your browser so that we don’t have to fetch it again the next time it’s needed. This means faster page loads and faster navigation, and less API traffic (and thus a more stable and scalable API). The data cached in your browser exists as long as the current Reboot session — until you refresh or leave Reboot for a non-Rebooted page. However, this also meant that data could become stale. You change the date taken of your photo, someone else adds a comment, you navigate to a page with cached data…and you don’t see the changes. Wat? Yeah. So, this was not a huge problem until we moved lots of pages onto Reboot in the beginning of May. From that point forward, most Flickr user sessions have spent their entirety on Reboot, feeding off the same stale loaves of cached data. https://www.flickr.com/photos/recyclethis/157108084/ Staleness The thinking (design / prototypes) We considered a number of possibilities for freshening up data during a user session. A brief history of the strategies we sampled, and their results: 1. Refresh on update The first stab focused on updating data locally after it was changed by the user. Most of our simpler use cases already updated as expected, but some trickier cases with indirect relationships did not. For example, changing the date taken of a photo updated the data model for the photo, but deleting a photo did not necessarily ensure the photo was removed from all the cached albums, groups, and galleries to which it belonged. (Note that the photo was removed correctly from the backend, just not from the cached representation of those entities on the client.) Cleaning up these relationships using change events between models helped, but didn’t solve all our problems. When someone outside of the local session (read: another user) changed data, it would not reflect in the current session. The only way to catch changes from outside the current session was to be more aggressive about evicting models. 2. Nuclear option https://www.flickr.com/photos/sdasmarchives/8091816484/ The pendulum swung all the way in the other direction — instead of surgical removal of data models we knew to be out-of-date, what would happen if we removed all cached data on every navigation? This prototype was quick to build, and incredibly destructive. By doing this, all our cached data always remained as fresh as could be, but we essentially reverted to Web 1.0 — with the exception of the Reboot framework, everything was reloaded on every page. Not surprisingly, this blew up API traffic (locally only! did not unleash that disaster at scale), and inflated page load times like a Jeff Koons sculpture. It did give us some baseline timing metrics we could point to as worst-case scenarios, however. The next step was to swing the pendulum back toward the middle — to a carefully-knitted solution that would preserve fast page loads and navigation, while ensuring the freshest data we could serve up. 3. Refetch on navigate At this point, our challenge was to find a solution that would keep navigation fast, API traffic slim, and pick up all changes to session data, whether local or remote. We ended up with a solution we call “refetching”: evicting and requesting new data models as the model is needed by the application. But when? We could refetch periodically or on a user action; we determined that the best time to trigger a refetch was on navigation — when the user navigates, cached models become eligible for refetching. Specifically, when the user navigates between sections of the site, refetching is triggered. This proved to be the happiest medium between speed and freshness. A high-level outline of how the refetching strategy works: The user loads a page; data are requested from the API, and models are cached. As new models are created, they’re marked as being fresh. The user navigates to another site section (e.g. Photostream → Search); all freshness marks are removed from all models. They’re now all eligible for refetching. As Reboot builds the new page, it requests data models from the cache. Since they no longer have their seal of freshness, they are refetched, and marked as fresh once retrieved and cached. One important note — refetching is not triggered on browser back/forward navigation. Users expect near-immediate navigation, thanks to browser caching, when navigating to already-viewed content. Therefore, we refetch only when the user clicks a link to navigate to a new site section. 4. Miscellany There were a couple other options we considered and rejected from the start, but they’re worth mentioning here. One was a TTL (time-to-live) algorithm , commonly used in caching applications. TTL algorithms expire data and evict from the cache a certain amount of time after they’re written or last updated. The arbitrary nature of TTL would mean that users would sometimes have fresh data and sometimes stale; it would be fresh more often than without any solution, but freshness would vary arbitrarily and would not result in much of an improvement on user experience. The other was to write an algorithm that tracks the amount of time since a data model was last accessed, and refetch when it grows too old. While this sounded interesting at first, it has the same flaw as a standard TTL algorithm — freshness becomes arbitrary. It’s also more complex to implement, and might end up not being worth the complexity. The doing (implementation) So that was it! Refetch on navigate, all done. Right?….of course not. With the general strategy in place, the devil started sneaking around in all the details. Some of the highlights: Exemptions It proved to be not the best idea to evict on all navigation. For example, in Reboot we often preload photo metadata models on pages with lists of photos, in order to make navigation into the photo page snappy. The refetch setup therefore has an exemption config that allows us to easily retain models when navigating into, away from, or between specific site sections. Child models We often have parent-child associations between data models. For example, the data model for a photo has a reference to a data model for the author of the photo. When the photo model is refetched, the person model must be refetched as well. This means the function doing the eviction and refetching has to recurse through all child models. Collections An issue similar to child models above, but more complex, is the case of a model containing a list of other models. For example, the data model for a person’s photostream contains a list of photo models. What made this particularly tricky is pagination and filtering — say you load the first 2 pages of your photostream, set your view filter to private, jump to page 5, switch the view to “Date taken”, and navigate away and back to your photostream…imagine the mess of different models with partially-loaded collections. Evicting one parent model, and its children, might evict photo models from the collection within another, without properly refetching. The solution here actually lay in the controller responsible for fetching pages: if a requested page of models is not already completely in-cache, a refetch will always happen to ensure we have all the data, in its freshest state. Refetch only once per page view Critical to the refetch-on-navigation strategy is to refetch only once per navigation. This was not too difficult, but essential to get right. We accomplish this by adding a flag when a model is initially fetched and upserted into the cache. When navigating to a new, non-exempt site section, all those flags are cleared, and any model requested by the new page will be refetched. When refetched, the model is again upserted into the cache and marked as fresh, until the next navigation. But did it fresh? With the thinking and the doing out of the way, it was time to push all this to production. Because these changes are essentially pulling the rug out from underneath the data layer on every navigation, we had to tread very carefully in order to prevent any negative impact to the end user experience. We did very thorough manual and automated testing across all of Reboot. We left the feature turned on for staff users for a while, to be able to respond to any bug reports. Finally, the time came to test on Real People. There were three things we needed to keep an eye on: errors (of course), impact on page navigation timing, and API traffic. Since refetching implies more requests for data, we needed to be sure that we were keeping the user experience smooth and fast, and also that we weren’t blowing up our data centers. https://www.flickr.com/photos/karolfranks/6296290871/ All in In order to get a good read on these things, though, we had to go all in. Letting in just a small percentage of users would not give reliable numbers for timing or traffic impacts, due to the noise inherent in relatively small sample sizes. So, we did something unusual: we turned on refetching for all users for a short period of time. We flipped on refetching and kept an eagle eye on our stats for 2 hours, then reverted; then, we took a careful look at the aggregated data to see how the experiment went. Surprisingly, the impact on both timing and traffic was relatively low. After some thought, we decided this is most likely because the changes disproportionately impact people on long sessions, say a Flickr tab open for hours or days. Most people don’t hang around that long; they come, they go. Also, the photo page represents north of 90% of our page views, and is exempt from refetching (see Exemptions above). So where did we end up? A negligible bump in navigation timing and API traffic, and fresher data for all. Perhaps an anticlimactic resolution, but the story we’ve heard today outlines a serious consideration for anyone building an application with a data caching layer: keep in mind from the beginning how you plan to deal with stale data, but in a way that keeps all the other benefits of a single-page application. Busting through staleness. Yep.", "date": "2015-09-1,"},
{"website": "Flickr", "title": "Powering Flickr’s Magic view by fusing bulk and real-time compute", "author": ["Bhautik Joshi"], "link": "https://code.flickr.net/2015/09/03/powering-flickrs-magic-view/", "abstract": "Try it for yourself! You can try out Flickr’s Magic View on your own photos here , and you can download a working code sample of the simplified lambda architecture here: https://github.com/yahoo/simplified-lambda Introduction In this post we’re going to talk about how we came up with a novel revision of the Lambda Architecture for fusing large-scale bulk compute with streaming compute to power Flickr’s Magic View. We were able to create a responsive, real time database operating at a scale of tens of billions of records, with tens to hundreds of millions of records updated per day. We turned to Yahoo’s Hadoop stack to find a way to build this at the massive scale we needed. Figure 1. Magic View in action Motivation: the Magic View Flickr’s Magic View takes the hassle out of organizing your photos by applying our computer-vision technology to automatically recognize objects or styles in your photos and present them to you in the Camera Roll’s scrolling view. This all happens in real time — as soon as a photo is uploaded, it is categorized and placed into the Magic View. Aggregating computer vision tags When a photo is uploaded, it is processed by a computer vision pipeline to generate a set of computer vision tags , which are text labels of the contents of the image. We already had an existing architecture for stream computation of tags on upload, but to implement the Magic View, we needed to maintain per-user reverse indexes and some aggregations of the tags. And we needed to make sure all the data was consistent — if a photo was added, removed or updated these indexes and aggregations would have to be updated to reflect this. Finally, we needed to initialize the system with tags for 12 billion photos and videos and run periodic backfills (every time we improved our computer vision algorithms and to cover cases where the stream compute missed images). The Problem We initially computed a snapshot of the Magic View indexes and aggregations using map-reduce (via Apache Oozie and Apache Pig ), and we were happy with the quick turnaround time (about 7 hours). We considered updating Magic View as a daily batch job, but soon realized this would not give our users the responsive, “live” experience we wanted. So, we built a streaming data layer using Apache Storm and were soon able to update the categories in Magic View in real-time. The next time we needed to run a backfill, we explored using this streaming layer to load the data. Unfortunately, the overhead of the read-modify-write process was simply too much for a load of this size — after kicking off the process we estimated it would take 28 days this way — much longer than the seven hours we had achieved with a bulk load. Twenty-eight days was a non-starter – we realized we needed a way to update our bulk aggregations independently of the real-time data streaming in. Solving this problem is how we arrived at our revision to Lambda Architecture. Before digging into the solution, let’s do a quick review of the Lambda Architecture.  If you’re already familiar with it, you can skip this next section. The Lambda Architecture We’ll start with Nathan Marz’s book ‘ Big Data ’, which proposes the database concept of  ‘Lambda Architecture.’ In his analysis, he states that a database query can be represented as a function – Query – which operates on all the data: result = Query(all data) In the Lambda architecture, a traditional database is replaced with both a real time and a bulk database. Then query function becomes a “combiner” function of independent queries to each database: result = Combiner(Query(real time data) + Query(bulk data)) An example of a typical Lambda Architecture is shown in figure 2. It is powered by an append-only queue for its system of record, which is fed by a real time stream of events. Periodically, all the data in the queue is fed into a bulk computation which pre-processes the data to optimize it for queries, and stores these aggregations in a bulk compute database. The real time event stream drives a stream computer, which processes the incoming events into real time aggregations. A query then goes via a query combiner, which queries both the bulk and real time databases, computes the combination, and stores the result. Figure 2. Typical Lambda Architecture While relatively new, Lambda Architecture has enjoyed popularity and a number of concrete implementations have been built. Some significant examples are the distributed analytics platform druid , Twitter’s Summingbird , and FiloDB . These implementations conveniently abstract away the databases behind the query combiner. A significant advantage with this style of architecture is robustness and fault-tolerance via eventual consistency. If a piece of data is skipped in the real time compute there is a guarantee that it will eventually appear in the bulk compute database. Criticism of the Lambda Architecture has centred around the complicated nature of the combiner . The combiner incurs a developer and systems cost from the need to maintain two different databases. It can be challenging to make sure both systems give the same result. Merging the two queries can become complicated, and finally, more points of failure may be introduced. The “Ah-ha” Moment Back to the problem. The data access layer we used for streaming compute uses the atomic read-modify-write pattern to ensure we write consistent data, one record-at-a-time to Apache HBase (a BigTable-style, non-relational database). Again, since this pattern was so much slower in the backfill case we needed to figure out how to get both consistent updates for streaming and fast loads of the full dataset. Since our bulk data was static, we realized that if we relaxed the consistency constraint we could just run a fast, streaming, write-only load of the bulk data, bringing the load time back down to hours instead of days. But how could we get around the consistency requirements? We didn’t want a bulk load to clobber data being written from the real time compute process. The insight was that we could just write bulk and streaming data to different column families in the same HBase row. So we added the concept of real time columns and bulk columns in a single row. Basically, bulk loads write to one set of columns and real time writes go to a different set of columns. Since HBase columns are sparse and data is updated relatively slowly we don’t pay much in storage or IO. We could  now simplify the equation back to: result = Combiner(Query(data)) The two sets of columns are managed separately by the real time and bulk subsystems. At query time, we perform a single fetch using the HBase API to get both the bulk and real time data. A separate combiner process assembles the final result. Implementation Figure 3. Magic View Architecture Figure 3 shows an overview of the system and our enhanced Lambda architecture. For the purposes of this discussion, a convenient abstraction is to consider that each row in the HBase table represents the current state of a given photo. The combiner stage is abstracted into a single Java process, which collects data from HBase and runs transformations on the data and sends it to a Redis cache which is used by the serving layer for the site. Consistency on read in HBase — the combiner We have two sets of columns to go with each row in HBase: bulk and real time. The combiner determines the final value for each attribute at read. In the case where data exists for real time but not for bulk (or vice versa) then there is only one value to choose. In the case where they both exist we always choose the real time value. This keeps the combiner very simple and fast. There is a trick though – whenever we do a backfill, we may need to repair the row since the backfill data may be newer than any real time data that is already present. It turns out this slows down the backfill from seven hours to about 14 — still far faster than loading with read-modify-write. Production throughput At scale, this architecture has been able to keep up very comfortably with production load. We can simultaneously run backfills to HBase and serve user information at the same time without impacting latency or the user experience. User experience An important measure for how the system works is how the viewer perceives it. The slowest part of the system is paging data from HBase into the serving cache; median time for above-the-fold latency – i.e. enough data is available to render the page – is around 10ms. Future directions Our experience has been very positive so far with Magic View and we’re looking at how we might enable users to browse their photos in other dimensions (location or color for example). Early tests have shown that building an OLAP or data cube in this architecture is certainly possible but it’s less clear that it will scale well. Contributors: Peter Welch, Bhautik Joshi, Hugo Haas, Srinivasan Singanallur, Ayan Ray, Pierre Garrigues, Ben Firestone, Sai Madhavan, Tim Miller Thanks to Nathan Marz for reviewing this post. Like this post? Have a love of online photography? Want to work with us? Flickr is hiring mobile, back-end and front-end engineers , in our San Francisco office. Find out more at flickr.com/jobs .", "date": "2015-09-3,"},
{"website": "Flickr", "title": "Perceptual Image Compression at Flickr", "author": ["Archie Russell"], "link": "https://code.flickr.net/2015/09/25/perceptual-image-compression-at-flickr/", "abstract": "Archie Russell, Peter Norby, Saeideh Bakhshi At Flickr our users really care about image quality.  They also care a lot about how responsive our apps are.  Addressing both of these concerns simultaneously is challenging;  higher quality images have larger file sizes and are slower to transfer.   Slow transfers are especially noticeable on mobile devices.   Flickr had historically aimed for high quality at the expense of larger files, but in late 2014 we implemented a method to both maintain image quality and decrease the byte-size of the images we serve to users.   As image appearance is very important to our users,  we performed an extensive user test before rolling this change out.   Here’s how we did it. Background:  JPEG Quality Settings Fig 1.    JPEG settings vs file size for a test image. JPEG compression has several tuneable knobs.   The q-value is the best known of these; it adjusts the level of spatial detail stored for fine details;  a higher q-value typically keeps more detail.    However,  as q-value gets very close to 100,  file size increases dramatically,  usually without improving image appearance. If file size and app performance isn’t an issue,  dialing up q-value is an easy way to get really nice-looking images; this is what Flickr has done in the past.    And if appearance isn’t very important,  dialing down q-value is a viable option.    But if you want both ,  you’re kind of stuck.   Additionally,  q-value isn’t one-size-fits-all,  some images look great at q-value 80 while others don’t. Another commonly adjusted setting is chroma-subsampling,  which alters the amount of color information stored in a JPEG file.    With a setting of 4:4:4,  the two chroma (color) channels in a JPG have as much information as the luminance channel.   In an image with a setting of 4:2:0, each chroma channel has only a quarter as much information as in an a 4:4:4 image. q=96,  chroma=4:4:4 (125KB) q=70, chroma=4:4:4 (67KB) q=96, chroma=4:2:0 (62KB) q=70, chroma=4:2:0 (62KB) Table 1:   JPEG stored at different quality and chroma levels.   The upper left image is saved at high quality and chroma level; notice the color and detail in the folds of the red flag.   The lower right image has the lowest quality;  notice artifacts along the right edges of the red flag. Perceptual JPEG Compression Ideally we’d have an algorithm which automatically tuned all JPEG parameters to make a file smaller, but which would limit perceptible changes to the image.  Technology exists that attempts to do this and can decrease image file size by 30-50%. This compression ratio is highly dependent on image content and dimensions. compressed: 112KB non-compressed: 224KB Fig 2. Compressed cropped JPEG is 50% smaller than not-compressed cropped JPEG, above, with no obvious defects.  Compression ratio is similar for a compressed 2048-pixel wide JPEG (475KB) of the entire scene and its corresponding not-compressed JPEG (897KB). We were pleased with perceptually compressed images in non-structured examinations.  The compressed images were smaller and nearly indistinguishable from their sources.   But we wanted to really quantify how well the technology worked before considering incorporating it into Flickr.  The standard computational tools for evaluating compression, such as SSIM, are fairly simplistic and don’t do a great job at modeling how a user sees things.  To really evaluate this technology had to use a better measure of perceptibility:  human minds. To test whether our image compression would impact user perception of image quality, we put together a “taste test.”  The taste test is constructed as a game with multiple rounds where users look at both compressed and uncompressed images.  Users accumulate points the longer they play, and get more points for doing well at the game.  We maintained a leaderboard to encourage participation and used only internal testers.The game’s test images came from a diverse collection of 250 images contributed by Flickr staff.  The images came from a variety of cameras and included a number of subjects from photographers with varying skill levels. Fig 3. A sampling of images used in our taste test. In each round, our test code randomly select a test image, and present two variants of this image side by side.  50% of the time we present the user two identical images; the rest of the time we present one compressed image and one uncompressed image.  We ask the tester if the two images look the same or different and we’d expect a user choosing randomly OR a user unable to distinguish the two cases would answer correctly about half the time.  We randomly swap the location of the compressed images to compensate for user bias to the left or the right.  If testers choose correctly, they are presented with a second question: “Which image did you prefer, and why?” Fig 4. Screenshot of taste test. Our test displays images simultaneously to prevent testers noticing a longer load time for the larger, non-compressed image.  The images are presented with either 320, 640, or 1600 pixels on their longest side.  The 320 & 640px images are shown for 12 seconds before being dimmed out.  The intent behind this detail is to represent how real users interact with our images.  The 1600px images stay on screen for 20 seconds, as we expect larger images to be viewed for longer periods of time by real users.   We award 100 points per round, regardless of whether a tester chose correctly and also award a bonus of 400 points when a tester correctly identifies whether images were identical or different.  We update the tester’s score every five tests so that the user perceives an increasing score without being rewarded immediately for any particular behavior. Taste Test Outcome and Deployment We ran our taste test for two weeks and analyzed our results.    Although we let users play as long as they liked,  we skipped the first result per user as a “warm-up” and considered only the subsequent ten results,  this limited the potential for users training themselves to spot compression artifacts.   We disregarded users that had fewer than eleven results. images total results # labeled “identical” by tester % labeled “identical” by tester two identical images 368 253 68.8% one compressed, one non-compressed 352 238 67.6% Table 2.   Taste test results.   Testers select “identical” at nearly the same rate, whether the input is identical or not. When our testers were presented with two identical images, they thought the images were identical only 68.8% of the time(!), and when presented with a compressed image next to a non-compressed image,  our testers thought the images were identical slightly less often:  67.6% of the time.  This difference was small enough for us,  and our statisticians told us it was statistically insignificant.  Our image pairs were so similar that multiple testers thought all images were identical and reported that the test system was buggy. We inspected the images most often labeled different, and found no significant artifacts in the compressed versions. So even in this side-by-side test,  perceptual image compression is just barely noticeable when images are presented side-by-side.  As the Flickr website wouldn’t ever show compressed and uncompressed images at the same time, and the use of compression had large benefits in storage footprint and site performance, we elected to go forward. At the beginning of 2014 we silently rolled out perceptual-based compression on our image thumbnails (we don’t alter the “original” images uploaded by our users).  The slight changes to image appearance went unnoticed by users, but user interactions with Flickr became much faster,  especially for users with slow connections, while our storage footprint became much smaller.  This was a best-case scenario for us. Evaluating perceptual compression was a considerable task,  but it gave the confidence we needed to apply this compression in production to our users.    This marked the first time Flickr had adjusted image settings in years, and, it was fun. Fig 5.  Taste test high score list Epilogue After eighteen months of perceptual compression at Flickr,  we adjusted our settings slightly to shrink images an additional 15%.   For our users on mobile devices,  15% fewer bytes per image makes for a much more responsive experience.We had run a taste test on this newer setting and users were were able to spot our compression slightly more often than with our original settings.   When presented a pair of identical images, our testers declared these images identical 65.2% of the time,  when presented with different images,  of our testers declared the images identical 62% of the time.   It wasn’t as imperceptible as our original approach, but, we decided it was close enough to roll out. Boy were we wrong!   A few very vocal users spotted the compression and didn’t like it at all.    The Flickr Help Forum had a very lively thread which Petapixel picked up .  We beat our heads against the wall considered our options and came up with a middle path between our initial and follow-on approaches,  giving us smaller, faster-to-load files while still maintaining the appearance our users expect. Through our use of perceptual compression,  combined with our use of on-the-fly resize and COS ,  we’ve been able to decrease our storage footprint dramatically, while simultaneously improving user experience. It’s a win all around but we’re not done yet — we still have a few tricks up our sleeves.", "date": "2015-09-25,"},
{"website": "Flickr", "title": "Flickr’s experience with iOS 9", "author": ["Rocir Santiago"], "link": "https://code.flickr.net/2015/11/18/flickrs-experience-with-ios-9/", "abstract": "In the last couple of months, Apple has released new features as part of iOS 9 that allow a deeper integration between apps and the operating system. Among those features are Spotlight Search integration, Universal Links, and 3D Touch for iPhone 6S and iPhone 6S Plus. Here at Flickr, we have added support for these new features and we have learned a few lessons that we would love to share. Spotlight Search There are two different kinds of content that can be searched through Spotlight: the kind that you explicitly index, and the kind that gets indexed based on the state your app is in. To explicitly index content, you use Core Spotlight, which lets you index multiple items at once. To index content related to your app’s current state, you use NSUserActivity: when a piece of content becomes visible, you start an activity to make iOS aware of this fact. iOS can then determine which pieces of content are more frequently visited, and thus more relevant to the user. NSUserActivity also allows us to mark certain items as public, which means that they might be shown to other iOS users as well. For a better user experience, we index as much useful information as we can right off the bat. We prefetch all the user’s albums, groups, and people they follow, and add them to the search index using Core Spotlight. Indexing an item looks like this: // Create the attribute set, which encapsulates the metadata of the item we're indexing\r\nCSSearchableItemAttributeSet *attributeSet = [[CSSearchableItemAttributeSet alloc] initWithItemContentType:(NSString *)kUTTypeImage];\r\nattributeSet.title = photo.title;\r\nattributeSet.contentDescription = photo.searchableDescription;\r\nattributeSet.keywords = photo.keywords;\r\nattributeSet.thumbnailData = UIImageJPEGRepresentation(photo.thumbnail, 0.98);\r\n\r\n// Create the searchable item and index it.\r\nCSSearchableItem *searchableItem = [[CSSearchableItem alloc] initWithUniqueIdentifier:[NSString stringWithFormat:@&quot;%@/%@&quot;, photo.identifier, photo.searchContentType] domainIdentifier:@&quot;FLKCurrentUserSearchDomain&quot; attributeSet:attributeSet];\r\n[[CSSearchableIndex defaultSearchableIndex] indexSearchableItems:@[ searchableItem ] completionHandler:^(NSError * _Nullable error) {\r\n                       if (error) {\r\n                           // Handle failures.\r\n                       }\r\n              }]; Since we have multiple kinds of data – photos, albums, and groups – we had to create an identifier that is a combination of its type and its actual model ID. Many users will have a large amount of data to be fetched, so it’s important that we take measures to make sure that the app still performs well. Since searching is unlikely to happen right after the user opens the app (that’s when we start prefetching this data, if needed), all this work is performed by a low-priority NSOperationQueue. If we ever need to fetch images to be used as thumbnails, we request it with low-priority NSURLSessionDownloadTask . These kinds of measures ensure that we don’t affect the performance of any operation or network request triggered by user actions, such as fetching new images and pages when scrolling through content. Flickr provides a huge amount of public content, including many amazing photos. If anybody searches for “Northern Lights” in Spotlight, shouldn’t we show them our best Aurora Borealis photos? For this public content – photos, public groups, tags and so on – we leverage NSUserActivity, with its new search APIs, to make it all searchable when viewed. Here’s an example: CSSearchableItemAttributeSet *attributeSet = [[CSSearchableItemAttributeSet alloc] initWithItemContentType:(NSString *) kUTTypeImage];\r\n// Setup attributeSet the same way we did before...\r\n// Set the related unique identifier, so it matches to any existing item indexed with Core Spotlight.     \r\nattributeSet.relatedUniqueIdentifier = [NSString stringWithFormat:@&quot;%@/%@&quot;, photo.identifier, photo.searchContentType];\r\n        \r\nself.userActivity = [[NSUserActivity alloc] initWithActivityType:@&quot;FLKSearchableUserActivityType&quot;];\r\nself.userActivity.title = photo.title;\r\nself.userActivity.keywords = [NSSet setWithArray:photo.keywords];\r\nself.userActivity.webpageURL = photo.photoPageURL;\r\nself.userActivity.contentAttributeSet = attributeSet;\r\nself.userActivity.eligibleForSearch = YES;\r\nself.userActivity.eligibleForPublicIndexing = photo.isPublic;\r\nself.userActivity.requiredUserInfoKeys = [NSSet setWithArray:self.userActivity.userInfo.allKeys];\r\n        \r\n[self.userActivity becomeCurrent]; Every time a user opens a photo, public group, location page, etc., we create a new NSUserActivity and make it current. The more often a specific activity is made current, the more relevant iOS considers it. In fact, the more often an activity is made current by any number of different users, the more relevant Apple considers it globally, and the more likely it will show up for other iOS users as well (provided it’s public). Until now we’ve only seen half the picture. We’ve seen how to index things for Spotlight search; when a user finally does search and taps on a result, how do we take them to the right place in our app? We’ll get to this a bit later, but for now suffice it to say that you’ll get a call to the method application:continueUserActivity:restorationHandler: to our application delegate. It’s important to note that if we wanted to make use of the userInfo in the NSUserActivity , iOS won’t give it back to you for free in this method. To get it, we have to make sure that we assigned an NSSet to the requiredUserInfoKeys property of our NSUserActivity when we created it. In their documentation, Apple also tells us that if you set the webpageURL property when eligibleForSearch is YES , you need to make sure that you’re pointing to the right web URL corresponding to your content, otherwise you might end up with duplicate results in Spotlight (Apple crawls your site for content to surface in Spotlight, and if it finds the same content at a different URL it’ll think it’s a different piece of content). Universal Links In order to support Universal Links, Apple requires that every domain supported by the app host an “apple-app-site-association” file at its root. This is a JSON file that describes which relative paths in your domains can be handled by the app. When a user taps a link from another app in iOS, if your app is able to handle that domain for a specific path, it will open your app and call application:continueUserActivity:restorationHandler: . Otherwise your application won’t be opened – Safari will handle the URL instead. {\r\n    &quot;applinks&quot;: {\r\n        &quot;apps&quot;: [],\r\n        &quot;details&quot;: {\r\n            &quot;XXXXXXXXXX.com.some.flickr.domain&quot;: {\r\n                &quot;paths&quot;: [\r\n                    &quot;/&quot;,\r\n                    &quot;/photos/*&quot;,\r\n                    &quot;/people/*&quot;,\r\n                    &quot;/groups/*&quot;\r\n                ]\r\n            }\r\n        }\r\n    }\r\n} This file has to be hosted on HTTPS with a valid certificate. Its MIME type needs to be “application/pkcs7-mime.” No redirects are allowed when requesting the file. If the only intent is to support Universal Links, no further steps are required. But if you’re also using this file to support Handoffs (introduced in iOS 8), then your file has to be CMS signed by a valid TLS certificate. In Flickr, we have a few different domains. That means that each one of flickr.com, http://www.flickr.com , m.flickr.com and flic.kr must provide its own JSON association file, whether or not they differ. In our case, the flic.kr domain actually does support different paths, since it’s only used for short URLs; hence, its “apple-app-site-association” is different than the others. On the client side, only a few steps are required to support Universal Links. First, “Associated Domains” must be enabled under the Capabilities tab of the app’s target settings. For each supported domain, an entry “applinks:” entry must be added. Here is how it looks for Flickr: That is it. Now if someone receives a text message with a Flickr link, she will jump right to the Flickr app when she taps on it. Deep linking into the app Great! We have Flickr photos showing up as search results and Flickr URLs opening directly in our app. Now we just have to get the user to the proper place within the app. There are different entry points into our app, and we need to make the implementation consistent and avoid code duplication. iOS has been supporting deep linking for a while already and so has Flickr. To support deep linking, apps could register to handle custom URLs (meaning a custom scheme, such as myscheme://mydata/123). The website corresponding to the app could then publish links directly to the app. For every custom URL published on the Flickr website, our app translates it into a representation of the data to be shown. This representation looks like this: @interface FLKRoute : NSObject\r\n\r\n@property (nonatomic) FLKRouteType type;\r\n@property (nonatomic, copy) NSString *identifier;\r\n\r\n@end It describes the type of data to present, and a unique identifier for that type of data. - (void)navigateToRoute:(FLKRoute *)route\r\n{\r\n    switch (route.type) {\r\n        case FLKRouteTypePhoto:\r\n            // Navigate to photo screen\r\n            break;\r\n        case FLKRouteTypeAlbum:\r\n           // Navigate to album screen\r\n            break;\r\n        case FLKRouteTypeGroup:\r\n            // Navigate to group screen\r\n            break;\r\n        // ...\r\n        default:\r\n            break;\r\n    }\r\n} Now, all we have to do is to make sure we are able to translate both NSURLs and NSUserActivity objects into FLKRoute instances. For NSURLs, this translation is straightforward. Our custom URLs follow the same pattern as the corresponding website URLs; their paths correspond exactly. So translating both website URLs and custom URLs is a matter of using NSURLComponents to extract the necessary information to create the FLKRoute object. As for NSUserActivity objects passed into application:continueUserActivity:restorationHandler: , there are two cases. One arises when the NSUserActivity instance was used to index a public item in the app. Remember that when we created the NSUserActivity object we also assigned its webpageURL ? This is really handy because it not only uniquely identifies the data we want to present, but also gives us a NSURL object, which we can handle the same way we handle deep links or Universal Links. The other case is when the NSUserActivity originated from a CSSearchableItem; we have some more work to do in this case. We need to parse the identifier we created for the item and translate it into a FLKRoute. Remember that our item’s identifier is a combination of its type and the model ID. We can decompose it and then create our route object. Its simplified implementation looks like this: FLKRoute * FLKRouteFromSearchableItemIdentifier(NSString *searchableItemIdentifier)\r\n{\r\n    NSArray *routeComponents = [searchableItemIdentifier componentsSeparatedByString:@&quot;/&quot;];\r\n    if ([routeComponents count] != 2) { // type + id\r\n        return nil;\r\n    }\r\n    \r\n    // Handle the route type\r\n    NSString *searchableItemContentType = [routeComponents firstObject];\r\n    FLKRouteType type = FLKRouteTypeFromSearchableItemContentType(searchableItemContentType);\r\n    \r\n    // Get the item identifier\r\n    NSString *itemIdentifier = [routeComponents lastObject];\r\n    \r\n    // Build the route object\r\n    FLKRoute *route = [FLKRoute new];\r\n    route.type = type;\r\n    route.parameter = itemIdentifier;\r\n    \r\n    return route;\r\n} Now we have all our bases covered and we’re sure that we’ll drop the user in the right place when she lands in our app. The final application delegate method looks like this: - (BOOL)application:(nonnull UIApplication *)application continueUserActivity:(nonnull NSUserActivity *)userActivity restorationHandler:(nonnull void (^)(NSArray * __nullable))restorationHandler\r\n{\r\n    FLKRoute *route;\r\n    NSString *activityType = [userActivity activityType];\r\n    NSURL *url;\r\n    \r\n    if ([activityType isEqualToString:CSSearchableItemActionType]) {\r\n        // Searchable item from Core Spotlight\r\n        NSString *itemIdentifier = [userActivity.userInfo objectForKey:CSSearchableItemActivityIdentifier];\r\n        route = FLKRouteFromSearchableItemIdentifier(itemIdentifier);\r\n        \r\n    } else if ([activityType isEqualToString:@&quot;FLKSearchableUserActivityType&quot;] ||\r\n               [activityType isEqualToString:NSUserActivityTypeBrowsingWeb]) {\r\n        // Searchable item from NSUserActivity or Universal Link\r\n        url = userActivity.webpageURL;\r\n        route = [url flk_route];\r\n        \r\n    }\r\n    \r\n    if (route) {\r\n        [self.router navigateToRoute:route];\r\n        return YES;\r\n    } else if (url) {\r\n        [[UIApplication sharedApplication] openURL:url]; // Fail gracefully\r\n        return YES;\r\n    } else {\r\n        return NO;\r\n    }\r\n} 3D Touch With the release of iPhone 6S and iPhone 6S Plus, Apple introduced a new gesture that can be used with your iOS app: 3D Touch. One of the coolest features it has brought is the ability to preview content before pushing it onto the navigation stack. This is also known as “peek and pop.” You can easily see how this feature is implemented in the native Mail app. But you won’t always have a simple UIView hierarchy like Mail’s UITableView, where a tap anywhere on a cell opens a UIViewController. Take Flickr’s notifications screen, for example: If the user taps on a photo in one of these cells, it will open the photo view. But if the user taps on another user’s name, it will open that user’s profile view. Previews of these UIViewControllers should be shown accordingly. But the “peek and pop” mechanism requires you to register a delegate on your UIViewController with registerForPreviewingWithDelegate:sourceView: , which means that you’re working in a much higher layer. Your UIViewController’s view might not even know about its subviews’ structures. To solve this problem, we used UIView’s method hitTest:withEvent: . As the documentation describes, it will give us the “farthest descendant of the receiver in the view hierarchy.” But not every hitTest will necessarily return the UIView that we want. So we defined a protocol, FLKPeekAndPopTargetView , that must be implemented by any UIView subclass that wants to support peeking and popping from it. That view is then responsible for returning the model used to populate the UIViewController that the user is trying to preview. If the view doesn’t implement this protocol, we query its superview. We keep checking for it until a UIView is found or there aren’t any more superviews available. This is how this logic looks: + (id)modelAtLocation:(CGPoint)location inSourceView:(UIView*)sourceView\r\n    \r\n    // Walk up hit-test tree until we find a peek-pop target.\r\n    UIView *testView = [sourceView hitTest:location withEvent:nil];\r\n    id model = nil;\r\n    while(testView &amp;&amp; !model) {\r\n      \r\n        // Check if the current testView conforms to the protocol.\r\n        if([testView conformsToProtocol:@protocol(FLKPeekAndPopTargetView)]) {\r\n            \r\n            // Translate location to view coordinates.\r\n            CGPoint locationInView = [testView convertPoint:location fromView:sourceView];\r\n            \r\n            // Get model from peek and pop target.\r\n            model = [((id&lt;FLKPeekAndPopTargetView&gt;)testView) flk_peekAndPopModelAtLocation:locationInView];\r\n            \r\n        } else {\r\n            //Move up view tree to next view\r\n            testView = testView.superview;\r\n        }\r\n    }\r\n    \r\n    return model;\r\n} With this code in place, all we have to do is to implement UIViewControllerPreviewingDelegate methods in our delegate, perform the hitTest and take the model out of the FLKPeekAndPopTargetView ‘s implementor. Here’s is the final implementation: - (UIViewController *)previewingContext:(id&lt;UIViewControllerPreviewing&gt;)previewingContext\r\n              viewControllerForLocation:(CGPoint)location {\r\n    \r\n    id model = [[self class] modelAtLocation:location inSourceView:previewingContext.sourceView];\r\n    UIViewController *viewController = nil;\r\n    if ([model isKindOfClass:[FLKPhoto class]]) {\r\n        viewController = // ... UIViewController that displays a photo.\r\n    } else if ([model isKindOfClass:[FLKAlbum class]]) {\r\n        viewController = // ... UIViewController that displays an album.\r\n    } else if ([model isKindOfClass:[FLKGroup class]]) {\r\n        viewController = // ... UIViewController that displays a group.\r\n    } // ...\r\n    return viewController;\r\n    \r\n}\r\n\r\n- (void)previewingContext:(id&lt;UIViewControllerPreviewing&gt;)previewingContext\r\n     commitViewController:(UIViewController *)viewControllerToCommit {\r\n    \r\n    [self.navigationController pushViewController:viewControllerToCommit animated:YES];\r\n    \r\n} Last but not least, we added support for Quick Actions. Now the user has the ability to quickly jump into a specific section of the app just by pressing down on the app icon. Defining these Quick Actions statically in the Info.plist file is an easy way to implement this feature, but we decided to go one step further and define these options dynamically. One of the options we provide is “Upload Photo,” which takes the user to the asset picker screen. But if the user has Auto Uploadr turned on, this option isn’t that relevant, so instead we provide a different app icon menu option in its place. Here’s how you can create Quick Actions: NSMutableArray&lt;UIApplicationShortcutItem *&gt; *items = [NSMutableArray array];\r\n    \r\n[items addObject:[[UIApplicationShortcutItem alloc] initWithType:@&quot;FLKShortcutItemFeed&quot;\r\n                                                  localizedTitle:NSLocalizedString(@&quot;Feed&quot;, nil)]];\r\n    \r\n[items addObject:[[UIApplicationShortcutItem alloc] initWithType:@&quot;FLKShortcutItemTakePhoto&quot;\r\n                                                  localizedTitle:NSLocalizedString(@&quot;Upload Photo&quot;, nil)] ];\r\n\r\n[items addObject:[[UIApplicationShortcutItem alloc] initWithType:@&quot;FLKShortcutItemNotifications&quot;\r\n                                                  localizedTitle:NSLocalizedString(@&quot;Notifications&quot;, nil)]];\r\n    \r\n[items addObject:[[UIApplicationShortcutItem alloc] initWithType:@&quot;FLKShortcutItemSearch&quot;\r\n                                                  localizedTitle:NSLocalizedString(@&quot;Search&quot;, nil)]];\r\n    \r\n[[UIApplication sharedApplication] setShortcutItems:items]; And this is how it looks like when the user presses down on the app icon: Finally, we have to handle where to take the user after she selects one of these options. This is yet another place where we can make use of our FLKRoute object. To handle the app opening from a Quick Action, we need to implement application:performActionForShortcutItem:completionHandler: in the app delegate. - (void)application:(UIApplication *)application performActionForShortcutItem:(UIApplicationShortcutItem *)shortcutItem completionHandler:(void (^)(BOOL))completionHandler {\r\n    FLKRoute *route = [shortcutItem flk_route];\r\n     [self.router navigateToRoute:route];\r\n    completionHandler(YES);\r\n} Conclusion There is a lot more to consider when shipping these features with an app. For example, with Flickr, there are various platforms the user could be using. It is important to make sure that the Spotlight index is up to date to reflect changes made anywhere. If the user has created a new album and/or left a group from his desktop browser, we need to make sure that those changes are reflected in the app, so the newly-created album can be found through Spotlight, but the newly-departed group cannot. All of this work should be totally opaque to the user, without hogging the device’s resources and deteriorating the user experience overall. That requires some considerations around threading and network priorities. Network requests for UI-relevant data should not be blocked because we have other network requests happening at the same time. With some careful prioritizing, using NSOperationQueue and NSURLSession , we managed to accomplish this with no major problems. Finally, we had to consider privacy, one of the pillars of Flickr. We had to be extremely careful not to violate any of the user’s settings. We’re careful to never publicly index private content, such as photos and albums. Also, photos marked “restricted” are not publicly indexed since they might expose content that some users might consider offensive. In this blog post we went into the basics of integrating iOS 9 Search, Universal Links, and 3D Touch in Flickr for iOS. In order to focus on those features, we simplified some of our examples to demonstrate how you could get started with them in your own app, and to show what challenges we faced. Like this post? Have a love of online photography? Want to work with us? Flickr is hiring mobile, back-end and front-end engineers , in our San Francisco office. Find out more at flickr.com/jobs .", "date": "2015-11-18,"},
{"website": "Flickr", "title": "The 32 Days Of Christmas!", "author": ["Jofish, John, Ayman in Labs and Frank in UXR"], "link": "https://code.flickr.net/2015/12/22/32-days-of-christmas/", "abstract": "When you have thousands of photos, it can be hard to find the photo you’re looking for. Want to search for that Christmas cat you saw at last year’s party? And what if that party wasn’t on Christmas day, but sometime the week before?  To help improve the search ranking and relevance of national, personal, and religious holiday photos, we first have to see when the photos were taken; when, for example, is the Christmas season ? Understanding what people are looking for when they search for their own photos is an important part of improving Flickr. Earlier this year, we began a study (which will be published at CHI 2016 under the same name as this post) by trying to understand how people searched for their personal photos. We showed a group of 74 participants roughly 20 of their own photos on Flickr, and asked them what they’d put into the Flickr search box to find those photos. We did this a total of 1492 times. It turns out 12% of the time people used a temporal term in searches for their own photos, meaning a word connected to time in some way. These might include a year (2015), a month (January), a season (winter), or a holiday or special event (Thanksgiving, Eid al-Fitr, Easter, Passover, Burning Man). Often, however, the date and time on the photograph didn’t match the search term: the year would be wrong, or people would search for a photograph of snow the weekend after Thanksgiving with the word “winter,” despite the fact that winter doesn’t officially begin until December 21st in the U.S. So we wanted to understand that situation: how often does fall feel like winter ? To answer this, we mapped 78.8 million Flickr photos tagged with a season name to the date the photo was actually taken. As you’d expect , most of the photographs tagged with a season are taken during that season: 66% of photos tagged “winter” were taken between December 22 and March 20. About 9% of search words are off by two seasons: photos tagged “summer” that were taken between December 21st and March 20th, for example. We expect this may reflect antipodean seasons: while most Flickr users are in the Northern Hemisphere, it doesn’t seem unreasonable that 5% of “summer” photographs might have been taken in the Southern Hemisphere. More interesting, we think, are the off-by-one cases, like fall photographs labeled as “winter,” where we believe that the photo represents the experience of winter, regardless of the objective reality of the calendar. For example, if it snows the day after Thanksgiving, it definitely feels like winter. On the topic of Thanksgiving, let’s look at photographs tagged “thanksgiving.” The six days between November 22nd and 27th—the darkest blue area—cover 65% of the photos. Expanding that range to November 15–30th covers 83%. Expanding to all of November covers 85%, and including October (and thus Canadian Thanksgiving, in gray in early October) brings the total to 90%. But that means that 10% of all photos tagged “thanksgiving” are outside of this range. Every date in that image represents a total of a minimum of 40 photographs taken on that day between 2003 and 2014 inclusive, uploaded to Flickr and tagged “thanksgiving” with the only white spaces being days that don’t exist, like February 30th or April 31st. Manual verification of some of the public photos tagged “thanksgiving” on arbitrarily chosen dates finds these photographs tagged “thanksgiving” included pumpkins or turkeys, autumnal leaves or cornucopias—all images culturally associated with the holiday. Not all temporal search terms are quite so complicated; some holidays are celebrated and photographed on a single day each year, like Canada Day (July 1st) or Boxing Day (December 26th). While these holidays can be easily translated to date queries, other holidays have more complicated temporal patterns. Have a look at these lunar holidays. There are some events that occur on a lunar calendar like Chinese New Year, Easter, Eid (both al-Fitr and al-Adha), and Hanukkah. These events move around in a regular, algorithmically determinable, but sometimes complicated, way. Most of these holidays tend to oscillate as a leap calculation is added periodically to synchronize the lunar timing to the solar calendar. However Eids, on the Hijri calendar , have no such leap correction, and we see photos tagged “Eid” edge forward year after year. Some holidays and events, like birthdays, happen on every day of the week. But they’re often celebrated , and thus photographed, on Friday, Saturday, and Sunday: So to get back to our original question: when are photos tagged “Christmas” actually taken? As you can see, more photos tagged “Christmas” are taken on December 25th than on any other day (19%). Christmas Eve is a close second, at 12%. If you look at other languages, this difference practically goes away: 9.2% of photos tagged “Noel” are taken on Christmas Eve, and 9.6% are taken on Christmas; “navidad” photos are 11.3% on Christmas Eve and 12.0% on Christmas. But Christmas photos are taken throughout December. We can now set a threshold for a definition of Christmas: say if at least 1% of the photos tagged “Christmas” were taken on that day, we’d rank it more relevant. That means that every day from December 1st to January 1st hits that definition, with December 2nd barely scraping in. That makes…32 days of Christmas! Merry Christmas and Happy Holidays —for all the holidays you celebrate and photograph. PS: Flickr is hiring ! Labs is hiring ! Come join us!", "date": "2015-12-22,"},
{"website": "Flickr", "title": "Configuration management for distributed systems (using GitHub and cfg4j)", "author": ["norbertpotocki"], "link": "https://code.flickr.net/2016/03/24/configuration-management-for-distributed-systems-using-github-and-cfg4j/", "abstract": "Norbert Potocki, Software Engineer @ Yahoo Inc. Warm up: Why configuration management? When working with large-scale software systems, configuration management becomes crucial; supporting non-uniform environments gets greatly simplified if you decouple code from configuration. While building complex software/products such as Flickr , we had to come up with a simple, yet powerful, way to manage configuration. Popular approaches to solving this problem include using configuration files or having a dedicated configuration service. Our new solution combines the extremely popular GitHub and cfg4j library, giving you a very flexible approach that will work with applications of any size. Why should I decouple configuration from the code? Faster configuration changes (e.g. flipping feature toggles): Configuration can simply be injected without requiring parts of your code to be reloaded and re-executed. Config-only updates tend to be faster than code deployment. Different configuration for different environments: Running your app on a laptop or in a test environment requires a different set of settings than production instance. Keeping credentials private: If you don’t have a dedicated credential store, it may be convenient to keep credentials as part of configuration. They usually aren’t supposed to be “public,” but the code still may be. Be a good sport and don’t keep credentials in a public GitHub repo. :) Meet the Gang: Overview of configuration management players Let’s see what configuration-specific components we’ll be working with today: Figure 1 –  Overview of configuration management components Configuration repository and editor : Where your configuration lives. We’re using Git for storing configuration files and GitHub as an ad hoc editor. Push cache : Intermediary store that we use to improve fetch speed and to ease load on GitHub servers. CD pipeline : Continuous deployment pipeline pushing changes from repository to push cache, and validating config correctness. Configuration library : Fetches configs from push cache and exposing them to your business logic. Bootstrap configuration : Initial configuration specifying where your push cache is (so that library knows where to get configuration from). All these players work as a team to provide an end-to-end configuration management solution. The Coach: Configuration repository and editor The first thing you might expect from the configuration repository and editor is ease of use. Let’s enumerate what that means: Configuration should be easy to read and write. It should be straightforward to add a new configuration set. You most certainly want to be able to review changes if your team is bigger than one person. It’s nice to see a history of changes, especially when you’re trying to fix a bug in the middle of the night. Support from popular IDEs – freedom of choice is priceless. Multi-tenancy support (optional) is often pragmatic. So what options are out there that may satisfy those requirements? The three very popular formats for storing configuration are YAML , Java Property files, and XML files. We use YAML – it is widely supported by multiple programming languages and IDEs, and it’s very readable and easy to understand, even by a non-engineer. We could use a dedicated configuration store; however, the great thing about files is that they can be easily versioned by version control tools like Git, which we decided to use as it’s widely known and proven. Git provides us with a history of changes and an easy way to branch off configuration. It also has great support in the form of GitHub which we use both as an editor (built-in support for YAML files) and collaboration tool (pull requests, forks, review tool). Both are nicely glued together by following the Git flow branching model. Here’s an example of a configuration file that we use: Figure 2 –  configuration file preview One of the goals was to make managing multiple configuration sets (execution environments) a breeze. We need the ability to add and remove environments quickly. If you look at the screenshot below, you’ll notice a “prod-us-east” directory in the path. For every environment, we store a separate directory with config files in Git. All of them have the exact same structure and only differ in YAML file contents. This solution makes working with environments simple and comes in very handy during local development or new production fleet rollout (see use cases at the end of this article). Here’s a sample config repo for a project that has only one “feature”: Figure 3 –  support for multiple environments Some of the products that we work with at Yahoo have a very granular architecture with hundreds of micro-services working together. For scenarios like this, it’s convenient to store configurations for all services in a single repository. It greatly reduces the overhead of maintaining multiple repositories. We support this use case by having multiple top-level directories, each holding configurations for one service only. The sprinter: Push cache The main role of push cache is to decrease the load put on the GitHub server and improve configuration fetch time. Since speed is the only concern here, we decided to keep the push cache simple: it’s just a key-value store. Consul was our choice, in part because it’s fully distributed. You can install Consul clients on the edge nodes and they will keep being synchronized across the fleet. This greatly improves both the reliability and the performance of the system. If performance is not a concern, any key-value store will do. You can skip using push cache altogether and connect directly to Github, which comes in handy during development (see use cases to learn more about this). The Manager: CD Pipeline When the configuration repository is updated, a CD pipeline kicks in. This fetches configuration, converts it into a more optimized format, and pushes it to cache. Additionally, the CD pipeline validates the configuration (once at pull-request stage and again after being merged to master) and controls multi-phase deployment by deploying config change to only 20% of production hosts at one time. The Mascot: Bootstrap configuration Before we can connect to the push cache to fetch configuration, we need to know where it is. That’s where bootstrap configuration comes into play. It’s very simple. The config contains the hostname, port to connect to, and the name of the environment to use. You need to put this config with your code or as part of the CD pipeline. This simple yaml file binding Spring profiles to different Consul hosts suffices for our needs: Figure 4 –  bootstrap configuration The Cool Guy: Configuration library The configuration library takes care of fetching the configuration from push cache and exposing it to your business logic. We use the library called cfg4j (“configuration for java”). This library re-loads configurations from the push cache every few seconds and injects them into configuration objects that our code uses. It also takes care of local caching, merging properties from different repositories, and falling back to user-provided defaults when necessary (read more at http://www.cfg4j.org/ ). Briefly summarizing how we use cfg4j’s features: Configuration auto-reloading: Each service reloads configuration every ~30 seconds and auto re-configures itself. Multi-environment support: for our multiple environments (beta, performance, canary, production-us-west, production-us-east, etc.). Local caching: Remedies service interruption when the push cache or configuration repository is down and also improves the performance for obtaining configs. Fallback and merge strategies: Simplifies local development and provides support for multiple configuration repositories. Integration with Dependency Injection containers – because we love DI! If you want to play with this library yourself, there’s plenty of examples both in its documentation and cfg4j-sample-apps Github repository . The Heavy Lifter: Configurable code The most important piece is business logic. To best make use of a configuration service, the business logic has to be able to re-configure itself in runtime. Here are a few rules of thumb and code samples: Use dependency injection for injecting configuration. This is how we do it using Spring Framework (see the bootstrap configuration above for host/port values): https://gist.github.com/norbertpotocki/e91aa64b524592432630 Use configuration objects to inject configuration instead of providing configuration directly – here’s where the difference is: Direct configuration injection (won’t reload as config changes) https://gist.github.com/norbertpotocki/eac0a927ca2df45c2a0b Configuration injection via “interface binding” (will reload as config changes): https://gist.github.com/norbertpotocki/0c0b5b9aa9d11c06c937 The exercise: Common use-cases (applying our simple solution) Configuration during development (local overrides) When you develop a feature, a main concern is the ability to evolve your code quickly.  A full configuration-management pipeline is not conducive to this. We use the following approaches when doing local development: Add a temporary configuration file to the project and use cfg4j’s MergeConfigurationSource for reading config both from the configuration store and your file. By making your local file a primary configuration source, you provide an override mechanism. If the property is found in your file, it will be used. If not, cfg4j will fall back to using values from configuration store. Here’s an example (reference examples above to get a complete code): https://gist.github.com/norbertpotocki/289f3943249ea2813dcf Fork the configuration repository, make changes to the fork and use cfg4j’s GitConfigurationSource to access it directly (no push cache required): https://gist.github.com/norbertpotocki/dacdcc6671a2158ded5e Set up your private push cache, point your service to the cache, and edit values in it directly. Configuration defaults When you work with multiple environments, some of them may share a configuration. That’s when using configuration defaults may be convenient. You can do this by creating a “default” environment and using cfg4j’s MergeConfigurationSource for reading config first from the original environment and then (as a fallback) from “default” environment. Dealing with outages Configuration repository, push cache, and configuration CD pipeline can experience outages. To minimize the impact of such events, it’s good practice to cache configuration locally (in-memory) after each fetch. cfg4j does that automatically. Responding to incidents – ultra fast configuration updates (skipping configuration CD pipeline) Tests can’t always detect all problems. Bugs leak to the production environment and at times it’s important to make a config change as fast as possible to stop the fire. If you’re using push cache, the fastest way to modify config values is to make changes directly within the cache. Consul offers a rich REST API and web ui for updating configuration in the key-value store. Keeping code and configuration in sync Verifying that code and configuration are kept in sync happens at the configuration CD pipeline level. One part of the continuous deployment process deploys the code into a temporary execution environment, and points it to the branch that contains the configuration changes. Once the service is up, we execute a batch of functional tests to verify configuration correctness. The cool down: Summary The presented solution is the result of work that we put into building huge-scale photo-serving services. We needed a simple, yet flexible, configuration management system. Combining Git , Github , Consul and cfg4j provided a very satisfactory solution that we encourage you to try. I want to thank the following people for reviewing this article: Bhautik Joshi, Elanna Belanger, Archie Russell . PS. You can also follow me on Twitter , GitHub , LinkedIn or my private blog .", "date": "2016-03-24,"},
{"website": "Flickr", "title": "Our Justified Layout Goes Open Source", "author": ["jimwhimpey"], "link": "https://code.flickr.net/2016/04/05/our-justified-layout-goes-open-source/", "abstract": "We introduced the justified layout on Flickr.com late in 2011. Our community of photographers loved it for its ability to efficiently display many photos at their native aspect ratio with visually pleasing, consistent whitespace, so we quickly added it to the rest of the website. It’s been through many iterations and optimizations. From back when we were primarily on the PHP stack to our lovely new JavaScript based isomorphic stack. Last year Eric Socolofsky did a great job explaining how the algorithm works and how it fits into a larger infrastructure for Flickr specifically. In the years following its launch, we’ve had requests from our front end colleagues in other teams across Yahoo for a reusable package that does photo (or any rectangle) presentation like this, but it’s always been too tightly coupled to our stack to separate it out and hand it over. Until now! Today we’re publishing the justified-layout algorithm wrapped in an npm module for you to use on the server, or client, in your own projects. Install/Download npm install justified-layout --save Or grab it directly from Github . Using it It’s really easy to use. No configuration is required. Just pass in an array of aspect ratios representing the photos/boxes you’d like to lay out: var layoutGeometry = require('justified-layout')([1.33, 1, 0.65] [, config]); If you only have dimensions and don’t want an extra step to convert them to aspect ratios, you can pass in an array of widths and heights like this: https://gist.github.com/jimwhimpey/825377b78ef8d9b10e702aa6adc41eb4 What it returns The geometry data for the layout items, in the same order they’re passed in. https://gist.github.com/jimwhimpey/faaf2c95809647abcbea481d8445ecf9 This is the extent of what the module provides. There’s no rendering component. It’s up to you to use this data to render boxes the way you want. Use absolute positioning, background positions, canvas, generate a static image on the backend, whatever you like! There’s a very basic implementation used on the demo and docs page . Configuration It’s highly likely the defaults don’t satisfy your requirements; they don’t even satisfy ours. There’s a full set of configuration options to customize the output just the way you want. My favorite is the fullWidthBreakoutRowCadence option that we use on album pages . All config options are documented on the docs and demo page . Compatibility Latest Chrome Latest Safari Latest Firefox Latest Mobile Safari IE 9+ Node 0.10+ The future The justified layout algorithm is just one part of our photo list infrastructure. Following this, we’ll be open sourcing more modules for handling data, handling state, reverse layouts, appending and prepending items for pagination. We welcome your feedback, issues and contributions on Github . P.S. Open Source at Flickr This is the first of quite a bit of code we have in the works for open source release. If working on open source projects appeals to you, we’re hiring !", "date": "2016-04-5,"},
{"website": "Flickr", "title": "We Want You… and Your Teammates", "author": ["Xanthe Travlos"], "link": "https://code.flickr.net/2016/05/11/we-want-you-and-your-teammates/", "abstract": "We’re hiring here at Flickr and we got pretty excited the other week when we saw Stripe’s post: BYOT (Bring Your Own Team) . The sum of the parts is greater than the whole and all that. Genius <big hat tip to them>. In case you didn’t read Stripe’s post, here’s the gist: you’re a team player, you like to make an impact, focus on a tough problem, set a challenging goal, and see the fruits of your labor after blood, sweat, and tears (or, maybe just brainpower). But you’ve got the itch to collaborate, to talk an idea through, break it down, and parallelize tasks or simply to be around your mates through work and play. Turns out you already have your go-to group of colleagues, roommates, siblings, or buddies that push, inspire, and get the best out of you. Well, in that case we may want to hire all of you! Like Stripe, we understand the importance of team dynamics. So if you’ve already got something good going on, we want in on it too. We love Stripe and are stoked for this initiative of theirs, but if Flickr tickles your fancy (and it does ours :) consider bringing that team of yours this way too, especially if you’ve got a penchant for mobile development. We’d love to chat! Email us: jobs at flickr.com Photos by: @Chris Martin and @Captain Eric Willis", "date": "2016-05-11,"},
{"website": "Flickr", "title": "Introducing yakbak: Record and playback HTTP interactions in NodeJS", "author": ["jeremyruppel"], "link": "https://code.flickr.net/2016/04/25/introducing-yakbak-record-and-playback-http-interactions-in-nodejs/", "abstract": "Did you know that the new Front End of www.flickr.com is one big Flickr API client? Writing a client for an existing API or service can be a lot of fun, but decoupling and testing that client can be quite tricky. There are many different approaches to taking the backing service out of the equation when it comes to writing tests for client code. Today we’ll discuss the pros and cons of some of these approaches, describe how the Flickr Front End team tests service-dependent libraries, and introduce you to our new NodeJS HTTP playback module: yakbak ! Scenario: Testing a Flickr API Client Let’s jump into some code, shall we? Suppose we’re testing a (very, very simple) photo search API client: https://gist.github.com/jeremyruppel/fd25c723a5962a49936f174d765aa11a Currently, this code will make an HTTP request to the Flickr API on every test run. This is less than desirable for several reasons: UGC is unpredictable . In this test, we’re asserting that the response code is an HTTP 200, but obviously our client code needs to provide the response data to be useful. It’s impossible to write a meaningful and predictable test against live content. Traffic is unpredictable . This photos search API call usually takes ~150ms for simple queries, but a more complex query or a call during peak traffic may take longer. Downtime is unpredictable . Every service has downtime (the term is “four nines,” not “one hundred percent” for a reason), and if your service is down, your client tests will fail. Networks are unpredictable . Have you ever tried coding on a plane? Enough said. We want our test suite to be consistent, predictable, and fast. We’re also only trying to test our client code, not the API. Let’s take a look at some ways to replace the API with a control, allowing us to predictably test the client code. Approach 1: Stub the HTTP client methods We’re using superagent as our HTTP client, so we could use a mocking library like sinon to stub out superagent’s Request methods: https://gist.github.com/jeremyruppel/8b837f439663db325aaa2437a2259934 With these changes, we never actually make an HTTP request to the API during a test run. Now our test is predictable, controlled, and it runs crazy fast . However, this approach has some major drawbacks: Tightly coupled with superagent. We’re all up in the client’s implementation details here, so if superagent ever changes their API, we’ll need to correct our tests to match. Likewise, if we ever want to use a different HTTP client, we’ll need to correct our tests as well. Difficult to specify the full HTTP response . Here we’re only specifying the statusCode ; what about when we need to specify the body or the headers? Talk about verbose. Not necessarily accurate . We’re trusting the test author to provide a fake response that matches what the actual server would send back. What happens if the API changes the response schema? Some unhappy developer will have to manually update the tests to match reality (probably an intern, let’s be honest). We’ve at least managed to replace the service with a control in our tests, but we can do (slightly) better. Approach 2: Mock the NodeJS HTTP module Every NodeJS HTTP client will eventually delegate to the standard NodeJS http module to perform the network request. This means we can intercept the request at a low level by using a tool like nock : https://gist.github.com/jeremyruppel/d92a62400f635b42249adc041cdecc96 Great! We’re no longer stubbing out superagent and we can still control the HTTP response. This avoids the HTTP client coupling from the previous step, but still has many similar drawbacks: We’re still completely implementation-dependent. If we want to pass a new query string parameter to our service, for example, we’ll also need to add it to the test so that nock will match the request. It’s still laborious to specify the response headers, body, etc. It’s still difficult to make sure the response body always matches reality. At this point, it’s worth noting that none of these bullet points were an issue back when we were actually making the HTTP request. So, let’s do exactly that (once!). Approach 3: Record and playback the HTTP interaction The Ruby community created the excellent VCR gem for recording and replaying HTTP interactions during tests. Recorded HTTP requests exist as “tapes”, which are just files with some sort of format describing the interaction. The basic workflow goes like this: The client makes an actual HTTP request. VCR sits in front of the system’s HTTP library and intercepts the request. If VCR has a tape matching the request, it simply replays the response to the client. Otherwise, VCR lets the HTTP request through to the service, records the interaction to a new tape on disk and plays it back to the client. Introducing yakbak Today we’re open-sourcing yakbak , our take on recording and playing back HTTP interactions in NodeJS. Here’s what our tests look like with a yakbak proxy: https://gist.github.com/jeremyruppel/7050b34342a10d8e3dd8bc2dba0d50c0 Here we’ve created a standard NodeJS http.Server with our proxy middleware. We’ve also configured our client to point to the proxy server instead of the origin service. Look, no implementation details! yakbak tries to do things The Node Way™ wherever possible. For example, each yakbak “tape” is actually its own module that simply exports an http.Server handler, which allows us to do some really cool things. For example, it’s trivial to create a server that always responds a certain way. Since the tape’s hash is based solely on the incoming request, we can easily edit the response however we like. We’re also kicking around a handful of enhancements that should make yakbak an even more powerful development tool. Thanks to yakbak, we’ve been writing fast, consistent, and reliable tests for our HTTP clients and applications. Want to give it a spin? Check it out today: https://github.com/flickr/yakbak P.S. We’re hiring! Do you love development tooling and helping keep teams on the latest and greatest technology? Or maybe you just want to help build the best home for your photos on the entire internet? We’re hiring Front End Ops and tons of other great positions. We’d love to hear from you!", "date": "2016-04-25,"},
{"website": "Flickr", "title": "Personalized Group Recommendations on Flickr", "author": ["Mehul Patel"], "link": "https://code.flickr.net/2016/09/30/personalized-group-recommendations-on-flickr/", "abstract": "There are two primary paradigms for the discovery of digital content. First is the search paradigm, in which the user is actively looking for specific content using search terms and filters (e.g., Google web search , Flickr image search , Yelp restaurant search , etc.). Second is a passive approach, in which the user browses content presented to them (e.g., NYTimes news , Flickr Explore , and Twitter trending topics ). Personalization benefits both approaches by providing relevant content that is tailored to users’ tastes (e.g., Google News , Netflix homepage , LinkedIn job search , etc.). We believe personalization can improve the user experience at Flickr by guiding both new as well as more experienced members as they explore photography. Today, we’re excited to bring you personalized group recommendations. Flickr Groups are great for bringing people together around a common theme, be it a style of photography, camera, place, event, topic, or just some fun. Community members join for several reasons—to consume photos, to get feedback, to play games, to get more views, or to start a discussion about photos, cameras, life or the universe. We see value in connecting people with appropriate groups based on their interests. Hence, we decided to start the personalization journey by providing contextually relevant and personalized content that is tuned to each person’s unique taste. Of course, in order to respect users’ privacy, group recommendations only consider public photos and public groups. Additionally, recommendations are private to the user. In other words, nobody else sees what is recommended to an individual. In this post we describe how we are improving Flickr’s group recommendations. In particular, we describe how we are replacing a curated, non-personalized, static list of groups with a dynamic group recommendation engine that automatically generates new results based on user interactions to provide personalized recommendations unique to each person. The algorithms and backend systems we are building are broad and applicable to other scenarios, such as photo recommendations, contact recommendations, content discovery, etc. Figure : Personalized group recommendations Challenges One challenge of recommendations is determining a user’s interests. These interests could be user-specified, explicit preferences or could be inferred implicitly from their actions, supported by user feedback. For example: Explicit: Ask users what topics interest them Ask users why they joined a particular group Implicit: Infer user tastes from groups they join, photos they like, and users they follow Infer why users joined a particular group based on their activity, interactions, and dwell time Feedback: Get feedback on recommended items when users perform actions such as “Join” or “Follow” or click “Not interested” Another challenge of recommendations is figuring out group characteristics. I.e.: what type of group is it? What interests does it serve? What brings Flickr members to this group? We can infer this by analyzing group members, photos posted to the group, discussions and amount of activity in the group. Once we have figured out user preferences and group characteristics, recommendations essentially becomes a matchmaking process. At a high-level, we want to support 3 use cases: Use Case # 1 : Given a group, return all groups that are “similar” Use Case # 2 : Given a user, return a list of recommended groups Use Case # 3 : Given a photo, return a list of groups that the photo could belong to Collaborative Filtering One approach to recommender systems is presenting similar content in the current context of actions. For example, Amazon’s “Customers who bought this item also bought” or LinkedIn’s “People also viewed.” Item-based collaborative filtering can be used for computing similar items. Figure : Collaborative filtering in action By Moshanin (Own work) [ CC BY-SA 3.0 ] from Wikipedia Intuitively, two groups are similar if they have the same content or same set of users. We observed that users often post the same photo to multiple groups. So, to begin, we compute group similarity based on a photo’s presence in multiple groups. Consider the following sample matrix M ( G i -> P j ) constructed from group photo pools, where 1 means a corresponding group ( G i ) contains an image, and empty (0) means a group does not contain the image. From this, we can compute M. M’ ( M ’s transpose ), which gives us the number of common photos between every pair of groups (G i , G j ): We use modified cosine similarity to compute a similarity score between every pair of groups: To make this calculation robust, we only consider groups that have a minimum of X photos and keep only strong relationships (i.e., groups that have at least Y common photos). Finally, we use the similarity scores to come up with the top k-nearest neighbors for each group. We also compute group similarity based on group membership —i.e., by defining group-user relationship (G i -> U j ) matrix. It is interesting to note that the results obtained from this relationship are very different compared to (G i , P j ) matrix. The group-photo relationship tends to capture groups that are similar by content (e.g.,“macro photography”). On the other hand, the group-user relationship gives us groups that the same users have joined but are possibly about very different topics, thus providing us with a diversity of results. We can extend this approach by computing group similarity using other features and relationships (e.g., autotags of photos to cluster groups by themes, geotags of photos to cluster groups by place, frequency of discussion to cluster groups by interaction model, etc.). Using this we can easily come up with a list of similar groups (Use Case # 1). We can either merge the results obtained by different similarity relationships into a single result set, or keep them separate to power features like “Other groups similar to this group” and “People who joined this group also joined.” We can also use the same data for recommending groups to users (Use Case # 2). We can look at all the groups that the user has already joined and recommend groups similar to those. To come up with a list of relevant groups for a photo (Use Case # 3), we can compute photo similarity either by using a similar approach as above or by using Flickr computer vision models for finding photos similar to the query photo. A simple approach would then be to recommend groups that these similar photos belong to. Implementation Due to the massive scale (millions of users x 100k groups) of data, we used Yahoo’s Hadoop Stack to implement the collaborative filtering algorithm. We exploited sparsity of entity-item relationship matrices to come up with a more efficient model of computation and used several optimizations for computational efficiency. We only need to compute the similarity model once every 7 days, since signals change slowly. Figure : Computational architecture (All logos and icons are trademarks of respective entities) Similarity scores and top k-nearest neighbors for each group are published to Redis for quick lookups needed by the serving layer. Recommendations for each user are computed in real-time when the user visits the groups page. Implementation of the serving layer takes care of a few aspects that are important from usability and performance point-of-view: Freshness of results : Users hate to see the same results being offered even though they might be relevant. We have implemented a randomization scheme that returns fresh results every X hours, while making sure that results stay static over a user’s single session. Diversity of results : Diversity of results in recommendations is very important since a user might not want to join a group that is very similar to a group he’s already involved in. We require a good threshold that balances similarity and diversity. To improve diversity further, we combine recommendations from different algorithms. We also cluster the user’s groups into diverse sets before computing recommendations. Dynamic results : Users expect their interactions to have a quick effect on recommendations. We thus incorporate user interactions while making subsequent recommendations so that the system feels dynamic. Performance : Recommendation results are cached so that API response is quick on subsequent visits. Cold Start The drawback to collaborative filtering is that it cannot offer recommendations to new users who do not have any associations. For these users, we plan to recommend groups from an algorithmically computed list of top/trending groups alongside manual curation. As users interact with the system by joining groups, the recommendations become more personalized. Measuring Effectiveness We use qualitative feedback from user studies and alpha group testing to understand user expectation and to guide initial feature design. However, for continued algorithmic improvements, we need an objective quantitative metric. Recommendation results by their very nature are subjective, so measuring effectiveness is tricky. The usual approach taken is to roll out to a random population of users and measure the outcome of interest for the test group as compared to the control group (ref: A/B testing ). We plan to employ this technique and measure user interaction and engagement to keep improving the recommendation algorithms. Additionally, we plan to measure explicit signals such as when users click “Not interested.” This feedback will also be used to fine-tune future recommendations for users. Figure : Measuring user engagement Future Directions While we’re seeing good initial results, we’d like to continue improving the algorithms to provide better results to the Flickr community. Potential future directions can be classified broadly into 3 buckets: algorithmic improvements, new product use cases, and new recommendation applications. If you’d like to help, we’re hiring. Check out our jobs page and get in touch. Product Engineering : Mehul Patel, Chenfan (Frank) Sun,  Chinmay Kini", "date": "2016-09-30,"},
{"website": "Flickr", "title": "A Year Without a Byte", "author": ["Archie Russell"], "link": "https://code.flickr.net/2017/01/05/a-year-without-a-byte/", "abstract": "One of the largest cost drivers in running a service like Flickr is storage. We’ve described multiple techniques to get this cost down over the years: use of COS , creating sizes dynamically on GPUs and perceptual compression . These projects have been very successful, but our storage cost is still significant. At the beginning of 2016, we challenged ourselves to go further — to go a full year without needing new storage hardware. Using multiple techniques, we got there. The Cost Story A little back-of-the-envelope math shows storage costs are a real concern. On a very high-traffic day, Flickr users upload as many as twenty-five million photos. These photos require an average of 3.25 megabytes of storage each, totalling over 80 terabytes of data. Stored naively in a cloud service similar to S3, this day’s worth of data would cost over $30,000 per year, and continue to incur costs every year. And a very large service will have over two hundred million active users. At a thousand images each, storage in a service similar to S3 would cost over $250 million per year (or $1.25 / user-year) plus network and other expenses. This compounds as new users sign up and existing users continue to take photos at an accelerating rate. Thankfully, our costs, and every large service’s costs, are different than storing naively at S3, but remain significant. Cost per byte have decreased, but bytes per image from iPhone-type platforms have increased. Cost per image hasn’t changed significantly. Storage costs do drop over time. For example, S3 costs dropped from $0.15 per gigabyte month in 2009 to $0.03 per gigabyte-month in 2014, and cloud storage vendors have added low-cost options for data that is infrequently accessed. NAS vendors have also delivered large price reductions. Unfortunately, these lower costs per byte are counteracted by other forces. On iPhones, increasing camera resolution, burst mode and the addition of short animations (Live Photos) have increased bytes-per-image rapidly enough to keep storage cost per image roughly constant. And iPhone images are far from the largest. In response to these costs, photo storage services have pursued a variety of product options. To name a few: storing lower quality images or re-compressing, charging users for their data usage, incorporating advertising, selling associated products such as prints, and tying storage to purchases of handsets. There are also a number of engineering approaches to controlling storage costs. We sketched out a few and cover three that we implemented below: adjusting thresholds on our storage systems, rolling out existing savings approaches to more images, and deploying lossless JPG compression. Adjusting Storage Thresholds As we dug into the problem, we looked at our storage systems in detail. We discovered that our settings were based on assumptions about high write and delete loads that didn’t hold. Our storage is pretty static. Users only rarely delete or change images once uploaded. We also had two distinct areas of just-in-case space. 5% of our storage was reserved space for snapshots, useful for undoing accidental deletes or writes, and 8.5% was held free in reserve. This resulted in about 13% of our storage going unused. Trade lore states that disks should remain 10% free to avoid performance degradation, but we found 5% to be sufficient for our workload. So we combined our our two just-in-case areas into one and reduced our free space threshold to that level. This was our simplest approach to the problem (by far), but it resulted in a large gain. With a couple simple configuration changes, we freed up more than 8% of our storage. Adjusting storage thresholds Extending Existing Approaches In our earlier posts, we have described dynamic generation of thumbnail sizes and perceptual compression. Combining the two approaches decreased thumbnail storage requirements by 65%, though we hadn’t applied these techniques to many of our images uploaded prior to 2014. One big reason for this: large-scale changes to older files are inherently risky, and require significant time and engineering work to do safely. Because we were concerned that further rollout of dynamic thumbnail generation would place a heavy load on our resizing infrastructure, we targeted only thumbnails from less-popular images for deletes. Using this approach, we were able to handle our complete resize load with just four GPUs. The process put a heavy load on our storage systems; to minimize the impact we randomized our operations across volumes. The entire process took about four months, resulting in even more significant gains than our storage threshold adjustments. Decreasing the number of thumbnail sizes Lossless JPG Compression Flickr has had a long-standing commitment to keeping uploaded images byte-for-byte intact. This has placed a floor on how much storage reduction we can do, but there are tools that can losslessly compress JPG images. Two well-known options are PackJPG and Lepton , from Dropbox. These tools work by decoding the JPG, then very carefully compressing it using a more efficient approach. This typically shrinks a JPG by about 22%. At Flickr’s scale, this is significant. The downside is that these re-compressors use a lot of CPU. PackJPG compresses at about 2MB/s on a single core, or about fifteen core-years for a single petabyte worth of JPGs. Lepton uses multiple cores and, at 15MB/s, is much faster than packJPG, but uses roughly the same amount of CPU time. This CPU requirement also complicated on-demand serving. If we recompressed all the images on Flickr, we would need potentially thousands of cores to handle our decompress load. We considered putting some restrictions on access to compressed images, such as requiring users to login to access original images, but ultimately found that if we targeted only rarely accessed private images, decompressions would occur only infrequently. Additionally, restricting the maximum size of images we compressed limited our CPU time per decompress. We rolled this out as a component of our existing serving stack without requiring any additional CPUs, and with only minor impact to user experience. Running our users’ original photos through lossless compression was probably our highest-risk approach. We can recreate thumbnails easily, but a corrupted source image cannot be recovered. Key to our approach was a re-compress-decompress-verify strategy: every recompressed image was decompressed and compared to its source before removing the uncompressed source image. This is still a work-in-progress. We have compressed many images but to do our entire corpus is a lengthy process, and we had reached our zero-new-storage-gear goal by mid-year. On The Drawing Board We have several other ideas which we’ve investigated but haven’t implemented yet. In our current storage model, we have originals and thumbnails available for every image, each stored in two datacenters. This model assumes that the images need to be viewable relatively quickly at any point in time. But private images belonging to accounts that have been inactive for more than a few months are unlikely to be accessed. We could “freeze” these images, dropping their thumbnails and recreate them when the dormant user returns. This “thaw” process would take under thirty seconds for a typical account. Additionally, for photos that are private (but not dormant), we could go to a single uncompressed copy of each thumbnail, storing a compressed copy in a second datacenter that would be decompressed as needed. We might not even need two copies of each dormant original image available on disk. We’ve pencilled out a model where we place one copy on a slower, but underutilized, tape-based system while leaving the other on disk. This would decrease availability during an outage, but as these images belong to dormant users, the effect would be minimal and users would still see their thumbnails. The delicate piece here is the placement of data, as seeks on tape systems are prohibitively slow. Depending on the details of what constitutes a “dormant” photo these techniques could comfortably reduce storage used by over 25%. We’ve also looked into de-duplication, but we found our duplicate rate is in the 3% range. Users do have many duplicates of their own images on their devices, but these are excluded by our upload tools.  We’ve also looked into using alternate image formats for our thumbnail storage. WebP can be much more compact than ordinary JPG but our use of perceptual compression gets us close to WebP byte size and permits much faster resize.  The BPG project proposes a dramatically smaller, H.265 based encoding but has IP and other issues. There are several similar optimizations available for videos. Although Flickr is primarily image-focused, videos are typically much larger than images and consume considerably more storage. Conclusion Optimization over several releases Since 2013 we’ve optimized our usage of storage by nearly 50%.  Our latest efforts helped us get through 2016 without purchasing any additional storage,  and we still have a few more options available. Peter Norby, Teja Komma, Shijo Joy and Bei Wu formed the core team for our zero-storage-budget project. Many others assisted the effort.", "date": "2017-01-5,"},
{"website": "Flickr", "title": "Introducing Similarity Search at Flickr", "author": ["Clayton Mellina"], "link": "https://code.flickr.net/2017/03/07/introducing-similarity-search-at-flickr/", "abstract": "At Flickr, we understand that the value in our image corpus is only unlocked when our members can find photos and photographers that inspire them, so we strive to enable the discovery and appreciation of new photos. To further that effort, today we are introducing similarity search on Flickr. If you hover over a photo on a search result page, you will reveal a “…” button that exposes a menu that gives you the option to search for photos similar to the photo you are currently viewing. In many ways, photo search is very different from traditional web or text search. First, the goal of web search is usually to satisfy a particular information need, while with photo search the goal is often one of discovery ; as such, it should be delightful as well as functional. We have taken this to heart throughout Flickr. For instance, our color search feature, which allows filtering by color scheme, and our style filters, which allow filtering by styles such as “minimalist” or “patterns,” encourage exploration. Second, in traditional web search, the goal is usually to match documents to a set of keywords in the query. That is, the query is in the same modality—text—as the documents being searched. Photo search usually matches across modalities: text to image. Text querying is a necessary feature of a photo search engine, but, as the saying goes, a picture is worth a thousand words. And beyond saving people the effort of so much typing, many visual concepts genuinely defy accurate description. Now, we’re giving our community a way to easily explore those visual concepts with the “…” button, a feature we call the similarity pivot . The similarity pivot is a significant addition to the Flickr experience because it offers our community an entirely new way to explore and discover the billions of incredible photos and millions of incredible photographers on Flickr. It allows people to look for images of a particular style , it gives people a view into universal behaviors , and even when it “messes up,” it can force people to look at the unexpected commonalities and oddities of our visual world with a fresh perspective . What is “similarity”? To understand how an experience like this is powered, we first need to understand what we mean by “similarity.” There are many ways photos can be similar to one another. Consider some examples. It is apparent that all of these groups of photos illustrate some notion of “similarity,” but each is different. Roughly, they are: similarity of color, similarity of texture, and similarity of semantic category. And there are many others that you might imagine as well. What notion of similarity is best suited for a site like Flickr? Ideally, we’d like to be able to capture multiple types of similarity, but we decided early on that semantic similarity—similarity based on the semantic content of the photos—was vital to facilitate discovery on Flickr. This requires a deep understanding of image content for which we employ deep neural networks. We have been using deep neural networks at Flickr for a while for various tasks such as object recognition, NSFW prediction, and even prediction of aesthetic quality. For these tasks, we train a neural network to map the raw pixels of a photo into a set of relevant tags, as illustrated below. Internally, the neural network accomplishes this mapping incrementally by applying a series of transformations to the image, which can be thought of as a vector of numbers corresponding to the pixel intensities. Each transformation in the series produces another vector, which is in turn the input to the next transformation, until finally we have a vector that we specifically constrain to be a list of probabilities for each class we are trying to recognize in the image. To be able to go from raw pixels to a semantic label like “hot air balloon,” the network discards lots of information about the image, including information about  appearance, such as the color of the balloon, its relative position in the sky, etc. Instead, we can extract an internal vector in the network before the final output. For common neural network architectures, this vector—which we call a “feature vector”—has many hundreds or thousands of dimensions. We can’t necessarily say with certainty that any one of these dimensions means something in particular as we could at the final network output, whose dimensions correspond to tag probabilities. But these vectors have an important property: when you compute the Euclidean distance between these vectors, images containing similar content will tend to have feature vectors closer together than images containing dissimilar content. You can think of this as a way that the network has learned to organize information present in the image so that it can output the required class prediction. This is exactly what we are looking for: Euclidian distance in this high-dimensional feature space is a measure of semantic similarity. The graphic below illustrates this idea: points in the neighborhood around the query image are semantically similar to the query image, whereas points in neighborhoods further away are not. This measure of similarity is not perfect and cannot capture all possible notions of similarity—it will be constrained by the particular task the network was trained to perform, i.e., scene recognition. However, it is effective for our purposes, and, importantly, it contains information beyond merely the semantic content of the image, such as appearance, composition, and texture. Most importantly, it gives us a simple algorithm for finding visually similar photos: compute the distance in the feature space of a query image to each index image and return the images with lowest distance. Of course, there is much more work to do to make this idea work for billions of images. Large-scale approximate nearest neighbor search With an index as large as Flickr’s, computing distances exhaustively for each query is intractable. Additionally, storing a high-dimensional floating point feature vector for each of billions of images takes a large amount of disk space and poses even more difficulty if these features need to be in memory for fast ranking. To solve these two issues, we adopt a state-of-the-art approximate nearest neighbor algorithm called Locally Optimized Product Quantization (LOPQ). To understand LOPQ, it is useful to first look at a simple strategy. Rather than ranking all vectors in the index, we can first filter a set of good candidates and only do expensive distance computations on them. For example, we can use an algorithm like k -means to cluster our index vectors, find the cluster to which each vector is assigned, and index the corresponding cluster id for each vector. At query time, we find the cluster that the query vector is assigned to and fetch the items that belong to the same cluster from the index. We can even expand this set if we like by fetching items from the next nearest cluster. This idea will take us far, but not far enough for a billions-scale index. For example, with 1 billion photos, we need 1 million clusters so that each cluster contains an average of 1000 photos. At query time, we will have to compute the distance from the query to each of these 1 million cluster centroids in order to find the nearest clusters. This is quite a lot. We can do better, however, if we instead split our vectors in half by dimension and cluster each half separately. In this scheme, each vector will be assigned to a pair of cluster ids, one for each half of the vector. If we choose k = 1000 to cluster both halves, we have k 2 = 1000 * 1000 = 1e6 possible pairs. In other words, by clustering each half separately and assigning each item a pair of cluster ids, we can get the same granularity of partitioning (1 million clusters total) with only 2 * 1000 distance computations with half the number of dimensions for a total computational savings of 1000x. Conversely, for the same computational cost, we gain a factor of k more partitions of the data space, providing a much finer-grained index. This idea of splitting vectors into subvectors and clustering each split separately is called product quantization . When we use this idea to index a dataset it is called the inverted multi-index , and it forms the basis for fast candidate retrieval in our similarity index. Typically the distribution of points over the clusters in a multi-index will be unbalanced as compared to a standard k-means index, but this unbalance is a fair trade for the much higher resolution partitioning that it buys us. In fact, a multi-index will only be balanced across clusters if the two halves of the vectors are perfectly statistically independent. This is not the case in most real world data, but some heuristic preprocessing—like PCA-ing and permuting the dimensions so that the cumulative per-dimension variance is approximately balanced between the halves—helps in many cases. And just like the simple k-means index, there is a fast algorithm for finding a ranked list of clusters to a query if we need to expand the candidate set. After we have a set of candidates, we must rank them. We could store the full vector in the index and use it to compute the distance for each candidate item, but this would incur a large memory overhead (for example, 256 dimensional vectors of 4 byte floats would require 1Tb for 1 billion photos) as well as a computational overhead. LOPQ solves these issues by performing another product quantization, this time on the residuals of the data. The residual of a point is the difference vector between the point and its closest cluster centroid. Given a residual vector and the cluster indexes along with the corresponding centroids, we have enough information to reproduce the original vector exactly. Instead of storing the residuals, LOPQ product quantizes the residuals, usually with a higher number of splits, and stores only the cluster indexes in the index. For example, if we split the vector into 8 splits and each split is clustered with 256 centroids, we can store the compressed vector with only 8 bytes regardless of the number of dimensions to start (though certainly a higher number of dimensions will result in higher approximation error). With this lossy representation we can produce a reconstruction of a vector from the 8 byte codes: we simply take each quantization code, look up the corresponding centroid, and concatenate these 8 centroids together to produce a reconstruction. Likewise, we can approximate the distance from the query to an index vector by computing the distance between the query and the reconstruction. We can do this computation quickly for many candidate points by computing the squared difference of each split of the query to all of the centroids for that split. After computing this table, we can compute the squared difference for an index point by looking up the precomputed squared difference for each of the 8 indexes and summing them together to get the total squared difference. This caching trick allows us to quickly rank many candidates without resorting to distance computations in the original vector space. LOPQ adds one final detail: for each cluster in the multi-index, LOPQ fits a local rotation to the residuals of the points that fall in that cluster. This rotation is simply a PCA that aligns the major directions of variation in the data to the axes followed by a permutation to heuristically balance the variance across the splits of the product quantization. Note that this is the exact preprocessing step that is usually performed at the top-level multi-index. It tends to make the approximate distance computations more accurate by mitigating errors introduced by assuming that each split of the vector in the production quantization is statistically independent from other splits. Additionally, since a rotation is fit for each cluster, they serve to fit the local data distribution better. Below is a diagram from the LOPQ paper that illustrates the core ideas of LOPQ. K-means (a) is very effective at allocating cluster centroids, illustrated as red points, that target the distribution of the data, but it has other drawbacks at scale as discussed earlier. In the 2d example shown, we can imagine product quantizing the space with 2 splits, each with 1 dimension. Product Quantization (b) clusters each dimension independently and cluster centroids are specified by pairs of cluster indexes, one for each split. This is effectively a grid over the space. Since the splits are treated as if they were statistically independent, we will, unfortunately, get many clusters that are “wasted” by not targeting the data distribution. We can improve on this situation by rotating the data such that the main dimensions of variation are axis-aligned. This version, called Optimized Product Quantization (c), does a better job of making sure each centroid is useful. LOPQ (d) extends this idea by first coarsely clustering the data and then doing a separate instance of OPQ for each cluster, allowing highly targeted centroids while still reaping the benefits of product quantization in terms of scalability. LOPQ is state-of-the-art for quantization methods, and you can find more information about the algorithm, as well as benchmarks, here . Additionally, we provide an open-source implementation in Python and Spark which you can apply to your own datasets. The algorithm produces a set of cluster indexes that can be queried efficiently in an inverted index, as described. We have also explored use cases that use these indexes as a hash for fast deduplication of images and large-scale clustering. These extended use cases are studied here . Conclusion We have described our system for large-scale visual similarity search at Flickr. Techniques for producing high-quality vector representations for images with deep learning are constantly improving, enabling new ways to search and explore large multimedia collections. These techniques are being applied in other domains as well to, for example, produce vector representations for text , video , and even molecules . Large-scale approximate nearest neighbor search has importance and potential application in these domains as well as many others. Though these techniques are in their infancy, we hope similarity search provides a useful new way to appreciate the amazing collection of images at Flickr and surface photos of interest that may have previously gone undiscovered. We are excited about the future of this technology at Flickr and beyond. Acknowledgements Yannis Kalantidis, Huy Nguyen, Stacey Svetlichnaya, Arel Cordero. Special thanks to the rest of the Computer Vision and Machine Learning team and the Vespa search team who manages Yahoo’s internal search engine.", "date": "2017-03-7,"},
{"website": "Flickr", "title": "Together", "author": ["Matthew Roth"], "link": "https://code.flickr.net/2018/04/20/together/", "abstract": "Flickr is excited to be joining SmugMug ! We’re looking forward to some interesting and challenging engineering projects in the next year, and would love to have more great people join the team! We want to talk to people who are interested in working on an inclusive, diverse team, building large-scale systems that are backing a much-loved product. You can learn more about open positions at: http://jobs.smugmug.com/ Read our announcement blog post and our extended Q&A for more details. ~The Flickr Team", "date": "2018-04-20,"}
]