[
{"website": "Invision", "title": "Better Pull Request Management with Custom Labels", "author": ["Kevin Lamping"], "link": "https://engineering.invisionapp.com/post/pr-labels/", "abstract": "GitHub has some neat features for managing pull requests, but sometimes it’s tough to quickly get a high-level status for each request. To improve this workflow, we’ve implemented a custom labeling pattern for pull requests on some of our code repos. Using GitHub Labels , we mark the state each PR is in, so that it’s clear where the Pull Request is at in its flow. The Main Flow Almost all PRs move through the following cycle: Code added/updated, needs review Code reviewed, needs update Repeat as necessary until good to merge We mark where the PR is in the cycle with the following labels: PR: Needs Review After opening a Pull Request, you’ll likely want someone to review it. Mark it with the PR: Needs Review label to identify to others on your team that your PR is ready for review. PR: Reviewed w/ Comments After the initial review, most PRs are left with a few comments/questions about the changes. As a cue to the original author that you’ve passed through the changes and have finished your thoughts, the reviewer removes the PR: Needs Review label (if applicable) and adds PR: Reviewed w/Comments . Next, the submitter responds to the comments and updates the code as necessary. They then begin the cycle again by adding PR: Needs Review and removing PR: Reviewed w/ Comments . PR: Good to Merge At some point, the PR should be good to go. To help avoid any ambiguity in state, the reviewer should mark the PR as PR: Good to Merge . This lets the author know that their code is fully accepted by the team. After merging, the labels can be left as is. The Pull Request is complete and won’t need updating again. Other Scenarios There are a few special cases that deserve their own label. They don’t come up as often, but are good to have around when needed. PR: Needs Manual Merge Sometimes PRs get out of date with the main branch and need a manual merge. GitHub will let you know this by greying out the “Merge” button, but it can be helpful to mark this via a label so the team can easily know the status of the PR. PR: Merge on CICD Pass For small PRs, or PRs reviewed and small updates applied, there are times when the code has been reviewed but the CICD task hasn’t completed yet. A reviewer can mark the PR as good to merge with the condition that the build passes by using the PR: Merge on CICD Pass label. DO NOT MERGE There are times when you have some experimental work in the form of a PR that you’d like to open up for review. Ensure it doesn’t accidentally get merged in by adding the DO NOT MERGE label. While this doesn’t disable the “Merge” button, it should be a red flag to anyone reviewing the code not to merge it in. On Hold/Work in Progress For when you want to put some future functionality out there for review, or an unexpected blocker arises during the PR process, or you just need to wait until another PR goes through, the On Hold and Work in Progress labels help reviewers know the status of a PR that isn’t progressing in the workflow. Using this label is helpful in avoiding review fatigue, where PRs seem to die out unexpectedly. At least now if they die out, reviewers aren’t left wondering the state of the PR. They know to just let it sit until time is found to continue work on the PR, or the PR is closed without merge because circumstances changed. Similar to the “DO NOT MERGE” label, you can also update the title of the PR to be extra clear of its current state. Challenges There are two major challenges with this system: Newcomers won’t be familiar with the convention and may not follow the steps. It usually doesn’t take much effort to get everyone up to speed, but it can be confusing when people forget to update the label. The labels have to be manually added to every Github repo, as the defaults aren’t configurable. It’s tedious to take this step when you’re just trying out a new idea and also makes it difficult to validate the convention is being followed across all repos. One way to overcome both of these drawbacks would be to add a PR Review bot, which would help set up and enforce the labelling. Something similar to MentionBot . In fact, this is something we hope to look at in the near future using the Github API . There’s certainly always room for improvement, but using this pattern for our PRs has already paid off. Give it a shot and let us know if there are any labels you think we should add to our bucket. Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-05-05"},
{"website": "Invision", "title": "Testing Our Shared ESLint Configs", "author": ["Kevin Lamping"], "link": "https://engineering.invisionapp.com/post/testing-eslint/", "abstract": "It started out as a string of Fiddler on the Roof jokes about our favorite JavaScript linting tool : I kind of wish @geteslint would end its error report with \"TRADITION!\" and a link to: https://t.co/9o0ig7UJuQ — Kevin Lamping (@klamping) February 17, 2016 ESLint: \"Tradition!\" BabelJS: \"Transpiler, Transpiler\" Typescript: \"If I Were A Typed Lang\" https://t.co/HVsWdHbLpZ — Evan Goer (@evangoer) February 17, 2016 \"Because of eslint, we've kept our balance for many, many years. Here in Anatevka, we have rules for everything: how to indent, how to name\" — Kevin Lamping (@klamping) February 17, 2016 \"Traditions, traditions. Without our traditions, our lives would be as shaky as... as... as a file without semicolons! \" — Kevin Lamping (@klamping) February 17, 2016 Then Nicholas C. Zakas took me seriously: @klamping you can write a custom formatter that does that. :) — Nicholas C. Zakas (@slicknet) February 17, 2016 When the creator of ESLint calls you out, you have to oblige, and so I did: . @slicknet YES! module.exports = function(results) { if (results.length > 0) { console.log(\"TRADITION! https://t.co/xDHnyQA2nX \") } } — Kevin Lamping (@klamping) February 17, 2016 We then joked about the usefulness of the formatter: @klamping @slicknet I feel like I am witnessing the creation of an important piece of technology, proud to be a small part of it — Evan Goer (@evangoer) February 17, 2016 @evangoer @slicknet (at tomorrow's standup) \"So Kevin, what did you do yesterday\" \"Looked into eslint custom formatters, very useful stuff!\" — Kevin Lamping (@klamping) February 17, 2016 @klamping @evangoer I wish for this to be on npm by week's end. — Nicholas C. Zakas (@slicknet) February 18, 2016 @slicknet @evangoer It wasn't week's end, but I did finally get around to publishing it: https://t.co/a1iPbKPbv2 — Kevin Lamping (@klamping) April 21, 2016 All of this was fun to play around with, but getting a chance to see ESLint’s custom formatters in action spurred an idea for an actual improvement to InVision’s shared ESLint configs. Stepping Back When we first put our ESLint configs together, we included a basic test to verify everything worked as expected. To do this, we created a sample file (we’ll name it pass.js )  of how we’d like our code to look and then ran that through our ESLint rules. It was nothing more complicated than running eslint . If the file failed, it meant one of our rules was misconfigured. This helps when upgrading ESLint, especially when we transitioned to 1.x. Having this check was great, but I couldn’t help remember a quote by Edsger W. Dijkstra that a previous co-worker told me: “Testing shows the presence, not the absence of bugs.” False Positives There was a distinct issue with our test suite: The passing result looks exactly the same as the result from it not running at all. In other words, when we run ESLint, we’re looking for a “0 errors/warnings” result, which could possibly happen if our rules are misconfigured (e.g. it’s not catching errors it should). To work around this, we need a “negative” test. A way to say “look at this file and validate that errors are caught”. So we created a doppleganger of our pass.js file called fail.js . Fail.js This file contains syntax that should fail an ESLint check. I have to admit, it was kind of fun to write. Mismatched indention, unused variables; we broke all the rules. The only problem was that the errors in the file would mean the test run would report as a failure. To invert the results, I found a clever little command line hack: eslint examples/fail.js -f compact && echo 'Error: failures not caught' && exit 1 || exit 0 This script essentially reverses the status code returned from ESLint, exiting with a failure if ESLint passes, and passing if ESLint fails. It’s a bit backwards, but it serves our purpose of validating that ESLint is catching errors. Getting Specific This solution worked for a bit, but it was pretty fragile. It didn’t actually count the number of errors returned, just that an error was caught. So even if only 9 out of 10 errors are caught, the test still shows as passing. This is where we harken back to the conversation that I began the post with. Witnessing the ability to handle the results from ESLint in a custom manner gave me an idea for improving our failure test suite. Instead of blindly accepting that any sort of failure is a good thing, what if we checked the specific number of errors and warnings against an expected result? Here’s what our new script looks like: ERRORS = 13 WARNINGS = 1 eslint examples/fail.js -f ./failure-reporter.js A little shorter, and less complexity. We’re passing in two environmental variables, defining the number of errors and warnings we’re expecting to see. We also tell ESLint to use a customer formatter, which looks like: function validateCountMatch ( expected , actual , type ) { if ( expected != actual ) { console . log ( \"Expected \" + expected + \" \" + type + \" but found \" + actual ); return false ; } return true ; } module . exports = function ( results ) { results = results || [ ]; // accumulate the errors and warnings var summary = results . reduce ( function ( seq , current ) { seq . errors += current . errorCount ; seq . warnings += current . warningCount ; return seq ; }, { errors : 0 , warnings : 0 } ); var errorCountMatches = validateCountMatch ( process . env . ERRORS , summary . errors , \"errors\" ); var warningCountMatches = validateCountMatch ( process . env . WARNINGS , summary . warnings , \"warnings\" ); if ( errorCountMatches && warningCountMatches ) { process . exit ( 0 ); } else { process . exit ( 1 ); } }; The reporter is pretty straightforward. It tallies the number of errors and warnings found, then checks that count against the expected values, reporting a failure message if they differ. It then calls process.exit with the proper exit code to complete the script (Note: We do have to call process.exit(0) , otherwise it will respond with ESLint’s failure code due to the code errors.) Here’s what the output looks like if our tests don’t pass: With our new formatter, we can now be more certain that updates don’t unexpectedly turn off flags or miss failures. If you haven’t checked out ESLint yet, the flexibility of the tool shown here should hopefully convince you to invest some time in researching it. Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-05-12"},
{"website": "Invision", "title": "Logging for CoreOS and Kubernetes: How Containerization Saved the Day!", "author": ["Chuck Freitas"], "link": "https://engineering.invisionapp.com/post/coreos-system-logs/", "abstract": "It seemed a simple request: store the system logs from our EC2 instances living in AWS. Seems pretty fundamental. After all, Amazon already provides an agent for shipping logs to their API. This should be a no-brainer. A slam-dunk! Not so fast! Our instances run CoreOS, part of our Kubernetes infrastructure, and nothing was showing up. Need to get creative… Things get interesting On a more traditional Linux based system it would be a simple matter of running the CloudWatch agent which would tail one or more log files sending that information to AWS via the CloudWatch APIs. CoreOS is a minimal OS designed for running containers and it comes with Docker pre-installed as a systemd service. This, along with its focus on security and reliability, is a large part on why we choose CoreOS for running our Kubernetes Infrastructure - more on this in a future blog post. As part of this minimal design, CoreOS does not provide a package management system, like yum or apt , and does not come with a traditional syslog daemon. Accessing the logs If you’re familiar with CoreOS you will know that CoreOS uses systemd 's logging system journald for managing logs. The journald daemon stores the log information in a binary format. A quick look at the docs for journald and journald.conf shows some configuration options - none of which gave us access to the logs in a format required. CoreOS does provide the journalctl utility for accessing the machine’s journal/logs. With this information the obvious solution is to use journalctl .  The journalctl utility has the ability to tail the systemd journal with the -f flag: journalctl -f . Using this approach we can pipe the logs somewhere. But where? One option would be to pipe them to a file and then have the CloudWatch agent read the file that way. However without a package-management solution installing the CloudWatch Agent is not a simple solution. Plus we want something that is easy to update, and would work with our existing container infrastructure. We need something else… Sending the logs We are big fans of Docker at InVision and it’s only natural to look for a solution running in a container. A while back, Amazon announced its Container Service, called ECS.  They posted a blog article about sending container logs to CloudWatch “[Send ECS Container Logs to CloudWatch Logs for Centralized Monitoring]( http://blogs.aws.amazon.com/application-management/post/TxFRDMTMILAA8X/Send-ECS-C ontainer-Logs-to-CloudWatch-Logs-for-Centralized-Monitoring)\". They were also kind enough to provide a working example that someone had already ported to CoreOS: cloudwatchlogs . This implementation involves running rsyslog and the CloudWatch agent in a container. rsyslog listens on a port (in this case 514 ) writing the sys logs it receives to the container’s filesystem. The CloudWatch agent reads these files sending the logs to the CloudWatch API. These are all managed by a supervisord process manager. We modified this implementation a bit internally - mostly to allow setting the Log Group name via an environment variable and changing the log file location. Now we have somewhere to send the output of journalctl -f , but how to get them to the container? There just so happens to be a great utility already installed on CoreOS that works great for this situation, ncat . Ncat is a small utility for reading and writing data across a network. Using ncat allows us to do the following: /usr/bin/journalctl -o short -f | /usr/bin/ncat 127.0.0.1 514 . Putting it all together The CloudWatch Agent requires permission to write to the CloudWatch Service. You will need to grant the AWS User Account the following permissions: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : [ \"logs:Create*\" , \"logs:PutLogEvents\" ], \"Effect\" : \"Allow\" , \"Resource\" : \"arn:aws:logs:*:*:*\" } ] } CoreOS uses cloud-config for its instance setup . We make extensive use of cloud-config for the initial configuration of all our EC2 instances. It’s a simple matter of adding these as systemd services to the cloud-config script. First we need to configure the CloudWatch container to listen for logs. Since this is a Docker container we want it to run after the Docker service has started. - name: cloudwatchlogs.service\n  command: start\n  content: | [ Unit ] Description = Cloudwatch Logs Service After = docker.service Requires = docker.service [ Service ] User = core Restart = always TimeoutStartSec = 30 RestartSec = 10 ExecStartPre = -/usr/bin/docker kill cloudwatchlogs ExecStartPre = -/usr/bin/docker rm cloudwatchlogs ExecStartPre = /usr/bin/docker pull roundsphere/cloudwatchlogs:latest ExecStart = /usr/bin/docker run --name cloudwatchlogs -p 514:514 -e \"AWS_ACCESS_KEY_ID=[YOUR-AWS-ACCESS-KEY]\" -e \"AWS_SECRET_ACCESS_KEY=[YOUR-AWS-SECRET-KEY]\" roundsphere/cloudwatchlogs:latest ExecStop = /usr/bin/docker stop cloudwatchlogs [ Install ] WantedBy = multi-user.target For our worker nodes that are running as part of our Kubernetes infrastructure, the cloudwatchlogs.service can be deployed as a DaemonSet . This allows kubernetes to manage the lifecycle of the Cloudwatch log service versus systemd and fits in well with the rest of our application infrastructure. apiVersion : extensions/v1beta1 kind : DaemonSet metadata : name : cloudwatch-agent labels : app : system tier : backend release : stable spec : template : metadata : labels : app : system name : cloudwatch-pod version : v1 spec : containers : - name : cloudwatch-con imagePullPolicy : IfNotPresent securityContext : privileged : true ports : - hostPort : 514 containerPort : 514 name : rsyslogport image : 'roundsphere/cloudwatchlogs:latest' env : - name : AWS_ACCESS_KEY_ID value : [ YOUR-AWS-ACCESS-KEY ] - name : AWS_SECRET_ACCESS_KEY value : [ YOUR-AWS-SECRET-KEY ] Second we need to configure a service to send the logs to the container. It is just a matter of implementing our journalctl command as a systemd service unit. We want this to start after the cloudwatchlogs.service has started. - name: journalctl-output.service\n  command: start\n  content: | [ Unit ] Description = Sends log output to container After = cloudwatchlogs.service [ Service ] Type = simple Restart = always TimeoutStartSec = 60 RestartSec = 60 ExecStart = /usr/bin/bash -c '/usr/bin/journalctl -o short -f | /usr/bin/ncat 127.0.0.1 514' ExecStop = [ Install ] WantedBy = multi-user.target If you prefer to manually install these to test it out, you can save the content of each of these services to individual files (in this example journalctl-output.service and cloudwatchlogs.service ) and place them in the /etc/systemd/system/ directory. Make sure permissions are set correctly and then enable them like so: sudo systemctl daemon-reload\n\nsudo systemctl enable cloudwatchlogs\nsudo systemctl start cloudwatchlogs\n\nsudo systemctl enable journalctl-output\nsudo systemctl start journalctl-output And That’s It! You should start to see your logs showing up in the AWS CloudWatch Logs console. Note: logs will be grouped by the defined awslogs and then logged under each instance name. You can modify this by editing awslogs.conf file. It’s important to note that this implementation does not guarantee that 100% of the journald logs will be sent to CloudWatch. While the container will be restarted if it crashes ( supervisord will manage individual processes in the container), due to timing issues some logs may get lost. We have been running this in Production for a few months now and have not had any issues. The overall system impact is pretty minimal and the combined services seem stable. Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-06-10"},
{"website": "Invision", "title": "The Trials of Mobile Automation at InVision", "author": ["Josh Barber"], "link": "https://engineering.invisionapp.com/post/trials-of-mobile-automation/", "abstract": "Anyone who works in mobile development can relate to the pain and frustration of long build times, device provisioning and installation management.   The InVision mobile team struggled with these things for some time before we threw up our hands and shouted to the heavens for help!  After much toiling and iterating we have completely automated our mobile build process and nearly eradicated all frustration around this part of our job. I invite you to read on where I share some tools and processes which we have developed, that could remove all pain and frustration for your engineering team around these frustrations as well. Feel the Pain!!! - Back Story InVision is a 100% remote company and without going into how awesome that is for a variety of reasons, it does present some unique challenges when it comes to managing physical devices for testing. In a traditional office setting, I could just walk over and grab your phone to install our app, or hand out pre-installed devices for testing.  Being remote, our team had to figure out a fast and painless way to get builds on devices remotely with as little friction as possible to our testers. Our current process involved a lot of back and forth which was time consuming and frustrating for both developers and testers. We also repeatedly spent a not-insignificant amount of time merely creating builds for people to test. Xcode builds would take five to ten minutes to compile (longer when we needed to update our Cocoapods).  With context-switching time costs this could eat up an hour or more of daily developer time. We felt the pain and we decided to stop the madness. We had the-twinkle-of-a-vision of a promised land where we could create builds that didn’t require a developer to get involved, that anyone could run on-demand without developer assistance. Step 1 - We say goodbye to Testflight Early on we used Testflight to manage our builds for testing, but its limitations forced us to look elsewhere.  One issue we had was that Testflight only allows one build available at any given time. We frequently had the need for testers to test an older version; Strike 1.  We also wanted to test a special build with a small group of people; Strike 2.  Finally, we ran into issues where Testflight builds would hang, and our only recourse was to get Apple involved; Strike 3.  It became obvious that we had to go with someone else, (but don’t worry it’s not you, it’s me.) Step 2 - Say hello to Fabric (Crashlytics) Our problems with Testflight were all solved by switching to Fabric .  Fabric (formerly known as Crashlytics) is the build management and reporting tool that is offered by Twitter .  With Fabric we were able to begin making groups of testers, a history of builds, and start on the path to self service deployments. As we continued our journey to self serve deployments we discovered that Fabric has an external API which could be leveraged to generate builds, which would prove invaluable to us once we discovered Bitrise. Step 3 - Things start getting better Bitrise is an amazing product that got us nearly to the finish line.  Bitrise is a SaaS service which offers several features that we leveraged including customizable workflows that allow you to set up automation like cloning specific branches on git repositories, installing mobile provisioning profiles, building in XCode, and deploying those builds to Fabric. We have a few different workflows that we use, but here are the steps to our most common build process: Preparing the Build Environment: that’s a required Bitrise step, and it’s obvious what it’s doing. Git Clone Repository: this pulls from our Github repo and it takes an argument that says what branch it should load (more on this later). Certificate and profile installer: this step gets the latest certificate and mobile profiles.  It’s a HUGE time saver because when new devices were added everyone would have to manually refresh locally in XCode.  This step automates it. Set the XCode/Git Build Number: This step sets the build number based off the date and latest git commits.  It’s a step that Peter Gulyas wrote in house ( https://github.com/InVisionApp/steps-set-xcode-git-build-number ) and you should really check it out if you want to automate build numbers. XCode Create Archive: Here’s where the magic happens!  Bitrise will generate a new build from the code that it downloaded in step #2. Fabric deployer: Once our build is finished in step 5, Bitrise will automatically upload it to Fabric for us. Custom Jira Step: Another tool developed by Peter , this step takes the Build Number and adds it to JIRA.  We used to do this manually (and of course we’d frequently forget about it). Send a Slack message: finally we send a message to our team’s Slack channel saying the build has started. This move to Bitrise yielded a dramatic time savings for our team because now builds and deploys could happen without developer intervention.  If someone wanted a build, they could just go to Bitrise and hit a button.  Bitrise would pull from our develop branch and things would hum along magically. It was after a few weeks using Bitrise that we got an idea, did someone really need to log into Bitrise to trigger a build?  Couldn’t we make it easier? Nearing the Promised Land - Rosie closes the circle InVision uses Slack in our day to day communications and some of our incredible engineers have configured an instance of Hubot (we call it Rosie) to manage many of our chatops functions, including web deployments.  The workflow is amazing - teams can issue commands to Rosie in Slack and she merges branches, handles reverts, deploys code, and reports back the status. If you remember from Step 8 in our Bitrise workflow, we have messages already integrated into Slack that give us alerts to the progress of builds.  The next logical step was duplicating what Rosie did for web deploys, and as it turns out Bitrise was already able to handle what we wanted. Any of the Bitrise workflows can be executed via a curl command, here’s a sample of what one looks like: $ curl https://www.bitrise.io/app/somethingunique/build/start.json --data '{\"hook_info\":{\"type\":\"bitrise\",\"api_token\":\"yourtoken\"},\"build_params\":{\"branch\":\"yourbranch\"}}' The fantastic detail here is that the command can take a branch which Bitrise will use in Step 2 mentioned above.  They also accept multiple build_params , which we used for passing in the build notes from the latest commits.  We can also set the specific workflow that we want to use. With that piece of the puzzle already in place, it was straight forward to extend Rosie (Hubot) to handle commands in our Slack channel that would execute the necessary curl commands to Bitrise. In our Bitrise utility class in Hubot we added code to manage the optional params and workflow: Bitrise . prototype . startWorkflow = function ( branch , workflow , notes ) { var _self = this ; branch = branch ? branch : this . defaultBranch ; workflow = workflow ? workflow : this . defaultWorkflow ; var bitriseData = { hook_info : { type : \"bitrise\" , api_token : process . env . BITRISE_API_TOKEN }, build_params : { branch : branch , workflow_id : workflow } }; if ( notes ) { bitriseData [ \"build_params\" ]. environments = [ { mapped_to : \"BUILD_NOTES\" , value : notes } ]; } return _self . sendCommand ( bitriseData ); }; In this BUILD_NOTES maps to a custom environment variable that we have set up in Bitrise.  What it does is replace any occurrence of the variable with that value. Self Service Build Generation - Putting it all together With all of this in place, with just a short slack command anyone in our channel can start a mobile build that auto deploys to Fabric off a specific branch automagically. Behind the scenes here’s all that’s happening: Rosie is listening for that command, and she makes a REST request to Bitrise with the values we’ve set. Bitrise does the heavy lifting, making the build and shipping it off to Fabric. Once the build is complete and uploaded, Bitrise messages back into our Slack channel to let everyone know the build is completed. I hope this little history in our journey towards automation was useful to you.  If you have any comments or questions, please feel free to drop us a comment! Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-06-16"},
{"website": "Invision", "title": "Express Integration Testing with SuperTest", "author": ["JoshMatz"], "link": "https://engineering.invisionapp.com/post/express-integration-testing-supertest/", "abstract": "Put down that REST Client (or, gasp , the browser!) you’re using to test that API you’re developing and back away slowly! There’s a better way! With SuperTest there’s no need to verify your API by hand. Plus, using it gives you virtually free integration tests. Now you can test and code new features at the same time. I’ve recently used SuperTest to verify complex data access roles per user. If you’ve developed an API, you’ve probably had a requirement to do something similar. I might’ve picked up something like Advanced REST client . REST Clients like this can be great to check the status of an API endpoint in production, but they can quickly become tedious if you’re using as the primary means to develop your API. There was no way I’d be able to test data access accurately and often enough with a manual client. Luckily with SuperTest you gain advantages like: Storing and using tokens to programmatically switch between users Programatically resetting your test database Automated testing instead of manual verification SuperTest was a big help for my productivity and turned out to be a genuinely pleasant way to develop an API! Let’s go over how to quickly set it up for something like user registration and login. From this, you’ll be able to move into storing cookies or tokens to easily switch between sessions and test data access per user! App Setup SuperTest can be used with any server available on your local network (or the Internet), but it also has a super power: giving it an Express server directly. The server need not even be running! When developing, we often use tools like nodemon to automatically restart our node servers. Combine Express with SuperTest and instead of restarting your server you could automatically run your tests to verify your changes. Don’t want to run all your tests? Use Mocha’s .only specifier. There’s a lot of flexibility, we just have to set it up. (I’ve set up a GitHub project with the code if you’d like to skip the explanations.) Creating the project and installing dependencies To get started create a new directory and navigate into it: mkdir integration-tests && cd $_ Run npm init . When it asks for a test command, enter mocha '**/*.spec.js' . We’ll use this later. Feel free to answer other prompts however you’d prefer. Next, install the Express goodies: npm i express supertest mocha chai -s Excellent work! Establishing working tests We’re going to start with a barebones Express server just so we can verify our architecture works like we’re intending. To do this we’ll create our server and tests in separate files. Create your server file touch server.js and copy and paste this into it: var express = require ( 'express' ); var app = new express (); // Just to test our server is working app . get ( '/api' , function ( req , res ) { res . send ({ version : '1.0.0' }); }); module . exports = app ; The above code simply pulls in express, creates a new instance of it and gives us an endpoint to access: /api . You’ll note there’s no server being started. That’s because it’s unnecessary with SuperTest! Realistically you’ll use something like app.listen() and that will be A-OK to do, it won’t interfere with SuperTest. Next, create your tests file touch tests.spec.js and copy and paste this into it: var app = require ( './server' ); var chai = require ( 'chai' ); var request = require ( 'supertest' ); var expect = chai . expect ; describe ( 'API Tests' , function () { it ( 'should return version number' , function ( done ) { request ( app ) . get ( '/api' ) . end ( function ( err , res ) { expect ( res . body . version ). to . be . ok ; expect ( res . statusCode ). to . equal ( 200 ); done (); }); }); }); Fantastic job! Reviewing the above code, we see that we’re importing our server, Chai and SuperTest. SuperTest includes its own .expect() but I prefer Chai’s syntax. The code sets a group of API Tests and creates one test to check if the endpoint /api returns a version number. Note that the done() function is important to declare these asynchronous tests complete. Now, let’s see if it works. Run: npm test . You should get this: » npm test > integration-tests@1.0.0 test /Users/joshmatz/Projects/integration-tests\n> mocha '**/*.spec.js' API tests\n    ✓ should have return version number 1 passing ( 41ms ) Huzzah! We have tests verifying our API works. Next up? Adding some more complex testing. Advanced Testing So we have some integration tests being run against our newly created Express server. That’s fine, I guess. But let’s get a little more complex by adding some faux-authentication endpoints that validate parameters and return errors under different circumstances. To get more complex, let’s add the express-validator package. It requires the Express body-parser package, so we’ll install and save both: npm i express-validator bodyparser -s . And then add them to the top of the server.js , right below the Express import: var expressValidator = require ( 'express-validator' ); var bodyParser = require ( 'body-parser' ); Faux-authentication All right, to keep it simple — and not add silly complications like passwords — we’re going to store a list of users in memory in an array. This array will reset every time our server is started. We’ll create an /api/register endpoint that pushes a user into the array and we’ll create an /api/login endpoint that returns us an item from that array. We’ll require some validation on these endpoints to ensure a valid user is created upon registration and a valid user is being requested upon login. Let’s get started. First, copy and paste this code right below var app = new express(); in your server.js file: var users = []; // Required to get access to `req.body`. app . use ( bodyParser . json ()); // Connects expressValidator so it can transform the req object. app . use ( expressValidator ({ customValidators : { isExistingUser : function ( value ) { return !! users [ value ]; } } })); This code establishes the users array we’ll be using, connects the required bodyParser (importantly done before expressValidator ), and then connects expressValidator and creates a custom validator method to check if a user exists. Now that we’ve got those out of the way, let’s add our endpoints below the code we just added: app . post ( '/api/register' , function ( req , res ) { req . checkBody ({ name : { isAlpha : true , isLength : { options : [{ min : 2 , max : 50 }], errorMessage : 'Name must be between 2 and 50 characters.' }, errorMessage : 'Name must have only alphabetical characters.' } }); var errors = req . validationErrors (); if ( errors ) { return res . status ( 400 ). json ({ errors : errors }); } var userIndex = users . push ( req . body ) - 1 ; res . json ( users [ userIndex ]); }); app . post ( '/api/login' , function ( req , res ) { req . checkBody ({ userID : { isNumeric : true , isExistingUser : { errorMessage : 'That user does not exist.' }, errorMessage : 'Authentication requires a number.' } }); var errors = req . validationErrors (); if ( errors ) { return res . status ( 400 ). json ({ errors : errors }); } res . json ( users [ req . body . userID ]); }); To recap, we just added two endpoints: /api/register and /api/login . These two requests will verify that the req.body.userID is formatted how we need it. If it’s not formatted correctly, we’ll return which validation checks failed. If it is formatted correct, we’ll return the user. Next up are the tests to see if this code actually works. Open up your tests.spec.js file and add this below our version test: describe ( 'Registration Tests' , function () { it ( 'should return the user if the name is valid' , function ( done ) { request ( app ) . post ( '/api/register' ) . send ({ name : 'JoshMatz' }) . end ( function ( err , res ) { expect ( res . body . name ). to . be . equal ( 'JoshMatz' ); expect ( res . statusCode ). to . be . equal ( 200 ); done (); }); }); }); describe ( 'Login Tests' , function () { it ( 'should return the user if valid' , function ( done ) { request ( app ) . post ( '/api/login' ) . send ({ userID : 0 }) . end ( function ( err , res ) { expect ( res . body . name ). to . be . equal ( 'JoshMatz' ); expect ( res . statusCode ). to . be . equal ( 200 ); done (); }); }); }); This creates tests for our registration and login routes and verifies that the returned data is what we’d expect it to be. Now, when you run npm test , you should see something like this: » npm test > integration-tests@1.0.0 test /Users/joshmatz/Projects/InVision/integration-tests\n> mocha '**/*.spec.js' API tests\n    ✓ should return version number\n    Registration Tests\n      ✓ should return the user if the name is valid\n    Login Tests\n      ✓ should return the user if valid 3 passing ( 83ms ) Wrapping up That’s it! We’ve learned how to install SuperTest and connect it to Express so we test against our code quickly and efficiently. You might’ve noticed that we added validation but we never created tests for it. I’ll leave that up to you for a small code challenge. But, if you get stuck, I’ve included some tests for it in the GitHub repository . Happy testing! Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-06-27"},
{"website": "Invision", "title": "Exceptional Engineers", "author": ["Bjorn Freeman-Benson"], "link": "https://engineering.invisionapp.com/post/exceptional-engineers/", "abstract": "At InVision, we’re building an amazing company and delivering fantastic products that our customers love. It turns out that in order to build an amazing company with fantastic products, you need exceptional engineers. At InVision, we are growing our engineering organization by first hiring people with the skills, drive, and desire to build great tools for digital product design; and then by helping them increase their skills and effectiveness through coaching, mentoring, and training opportunities. So how do we identify exceptional engineers? In this post, I outline a set of common traits shared by exceptional engineers: Exceptional Engineers Technically Deep. Exceptional engineers are technically deep and technically broad. Even when working in a small area of the stack, the exceptional engineer understands how the whole stack works and she understands the whole process from idea to design to architecture to tests to coding to deployment to production. Exceptional engineers are always driven to follow a problem across all kinds of boundaries. Put another way, the exceptional engineer doesn’t wait for others to provide something: she finds a way, builds a way, finagles a way to satisfy her curiosity to know the full story. Driven by Results. Exceptional engineers are driven by results: they want to get their work in the hands of customers, today. The exceptional engineer prides himself on delivering great work that is right in line with the business priorities; his currency of merit is production code and thus he is driven to show his ideas in action rather than talk about what he could do in the future. The exceptional engineer doesn’t let organizational boundaries stop him from getting things done because of his bulldogged tenacity to deliver. His motto is “never give up, never surrender.” Fanatically Frugal. Exceptional engineers are deliberately frugal in their use of time and resources. The exceptional engineer knows that in a start-up, time is the most valuable resource, the only resource that we cannot get more of, and thus she doesn’t waste any of it. Biased to action instead of meetings, biased to quicker solutions instead of perfect ones, biased to technical solutions that are less complex (and thus faster), the exceptional engineer is always focused on how quickly and efficiently she can deliver results. Team Multipliers. Exceptional engineers make everyone better: they lift the skills and expertise of those around them. An exceptional engineer is someone that others want to work with because although he’s forceful about his opinions, he’s not a jerk about them and you go away from an interaction knowing something more than you did before. The exceptional engineer is a multiplier, not just adding his skills to the group but multiplying the value of everyone in the group through his positive, constructive, can-do attitude; he is a technical multiplier through frameworks and solutions that make the whole company better and faster; he is a group multiplier by making estimates and commitments because he knows that everything in interdependent. The exceptional engineer does not compromise for the sake of social cohesion but once a decision is made, he commits wholeheartedly. The exceptional engineer works well individually or in a team because he is flexible: he doesn’t have to be a star, and as a result, he often is one. Continuous Learners. Exceptional engineers are completely confident in their ability to learn anything and they are constantly doing so. The exceptional engineer is driven to self improvement and you will always find her learning something new. She is a voracious but discriminatory reader so that she balances continuous self-improvement against constantly jumping to the next shiny thing. The exceptional engineer is open to feedback and eminently coachable; she is constantly curious about all of the dimensions of a problem. Want the Hard Problems. Exceptional engineers want to take on the hardest, most important problems. The exceptional engineer wants to be working on challenging interconnected problems; if something is considered impossible, that’s where you will find the exceptional engineers gathered looking for solutions. The exceptional engineer wants to be given the degrees of freedom to make important decisions and he knows that those are found around the hard problems. Hard problems are often accompanied by many failures before success; he achieves the success because those failures really bug him, really get under his skin, and he can’t stop until he’s solved them. The exceptional engineer wants the hard problems that may take a long time (months, a year) to solve and he knows how to approach those problems incrementally until they are completed. Are Right, A Lot. Exceptional engineers are not perfect, they make mistakes like the rest of us, but they are right, not just more often than not, but a lot more often than not. The systems they build are robust, not brittle; flexible, not rigid; reliable, scalable, maintainable, and elegant. The exceptional engineer has strong business judgment and good instincts. She seeks diverse perspectives and works to disconfirm her beliefs so as to be right more often. She is opinionated but not dogmatic. As a result of being right a lot, she helps with convergence rather than creating divergence. Stay tuned for an upcoming blog post about how we provide useful feedback to our engineers to help them be exceptional! Some Further Reading On Being a Senior Engineer by John Allspaw Amazon’s Leadership Principles On Being a Senior Facebook Engineer by Pedram Keyani Solving a Mystery (an example of never giving up) by Nathan Bronson Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-06-30"},
{"website": "Invision", "title": "Retaining and Developing the Best through Evaluative Feedback", "author": ["Jeremy Wight"], "link": "https://engineering.invisionapp.com/post/developing-the-best/", "abstract": "In the fast-paced startup world the mantra is most often: more, better, faster . And while that statement is a bit tongue-in-cheek, it is often not far from the truth and in some cases for good reason. With the great many factors fighting against business (specifically startup) success, it’s no wonder that insightful, evaluative feedback is very often one of the first things  cut out in the constant search for more time. This has been my experience at many fast paced companies, but I believe critical feedback is one of the most crucial aspects of retaining and developing the best talent in any industry . At InVision we seek to hire and develop the absolute best, and while I have worked other places that have stated that as a mission, InVision is the first place which I have worked that has embraced the values necessary to achieve it. The Best (Engineers or otherwise) want to know specifically where they are doing well, where they need to continue to press into, and where they need to improve to grow. In most companies this type of feedback typically only occurs when someone is doing poorly and needs to improve to stay employed, or in some cases not at all. Due to the negative focus inherent in this methodology (whether explicitly or implicitly chosen) this type of environment does not cultivate a culture that the best are drawn to. Tips for retaining and developing the Best Whether you are in an organization which already executes on evaluative feedback and looking to get better, or are in a company that never seems to find the time; Below are some tips which can help you to shape your culture to attract, retain and develop the best: Don’t wait for the organization to start This may seem obvious, but many times I’ve seen people waiting for someone else to give them permission, or criteria, or a timeline, or… The point is, make a way. If your organization doesn’t have any timeline or expectation set for giving feedback, schedule an informal 1-on-1 with a teammate. If you’re in an environment that won’t seem to give you the time to do it, give him/her a heads up that you would like to share some feedback, and then take them out to coffee or lunch. The point is, in a fast paced environment, you just have to make some space for giving feedback. Leverage shared values Not everyone will agree with the feedback that you give regardless of how you are viewed, to minimize differences in expectations it’s best to start with shared values. If your organization or department has stated values that you feel that you can relate to specific actions, then it’s best to start with those. At InVision we have shared values which describe the heuristics of Exceptional Engineers . This is a list of 7 values which help to guide our actions. Everyone in our department is aware of these, and as a manager I leverage these values as a guide to help give specific feedback to my team. Since everyone in our department is aware of these values it makes evaluations fair and clear. It’s fair because we are all being judged based on the same defined criteria and it’s clear because it is stated in black and white rather than some vague or ever changing criteria in someones head. When clear criteria are established it creates an environment where evaluations are more about understanding the engineers experience, goals and perceiving where their activities over the past several weeks have helped them to converge and expand upon these values. This is much more useful feedback than a vague “You’re doing great!” or “You need to step it up.”, and because you are leveraging clearly stated values, it takes much of the uncertainty out of the conversation. If your organization doesn’t yet have clearly established shared values which you can leverage, start reviewing blogs on leadership and management and socializing them amongst your team. I find this to be a huge trust building activity and a great spring-board to evaluative feedback, in any case I can almost guarantee it will spark some great discussions. Make it specific When sharing feedback, make it specific. Vague feedback, much like unclear expectations continues to leave the receiver in a state of uncertainty. To give specific feedback requires more attention as a manager. It means that you have to be constantly thinking about your values and watching for actions and behaviors which exemplify those, and call them out. In my experience it’s best to address these positive and negative actions and behaviors as close to the time of enacting as possible. This requires that you have the trust of those that you lead (and that you show some tact), but when applied properly it allows you to reinforce those behaviors while they are still fresh in the mind. I find that when you give timely reinforcement like this, it’s like giving the somewhat opaque values mental coat hooks to hang those behaviors and frame future ones off of. In addition to calling these behaviors out close to the action, keep a document with them for your next feedback session. This will help you to reinforce the values and behaviors as you give specific feedback some time after the action has occurred. When it’s reinforcing positive behaviors it also serves as an opportunity to recognize and celebrate the growth of your team member since the last evaluation. Incorporate self evaluation A great tool is asking your team mate to self evaluate based on your shared values before you share. This is often a very revealing exercise to whether the individual is self aware, and in most cases allows them to introduce areas of weakness in a safe way. By allowing them the opportunity to self evaluate, they become collaborators in the process adding to the trust and feeling of fairness in the evaluation. Also, since many engineers are hyper critical of themselves it gives you the opportunity to point them to their strengths and celebrate areas of growth, all while making the areas of opportunity easier to bring up. Make it happen In the startup life there are ebbs and flows, but more often than not we are all one alert or call away from another fire. No one is going to make this happen for you, so you have to make it happen. If you work in an organization like ours which supports this type of critical feedback it makes it much easier, but even still there is always the pull of the pressing activity over ones such as this which take longer but which bear significant fruit. So keep at it! Don’t allow the short-term positive impact, or the social discomfort of a feedback session keep you from doing it consistently. Make it a recurring meeting every 4-6 weeks and set the expectation with your team so they know it’s coming. When it’s happening frequently, it takes most of the nerves out of it, and in-time it can turn out to be one of the most rewarding and enjoyable parts of the job. Breach the awkwardness and make evaluative feedback happen to add to the culture of retaining and developing the best! Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-07-07"},
{"website": "Invision", "title": "Tidier Commits using Git --patch", "author": ["DarrellBanks"], "link": "https://engineering.invisionapp.com/post/tidier-commmits-using-git-patch-mode/", "abstract": "Git is one of the most powerful tools in a software workflow, and knowing tricks to bend it to your will can help you to be more efficient and keep your repository better organized. It has an overwhelming amount of tools under the hood, and today we’ll be looking at the enormously useful --patch mode to see how it’s often a better option than a plain old git add . A Simple Example Let’s start by creating a simple test repository containing a few text files. mkdir ./patch-mode && cd ./patch-mode\ngit init . echo \"Superman\" >> heroes echo \"Wonder Woman\" >> heroes echo \"Spider Man\" >> heroes echo \"Iron Man\" >> heroes echo \"R2D2\" >> robots echo \"Johnny #5\" >> robots echo \"Kitt\" >> robots Because these are brand new files, we’ll go ahead and run an initial git add with the special -N flag to let git know that we intend to add these files later, but for now, to act as if these files were already staged. git add -N heroes robots Let’s stop and have a look at the diff to see what our changes look like git diff diff --git a/heroes b/heroes\nindex e69de29..81533d0 100644 --- a/heroes\n+++ b/heroes\n@@ -0,0 +1,3 @@\n+Superman\n+Wonder Woman\n+Spider Man\n+Iron Man\ndiff --git a/robots b/robots\nindex e69de29..3572eed 100644 --- a/robots\n+++ b/robots\n@@ -0,0 +1,3 @@\n+R2D2\n+Johnny #5 +Kitt Exactly what we’d expect. We see the addition of four heroes and three robots. At this point we could git add -a , git add . , or git add heroes robots to stage all of the changes at one time. Sometimes, though, you’ve made changes elsewhere that you might not want to commit, and any of those commands would cause you to unintentionally stage those changes. Let’s see if we can avoid that. The –patch flag git add --patch diff --git a/heroes b/heroes\nindex e69de29..cf1fcb8 100644 --- a/heroes\n+++ b/heroes\n@@ -0,0 +1,4 @@\n+Superman\n+Wonder Woman\n+Spider Man\n+Iron Man\nStage this hunk [ y,n,q,a,d,/,e,? ] ? Running git add --patch is showing us the first hunk of changes, our heroes file with four additions. It’s also giving us a list of options for staging the hunk; [y,n,q,a,d,/,e,?] . The list shows the options available to us given our current hunk, but you can type ? and hit [Enter] to get a better understanding of all of the options. Our available options are: y - stage this hunk n - do not stage this hunk q - quit; do not stage this hunk or any of the remaining ones a - stage this hunk and all later hunks in the file d - do not stage this hunk or any of the later hunks in the file / - search for a hunk matching the given regex e - manually edit the current hunk ? - print help This time, it’s a good thing we opted for --patch ! We haven’t decided if Iron Man is a hero. I mean, he doesn’t have any true super powers after all. Let’s not include him in our list, at least for this commit. To do that, we’ll choose e to manually edit this hunk. Keep in mind we’re not editing the actual file, just the diff to be applied to this hunk. e [ Enter ] # Manual hunk edit mode -- see bottom for a quick guide @@ -0,0 +1,4 @@\n+Superman\n+Wonder Woman\n+Spider Man\n+Iron Man # --- # To remove '-' lines, make them ' ' lines (context). # To remove '+' lines, delete them. # Lines starting with # will be removed. # # If the patch applies cleanly, the edited hunk will immediately be # marked for staging. If it does not apply cleanly, you will be given # an opportunity to edit again. If all lines of the hunk are removed, # then the edit is aborted and the hunk is left unchanged. We now have the ability to move about freely in the hunk and modify it according to the instructions in the comment at the bottom. In our case, we’d like to remove a ‘+’ line from the hunk. Namely, the Iron Man addition. Removing that line should result in the following hunk # Manual hunk edit mode -- see bottom for a quick guide @@ -0,0 +1,4 @@\n+Superman\n+Wonder Woman\n+Spider Man # --- # To remove '-' lines, make ... Again, we’re not telling git that we want to delete “Iron Man” from the heroes file, just that we don’t want to commit that specific piece of the hunk right now. Save the hunk with :wq and git will move on to the next hunk of changes. diff --git a/robots b/robots\nindex e69de29..3572eed 100644 --- a/robots\n+++ b/robots\n@@ -0,0 +1,3 @@\n+R2D2\n+Johnny #5 +Kitt\nStage this hunk [ y,n,q,a,d,/,e,? ] ? These changes look good! Let’s choose y to let git know that we’d like to stage this hunk of changes as is. y [ Enter ] At this point, there are no more changes to be added and git will exit the interactive --patch mode. If we take a look at git status , we’ll see that new heroes file is staged for commit and that it has modifications that are not staged for commit: On branch master\n\nInitial commit\n\nChanges to be committed:\n        new file:   heroes\n        new file:   robots\n\nChanges not staged for commit:\n        modified:   heroes Hopfully it’s clear that this is happening because the heroes file still contains our “Iron Man” line, and that line was not staged for commit thanks to us using the interactive --patch mode. Running git diff will confirm that that is indeed the change in question: git diff diff --git a/heroes b/heroes\nindex 81533d0..cf1fcb8 100644 --- a/heroes\n+++ b/heroes\n@@ -1,3 +1,4 @@\n Superman\n Wonder Woman\n Spider Man\n+Iron Man That’s It! Using --patch mode effectively is a great way to keep your commits logical and narrow. I’ve found that it also helps to force me to take a closer look at all of my changes to ensure I’m only commiting code that should be commited. Take a moment to get comfortable using it in your workflow and it will pay great dividends in the long run with regard to your commit history. Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-07-14"},
{"website": "Invision", "title": "GopherCon 2016 Highlights", "author": ["Daniel Selans"], "link": "https://engineering.invisionapp.com/post/gophercon-2016/", "abstract": "At InVision, we are huge fans – and avid users – of Golang. We love its simplicity, low overhead, excellent concurrency model, and its well thought out standard libraries.  And, to be honest, it’s just a lot of fun to write code in Go. As such huge fans of Go, and as a shop that likes to stay on the cutting edge, we were excited to be able to attend GopherCon 2016 this year.  It was an opportunity to get a pulse on the general ecosystem and to hear talks from some of the brightest minds in the industry.  I was fortunate to be able to attend with my friends and colleagues Jesse Dearing and Jon Dowdle. There were a ton of great talks this year.  While I wish we could have attended all of them, the ones we were able to attend were fantastic.  In this blog post, Jon, Jesse, and I highlight some of our favorite talks and some key takeaways. This year there were a number of high-level talks that touched on topics such as the internals of nil and map , work that went into the Go assembler and the origin/inspiration for goroutines and channels. In addition, there were a number of practical talks that should improve the skills of even seasoned Go developers. Of note is the “Practical Advice for Library Authors” talk presented by Jack Lindamood, who provided a number of great insights about best practices for writing libraries. Some of the advice served as acknowledgement that we’ve been “doing things the right way” (passing in *Config structs to constructors versus multiple params), while in some cases, things that we could probably improve upon (avoiding vendoring in libs; avoiding goroutine and channel usage in libs). Rob Pike’s “The Design of the Go Assembler” was another interesting talk which dove into the details of the Go compilation tool chain and shed light on the process involved in introducing additional architecture support to Golang. We learned that the “abstracted” assembly language used by the Go compiler, allows one to introduce new architecture support much faster than ever before. You can read more about the Go assembler here . Most importantly though, in a talk provided by Renee French, we learned about the history of the Go gopher and how exactly he is able to hold things with nubs (hint: var der Waals’ forces). Finally, it was refreshing to see that the GopherCon organizers really care about representing the diverse crowd that attends. One comment we heard from the conference organizers was about an engineer who was typically asked if she was working the booth, whereas here she was asked about her github handle. It is great to see more inclusiveness! Needless to say, the conference was a major success - we are eager to see the language continue evolving and meeting fellow gophers next year at GopherCon! Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-07-22"},
{"website": "Invision", "title": "Making remote working work", "author": ["Matt Borgato"], "link": "https://engineering.invisionapp.com/post/making-remote-working-work/", "abstract": "Remote working is a fast-growing trend in the modern workforce with many benefits such as organizational commitment, increased job satisfaction, better job performance, and lower work stress and exhaustion. I believe, along with many others, that this is simply awesome! It’s a remarkable opportunity for us — programmers from all over the world — to collaborate and work together on interesting and challenging projects with amazing and talented people. Working from home – or anywhere! Unlike the traditional office, we are not bound to a single physical location. We can work from home or wherever there’s a decent Internet connection. Away from the noise of the office and stress of commuting, in our hometowns or even in exotic locations. I joined the company more than a year ago, and so far, I’ve traveled and lived in six countries and two continents. I was able to do a road trip in Italy, marry my wonderful wife in Brazil, and meet with my friends in the Netherlands and United Kingdom. I love it. Making it work You may have worked at companies where they allow remote working.  Or even at a small company where most people are remote.  While this new way of collaboration seems quite doable for small companies or tiny groups of people, can you imagine to teleworking with dozens or hundreds of other people ? At InVision, we strongly believe that this is possible. In fact, we’re doing it every day!  As you may already know, we are entirely distributed . Everybody works remotely across 14 countries. It has even been proven that those who work remotely tend to be less distracted and more productive (See WFH experiment at CTrip - Standford University ).  However, doing this effectively is not without its challenges. We have found several strategies and tools for working efficiently together. Here I share some of the techniques we’ve discovered to make it work: Communication Small, connected teams Making the virtual more like the physical Recognition - every little bit helps Communication It’s both the biggest obstacle and the solution to developing trust within remote teams. As Avi Posluns — our Director of Employee Hapiness says: “When working remote, there is almost no such thing as over communication. As human beings, so much of what we know about each other is not learned through direct communication.  Body language, a picture on someone’s desk, and the way someone dresses, are just examples of how we learn about each other, both personally and professionally, in an office environment. The remote setup creates a need to communicate so much more in either written or verbal form.  When working remote, always err on the side of more communication than less.\" We use Slack to communicate. It serves as our building, our virtual office. We treat its channels like physical rooms.  Even though we have many channels, we join only a few at a time. This way no room is overcrowded and we are not overwhelmed with too much noise. Our communications are effective and flow rapidly with ease. Small, connected teams We found that the best way to make a team as flexible and lean as possible, is to keep it small . Internally, we formed tiny groups that work on different projects. Each of them is like a small start-up; a self-contained system. For example, I am member of the Red Team, which is made up of four Engineers (one specialized in QA), one Engineering Manager, one Product Manager, and one Designer. We are able to minimize the number of communication points necessary to deliver great things. This is possible because every project team is a different entity, decoupled from the other teams. However, if by any chance, we do need some help from another team, we just jump in their Slack channel and ask. Easy and simple. This approach promotes a fantastic synergy between teammates and other teams. Making the virtual more like the physical Like any typical start-up, we have our daily stand-up.  Only ours is over Google Hangouts .  This helps keep us connected. Also, in order to keep each team engaged and informed on everybody’s progress, on Fridays we post a short show-and-tell demo on Slack. We call them: demo bits .\nIt’s a great way to demostrate what has been done during the week and get feedback. When I used to work in office, I drew a lot on the whiteboard. I found it really expressive and useful, especially when analyzing complex subjects.  It’s something I can’t really live without as an engineer. One of our tools — liveshare — is exactly a virtual whiteboard, where every collaborator can write and draw on images or plain backgrounds. I find it extremely handy. Recognition - Every little bit helps Praise and recognition is something that can be easily forgotten when working remotely. We use Bonus.ly to reward and motivate each other. The concept is simple, yet really effective and it helps incentivize positive communication such as meaningful praise and recognition. Every time a peer helps me to resolve an impediment, find a bug, or fix an issue, I can reward her/him with a few dollars. The amount of money isn’t so important, but it’s a really good way to show gratitude and appreciation. Conclusion These are just some of the practices that we have decided to follow, to make our distributed team work at its best. Of course, there is still considerable room to improve. Every day we figure out new strategies to collaborate efficiently, and new ways to make our virtual office a fantastic place to work.  If you have any tips and tricks, please share them in the discussion area below! Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-07-29"},
{"website": "Invision", "title": "How ESLint Saved Our Soul", "author": ["Kevin Lamping"], "link": "https://engineering.invisionapp.com/post/how-eslint-saved-our-soul/", "abstract": "I admit, it’s not on my list of “great times” to hear that the code style you’ve spent weeks defining will require “having to suck the soul out of the codebase”. But truthfully, it was a breath of fresh air. Okay, maybe the messaging could have been less dramatic. Maybe less “Fire and brimstone”. But we’ve all been there, right? A new person joins a team and starts changing the way things have been done for what seems like no good reason at all. While they may not be huge changes, they can feel like the end of the world. That’s the difficulty: many of our judgements come from an emotional response, and it’s hard to quantify those feelings. We know there’s a reason for our discomfort, but can’t always describe why. It’s tough to speak up when we’re not able to objectively state our hesitation, and sometimes we end up stating things a little more dramatically than we intended. So I knew my co-worker had just taken a big risk by expressing concerns about “the soul” of the project. I had to respect that risk by looking past the specific delivery of the message. Nothing in our collective past indicated malice behind the meaning (I’m fortunate to work with some great engineers), so it was only reasonable to assume good intentions. With that in mind, I indulged my co-worker. What did they mean by their words? Did they dislike the entire concept of linting, or just a specific implementation of the rules? I didn’t focus on the emotional response, but rather took it as a sign that something wasn’t quite right with the setup we came up with. Tabs vs. Spaces; Single Quotes vs. Double; Blue bikeshed vs Red. It turns out, they weren’t crazy about the choice to use tabs instead of spaces and double quotes instead of single. Honestly, it wasn’t my personal preference either. Most of my career had been spent using two spaces for indention, and I often found myself replacing single quotes due to muscle memory when typing. It wasn’t the style I personally wanted, but for legacy code reasons, tabs and double quotes were the choices we made. I knew that switching everyone to spaces wasn’t the answer. No matter the decision, there would always be someone who preferred it one way over the other. And if we did switch over, we’d have to update hundreds of lines of stable code just to use a different indention, introducing risk of breaking production due to some weird error. Thankfully, ESLint is perfectly set up for this situation. When you import a common configuration, you can add individual overrides as needed. Here’s an example .eslintrc file: { \"extends\" : \"eslint-config-invision\" , \"rules\" : { \"quotes\" : [ 1 , \"single\" , \"avoid-escape\" ], \"no-use-before-define\" : 0 , \"indent\" : [ \"error\" , 2 ] } } The majority of rules follow the global pattern, but for individual instances where a team prefers something special, they can have their specific configuration. This ensures the project team is 99% compliant, rather than 0%. And really, that 1% matters as much as the color of a bikeshed . Seriously, tabs vs. spaces is a minor detail compared to code architecture and overall structure. With this in mind, we created a hierarchy for style compliance: New common code would strictly follow the company standard Legacy code would follow the company standard, with exceptions made when updating files isn’t worth the risk. Individual project teams reserve the right to modify any style they choose, so long as they’re the sole owners of the code and they accept the “risk of non-compliance”. Opening Up Discussion Along with individually meeting teams through lunch and learns, we also worked on building a community across the company through an internal ESLint Slack channel. This channel is open to anyone at InVision to comment or question the configs, or just to say that something isn’t working for them (or that it is). Additionally, we’ve published our configuration as an open-source package on NPM. If you have an opinion on code style, check ours out . Feedback is welcome (although we can’t promise we’ll change it for you). You’re always welcome to roll your own though. Taking Time It’s been over a year since we switched from duplicating the same .jshintrc and .jscsrc files in our repos to standardizing on a common ESLint config. I’d love to say it was an overnight success, but it took time. Time to make the initial choice, then time to work those choices in with reality. It also took time to get feedback. People were quiet at first. But by staying open to reactions after the implementation, we allowed people time to experience how the new configs worked and felt to them. Many times things look great on paper, but until you actually experience them, you don’t really know how it works. The Soft Side Matters It’s easy to focus on the hard evidence of a codebase. Is it well maintained? Does it have too many bugs? Is documentation accurate? But a large majority of the success of a project (and team) comes down to the softer, more human side of the codebase. Is opposition encouraged? Are contrary viewpoints welcomed? Can folks change little things they don’t like, so that they can achieve bigger goals together? Our goal is to promote a healthy project ecosystem. To do that, we have to realize that sometimes only part of an argument is the actual truth. The rest is just needing to be heard. We focus on the truth, work on the compromise and understand that even though it sometimes requires concession, it’s for a better result in the end. When that co-worker spoke out against the new lint rules, I had to take a step back and view the situation a little more empathetically. By digging in to what they really meant, we were able to come to a great compromise, which avoided weeks of disagreeing about which style is better. By using a tool that’s open to this flexibility, we were able to adapt to each project’s desires. In your work you’ll rarely get it perfect the first try. This is true for both the technical and communication aspects of projects. Know this, stay open to feedback, search for dissidence, and with it, make the “soul-saving” choice in the end. Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-08-12"},
{"website": "Invision", "title": "Passwords don't have to be hard", "author": ["Tracy Reed"], "link": "https://engineering.invisionapp.com/post/choosing-good-passwords/", "abstract": "There sure seem to be a lot of people and organizations getting their emails hacked lately, aren’t there? While they all love to blame “sophisticated hacking techniques” wielded by “powerful nation-states” I’m going to let you in on a little security secret: Many of these hacks happen at least in part due to poorly chosen passwords. So simple. So embarrassing! Better to play it up as if it was inevitable, unavoidable, like it basically came down to force majeure, right? But yes, it often comes down to something as simple as poorly chosen passwords. Sure, two factor auth is great and always recommended but it isn’t available with most services. And experience has shown that we over-estimate how difficult it would be to guess our password. I know, you’ve all heard about passwords and the necessity of picking good ones for years. But I’m going to overturn some conventional wisdom and suggest to you a way of not becoming a victim to password (or “Security question” answer!) guessing. We all know we need to have a long ugly random password to make it hard to guess. But we can’t remember such things and they are a pain to type in. It’s so tempting to choose something simple or to re-use a secure password on many sites. And ever since passwords were first created and then promptly snooped from the post-it note under the keyboard we have been told to never write down our password. I still see this password advice given out regularly. This advice is not only wrong but it is now counter-productive. This advice comes from back before the Internet connected all of our computers together. The biggest threat to your password back then actually was someone snooping around your desk and finding it written down. Now that your account is online and every miscreant in the world can constantly guess at your passwords things are a lot different. In risk-management speak we say the “threat model” has changed. You must write down your password if it is a good and unique one, as it must be. But it is best to do it in a particular way. The best way to do it is to use a password manager and let it generate and “write down” (aka save) the passwords for you. I’m partial to LastPass but there are several good ones. A good password manager will generate very strong passwords for you, prevent you from re-using the same password on multiple sites, keep track of the site they belong to, automatically fill them in for you (so you don’t have to type in those ugly passwords), and keep them encrypted so that only you can access them and nobody else ever has access to the unencrypted passwords. A good password manager will not only make it safer but easier to use your passwords such that you will actually prefer to use it rather than choosing weak but memorizeable passwords or re-using the same passwords on multiple sites (which we all know is a very bad idea). But we seem to have a chicken and egg problem: your password manager requires a password to encrypt all of those other passwords! Yes, it’s true: A master password. So now what? Generate one of those ugly hard to remember passwords either randomly (your password manager can do this) or potentially use another method which is more memorizeable but still reasonably secure. I like the first letter of each word from a line or two which you can remember from your favorite book. “From now on the enemy is stronger than you. From now on you are always about to lose.” (from “Ender’s Game”) becomes “Fnoteisty.Fnoyaaa2l.” which makes for a  secure and memorizeable password which can be typed in quickly with just a small amount of practice. You could probably even get by with something somewhat shorter but I recommend at least 12 characters. You’ll only ever need to type this in when you first fire up your computer and the password manager starts. And because the chance of exposure of this password is so low (it never leaves your computer) you will very rarely have to change it. But even that password is far from trivial to remember. What if you forget that one? You would lose access to all of your other saved passwords! The solution: Write it down! Yes, physically with pen and ink. Famous information security guru Bruce Schneier tells us to write down such passwords and then protect them just like we protect our cash. If you really have to it’s even ok to carry it written on a slip of paper in your wallet next to all of those other slips of paper with pictures of dead presidents. It’s still safer than not using a password manager at all. If you have a small safe at home write the master password down and stick it in there. Wherever you would be comfortable keeping your cash will do. How about those security questions like “What’s your mother’s maiden name?” or “What’s the name of your first pet?” that are often used as password recovery questions? The real problem with such questions is that a lot of this sort of information is publicly available. And plenty of people have been compromised in this way. So I typically don’t answer these questions truthfully unless somehow absolutely required to. In most cases it makes no difference what you answer as long as you can come up with the same answer later. So I have created an alter-ego with made up answers to these questions. Nobody else knows these answers. And I use the “Secure Notes” feature of LastPass to write it all down and keep my story straight. I’ve got an alternate high school mascott,  first dog’s name, street I grew up on, etc. Whenever a new security question comes up I just make up a new answer and add it to the list. And my password manager’s note keeping feature keeps it safely encrypted for me to look up should I ever need it. For an excellent explanation of how passwords are cracked and how to choose good ones check out these videos: Password Cracking - Computerphile Password Choice - Computerphile These are produced by Brady Haran who has a number of other very interesting YouTube channels which I suggest you check out, especially if you are the sort of person who likes to know things. He does such excellent work that he recently earned an honorary doctorate for his YouTube videos! Security doesn’t have to be hard. Using a password manager actually makes your life easier compared to what you are probably doing now. Consider the added security a bonus. Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-08-19"},
{"website": "Invision", "title": "Beyond hangouts: InVision’s first engineering offsite", "author": ["Heather Roberts"], "link": "https://engineering.invisionapp.com/post/red_blue_off-site/", "abstract": "Working at InVision has been my introduction to full time remote employment. While I had telecommuted before, it had always been a day here and there, or maybe a week if I was sick and didn’t want to spread cooties around the office. It’s been over a year now and I have fallen completely in love with working from home. Although, it’s not just working with my cat in my lap that makes it enjoyable. It’s that it affords me the opportunity to work with talented professionals all over the world. InVisions’ teams are named by color and I’m the QA Engineer on the Red team. I reside in Washington state while my teammates hail from Montana, Arkansas, Oklahoma, and Italy. More often than not, I forget that we’re in different locations. So far, using Slack, email and Google Hangouts to communicate is completely effective when you work with dedicated people. When I heard I was going to travel to Portland to meet my teammates in person, I was thrilled for two reasons. One, I wanted to see these people below the neck. Two, It felt amazing to know the team’s working relationship was important enough for InVision to plan this off-site. Team Building Actually Works I had come to the off-site with a challenging set of automation scripts to work on. I knew I needed some help to streamline them and I wanted to take advantage of having everyone together. The itinerary allotted for an hour to discuss automation so I launched into the issues I was having, hoping I could get through most of it. After a few minutes, the entire Red Team pulled up chairs to sit with me, giving me troubleshooting ideas, suggestions and other feedback I know I never would have gotten on Google Hangouts. I was almost giddy with how quickly we were able to get past some of the blockages I had. An hour later we had made serious progress but hadn’t quite gotten all of it. I figured I’d work through the rest when I got home but my teammates were having none of that. They decided to forgo the set agenda and continue with me until one particular problematic test set was running like Usain Bolt. We’d make some changes, run the tests and groan in unison at a failure. When something passed, we’d cheer and throw our arms in the air. Working with them in person was really fun! In that moment, I felt a sense of camaraderie I’d never felt with this team before. I remember smiling to myself at one point, realizing that just a few hours into the trip I was already reaping the benefits. Direct Messages - Now Guilt Free! Over the last year there were times I felt a little uneasy sending Slack direct messages, for fear of bothering people. In an office environment you can walk by someone’s office to see if they are busy, but when you work from home you never really know what your co-workers are doing. My concern was that someone would be ‘in the zone’ on a project and my message would pop up, throwing off their concentration completely. When I came together with my team and we were able to talk to each other from a few feet away, that anxiety evaporated and I felt silly for even having it. We spent a lot of time wandering the streets of Portland, drinking coffee, eating, laughing and getting to know each other. I came home feeling closer to my team than ever. I no longer feel shy about sending a message to them, or anyone at InVision. We’re all in this together and we all want the same thing - the best bug-free product on the market. In Conclusion The trip to Portland totally changed the dynamic within the team for me. I feel more connected to my boss and co-workers and more confident in myself. I’m thankful that InVision understands the importance of team companionship and was willing to give us this opportunity. There is something about sharing a meal or just walking to get coffee that helps people bond. I also can’t thank my teammates enough for their time and assistance during the trip. I am blessed to work with such great people at such a great company. Overall, it was a fantastic off-site and I hope we can do it again sometime! It didn’t change my view on working from home though. Being able to attend meetings with a purring kitten in my lap is pretty hard to beat. Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-08-26"},
{"website": "Invision", "title": "Refactoring Permissions - Part 1", "author": ["Brian Kotch"], "link": "https://engineering.invisionapp.com/post/refactoring-permissions/", "abstract": "Welcome to Thunderdome … (refactoring important logic at scale) Permissions affect every aspect of an application with users who have varying rights. This three part series outlines the process and the techniques we used to pull off a hat trick of decoupling crucial business logic and reducing the overall complexity of our application. Part 1 - Refactoring Permissions Part 2 - Probing and Comparing your Results Part 3 - The lessons we learned the hard way Why refactor permissions anyway? “Can a user do an action to an object?” - that question lies at the intersection of security and user action. A wrong answer can create a security hole. A wrong answer can deny a user the rights to do something. A right answer goes unnoticed and unloved. A right answer just allows the user to go on their merry way. It doesn’t get recorded in the logs. It’s a game of screwing up or maintaining the status quo. This situation becomes tenuous with millions of users with varying permissions. Add in a robust security model with different levels of user rights.  Add in different types of organizations or settings and you have a scary problem. Now, screwing up irritates a lot of people. Screwing up creates security holes that affect millions of records. We won’t even mention audit logs - tracking down why user 12345 had the ability to delete a record. Getting right just keeps things churning along. It’s no longer about what action a user can do to an object. It’s about what kinds of users can do what kind of action to objects. All those roles, flags, options become a puzzle. What if a user is a manager but we downgraded their account to read-only status? What if there is an arcane flag buried in a hidden menu that only one person in support knows what it does? You need to be insane, brave or tenacious to refactor it. In an ideal world, we would have centralized it from the get-go. We would have a centralized api. We would have locked up our business rules into one place. We would have discrete functions responsible for calculating our rights. In the real world, permissions tend to grow with iterations of the product. We build our products with minimal architecture and focus on getting stuff done. As we build to the feature set or users want, the features and allowable actions change. Two years later, you look at your entire code base and see permissions everywhere. A naive example If an application’s permissions develop with each iteration, they grow like kudzu. It starts off with the same clean innocence of all new development projects. It starts off with the user object. That user object contains some logic like this: User is an object containing all the information about a user. Transaction are the entities we care about in our hypothetical scenario. * //Check to see if the user is an admin. if ( user . isAdmin ) { delete ( transaction ) } Later, a business rule adds managers. They are a lot like administrators, but they don’t quite have the same level of access. For our scenario, they lack the ability to delete special transactions. Here’s a few naive examples to illustrate code we have all encountered at some point in our career. //Now, admins and managers can edit because Product wants to empower managers if ( user . isAdmin || user . isManager ) { edit ( object ); } //Managers still can't delete though. if ( user . isAdmin || ! user . isManager ) { delete ( object ); } Getting more complicated Assume success. Assume millions of users with lots of objects and lots of potential actions. Now, assume we need to change that logic. What if our manager cannot delete transactions in certain scenarios? What if they can in others?  One possible solution is to garden our kudzu and make the changes in all those places … . Maybe: //Still can edit if ( user . isAdmin || user . isManager ){ edit ( object ); } //Only admin or a manager with the special privileges flag if ( user . isAdmin || ! ( user . isManager && user . isSpecial ) { delete ( object ); } Seeing the light Brothers and sisters, there is a better way. We centralize and decouple. We make things a squinch more efficient. This magical panacea of all issues at scale works wonders here. We set up a RESTful api to return buckets of business objects ids. For each type of business object, we have buckets derived from the individual actions. In each of those buckets, we have the ids of the individual objects. A user can do an action to an business object if that id is in the bucket. Let your microservice do all the calculations and let your modules consume it. At the end of the day, a developer should only need to ask: http://permissions.application.com/v1/:userID The consumer gets back the barest possible information need to answer the question. We’re optimizing for speed here. IDs are small. Arrays are fast. Buckets provide the ability to lookup through hashtables the exact nodes. We develop a two pass system for figuring out permissions. We can make it fast because we only care about small bits of information - what ids go in which bucket. We are free of the heavy lifting of returning all the user’s data.  We can build out the permissions and only return the barest amounts of information. Since they are all ids, they have indexes in the database. Our looks up are fast. Our explained queries made the database administrators happy. To calculate permissions, we retrieve all the header information for a user. We query our user tables and join their subscriptions and special settings. We build up object number one - the user permissions object. { userID : 1 canEditTransactions : true , canDeleteTranactions : true , canDeleteSpecialTransactions : true } { userID : 2 canEditTransactions : true , canDeleteTranactions : true , canDeleteSpecialTransactions : false } Then, we get all the business objects a user cares about. Let SQL do what it does best - return relationships between ids and tables. We return the smallest amount of data.  We return only the ids of the objects in their respective bucket. Now, we have the user permissions and their appropriate data. Reading through those returned record sets, we build a dictionary. This dictionary of permissions contains relevant user information and  buckets of ids. It breaks down into the following actions: transactions : { userID : 1 , role : admin . //these are the transaction ids user 1 can delete canDelete : [ 1 , 2 , 3 , 4 ], //user 1 can edit these canEdit : [ 1 , 2 , 3 , 4 ], //user 1 can view these canView : [ 1 , 2 , 3 , 4 ], ... } Or for a different user: transactions : { userID : 2 , role : manager . //Hmm, one's missing. canDelete : [ 1 , 2 , 3 ], //Same here canEdit : [ 1 , 2 , 3 ], //But they can view them all. canView : [ 1 , 2 , 3 , 4 ], ... } From the data alone, we can see there is something special about object number 4. User 2 does not have the delete special transactions flag. Why can only administrators delete it? Administrators have the delete special transactions flag. What complicated business rules had fired to remove it from the bucket? The business rules governed by the permissions service. There’s as many answers as there are permissions in your application. The key is that we only care about figuring that out within the permissions api. The front end views and controllers rendering the templates should not have to care. They should only have to ask, you guessed it, “What can this  user do to an object?” Based just on the user id and the object’s keys, we can now answer: they can do it if the id is in the appropriate bucket. When rendering these transactions, a front end developer only needs to query the endpoint. They then control which widgets get rendered based on basic code. This code is completely independent of any changes to the permissions system. //render a grid of transactions for a user foreach ( transaction in transactions ) { //only show the ones we canView if ( transaction . id in permissions . canView ) { render ( transaction ); //But only show the big red button if they can delete it If ( transaction . id in permissions . canDelete ) { renderDeleteButton ( transaction ); } } } Same thing goes for checking assertions on the backend. When the server side controller receives a DELETE method on a given resource. It can use the exact same check. /delete/ : transactionID if ( transactionID in permissions . canDelete ) { delete ( transactionID ); } else { //deny the user throw ( 401 ); } But what about mutable permissions? What if permissions change based on parameters? Maybe the same user can edit transactions for one company?  Maybe they can only read transactions for another? These business rules live in the permissions api. We answer those varied questions with query string parameters. We’re still getting the permissions for a given user for a given business object. We just parameterize our calls: http://permissions.application.com/v1/:userID?companyID=1 Now, we GET the permissions. Here’s where this gets cool: when the business rules change. A feature comes in and it tweaks the way a manager can interact with a transaction. In our naive example, every place where we checked for permissions, we would need to update our code. If that code has spread throughout our code base, well, that’s a lot more places to get it wrong. If we make the changes on the api, we need to make it only once. We have great impact with minimal code. Of course, this is perilous. One wrong move in the game of permissions, and you can answer that question wrong for a whole lot of users all at once. That’s where part 2 comes in - asynchronous probing and comparing your results. Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-09-02"},
{"website": "Invision", "title": "Deploying to Multiple Kubernetes Clusters with kit", "author": ["Chesley Brown"], "link": "https://engineering.invisionapp.com/post/deploying-to-multiple-kubernetes-clusters-with-kit/", "abstract": "Here at InVision, we have a unique challenge. We don’t just have a single Kubernetes cluster, but several, all completely independent of each other and all needing automated updates via our CI/CD process. And although these clusters generally run the same code, the configurations differ. Things needed to work smoothly and automatically as we couldn’t afford to add friction to the deploy process or encumber our engineering teams. This led us to building and open sourcing our own continuous deployment solution for Kubernetes that we called kit . Read more about our experience with Docker, Kubernetes and the tooling we created around multi-cluster deployments on the Kubernetes.io blog ! Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-09-08"},
{"website": "Invision", "title": "The Power of the Why", "author": ["Jeremy Wight"], "link": "https://engineering.invisionapp.com/post/the-power-of-the-why/", "abstract": "More and more the answer of the question, ‘Can we build it?’ is increasingly, yes, yes we can, but as we continue to mature the question more and more becomes, ‘Should we build it?’ - Design Disruptors Understanding the power of the Why of what you are doing– by understanding your users’ story, provides both the passion and insight to deliver better products in the face of seemingly insurmountable pressures. More and more millennials are citing a primary motivating factor in their work being that they want to understand that the things that they make meaningfully impact the world 1 . As an engineer, we are frequently solving difficult and complex problems, implementing beautiful UI’s and other fun engineering-maker type tasks, but without understanding the context of the Why, even the most exciting problems can lose their luster. Besides losing their luster, an even bigger issue that comes about by not having experiences which help to contextualize the work you are doing is a loss of understanding and empathy for the problems your users are facing. When engineers (and makers-at-large) begin to lack clarity on who their users are, the products that they are building begin to lose continuity for those users. I attended the premier of Design Disruptors in Charleston, SC and had the opportunity to meet with some of the users of our product at InVision. The ability to get first hand, unfiltered insight into the pain of our users and the problems that they are attempting to solve, gives great insight into the Why of what I am doing. When you are determining the Why, it is very powerful to understand the challenges and pain of your users. After the premier I spoke with InVision user Matthew Brown , VP of Design and UX at Benefitfocus a leading online health insurance provider. He shared one of the design challenges that they face which really struck me: When a customer of theirs passes away, the way that their product interacts with the surviving family members can either create an amazing emotional connection with the company, or completely destroy it. At this moment in their lives, how we present seemingly minor technical details has a huge impact on their state. As he described, detailed nuances of how their design team is handling such an important moment and season within someone’s life, with empathy through design, he was giving me – a developer of the tools for crafting that experience, a deep sense of responsibility. Only by interacting with our users can we uncover more and more the why of what we are doing, providing empathy, passion and sense of responsibility to continue to ship amazing tools to solve these complex problems. 1 : What Millenial Employees Really Want, Fast Company , The Motivating Trifecta: Autonomy, Mastery and Purpose , Drive: The Surprising Truth About What Motivates Us, Dan Pink Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-09-16"},
{"website": "Invision", "title": "Welcome to the InVision Engineering Blog", "author": ["Bjorn Freeman-Benson"], "link": "https://engineering.invisionapp.com/post/welcome-to-invision-engineering-blog/", "abstract": "Welcome to our new engineering blog. InVision is known as a design company – more accurately a collaborative design company – but there is a lot of interesting engineering that goes on behind the scenes, so we are an engineering company too. We work on real-time interactive collaboration tools that have to be pixel perfect, 100% reliable, fantastically usable, and enterprise-grade secure. On this blog, we’ll talk about how we do all that from an engineering perspective: Our technology and architecture, our tools and techniques, our people and processes. We plan to write about diverse topics from the obvious (we use a lot of Javascript, so we’ll write about that) to the moderately obvious (we’re fully cloud-hosted: that’s got pluses and minuses) to the not-at-all-obvious (how we organize so that we don’t need formal project managers). We will write about the lessons we’ve learned along the way and, in the spirit of helping the community, we’ll even write about some of the mistakes we’ve made that you should avoid. We will introduce you to key engineers who’ve helped building InVision into what it is today and who are busy solving the hard engineering challenges that will take us into the future. We love being a fully remote, fully WFH company and we love how much more productive we are every day. We work here because we are the kind of engineers who love shipping new features to our customers every single day. If you are too, come join us! Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-03-27"},
{"website": "Invision", "title": "Understanding JavaScript Promises", "author": ["Scott Rippey"], "link": "https://engineering.invisionapp.com/post/understanding-promises/", "abstract": "JavaScript Promises have been growing in popularity for several years, and have finally become a part of the JavaScript specification ! Promises are easy to understand, and make it easy to write asynchronous code. A simple explanation of a Promise A Promise makes it easy to write code that waits for a background task to finish, and then continues once the result is ready. This happens by calling promise.then(...) : // Let's start by getting a \"user\" object over the network: var promiseForAUser = getUserFromTheNetwork (); // We don't have a \"user\" yet!  We have a \"Promise for a user\". // So, we must wait for the background work to finish: promiseForAUser . then ( function ( user ) { // And now we can continue! // The background work is finished, // and the result was a \"user\" }) What’s so great about Promises? JavaScript applications do a lot of work in the background – especially network communication. And there are several ways to wait for background work to finish. Events and callbacks are very common approaches, and they’ve been around for a long time. But Promises have 2 features that make them such a powerful solution: Promises are easy to chain and nest . This makes it easy to compose several operations, run things sequentially and in parallel, and wait for everything to finish before continuing. Errors will automatically bubble-up in a Promise chain. This means your code can focus on the “happy path”, but still ensure errors will be caught! All this power comes from a single function All the power of a Promise sits in the .then() method. This method has 4 important features: It lets you wait for the value var userPromise = getUserAsync (); // Waiting for the `user`... userPromise . then ( function ( user ) { // Continue: alert ( user . name ); }); `` It creates the next link in the chain. Whatever you return from a callback will be sent to the next link. This means you can transform the result of the Promise: getUserAsync (). then ( function ( user ) { // The next link will get this return value: return user . name ; }). then ( function ( name ) { alert ( name ); }); `` If you return a Promise from the callback –  something special happens – the next link waits for this Promise too! This means your Promises can be nested and depend on each other: getUserAsync () // Wait for the first promise: . then ( function ( user ) { // Return a nested promise: return getProfileAsync ( user . id ); }) // Wait for both promises to finish: . then ( function ( profile ) { console . log ( profile ); }); `` It lets you catch errors from anywhere in the chain, by passing a callback as the second parameter: getUserAsync () . then ( function successHandler ( user ) { throw new Error ( \"Oh no, some problem with \" + user . name ); }) . then ( function () { // This function will be skipped, because of the Error. }) . then ( function () { // This function will be skipped too. }, function _catch ( err ) { // Any errors in the chain will be caught here! // This not only includes the `throw new Error` line above, // but would also include any errors from `getUserAsync()` alert ( \"Failed to retrieve user! \" + err . message ); // Rethrow the error, to continue \"bubbling-up\" through the rest of the Promise chain: throw err ; }); ``\nThis behaves much like a try { ... } catch (err) { ... } block: An error, whether it’s a connection error or any thrown Error , will stop normal execution of the Promise chain. Errors “bubble-up” through the rest of the Promise chain, unless an error handler is found. The error handler has the same features as the success handler: If the error handler rethrows an error , then the error will continue to “bubble-up” through the remainder of the Promise chain. If the error handler returns successfully , then the Promise chain will continue normal execution . If the error handler returns a value , the value will be sent to the next link. If the error handler returns a Promise , the next link waits for this Promise too. There are lots of different Promise libraries out there, with more methods than just .then() , but ultimately all those methods are just utilities that all rely on .then() . That’s it! Now that you understand how simple a Promise is, stay tuned - in my next article I’ll share 8 tips for mastering promises. Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-03-28"},
{"website": "Invision", "title": "8 Tips for Mastering JavaScript Promises", "author": ["Scott Rippey"], "link": "https://engineering.invisionapp.com/post/mastering-promises/", "abstract": "In the previous post, Understanding JavaScript Promises , we learned how simple and useful Promises can be. Here are 8 tips to help you take advantage of that simplicity, and become a Master of Promises! 1. Know the terminology Talking about Promises is much easier when you know the right terminology. A Promise is pending while it is still working. When the work is complete, the Promise is fulfilled (or resolved ). The result of the work is called the fulfillment value . When an error occurs, the Promise is rejected .  The Error is called the rejection reason . 2. You don’t need Deferred ! The Deferred pattern (which includes both deferred objects and the new Promise constructor) was designed for wrapping low-level APIs, such as XmlHttpRequest and setTimeout . You should rarely need to do this work yourself! If you’re already using a Promise-returning library, using Deferred like this is a very common anti-pattern : function getUserProfile ( username ) { var defer = Promise . pending (); getUserAsync ( username ). then ( function ( user ) { getProfileAsync ( user . id ). then ( function ( profile ) { defer . resolve ( profile ); }, function ( err ) { defer . reject ( err ); }); }); return defer . promise ; } It’s messy, and prone to leaks.  Can you spot the leak in the code above? The alternative is much more powerful: 3. Chain, chain, chain You’ve already got a Promise. Calling .then creates a new link in the chain. All you’ve got to do is return the chain ! Look how much better this is: function getUserProfile ( username ) { return getUserAsync ( username ). then ( function ( user ) { return getProfileAsync ( user . id ); }); } When you chain, you no longer need to explicitly handle errors – all the “wiring” is automatic. 4. Watch for “runaway Promises” Whenever you have a Promise, you have to do something with that Promise. If you’re not returning the Promise , then you should be handling the errors . Otherwise, a “runaway Promise” will not be connected to your Promise chain, and will easily cause race conditions or hide errors. As a rule of thumb, the entry-point (eg. the UI layer, the request handler) should be handling errors, and the rest of your code should be returning the Promises. function updateUser () { // By returning the Promise, I don't have to handle errors: return getUserAsync (). then ( function ( user ) { // Always return nested Promises too! return getProfileAsync ( user ). then ( function ( profile ) { $scope . user = user ; $scope . profile = profile ; }); }); } button . addEventListener ( \"click\" , function ( ev ) { // This is an \"entry-point\" of the application, // and I can't simply \"return the Promise\" here, // so I need to handle errors: showSpinner (); updateUser (). then ( function () { hideSpinner (); }). catch ( function () { hideSpinner (); showErrorMessage (); }); }); 5. Treat Errors like you’ve always treated Errors All the traditional rules of throw ing and catch ing errors should be applied to Promises as well! Only throw if you can’t do what you said you can (eg. network error, unexpected data). Only catch if you can do something about it (eg. retry the connection, show an error message). Don’t “catch and release” – always “catch and rethrow”. If the error handler doesn’t rethrow, the rest of the Promise chain will continue normal execution . This is rarely what you intended! getUserAsync (). then ( function ( user ) { return user . name ; }). catch ( function ( err ) { log . error ( err ); // Oops, forgot to rethrow! }); // The above code is the async equivalent of this: try { return getUser (). name ; } catch ( err ) { log . error ( err ); // Oops, forgot to rethrow! // Continue normal execution: } 6. Learn your library In theory, a Promise only requires a .then method.  If it is “thenable”, it’ll interop with any other Promise library. However, every Promise library adds its own host of additional features for handling everyday tasks. Learn your library, and you’ll unlock the power of your Promise’s potential! Here are some of the most popular Promise implementations: Bluebird - very popular for NodeJS applications Q which inspired Angular’s $q service jQuery’s Promise Native ES6 Promises , or the ES6 Promise polyfill 7. NodeJS developers can promisify all the things! The majority of NodeJS modules use an async pattern called error-first callbacks instead of Promises. Fortunately, there is an effortless way to convert any error-first-callback module into a Promise-returning module! An incredible utility, called promisifyAll , does this for you, and it couldn’t be easier to use: var Promise = require ( \"bluebird\" ); var fs = Promise . promisifyAll ( require ( \"fs\" )); This will give you the original fs module, unharmed. It still has its original methods, like fs.readFile(filename, callback) and fs.writeFile(filename, data, callback) . However, the fs module has now been augmented with Promise-returning methods, called readFileAsync(filename) => Promise and writeFileAsync(filename, data) => Promise and so on! // Now, I can use `readFileAsync` instead of `readFile` fs . readFileAsync ( \"data.txt\" ). then ( function ( data ) { console . log ( data ); }); promisifyAll cleverly traverses the whole module, too, and adds *Async methods onto nested modules and class prototypes! 8. Look into the Future: Async + Await Looking into the future will change the way you see today’s Promises! Here’s a sample of an upcoming feature: an async function : async function getUserInfo ( username , password ) { var userId = await getUserId ( username , password ); var profile = await getProfile ( userId ); var projects = await getProjects ( userId ); return { userId , profile , projects }; } The async + await syntax makes Promises an implementation detail , that you no longer need to think about! Technically, the getUserInfo function returns a Promise, as do getUserId , getProfile , and getProjects . However, the await keyword waits for the promise, and then gives you the value. It’s the same as calling .then to get the results, but this completely eliminates the callbacks and the nesting! This code looks almost identical to synchronous code. If you’re not lucky enough to be bleeding-edge, or transpiling with something like Babel , you’ll have to stick with the Promise equivalent: // Same code, using Promises directly function getUserInfo ( username , password ) { return getUserId ( username , password ). then ( function ( userId ) { return getProfile ( userId ). then ( function ( profile ) { return getProjects ( userId ). then ( function ( projects ) { return { userId , profile , projects }; }); }); }); } So when you look at Promise code like this, you should realize that 3 of these return statements are actually just await statements in disguise. Realize that this function only has one real return point: the final return { userId, profile, projects }; Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-03-31"},
{"website": "Invision", "title": "Sharing ESLint Across Teams", "author": ["Kevin Lamping"], "link": "https://engineering.invisionapp.com/post/sharing-eslint-across-teams/", "abstract": "ESLint has won us over, and any doubts we had about the need for “Yet Another Linter” have been answered. Anyone can customize ESLint as they please, using both the built-in rules and creating their own. Then when you’re done with that, sharing your settings is amazingly simple. You Get ESLint! And You Get ESLint! One looming question we had when researching ESLint was how we’d manage our configuration across dozens of different code repos. While we could set up a template .eslintrc file for teams to use when they first initialize their codebase, keeping them all up to date was going to be a maintenance headache. That’s why we did a little dance when we read about ESLint’s Sharable Configs functionality. There are already hundreds of shared ESLint configurations out there, including popular rulesets like Standard and Semistandard . Even companies like Airbnb are getting in on the fun. Shareable configs seemed perfect for our needs; all we had to do was define our rules and publish them as an NPM module. Setting up our NPM Module The ESLint documentation covers setting up the NPM module very well, so read through their docs for more information on that. The one thing to note is that we decided to publish our module on our private NPM instance (named eslint-config-invision to match the preferred ESLint naming scheme). We’re looking in to making this part of the public sphere, so keep an eye out on it for the future. Project Settings Now that we had our NPM module, projects could use it by simply running npm install eslint-config-invision , then adding a basic .eslintrc file to their project root: { extends : \"eslint-config-invision\" } One of the great benefits of this setup is the ability for projects to have their own overrides to our default rules. While we encourage everyone to use the standard configuration, sometimes it’s necessary to break the rules. Here’s an example of a project setting up a specific override to allow for console usage (as the code is a simple command line utility): { extends : \"eslint-config-invision\" , \"rules\" : { \"no-console\" : 0 } } Multiple Configurations Since our codebases represent both Browser and NodeJS code, we left out the browser specific settings from our base configuration. To provide these browser configurations for UI projects, we created a couple more rulesets: one for our common UI code ( browser.js , and another for our React code ( react.js ). Here’s what our browser.js configuration looks like: { \"extends\" : \"./index.js\" , \"env\" : { \"browser\" : true , \"jquery\" : true }, \"globals\" : { \"angular\" : false } } The browser.js and react.js files live right next to our main index.js file, so everything is neatly packaged together. One specific point to make is that we’re extending a js file, which is different than our normal eslint-config-invision value for extends . This is due to the fact that we’re doing this from within our NPM module, which doesn’t have access to the eslint-config-invision namespace. Instead, we load the file directly. The new settings file is saved as browser.js and it’s ready for use via our module. Using these specialized settings is only little different from the original setup: { extends : \"eslint-config-invision/browser\" } We also have a React specific file, named react.js , that can be loaded via eslint-config-invision/react . In the future, we may add a tests file for our test files, or any other configuration we find the need to have. There are many reasons why we love ESLint, and shareable configs is just one of them. Be sure to check out their homepage for more details on the project. Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-04-05"},
{"website": "Invision", "title": "Optimizing Webpack for Faster React Builds", "author": ["Jonathan Rowny"], "link": "https://engineering.invisionapp.com/post/optimizing-webpack/", "abstract": "If you’ve got a slow Webpack build with a ton of libraries - fear not, there’s a way to increase incremental build speed! Webpack’s DLLPlugin lets you build all of your dependencies into a single file. It’s a great alternative to chunking. The file is later consumed by your main Webpack configuration or even on other projects sharing the same set of dependencies. Your typical React app may contain dozens of vendor libs depending on your flavor of Flux, add-ons, router, and other utilities like lodash . We’ll save precious build time by allowing Webpack to skip over any reference contained within the DLL. This article assumes you’re already familiar with a typical Webpack and React setup. If not, please checkout SurviveJS ‘s excellent section on Webpack and React and come back here when your build time starts to creep upwards. Step 1, List your Vendors The easiest way to build and maintain a DLL is by creating a JS file in your project, let’s call it vendors.js and requiring all of the libs you use. For example, on our recent projects, our vendors.js file looks like this: require ( \"classnames\" ); require ( \"dom-css\" ); require ( \"lodash\" ); require ( \"react\" ); require ( \"react-addons-update\" ); require ( \"react-addons-pure-render-mixin\" ); require ( \"react-dom\" ); require ( \"react-redux\" ); require ( \"react-router\" ); require ( \"redux\" ); require ( \"redux-saga\" ); require ( \"redux-simple-router\" ); require ( \"redux-storage\" ); require ( \"redux-undo\" ); This is the file we will “build” into a DLL. It has no functionality, it just imports the libraries we use. Note: You could also use ES6 style import here, but then we’d need Babel just to build the DLL. You can still use import and all the rest of the ES2015 sugary goodness in your main project just as you’re used to. Step 2, Build your DLL Now we can create a Webpack configuration to build the DLL. This is completely separate from your app’s main Webpack configuration and will result in a few files. It won’t be called by your Webpack middleware, Webpack server, or anything else (except manually or through a pre-build step). Let’s call this file webpack.dll.js var path = require ( \"path\" ); var webpack = require ( \"webpack\" ); module . exports = { entry : { vendor : [ path . join ( __dirname , \"client\" , \"vendors.js\" )] }, output : { path : path . join ( __dirname , \"dist\" , \"dll\" ), filename : \"dll.[name].js\" , library : \"[name]\" }, plugins : [ new webpack . DllPlugin ({ path : path . join ( __dirname , \"dll\" , \"[name]-manifest.json\" ), name : \"[name]\" , context : path . resolve ( __dirname , \"client\" ) }), new webpack . optimize . OccurenceOrderPlugin (), new webpack . optimize . UglifyJsPlugin () ], resolve : { root : path . resolve ( __dirname , \"client\" ), modulesDirectories : [ \"node_modules\" ] } }; This is a fairly typical Webpack config except for the webpack.DLLPlugin , which contains properties for the name, context, and manifest path. The manifest is very important, it gives other Webpack configurations a map to your already built modules. The context is the root of your client source code and the name is the name of the entry, in this case “vendor”. Go ahead and try running this build with the command webpack --config=webpack.dll.js . You should end up with a dll\\vendor-manifest.json that contains a nice little map to your modules as well as a dist\\dll\\dll.vendor.js which contains a nicely minified package containing all of your vendor libs. Step 3, Build your Project Note: The following example does not include sass, assets, nor a hotloader. They should still work just fine if you’ve got them in your config. Now all we need to do is add the DLLReferencePlugin and point it at our already built DLL. Here’s what your webpack.dev.js might look like: var path = require ( \"path\" ); var webpack = require ( \"webpack\" ); module . exports = { cache : true , devtool : \"eval\" , //or cheap-module-eval-source-map entry : { app : path . join ( __dirname , \"client\" , \"index.js\" ) }, output : { path : path . join ( __dirname , \"dist\" ), filename : \"[name].js\" , chunkFilename : \"[name].js\" }, plugins : [ //Typically you'd have plenty of other plugins here as well new webpack . DllReferencePlugin ({ context : path . join ( __dirname , \"client\" ), manifest : require ( \"./dll/vendor-manifest.json\" ) }), ], module : { loaders : [ { test : /\\.jsx?$/ , loader : \"babel\" , include : [ path . join ( __dirname , \"client\" ) //important for performance! ], query : { cacheDirectory : true , //important for performance plugins : [ \"transform-regenerator\" ], presets : [ \"react\" , \"es2015\" , \"stage-0\" ] } } ] }, resolve : { extensions : [ \"\" , \".js\" , \".jsx\" ], root : path . resolve ( __dirname , \"client\" ), modulesDirectories : [ \"node_modules\" ] } }; We’ve also done a few other things to increase performance including: Make sure we have cache: true Make sure that the babel loader has cacheDirectory:true in the query Used an include for the babel loader (you should do this for all loaders) Set devtool to eval because we’re optimizing for build time #nobugs Step 4, include the DLL At this point, you’ve generated a vendor DLL file and you’ve got a Webpack build going to generate your app.js file. You need to serve and include both files in your template, but the DLL should come first. You’ve likely already got a template set up using the HtmlWebpackPlugin . Since we don’t care about hot reloading a DLL, you don’t really need to do anything special except including a <script src=\"dll/dll.vendor.js\"></script> before your main app.js. If you’re using webpack-middleware or your own custom server, you’ll also need to make sure that DLL is being served. At this point, everything should be running as it was, but incremental builds with Webpack should be blazing fast. Step 5, build scripts We can use NPM and package.json to add a few simple scripts to take care of building for us. To clean out the dist folder, go ahead and run npm i rimraf --saveDev . Now add to your package.json: \"scripts\" : { \"clean\" : \"rimraf dist\" , \"build:webpack\" : \"webpack --config config.prod.js\" , \"build:dll\" : \"webpack --config config.dll.js\" , \"build\" : \"npm run clean && npm run build:dll && npm run build:webpack\" , \"watch\" : \"npm run build:dll && webpack --config config.dev.js --watch --progress\" } Now you can run npm run watch . If you’d rather run build:dll manually, you can remove it from the watch script for faster startups. That’s all, folks! I hope this gives you insight into how InVision uses Webpack’s DLLPlugin to increase our build speed. If you have any thoughts or questions, feel free to leave a comment! Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-04-12"},
{"website": "Invision", "title": "Higher-order Components", "author": ["Matt Vickers"], "link": "https://engineering.invisionapp.com/post/higher-order-components/", "abstract": "A Higher-order Component is a function, or component, that wraps around your regular React components allowing you create more reusable code. A HoC has the same lifecycle methods a regular React component, because under the hood, it is a regular React component. Your HoC can have it’s own state, methods and props, which are then made available, via props, to the wrapped component(s). This comes in handy when you have a handful of components doing 90% of the same setup, rendering or actions. Enough explanation, let’s get our feet wet Let’s say you were tasked with creating an artboard, or canvas. You’re given an array of layers, each with different types, with the following criteria: Text layers should render out the text Image layers should render out the image Shapes should render out the shape Clicking on any layer type should select it There’s a few ways you could tackle this. You could have a giant switch statement that decided what to render based on layer type, but that could get ugly if you want to add complex interactions to specific layer types. I think we’ll just create 3 separate components. Each component will take care of rendering out the layer and will also take care of selecting that layer. Note: You will not be able to copy and paste the following code. We’re just going to focus on the overall idea of HoCs. // TextLayer.jsx class TextLayer extends React . Component { handleClick () { this . props . actions . selectLayer ( this . props . layerId ); } render () { const styles = { top : layer . y , left : layer . x , height : layer . height , width : layer . width }; return ( < div className = \"layer text\" onClick = { this . handleClick } > { layer . text } < /div> ); } } // ImageLayer.jsx class ImageLayer extends React . Component { handleClick () { this . props . actions . selectLayer ( this . props . layerId ); } render () { const styles = { background : `url( ${ layer . url } )` , top : layer . y , left : layer . x , height : layer . height , width : layer . width }; return ( < div className = \"layer image\" style = { styles } onClick = { this . handleClick } /> ); } } // ShapeLayer.jsx class ShapeLayer extends React . Component { handleClick () { this . props . actions . selectLayer ( this . props . layerId ); } render () { const styles = { backgroundColor : layer . backgroundColor , top : layer . y , left : layer . x , height : layer . height , width : layer . width , border : ` ${ layer . borderWidth } solid ${ layer . borderColor } ` }; return ( < div className = \"layer shape\" style = { styles } onClick = { this . handleClick } /> ); } } So, as you can see we have 3 pretty simple components that basically do the same thing, with a few slight differences. While each component renders differently, they’re all sized and positioned the same and they fire off the same action to select a layer. While this may not be the end of the world, it’ll get messy when you have to add n more layer types in the future. The old method of cleaning this up would be to write a Mixin . While not deprecated yet, there’s talks of Mixins going the way of the dinosaur, so we’re going to use the new method (called Composition) and create an HoC we can wrap these components with. HoC Skeleton export default ComposedComponent => { class LayerBase extends React . Component { constructor ( props ) { super ( props ); } render () { return ( < ComposedComponent {... this . props } {... this . state } /> ); } } return LayerBase ; }; Like we mentioned before, a HoC is just a React component, which you’ll notice when you take a look at the HoC skeleton. We’re taking LayerBase , which extends React.Component , and passing it into a function called ComposedComponent . Any component we wrap with LayerBase() will now be passed our state, props and methods. If we wanted to have ImageLayer make use of our HoC skeleton, we could alter the ImageLayer component to look something like the code below: import LayerBase from \"LayerBase.jsx\" ; class ImageLayer extends React . Component { // ... } // This is the meat and potatoes. // // We're wrapping our ImageLayer component with LayerBase. // Once we do this, ImageLayer has access to all the props // we'll pass down from LayerBase export LayerBase ( ImageLayer ); When your ImageLayer component is mounted, the LayerBase HoC is going to render first, set up whatever state, props and methods are required and then mount your ImageLayer component. You can see that we spread the props and state into ComposedComponent which passes them into ImageLayer as regular props. A little later on you’ll see that we pass methods down exactly the same way. So, let’s get some of that positioning code DRY’d up a bit. For now we’ll just tackle the ImageLayer component, but this can be applied exactly the same way to our TextLayer , ShapeLayer , or any other component(s). export default ComposedComponent => { class LayerBase extends React . Component { constructor ( props ) { super ( props ); // Setup our new method and bind it so we // have access to this scope this . getLayerPosition = this . getLayerPosition . bind ( this ); } getLayerPosition () { // Return all the common positioning styles return { top : layer . y , left : layer . x , height : layer . height , width : layer . width }; } render () { return ( < ComposedComponent {... this . props } {... this . state } getLayerPosition = { this . getLayerPosition } /> ); } } // ... }; We’ve added a new getLayerPosition() method to return some common values and passed it down as a prop to our ImageLayer . We can now access the getLayerPosition() just like you would any other prop method that was passed down to your component. class ImageLayer extends React . Component { // ... render () { // Grab our common positioning styles const position = this . props . getLayerPosition (); // Merge the common styles with our layer type // specific styles const styles = Object . assign ({}, position , { background : `url( ${ layer . url } )` }); return ( < div className = \"layer image\" style = { styles } onClick = { this . onClick } /> ); } } Now that we’ve cleaned up the styling a bit, let’s use what we’ve learned and add our selection action to our HoC. export default ComposedComponent => { class LayerBase extends React . Component { constructor ( props ) { super ( props ); this . getLayerPosition = this . getLayerPosition . bind (); this . handleClick = this . handleClick . bind (); } getLayerPosition () {} handleClick ( layerId ) { this . props . actions . selectLayer ( layerId ); } render () { return ( < ComposedComponent {... this . props } {... this . state } getLayerPosition = { this . getLayerPosition } handleClick = { this . handleClick } /> ); } } // ... }; Once again, we’ve setup and bound our method and passed it down as a prop so that our ImageLayer component has access to it. class ImageLayer extends React . Component { // ... handleClick () { // Pass the layerId up into out HoC so it // can use it when we call our action this . props . handleClick ( this . props . layer . id ); // Having this method each of our components is strictly // a preference. If you wanted to get away from having // to create another method here, you can pass the handleClick // prop directly to the onClick handler and then bind // the layerId properly to pass it to the HOC. // // Example: onClick={this.props.handleClick.bind(this, this.props.layer.id)} } render () { // Grab our common positioning styles const position = this . props . getLayerPosition (); // Merge the common styles with our layer type // specific styles const styles = Object . assign ({}, position , { background : `url( ${ layer . url } )` }); return ( < div className = \"layer image\" style = { styles } onClick = { this . props . handleClick } /> ); } } We’ve changed our click handler a bit to offset the action call to our HoC. It’s a small change but it allows us to keep the components a little cleaner. Fin You’re probably thinking “We started with 3 components and ended up with 4, how is that making things simpler?!\" . While that is true, we’ve removed a bunch of unnecessary duplication that will allow us to quickly add new layer types without having to copy and paste properties over and over. We’ve also created smaller components that are easier to test. I always like to make my components do one or two things really well. While that doesn’t always work out in the real world, it allows you to remove as much possibility of introducing bugs as you can. Higher-order components seem a bit confusing at first, but when you boil it down, you’re still just working with components - Passing props down the chain, but now it allows you to keep your components small and lightweight, while creating a bit more re-usability. Light Bed-Time Reading We’ve just skimmed the surface of HoCs, but if you’d like to really dive in and see how they tick, here’s a couple good articles/gists: Higher-order Components by Sebastian Markbåge Mixins Are Dead. Long Live Composition by Dan Abramov Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-04-19"},
{"website": "Invision", "title": "React Performance Tune-Up", "author": ["Dave Johnson"], "link": "https://engineering.invisionapp.com/post/react-performance-tune-up/", "abstract": "At our recent engineering offsite my team took the opportunity not only to plan out and design some new features we’re starting to build but we also spent some time tuning our existing React application to make it even faster. Here are a couple of the quick wins we had and a few additional techniques that might help you spot bottlenecks and tune up your own app. You can’t tune what you can’t measure, so a large part of any performance effort is properly instrumenting both your application and your testing environment to measure and analyze the deltas as you work. In the following examples we’ll be quantifying performance using some React-specific tools along with the great profiling tools already built into browsers. Getting Wasted() React has a ready-made set of performance measuring methods available via the Perf object from the Performance Tools library. Included is a handy function called printWasted() which reports where unnecessary work is happening in component render() methods and the virtual DOM in JavaScript that don’t ultimately cause a change in the HTML DOM. To get setup, install the react-addons-perf package and include it in your project. npm install react-addons-perf --save-dev import Perf from \"react-addons-perf\" ; // Expose the React Performance Tools on the`window` object window . Perf = Perf ; Exposing the tools directly on the window object will allow control of performance data collection from the browser console while your app is running. Give it a try! window . Perf . start () // ... do some stuff with the app related to whatever components you are testing window . Perf . stop () window . Perf . printWasted () TMI: Oversharing Props The printWasted() reports are a great way to reveal places in your app where you might be passing too much information through your component stack, causing unnecessary render() calls, even when you’re optimizing with PureRenderMixin . As we were developing our app we initially had a small section of our Redux store tracking a couple client-side only UI states. For velocity of early development, we simply passed that entire object through to the various components that used it. As is typical though, that part of our store grew over time and as you can see from this report our components were doing a lot of unnecessary work. It is also very apparent on the CPU profile and flame chart from the Chrome DevTools Timeline that our app was very busy in scripting land. After some refactoring to prune props from being passed to components where they weren’t used you can see a huge improvement in both the printWasted() report and the Timeline profile. Though we were able to achieve these gains through straightforward pruning of unused props being passed through our component stack, there are cases where you could improve performance further by rerouting data flow. When you find some intermediate components with substantial wasted time that are only receiving certain props so that they can then be passed through to children, you can side-step triggering those unnecessary render() calls by using something like Redux’s connect to directly provide any props required by deeply nested child components. This does lead to tight coupling between your components and a specific state container technology (e.g. Redux), and makes them less reusable, so make sure the performance gains are material and your intentions well documented. Alternatively, there is also React’s Context , though as an experimental feature with some counterintuitive behavior and  the potential to obscure data flow through your app, I wouldn’t recommend it. Getting out of a bind() Another thing to watch out for if you are finding a component wasting time unexpectedly, even when you are optimizing with PureRenderMixin or your own shouldComponentUpdate() check, is that doing a bind() in your render() method for a function that you pass to children components as props will create and send a new function each time. This means that the nextProps provided to the child’s shouldComponentUpdate will be different when you might not expect them to be. // Creates a new `handleUpload` function during each render() < TopBar handleUpload = { this . handleUpload . bind ( this )} /> // ...as do inlined arrow functions < TopBar handleUpload = { files => this . handleUpload ( files )} /> When possible, bind() any passed functions in the component’s constructor so they are bound only once during creation and then reused during the entire lifecycle of the instance. class App extends React . Component { constructor ( props ) { super ( props ); this . handleUpload = this . handleUpload . bind ( this ); } handleUpload ( files ) { // Upload 'em... } render () { < TopBar handleUpload = { this . handleUpload } /> } } There is also a helpful eslint-plugin-react rule called jsx-no-bind to help find these inefficient inline bind() and arrow function props in JSX. I got this, DOM One great way to improve performance of React components is to minimize mutating the DOM at all and moving work over to CSS where possible. The below example shows a performance snapshot where a drop target indicator for reordering list elements used a React component that was being reordered directly in the DOM along with the list items while mouse dragging. You can see all the wasted time, in our case largely due to a bunch of element position, offset and dimension calculations happening for our CustomScrollBars component, which doesn’t actually change since the list elements and their layout are constant while dragging in our UI. The Timeline profile confirms the slowness experienced in the UI while dragging. A more efficient way to implement this is to use a single persistent element, in this case implemented via a stateless function component, that is then positioned via CSS transform , without needing to reorder any DOM elements at all. const ReorderIndicator = props => { const { index } = props ; const styles = { transform : `translateY( ${ POSITION_OFFSET * index } px)` }; return ( < div style = { styles } /> ); }; With that change, BuildMode > LayersPanel is no longer wasting any time, and wasted calls to the LayersPanel > CustomScrollBars have dropped from over 2000 to just 2. The CPU is also no longer saturated by scripting and the FPS is much improved! We can also clearly see that converting the reorder indicator positioning to use the CSS transform property is allowing that work to move onto the GPU. Profiling Tips Here are some tips for improving the signal-to-noise ratio during your profiling sessions. Disable browser extensions (unless of course your React app is a browser extension!) since their interactions with your page and memory usage can appear in profiling data and heap snapshots. (You can spot these extension-related items, if present, as having a chrome-extension:// prefix) Trigger garbage collection by clicking the trash can icon immediately before starting a Timeline profile recording and also just before stopping so you have a consistent baseline from which to detect possible heap, node or listener leaks. If you create vendor bundles as part of your build process (like with Webpack’s DLLPlugin ) disable that while you profile so you’ll see original file names and locations, making it easier to explore and understand call stacks. If you are minifying/uglifying code, disable that step while you profile so you’ll see non-obfuscated function and variable names. Enable sourcemaps if you are transpiling and/or bundling with Babel, Browserify, jspm, closure, traceur, etc. Build or include the React library for your project in NODE_ENV=production mode to avoid the development-only PropTypes checking code calls and related performance overhead. Additional resources Along with the existing Network Throttling tool available in Chrome, there is also a recently added CPU Throttling tool now in Chrome Canary, so you can experience your app as someone that isn’t using your amazing supercharged and fully upgraded development machine. Since this will increase the timing measurements for function calls it can also make it easier to drill down into flame charts that have a lot of fast calls. More and Faster I hope you find these techniques helpful and are inspired to spend some time profiling and tuning your own app… your users will thank you! Feel free to share some of your own React tuning tips in the comments and let me know what other performance topics would be of interest for me to explore next time (CPU profiling, network tuning, memory optimization and leak detection, incremental rendering via React Fiber , etc.) Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-09-23"},
{"website": "Invision", "title": "Persist Redux state by using Sagas", "author": ["João Portela"], "link": "https://engineering.invisionapp.com/post/persist-redux-state-by-using-sagas/", "abstract": "redux-saga is a fairly new library in the Redux ecosystem, one that has quickly gained adoption and traction in the community. We’ve started using sagas to manage image uploads and to implement a history mechanism (undo/redo). We really liked the way it helps orchestrate complex flows that have side-effects, by using ES6 generators , which promote more readable asynchronous code and simpler unit tests. Our app already persisted the state by loading it from the server on page navigation and by saving it periodically, after some specific actions were triggered. It relied on the redux-storage library, a simple plug and play library for loading/storing data, but we started having more complex requirements on the persistence mechanism. That prompted us to replace redux-storage with a custom saga and liked the result so much that decided to share it with you. This article assumes that you have some redux and redux-saga knowledge, but I’ve aimed for the explanations to be gradual and simple, so newcomers to these libraries can learn from them. The documentation for Redux and redux-sagas are great starting points and a good reference if you have questions throughout this article. Scope The persistence mechanism described in this article assumes that you’ll save the whole state to the server (although you can easily select just a subset of it). The save operation follows these requirements: There must be a whitelist of actions (only actions on the whitelist will trigger a save) Some actions (such as dragging an image) should be debounced, in order to only trigger a save operation to the server after an amount of time Other actions (such as creating an image) should save immediately There should be an “unsaved changes” indicator on the UI, that is displayed when a change is first recorded and hidden when a successful save response is received from the server This article does not explain how to setup sagas or a react/redux project (as there’s already the documentation and countless examples). It does provide a github repository with a complete example for you to check how it was setup and how it plays together with React/Redux: https://github.com/jportela/redux-saga-persistence Starting small Let’s start by implementing a simple saga that, on all actions, triggers a save immediately to the server: import { call , put , select , take } from 'redux-saga/effects' ; import { serverSave } from '../actions' ; import * as PersistenceEngine from './persistence/engine' ; export default function * persistenceSaga () { while ( true ) { const action = yield take (); const state = yield select (); yield put ( serverSave . request ( action )); yield call ( PersistenceEngine . save , state , action ); yield put ( serverSave . success ()); } } By wrapping the generator body in a while(true) , it will be run for all actions, looping through the following instructions: Retrieve any action (the take effect intercepts actions dispatched to the store) Get the state from the redux store (the select effect does that) dispatch a serverSave.request action (the put effect dispatches an action to the redux store) call the PersistenceEngine.save function, which is a promise that does a server request, fulfilled when the server responds dispatch a serverSave.success action The serverSave actions indicate the app that a request is being made and when it’s fulfilled (it’s your choice to use it or not, depending if you want to show any indication on the UI). One obvious improvement we can make is to add error handling, which redux-saga makes it as easy as synchronous code, by using the familiar try/catch : import { call , put , select , take } from 'redux-saga/effects' ; import { serverSave } from '../actions' ; import * as PersistenceEngine from './persistence/engine' ; export default function * persistenceSaga () { while ( true ) { const action = yield take (); const state = yield select (); yield put ( serverSave . request ( action )); try { yield call ( PersistenceEngine . save , state , action ); yield put ( serverSave . success ()); } catch ( e ) { yield put ( serverSave . failure ()); } } } That was simple and easy, but it’s hardly a good solution. All actions are triggering a save, so your server will be mindlessly bombarded with save requests. To help prevent this, we’ll implement a simple whitelist, so only specific actions will trigger a save. Implementing a whitelist This whitelist just uses an Object as a map, to check if the action type exists on the whitelist. If it doesn’t exist, it will just continue the loop, not saving anything. I recommend separating the Whitelist into its own module, as it will quickly grow. // persistence/whitelist.js import * as types from '../../constants/ActionTypes' ; const Whitelist = { [ types . CREATE_IMAGE ] : true , [ types . MOVE_IMAGE ] : true }; export default Whitelist ; import { call , put , select , take } from 'redux-saga/effects' ; import { serverSave } from '../actions' ; import * as PersistenceEngine from './persistence/engine' ; import Whitelist from './persistence/whitelist' ; export default function * persistenceSaga () { while ( true ) { const action = yield take (); if ( ! Whitelist [ action . type ]) { continue ; } const state = yield select (); yield put ( serverSave . request ( action )); try { yield call ( PersistenceEngine . save , state , action ); yield put ( serverSave . success ()); } catch ( e ) { yield put ( serverSave . failure ()); } } } Remember the requirements that we had? Let’s implement debouncing, as we don’t want that MOVE_IMAGE action to bomb our server with requests on every mouse move event. Implementing debounce We’ll start by implementing a delay promise (copied from redux-saga docs), that, by using generators, will look almost like a sleep function (with the advantage of not actually blocking the UI, it will just block the generator execution). The idea is to yield that “sleep” promise so the save operation can only be executed later. import { call , put , select , take } from 'redux-saga/effects' ; import { serverSave } from '../actions' ; import * as PersistenceEngine from './persistence/engine' ; import Whitelist from './persistence/whitelist' ; const DEBOUNCE_TIME = 3000 ; // debounce time in milliseconds function delay ( ms ) { return new Promise ( resolve => setTimeout ( resolve , ms )); } export default function * persistenceLayer () { while ( true ) { const action = yield take (); if ( ! Whitelist [ action . type ]) { continue ; } const state = yield select (); yield call ( delay , DEBOUNCE_TIME ); yield put ( serverSave . request ( action )); try { yield call ( PersistenceEngine . save , state , action ); yield put ( serverSave . success ()); } catch ( e ) { yield put ( serverSave . failure ()); } } } While this code works, it doesn’t exactly meet the requirements we’ve set for the app. Say that we have three actions (A, B, C) that occur in a 2-second interval between each other. With the code above, there would be two save operations, one 3 seconds after the A action, and another 3 seconds after the C action. The B action would be ignored because the generator would be “blocked” waiting for the delay promise to be fulfilled. The behavior I’m looking to implement is to only trigger a save operation when no actions are triggered for 3 seconds. So, it would only save 3 seconds after the C operation. To accomplish that, we need a way to say that, if there’s already a scheduled save, reset the delay timer and cancel the old scheduled save. Running sagas in the background (and canceling them) To implement this, we’ll use the fork effect, that runs a task concurrently ( call waits for the promise to be completed, blocking the saga – we need to keep receiving actions). I like the fork analogy, as in redux-saga they behave a lot similar to processes forks. Furthermore, fork ed processes can be canceled so they’ll work well with our requirements. import { cancel , call , fork , put , select , take } from 'redux-saga/effects' ; import { serverSave } from '../actions' ; import * as PersistenceEngine from './persistence/engine' ; import Whitelist from './persistence/whitelist' ; const DEBOUNCE_TIME = 3000 ; // debounce time in milliseconds function delay ( ms ) { return new Promise ( resolve => setTimeout ( resolve , ms )); } // let's separate this function for better readability function * save ( state , action ) { yield put ( serverSave . request ( action )); try { yield call ( PersistenceEngine . save , state , action ); yield put ( serverSave . success ()); } catch ( e ) { yield put ( serverSave . failure ()); } } function * debounceSave ( state ) { try { yield call ( delay , DEBOUNCE_TIME ); yield call ( save , state ); } catch ( e ) { // empty exception handler because the cancel effect throws an exception } } export default function * persistenceLayer () { // if there's already a delay task running, we want to cancel it let debounceTask = null ; while ( true ) { const action = yield take (); if ( ! Whitelist [ action . type ]) { continue ; } const state = yield select (); if ( debounceTask ) { yield cancel ( debounceTask ); } debounceTask = yield fork ( debounceSave , state , action ); } } We started by separating the save and debounceSave generator functions, in order to make the code a bit more easy to read. There’s also a task concept that was added, which is the result yielded by fork : debounceTask - this is the yielded value from a fork . We need to store it so we can cancel the debounce event ( check the API for a Task here ) cancel - this is another redux-saga effect, that cancels a forked process. Note that canceling a task throws a SagaCancellationException to the generator that was forked. Creating a more robust whitelist So let’s continue with our requirements. The CREATE_IMAGE action needs to be saved immediately, canceling the debouncing, if it’s running. That can easily be done by adding types of persistence to the Whitelist. Let’s separate it into it’s own module and provide an utility function to retrieve the type of persistence: // persistence/whitelist.js import * as types from '../../constants/ActionTypes' ; export const PersistenceType = { IMMEDIATE : 'IMMEDIATE' , DEBOUNCE : 'DEBOUNCE' }; const Whitelist = { [ types . CREATE_IMAGE ] : PersistenceType . IMMEDIATE , [ types . MOVE_IMAGE ] : PersistenceType . DEBOUNCE }; export function getPersistenceType ( type ) { return Whitelist [ type ] || null ; } import { cancel , call , fork , put , select , take } from 'redux-saga/effects' ; import * as types from '../constants/ActionTypes' ; import { serverSave , signalUnsavedChanges , signalSavedChanges } from '../actions' ; import * as PersistenceEngine from './persistence/engine' ; import { getPersistenceType , PersistenceType } from './persistence/whitelist' ; const DEBOUNCE_TIME = 3000 ; // debounce time in milliseconds function delay ( ms ) { return new Promise ( resolve => setTimeout ( resolve , ms )); } // let's separate this function for better modularity function * save ( state , action ) { yield put ( serverSave . request ( action )); try { yield call ( PersistenceEngine . save , state , action ); yield put ( serverSave . success ()); } catch ( e ) { yield put ( serverSave . failure ()); } } function * debounceSave ( state ) { try { yield call ( delay , DEBOUNCE_TIME ); yield call ( save , state ); } catch ( e ) { // empty exception handler because the cancel effect throws an exception } } export default function * persistenceSaga () { let debounceTask = null ; while ( true ) { const action = yield take (); const type = getPersistenceType ( action . type ); if ( ! type ) { continue ; } const state = yield select (); if ( debounceTask ) { yield cancel ( debounceTask ); } if ( type === PersistenceType . IMMEDIATE ) { yield fork ( save , state ); // save immediately } else if ( type === PersistenceType . DEBOUNCE ) { debounceTask = yield fork ( debounceSave , state ); } } } Notice that we need to fork (not call ) both the save and debounceSave operations. That way the saga can keep retrieving actions in the background, so that a new IMMEDIATE action may cancel a previously scheduled one. Signaling Unsaved Changes So there’s only one missing requirement: we need to signal the UI when there are unsaved changes. We already dispatch actions when a save request is sent to the server and when it’s fulfilled. But that does not tell us when there are unsaved changes, as there can be changes that are still waiting the debounce timer to complete, which happens before the server request. We will implement the signaling by dispatching a UNSAVED_CHANGES action, waiting for the SERVER_SAVE_SUCCESS action, and finally dispatching a SAVED_CHANGES action. We will also rely on the Task interface to act as a lock, to prevent multiple UNSAVED_CHANGES dispatches. import { cancel , call , fork , put , select , take } from 'redux-saga/effects' ; import * as types from '../constants/ActionTypes' ; import { serverSave , signalUnsavedChanges , signalSavedChanges } from '../actions' ; import * as PersistenceEngine from './persistence/engine' ; import { getPersistenceType , PersistenceType } from './persistence/whitelist' ; const DEBOUNCE_TIME = 3000 ; // debounce time in milliseconds function delay ( ms ) { return new Promise ( resolve => setTimeout ( resolve , ms )); } // let's separate this function for better modularity function * save ( state , action ) { yield put ( serverSave . request ( action )); try { yield call ( PersistenceEngine . save , state , action ); yield put ( serverSave . success ()); } catch ( e ) { yield put ( serverSave . failure ()); } } function * debounceSave ( state ) { try { yield call ( delay , DEBOUNCE_TIME ); yield call ( save , state ); } catch ( e ) { // empty exception handler because the cancel effect throws an exception } } // signals to the UI that there are unsaved changes export function * signalPersistenceState () { yield put ( signalUnsavedChanges ()); yield take ( types . SERVER_SAVE_SUCCESS ); // waits for a SERVER_SAVE success to continue yield put ( signalSavedChanges ()); } export default function * persistenceSaga () { let debounceTask = null ; let unsavedTask = null ; while ( true ) { const action = yield take (); const type = getPersistenceType ( action . type ); if ( ! type ) { continue ; } const state = yield select (); if ( debounceTask ) { yield cancel ( debounceTask ); } if ( ! unsavedTask ) { unsavedTask = yield fork ( signalPersistenceState ); unsavedTask . done . then (() => { unsavedTask = null ; }); } if ( type === PersistenceType . IMMEDIATE ) { yield fork ( save , state ); // save immediately } else if ( type === PersistenceType . DEBOUNCE ) { debounceTask = yield fork ( debounceSave , state ); } } } The trick here is to use take(ActionTypes.SERVER_SAVE.SUCCESS) . It will wait until that action has been dispatched, and only then it will signal that there are no saved changes. The UI reducer can reduce those events into a boolean flag, indicating that there are unsaved changes. Dressing it up a little To better abstract and reuse the Lock functionality, I created a Lock class, resulting in much cleaner code: // utils/lock.js import { cancel , fork } from 'redux-saga/effects' ; export default class Lock { constructor ( func ) { this . isLocked = false ; this . task = null ; this . func = func ; } * execute (... args ) { if ( ! this . isLocked ) { // do not execute if it's locked this . isLocked = true ; this . task = yield fork ( this . func , ... args ); this . task . done . then (() => { this . isLocked = false ; }); } } * cancel () { if ( this . task ) { yield cancel ( this . task ); // reset the delay timeout } } } import { call , fork , put , select , take } from 'redux-saga/effects' ; import * as types from '../constants/ActionTypes' ; import { serverSave , signalUnsavedChanges , signalSavedChanges } from '../actions' ; import * as PersistenceEngine from './persistence/engine' ; import { getPersistenceType , PersistenceType } from './persistence/whitelist' ; import Lock from './utils/lock' ; const DEBOUNCE_TIME = 3000 ; // debounce time in milliseconds function delay ( ms ) { return new Promise ( resolve => setTimeout ( resolve , ms )); } // let's separate this function for better modularity function * save ( state , action ) { yield put ( serverSave . request ( state )); try { yield call ( PersistenceEngine . save , state , action ); yield put ( serverSave . success ()); } catch ( e ) { yield put ( serverSave . failure ()); } } function * debounceSave ( state ) { try { yield call ( delay , DEBOUNCE_TIME ); yield call ( save , state ); } catch ( e ) { // empty exception handler because the cancel effect throws an exception } } // signals to the UI that there are unsaved changes export function * signalPersistenceState () { yield put ( signalUnsavedChanges ()); yield take ( types . SERVER_SAVE_SUCCESS ); // waits for a SERVER_SAVE success to continue yield put ( signalSavedChanges ()); } export default function * persistenceSaga () { let debounceLock = new Lock ( debounceSave ); let unsavedLock = new Lock ( signalPersistenceState ); while ( true ) { const action = yield take (); const type = getPersistenceType ( action . type ); if ( ! type ) { continue ; } const state = yield select (); // each persistent action cancels the debounce timer yield debounceLock . cancel (); // this lock prevents multiple unsaved changes actions from being dispatched yield unsavedLock . execute (); if ( type === PersistenceType . IMMEDIATE ) { yield fork ( save , state ); // save immediately } else if ( type === PersistenceType . DEBOUNCE ) { // a new debounce timer is created yield debounceLock . execute ( state , action ); } } } Wrapping up And that’s all we have to show you today. You can see the example in action (and also a load saga, if you were wondering about it) in https://github.com/jportela/redux-saga-persistence There are a lot more features that we are implementing in our app which may warrant a part II on this topic. I’d personally want to see features such as diffing , so we only send to the server what was changed (although that may be the single responsibility of the PersistenceEngine ). I hope you liked this article, if you did please share it and let us know your thoughts on it. Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-04-26"},
{"website": "Invision", "title": "Hail the Heroes!", "author": ["Jon Dowdle"], "link": "https://engineering.invisionapp.com/post/hail-the-heroes/", "abstract": "If you were asked to identify the biggest killer of productivity for a software\nengineer, what would you say it is? You might say “email” or “drop in managers” but those two things have the same\nroot cause. Disruptions! : The mother of all flow killing work place\nactivities. Disruptions can obliterate an engineer’s productivity. After each distraction is\nover it takes about 23 minutes to refocus on the task that was at hand 1 . This\nleads to the average developer having only have two hours of disruption-free time per day 2 . We all know how this feels — even after a long day. You keep wondering, “What\ndid even I do today?” If you find yourself asking that question a lot, I have\ngreat news for you. You need a hero. Note: This is the post version of the talk given at Denver Startup Week 2016.\nSee this post for\nthe slides and information on the talk. A little background Before I answer what a hero is and how it can help you, let me give some\nbackground on where we started. I lead the SRE team at InVision and we were plagued with disruptions.\nUnderstandably, we had many days in which we didn’t know “where the time went.”\nWe needed to remove these disruptions so that we could build and maintain the\nplatform that the entire company depended on. What was disrupting us? After a quick and informal survey, we had a lot of the\ntypical disruptions that most systems administrators or IT help desks would\nhave. That would be fine, but we weren’t a help desk, we were a SRE group.\nBuilding the infrastructure that would power the future of our exciting SaaS\noffering was being neglected and we were just helping with\nprinters . When we looked at how critical the disruptions where, the results were fairly\nclear in that the disruptions were required but they were still killing our\nproductivity. We solved the problem by leveraging a concept that we already were familiar\nwith: being on call. Except this wasn’t the kind of on call that wakes you up at\n3am with that horrible PagerDuty voice. This was a different kind of on call and\nwe wanted to set that apart while giving the role a dose of dignity and\nawesomeness. The result is what we call our Heroes! _Source: https://www.flickr.com/photos/83346641@N00/9204125721_ What is a Hero? Our hero isn’t the kind that wears tight pants while flying around. Our hero is\nthe team’s representative to the rest of the company. Our hero lets the rest of\nus focus on making progress with our forward-thinking work, knowing that the\ndisruptions are being handled. The hero role rotates like an on call rotation would and it also has a few other\nexpectations. As a hero: You’re responsible for the random questions and small change requests from\nthe organization as a whole. You’re responsible for automating the issues/questions that come up. The first responsibility of a hero helps a little but it doesn’t go very far on\nit’s own. As the organization grows, without the second responsibility, hero\nwork only gets to be more and more until we are overwhelming the hero. The\nsecond responsibility is key. With that in mind, we can decrease both the raw\nnumber of disruptions as well as the time to complete them. Mechanics Since we wanted to approach the hero role as an experiment and only roll out an\nMVP until we were sure it would work, we wanted to keep it very light on the\nimplementation side. What we ended up with is a fully SaaS solution that serves\nit purpose while adding very little overhead to both the requesting teams or the\nhero. To start, we use Slack as our primary communication tool. One of the things that\nSlack gives you is the ability to define a “slash” command that will post to any\nendpoint that you give it. The slash command “/hail-hero” is the first part of\nour hero workflow. The receiving end of the web hook that the slash command\nposts to is hosted on a SaaS based rules engine, Zapier. We used Zapier for the\ninitial ease of setup for the MVP, anything that can accept and post webhooks\nwill work here. By using Zapier we can tie the receiving of that\nweb hook into our other tools, most notably JIRA and PagerDuty. Why both JIRA and PagerDuty? We use JIRA day in and day out for tracking our\nother work, so it was a natural fit. Also, by using JIRA we can create and link\nany other work that would take more time than we wanted to allocate in the hero\nprocess. PagerDuty was used because it helps us easily create and use a rotation\nof users who will be on hero duty. As an added benefit, PagerDuty can escalate\nhero tickets if they aren’t being responded to in a timely fashion. Outcome Overall, the hero system has worked phenomenally well. The business continuity\nis being maintained and our team has a sense of focus. We are making progress on\nour most substantial goals. In addition to this being quite successful for the\nSRE team, four other teams have adopted this system and we have more looking to\nget their own heroes. Future Plans Since this has been a huge success, we’re looking at moving away from using\nZapier as the logical glue and moving towards integrating this into a chat bot\nthat we use for other business functions. As mentioned earlier in the choice for\nusing Zapier, it’s will be a fairly easy component to replace. If you have any questions or are thinking about implementing ways in which you\nwant your engineers to focus, please reach out to me on Twitter\n@jdowdle . https://www.washingtonpost.com/news/inspired-life/wp/2015/06/01/interruptions-at-work-can-cost-you-up-to-6-hours-a-day-heres-how-to-avoid-them/ ↩︎ http://blog.ninlabs.com/2013/01/programmer-interrupted/ ↩︎ https://www.washingtonpost.com/news/inspired-life/wp/2015/06/01/interruptions-at-work-can-cost-you-up-to-6-hours-a-day-heres-how-to-avoid-them/ ↩︎ http://blog.ninlabs.com/2013/01/programmer-interrupted/ ↩︎ Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-09-30"},
{"website": "Invision", "title": "Monitor NOT All The Things", "author": ["Brian Blocker"], "link": "https://engineering.invisionapp.com/post/monitor_not_all_the_things/", "abstract": "git commit -m \"Initial Commit\" “Why does Support know about issues before we do?” asked the CTO. “A better question is, why do customers know about issues before we do?” A series of e-mails followed the conversation that gave some more insight into what he was thinking. He knew the answer, and he knew that we didn’t. Almost all of us had written what we would have considered “enterprise” software. We had cool tools like New Relic and Kibana that we would use during an incident to help point us (eventually) at the cause of the problem. We were able to (eventually) fix whatever was buggy or broken and get users back on track. What we were failing at, however, was getting ahead of users . We needed to get ahead of users. git history Engineering at InVisionApp is awesome. We’re a design-driven company. For companies that use prototyping tools, like… ours… it’s a wonderful thing. The projects we work on have already been designed and prototyped and tested and everything else before it comes to us, so we get to do what we love to do: build all the things . We aren’t building prototypes that get tossed, we build production systems that get used by millions of people immediately. It’s exciting, it’s challenging, it’s rewarding, and did I mention it’s challenging? It’s rewarding. But it’s challenging. Like any codebase, over time it gets bigger and more complex and creates new dependencies and things break. Like every good developer dreams of doing, we write tests so we can be aware of code that will break things if it gets merged, we use linting to make sure all of our tabs|spaces|parens are aligned the same across the entire organization, etc etc etc. But we arrived at a place where our tests weren’t catching everything. Systems would go down, bugs would be introduced, something would get used in a way that wasn’t intended and… uh oh. The worst part was that we were told when things were broken by our support team… and they were told by our users. We needed to get ahead of users. git commit --amend -m \"Figuring out our flaws\" Back to my conversation with the CTO… “Why does Support know about issues before we do?” asked the CTO. “A better question is, why do customers know about issues before we do?” “I think we just need to do a better job of looking at the logs.” I said, faking confidence in my response. “That will tell us what happens after users are experiencing issues. What we need to do is implement monitoring and automatic alerts so that we can know about issues before users experience them.” the CTO said, with actual confidence, backed by experience. This was frustratingly profound. How could I have missed something so simple? Of course we should setup automatic alerts on our monitoring systems. The picture of what needed to be done was becoming more clear. With lots of confidence, but little experience, I thought to myself “I need to monitor all the things !” git commit -m \"Initial monitoring setup\" Now here’s a caveat: you can’t monitor all the things. You need to identify the value that XYZ service delivers to users, and setup your monitoring to ensure that this value is being delivered. It took us some time to figure out exactly what that meant and how to do it, but like anything in life, it’s more about the journey than the destination. At first, we made it rain monitoring on everything we could think of. My team is responsible for commenting throughout all of our various systems, so we created graphs in DataDog for things like: Comments added from Share links Comments added from Console Comments added from Inbox Comments added via quick replies Comments added from anonymous users Comments edited Duration of calls to create comments Duration of calls to delete comments CPU usage of the database Etc, etc, etc We had a lot of data. We overlayed the deployment events on top of these graphs so we could see how a deployment affected the timings and counts of our metrics. It was interesting to see how our code changes affected (positively or negatively) the performance of our API calls or error counts. We felt like we were on to something finally. git commit -m \"Initial alerting setup\" Looking at ALL of the data at once was a bit overwhelming. Over time we discovered (through a bit of trial and error) that we had a few key metrics that could tell us before users were going to have a negative impact, and some metrics that could tell us when code changes caused users to experience a bug in real time . Specifically, we were looking at: CPU usage of the database Error rates We setup automatic alerting for when CPU or error rates exceeded a certain threshold. This was efficiently sufficient for us. There’s no way we would have thought these two metrics would have been all we needed without going through the process and looking at all of our options. We leveraged DataDog’s alerting system to talk to PagerDuty and, in general, it’s been wonderful (there have been a few times where the CPU spiked and it didn’t have any effect on our systems, but this is rare). git commit -m \"Summary of all the things\" Here’s the catch. You’re never done with monitoring/alerting. We have identified key metrics for now, but they will change and evolve over time. What’s valuable to monitor today might not make sense tomorrow, and we may discover other metrics that better represent the user experience. Like most things in life, it’s a journey, and not a destination. Monitoring the right things has become the focus of our journey. We don’t always get it right the first time, but a focus on building with monitorability in mind has fundamentally shifted how we think about operating our services in production. We have learned to focus on a few key metrics that help alert us when users are experiencing issues, but more importantly we are now able to detect and resolve issues before users experience them . What would it look like if you knew about issues before your customers did? Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-10-07"},
{"website": "Invision", "title": "InVision Achieves SOC 2 Type 1 Examination and PCI Compliance", "author": ["Johnathan Hunt"], "link": "https://engineering.invisionapp.com/post/soc2-type1-examination-and-pci-compliance/", "abstract": "We’re excited to share with you today that we have successfully completed a Service Organization Controls (SOC) 2 Type 1 examination related to the Security principle of the American Institute of Certified Public Accountants (AICPA) Trust Services Principles (TSPs). The third-party examination, completed by Schellman & Company, Inc., covered the design and effectiveness of Opower’s data security controls over the Enterprise and private cloud offerings of InVision’s award-winning design collaboration platform. The SOC 2 examination attests to the effectiveness of controls a company has in place that relates to the trust principles of security, privacy, confidentiality, availability and processing integrity.  We are committed to delivering the highest level of service to our customers. Completion of the SOC 2 Type 1 gives our more than 2 million users the assurance that we have established processes and practices that are designed and focused on protecting and securing our customer data with industry best practices and world leading security technologies and tools. Additionally, we are proud to announce we have achieved compliance with the Payment Card Industry Data Security Standard (PCI DSS) version 3.1 for projects.invisionapp.com. PCI DSS 3.1 is a stringent standard of technical and operational requirements set by the PCI Security Standards (PCI SSC) to protect cardholder data.  The standards apply to all organizations that store, process or transmit cardholder data.  The standard consists of 12 requirements focused on securing your network, system, application and environment against threats, vulnerabilities and compromise. By exceeding the expectations and requirements of SOC and PCI, we endeavor to hold ourselves to the highest standards.  We are committed to improving the lives of designers everywhere and by enhancing our security, we are putting the protection of our customers – and their data – above all else. Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-11-02"},
{"website": "Invision", "title": "InVision Launches Public Bug Bounty Program with Bugcrowd", "author": ["Johnathan Hunt"], "link": "https://engineering.invisionapp.com/post/bugcrowd_public_bug_bounty/", "abstract": "InVision has embraced an aggressive approach to vulnerability and risk management. We have always effectively and successfully managed the fundamentals including monthly external and internal vulnerability scans, a standard 30-day patch cycle with out-of-band exceptions for critical and high-risk vulnerabilities, static and dynamic analysis code scanning, subscriptions to vendor and media notification lists and more. Covering even minimum best practices, security teams become quickly overwhelmed with rapid company growth, legal and regulatory standards, an increasingly complex service offering and a growing threat environment.  The result can lead to an organizational compromise or worse. The Identity Theft Resource Center reported 845 breaches to date in 2016 and near 7,000 over the last 10 years, but if we’re being honest, that’s likely under reported. I think it’s more realistic to say the number is 2x that or even 10x. Despite InVision’s sizeable security team with decades of experience, education, and security certifications, we have opened our doors to invite the world’s best and brightest minds to hack away at InVision—and make money doing it. Today we’re launching our public bug bounty program with Bugcrowd as our next step toward improving the security of InVision’s systems and services. Our vulnerability reward payments will go up to $1,500 USD for each submission accepted, depending on impact and severity. Bugcrowd specializes in bug bounty programs for some of the world’s most trusted brands including Tesla, Mastercard and Fiat-Chrysler. They employ some of the best security experts in our field with a team that can easily manage the volume of submissions we see here at InVision. We are very excited about this opportunity. If you want to test your hacking skills with an opportunity for financial reward, here is the information you need: Bugcrowd InVision Program Page Site: projects.invisionapp.com Built with: ColdFusion, Java To learn more about our public bug bounty program, visit Bugcrowd’s blog here . Happy hacking! Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-11-09"},
{"website": "Invision", "title": "Making Time for the Good Stuff", "author": ["Kirby Frugia"], "link": "https://engineering.invisionapp.com/post/making_time_for_the_good_stuff/", "abstract": "We all get really busy and when we’re really busy, it’s easy to get into reactive mode.  Answer that @mention in slack, make decisions in meetings, or jump on quick ad-hoc calls.  This can feel really productive since you’re keeping things moving forward.  However, when you reflect on yesterday’s achievements, can you even remember what you did?  Do you find yourself looking at your calendar to see what you accomplished for the week? That’s no way to live your life. You’re never going to have a significant impact operating this way.  Sure, you can get things done.  You might even be seen as one of the most productive people around since you are always on top of things. However, if you’re working this way you’re artificially limiting your impact. But that’s not the best work you could be doing even if it seems that way sometimes. Your biggest potential impact comes from doing creative work.  If that’s not the case, you need to find a different job. And while inspiration comes in bursts, creativity takes time.  You have to cultivate it and give it the room it needs to breathe.  There’s nothing worse than a spark of inspiration that gets forgotten or remains unexplored.  That’s your best stuff and you’re wasting it. Creative work and interruptions What do I mean by creative work?  Creative work is the work that is setting you and those around you up for future success rather than just success in the moment.  Maybe it’s a blog post that you think could be useful to others.  Maybe it’s an idea about how your team can do something better.  Maybe it’s preparation for a 1-on-1 or a review. If you’re an software dev, maybe it’s a refactor that you’ve been wanting to do for a long time but never seem to get to.  Whatever it is, it’s important and it’s these kinds of things that often slip when you’re living day-to-day. How do interruptions kill creativity? One limiting factor is context switching.  Every time you get interrupted, your brain has to do a reset to get you into the right state of mind to deal with the interruption.  Some people are good at this, but most aren’t.  And what happens to the work you were doing before the interruption?  In the best case, it takes you quite a bit of time and energy even to get back to where you left off. In the worst case, you totally forget what you were doing or jump on something else and you end up not being your best, creative self. Realistically, interruptions are going to happen. I once measured how many people I spoke to in slack in a two week period. The answer?  Fifty people.  Fifty.  That’s not even counting the walk-up conversations, ad-hoc meetings, planned meetings, etc. I bet if you did a similar measurement, you would see similar results. That’s the world we live in now. If people respect you, they’re going to seek you out. As you build your career and have a greater and greater impact, you get pulled into more and more things. What can we do? The answer is actually really simple. Ok, sit down because this is going to blow your mind.  You have to make time.  Are you amazed with my super inspirational thought? Seriously, though, there’s a key word in that sentence that you shouldn’t overlook: make. You can’t be thinking along the lines of “I have some time later in the week where I can do that important thing I need to do.”  Gaps in your schedule fill up.  People need things.  Stuff happens. Making focus time You have to block off time.  Create two to three hour slots in your calendar and let everyone know that you’re going to be inaccessible during that period.  Log off.  Seriously, log off.  Don’t peek at your email. Don’t look at your slack notifications. Ok, so I do cheat a little.  I let people know that they can text me or call me if an emergency comes up. You might be thinking, but didn’t you say you had fifty people who needed something from you in a two week period?  Won’t those fifty people actually need to get in touch with you?  Turns out, the answer is usually “no”.  We have an expectation of instant answers today, which can actually stunt the growth of the people around you.  Think of how we used to navigate when we were going to unknown places.  You’d get some basic directions like “turn left at the big red barn.”  You had to be aware of what you were doing even if you had directions.  These days, with GPS, it’s too easy.  I find that it takes me many more trips to learn a route.  The same is true when people get instant answers.  They learn less than they would if they have to figure things out sometimes. It also helps to have a supportive environment if you’re using this technique.  I’ve never had a problem when I let people know I need focus time.  People get it and they may even start doing it, too.  And if you don’t have a supportive environment?  Again, you might want to think about finding another job that recognizes what people need in order to be productive and creative. Pomodoro Another idea I’ve explored is the pomodoro technique , in which you break your work down into short, measured intervals.  It’s definitely worth trying, but I couldn’t sustain it.  I tried it for a while and it can definitely be useful for accomplishing small to medium tasks.  However, I don’t believe it gives you the time you need to really be creative. Not to mention the pressure of the clock. You may disagree and it’s definitely worth a try, but I’m a much bigger fan of bigger chunks. Don’t lose the spark Finally, I love the idea of jumping on inspirational ideas right away when you have them.  In fact, that’s what I did when I wrote this blog post.  However, sometimes you really don’t have time to act on an inspirational spark in the moment. I keep a running list of inspirational ideas that I can explore during my creative blocks.  Inspiration can strike at inconvenient times, but you don’t want to lose it.  So get a notepad or throw the idea in a text file or use a note taking tool.  The point is, write it down and provide enough context that you will remember what you need to remember to get all creative on it. I’ve found that these techniques have gone a long way to increase my satisfaction and help me be more productive.  That being said, I don’t consider this to be a solved problem for myself.  I still struggle with it. I would love to hear your ideas, so if you have more ideas you’d like to share, please drop them in the comments below. Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-11-18"},
{"website": "Invision", "title": "Automated Mobile Testing with Docker and Codeship", "author": ["Jeremy Wight"], "link": "https://engineering.invisionapp.com/post/automated-mobile-testing-with-docker-and-codeship/", "abstract": "Automated mobile functional testing is hard. When you support a plethora of devices, OS versions, screen sizes and complicated math for positioning it can be very difficult to automate the validation of your application in a reliable way. So for a while we didn’t… Gasp! Hiss! I know… As an Engineering Manager, I had some specific requirements if we were going to go down the path of investing the development time and effort into setting this up properly. We knew there was a better way, but hadn’t the expertise to execute on it. When Senior Quality Engineer, Manju Kasireddy joined the team she believed that she could accomplish our objectives and get us to a place where our developing engineers could add to a suite of automated functional mobile tests running on emulated devices in a way that didn’t interrupt their development workflow. Hope was alive to get us back to a better place. So we began with my requirements: Tests must be effective We can trust that the tests reflect what a user is actually seeing Tests must be able to be added by developing engineers without any gotchas Pull down a Docker container with all dependencies and start adding tests in a familiar language Test runs must be automated Execution must be a part of the workflow with failures alerting the team Manju accepted these requirements and began planning out a better way forward… The Solution A Docker container built off of a GitHub repo written in JS that could be run locally with all dependencies encapsulated. A developer can add and trigger tests locally, which execute in a mobile emulator running on SauceLabs and report back to the container. When a developer completes their work and commits to the repo, CodeShip CI triggers a build and executes all tests on SauceLabs and reports the status to our #mobile-team-build Slack channel. Test Framework Stack The test framework is built to work inline with the rest of our application development stack, and hence built with Mocha , test framework and Chai , assertion library with Appium and WebDriver as the underlying tools to drive the user actions on Mobile and Desktop Web platforms respectively. The test repo is built as a Docker Image just like the rest of our services, so that test developers can pull in the container with all dependencies encapsulated. Handling the Requirements Tests must be effective One of the difficult things about mobile functional tests is that all of the different device types, OS versions, and browser versions adds a lot of test variables. For our tests to be effective, we needed to be able to verify on the most frequently used devices that what our tests showed us was what the user would see. After investigation into the different possible tools, Manju recommended SauceLabs due to the cost, service and tools which fit with our needs. SauceLabs allows for you to dynamically generate emulated device tests with your defined OS version and execute your functional test code. It gives you a lot of information about the test run, including: a video screencast, screenshots of points of failure and logs at each point of test code execution making debugging easy and clear. To complete end-to-end testing on this product, we needed to be able to automate both mobile and desktop web interactions. The mobile actions are automated using appium.io, which runs iOS, Android and Windows apps using the Webdriver Protocol. The desktop actions we automated using webdriver.io, which is a Node.js binding library of Selenium 2.0. The end result is that by using these tools, we were able to automate the user actions end-to-end and see the functional tests executing to validate that they are effective. Requirement pass! Tests must be able to be added by developing engineers without any gotchas In a startup environment developers need to be shipping working functional code as frequently as possible. In my experience, when adding functional tests is cumbersome, it typically gets abandoned as a process or test become stale and do not accurately represent what the user of the app is seeing. Thus, it was imperative that adding to the functional tests could be done without a brittle environment. Also, developers need to be able to get the tests going on their machine and keep them running effectively without assistance from Manju or any other Quality Engineer. That meant that we needed all of the special test framework setup considerations encapsulated in the Docker container so that when a developer created a new piece of functionality or updated our application, they could pull down the container and have simple steps to begin adding to the test suite. The majority of this repo is written in JS since it is a familiar language for every developer and SauceLabs tests can be executed from within the Docker container, removing as many gotchas as possible. Gotchas contained, let’s automate! Test runs must be automated In most cases test automation would be a simple task with Jenkins. Since nearly all test frameworks integrate with Jenkins for most setups this would not be a major consideration. However, when we switched from our Vagrant/Chef based system to Docker/Kubernetes we also shifted away from Jenkins CI to CodeShip CI. Since CodeShip is so new there was no available documentation on triggering SauceLabs tests from Docker using CodeShip, but Manju was up to the task. Manju added in the integration within CodeShip so that when a new commit is made to the GitHub repo, our CodeShip CI takes over and begins executing the tests found in the repo. We added a slack webhook to our team-builds channel which reports back on the success or failure of that CodeShip build closing the loop. If a build fails, everyone on the team is aware and we know that we need to look at the tests or review the SauceLabs screencast to determine the point of failure. Conclusion With Manju’s help we were able to overcome the struggles of automated mobile functional testing. We are now in a much better state and all of our engineers add to our test suite without pain! It’s an amazing place to be! If you’ve got any questions about mobile test automation with SauceLabs , Docker , and CodeShip leave a comment below! Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-12-02"},
{"website": "Invision", "title": "Schema Migration", "author": ["Brad Brewer"], "link": "https://engineering.invisionapp.com/post/schema-migration/", "abstract": "At InVision, we manage dozens of database schemas across hundreds of instances.  Some of these schemas contain hundreds of millions of records and live on large MySQL clusters while others are small, brand new services with fledgling databases.  Whether your databases are large or small, schema migrations play an integral part in any rapidly growing platform. The InVision engineering organization continues leveraging microservice architecture more and more, resulting in parallel development that screams for automation.  It isn’t uncommon for the Data Services team to provision a dozen new database instances in a day. With the paralleled development of numerous engineering teams frequently updating their respective services, hands on DBA schema management becomes a choke-point for velocity, something our fearless CTO ensures we don’t lose sight of. If you aren’t familiar with the concept, schema migrators are tools that serve to manage incremental changes to relational database schemas. What we created: A schema migrator that leverages existing infrastructure built on top of docker/ kubernetes , automatically deploying database migrations to all environments.  We did so with the following goals in mind: Empowering engineering teams to own their database changes Well-tracked schema migrations Reproducible and predictable migrations across all environments Management of seed data - a high priority when your engineering team is growing rapidly, and forging a new code-base every week! Automated migration deployment from development to production How does it work? Make the schema changes to your local MySQL server Run  [create-my-changeset.sh]   -  This step executes a schema diff between your local environment and the pristine state of the database, producing a file that can be submitted as a pull request Submit the PR for review Upon Data Services review/approval, the engineer who initiated the change can deploy.  This step spins up a dedicated migration container that will apply these changes to all environments. What went well? Developers are now empowered to initiate and deploy data migrations via an automated channel the Data Services team maintains.  This provides the greatest possible velocity for developers, as well as oversight into production database changes for the Data Services team. We used off the shelf open-source tools, focusing our time on composing our own application logic to stitch those tools together to fit our specific needs.  Specifically, we leveraged Liquibase for basic migration management and tracking, and Percona Toolkit to ensure our migrations could take place without downtime. You might be wondering: “Brad, you mentioned tables with hundreds of millions of records.  If you’re going to alter these giant tables, some of which are on master/slave pairs, what about replication lag?  Don’t some of these schema changes lag the slaves when your fancy automated deploy executes your database changeset?” Typically, no.  One of the more exciting elements of our schema migration tool is that we stood on the shoulders of giants, leveraging Percona’s pt-online-schema-change tool to handle large-scale changesets.  Whether you’re adding a new column to a small table, or changing the data type for a 500 million record table, our tool will handle those changes with poise and grace.  Though, the latter will take some time, as the migrator works diligently [via ptosc —max-lag] ( https://www.percona.com/doc/percona-toolkit/LATEST/pt-online-schema-change.html#cmdoption-pt-online-schema-change--max-lag ) to keep the slaves up to date. “The DBAs where I work are subject matter experts that see themselves as gods over all things databases, your fancy automated migrator would never fly here.” Sure it would!  The PR process for schema changes provides a well tracked pipeline for developers to propose changes to us in Data Services, while allowing us to approve potentially impactful changes to our large production databases. “Aren’t you automating yourself out of a job!?” Not at all!  The more we automate, the more we can accomplish.  In addition to automating common tasks (of which there is no shortage!), the Data Services team is always available, and often sought out by our engineers, to consult with our internal engineering teams on everything from initial architectural decisions to query performance. That’s it! That’s how we scale schema migrations at InVision.  If you have any questions/comments, please share! Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2016-12-16"},
{"website": "Invision", "title": "Seattle Go Meetup: Middleware in Go, InVision Rye", "author": ["Cale Hoopes"], "link": "https://engineering.invisionapp.com/post/invision_rye_at_seattle_go_meetup/", "abstract": "I got an opportunity to represent InVision Engineering at the January 2017 Seattle Golang Meetup with a discussion about Middleware in Golang, and our open source project Rye which brings yet another player to the middleware space.  The meetup took place on January 10th, in the Tune space which used to be Seattle’s iconic Belltown Billiards. The meetup featured mostly enthusiasts of Go from the greater Seattle area. Presentation Talks The first talk that was presented was two representatives from Microsoft discussing Visual Studio Code and the Go plugin. After that, I presented a 35 minute talk that did an overview of what middleware is in modern web applications, how to build your own middleware, overviews of Negroni , Interpose and Alice and a full overview of the Rye middleware we’ve built here at InVision ( https://github.com/InVisionApp/rye) . The presentation went over how to use Rye to easily grab metrics from your endpoints, support CORS, JWT and CIDR validation in addition to doing Route Logging. The talk was wrapped up with a challenge to the audience to look through the space and make some choices in addition to pushing your own company into open source. The talk was met with some great enthusiasm and some excellent questions. We were able to interact with the community here and it looks like Seattle’s Go enthusiasts are growing and excited about where Golang can take them. Personal Benefit Additionally, for a remote worker I was able to meet up with three other teammates face-to-face for the first time! The meetup was a great success and I look forward to seeing InVision getting out in the community and talking about our great engineering and excellent company culture. InVision is in production with Go APIs that get deployed with Docker and Kubernetes. We have a great engineering culture that is actually doing a bunch of the things that are being talked about as “migration” at other companies. We’re not fully mature yet in all the greatest technologies, but every day we move closer and it’s a great place for a tech career. I’m loving it here. Personally, it was a chance for me to get an opportunity to practice my presentation skills. If you have the desire to potentially speak at some of the larger conferences, I suggest participating in a local meetup. It affords you the chance to practice those skills and hone them for being an excellent conference speaker. You’ll quickly figure out how to make compelling slides, how to present, what tools you need for an emergency and how to convince the audience of your subject matter. Just do it! Come Contribute Finally, I’m really hoping that Rye can become an opportunity for you to bring your ideas and contribute to our project. There are many more middleware additions we could build into Rye with your help! Please come and submit a Pull Request on GitHub of your favorite middleware ideas and we’ll grow the base functionality in the library! We will be gentle! This is a great opportunity to get your first interaction on a public open source project. There are so many simple things you could add to the library in an isolated way. Get your feet wet with open source and even with Golang! Presentation Media If you’d like to check out our presentation you can find it here on YouTube . Additionally, here are the slides on SlideShare . Questions Have any questions about Rye, InVision, Golang, or any subject, feel free to reach out to me on Twitter . And remember, we’re hiring great engineers! Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2017-01-13"},
{"website": "Invision", "title": "Live Webinar: Reconsider Bug Bounty Programs", "author": ["Ryan Scheuermann"], "link": "https://engineering.invisionapp.com/post/live-webinar-reconsider-bug-bounty-programs/", "abstract": "Bug bounty programs have been around since the 1990s, but it’s only in recent years they’ve gained recognition with high profile companies like Google, Facebook, and Microsoft offering lucrative rewards to researchers or hackers who find and report bugs, exploits, and vulnerabilities. InVision launched our first managed bug bounty program with Bugcrowd in November 2016, and now we’re teaming up with Bugcrowd’s SVP of Marketing, Paul Ross, to discuss why these programs are critical to any organization’s security program. There are many reasons you might not have a bug bounty program today, including fear of exposure, the cost, or the belief you can handle this on your own. We had the same reservations. Join our very own VP of Information Security, Johnathan Hunt, as he outlines: How he changed his mind about bug bounties Why you’re missing vulnerabilities without a bug bounty How Bugcrowd finds a P1 bug every 27 hours Register for the webinar here We will be answering your questions live on Wednesday, January 25th at 12:00PM Central Time . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2017-01-19"},
{"website": "Invision", "title": "Mobile Conversations: A Journey in Product Development", "author": ["Jeremy Wight"], "link": "https://engineering.invisionapp.com/post/mobile-conversations-a-journey-in-product-development/", "abstract": "Sometimes you make the wrong choice and have to throw away substantial investment for the sake of a better user experience. At InVision we have a big vision of creating the tools that are shaping the future of our world. By equipping designers, developers and makers, we are contributing to that future by providing the tools to design and develop it rapidly and collaboratively. As we continue to mature as a product, we are constantly attempting to better address our users pain, and mobile-done-right can be a key contributor to addressing the pain felt during rapid design collaboration. This year we have begun to re-imagine our mobile experience, and what it would look like to take it from a mere prototyping sidekick to a key in the design collaboration workflow. One of those keys, is our newest tool Conversations . Building software is hard, and Conversations was no different. Properly addressing our users pain around mobile collaboration was not an easy process, from both a technical and product standpoint. Sometimes, your first attempt misses the mark We had a beta version of Conversations for several months, which, mirrored the existing Inbox and Commenting systems in our web application. Mistake: Assuming what works on the web, will also work in mobile. As we were initially developing, we tried to fit into mobile what appeared to be working in the web app. This seemed like a good method to quickly deliver value and a consistent experience to our users, and we executed on it quite rapidly. We were very excited when it initially launched and we had given our beta users a way to aggregate their commenting around a prototype into those threads. [Digital] High fives all around, we shipped it! Unfortunately, we failed to step far enough back and really evaluate what the need behind the need was, and how we could best address this in the mobile context. Our new Product Manager, Jason Lang, stepped in just before we delivered the initial beta. With his deep experience in mobile and enterprise messaging, he began to listen, question, and dig deeper into the value that we could deliver on a mobile platform. It did not take long for Jason to dig far enough before the solution became clear. By reframing the focal point of the user interaction around the user, rather than around the prototype, we could provide a much better experience. It sounded great from a user perspective, however at the time, this was frustrating as an engineer. We had invested a not-trivial amount of time into the current version, and the new concept (which would become Conversations) required a lot of rework: API endpoints to deliver data in different ways, a good deal of background logic and a very different UI than before, which required new testing patterns, research, etc, etc… Although as an engineering leader I was frustrated, as a user advocate and product developer, I realized that Jason was right. We could deliver a much better experience to our users, who we now understood better, by halting further work on the Inbox Beta and beginning the work for Conversations. The new Conversations …  Design Collaboration. Anywhere. As a developer, whenever we realize that we can move from an experience which is sufficient, to one that will delight, even if it means sacrificing something which you have built, we have to be willing to sacrifice that time and effort to build the right thing. Although it took several more sprints of effort to get right, Conversations is a much more user-centric experience, which brings collaboration to a near real time experience. We hope Conversations delights as much as we expect it to, check it out in the App Store, InVision - Design Collaboration . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2017-02-01"},
{"website": "Invision", "title": "InVision Home Offices: What Working Remotely Looks Like", "author": ["Heather Roberts"], "link": "https://engineering.invisionapp.com/post/home-work-station/", "abstract": "Working remotely is growing in popularity all over the world. The Bureau of Labor Statistics data in 2015 showed that 24% of employees reported doing some of their work remotely, up from 19% in 2003. Personally, I love it and can’t imagine going back into an office setting. I’ve made my home office into my own little oasis, a place I feel comfortable being in for hours at a time. Curious how others have set up their home office, I asked my colleagues at InVision to submit a photo of their workspace with a few words on why it works for them. Since we are now over 300 team members strong in 19 countries around the world, the responses varied but provided a fun peek into the lives of my amazing team. “Design makes everything possible” – Brandon Wolf , VP, User Enablement There are three things I find important when setting up a home office: the lighting, your chair and your desk. As you’ll see below, lighting can be done in many ways while chairs and desks are not as simple as they used to be. “I am on the road a lot for work so when I finally get to be home, I like my home office to be as peaceful and comfortable as possible. I rarely start my day without a candle lit and a hot cup of coffee.” – Michaela Alexander , Marketing Events Manager “I’m a big fan of my office. It’s connected to my bedroom so the commute is wonderful. It also has a ton of sunlight, so it really helps me enjoy the day and weather, even if I don’t step outside. Not pictured: the cat tree, where my\nthree cats hang with me all day and look adorable.” – Lindsey Redinger (Campbell) , Product Manager Let there be light If your office has windows, natural light is always the best way to go. According to a 2014 study at Northwest University ,  office workers with natural light exposure experienced better sleep, more physical activity and better quality of life. In addition, natural light keeps you alert and the sunlight helps warm your office, which makes it a more comfortable space. Beware the glare, though. Make sure your desk is situated so the light enhances your work area and doesn’t distract from it. “I couldn’t live without the daylight or my Bluetooth speaker! My Homer Simpson Buddha on my desk reminds me to remain calm and keep a clear head, especially on intense or stressful days.” – Emily Flannery , Front-End Marketing Developer “Working remotely gave me the ability to surround myself with windows and loads of natural light. I alternate between sitting at my dining table and standing at a desk to keep my back strong. Keeping the space clean helps me clear my head and stay focused on tasks for the day. The less distractions, the better!” – Andrew Mattock , Senior Marketing Automation Manager “For me, an ideal work station is all about being a safe distance from the snacks in the pantry and having tons of natural light.” – Beth Vanderkolk , Support Documentation If your office has no windows, take steps to ensure the light bulbs you use are the right ones. Poor lighting can affect your mood, your quality of work, and can cause headaches if you’re straining your eyes. LED’s are far superior to the unhealthy fluorescent lights in most offices today, plus they are inexpensive to use and can last for years, meaning less landfill. “I live in a studio apartment so space is limited. This space doubles as a desk during the day and entertainment station at night. My puppy Sis sits near my feet when she isn’t playing outside.” – Jessi Thorp , Event Program Manager “My Colorado home office is the perfect place where chaos meets OCD organization. It’s the place that keeps me sane during the constant crazy production and planning between traveling to InVision events. I can hear the creek babbling in my backyard and my dog Grendel Pants shadows me throughout the day.” – Sadie Short , Events & Sponsorship Manager Home office desks and chairs When you spend 9+ hours a day working in the same space, your desk and chair matter. It’s easy to just throw a laptop on your dining table and work from there, but your productivity may suffer. Sitting on a kitchen chair isn’t so bad during dinner but after six or seven hours, your bum may protest. Having a designated area to work, and designated tools, make for a more successful experience. “My battle station with way too many devices, gadgets, and toys.” – Jonathan Rowny , Engineer Sitting desks Sitting desks are by far the most commonly used at InVision. They’re easy to find and inexpensive. There’s some debate as to if they’re the healthiest choice, as sitting for long periods of time lowers your heart rate and can lead to weight gain. Users of sitting desks tend to hunch over without knowing it, putting a terrible strain on the neck, shoulders and spine. A good chair helps with that, but it’s something to consider when deciding on what desk works for you. “That’s my bed on the right. I wake up, roll up 90 degrees to the side, and boom, there I am in Zoom having meetings with my team.” – Steven Fabre , Product Designer “I use this office during the week with my assistant, Harley. On the weekends my partner uses it to play games. We couldn’t find a desk large enough, so we settled on an Ikea dining table.” – Sam Raphael , Customer Advocate Standing desks In recent years, the popularity of standing or stand-up desks has soared, particularly for those of us in tech. They allow you to raise or lower your workspace so you can switch between sitting and standing during your day. Studies have shown that a standing desk can reduce upper back pain by over 50% . If you already suffer from back pain and will be working from home, a standing desk may be your best bet. Studies have also shown that you are more focused and productive if you move around throughout the day, as it helps to expend energy and maintain focus. “After years of chronic shoulder pain from working in a cube, ergonomics became top priority when building my home office. Having the option to switch between sitting and standing has alleviated my shoulder issues. I also try to surround myself with non-work things for wee breaks to help maintain focus: racing wheel for a few hot laps, my bass for jamming, and seating for my two cats who insist on being collaborators.” – Mike Waecker , Project Manager “A desk so high the cat gives up!” – Dana Lawson , VP of Engineering “This “office” is a little nook in my bedroom. It’s nice as it’s right next to the backyard, so it gets lots of light. This way I get a beautiful view (lots of hummingbirds!) and I show up well-lit on calls. I bought a beautiful desk that I love, but sadly after one week I was hit with awful lower back pain. I then decided to buy a standing desk add-on.” – Aliisa Rosenthal , Director of Enterprise Accounts Office Chairs There are many types of office chairs and it can be hard to select the right one. The most important part? That it’s the correct height for the desk you’re using. Not sure what works best for you? Try this desk height calculator to determine what height will work for your desk and your home space. “The benefit of working remote #24: feeling your music thumping in your chest instead of in your earbuds.” – Payam Rajabi , Product Designer “I like to keep it neat and simple!” – Leslie Carillo , Support Options for an office chair range from some that look like bicycle seats to others that are literally large exercise balls. They also range in price from $50 to well over $1,000. While it’s easy to grab whatever Costco happens to have, I encourage you to do your research. Read reviews on Amazon and Google, sit in lots of different chairs, swivel around, and adjust the height and arm rests until you find the one that suits you perfectly. “When I joined InVision, I wanted to work as comfortably as possible. I can’t sit all day without some pain so I alternate between sitting and standing. I’m starting to stand more and more, and find it helps me to prioritize stretching throughout the day. When transitioning to a sit/stand desk, I highly recommend getting an anti-fatigue mat to stand on. I also love being able to have the dogs with me all day keeping me company.” – Sara Dunnack , Cyber Security “For me, ergonomics is very important, which is why I have a sit stand desk along with my Wobble Stool. Lots of direct light, Wyatt the dog and all the monitors I can fit on my desk keep me happy. Sometimes I peddle on my spin bike, which gives me time to think.” – Vicky Couturier , IT Director “There isn’t much I don’t like about my home office. If I could change one thing, it would be to have more wall space. I’ve run out :)” – Matt Vickers , Engineer Inspiration Below are additional work space photos submitted by my fellow InVisioneers. Let them inspire you! One of the best things about working from home is you’re in charge of everything in it. You aren’t forced to endure uncomfortable, non-adjustable chairs, florescent lights, or your cubemate’s smelly microwaved leftovers. If you take the time to do your lighting right, and with your comfy desk and chair, you may not want to leave your home office at all. “I’m lucky I had an extra room to transform into my in-home office. It’s so convenient that it’s just a step away from my bedroom and more importantly, it gives Louie a perfect seat to watch people as they walk past our window.” – Ali Copriviza , Sales Development Representative “Your workspace is an extension of you, so always make sure to have a little piece of sunshine nearby.” – Erica Simmons ,  Team Lead, Support Engineering “(Overly) Well-Lit business in the front.” – Jonathon Wilson , Engineer “Party in the back.” – Jonathon Wilson , Engineer “After our third child was born, we needed more room in the house, so I decided to purchase a 2005 Carson Fun Runner and convert it into a full-time office. I’m using a Jarvis Bamboo Standing Desk with Topo Mat, which I love. I stand 2⁄3 of my day and sit on a Herman Miller Aeron for the other 1⁄3. I painted the back wall with chalkboard paint and use these lovely clocks as a decorative element to quickly reference teammate time zones. This has been a fun adventure thus far!” – Stephen Olmstead - VP of Design Partnerships Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2017-03-14"},
{"website": "Invision", "title": "Adapting to Serverless Computing with AWS Lambda", "author": ["Casey Flynn"], "link": "https://engineering.invisionapp.com/post/adapting-to-serverless-computing-with-lambda/", "abstract": "Recently, we at InVision switched our image processing from using a self managed traditional service to using AWS Lambda for better performance, throughput and reliability. Every day, InVision processes over 250K Sketch or Photoshop artboard images uploaded through Craft Sync. Our self managed service did the job, but it inherently came with maintenance and monitoring overhead, which was a drag on our development velocity. When we needed to make a major change, we took the opportunity to try a new approach: “serverless computing” with AWS Lambda. What is AWS Lambda? AWS Lambda has grown quickly since its launch in 2014. The service allows you to run code with instantaneous horizontal scaling to parallel process tasks without having to worry about operations or servers. Essentially, each invocation of your Lambda “function” is executed on a fresh EC2 instance with dedicated resources. Capacity provisioning, monitoring, deployments and logging are all handled via AWS. Thanks to a fee structure where prices are based on 100ms blocks of execution time over-capacity and under-capacity events are virtually eliminated with significant cost savings. Lambda has proven to be a simple and reliable way to perform event driven, scalable processing of our images. By switching to using Lambda, we successfully simplified our infrastructure, reduced our system monitoring overhead, and reduced our AWS bill. When we first started investigating Lambda as a possible solution, we had all of the typical questions of any team new to Lambda: How do we test our code? Can we replicate the AWS Lambda environment in tests? How will we deploy? How will we invoke Lambda? How will we be notified of Lambda completion? How will we handle monitoring and logging? How will we allocate memory, CPU cycles, etc? Eventually, through trial and error, we settled on a set of processes, tools and AWS configurations that met our requirements. Invocation Actually running your Lambda code is easy. AWS provides multiple “triggers” to invoke a Lambda function , such as: Events on S3 buckets (file uploads, file deletes, etc) A DynamoDB database operation An SNS message The key takeaway here is each invocation gets a fresh, dedicated EC2 instance. AWS will take care of running your code on as many EC2 instances as you can send triggers. Need to process 10 or 1,000,000 things in parallel? No problem, AWS will instantly scale allocated hardware to your processing needs. Our image processing system invokes Lambda via both S3 bucket event and SNS triggers. When our Lambda function is invoked via SNS or an S3 event, the message passed to the function contains all the relevant information to perform a task (such as image file location, mutation parameters, file final location, etc) When our Lambda function completes, it notifies other services by enqueuing a message in SQS. Resource allocation AWS gives you the option to select how much CPU and RAM are available to your Lambda function per invocation. You can select from 128MB to 1.5GB of RAM. CPU plus other resources will be allocated proportionately to the amount of RAM you select. Cost per 100ms block of execution time is correlated to resource usage. For example, 100ms of execution time at 128MB costs $0.000000208 and 100ms of execution time at 1.5GB costs $0.000002501 (12x increase over minimum). Key Takeaway: Depending on your processing needs, it may be cheaper if your Lambda function uses more resources but completes in less time. Deploying There are several ways you can update your Lambda function code. The most simple is to just edit the code inline with the AWS UI. This is useful when you’re just starting out and you want to test, but for any production-level SaaS product, like InVision, this doesn’t scale. To deploy our Lambda function, we settled on a process that uses our CI system as well as our automated deployment bot. When a feature branch of the repository that hosts our Lambda function code is merged into master: Our CI system runs the tests Builds a zip archive of our Lambda function code Uploads that to S3 as an artifact Then when we’re ready to deploy our Lambda function, we issue a command to our automated deployment bot in Slack, which in turn sends an API request to AWS to update the code of the Lambda function from the artifact hosted in S3. Testing One of our earliest concerns was how can we effectively simulate the environment of Lambda for testing and local development. We accomplished this with Docker and running our code and tests inside a container based on the lambci/lambda image from Docker Hub . This image mimics the AWS Lambda execution environment. For functional testing we run the Lambda function inside a docker-compose managed environment with simulated external dependency services such as S3 and SQS. The simulated external dependencies are simply linked containers running small servers with “canned” API responses to specific S3/SQS API requests. This approach allows us to build and test our Lambda function locally with predictable results without having to constantly deploy to AWS during development. Here are some key files from our testing environment setup Dockerfile FROM lambci/lambda\nCOPY functions/screen-resize /var/task\nENV NODE_PATH=/var/task docker-compose.yml app:\n\tbuild:\n\t\tdockerfile_path: Dockerfile\n\tcached: true\n\tentrypoint: npm\n\t#volumes:\n\t# - ./functions/screen-resize:/var/task\n\tenvironment:\n\t\t- \"AWS_S3_OVERRIDE_ENDPOINT=http://fake_s3.local\"\n\tlinks:\n\t\t- \"nginx:the-perm-bucket.fake_s3.local\"\n\t\t- \"nginx:fake_sqs.local\"\n\nnginx:\n\tcontainer_name: nginx\n\timage: jwilder/nginx-proxy\n\tadd_docker: true\n\tvolumes:\n\t\t- /var/run/docker.sock:/tmp/docker.sock\n\tports:\n\t\t- \"80:80\"\n\t\t- \"443:443\"\n\texpose:\n\t\t- \"80\"\n\t\t- \"443\"\n\tlinks:\n\t\t- fake_sqs\n\t\t- fake_s3\n\nfake_s3:\n\t# command: nodemon server.js\n\tbuild:\n\t\tdockerfile_path: ./functions/screen-resize/tests/fakeServices/s3/Dockerfile\n\t#volumes:\n\t# - ./functions/screen-resize/tests/fakeServices/s3:/var/www\n\texpose:\n\t\t- \"80\"\n\tenvironment:\n\t\t- \"SILENCE_LOGS=1\"\n\t\t- \"VIRTUAL_HOST=the-perm-bucket.fake_s3.local\"\n\t\t- \"PORT=80\"\n\nfake_sqs:\n\t# command: nodemon server.js\n\tbuild:\n\t\t# image: node:6.3.1-slim\n\t\tdockerfile_path: ./functions/screen-resize/tests/fakeServices/sqs/Dockerfile\n\t#volumes:\n\t# - ./functions/screen-resize/tests/fakeServices/sqs:/var/www\n\texpose:\n\t\t- \"80\"\n\tenvironment:\n\t\t- \"SILENCE_LOGS=1\"\n\t\t- \"VIRTUAL_HOST=fake_sqs.local\"\n\t\t- \"PORT=80\" We have our Lambda function code running inside the app container, and our fake s3 and sqs services. The nginx container takes care of routing requests within the integration testing environment. This setup is a reasonable local emulation of the AWS environment in which we can run our tests. Monitoring We need to know how quickly images are being processed in our system, and we need to be alerted if anything breaks. We explored several monitoring approaches when developing our Lambda based image processing system. We started by using the tools provided by AWS. Any output sent from your Lambda to stdout or stderr will be captured and stored in CloudWatch. You can also configure a CloudWatch Alarm to notify if invocations of a Lambda function report errors. The AWS UI will report information regarding error counts, execution times, and account limit invocation rate throttling. CloudWatch worked decently well, however we at InVision are heavy users of DataDog and we wanted to integrate our Lambda function directly with our existing monitors in Datadog. Fortunately, there is a Datadog-AWS integration for Lambda that makes it easy to get timing and count stats in your DataDog account from Lambda. We knew we wanted to avoid making API requests to Datadog directly from our Lambda function. Since Lambda billing is based on execution time, making an HTTP request to DataDog’s API would delay a Lambda function completion and add cost overhead. The Datadog-AWS integration allows you to send stats to DataDog from Lambda by simply logging to stdout in a specific format. For example: MONITORING|unix_epoch_timestamp|value|metric_type|my.metric.name|#tag1:value,tag2 The integration will take care of collecting your stats and your lambda function execution time wont be affected. To monitor our system, we use CloudWatch for inspecting Lambda logs as well as DataDog for performance stats tracking and alerting. We rely on a combination of stats reported from both our Lambda function itself, as well as services that invoke Lambda and ingest the output of our lambda. We alert if lambda execution times exceed our configured threshold. Conclusion The serverless computing processing model creates new opportunities to build more efficient, simple and scalable software. As with any new technology, understanding it can take time. Now that we’ve tested Lambda, we’ve found it to be a reliable and useful tool and we’re excited about additional opportunities to use it in the future. Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2017-05-05"},
{"website": "Invision", "title": "InVision Rye: Serving Static Files and Performing Magic", "author": ["Cale Hoopes"], "link": "https://engineering.invisionapp.com/post/invision_rye_and_static_file_magic/", "abstract": "I work on a services team. We don’t do websites, generally. Of course, this\ndoesn’t mean we don’t have a need for websites. But it is somewhat odd to hear\nthat a design-centric company has a small team of developers who really don’t\nwork on websites. We spend most of our time deep in the depths of RESTful API\ndevelopment or server-side multi-threaded processing. Now, granted, all of us on\nthe team have worked on websites in the past. We know how, we\njust don’t do it very often. The Problem… Recently, we were tasked with building a simple service to act as an overall\nservice catalog. As we built out the service-catalog, which would have some\ninteractions with a chat-bot ultimately, we realized that to interact with\nupdating and viewing data, we needed a front-end. This required much discussion.\nHere’s a team of non-website folks, arguing over building a site in various\ntechnologies. We all come to the table with some ideas, but we came to using\nReact because other teams in our company were starting to build out internal and\nexternal sites with React. Myself, I felt very comfortable with this decision. I’ve build a lot of things\nusing React and am very familiar with it. My most popular open source project\nwas at my last company and it was written for React Native. Never thought that\nsomething I worked on would have a whole 200+ stars on Github. Anyhow, I\ndigress. Building a website in React is not a hard task, in my opinion. Building\nit with all the Javascript dependencies, proper build, navigation, data\nmanagement, and all the extras can be another story, however. Deployment can be\ndifficult. NPM packaging and versions can be difficult. Building it as a\nsingle-page application can be difficult. And ultimately, serving it can be\ndifficult. The Desire… What we wanted was this: we wanted to serve our single-page application from our\nGolang written API server. PLUS, we’d really like to not have to copy assets\ninto our Docker container that relied on any other dependencies… like node_modules or npm or node or anything that wasn’t just the executable\nsitting in an Alpine Golang docker image. This is what we wanted, and this is\nhow we solved it: using InVision Rye, serving static files and then figuring out\nhow to bundle those static files into one Golang executable. The Approach… So, we went about creating a single-page Application using React. This required\na metric-ton of named Javascript projects. We setup NPM, then created a webpack\nconfiguration, then built out an index.html that would load our single-page\napplication. Of course we needed some npm commands specifically to build our\nReact application in Babel and then deliver a bundle.js to a dist folder.\nAdditionally, with React, we added react-router , and react-redux to manage\ndata interactions. On top of that we had static resources we wanted to deliver\nsuch as react-bootstrap which included fonts and images. Great! This is all\nfantastic and works great in a folder ~ as long as you have some way to SERVE\nit. For the time being, I would just use Python to serve it. Our Webpack was\nbuilt out to deliver all the assets in a single folder. One command pointed at your folder, and you’re off to the races: cd dist\npython -m SimpleHttpServer 8081 This worked pretty well for development. But at some point we were going to have\nto deploy this. Since our service, Astro, was written in Golang using the Rye middleware library, we needed to\nfigure out how to properly serve this from there. The plot thickens. Golang http.FileServer for delivering static files So, we needed to figure out how to easily deliver static files for our UI from\nRye. We also use Gorilla as a router,\nso getting the right mix was a big part of that. There were actually some tricky\nbits around this, in regard to serving out of a Rye handler. Golang provides http.FileServer() which was our first approach. Since Python’s SimpleHttpServer was working for the most part, we figured this would be\nsimple. However, not true. We kept having bugs with React-Router . You couldn’t\nnavigate to paths in our app using http.FileServer() . You could visit the\nhomepage, and even navigate in the app using our Bootstrap Nav Bar, but you\ncould not go DIRECTLY to a link. Why? Our Gorilla router was interrupting the\nrequest and not allowing React-Router to do its work. Finally, we realized that to get what we needed out of the single-page\napplication, we needed to: Provide a path to get static assets and a route that served those static\nassets through http.FileServer() Provide a path that on the server side would ALWAYS serve our index.html file which makes up the single-page application Since we were building this as an add-on to the service, we wanted our main-path\nto be /dashboard . Therefore, we decided that other static assets would get\nserved from /dist such as fonts, images, bundle.js , CSS, etc. The configuration for this, using Rye, ended up not being terribly difficult\n(found in static_example.go ). pwd , err := os . Getwd () if err != nil { log . Fatalf ( \"NewStaticFile: Could not get working directory.\" ) } routes . PathPrefix ( \"/dist/\" ). Handler ( middlewareHandler . Handle ([] rye . Handler { rye . MiddlewareRouteLogger (), rye . NewStaticFilesystem ( pwd + \"/dist/\" , \"/dist/\" ), })) routes . PathPrefix ( \"/dashboard/\" ). Handler ( middlewareHandler . Handle ([] rye . Handler { rye . MiddlewareRouteLogger (), rye . NewStaticFile ( pwd + \"/dist/index.html\" ), })) Two new middlewares We required a middleware that could always serve one file and a middleware that\ncould serve a whole filesystem. This led to: middleware_static_file.go and middleware_static_filesystem.go . Middleware Static File Taking in an absolute path local to the server, you can serve a single file.\nUsing the PathPrefix in the Gorilla Mux sample above, this allows you to have\na path ALWAYS load this single file. The handler is really simple: func ( s * staticFile ) handle ( rw http . ResponseWriter , req * http . Request ) * Response { http . ServeFile ( rw , req , s . path ) return nil } Middleware Static Filesystem Taking in an absolute path local to the server, you can serve a full path. Using\nthe PathPrefix in the Gorilla Mux sample above, this allows you to serve\nINDIVIDUAL files in that filesystem on that prefix. Another simple handler: func ( s * staticFilesystem ) handle ( rw http . ResponseWriter , req * http . Request ) * Response { x := http . StripPrefix ( s . stripPrefix , http . FileServer ( http . Dir ( s . path ))) x . ServeHTTP ( rw , req ) return nil } A small caveat… StripPrefix Since our static directory in the sample above is called /dist/ , we want to\nstrip that prefix from the request path BEFORE searching the file system for the\nfile. The StripPrefix function is described here: StripPrefix from http\npackage in Golang . This\nultimately means that since you’re pointing at a dist folder on the\nfilesystem, the matching of the URL will go straight to the sub-path. So, what does this mean for my static assets, how do I point at them? Well, from HTML (index.html), that’s being served from the /dashboard/ above,\nhow do you point at your bundled javascript, or CSS? An example can be found in\nthe static-examples folder in the Rye project. What you do here, is point at the static assets under\nthe /dist URL prefix. Here’s an example index.html with a reference to a\nstatic CSS: < html > < head > < title > Index.html </ title > < link rel = \"stylesheet\" type = \"text/css\" href = \"/dist/styles/index.css\" > </ head > < body > < h1 > Index.html </ h1 > </ body > </ html > Great! But wait, there’s more! Now, we had a way, using Rye, to deliver our single-page application. NIFTY!\nBut, we would build out the Astro executable in Go as a single file, and then\nwe’d need to point at files in the file system to run the dashboard. YUK! Since\nwe run everything in Docker, this got worse! We would have to copy the\nfilesystem of the /dist folder to the Docker image, and then configure the\npaths in our route handlers to point at the local location. This added a lot of\ncomplexity. What we really wanted was to serve our single-page application directly from the\nexecutable. EMBEDDED RESOURCES! What if we had a way to bundle our entire\napplication in one executable and still be able to serve the single-page\napplication? Magic (and a little help) There are various resource bundling options for Golang. This article had some\ngreat examples and some added complexity: Embedded resources in\nGolang . However,\nthere’s a great project out there created by Jaana B.\nDogan called Statik . The project is super simple. It uses\na command line tool, to take an entire filesystem and stuff it into a single\nGolang package that you can bundle and build with your application. All we\nneeded was to add statik -src=/path/to/dist/folder to our build pipeline and\nbuild some Rye middlewares to support it. These middlewares are not found in the\nRye library, but you can use the code yourself if you want to try it out! You’ll notice below, there’s also a _ \"github.com/MyOrg/MyProject/statik\" .\nThis is a requirement so that the handlers can get at the static assets. NOTE,\nsometimes some IDE’s will have trouble with this syntax. You can ignore it, it\nworks just fine. package api import ( \"bytes\" \"fmt\" \"io/ioutil\" \"net/http\" \"path/filepath\" \"runtime\" \"time\" // This is required by the statik package _ \"github.com/MyOrg/MyProject/statik\" \"github.com/InVisionApp/rye\" \"github.com/rakyll/statik/fs\" ) // This handler serves all Dashboard public artifacts from a static file system in memory func ( a * Api ) uiDistStatikHandler ( rw http . ResponseWriter , r * http . Request ) * rye . Response { statikFS , err := fs . New () if err != nil { return & rye . Response { StatusCode : http . StatusInternalServerError , Err : err , } } x := http . StripPrefix ( \"/dist/\" , http . FileServer ( statikFS )) x . ServeHTTP ( rw , r ) return nil } // This handler serves the index.html from a static file system in memory func ( a * Api ) uiStatikHandler ( rw http . ResponseWriter , r * http . Request ) * rye . Response { statikFS , err := fs . New () if err != nil { return & rye . Response { StatusCode : http . StatusInternalServerError , Err : err , } } file , err := statikFS . Open ( \"/index.html\" ) if err != nil { return & rye . Response { StatusCode : http . StatusInternalServerError , Err : err , } } b , err := ioutil . ReadAll ( file ) if err != nil { return & rye . Response { StatusCode : http . StatusInternalServerError , Err : err , } } http . ServeContent ( rw , r , \"index.html\" , time . Now (), bytes . NewReader ( b )) return nil } Routing for Dev Mode and Statik Mode Finally, how can you develop your single-page application if it gets bundled\ninto bytes in a Golang file? You need a DEV MODE. The way we solved this was\nsimple. We created a command-line flag to represent dev mode. That meant that we\ncould serve the ACTUAL assets (and could hot-load changes) when in DEV mode and\nthen when we ran in production we could run off the STATIK assets. This allows\nus an easy way to make modifications and yet still be able to deliver a great\nbundled experience for our docker container. if a . DebugUI { pwd , err := os . Getwd () if err != nil { log . Fatalf ( \"NewStaticFile: Could not get working directory.\" ) } routes . PathPrefix ( \"/dist/\" ). Handler ( middlewareHandler . Handle ([] rye . Handler { rye . MiddlewareRouteLogger (), rye . NewStaticFilesystem ( pwd + \"/dist/\" , \"/dist/\" ), })) routes . PathPrefix ( \"/dashboard/\" ). Handler ( middlewareHandler . Handle ([] rye . Handler { rye . MiddlewareRouteLogger (), rye . NewStaticFile ( pwd + \"/dist/index.html\" ), })) } else { log . Info ( \"ui: statik mode (from statik.go)\" ) routes . PathPrefix ( \"/dist\" ). Handler ( middlewareHandler . Handle ([] rye . Handler { rye . MiddlewareRouteLogger (), a . uiDistStatikHandler , })) routes . PathPrefix ( \"/ui\" ). Handler ( middlewareHandler . Handle ([] rye . Handler { rye . MiddlewareRouteLogger (), a . uiStatikHandler , })) } Finally, UI from a Services Team. So, there we have it. If Astro wasn’t a super secret project, I’d drop a\nscreenshot here. However, the UI (from a service team) is managable, updatable\nand usable. But, not super pretty. We learned a ton in delivering this and were\nable to spread the idea to other projects. One of those other projects is an Open-Source project called 9Volt which uses Rye as it’s mechanism for\nMiddleware. In 9Volt the UI is different but there is a React application\nusing Redux and Semantic-UI for a dashboard experience. This example is public\nand fully-fleshed out if you would like to see how it all hooks together\n(including a build pipeline). The setup of the routing is here: api.go , the setup of\nthe UI middlewares (doesn’t use the new Rye static middleware) is here: ui.go , the single-page\napplication is here: 9Volt/ui ,\nthe MAKEFILE , builds out\nthe UI in target build/ui and supports dev in ui/dev , and finally, to see a\nstatik build, look here: 9Volt/statik/statik.go . We will likely build more dashboard user interfaces using this technique. We\nhope this will help you out in building easily deployable web applications along\nwith your RESTful APIs. Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2017-03-17"},
{"website": "Invision", "title": "OSCON 2017 and InVision Engineering in Open Source", "author": ["Cale Hoopes"], "link": "https://engineering.invisionapp.com/post/oscon-2017-invision-opensource/", "abstract": "Our Platform Team grouped up for a great trip to Austin, Texas for this year’s\nO’Reilly Open Source Convention - (OSCON-2017) . It was a great\nopportunity to see what kind of innovations were going on in open source,\nconnect with companies whose products we use, and generally catch up on\ntechnology trends. InVision builds much of it’s software stack on open source\nprojects. Docker, Kubernetes, Linux, Golang, NodeJS, React and many other\nprojects make up the stack at InVision. Therefore, we really benefit from\nconnection with the community. Additionally, we want to contribute back as we\nbuild great services and libraries that can be reusable to the community as a\nwhole! Below, are some summaries from great talks we saw at OSCON 2017. Also,\nyou’ll find some summaries of our current open source offerings. Come check out\nour projects, contribute and use them! Open Source AI at AWS and Apache MXNet Summary by Tatsuro Alpert, Senior Software Engineer, Core Services Adrian Cockroft talked about AWS’s open source library for AI algorithms. They\noffer easy access to these AI engines as well as EC2 instances with specs that\nare ideal to running them such as the P2 with multiple GPUs and lots of RAM.\nThey also offer access to the services that back some of Amazon’s products such\nas the Alexa. You can use their APIs to take advantage of conversation engines as\nwell as face and image recognition engines. Cockroft concluded with a demo of a\nself learning, self driving toy car. The car had an onboard RaspberryPi and\ncamera that ran a MXNet to control it. The data processing and generation of the\nmodel is done in EC2 on one of the powerful machines. Slides Scaling massive, real-time data pipelines with Go Summary by Tatsuro Alpert, Senior Software Engineer, Core Services Jean de Klerk spoke about data pipelines written in Go. He began with a\ncomparison of several network protocols and their strengths and weaknesses for\ntransferring large amounts of data. He compared across HTTP, UDP, gRPC unary,\nwebsocket streaming, and gRPC streaming. While the streaming mechanisms were by\nfar the fastest, they are more difficult to use from an implementation\nperspective. He then went on to discuss queueing data between producers and\nconsumers. He compared several models on Go using arrays, channels, and ring\nbuffers. He concluded that any methods that required the use of mutices did not\nperform well. Atomics on the other hand perform well, but are very complex to\nimplement. Channels are the easiest to implement, but they do not have the\nflexibility of decoupling the producer from the consumer as they will eventually\nblock. If your use case requires this decoupling, ring buffers are the best\nsolution. Slides Monitoring at scale at Salesforce Summary by Adam Frank, Engineering Manager, SRE Salesforce is a giant HBase shop so it’s always really interesting how they run\nsomething like that at scale. This talk was about Argus, the tool Salesforce\nuses for gathering millions of metrics which, naturally, uses HBase on the\nbackend. The data structure it uses in HBase is defined by OpenTSDB, which I\nbelieve makes it compatible with the various OpenTSDB tools for data collection,\nlike tcollector. It uses Kafka to queue ingest, which makes it extremely flexible\nand not as susceptible to the sort of back pressure you see in other solutions,\nespecially 3rd party hosted solutions. The talk presented a great world where we\ncan collect as many metrics as we want with minimal sampling, but is of\nsomewhat-limited use to us, because anyone who’s run an HBase cluster before\nknows you only do so if you have to 🙂 All the same, it\nclearly demonstrated the value of treating your internal metrics with the same\nbig data analytics you’d consider for customer data. Slides Using NGINX as an effective and highly available content cache Summary by Adam Frank, Engineering Manager, SRE This ended up being the most technical NGINX-related talk at the conference,\nwhich surprised me since NGINX is so integral to most ingress tiers in the\ncontainer and VM space. Although the talk was specifically about content\ncaching, which didn’t seem particularly sexy, it ended up being a very\ninformative talk (which ended with the speaker giving everyone developer\nlicenses to NGINX Plus, which was nice). One thing that was covered was using\ndynamic variables in NGINX. For example, you can set up a variable in the nginx config and map a value to it\nbased on the output of a regex: map $http_user_agent $dynamic { \"~*oogle\" google; default not_google; } In that example the variable $dynamic will either get google or not_google depending on the user agent. The speaker also covered the split_clients option, which lets you apply a certain value to a variable based on percentages. For example: split_clients $request_uri $variable { 50% \"var1\" ; 50 % \"var2\" ; } The config related specifically to caching that was discussed was too much to\ndescribe here, but generally he covered different types of hashes, improved\nlogging, a couple fairly-clever ways of doing HA, and how to properly configure\ndisk caching without your disks becoming the bottleneck. Slides How and why we’re opening our code at Octopus Deploy Summary by Chuck Freitas, Lead Software Engineer, Engineering Velocity Damian Brady gave a talk on how Octopus Deploy decided to open source their deployment tooling\nas well as determining how much of the tools to open source. Since I work on the deployment tools\nat InVision and we are also in the process of open sourcing some of our internal projects I was\nespecially interested in this talk. While some of this talk revolved around the business case to be made for releasing a core part of\nthe company revenue stream as open source - which is not directly applicable to my current teams\ncase, I did enjoy the talk around defining what makes a tool truly useful as a public\nopen source tool vs just a exercise in the process. Part of this comes down to is it a general\nenough tool to be useful and is it ready to be released? Also, is there an user base for this project\nand is there leadership to help steer it in the future? Slides Evolutionary architectures Summary by Chuck Freitas, Lead Software Engineer, Engineering Velocity Great talk around architecture patterns for supporting evolution of your application stack.\nAs part of this Neal Ford discussed the idea of fitness functions. These fitness functions can\nrate or validate certain characteristics of your application architecture. For example, security\nor performance. This can then be used as part of your continuous deployment pipeline to validate\nthe “fitness” of your application and that your changes have not hindered any important aspects of\nyour application. In describing a architecture that is flexible and can support evolution Neal showed an architecture diagram\nusing micro-services that allows for scalability and isolation of concerns and an API gateway to isolate any\nchanges and support a more flexible design. Since InVision is actively implementing this sort of design, it\nwas good validation that we are on the right path. Slides Podcast Open Source at InVision InVision has only just begun to contribute some of our in-house projects back to\nthe community. We have more planned in the future, but currently our four\nefforts are out and waiting on GitHub for your contribution, input and use.\nBelow is a quick summary of each project and how we use them. Kit Kubernetes +\nGit github.com/InVisionApp/kit Kit is a full system to push Kubernetes deployment out based on a docker\npipeline. There are multiple pieces of Kit that helps keep your Kubernetes\ndeployments simple and manageable. We run many Kubernetes clusters here at\nInVision and we use Kit to manage the interactions. Our continuous integration\npipeline allows us to do a great deal of deployments. We still take action on\nindividual deployments (not continuous deployment), however those actions are\nall automated through our own internal chat-bot. Kit helps us push these\ndeployments to MANY clusters without hassle. Feel free to take a gander at Kit\nand give it a try! Kit-Overwatch github.com/InVisionApp/kit-overwatch Kit-Overwatch is a simple service who’s sole responsibility is to watch the\nKubernetes event stream and push notifications to other services. This can be\nreally useful to get the stream into something your engineering staff can work\nwith. Currently, we only have a few notifiers built, but this could easily be\nexpanded based on your needs. We run this in a docker container and it helps us\nget the Kuberenetes event stream into Slack and DataDog. There’s also a stdout\nlogging feature for testing. Enjoy! Rye github.com/InVisionApp/rye So, everyone needs middleware. That’s the truth. However, in Golang, middleware\nis one of those things you could do eight different ways and it wouldn’t matter,\nthey would all work. Rye is our answer to middleware. We built a very simple\nmiddleware library to give us some out-of-the-box functionality such as StatsD\nintegration and timing on our middleware methods. Additionally, we built out\nsome base middlewares to go with the library including request logging, CORS,\nJWT verification, and Golang 1.7 context support. Rye has turned out to be very\nuseful for us and is being used by multiple teams here at InVision. Feel free to\ngive it a whirl! Conjungo github.com/InVisionApp/conjungo So! Have you ever had a situation where you had to instances of the same struct\nand needed to merge them together? Well, we did. Basically, imagine that you\nhave a PATCH endpoint that takes in your struct in your service, but you need to\nmerge that with the value in your Mongo database. In that case, you can use\nConjungo to merge the structs together. Conjungo allows you to control much of\nthe process of merging and supports many use cases. This library was put\ntogether by our Senior Software Engineer, Tatsuro Alpert to solve a problem in a\nservice catalog service that we built in-house. It’s turned out to be a very\nuseful library for us! Check it out, use it and enjoy it. We’d love to know how\nwe can make it better! Coming Soon We have other open source projects in the hopper here at InVision. One of the\nprojects coming is named Chronos which is a very tiny Golang library for\nmanaging the scheduling, logging and reporting of recurring tasks. Look for this\nin the near future! Many other projects are growing here at InVision as we\ncontinue to improve our stack. That being said, look to us in the future as we\nTweet out new projects as they become available. The last word… InVision is striving to build a platform we can be proud of. Open source has\nbeen a big part of that. As an engineering practice, not only do we rely on open\nsource to build a platform that we are proud of, but we have started to\ncontribute back our internal efforts. We’d love your input and help as we grow\nthis effort. We welcome contributions, Github issues, pull requests and\nfeedback. Additionally, come join us at InVision and help us contribute more\nopen source efforts to the community! We can all be better together! Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2017-05-26"},
{"website": "Invision", "title": "Introduction to CSS modules", "author": ["Matt Borgato"], "link": "https://engineering.invisionapp.com/post/introduction-css-modules/", "abstract": "These days there are several problems when you’re writing cascading style sheets.\nCSS files usually grow really fast, and you’ve probably experienced the following difficulties: Classes defined in global scope; Dependencies; Elimination of dead code; Minification; Sharing constants; Non-deterministic resolution; Encapsulation. In addition to these points, when a development team doesn’t adopt good conventions and best practices , all the style sheets become rather difficult to maintain and extend . This is mainly because CSS does not have any kind of native solution for writing dynamic results .\nIt is also too permissive in relation to global styles. In other words, everything is completely global, every selector has complete access to the entire page, and even worse, every selector can be (re)defined in multiple files. In this scenario, the side effects can multiply and be completely unpredictable. Naming collisions and long subtree selectors can easily produce brittle and unmaintainable CSS files. It’s like a house of cards , where simple changes, such as updating a background color of a button, can be a nightmare.\nAll those problems become even bigger as soon as your project grows in size, complexity, and team members. Most of the solutions proposed over the past few years have handled the writing issue very well including writing and organizing methodologies (e.g.: OOCSS , SMACSS , BEM ) and preprocessors (e.g.: SASS , Less , Stylus ).\nBefore them, building a function and reusing a definition were quite hard to implement. Although, even if they are very easy to maintain for the developers, these solutions failed to solve the architectural problems of CSS. These methologies have increazed the complexity of the style architecture . Moreover, the CSS should be completely agnostic to the application. Low specificity is key to good CSS design . Portability and reusability are two important aspects of it.\nSimply put, every application you build shouldn’t be dependent to its CSS files. At this point, the questions are: How do we work around the architectural problems of CSS? How to have maintainability without an unnecessarily complex architecture? How to scope and reuse styles as an integral part of the application? How to stop worrying about side effects? CSS Modules to the rescue Fortunately, the guys at CSS Modules have solved the functional problems we’ve been facing when using CSS in component-based JavaScript applications (e.g.: React , Angular , Ember ). It represents a real paradigm break . When we started to develop Freehand , we decided to introduce CSS modules.\nWe’ve been adding a ton of new features and components so far, and it’s now super easy and fun to work with well organized CSS files. How it works Here is a simple component (button.js): import styles from './button.css';\n\nexport const button = `\n  <button class='${ styles.button }'>\n    Like\n  </button>\n`; and its CSS file (button.css): .button {\n  font-size: .8em;\n  color: #FFF;\n  background-color: #333;\n  padding: .5em;\n  border-radius: .5em;\n} It is indeed a very simple component, yet it’s perfect for the sake of clarity. Notice that the component variable is being exported.\nThis will give more control of when and where it will be used in the file it is imported into. Let’s then use this component and print the button it creates in the file (list.js): import { button } from './button.js';\n\nlet list = `\n  <ul>\n    <li>Plo Koon ${ button }</li>\n    <li>Yoda ${ button }</li>\n    <li>Luke Skywalker ${ button }</li>\n  </ul>\n`;\n\ndocument.write( list ); You can immediately see that instead of writing the button multiple times in each file (list), a new file has been created, which exports the button component so that it can be freely used by other components. This component already has its styles loaded with it. You can notice that we’ve already reused styles simply by an architectural decision . The possibility of finally having a sense of scope and security regarding side-effects takes a lot of weight from the developers. Styles are scoped by default. CSS Modules are basically files that are “local”. Anything that one writes in this module doesn’t leak out in other files. Another benefit is that we can get rid of several bad practices introduced by the other solutions mentioned above.\nFor instance, BEM naming conventions introduce unreadable classnames or preprocessors like SASS and Less generate super long and heavy CSS files.\nWith CSS components everything is much more simple, neat and it has its own order. Conclusion One thing to keep in mind, when using CSS Modules, is that the main application is for componentized systems . The way in which you implement the components can influence their reusability.\nIn particular, since we consider styles as an integral part of a component, we do not need to rewrite the same styles anywhere else, we just have to reuse the entire component, including CSS. It’s like a Lego brick with specific properties. Some advantages of creating reusable styles especially in large projects are: Re-use between projects it will be possible to use the same module in different projects with ease.\nThis would be quite useful in the case of a client with multiple projects following a similar styleguide. Declared dependencies the module will be explicitly declared as a dependency of the project, which will make it easy to identify where the reused styles come from. Building libraries for open-source this is fantastic! Exporting, sharing and importing libraries is a cakewalk. Note: the diagram above was crafted with Freehand . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2017-06-20"},
{"website": "Invision", "title": "Cross-Team Project Management at InVision", "author": ["Ryan Scheuermann"], "link": "https://engineering.invisionapp.com/post/cross-team-project-management/", "abstract": "InVision currently has over 20 engineering teams focused on building and operating a piece of the InVision platform. To scale this model, each team operates much like a small startup – with their own product manager, designer, engineering manager, and engineers. With minimal coordination outside their team, they ship features quickly. But as we mature the product, some changes affect more than a single team. New architectural foundations, integrations, or UX design work often impact large areas of the product and require coordination to ensure consistency and an on-time delivery. Growing from a team of 3 to over 100 distributed engineers in a very short time, we didn’t have much experience managing cross-team or cross-department projects outside an in-office environment. Without some best practices, we often experienced delays due to redundant effort, rework, and confusion on who is responsible for what. Here’s what we’ve learned so far, and we’re still learning. One Project Lead After you’ve identified the need to escalate a project to a cross-team initiative, the first task is to identify a single Project Lead . This doesn’t need to be a new individual dedicated to the role, and in many cases, it’s more desirable to choose an individual who has the most vested interest in the success of the project. Typically, for us, this means the engineering manager of the team with the biggest part to play or who is the prime mover of the project. Start A Project Plan Document The first duty of the Project Lead is to start or identify a document to serve as the project plan . This document acts as the central source-of-truth for the initiative, and should be the main portal for all other documents or resources on the project. Define The Goal Working with the lead Product Manager on the initiative, the Project Lead sets out the goals of the project. What does success look like? What’s our definition of done – the moment when this initiative is marked complete? The goals and metrics for success should be captured and documented in your project plan document. Roles & Responsibilities The Project Lead must identify up front every team that may have a part to play in the success of the project. This includes teams outside the immediate department, such as the platform, analytics, support, or marketing teams. Each team involved should designate a primary point-of-contact for their team on the project. This is often the engineering manager or lead engineer handling the work on the team. They are the individual responsible for reporting status of their work and disseminating information to their respective teams. Regardless of title, the Project Lead is empowered to direct the actions of the other team managers as it relates to the initiative. They often do this by providing and documenting clarity on the roles and responsibilities of each team . Communication, communication, communication With the project underway, the Project Lead must adopt a mantra of “avoid surprises” . No team involved in the project should be left out of the loop. The project is far more likely to succeed when everyone involved is informed and motivated. While not every team involved needs to have a say in making the necessary decisions, we can’t stress enough the importance of every team knowing those decisions as soon as possible after they’ve been made. Pro Tip: If you use Slack, the Reacji Channeler is a great integration for tagging and funneling :gavel: decisions to the teams that need to know. Daily Stand-ups Daily stand-ups for a cross-team project may seem like a lot of overhead, but stand-ups serve as an important checkpoint in ensuring the project stays on-track. So have them! Take running notes in a document. Ensure the lead from every team is present, and when unable to make it, they should send a representative to standup. As a last resort, when no one is available to attend, they should drop their standup notes into the document ahead of time. Task Tracking We use JIRA at InVision, and every team has its own JIRA project. Because of this, a single view of individual team efforts across a cross-team initiative requires some additional setup. To do this, we implement a standard JIRA label for the project and set up a filter or board that displays all the tickets across each team. This can also be accomplished with a Confluence page. Documentation Document sprawl is a problem for any widespread initiative. In almost every case to date, a cross-team project starts on a single team and involves more teams as it approaches implementation. As a result, there are often many discovery and ideation documents floating around – and many of them contain out-of-date decisions. A single team working from outdated information is a serious schedule killer. The Project Lead must prioritize the centralization of the project’s documents . Rigorously deprecate out-of-date documents as they’re discovered. Putting all project documents into a folder or project space is a great first step, but the project plan should link out to all auxiliary documents that remain relevant. Dependencies & Blockers Every cross-team initiative has dependencies – by definition. Determining the dependencies up-front is often the most difficult aspect of a cross-team initiative, and no one person or team can discover them all. Each team involved is responsible for harvesting their dependencies and reporting them in daily stand-ups. I wonder if anyone’s ever run CS classes in pairs, where one class has to write libraries for another class to use. Because that’s life. – @dakami The Project Lead must document and track the dependencies and blockers across the project, but it’s not the Project Lead’s responsibility to identify or resolve dependencies. Meeting and Discussion Facilitation The Project Lead should identify when it’s necessary to schedule an ad-hoc meeting to discuss a topic. In the event of disagreement, after listening to each team’s input, the Project Lead is empowered to make a decision to move the project forward. But it’s also the Project Lead’s responsibility to recognize when a decision requires executive sponsorship or approval. Milestones The Project Lead should establish interim milestone dates to keep everyone tracking to the overall schedule. The daily stand-ups should be used to keep everyone accountable to these important dates. Schedule The Project Lead keeps the overall schedule up-to-date. A best practice is to separate the status and schedule reporting of a cross-team project from any projects for which a single team alone is responsible. Reporting Status and Risks Lastly, the Project Lead is responsible for reporting the project’s status to leadership . This should include an assessment on how well the project is tracking against the target date, the quality of the work, the dependencies, and any risks that may affect our ability to hit that date. Risks could be resource constraints, scope changes, technical limitations, etc. The mantra of “avoid surprises” applies not only to the teams doing the work, but executive leadership as well. Retrospective More than any other project, with all its moving parts, a cross-team initiative requires a retrospective to review what went well and what could be improved for next time. Be sure to include all the major stakeholders across each team. Retrospectives can sometimes be difficult to schedule, and while not preferred, it’s possible to hold them in an asynchronous manner using a collaborative document. Conclusion By their very nature, cross-team initiatives require more coordination to increase the likelihood of success. Follow these best practices and your next cross-team initiative will suffer from far less confusion, uncertainty, and doubt: Designate a single Project Lead Keep a single document for the project plan Set out the definition of success ahead of time Designate a primary point-of-contact for each team involved Document the roles and responsibilities of each team Ensure every team involved knows decisions as soon as possible Have daily stand-ups and take notes Use a JIRA label or other mechanism to track the work across teams Prioritize the centralization and cleanup of the project’s documents Ensure each team is harvesting their dependencies Empower the Project Lead to make decisions Establish interim milestone dates Separate the schedule, status, and risk reporting of a cross-team project from other projects Have a retrospective Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2017-07-26"},
{"website": "Invision", "title": "Five Tools Kotlin Gives Android", "author": ["Josh Barber"], "link": "https://engineering.invisionapp.com/post/five-tools-kotlin-gives-android/", "abstract": "In the history of native Android development we’ve had several options for languages to code in.  But Google historically hasn’t given us any first party support for other languages other than Java and C++.  That all changed at Google IO 2017 when they announced support for Kotlin. If you’re using the latest version of Android Studio then you’re already set up to use Kotlin.  The IDE can also convert your existing Java code into Kotlin in case you want to see how a rewrite looks.  And the two languages have complete interop - you can write a Kotlin class and the call it inside of Java. Why bother with Kotlin though?  If you’re an experienced Java developer on Android, why shouldn’t you keep moving right along with what you’ve used in the past?  Apart from being the new and shiny, the Kotlin language also brings several new tools to the table that can make your code easier to read and faster to write.  Here are my top five favorites: #1 - Null Safety Who doesn’t love Java Null Pointer Exceptions?  If I could have a penny for every NPE that I’ve encountered in my career - I could solve global warming and buy everyone a pony with the left over money. I’m sure you’ve encountered something like this situation in the past: Java: SomeCoolClass someCoolClass ; String firstValue = someCoolClass . first (); //NullPointerException In this case I’m declaring a new SomeCoolClass instance with the intention of using it later.  Then on line two I forget that it’s not instantiated.  Obviously when I call first() , Java explodes because someCoolClass is null . With the abundance of asynchronous coding and more complex architecture, these kinds of simple mistakes are really easy to make (and my mind is on global warming and buying the world ponies remember?) Kotlin gives us a way to protect us from ourselves with Null Safety.  It’s not a new concept - the idea here is that for those cases where null is a possible value we’ll stop first() from being called it if someCoolClass isn’t instantiated. Kotlin: var someCoolClass : SomeCoolClass ? = null var firstValue : String ? = someCoolClass ?. first () //Returns null not a NPE If you’re wondering what the whole business with the ? mark after the type declaration for firstValue is about, it’s because Strings aren’t allowed to be null in Kotlin by default.   You can still allow it with the ? .  In the case where someCoolClass isn’t instantiated, firstValue will be equal to null . After I came up with the example above I thought of more realistic example of when this could happen.  Let’s say you want a default value in the case where someCoolClass is null.  In that case you can use the ?: operator.  It functions just like a ternary operator except it handles that case where the conditional is null . var someCoolClass : SomeCoolClass ? = null var firstValue : String = ( someCoolClass ?. first () ?: \"My Default Value\" ) //Returns 'My Default Value' someCoolClass = SomeCoolClass () firstValue = ( someCoolClass ?. first () ?: \"My Second Default Value\" ) //Returns the value of someCoolClass.first() In this example, I can drop the ? null check for String because my value will always be either the value of first() or the default.  It someCoolClass is null or if first() for some reason returns null, then the default value would be what the firstValue variable is set to. #2 - Ranges Ranges are something you aren’t going to use a lot, but when you do you’ll get away with writing much less code than you would in Java. Here’s a counting loop in Java: for ( int k = 1 ; k < 11 ; k ++) { println ( \"Current number is \" + k ); } //Prints 1 through 10 Despite having written possibly millions of loops in my career, I sometimes still have to pause and consider the termination condition.  When k < 11 and I’m incrementing by one, that means k is going to stop at 10.  It’s very easy to forget and when you’re in a hurry those are the kinds of bugs that can eat away your day. By using ranges in Kotlin, we can convert that loop back into the way our brains naturally thing about counting.  You don’t say, count from 1 to the max integer less than 11, you’re saying count from 1 to 10. for ( k in 1. . 10 ) println ( \"Current number is $k\" ) //Also prints 1 through 10 Decrementing using ranges is equally simple if you use the keyword downTo you can reverse the whole thing. for ( k in 10 downTo 1 ) println ( \"Current number is $k\" ) //Prints in reverse, 10 to 1 I’ve also thrown in string templates in my example for you.  Instead of concatenating a string you can drop the variable into the string with a $ that designates its position. #3 - Extension functions Other languages have extension functions and you can really get yourself into trouble if you aren’t careful.  I still think they’re a useful tool for the language because you can reduce a lot of code by adding a function onto an existing class without extending it. fun String . cheez () : String { return \"$this has cheezburger?\" } var burgered = \"Josh\" . cheez () println ( burgered ) //Josh has cheezburger? You could use extension functions for formatting of values that are used throughout your app. #4- Lambda Expressions If you want to use Lambdas in Android with Java you’ll have to switch to using Java 8 or use the Retrolambda Gradle plugin .  Kotlin can make this a bit easier because you don’t have to configure anything additional once you’ve added the language to your build.gradle file. In case you haven’t used Lambdas with Java before they allow you to reduce a fair amount of boilerplate.  The most common example is adding a listener: Java: feedbackButton . setOnClickListener ( new View . OnClickListener () { @Override public void onClick ( View v ) { presenter . showFeedback (); } } Kotlin: feedbackButton . setOnClickListener ({ view -> presenter . showFeedback () }) By using a Lambda you can drop the anonymous function and since the OnClickListener only has one public method that can be overridden you can reduce it to just the argument view . In Kotlin you can take it one step further if you aren’t using the view argument, you can reduce it all the way down to the handler internal code. feedbackButton . setOnClickListener { presenter . showFeedback () } People either love or hate Lambda’s, but in my opinion they can reduce a lot of code and still maintain readability. #5 - Coroutines While coroutines are still experimental in Kotlin, I think they’re worth mentioning as an exciting new way to approach asynchronous code. fun startCountDown ( hello : Text , button : Button ) { launch ( UI ) { // launch coroutine in UI context for ( i in 10 downTo 1 ) { // countdown from 10 to 1 hello . text = \"Countdown $i ...\" // update text delay ( 1000 ) // wait a second } hello . text = \"Done!\" } } In my first example I’m starting a launch coroutine on UI thread.  There’s a counter inside that pauses for 1 second before it continues.  In this case it doesn’t block the UI like you might expect, the delay function is just suspending the coroutine .  The code inside launch is thread safe and can readily update the UI. fun startCountDown ( hello : Text , button : Button ) { var job = launch ( UI ) { // launch coroutine in UI context for ( i in 10 downTo 1 ) { // countdown from 10 to 1 hello . text = \"Countdown $i ...\" // update text delay ( 1000 ) // wait a second } hello . text = \"Done!\" } button . setOnClickListener { job . cancel () } } If you want the ability to cancel the coroutine, it’s possible to assign the launch builder to a variable.  And then from outside the code it’s possible to stop the whole coroutine. I like coroutines because I think they’re an easy way to contain async code in the context where the code is being executed.  It might be worth considering as a way to reduce complexity while still being thread safe. Coroutines are big topic and if you want to read more about them check out https://github.com/Kotlin/kotlinx.coroutines/blob/master/ui/coroutines-guide-ui.md Conclusion While Java has historically served the needs of the Android development community, it’s great that Google has expanded their first party support to give us more options. If you’d like to give Kotlin a try, there’s very little configuration you have to do in order to bring it into a project.  And since it’s supported by Google, you can easily convert existing code to Kotlin or make new classes in Kotlin that interop with your Java code. Thanks for reading and I hope this has been useful!  If you manage to figure out the cure for global warming or worldwide pony delivery, be sure to let us know in the comments! If you want to read more on Kotlin, check out https://kotlinlang.org/ Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2017-07-18"},
{"website": "Invision", "title": "Twin Cities Networking Event", "author": ["Jose Fernandez"], "link": "https://engineering.invisionapp.com/post/twin-cities-networking-event/", "abstract": "The InVision Engineering team is coming to the Twin Cities! To register, sign up on our nVite page . At Invision, we’re building an amazing company with fantastic collaborative design products that our customers love . But to build an amazing company with fantastic products, you need exceptional engineers . It turns out exceptional engineers are hard to find; they don’t reply to cold call emails or LinkedIn messages. Exceptional engineers value word of mouth, in-person experiences, and want to see you hustle. They want to meet their prospective teammates in person and discuss their engineering challenges. This is why we’re bringing the InVision engineering team to you! We’re hosting a Networking Happy Hour at CoCo Downtown in Minneapolis on August 24th at 6pm. Come meet with our engineers to learn more about our  tech stack (Golang, node, AWS), the perks of working for a distributed company , and the variety of open source projects we are developing at InVision. We see the Twin Cities thriving as a tech hub, with plenty of untapped talent looking for challenging and exciting opportunities — without having to relocate to Silicon Valley. We’ll have swag for the early birds, Fulton beer, and Pizza Luce. At the end of the night one lucky attendee will win a $300 gift card for our favorite local bike shop, The Hub . Did I mention we’re a fully distributed company? Even our CEO works from home. We have engineers in over 20 different countries. Join us and you can work anywhere. Don’t miss this exciting opportunity, register for the event on our nVite page . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2017-07-28"},
{"website": "Invision", "title": "6 Unexpected Things I Learned from Working as a Remote Intern", "author": ["Sharmila Tamby"], "link": "https://engineering.invisionapp.com/post/working-as-a-remote-intern/", "abstract": "“See you guys on Monday!” I said as I waved goodbye to my coworkers. The twist? While I was in New Jersey, they were working from Austin, Seattle, Newfoundland, and Madrid. My farewell was quite literally a sign-off- I waved by inserting a 👋 emoji, logged out of Slack, then walked ten feet to my kitchen to start cooking dinner. This summer, I worked at InVisionApp as a software engineering intern. The cool thing about InVision is that it’s a 100% globally distributed company, meaning everyone can work from wherever they’d like, every day. There’s no central office or city where you need to relocate. Even though I thought it was a pretty cool setup, I had a lot of questions about what this internship would look like. I knew I was in for a unique experience, one quite different from those that my friends were having, working 9 to 5 in an office with an army of interns by their side. At home in New Jersey and as their first (and at the time, only) engineering intern, I was scared that it would be hard to engage, not only with my work but with my coworkers . In the end, my time at InVision totally blew me away, and I learned so much along the way… 1. Daily communication is important I started my days by logging in for our daily stand-ups, a 30-minute video conference call with our team. We function via Slack and at any point during the workday, I was able to confidently ping someone for help and they usually responded within minutes. To me, this was the equivalent of walking over to someone’s desk and asking for help . Everything here is super fast-paced and project and team based, so the structure and frequency of communication played a big role in how attached I felt in the process. 2. You’re not just an intern, but 100% a part of the team I felt welcomed here starting from day one. Fellow engineers, upper-level management, and coworkers outside of my team personally reached out to welcome me. From daily stand-ups to company-wide meetings, I was able to witness and be part of their day-to-day operations. As a member of their infrastructure team, I also had the chance to work on a challenging new project, write code, and work on my web development skills. It was cool making real contributions to the company and being able to see what working here full-time would actually be like. One of the highlights was being invited to a company offsite trip in Lisbon, Portugal, alongside twenty of InVision’s engineers, designers, and managers. I loved how I was treated just like everybody else, and for four days we worked, hacked, and discussed all as one team. 3. Everyone is super smart and willing to share their wisdom While a lot of my learning was done through screenshare tutorials and stack overflowing on my own time, it was reassuring to know that if I was stuck on something, help was just a ping away . I was working on a project in React and web dev was still a new concept to me. On a day to day basis, my teammates would partner up with me to accomplish different tasks. There was one point when I reached out to someone on a totally different team and they worked through a problem with me for over an hour. While this taught me a ton of new technical skills, I also learned that people here are very receptive to questions, and will always be quick and patient to answer them. 4. There are a lot of resources geared towards company culture. Take advantage of them! InVision does a lot to keep everyone engaged. Everything from a thoughtful welcome package to regular meetings with the Director of Employee Happiness works to keep people feeling like they are a part of a bigger community. There is an integrated Slack feature called Donut that pairs you up with a buddy from the company. It allowed me to connect with some awesome people in other departments and to learn so much about entirely different parts of the company. InVision also partners with WeWork for people who work best in coworking spaces, with offices in Boston, NYC, Austin, among others. I really enjoyed going to the one in New York to experience a vibe similar to a real office space and to spend time with InVisioners in person. 5. There are a million and one things you could do when you don’t have a traditional 9 to 5 job InVisioners are some of the most interesting people I’ve met. Jack lives in New York City and works from a newly discovered coffee shop every Friday, with an open invitation. Chesley is the father of a (super adorable) one-year-old and can be with him during the day. Tomas lives in Prague but has been traveling across Europe and Asia throughout the past year. People here are, among so many other things, self-starters, entrepreneurs, and Moms and Dads . Some are all of the above. Working remotely allows you to navigate your time so you can pursue other projects that help you feel most fulfilled and productive. It’s a total game changer. 6. Collaboration is key People here like to err on the side of overcommunication . Working remotely, each ping and video call has a purpose and it’s important not to take an interaction for granted. One of my greatest takeaways from the company offsite in Portugal was when someone proclaimed, “I do care about what people think, that’s why I always say what’s on my mind!” Being unafraid to clearly communicate your ideas is so important to making thoughtful progress. Teams here function in an impressive, high-performance way. I learned a ton just from joining in on their meetings and seeing how they speak their minds, express disagreements, and compromise. They successfully work through problems and it’s all fueled by their common goal of building incredibly impactful products. Conclusion Every time I logged onto Slack, we were six people spanning three countries and five time zones. And somehow, we’re all able to join in on this exciting and fast-paced startup world, and push products that people don’t just love using, but love building. By the end of my internship, I realized how insanely smart, creative, and driven InVisioners are. I realized the secret to this company’s success is that their products truly mirror their people. And above all, I realized how remote life is one of InVision’s many strengths, and allows them to tap into talent from all around the world. It took time to get used to all virtual interactions, but I learned that remote internships are honestly a pretty sweet and innovative way for teams to engineer. It’s a unique experience and done right, it pushes people to work their hardest . While both remote and in-person work have their pros and cons, InVision is a benchmark for the remote lifestyle as it becomes more and more popular. Ultimately, it’s bigger than ourselves and this company, as we pave the way for future remote interns, remote workers, and a more flexible lifestyle. Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2017-08-09"},
{"website": "Invision", "title": "How we learned go in a day", "author": ["David Gyori"], "link": "https://engineering.invisionapp.com/post/how-we-learned-go-in-a-day/", "abstract": "I really like going to hackathons . Usually I gain a lot of new experience, meet new people and have fun . It doesn’t bother me that I have to be awake for 24 hours, it is always worth the time. I’m part of the mobile team that has normally written APIs with NodeJS, but when the company decided to use go as our primary backend language, we realized that no one in the team had any experience with this language. This got me thinking. I knew that we all could all benefit if we worked on an actual go project before we got our hands dirty, so I decided to organize a hackathon, where we all could work on the same project and use go in practice. Challenges I’m sure you’re familiar that InVision is a 100% distributed company , so an on-site hackathon wasn’t an option for us. I researched offline hackathons, then applied the findings on our online version. Let me share the results of our first online hackathon with you. Goal Our goal was crystal clear: we all wanted to use go in practice together.\nI think defining the expectations in advance is  good practice —it allows the measurement of the outcome in retrospect. Preparation In my opinion the blueprint for a great online hackathon is as follows: Choose one person who will lead the hackathon. Pick a date which is suitable for everyone. Select an idea /project for the event. Send out resources for preparation. Select the stacks in advance. Invite your designer. Start the event with a briefing . Start to work on the project , coordinate the team. Have a retrospective at the end of the hackathon. Send out a demo to the company. Choose who will lead It’s important to have someone in charge who passionately organizes the event despite the fact that they can’t contribute that much to the codebase. As every great project, a hackathon should be managed well if we want great results. Pick a date In my opinion, having the hackathon at the beginning or at the end of the week is good practice. At InVision we have a half-day on Fridays so we decided to hold our event on that day. Select an idea We used a Google Sheet to collect ideas worth working on for the hackathon. Each of us needed to think of an idea and add it to the sheet with the description, the estimated time for developing the idea, and the name from whom the idea came—in order to be able to ask questions if anything happened to be unclear. We used this template to the ideation process. The voting process was easy: every team member had to give out two points to the listed projects, one for each. We summed the points and selected the winning idea. The idea everyone liked most was a poll chatbot for Slack. The creator of the winning idea was asked to write the documentation of the project. Resources for preparation We collected a set of resources worth reading and sent them out to our team a week before the hackathon. I think this is generally good practice, no matter what the goal of the event is. Deliberate preparation is important. These are the resources we found useful. Select the stack To simplify the development process we standardized the development stack in advance. Because we have chosen an idea that included third party API, I prepared the workflow which we used at the hackathon, in advance . We also needed to decide on the communication stack : As the project was a Slack bot, we decided to communicate via Slack . We used Google Docs to store our documents related to the project and used Google Hangouts to have a video call through which we could chat about the development. We used Trello as a lightweight project management tool. Invite your designer Besides the fact that coding is fun, it’s also great to have a really nice-looking end result. Designers can add that nuance . We decided to invite Charles Patterson , our great designer teammate, to the hackathon. Not only did he create a nice look and feel for out chatbot’s message formatting, but he also created an intro for our team’s weekly demo videos. Briefing The day of the event started with a briefing session. I prepared a document with all the necessary information to get started, containing the agenda of the hackathon, the documentation , the architecture of the application, the workflows (for Slack and Github) and even a Spotify playlist for the event. Working on the project We split the team into two: engineering and product. While the engineering team was bootstrapping the application, the product team members were able to carefully architect the interface and document it in detail. When the engineers started the implementation it was crystal clear what to implement. Our product team included a manager, a designer, and an engineer. We had three engineers developing the project. As the application was developed we tested it through Slack. Eventually, we decided to extend the hackathon for one more Friday as there wasn’t enough time to finish. We worked approx. 12 hours on the project in total and had a working prototype—not bad. Retrospective After we committed the last chunk of the code to the repository, we had a short retrospective session. All of us enjoyed the event, the only part where we could do better next time is to do the bootstrapping in advance. Demo At InVision, every Friday, every team makes a demo about the awesome achievement from the given week. So we decided to also make a video demo about the poll bot we created—for visibility reasons and for encouraging other teams to organize hackathons in the future. Conclusion We learned a lot from organizing an online hackathon and I would encourage you to try it if your team needs to learn something new—it’ll give you astonishing results. It’s also a great team building exercise. Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2017-08-01"},
{"website": "Invision", "title": "My Time as an Intern on the Engineering Tools Team", "author": ["Ephraim Benson"], "link": "https://engineering.invisionapp.com/post/my-time-as-an-intern/", "abstract": "Entering InVision, I had programming experience from computer science classes throughout high school, but I didn’t have a good sense for how software is actually built by teams in the real world. My time at InVision allowed me to gain experience developing with new technologies while getting exposure to the practices and methodologies used in the industry. I’m based in Portland, Oregon, so on my first day I actually went into a physical InVision office downtown. Although I knew InVision had employees all over the world, I had the impression I would be entering a bustling office space. I was surprised to find the office sparsely populated. For a first day though, this was actually kind of nice. I only had to introduce myself to a small handful of people, and I got to enjoy a quiet view of the city as I set up a flurry of InVision related accounts. By the end of the afternoon, I was kneeling before my password manager. Given the freedoms of working remotely, it makes sense that few people use the office. To me there seemed little reason to commute when your teammates are located elsewhere anyway. As such, I spent most of my internship working from home, and I became more familiar with how a remote company works. All meetings happen online (no surprise there), but what struck me was that even if multiple people are in the office, they are required to move to separate rooms before joining the call so no one has an unfair advantage in communication. Despite this remote structure, I was really impressed with how well InVision maintains a company culture even when all interactions happen through a screen. Everyone is very positive and friendly towards one another and emojis abound throughout the Slack channels. At first I was nervous to ping strangers for help, thinking people with real, important jobs wouldn’t want to be bothered by the needs of a lowly intern. However, everyone I reached out to was more than willing to loan me their time. Though I spent most of my time working remotely, I did choose to go into the office about once a week, and I gained an appreciation for having an independent environment for work. At school, my productivity and leisure both occur mainly in the dorm, and I spend an evening in the library when I need a change of pace. Going to InVision’s physical office was similarly useful and helped me occupy two distinct headspaces. Plus, the air conditioning was a real savior during some of the hottest days in Portland’s history. Weekly trips to the office also enabled me to meet face to face with my manager, Sejal. She walked me through Agile development practices and broke down the differences between Kanban and Scrum methods. Since my project’s development was independent from the workflows of the other people on my team, I wasn’t expecting this kind of training. However, it was a welcome learning opportunity seeing as it seems to be used wherever software is made, and it was really helpful in contextualizing the daily rituals of team meetings. As for my project, I was tasked with building a video concatenator bot for Slack. Engineering teams regularly release video demos of what they have been working on, and when they start to pile up it can be cumbersome for a viewer to click through them one by one. So, the Video Concatenator puts all the videos uploaded within a week together into one file and posts that to the demo channel. Though simple in concept, I was excited by it because its function was practical and could potentially be useful to everyone within the company. As I started piecing together sample code for Slack’s API, I became convinced I could have the project completed within two weeks. Alas, it ended up taking several more as I rebuilt many features several times over. I found that implementations that worked at a scale of one didn’t work so well when multiple people were accessing the program through multiple Slack channels. Each time I realized a flaw in my current design and had to backtrack, I felt like I was getting a stern reminder in planning for scale instead of crafting jerry-rigged solutions. Unlike my CS classes, which were mostly concerned with using data structures to write algorithms, building the Video Concatenator required working with a lot of code from other parties. Much of my time was spent reading documentation and writing test programs as I figured out how to cobble together existing APIs to interface with the services I needed. During development on the project, I learned how to use Heroku, virtual environments, several AWS technologies, SQL databases, and some (very basic) web development. However, I think the most valuable lesson I learned was to always keep the user in mind. As the sole developer of the tool, I knew its ins and outs, but I needed to put thought into how to best familiarize a user. Features that should have been obvious for usability were initially overlooked, and some information just wasn’t being presented in an approachable way. To improve this, I tested the bot with a myriad of tweaked responses to see what felt best. In the end, I think I might have spent a little too much time picking emojis for the bot. As I leave InVision and return to the classroom, I’ll remember that although software is built by individuals and teams, it is made for the masses. After working for a company whose entire ethos is good design, I guess it was inevitable that I would come to appreciate it myself. I’d like to thank Sejal, Jon, and Victor for letting me tag along on the Engineering Tools team for the summer and for supporting me throughout this experience. May your backlog be spacious and your bonusly plentiful. If you’re an InVisioner curious about the Video Concatenator, you can find it in #engineering-demos on Slack, and you can check out the project here on GitHub . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2017-08-16"},
{"website": "Invision", "title": "InVision Expands Their Bug Bounty Program with Bugcrowd", "author": ["Sara Dunnack"], "link": "https://engineering.invisionapp.com/post/bugcrowd_expanded_bug_bounty/", "abstract": "Last year we wrote about our new bug bounty program launch with Bugcrowd, and now we are excited to announce that we are expanding the scope of our program and increasing our rewards by 80%! We are adding our marketing sites and InVision’s enterprise service offering to the program profile. This will give security researchers around the world greater opportunity to identify and submit application vulnerabilities with a new premium paid to expert level exploitation. InVision is the world’s most powerful and comprehensive digital design platform enabling over 3.5 million customers and 80% of the Fortune 100 to create the products you love and use every day. Our Bugcrowd bug bounty program is critical to InVision’s continued security success. We have paid out over $100,000 in bounties to date. Bugcrowd specializes in bug bounty programs for some of the world’s most trusted brands including Tesla, Mastercard and Fiat-Chrysler. They employ some of the best security experts in our field with a team that can easily manage the volume of submissions we see here at InVision. If you want to test your hacking skills with an opportunity for fame, glory and financial reward, here is the information you need: Programs: Enterprise (contact Bugcrowd for access), Non-Enterprise Enterprise Site: bugbounty.invisionapp.com Non-Enterprise Sites: projects.invisionapp.com, www.invisionapp.com , freehand.invisionapp.com, muz.li Built with: ColdFusion, ReactJS, Golang, Node.js, WordPress (blog), PHP (muz.li) Happy hacking! Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2017-12-15"},
{"website": "Invision", "title": "9 Tips for a Painless Microservices Migration", "author": ["Ryan Scheuermann"], "link": "https://engineering.invisionapp.com/post/painless-microservices-migration/", "abstract": "The InVision platform started on a monolithic codebase. Like many SaaS companies that have grown rapidly, we found\nourselves needing to move to microservices in order to scale our pace of development across multiple engineering teams. We’re not going to argue that we should have started with microservices first. In fact, we agree with Fowler’s MonolithFirst approach. The complexities of microservices will slow down a small team. As we’re in the middle of moving to microservices, we found ourselves asking: Knowing what we know now about microservices, what would we have done differently when we started building out\nInVision as a monolith? In other words, what lessons can we apply to the next startup in which we build a monolith? When we do this again, how\ncould we ensure it’s a smoother and (mostly) pain-free transition to microservices? While still being cautious not to slow down early-stage development for the sake of an unpredictable future. Time is\nmoney, and every startup is limited by their runway. So, where do we draw the line between sensible, frugal\nfuture-proofing and costly premature optimization? It’s tempting to say “we’d do nothing differently”. Hindsight is 20/20, after all. Even some of our own would argue to\navoid the trap of microservices: When you are starting out, you want to optimize for speed of development, and I’m not sure doing any sort of thinking\nabout microservices is the right choice. - Ben Darfler, Lead Engineer But I’m of the camp that there is always something to be learned from our past – so that our experience and choices may\nserve as a lesson for our future selves or others. Here are some lessons we learned to make your next monolith to microservices migration faster and less painful. 1. Draw Domain Lines Even if you’re not going to carve up your APIs and data storage into separate services, take the simple step of defining and documenting your business entities early on. These can and should be very coarse-grained at first. Your monolith should not be a meme. When you’re coding, be mindful of how you cross the boundaries of these entities. Try not to circumvent the class boundaries when coding complex tasks. Respect them! Dependency injection may make it\nconvenient to reference and use any other library or service class within your monolith, but this practice can quickly\nget you into an architectural rats nest. Try not to do SQL JOINs or DB transactions across boundaries. Not only will this not scale when you want to break out\nto microservices, it will slow down your query performance as the number of rows increase, too. I would have moved more SQL JOIN statements out of the database and into the application server. I love SQL, and\nthe younger me would often try to find ways to make the database do as much work as possible, joining this table to\nthat table, running aggregations, and coalescing various columns. However, as we began to scale, it quickly became\napparent that the database does not scale as well as the application servers. And, by moving the JOIN s into the code,\nwe could have made our queries faster, easier to reason about, and less CPU intensive. - Ben Nadel, Founding Engineer Other than future microservice implications, this will also eventually cause database lock timeouts and degrade your DB\nperformance. When you keep your SQL JOINs simple, it will be much less painful to break those JOINs out into cross-service calls\nlater on. 2. Document Your URL Route Domains By the time we started carving out microservices, we had over 700 URL routes defined in the monolith’s route handler.\nHow did we know which URL routes belonged in our brand new domain-specific microservice? We didn’t. As developers, we like to think that all our URL route structures will be entirely logical. You know, every\nusers-related URL path will start with /users and they’ll all be grouped together in the routes file. When you have\na team of 5, of course all the engineers will follow that standard (you hope). When you grow to a team of 50 and have\n100s of routes, assume that a new engineer will just add a new route somewhere else using a different convention. Adopt a standard in the routes files for documenting the domain to which that URL route belongs. // Domain: Comments\n{ \"$POST/api/comments\" = \"/api:comments/create\" },\n\n// Domain: Screen Groups (START)\n{ \"$POST/api/screen-group/create\" = \"/api:screen_groups/create\" },\n{ \"$POST/api/screen-group/update\" = \"/api:screen_groups/update\" },\n{ \"$POST/api/screen-group/update-sort\" = \"/api:screen_groups/updateSort\" },\n{ \"$POST/api/screen-group/delete\" = \"/api:screen_groups/delete\" },\n// Domain: Screen Groups (END) 3. Be Explicit About Routes And Methods Do not use wildcard routes, like these: // API: generic RESTful requests\n{ \"$DELETE/api/:resource/:id\" = \"/api::resource/delete/id/:id\" },\n{ \"$GET/api/:resource/:id\" = \"/api::resource/get/id/:id\" },\n{ \"$POST/api/:resource/:id\" = \"/api::resource/update/id/:id\" },\n{ \"$POST/api/:resource\" = \"/api::resource/create\" }, The above routes could be used to handle multiple domain entities, but how would you know which ones? When you’re about\nto move the handling of an entire domain entity out to a microservice, you’ll need to know. So be explicit! Furthermore, don’t use wildcard verbs/HTTP methods (is this a GET or a POST, or both?): { \"/api/layersync/items\" = \"/api:layerSync/items\" }, This pattern will give you no REST when moving to a microservice. Hopefully you’ve instrumented your URL endpoints, so you know how it’s being hit, but if you haven’t, it’s going to be\nvery difficult to determine how this endpoint is being used. And therefore, it’s going to be difficult to work out how\nto migrate it effectively to a microservice. This is also useful when sunsetting features, as noted by LinkedIn’s Lessons Learned from Decommissioning a Legacy Service : When removing one endpoint, I thought it was a simple endpoint that could be migrated by redirecting it to an\nalternative endpoint. But after doing this, another team complained that some functionality was broken. I figured out\nthat part of the endpoint that served very limited traffic used HTTP POST, while 99% of traffic used HTTP GET. 4. Assign URL Endpoint Ownership Carrying over from the last tip, you should also be explicit about ownership. Ownership isn’t about who is allowed to\ntouch the code , but rather about who is responsible\nfor its operation in production. Drawing domain lines will only aid in determining this ownership, and likely result\nin a clean formation of teams in the future. We didn’t identify owners early, but if we had, we would have used our routes file to document the team that owns each\nendpoint in the monolith: // Domain: Comments, Team: Red Team\n{ \"$POST/api/comments\" = \"/api:comments/create\" },\n\n// Domain: Screen Groups (START), Team: Gold Team\n{ \"$POST/api/screen-group/create\" = \"/api:screen_groups/create\" },\n{ \"$POST/api/screen-group/update\" = \"/api:screen_groups/update\" },\n{ \"$POST/api/screen-group/update-sort\" = \"/api:screen_groups/updateSort\" },\n{ \"$POST/api/screen-group/delete\" = \"/api:screen_groups/delete\" },\n// Domain: Screen Groups (END) When you’re a small group of engineers, it can be awkward to assign ownership at the engineer level because everyone\nfeels its a shared responsibility. Our advice is to be explicit, even if multiple owners are identified. Don’t let any\nURL endpoint have ambiguous ownership. Not only should you be using ownership documentation to determine which who is\nresponsible for the operation and monitoring of that endpoint, but when it comes time to carve it out into a\nmicroservice, you know exactly who to talk to about it. 5. Monitor URL Usage Did we mention you should be instrumenting all those endpoints? While you should also be graphing the error rate and\nperformance of every HTTP endpoint you expose, at the very least you should graph the request rate. We weren’t. When it came time to break endpoints off the monolith, one of the first steps was to identify which\nendpoints were still receiving traffic and how much. We identified approximately 25% of our endpoints were no longer used. Dead URLs are a waste of screen real estate for your dashboards. Some endpoints weren’t dead but were rarely hit. In the effort to break a section of the monolith out to a microservice,\nyou might also consider removing a rarely used feature from the product entirely. 6. Kill Dead Code When you’re growing rapidly, your codebase is going to grow rapidly too. Be extra diligent about cleaning up dead code.\nDelete it. Don’t just comment it out – don’t leave it there at all. // I am not sure if we need this, but too scared to delete. You have source code control for history, if you need it. But unless you’re rolling back a deployment, you will rarely,\nif ever, need to go back and reference that code. 7. Build (and Document) Ports & Adapters When you’re building your monolith, use an hexagonal architecture – build abstraction ports for all incoming requests and build abstraction adapters for all outgoing requests. Possibly common sense to most, but when you’re moving quickly and adding new team members, they don’t have all the\ncontext for what ports and adapters already exist. So it’s likely they end up building a direct connection to another\nAPI or building another adapter class for it. And this can lead to a big mess when you’re trying to carve out edges of\nyour monolith into microservices. For example, our monolith had a handful of places in the code that would read or write Amazon S3 objects. Some of those\nplaces used a standard service class and some of those places used the S3 API directly. When it came time to swap out\nour S3 implementation details (by moving them to a microservice), we had to find and swap out all the places using S3\ndirectly instead of just swapping out the adapter class. 8. Build a Shared Service Catalog When you’re building services, ports, or adapters, document them somewhere all your new contributors will reference. We\nrefer to this as our Service Catalog. This will decrease the likelihood new contributors re-invent the same systems,\nand thus decrease the complexity of carving that area of the code out into a microservice later. 9. Document Your Environment Variables Breaking out into microservices means you’re also breaking out the deployment configuration. Our monolith had over 200\nenvironment variables. 😱 Without documentation, it was extremely difficult to know what they were for, if they were\nstill necessary at all, and more importantly, which were relevant to the area of code that was being moved to a\nmicroservice. So, please, document what environment variables a service, class, module uses. You’ll thank yourself later. Conclusion Many of the above tips may seem like common sense, and most of them can be summarized as “create clean boundaries” and\n“write things down”, but when you’re moving fast, it’s easy to forget these disciplines. And that’s really the most important tip: when you get started, set out some standards and be disciplined in meeting\nthem. But be cautious your standards don’t add unnecessary overhead. We’ve tried to be diligent in outlining tips we\nbelieve balance that overhead with future value. Finally, the next time you’re wading through the monolithic codebase and feeling crushed by the weight of it, take a\nmoment to remember where that codebase got you. Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2017-09-08"},
{"website": "Invision", "title": "White Elephant Holiday Festivities", "author": ["Julie Griech"], "link": "https://engineering.invisionapp.com/post/remote-gift-exchange/", "abstract": "Here at InVision, we enjoy a fully distributed work environment. We get to work with people from all across the globe every single day! Our unique environment comes with many perks and great experiences, and we wanted to add to that by doing something special together for the holidays. We had to find something that would actually work and be fun for a fully remote team. Think new tradition and memorable as well. We quickly thought of trying to do a gift exchange, so we researched how other teams and remote companies have celebrated the holidays together. There were varying ideas and we found that each one had something small to offer us, so we chose what would work best for us. All of our research led us to one definite and clear conclusion: it is supposed to be fun and not add any stress to anyone’s life. We chose to celebrate together with a white elephant gift exchange! Our next big task was to iron out the finer details of our fun. We set the spending limit to $20 per gift. Since we have team members all across the globe (as far away as Australia 🇦🇺 ) and we wanted everyone to participate, we came up with an easy solution to get gifts there and back. We have an in-person company meeting in February, so gifts could be exchanged in person at that time if desired. We planned for a Zoom video call (Brady Bunch style) during the week before December 25th, since most people were not out on vacation yet. Lastly, we created a Slack channel for all communication related to the festivities, which we also used during our gift exchange. Next we got to share the plan with the team, and we did that on our November monthly call. Planning Details Celebrating Together Once we were on our party video call, we started with the first name on the list. That person selected a person’s gift to open. We found the easiest way to present a gift was to upload a picture into the Slack channel. Each next person on the ordered list could choose to either open an unopened gift, or steal a gift that was previously opened. If a gift was stolen from someone, that person would then choose another unopened gift (no circle of stealing). This went on until all gifts were opened. The last ordered person on the list had the best advantage because they could choose to steal any of the gifts. We had about 15 people participate in the gift exchange. We were able to work out some house rules as things were happening because we were not a larger group of people, and our exchange took under one hour. If the group was larger, we’d likely have a small challenge keeping track of who opened what. I believe this could be resolved with a slower pace so that we could keep an ongoing list as gifts were being opened. We used Slack to track the selected and stolen gifts, which made following along much easier. The gifts were varying in that some were funny gag gifts and some were more useful gifts…all gifts were well-received. Most of the gifts were bought and shipped via Amazon. I mean…look how much fun we are having Some people might feel these slippers are a serious gift 😂 The whole experience was great fun for us…lots of smiles. We will keep the tradition going next year, and I believe participation will be higher now that it has happened once and we know it can be done. With a little research and planning, we were able to bring some unique holiday fun to our team. We’ve definitely started our tradition and it will only get better from here. Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2018-01-23"},
{"website": "Invision", "title": "Introducing go-health", "author": ["Josh Brown"], "link": "https://engineering.invisionapp.com/post/go-health-scalable-checks-for-kubernetes/", "abstract": "Today, InVision is excited to introduce go-health — the scalable health checking library for Golang services. go-health is designed to help developers set up Kubernetes health check probes that are safe from DoS attacks and scalable for large deployments. The challenge Kubernetes has two kinds of health check probes: liveness probes and readiness probes. Readiness probes signal to the kubelet agent that a pod can begin handling traffic. When a readiness probe is defined, traffic will only reach a pod after a 2xx status response is received from the readiness probe endpoint. Liveness probes work similarly to readiness probes but they help your service communicate that a running pod has deadlocked or become unresponsive. Kubernetes will restart a pod after it fails to receive a positive response from a liveness probe. When we wrote our first health checks, InVision services used the same HTTP endpoint for both liveness and readiness probes. This approach can work well for small services and simple health checks. However, as InVision grew, we found ourselves creating longer, more complex health checks that put strain on downstream dependencies. A typical request to a /healthcheck endpoint led to a database query, network calls to other health check endpoints and test API calls to dependent services. These checks were often performed synchronously, in which case the /healthcheck endpoint could not return a response until they completed. go-health to the rescue Daniel Selans, Tatsuro Alpert, and Steve High on the InVision team developed go-health to improve the reliability of our core services - providing a standard, reliable way for all services to verify their own state. InVision now uses go-health in most of our production Go services. go-health helped solve our problem with complex health checks by performing them asynchronously in the background. When a request is made for a health check, the last known state is returned. This approach helps protect our health check dependencies from stress caused by restarting a large deployment of pods. It also insulates us from a DoS attack, should a malicious actor gain access to the health check endpoint. go-health comes with three types of checkers built in: HTTP checks, SQL queries and Redis queries. You can also create your own checker by implementing the ICheckable interface . h := health.New()\ngoodTestURL, _ := url.Parse(\"https://invisionapp.com\")\ngoodHTTPCheck, _ := checkers.NewHTTP(&checkers.HTTPConfig{\n        URL: goodTestURL,\n})\nh.AddChecks([]*health.Config{\n        {\n                Name:     \"good-check\",\n                Checker:  goodHTTPCheck,\n                Interval: time.Duration(2) * time.Second,\n                Fatal:    true,\n        },\n})\nh.Start() In the example above, the health check consists of a GET HTTP request to InVision’s homepage. An HTTP dependency check can send a GET , HEAD , POST or PUT request, with or without a request payload. You can configure the health check to fail based on a bad status code or a timeout (which is the default) or based on the contents of the response body. This library comes with built-in checkers for two types of datastores: SQL and Redis.  You can use go-health to ping the specified datastore and verify that it is reachable by your service, or execute a read or write operation to test that the datastore can be accessed. Using go-health, you can also register status listeners . Status listeners will call a function when a dependency begins to fail or when a dependency recovers from failure. Listeners are a great place to add logging functionality to your health check. Adding go-health to our toolkit did not completely solve InVision’s challenges with complex health checks. We also created guidelines to reduce the number and cost of the dependency checks within InVision’s health check probes. In particular, InVision engineers cut back on dependency checks that were performed in liveness probes and moved them to the readiness probe. Readiness probes are usually the best place to verify that your service can connect to datastores and other APIs that it requires to do its job. Get started go-health is available on GitHub under the MIT license. Take a look at the code examples , try it out in your own Golang services and let us know what you think. Have questions about the library? Leave us a comment below! Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2018-07-26"},
{"website": "Invision", "title": "Join us for the Global Day of Coderetreat on November 17th", "author": ["Philip Borlin"], "link": "https://engineering.invisionapp.com/post/global-day-of-coderetreat-2018/", "abstract": "InVision is hosting a remote-friendly event for the Global Day of Coderetreat . GDCR is an opportunity to practice software development and design in a friendly and low-pressure environment.  Come join us on Saturday November 17th starting at 10AM EST (UTC -5) and plan for a fun and enlightening day. There is no cost to participate in this event. What will we be doing for eight hours? After a quick introduction to the tooling for the day and some administrative words, participants will pair off into teams and begin a 45 minute coding session. During this session, each team will work together to solve Conway’s Game of Life . This exercise will be all about test-driven development and getting used to pair programming. When the first session is complete, each team will be asked to do one of the hardest things a programmer can do… delete their code. Starting from scratch will help us leave behind our initial solutions to the problem and begin the second round with a fresh approach. We will also have a quick retrospective, so that developers can discuss the exercise and share learnings with others at the Coderetreat. In the second round, each person at the Coderetreat will pair up with a new partner and again work to solve Conway’s Game of Life. We will still be pair programming and doing test-driven development, but there will be a twist. What will it be? You’ll have to come to find out. Over the course of the day, developers will participate in up to six pair programming sessions, each with a different and unique constraint. Any programming language can be used for these exercises and we hope you’ll have a chance to try a new language you have never used before. The emphasis is on fun and learning, so come prepared to challenge yourself. What do I need? You will need a computer and an internet connection that works well for video conferencing. Plan to spend the entire day at the Coderetreat. This event is open to junior and senior developers alike. Please also come prepared with a remote development environment, like Cloud9 or Visual Studio Code with Live Share .  Create a sample project in your language of choice that will give you a runnable Hello World program you can use as a template. This template will help you quickly spin up your environment, so you can be prepared for the next session. Expect that sometimes you will use your environment and sometimes you will use your partner’s environment. Anything else? We will have a lunch break at approximately 1PM EST. Due to timezones and the remote nature of the event, the break may not come at a time during which you are accustomed to eating. The break is a time to talk and socialize and it may or may not be about food for you. If 1PM EST isn’t a comfortable time for you to eat, we suggest you come with something ready that you can eat between sessions. We look forward to seeing you at the Coderetreat! Spaces are limited — you can find out more and register for InVision’s Day of Coderetreat here. Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2018-11-12"},
{"website": "Invision", "title": "Join us for the Global Day of Coderetreat on November 16th", "author": ["Jason Prasad"], "link": "https://engineering.invisionapp.com/post/global-day-of-coderetreat-2019/", "abstract": "For the 10th anniversary of the Global Day of Coderetreat , InVision is once again hosting a remote-friendly event. In the spirit of GDCR, the day will be dedicated to providing an encouraging space for practicing the fundamentals of software craftsmanship such as modular design, pair programming, and test-driven development. It’s free to attend and open to all skill levels. Join us starting at 10AM EST (UTC -5) on Saturday November 16th . What will we be doing for eight hours? We will start with a few administrative items, the agenda, and getting to know one another. Afterwards we will we will work together to implement Conway’s Game of Life in up to six ~45 minute sessions. In each session we’ll do the following: find a new partner, discuss and solve Conway’s Game of Life together, and delete all the code at the end. As if starting from scratch each time weren’t enough disruption, we’ll allow for picking a new language or framework and even add a constraint to how we solve the problem. It’s all about staying out of a comfortable routine and deliberately practicing our craft together. Active pair programming and test-driven development will be kept constant and highly encouraged throughout the day. After each session we’ll come together to have a brief retrospective. This will be the time to share what we learned, our solutions, the challenges of the constraint, what went well, or what did not. Expect to have some breaks throughout the day and an extended break around 1pm EST. If it’s convenient in your time zone the extended break may be used to have lunch and most certainly to socialize with your fellow software craftspeople. By the end of the day we will have been challenged, experienced in other languages and approaches, worked with other like-minded individuals, and improved in our craft. What do I need? You will need a computer, an internet connection that works well for video conferencing, and enough time to spend the day at the Coderetreat. Slack will be used for direct communication and coordination. To facilitate pairing we will be using Visual Studio Code with Live Share . Before the event, it will be helpful to have your development environment set up and integrated with Visual Studio Code. Create a template of the environment with a runnable Hello World program and preferably with a testing framework prepared. As we’ll be frequently starting over in each session expect to use other people’s environments as well. Anything else? This is a day about deliberate practice and learning. For many, pair programming and test-driven development may be new or less often used practices. We all have different experiences, approaches, and skill levels. Come to the event with a willingness to listen, to support, and to learn from one another! We look forward to seeing you at the Coderetreat! Spaces are limited — you can find out more and register for InVision’s Day of Coderetreat here. Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2019-10-23"},
{"website": "Invision", "title": "Building the Inspect measurement engine", "author": ["Devin Schulz"], "link": "https://engineering.invisionapp.com/post/building-the-inspect-measurement-engine/", "abstract": "Not too long ago I had the opportunity to construct a measurement system inside\nof Inspect that depicts and calculates the distances between two layers.\nIn-between each layer, a line is drawn with a label that displays the distance.\nSupporting lines are added to the nearest edges of the hovered layer to help\ngive the user an idea where the measurement lines reach. In this article, I’ll explain some of my ideas and how I managed to take\na fairly complex problem, break it down, and deliver something of value to an\nend user. I hope by you reading through this article; you’ll have a better\nunderstanding of how to dissect a problem you may be encountering. I know when\nI was handed the ticket to implement this functionality, it was intimidating. After sitting down and thinking about how to approach this problem, I concluded\nthat a higher-order component would handle the business logic and then pass the\nrequired data down to a stateless component. I came to this conclusion by\nthinking about how I could break a single piece of functionality down into\nindependent pieces. If you think about it, there are really only two main\npieces: the logic, and then the view. Breaking down the task at hand By breaking down a task into several smaller tickets, you immediately reap in\nall these benefits: You can give better estimates on how long a single ticket\ntakes since the scope is much smaller. You feel more productive shipping\nmultiple smaller tickets in a single sprint versus a giant one. At InVision, our team uses JIRA to\nmanage bugs, new features and everything that needs to be tracked or accounted\nfor. By separating the view from the business logic, not only can I now split\na single ticket into two smaller tickets, but was able to go one step further.\nI created a ticket to deal with outlining what values should be passed from the HOC to the child component. Three\ntickets have been created from what was originally one and can now be\ntracked and dealt with independently. The outline Every problem gives you better results if you sit down and think about how to\napproach it before execution. This step is a great time to bounce ideas off your\ncoworkers to see if they agree with your suggested approach or have a way to\nmake it even better. Without this first step, I may have over-engineered or made\nthis much more complicated than it needed to be. Looking at the design mockups from the designer, the first thing I see I can see\nthat I need to draw a rectangle around both the selected and a hovered layer.\nBoth of these values exist within the global state so that I can pass these\nproperties from a parent component and down to the view without additional data\ntransformation. type Props = { selectedLayer ?: Layer , hoveredLayer ?: Layer } The next obvious thing that needs to be pass down the view component is all the\nmeasurements. There can be up to four separate measurements rendered at a single\ntime, so the clear choice here is to use an array of objects. Now I need to\nfigure out the shape of each measurement. type Measurements Measurement [] Each measurement must have a direction which is either top, right, bottom or\nleft. Based on this information alone, I’ll use this to draw a specific side of\nthe border. The first property I’ll add to the measurement object is the\ndirection. Next, I need a start and end position for each measurement. It\ncontains an an x, y coordinate with either a width or height, depending on\nwhether it’s vertical or horizontal. type Position = { top : number , left : number , width ?: number , height ?: number } type Measurement = { direction : number , label : Label , position : Position } The last item that exists within each measurement is the label to display the\ndistance. Whenever the measurement is inside the artboard, the label is\npositioned in the middle of the measurement line. To position the label, I’ll\nneed an x and y coordinate. type Point = { x : number , y : number } type Label { point : Point , value : string } Side note: the outline was created using Flow type\ndefinitions to help validate that the data types passed down the view is\ncorrect. Both Flow and Typescrpt are\nexcellent choices if you want the type safety and reassurance that the right\ndata is being passed around in your application. You end up with fewer bugs and\nfeel way more confident in your codebase. Here is the entire structure: type Point = { x : number , y : number } type Label { point : Point , value : string } type Position { top : number , left : number , width ?: number , height ?: number } type Measurement { direction : number label : Label , position : Position } type Measurements Measurement [] The business logic The higher-order component contains all our business logic which is the heart of\nthis functionality. Its job is to take the hover and selected layer and\ntransform that data into the structure defined in the outline. The key thing to determine was where in relation one layer is to another, and\nthen calculate the distance between the closest sides. These values are\ntypically what a developer is most interested in to set the position, margin or\npadding of an element. There are two main paths which the logic can follow,\neither the selected and hover layer are overlapping each other, or they are not. Two layers which intersect Two layers in relation to eachother Before I dive into both paths, there are a couple of checks which need to\nhappen. First, I need to determine if only selected layers are passed in or both\nthe selected and hovered layers. If only the selected layer is passed in, I use\nthe properties from the artboard and create the hovered layer manually. This\nallows me to calculate the position of the layer in relation to the artboard\nitself without adding additional checks whether the layer is an artboard or not. The properties I worry about for each layer are the width, height, and x,\ny coordinates. Here is a simplification of the layer shape, but it gives you an\nidea of some basic properties it contains. type Layer = { id : number , width : number , height : number , x : number , y : number } To figure out which path I can take, I started by determining if the\nlayers are overlapping by both the x and y-axis. If both of these return true,\nI know which path I’ll need to follow. // Determine if the x axis plus width overlap const xIntersects = ( layer1 : Layer , layer2 : Layer ) : boolean => Math . max ( 0 , Math . min ( layer1 . x + layer1 . width , layer2 . x + layer2 . width ) - Math . max ( layer1 . x , layer2 . x ) ) > 0 // Determine if the y axis plus height overlap const yIntersects = ( layer1 : Layer , layer2 : Layer ) : boolean => Math . max ( 0 , Math . min ( layer1 . y + layer1 . height , layer2 . y + layer2 . height ) - Math . max ( layer1 . y , layer2 . y ) ) > 0 // Determine if both x and y-axis plus width and height overlap const overlaps = ( layer1 : Layer , layer2 : Layer ) : boolean => xIntersects ( layer1 , layer2 ) && yIntersects ( layer1 , layer2 ) The next piece of information shared among all different paths is the direction.\nThe way I’ve decided to calculate the direction is to think of a 3x3 grid which\nstarts at zero and increments in a clockwise fashion. Starting from the middle,\neight, I need to determine where the second layer is in relation to the first\none. Each direction numbered from one to eight in relation to the center const BOTTOM = 5 const BOTTOM_LEFT = 6 const BOTTOM_RIGHT = 4 const CENTER = 8 const LEFT = 7 const RIGHT = 3 const TOP = 1 const TOP_LEFT = 0 const TOP_RIGHT = 2 const DIRECTIONS = [ TOP , TOP_RIGHT , RIGHT , BOTTOM_RIGHT , BOTTOM , BOTTOM_LEFT , LEFT , TOP_LEFT ] Now that I’ve set up these variables I can work on the math portion to determine\nthe cardinal direction between the two layers. This can be accomplished by\ndetermining the absolute center of both layers and then calculating the angle\nbetween the two. Two shapes with their centers marked and a line drawn between the two to demonstrate the angle const getDirection = ( layer1 : Layer , layer2 : Layer ) : number => { // Get the x and y center of each layer const layer1Center : Point = getCenter ( layer1 ) const layer2Center : Point = getCenter ( layer2 ) // Check if this coordinate is dead center with other. if ( layer1Center . x === layer2Center . x && layer1Center . y === layer2Center . y ) { return CENTER } const angle = getAngle ( layer2Center , layer1Center ) return angleToDirection ( angle ) } // Convert the angle to a direction const angleToDirection = ( angle : number ) : number => { const directionCount = DIRECTIONS . length return DIRECTIONS [ Math . floor ( angle / ( 360 / directionCount ) + 0.5 ) % directionCount ] } // Calculate the angle between the center of two points const getAngle = ( point1 : Point , point2 : Point ) : number => { const angle = ( Math . atan2 ( point2 . y - point1 . y , point2 . x - point1 . x ) * 180 ) / Math . PI - 90 return angle < 0 ? 360 + angle : angle } // Get the absolute center point of a layer const getCenter = ( layer : Layer ) : Point => ({ x : layer . x + layer . width / 2 , y : layer . y + layer . height / 2 }) Now that the logic is out of the way I can carry on with actually using it. const layerOverlapsLayer = overlaps ( selected , highlighted ) const cardinal = getDirection ( selected , highlighted ) Layers which overlap Overlapping layers is the easier option of the two paths to calculate. The first\nstep I chose to do here was to calculate all four sides of the selected layer\nand build up each measurement. This achieved by calculating the minimum and\nmaximum values for each side, and then subtracting the same side of the opposite\nlayer. const top = Math . abs ( layer1 . y - layer2 . y ) const right = Math . abs ( layer1 . x + layer1 . width - ( layer2 . x + layer2 . width )) const bottom = Math . abs ( layer1 . y + layer1 . height - ( layer2 . y + layer2 . height )) const left = Math . abs ( layer1 . x - layer1 . x ) Performing the above calculations gives us the difference for each side which we\ncan use to display the label width and height of the measurement lines. Next, I need to determine the intersection point of all sides. The function\nbelow determines the central intersection point between two layers. This\nfunction is essential to center the line vertically or horizontally between two\nfacing sides. Two layers which intersect const intersection = ( layer1 : Layer , layer2 : Layer ) : Point => { const xMax = Math . max ( layer1 . x , layer2 . x ) const yMax = Math . max ( layer1 . y , layer2 . y ) return { x : xMax + Math . max ( 0 , Math . min ( layer1 . x + layer1 . width , layer2 . x + layer2 . width ) - xMax ) / 2 , y : yMax + Math . max ( 0 , Math . min ( layer1 . y + layer1 . height , layer2 . y + layer2 . height ) - yMax ) / 2 } } Now that we have the intersection and offsets, we can use this information to\ncan start building out the measurements. I decided to calculate all four sides\nand then only return what is required. const yMin = Math . min ( layer1 . y , layer2 . y ) const xMin = Math . min ( layer1 . x , layer2 . x ) const yHeightMin = Math . min ( layer1 . y + layer1 . height , layer2 . y + layer2 . height ) const yHeightMax = Math . max ( layer1 . y + layer1 . height , layer2 . y + layer2 . height ) const xWidthMin = Math . min ( layer1 . x + layer1 . width , layer2 . x + layer2 . width ) const xWidthMax = Math . max ( layer1 . x + layer1 . width , layer2 . x + layer2 . width ) const intersects = intersection ( layer1 , layer2 ) const topMeasurement : Measurement = { direction : TOP , position : { top : yMin , left : intersects . x , height : top }, label : { point : { y : yMin + top / 2 , x : intersects . x } } } const rightMeasurement : Measurement = { direction : RIGHT , position : { top : intersects . y , left : xWidthMin , width : right }, label : { point : { y : intersects . y , x : xWidthMax - right / 2 }, value : right } } const bottomMeasurement : Measurement = { direction : BOTTOM , position : { top : yHeightMin , left : intersects . x , height : bottom }, label : { point : { y : yHeightMin + bottom / 2 , x : intersects . x }, value : bottom } } const leftMeasurement : Measurement = { direction : LEFT , position : { top : intersects . y , left : xMin , width : left }, label : { point : { y : intersects . y , x : xMin + left / 2 }, value : left } } Now that all the measurements values are calculated, I can work on the returned\nvalue. When the selected layer is perfectly centred with the hover layer, all\nfour measurements will be returned since I cannot successfully determine which\nside the end user wants to see. if ( cardinal === CENTER ) { return [ topMeasurement , rightMeasurement , bottomMeasurement , leftMeasurement ] } When the two layers are not concentric, all sides which extend beyond the\nselected layer need to be omitted from the result. const measurements : Measurements = [] // Top if ( layer1 . y < layer2 . y ) { measurements . push ( topMeasurement ) } // Right if ( layer1 . x + layer1 . width > layer2 . x + layer2 . width ) { measurements . push ( rightMeasurement ) } // Bottom if ( layer1 . y + layer1 . height > layer2 . y + layer2 . height ) { measurements . push ( bottomMeasurement ) } // Left if ( layer1 . x < layer2 . x ) { measurements . push ( leftMeasurement ) } return measurements Non-overlapping layers Non-overlapping layers are when two layers do not intersect in any way. They are\na bit more complicated to calculate since there are many more variables to\nconsider when comparing it to two overlapping layers. My thought of approaching this is to create a switch statement based on the\ncardinal direction and calculate each side independently. There would be a total\nof eight cases to account for, top left, top, top right, right, bottom right,\nbottom, bottom left and left. Since these layers never overlap in any way, I’ll\nnever have to worry about calculating the CENTER . const distance = ( cardinal , layer1 , layer2 ) => { switch ( cardinal ) { case TOP_LEFT : ... break case TOP : ... break case TOP_RIGHT : ... break case RIGHT : ... break case BOTTOM_RIGHT : ... break case BOTTOM : ... break case BOTTOM_LEFT : ... break case LEFT : ... break } } Each measurement would be calculated independently and similarly as the\noverlapping layers. On paper, this seemed to work well and would account for\nmost cases I would expect. After some initial testing, I noticed\nthat some cases did not display what I had expected to see. Whenever a layer is\nin-between two directions, lets say TOP and TOP_LEFT , it would fall into the TOP_LEFT case. Actual result Expected I added an if statement to all four corners (sides 0, 2, 4, and 6) to check if\neither the hover layer overlaps the selected layer or the hover layer extends\npast the selected layer. Whenever this was true, I changed the cardinal\ndirection to either TOP , RIGHT , BOTTOM or LEFT , depending on the\noverlap, and returned the distance function. const xWidth = layer1 . x + layer1 . width const yHeight = layer1 . y + layer1 . height const x2Width = layer2 . x + layer2 . width const y2Height = layer2 . y + layer2 . height const overlapsMiddleY = layer2 . y < layer1 . y && y2Height > yHeight const overlapsMiddleX = layer2 . x < layer1 . x && x2Width > xWidth if ( overlapsMiddleX || layer2 . x >= layer1 . x || x2Width > layer1 . x ) { return distance ( TOP , layer1 , layer2 ) } else if ( overlapsMiddleY || layer2 . y >= layer1 . y ) { return distance ( LEFT , layer1 , l ) } Putting it all together So far I have thrown example after example at you but haven’t given you an idea\nhow it all fits together. In the next example, I put together a super stripped\ndown version, so you have an idea of how this all works. // composites/measurementHandler.js type Props = { artboard : Artboard , hoveredLayer ?: Layer , selectedLayer : Layer } export default ( ComposedComponent : React . ComponentType <*> ) => class SelectionMeasurementHandler extends React . Component < Props , void > { render () { let { selectedLayer , hoveredLayer , artboard } : Props = this . props // If there is only a selected layer and no hovered layer, use the artboard // measurements instead so you can see where the layer is in relation to // the artboard. if ( ! hoveredLayer ) { const { width , height , id } = artboard hoveredLayer = { x : 0 , y : 0 , id , width , height } } // Is the selected layer inside the hovered layer or is the hovered layer // inside the selected layer? const inside = contains ( selectedLayer , hoveredLayer ) || contains ( hoveredLayer , selectedLayer ) // Does the selected layer overlap the hoveredLayer in any way const layerOverlapsLayer = overlaps ( selectedLayer , hoveredLayer ) // Get the cardinal direction of the selected layer in relation to the // hovered layer const cardinal = getDirection ( selectedLayer , hoveredLayer ) // A simple ternary operator to determine which type of distance // measurements should be calculated. const measurements = layerOverlapsLayer || inside ? this . distanceOverlaps ( cardinal , selectedLayer , hoveredLayer ) : this . distance ( cardinal , selectedLayer , hoveredLayer ) return < ComposedComponent {... this . props } { measurements } > } distance ( cardinal : Direction , layer1 : Layer , layer2 : Layer ) : Measurements [] { // All the logic that is related to determining the measurements whenever // the layers do NOT touch. See the “Non-overlapping layers” section of // this article. } distanceOverlaps ( cardinal : Direction , layer1 : Layer , layer2 : Layer ) : Measurements [] { // All the logic that is related to determining the measurements whenever // the layers DO touch or overlap. See the “Layers that overlap” section // of this article. } } Again this is a super simplified version of the component but works to\nillustrate my thinking around how it works. There are a couple of areas that\nI skipped intentionally in this article since it’s already getting pretty\nlong. There are two main things I didn’t cover so I’ll give you a brief\nsynopsis. Zoom level Inspect allows users to adjust the zoom level from 13% to 800% which gets them\nup and close with the design. Since static measurements are used throughout,\nI needed to account for the scale. This is easily obtainable by multiplying each\nmeasurement x, y, width, and height by the zoom level. const displayScale = ( scale : number , value : number ) : number => value * scale Dotted helper lines Throughout the examples in this article, you may have noticed dotted lines which\nstart in a corner of the selected layer. These lines are calculated similarly\nas the measurement lines and are passed down as another property to the\nmeasurements component. They are simply an array of positions. type Dotted [] Position The view Now with the complicated part out of the way, I now needed to display the\nresulting measurements within the view. I try to use stateless components as\nmuch as possible because I like the functional aspect of it. You have the\nguarantee that the result is the same with the same input. With that in mind,\nI wrapped a stateless component in the composite component created in the\nbusiness logic section. // components/measurements.js import SelectionMeasurementHandler from ‘ composites / measurementHandler ’ // Export decorated components separately so they can // easily be unit tested. export const Measurements = ({ measurements }) => measurements . map ( measurement => < Measurement {... measurement } /> ) export default SelectionMeasurementHandler ( Measurements ) Single measurements are super simple in that they take the measurement\nproperties and output a line and label. The line styles are passed down through\nthe measurement position property and depending on whether it’s vertical or\nhorizontal, CSS styles apply a border right or bottom. The labels point ends up in the middle between the start and end measurements,\nhowever, relies on the parent class measurement--<direction> to determine if\nthe label floats above the line or to the right. // components/measurement.js import cx from 'classnames' type Props { measurement : Measurement } const Measurement = ({ label , position } : Props ) => { const classes = cx ( 'measurement' , { 'measurement--top' : !! position . top , 'measurement--right' : !! position . right , 'measurement--bottom' : !! position . bottom , 'measurement--left' : !! position . left }) return { < div className = { classes } style = { position } > < div className = 'measurement__line' style = { position } /> < div className = 'measurement__label' style = { label . point } > { label . value } < /div> < /div> } } export default Measurement Conclusion If you managed to read through this whole thing, hats off to you. There is quite\na bit here. I hope some of this information is relatable and gives you some\nideas about how to approach a project of your own. I sure learned a lot while\nbuilding it. If there is anything to take away from this article is that you should always\nthink and plan out the problem beforehand. Try breaking tickets into small\nshippable pieces instead of an entire thing all at once. If you don’t already do\nthis, try bouncing ideas off your coworkers to help validate your own. You may\neven work out a more appropriate solution to your problem. I want to give a shout out to my manager at the time Ryan Scheuermann\n( @rscheuermann ) for helping me work through\nsome of the challenges, and both Jeremy Wight and Blake Walters ( @markupboy ) for helping\nreview this post. Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2019-07-17"},
{"website": "Invision", "title": "Our New-Year Pairing Experiment", "author": ["John Newman"], "link": "https://engineering.invisionapp.com/post/our-new-year-pairing-experiment/", "abstract": "InVision is a distributed company with a good number of engineering teams. Because of the remote nature of the work, it’s easy to start feeling isolated or disconnected unless you put something in place to help combat those feelings. In our case, somewhere around mid January, my team got the crazy idea in our heads that we might be able to curb feelings of remote isolation by finding more ways to work together instead of alone. As we talked, we began to realize that, by and large, members of the team either really loved pair programming, or were open to trying it more frequently. So we decided to set up an experiment. We determined that over the next few weeks, we would try to pair on everything as much as possible . We didn’t make pairing fully mandatory. But if there was a task that seemed in any way 🍐-able, we were strongly encouraged to make a pairing session out of it. About a month later, we believe our efforts have been markedly successful. Weekly retrospectives are consistently surfacing a general sentiment of deep enjoyment over the time we’ve spent pairing. Many of us have felt more connected to the team and our personal relationships have grown. We also had one brand new InVisioner (myself) who benefitted like crazy from being able to work closely with others while getting his feet wet. From a business-oriented perspective (although we don’t have rigorous statistics to back this up), our efforts do not seem to have hurt our efficiency. Our sprint velocity appears to have remained right in line with what it has been in the past. But that’s not really what we want to focus on here. More importantly, we learned a lot about effective pairing during our experiment. So now we’d like to document what worked well for us on our team, in hopes that you’ll have a solid foundation for building even more pairing experiments in the future. So what exactly do we mean by “pairing”? When we initially started the experiment, one of the things we considered was the option of “pairing” vs. “mobbing” – the key difference being that pairing is usually thought of as two people working together whereas mobbing is usually thought of as the whole team gathered around a single “driver” controlling the code editor. What we quickly discovered is that caring too much about these definitions and the conditions for when each technique is more appropriate is a pretty useless endeavor. We figured we could probably just launch our experiment without participation rules and see how it went. If we needed to revise, we could always do so at any time. With that in mind, this was the breakdown we saw for organic participation over the course of a month: Between January 14, 2020 and February 14, 2020… 32 “visible” pairing sessions occurred. (These were calls posted in our team’s private Slack channel, as opposed to calls posted in direct messages between team members.) 20 (~63%) of these sessions had only 2 participants. 9 (~28%) had 3 participants. 2 (~6%) had 4 participants. 1 (~3%) had 5 participants. It’s important to note that some number of private pairing calls were also made during this time. Unfortunately, we don’t have insight into those numbers. As it turned out, the number of people involved in a pairing session sometimes grew organically based on the difficulty of the task, the length of time it had been in active development, and how many previous pairing sessions had already occurred related to that task (probably because, as those numbers grow, so does the desire for more help to get it done). In our case, we never struggled with any kind of paralysis due to having too many cooks in the kitchen. Given these numbers and the participation scaling we saw, we’ve sort of adapted the word “pairing” on our team to apply to all of this. Whatever happens with regard to participation numbers happens. We usually just call it all “pairing” and that’s what we mean when we use that word in this post. So now, without further ado… How to get your team pairing Based on our numbers, we think that a lot of teams will probably discover that some of their members will spend more time pairing than others. We believe this can be ok, but it’s also surprisingly easy to end up with the same couple of folks pairing on everything while everyone else hangs back and works independently. To solve this problem, we came up with a couple of tricks: Try not to start private calls. In the beginning, many pairing sessions will likely begin with a private Slack message between two team members in which Member A has identified Member B as the person who likely has the most complementary knowledge base for the task at hand. However, getting the task done more quickly is only a small part of why we pair. We mainly do it for knowledge sharing, bond building, and exposure to other people’s ideas and techniques. So on our team, we tried to move away from starting a pairing call within that private Slack conversation, and instead we started opening our calls in a team-wide channel. This way, anyone who wanted to join was allowed, and if someone else became interested in an ongoing call, they could jump in at any time. We don’t have an official number on this, but of those calls with more than two participants, many began with only two 🍐-ers and then accumulated the others along the way. Explain the call when you post it. As your team gets more comfortable with pairing and with each other, our guess is that this will become less and less necessary. But in the beginning, it’s good for every pairing call you post to have a quick explanation of the goal for that call. While you’re at it, there’s no harm in explicitly calling out the fact that anyone is welcome to join, if for no other reason than that some team members may be a little more reserved. Here’s a quick screenshot to illustrate: It doesn’t need to be complicated. 😃 Make personal invitations to broaden your pairing partner horizons. Things like shyness, newness, introversion, lone-wolfishness, health problems, and all sorts of other things can lead to a situation where some team members are pairing less often than others. Again, if this is a personal preference, it’s absolutely ok. But if you spot this, you’ll probably want to just reach out and make sure. One way to work toward ensuring that equal pairing opportunities exist is to take a look at the pool of team members you’ve paired with over the course of, say, a week, and determine which people you’ve worked with the least. Once you identify those people, make a point to personally invite them to pair with you on something. We think you’ll be glad you did. Preemptively attack the awkwardness. Even among seasoned engineers and friendly team members, imposter syndrome and a general fear of being judged can make pairing a little awkward. When you’re controlling the code editor and taking suggestions from your pairing partners, those feelings can be amplified. If you have much experience pairing, you’ll probably be able to recount numerous occasions where someone you knew to be highly skilled and fully competent struggled to figure out where to begin, or how to code something that was conceptually pretty simple. And you probably also knew that this was entirely because they could feel you watching them . And this person was probably you, more than once . Because this experience can make some people a little more hesitant to jump into pairing (and also for the benefit of everybody’s general sanity), we suggest explicitly calling out the fact that these are universal experiences and that we all understand and accept them. Here are some example points to call out: A pairing session is not a job interview. When you pair, your employment and position at InVision are not on the line. We all understand that being the driver is a little weird, and that’s totally cool. No pressure. If somebody finds a weakness in your code, it doesn’t mean they think less of you as an engineer. Pairing should be fun. Definitely work on your task, but also, it’s ok to hang out and chat a bit. Embrace multi-person pairing sessions. One of the things that surprised me personally about our experiment was the fact that more than a third of our publicly posted pairing sessions had more than two people in them. This kind of group work can be great for new hires as we mentioned before, but it can be similarly beneficial for anyone who’s recently changed teams. It provides opportunities to work with as many experienced team members as possible, providing insight into how they reason about challenges within the team’s codebases, areas where the gotchas lie, and the team’s preferred coding styles. The 3-person pairing dynamic is particularly good for allowing a new team member to be a little less active on the call while absorbing knowledge and jumping in to ask questions at sticking points. Another benefit to multi-person pairing is that having more voices in the room can lead to a smoother code review process. Code can often come out of a pairing session more polished than it would have otherwise been due to real-time feedback and debate. Plus, pairing tends to result in multiple team members already backing a given approach to solving a problem by the time a pull request is opened. Whether or not you want to allow anyone who was involved in pairing to actually review the resulting code is totally up to you. In our case, we feel like we did a pretty good job of judging it on a case-by-case basis. The hidden benefits of pairing As our number of pairing sessions grew, we began to dig up a few gems we hadn’t initially expected to find. As it turns out, the added time we spent sharing thoughts and ideas led to a lot of meaningful upgrades we are now making to our team’s systems and practices. We’ll try to categorize some of those hidden benefits here. Confirming meaningful tasks Have you ever been bothered by something in the code, but weren’t sure if it was worth the investment to go and tackle? Maybe you let it go because you worried your opinion was the odd one out. Or maybe you weren’t sure it was really worth bringing up in the format of a more official meeting when other priorities seemed more pressing. Try asking what your pairing partners think when the problem shows up during a multi-person session. They may give you just the push you need to get it taken care of. Multitasking Have you ever noticed a bug while working on something unrelated, tried to remember to make a ticket when you were finished, but then completely forgot about it? Try asking a pairing partner to make a ticket the moment you notice it, while you and another pairing partner continue to focus on the task at hand. You may end up making your QA engineer’s work a lot easier. Philosophy upgrades Have you ever wondered why your team always uses some particular technique that seems strange to you? Try asking about it during a pairing session when you see that technique come into play. Maybe your teammates know something you don’t. Maybe it’s the other way around. Either way, it can make for some enlightening discussion and a great venue for upgrading your team’s philosophies and approaches to problem solving. The future of pairing Throughout the course of our experiment, we didn’t spend a lot of time testing out apps and tools specifically designed for pairing. Instead, we relied mainly on Zoom and Slack calls, and whatever conveniences they provided. In the future we plan on digging into this area a lot more, testing out all the apps and techniques we can find. Hopefully we’ll have some updates for you soon. In the meantime, stop by on Twitter to ask us more about our experiment. Or even better, feel free to share some of your own experiences with pairing. With your help, maybe we can further solidify a culture of knowledge sharing and building relationships in the industry that will last for years to come. If that sounds interesting to you, let’s 🍐 on it. Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2020-02-19"},
{"website": "Invision", "title": "Why I Joined InVision as CTO", "author": ["Sanjeev Katariya"], "link": "https://engineering.invisionapp.com/post/sanjeev-katariya-invision-cto/", "abstract": "I may be a CTO, but people—even more than technology—are what excite me. I’ve been lucky enough to work at companies that touch nearly every corner of the world. At Microsoft, I was fortunate enough to help found major technological, team and product initiatives and see them through to rollout. These initiatives, from inception like Windows NT (1990), MSN (1993), the first Microsoft Internet Search service (1995), Natural User Interfaces (2001) and Microsoft Teams (2015), taught me that people and collaboration are the driving force behind successful product and tech design. At eBay (2015), I learned how to balance paying down decades of tech debt while innovating and advancing the state of platforms and AI, all while working across global multi-disciplinary teams as I led the technology stack’s evolution. Filling a tall order like that is only possible when designers, product managers, developers, and the c-suite are on the same page. I have spent 30 years in this industry, so I have seen the evolution of engineering, product and development in major companies. Every person at nearly every company wants to put out a world-class digital experience. And yet, digital product and tech teams are struggling. I know the challenges they face. InVision has tapped something that is gold: A true innovative platform to speed innovation in digital product design and development. As the CTO, I am excited about the technical innovations that we are shipping to our customers. We are producing a tool set that will be the end-to-end digital product design and development platform. InVision is changing the way industries within and outside enterprises, nonprofits, NGOs—really all types of organizations—create apps, websites and so much more. The work that InVision does touches everything. We have brought a technologically-advanced lens to pulling together connected flows that help developers inspect designs with a cloud offering that expedites customer code from design specifications through delivery. And we have built all of this on a robust web end-to-end secure and performant architecture along with state-of-the-art performance and quality frameworks. There is so much in our technological treasure chest with much more to come that will surely delight our customers. Within InVision, I am also stunned by the raw talent throughout all the ranks, how caring everyone is towards each other and our customers, and everyone’s willing and wanting to collaborate together and tackle roadblocks. InVision CEO Clark Valberg’s vision of a connected workflow is inspiring for us all. He identified this area 10 years ago, and was prescient enough to recognize the power of a fully distributed company well before that was commonplace. The rest of the executive leadership team are just as innovative, smart, and driven. I am especially pleased that Eleanor Morgan , our chief product officer, is here to be my partner in achieving our strategy and vision. Individuals can be talented, but it takes a team to be successful. The right team—a team infused with the spirit of innovation, a team dedicated to developing and growing talent, a team that does not fear failure and challenges—can create the technologies that change how we interact with our screens, and the world. I am so excited to join this team, and to empower teams everywhere. All in all, I believe I am truly lucky to have this opportunity! Now that I’m at InVision, I look forward to bringing to InVision increased focus on quality shipping, best-in-class product, and technical innovation. Having the right priorities clearly stated, agreed and committed is the essential starting point. We will continuously innovate and execute with precision, and we are on that journey already with the launch of the New Design System Manager last week, a platform that is already beloved and transformative for global-scale design and development teams. Our customers love it and their satisfaction is key. I’m thrilled to be here and to help take InVision to an entirely new level of innovation, technological rigor, and focus. Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2020-08-24"},
{"website": "Invision", "title": "Innovating with Distributed Hackathons", "author": ["Ben Darfler"], "link": "https://engineering.invisionapp.com/post/innovating-with-distributed-hackathons/", "abstract": "Innovation is the lifeblood of a tech startup. Keeping it flowing is a challenge for every tech startup that grows beyond a small team and that challenge is multiplied when a company is fully distributed like InVision. Last month we kicked off our first distributed, cross-team hackathon with an explicit goal of breaking team silos and getting that lifeblood flowing. Spoiler alert; it was a huge success. This article aims to capture that lightning in a bottle to help both our future selves and other companies replicate our success. Laying The Groundwork InVision is primarily organized into independent development teams that own their own product delivery. This approach comes with many strengths but it can also come at the cost of cross-team collaboration and enablement. To counter that, our primary intent for the hackathon was to build and strengthen personal relationships between the teams. A close second was to build a shared understanding of our combined product visions. Not only were these important goals in their own right but we also knew that focusing on them was the best way to create an environment for innovation. About a month before the event, the engineering and product managers for the teams got together and hashed out a plan. We picked a 4-day window for the hackathon and talked through the high-level details. From there we created a slack channel, invited all the participants, and shared an early outline of the event including the goals, dates, attendees, and a high-level schedule. Going forward we would use the channel to build excitement with the occasional reminder or update as we worked out the details. Focusing The Effort While many hackathons give their participants free rein to pick a project, we knew that having constraints would breed creativity. To ensure the hackathon had the impact we envisioned we narrowed our scope to the biggest blockers and most frequent pain points we encountered while building our product vision. Key technical issues that were slowing our teams down or preventing them from accomplishing their product goals were organized into a set of “Problems and Possibilities” which was shared in the hackathon slack channel to get everyone thinking about how to best use our time. Additionally, “Support Staff” with relevant experience in these areas were recruited from around the organization. This group of designers, product managers, engineering managers, and principal engineers committed to being “on-call” for consultation during the hackathon to ensure the teams had all the support they would need. Kicking It Off Finally, the first day of the hackathon had arrived. Around 35 attendees from multiple teams and disciplines helped us kick things off. Going around the room for introductions with 35 folks would have been a huge energy drain. To keep the meeting moving we went with Zoom break out room introductions. A quick five minutes later everyone was feeling the love and we moved on to a quick review of the goals and schedule. The meat of the kickoff came as our talented product managers walked us through our ambitious combined product vision across the multiple teams. As they went, they highlighted common themes and struggles across the teams, painting a clear picture of the value of the “Problems and Possibilities” we had shared. With everyone raring to go we took a page out of the unconference playbook and spun up a Freehand to self organize and dropped from Zoom. After a few hours of asynchronous discussion and discovery, we reconvened on Zoom. The group had coalesced around a small number of projects and three engineers offered to lead three of them. Everyone else picked a project and we were off to the races. Wrapping It Up Over the next three days, the teams put their all into proving out concepts, exposing new APIs, combining existing projects in innovative ways, and building a new architecture for sharing functionality between teams. Around mid-afternoon, on the final day of the hackathon, we held our demo party. The excitement in the “room” was palpable. One after another the teams unleashed incredible demos showing the power of cross-team collaboration. The recording of the Zoom and short write-ups from the teams (including a paragraph on “what would it take to do it for real”) was shared across the organization and garnered an enormous amount of excitement. While it wasn’t planned, the hackathon landed in the middle of quarterly planning which only amplified its success and impact and we are now executing on much of the potential that was shown over 4 exciting days in April. Epilogue - In The Details One question we struggled with was how to capture that co-located hackathon “feel”. This is an area ripe for fun and interesting ideas. For this hackathon, we provided a food and drink stipend to each of the participants to cover at a meal a day (maybe more) and a celebratory beverage. We also hosted a morning coffee to kick off each day, celebrate early successes, offer support, and keep building those cross-team personal bonds. The combination seemed to hit the mark but is by no means the only way to do it. After it was all over, we sent out a survey to the group for feedback. While we had nearly universal high marks, one theme for improvement did stand out. The participants wanted to see more structure to ensure the hackathon work didn’t land in the waste bin once the hackathon was over. While we lucked into quarterly planning cycles, this is an area that we would think about more deeply next time. Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2020-06-03"},
{"website": "Invision", "title": null, "author": "Unknown", "link": "https://engineering.invisionapp.com/post/", "abstract": "", "date": "2020-09-14"},
{"website": "Invision", "title": "Join InVision CTO Sanjeev Katariya for a talk on developing AI", "author": ["Sanjeev Katariya"], "link": "https://engineering.invisionapp.com/post/sanjeev-katariya-aztc-talk/", "abstract": "Hello—InVision CTO Sanjeev Katariya, here. I’d like to invite you to a virtual talk I’m giving for the Arizona Technology Council’s (AZTC) Tech Sector Speaker Series on Tuesday, Sept 15 from 3:30 - 5 p.m. GMT about the experiences organizations face on their AI journey. AI is transforming the world, but organizations need to understand the problems, opportunities, and solutions that come with the technology when implemented in the workspace. I’ll be joined by leaders from Mediato Technologies, MST Solutions, Botco.ai, and the Washington University in St. Louis to discuss the opportunities that AI, as well as machine learning and other emerging technologies, will change the way we work. The talk will be $10 for AZTC members, and $20 for non-members . It is sure to be an interesting time and I would love to share the experience with you. Hope to see you there! Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions . Like what you've been reading? Join us and help create the next generation of prototyping and collaboration tools for product design teams around the world. Check out our open positions .", "date": "2020-09-14"}
]